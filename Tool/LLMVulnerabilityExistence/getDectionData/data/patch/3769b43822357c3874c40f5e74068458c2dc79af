{
    "src/snowflake/connector/auth/_auth.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 52,
                "PatchRowcode": "     ProgrammingError,"
            },
            "1": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 53,
                "PatchRowcode": "     ServiceUnavailableError,"
            },
            "2": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 54,
                "PatchRowcode": " )"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+from ..file_util import owner_rw_opener"
            },
            "4": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": 56,
                "PatchRowcode": " from ..network import ("
            },
            "5": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 57,
                "PatchRowcode": "     ACCEPT_TYPE_APPLICATION_SNOWFLAKE,"
            },
            "6": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 58,
                "PatchRowcode": "     CONTENT_TYPE_APPLICATION_JSON,"
            },
            "7": {
                "beforePatchRowNumber": 625,
                "afterPatchRowNumber": 626,
                "PatchRowcode": "         )"
            },
            "8": {
                "beforePatchRowNumber": 626,
                "afterPatchRowNumber": 627,
                "PatchRowcode": "     try:"
            },
            "9": {
                "beforePatchRowNumber": 627,
                "afterPatchRowNumber": 628,
                "PatchRowcode": "         with open("
            },
            "10": {
                "beforePatchRowNumber": 628,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            TEMPORARY_CREDENTIAL_FILE, \"w\", encoding=\"utf-8\", errors=\"ignore\""
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 629,
                "PatchRowcode": "+            TEMPORARY_CREDENTIAL_FILE,"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 630,
                "PatchRowcode": "+            \"w\","
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 631,
                "PatchRowcode": "+            encoding=\"utf-8\","
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 632,
                "PatchRowcode": "+            errors=\"ignore\","
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 633,
                "PatchRowcode": "+            opener=owner_rw_opener,"
            },
            "16": {
                "beforePatchRowNumber": 629,
                "afterPatchRowNumber": 634,
                "PatchRowcode": "         ) as f:"
            },
            "17": {
                "beforePatchRowNumber": 630,
                "afterPatchRowNumber": 635,
                "PatchRowcode": "             json.dump(TEMPORARY_CREDENTIAL, f)"
            },
            "18": {
                "beforePatchRowNumber": 631,
                "afterPatchRowNumber": 636,
                "PatchRowcode": "     except Exception as ex:"
            }
        },
        "frontPatchFile": [
            "#",
            "# Copyright (c) 2012-2023 Snowflake Computing Inc. All rights reserved.",
            "#",
            "",
            "from __future__ import annotations",
            "",
            "import codecs",
            "import copy",
            "import json",
            "import logging",
            "import tempfile",
            "import time",
            "import uuid",
            "from datetime import datetime, timezone",
            "from os import getenv, makedirs, mkdir, path, remove, removedirs, rmdir",
            "from os.path import expanduser",
            "from threading import Lock, Thread",
            "from typing import TYPE_CHECKING, Any, Callable",
            "",
            "from cryptography.hazmat.backends import default_backend",
            "from cryptography.hazmat.primitives.serialization import (",
            "    Encoding,",
            "    NoEncryption,",
            "    PrivateFormat,",
            "    load_der_private_key,",
            "    load_pem_private_key,",
            ")",
            "",
            "from ..compat import IS_LINUX, IS_MACOS, IS_WINDOWS, urlencode",
            "from ..constants import (",
            "    DAY_IN_SECONDS,",
            "    HTTP_HEADER_ACCEPT,",
            "    HTTP_HEADER_CONTENT_TYPE,",
            "    HTTP_HEADER_SERVICE_NAME,",
            "    HTTP_HEADER_USER_AGENT,",
            "    PARAMETER_CLIENT_REQUEST_MFA_TOKEN,",
            "    PARAMETER_CLIENT_STORE_TEMPORARY_CREDENTIAL,",
            ")",
            "from ..description import (",
            "    COMPILER,",
            "    IMPLEMENTATION,",
            "    OPERATING_SYSTEM,",
            "    PLATFORM,",
            "    PYTHON_VERSION,",
            ")",
            "from ..errorcode import ER_FAILED_TO_CONNECT_TO_DB",
            "from ..errors import (",
            "    BadGatewayError,",
            "    DatabaseError,",
            "    Error,",
            "    ForbiddenError,",
            "    ProgrammingError,",
            "    ServiceUnavailableError,",
            ")",
            "from ..network import (",
            "    ACCEPT_TYPE_APPLICATION_SNOWFLAKE,",
            "    CONTENT_TYPE_APPLICATION_JSON,",
            "    ID_TOKEN_INVALID_LOGIN_REQUEST_GS_CODE,",
            "    PYTHON_CONNECTOR_USER_AGENT,",
            "    ReauthenticationRequest,",
            ")",
            "from ..options import installed_keyring, keyring",
            "from ..sqlstate import SQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED",
            "from ..version import VERSION",
            "",
            "if TYPE_CHECKING:",
            "    from . import AuthByPlugin",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "# Cache directory",
            "CACHE_ROOT_DIR = (",
            "    getenv(\"SF_TEMPORARY_CREDENTIAL_CACHE_DIR\")",
            "    or expanduser(\"~\")",
            "    or tempfile.gettempdir()",
            ")",
            "if IS_WINDOWS:",
            "    CACHE_DIR = path.join(CACHE_ROOT_DIR, \"AppData\", \"Local\", \"Snowflake\", \"Caches\")",
            "elif IS_MACOS:",
            "    CACHE_DIR = path.join(CACHE_ROOT_DIR, \"Library\", \"Caches\", \"Snowflake\")",
            "else:",
            "    CACHE_DIR = path.join(CACHE_ROOT_DIR, \".cache\", \"snowflake\")",
            "",
            "if not path.exists(CACHE_DIR):",
            "    try:",
            "        makedirs(CACHE_DIR, mode=0o700)",
            "    except Exception as ex:",
            "        logger.debug(\"cannot create a cache directory: [%s], err=[%s]\", CACHE_DIR, ex)",
            "        CACHE_DIR = None",
            "logger.debug(\"cache directory: %s\", CACHE_DIR)",
            "",
            "# temporary credential cache",
            "TEMPORARY_CREDENTIAL: dict[str, dict[str, str | None]] = {}",
            "",
            "TEMPORARY_CREDENTIAL_LOCK = Lock()",
            "",
            "# temporary credential cache file name",
            "TEMPORARY_CREDENTIAL_FILE = \"temporary_credential.json\"",
            "TEMPORARY_CREDENTIAL_FILE = (",
            "    path.join(CACHE_DIR, TEMPORARY_CREDENTIAL_FILE) if CACHE_DIR else \"\"",
            ")",
            "",
            "# temporary credential cache lock directory name",
            "TEMPORARY_CREDENTIAL_FILE_LOCK = TEMPORARY_CREDENTIAL_FILE + \".lck\"",
            "",
            "# keyring",
            "KEYRING_SERVICE_NAME = \"net.snowflake.temporary_token\"",
            "KEYRING_USER = \"temp_token\"",
            "KEYRING_DRIVER_NAME = \"SNOWFLAKE-PYTHON-DRIVER\"",
            "",
            "ID_TOKEN = \"ID_TOKEN\"",
            "MFA_TOKEN = \"MFATOKEN\"",
            "",
            "AUTHENTICATION_REQUEST_KEY_WHITELIST = {",
            "    \"ACCOUNT_NAME\",",
            "    \"AUTHENTICATOR\",",
            "    \"CLIENT_APP_ID\",",
            "    \"CLIENT_APP_VERSION\",",
            "    \"CLIENT_ENVIRONMENT\",",
            "    \"EXT_AUTHN_DUO_METHOD\",",
            "    \"LOGIN_NAME\",",
            "    \"SESSION_PARAMETERS\",",
            "    \"SVN_REVISION\",",
            "}",
            "",
            "",
            "class Auth:",
            "    \"\"\"Snowflake Authenticator.\"\"\"",
            "",
            "    def __init__(self, rest) -> None:",
            "        self._rest = rest",
            "",
            "    @staticmethod",
            "    def base_auth_data(",
            "        user,",
            "        account,",
            "        application,",
            "        internal_application_name,",
            "        internal_application_version,",
            "        ocsp_mode,",
            "        login_timeout: int | None = None,",
            "        network_timeout: int | None = None,",
            "        socket_timeout: int | None = None,",
            "    ):",
            "        return {",
            "            \"data\": {",
            "                \"CLIENT_APP_ID\": internal_application_name,",
            "                \"CLIENT_APP_VERSION\": internal_application_version,",
            "                \"SVN_REVISION\": VERSION[3],",
            "                \"ACCOUNT_NAME\": account,",
            "                \"LOGIN_NAME\": user,",
            "                \"CLIENT_ENVIRONMENT\": {",
            "                    \"APPLICATION\": application,",
            "                    \"OS\": OPERATING_SYSTEM,",
            "                    \"OS_VERSION\": PLATFORM,",
            "                    \"PYTHON_VERSION\": PYTHON_VERSION,",
            "                    \"PYTHON_RUNTIME\": IMPLEMENTATION,",
            "                    \"PYTHON_COMPILER\": COMPILER,",
            "                    \"OCSP_MODE\": ocsp_mode.name,",
            "                    \"TRACING\": logger.getEffectiveLevel(),",
            "                    \"LOGIN_TIMEOUT\": login_timeout,",
            "                    \"NETWORK_TIMEOUT\": network_timeout,",
            "                    \"SOCKET_TIMEOUT\": socket_timeout,",
            "                },",
            "            },",
            "        }",
            "",
            "    def authenticate(",
            "        self,",
            "        auth_instance: AuthByPlugin,",
            "        account: str,",
            "        user: str,",
            "        database: str | None = None,",
            "        schema: str | None = None,",
            "        warehouse: str | None = None,",
            "        role: str | None = None,",
            "        passcode: str | None = None,",
            "        passcode_in_password: bool = False,",
            "        mfa_callback: Callable[[], None] | None = None,",
            "        password_callback: Callable[[], str] | None = None,",
            "        session_parameters: dict[Any, Any] | None = None,",
            "        # max time waiting for MFA response, currently unused",
            "        timeout: int | None = None,",
            "    ) -> dict[str, str | int | bool]:",
            "        logger.debug(\"authenticate\")",
            "",
            "        if timeout is None:",
            "            timeout = auth_instance.timeout",
            "",
            "        if session_parameters is None:",
            "            session_parameters = {}",
            "",
            "        request_id = str(uuid.uuid4())",
            "        headers = {",
            "            HTTP_HEADER_CONTENT_TYPE: CONTENT_TYPE_APPLICATION_JSON,",
            "            HTTP_HEADER_ACCEPT: ACCEPT_TYPE_APPLICATION_SNOWFLAKE,",
            "            HTTP_HEADER_USER_AGENT: PYTHON_CONNECTOR_USER_AGENT,",
            "        }",
            "        if HTTP_HEADER_SERVICE_NAME in session_parameters:",
            "            headers[HTTP_HEADER_SERVICE_NAME] = session_parameters[",
            "                HTTP_HEADER_SERVICE_NAME",
            "            ]",
            "        url = \"/session/v1/login-request\"",
            "",
            "        body_template = Auth.base_auth_data(",
            "            user,",
            "            account,",
            "            self._rest._connection.application,",
            "            self._rest._connection._internal_application_name,",
            "            self._rest._connection._internal_application_version,",
            "            self._rest._connection._ocsp_mode(),",
            "            self._rest._connection.login_timeout,",
            "            self._rest._connection._network_timeout,",
            "            self._rest._connection._socket_timeout,",
            "        )",
            "",
            "        body = copy.deepcopy(body_template)",
            "        # updating request body",
            "        auth_instance.update_body(body)",
            "",
            "        logger.debug(",
            "            \"account=%s, user=%s, database=%s, schema=%s, \"",
            "            \"warehouse=%s, role=%s, request_id=%s\",",
            "            account,",
            "            user,",
            "            database,",
            "            schema,",
            "            warehouse,",
            "            role,",
            "            request_id,",
            "        )",
            "        url_parameters = {\"request_id\": request_id}",
            "        if database is not None:",
            "            url_parameters[\"databaseName\"] = database",
            "        if schema is not None:",
            "            url_parameters[\"schemaName\"] = schema",
            "        if warehouse is not None:",
            "            url_parameters[\"warehouse\"] = warehouse",
            "        if role is not None:",
            "            url_parameters[\"roleName\"] = role",
            "",
            "        url = url + \"?\" + urlencode(url_parameters)",
            "",
            "        # first auth request",
            "        if passcode_in_password:",
            "            body[\"data\"][\"EXT_AUTHN_DUO_METHOD\"] = \"passcode\"",
            "        elif passcode:",
            "            body[\"data\"][\"EXT_AUTHN_DUO_METHOD\"] = \"passcode\"",
            "            body[\"data\"][\"PASSCODE\"] = passcode",
            "",
            "        if session_parameters:",
            "            body[\"data\"][\"SESSION_PARAMETERS\"] = session_parameters",
            "",
            "        logger.debug(",
            "            \"body['data']: %s\",",
            "            {",
            "                k: v if k in AUTHENTICATION_REQUEST_KEY_WHITELIST else \"******\"",
            "                for (k, v) in body[\"data\"].items()",
            "            },",
            "        )",
            "",
            "        try:",
            "            ret = self._rest._post_request(",
            "                url,",
            "                headers,",
            "                json.dumps(body),",
            "                socket_timeout=auth_instance._socket_timeout,",
            "            )",
            "        except ForbiddenError as err:",
            "            # HTTP 403",
            "            raise err.__class__(",
            "                msg=(",
            "                    \"Failed to connect to DB. \"",
            "                    \"Verify the account name is correct: {host}:{port}. \"",
            "                    \"{message}\"",
            "                ).format(",
            "                    host=self._rest._host, port=self._rest._port, message=str(err)",
            "                ),",
            "                errno=ER_FAILED_TO_CONNECT_TO_DB,",
            "                sqlstate=SQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED,",
            "            )",
            "        except (ServiceUnavailableError, BadGatewayError) as err:",
            "            # HTTP 502/504",
            "            raise err.__class__(",
            "                msg=(",
            "                    \"Failed to connect to DB. \"",
            "                    \"Service is unavailable: {host}:{port}. \"",
            "                    \"{message}\"",
            "                ).format(",
            "                    host=self._rest._host, port=self._rest._port, message=str(err)",
            "                ),",
            "                errno=ER_FAILED_TO_CONNECT_TO_DB,",
            "                sqlstate=SQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED,",
            "            )",
            "",
            "        # waiting for MFA authentication",
            "        if ret[\"data\"] and ret[\"data\"].get(\"nextAction\") in (",
            "            \"EXT_AUTHN_DUO_ALL\",",
            "            \"EXT_AUTHN_DUO_PUSH_N_PASSCODE\",",
            "        ):",
            "            body[\"inFlightCtx\"] = ret[\"data\"].get(\"inFlightCtx\")",
            "            body[\"data\"][\"EXT_AUTHN_DUO_METHOD\"] = \"push\"",
            "            self.ret = {\"message\": \"Timeout\", \"data\": {}}",
            "",
            "            def post_request_wrapper(self, url, headers, body) -> None:",
            "                # get the MFA response",
            "                self.ret = self._rest._post_request(",
            "                    url,",
            "                    headers,",
            "                    body,",
            "                    socket_timeout=auth_instance._socket_timeout,",
            "                )",
            "",
            "            # send new request to wait until MFA is approved",
            "            t = Thread(",
            "                target=post_request_wrapper, args=[self, url, headers, json.dumps(body)]",
            "            )",
            "            t.daemon = True",
            "            t.start()",
            "            if callable(mfa_callback):",
            "                c = mfa_callback()",
            "                while not self.ret or self.ret.get(\"message\") == \"Timeout\":",
            "                    next(c)",
            "            else:",
            "                # _post_request should already terminate on timeout, so this is just a safeguard",
            "                t.join(timeout=timeout)",
            "",
            "            ret = self.ret",
            "            if (",
            "                ret",
            "                and ret[\"data\"]",
            "                and ret[\"data\"].get(\"nextAction\") == \"EXT_AUTHN_SUCCESS\"",
            "            ):",
            "                body = copy.deepcopy(body_template)",
            "                body[\"inFlightCtx\"] = ret[\"data\"].get(\"inFlightCtx\")",
            "                # final request to get tokens",
            "                ret = self._rest._post_request(",
            "                    url,",
            "                    headers,",
            "                    json.dumps(body),",
            "                    socket_timeout=auth_instance._socket_timeout,",
            "                )",
            "            elif not ret or not ret[\"data\"] or not ret[\"data\"].get(\"token\"):",
            "                # not token is returned.",
            "                Error.errorhandler_wrapper(",
            "                    self._rest._connection,",
            "                    None,",
            "                    DatabaseError,",
            "                    {",
            "                        \"msg\": (",
            "                            \"Failed to connect to DB. MFA \"",
            "                            \"authentication failed: {\"",
            "                            \"host}:{port}. {message}\"",
            "                        ).format(",
            "                            host=self._rest._host,",
            "                            port=self._rest._port,",
            "                            message=ret[\"message\"],",
            "                        ),",
            "                        \"errno\": ER_FAILED_TO_CONNECT_TO_DB,",
            "                        \"sqlstate\": SQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED,",
            "                    },",
            "                )",
            "                return session_parameters  # required for unit test",
            "",
            "        elif ret[\"data\"] and ret[\"data\"].get(\"nextAction\") == \"PWD_CHANGE\":",
            "            if callable(password_callback):",
            "                body = copy.deepcopy(body_template)",
            "                body[\"inFlightCtx\"] = ret[\"data\"].get(\"inFlightCtx\")",
            "                body[\"data\"][\"LOGIN_NAME\"] = user",
            "                body[\"data\"][\"PASSWORD\"] = (",
            "                    auth_instance.password",
            "                    if hasattr(auth_instance, \"password\")",
            "                    else None",
            "                )",
            "                body[\"data\"][\"CHOSEN_NEW_PASSWORD\"] = password_callback()",
            "                # New Password input",
            "                ret = self._rest._post_request(",
            "                    url,",
            "                    headers,",
            "                    json.dumps(body),",
            "                    socket_timeout=auth_instance._socket_timeout,",
            "                )",
            "",
            "        logger.debug(\"completed authentication\")",
            "        if not ret[\"success\"]:",
            "            errno = ret.get(\"code\", ER_FAILED_TO_CONNECT_TO_DB)",
            "            if errno == ID_TOKEN_INVALID_LOGIN_REQUEST_GS_CODE:",
            "                # clear stored id_token if failed to connect because of id_token",
            "                # raise an exception for reauth without id_token",
            "                self._rest.id_token = None",
            "                delete_temporary_credential(self._rest._host, user, ID_TOKEN)",
            "                raise ReauthenticationRequest(",
            "                    ProgrammingError(",
            "                        msg=ret[\"message\"],",
            "                        errno=int(errno),",
            "                        sqlstate=SQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED,",
            "                    )",
            "                )",
            "",
            "            from . import AuthByKeyPair",
            "",
            "            if isinstance(auth_instance, AuthByKeyPair):",
            "                logger.debug(",
            "                    \"JWT Token authentication failed. \"",
            "                    \"Token expires at: %s. \"",
            "                    \"Current Time: %s\",",
            "                    str(auth_instance._jwt_token_exp),",
            "                    str(datetime.now(timezone.utc).replace(tzinfo=None)),",
            "                )",
            "            from . import AuthByUsrPwdMfa",
            "",
            "            if isinstance(auth_instance, AuthByUsrPwdMfa):",
            "                delete_temporary_credential(self._rest._host, user, MFA_TOKEN)",
            "            Error.errorhandler_wrapper(",
            "                self._rest._connection,",
            "                None,",
            "                DatabaseError,",
            "                {",
            "                    \"msg\": (",
            "                        \"Failed to connect to DB: {host}:{port}. \" \"{message}\"",
            "                    ).format(",
            "                        host=self._rest._host,",
            "                        port=self._rest._port,",
            "                        message=ret[\"message\"],",
            "                    ),",
            "                    \"errno\": ER_FAILED_TO_CONNECT_TO_DB,",
            "                    \"sqlstate\": SQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED,",
            "                },",
            "            )",
            "        else:",
            "            logger.debug(",
            "                \"token = %s\",",
            "                (",
            "                    \"******\"",
            "                    if ret[\"data\"] and ret[\"data\"].get(\"token\") is not None",
            "                    else \"NULL\"",
            "                ),",
            "            )",
            "            logger.debug(",
            "                \"master_token = %s\",",
            "                (",
            "                    \"******\"",
            "                    if ret[\"data\"] and ret[\"data\"].get(\"masterToken\") is not None",
            "                    else \"NULL\"",
            "                ),",
            "            )",
            "            logger.debug(",
            "                \"id_token = %s\",",
            "                (",
            "                    \"******\"",
            "                    if ret[\"data\"] and ret[\"data\"].get(\"idToken\") is not None",
            "                    else \"NULL\"",
            "                ),",
            "            )",
            "            logger.debug(",
            "                \"mfa_token = %s\",",
            "                (",
            "                    \"******\"",
            "                    if ret[\"data\"] and ret[\"data\"].get(\"mfaToken\") is not None",
            "                    else \"NULL\"",
            "                ),",
            "            )",
            "            if not ret[\"data\"]:",
            "                Error.errorhandler_wrapper(",
            "                    None,",
            "                    None,",
            "                    Error,",
            "                    {",
            "                        \"msg\": \"There is no data in the returning response, please retry the operation.\"",
            "                    },",
            "                )",
            "            self._rest.update_tokens(",
            "                ret[\"data\"].get(\"token\"),",
            "                ret[\"data\"].get(\"masterToken\"),",
            "                master_validity_in_seconds=ret[\"data\"].get(\"masterValidityInSeconds\"),",
            "                id_token=ret[\"data\"].get(\"idToken\"),",
            "                mfa_token=ret[\"data\"].get(\"mfaToken\"),",
            "            )",
            "            self.write_temporary_credentials(",
            "                self._rest._host, user, session_parameters, ret",
            "            )",
            "            if ret[\"data\"] and \"sessionId\" in ret[\"data\"]:",
            "                self._rest._connection._session_id = ret[\"data\"].get(\"sessionId\")",
            "            if ret[\"data\"] and \"sessionInfo\" in ret[\"data\"]:",
            "                session_info = ret[\"data\"].get(\"sessionInfo\")",
            "                self._rest._connection._database = session_info.get(\"databaseName\")",
            "                self._rest._connection._schema = session_info.get(\"schemaName\")",
            "                self._rest._connection._warehouse = session_info.get(\"warehouseName\")",
            "                self._rest._connection._role = session_info.get(\"roleName\")",
            "            if ret[\"data\"] and \"parameters\" in ret[\"data\"]:",
            "                session_parameters.update(",
            "                    {p[\"name\"]: p[\"value\"] for p in ret[\"data\"].get(\"parameters\")}",
            "                )",
            "            self._rest._connection._update_parameters(session_parameters)",
            "            return session_parameters",
            "",
            "    def _read_temporary_credential(",
            "        self,",
            "        host: str,",
            "        user: str,",
            "        cred_type: str,",
            "    ) -> str | None:",
            "        cred = None",
            "        if IS_MACOS or IS_WINDOWS:",
            "            if not installed_keyring:",
            "                logger.debug(",
            "                    \"Dependency 'keyring' is not installed, cannot cache id token. You might experience \"",
            "                    \"multiple authentication pop ups while using ExternalBrowser Authenticator. To avoid \"",
            "                    \"this please install keyring module using the following command : pip install \"",
            "                    \"snowflake-connector-python[secure-local-storage]\"",
            "                )",
            "                return None",
            "            try:",
            "                cred = keyring.get_password(",
            "                    build_temporary_credential_name(host, user, cred_type), user.upper()",
            "                )",
            "            except keyring.errors.KeyringError as ke:",
            "                logger.error(",
            "                    \"Could not retrieve {} from secure storage : {}\".format(",
            "                        cred_type, str(ke)",
            "                    )",
            "                )",
            "        elif IS_LINUX:",
            "            read_temporary_credential_file()",
            "            cred = TEMPORARY_CREDENTIAL.get(host.upper(), {}).get(",
            "                build_temporary_credential_name(host, user, cred_type)",
            "            )",
            "        else:",
            "            logger.debug(\"OS not supported for Local Secure Storage\")",
            "        return cred",
            "",
            "    def read_temporary_credentials(",
            "        self,",
            "        host: str,",
            "        user: str,",
            "        session_parameters: dict[str, Any],",
            "    ) -> None:",
            "        if session_parameters.get(PARAMETER_CLIENT_STORE_TEMPORARY_CREDENTIAL, False):",
            "            self._rest.id_token = self._read_temporary_credential(",
            "                host,",
            "                user,",
            "                ID_TOKEN,",
            "            )",
            "",
            "        if session_parameters.get(PARAMETER_CLIENT_REQUEST_MFA_TOKEN, False):",
            "            self._rest.mfa_token = self._read_temporary_credential(",
            "                host,",
            "                user,",
            "                MFA_TOKEN,",
            "            )",
            "",
            "    def _write_temporary_credential(",
            "        self,",
            "        host: str,",
            "        user: str,",
            "        cred_type: str,",
            "        cred: str | None,",
            "    ) -> None:",
            "        if not cred:",
            "            logger.debug(",
            "                \"no credential is given when try to store temporary credential\"",
            "            )",
            "            return",
            "        if IS_MACOS or IS_WINDOWS:",
            "            if not installed_keyring:",
            "                logger.debug(",
            "                    \"Dependency 'keyring' is not installed, cannot cache id token. You might experience \"",
            "                    \"multiple authentication pop ups while using ExternalBrowser Authenticator. To avoid \"",
            "                    \"this please install keyring module using the following command : pip install \"",
            "                    \"snowflake-connector-python[secure-local-storage]\"",
            "                )",
            "                return",
            "            try:",
            "                keyring.set_password(",
            "                    build_temporary_credential_name(host, user, cred_type),",
            "                    user.upper(),",
            "                    cred,",
            "                )",
            "            except keyring.errors.KeyringError as ke:",
            "                logger.error(\"Could not store id_token to keyring, %s\", str(ke))",
            "        elif IS_LINUX:",
            "            write_temporary_credential_file(",
            "                host, build_temporary_credential_name(host, user, cred_type), cred",
            "            )",
            "        else:",
            "            logger.debug(\"OS not supported for Local Secure Storage\")",
            "",
            "    def write_temporary_credentials(",
            "        self,",
            "        host: str,",
            "        user: str,",
            "        session_parameters: dict[str, Any],",
            "        response: dict[str, Any],",
            "    ) -> None:",
            "        if (",
            "            self._rest._connection.auth_class.consent_cache_id_token",
            "            and session_parameters.get(",
            "                PARAMETER_CLIENT_STORE_TEMPORARY_CREDENTIAL, False",
            "            )",
            "        ):",
            "            self._write_temporary_credential(",
            "                host, user, ID_TOKEN, response[\"data\"].get(\"idToken\")",
            "            )",
            "",
            "        if session_parameters.get(PARAMETER_CLIENT_REQUEST_MFA_TOKEN, False):",
            "            self._write_temporary_credential(",
            "                host, user, MFA_TOKEN, response[\"data\"].get(\"mfaToken\")",
            "            )",
            "",
            "",
            "def flush_temporary_credentials() -> None:",
            "    \"\"\"Flush temporary credentials in memory into disk. Need to hold TEMPORARY_CREDENTIAL_LOCK.\"\"\"",
            "    global TEMPORARY_CREDENTIAL",
            "    global TEMPORARY_CREDENTIAL_FILE",
            "    for _ in range(10):",
            "        if lock_temporary_credential_file():",
            "            break",
            "        time.sleep(1)",
            "    else:",
            "        logger.debug(",
            "            \"The lock file still persists after the maximum wait time.\"",
            "            \"Will ignore it and write temporary credential file: %s\",",
            "            TEMPORARY_CREDENTIAL_FILE,",
            "        )",
            "    try:",
            "        with open(",
            "            TEMPORARY_CREDENTIAL_FILE, \"w\", encoding=\"utf-8\", errors=\"ignore\"",
            "        ) as f:",
            "            json.dump(TEMPORARY_CREDENTIAL, f)",
            "    except Exception as ex:",
            "        logger.debug(",
            "            \"Failed to write a credential file: \" \"file=[%s], err=[%s]\",",
            "            TEMPORARY_CREDENTIAL_FILE,",
            "            ex,",
            "        )",
            "    finally:",
            "        unlock_temporary_credential_file()",
            "",
            "",
            "def write_temporary_credential_file(host: str, cred_name: str, cred) -> None:",
            "    \"\"\"Writes temporary credential file when OS is Linux.\"\"\"",
            "    if not CACHE_DIR:",
            "        # no cache is enabled",
            "        return",
            "    global TEMPORARY_CREDENTIAL",
            "    global TEMPORARY_CREDENTIAL_LOCK",
            "    with TEMPORARY_CREDENTIAL_LOCK:",
            "        # update the cache",
            "        host_data = TEMPORARY_CREDENTIAL.get(host.upper(), {})",
            "        host_data[cred_name.upper()] = cred",
            "        TEMPORARY_CREDENTIAL[host.upper()] = host_data",
            "        flush_temporary_credentials()",
            "",
            "",
            "def read_temporary_credential_file():",
            "    \"\"\"Reads temporary credential file when OS is Linux.\"\"\"",
            "    if not CACHE_DIR:",
            "        # no cache is enabled",
            "        return",
            "",
            "    global TEMPORARY_CREDENTIAL",
            "    global TEMPORARY_CREDENTIAL_LOCK",
            "    global TEMPORARY_CREDENTIAL_FILE",
            "    with TEMPORARY_CREDENTIAL_LOCK:",
            "        for _ in range(10):",
            "            if lock_temporary_credential_file():",
            "                break",
            "            time.sleep(1)",
            "        else:",
            "            logger.debug(",
            "                \"The lock file still persists. Will ignore and \"",
            "                \"write the temporary credential file: %s\",",
            "                TEMPORARY_CREDENTIAL_FILE,",
            "            )",
            "        try:",
            "            with codecs.open(",
            "                TEMPORARY_CREDENTIAL_FILE, \"r\", encoding=\"utf-8\", errors=\"ignore\"",
            "            ) as f:",
            "                TEMPORARY_CREDENTIAL = json.load(f)",
            "            return TEMPORARY_CREDENTIAL",
            "        except Exception as ex:",
            "            logger.debug(",
            "                \"Failed to read a credential file. The file may not\"",
            "                \"exists: file=[%s], err=[%s]\",",
            "                TEMPORARY_CREDENTIAL_FILE,",
            "                ex,",
            "            )",
            "        finally:",
            "            unlock_temporary_credential_file()",
            "",
            "",
            "def lock_temporary_credential_file() -> bool:",
            "    global TEMPORARY_CREDENTIAL_FILE_LOCK",
            "    try:",
            "        mkdir(TEMPORARY_CREDENTIAL_FILE_LOCK)",
            "        return True",
            "    except OSError:",
            "        logger.debug(",
            "            \"Temporary cache file lock already exists. Other \"",
            "            \"process may be updating the temporary \"",
            "        )",
            "        return False",
            "",
            "",
            "def unlock_temporary_credential_file() -> bool:",
            "    global TEMPORARY_CREDENTIAL_FILE_LOCK",
            "    try:",
            "        rmdir(TEMPORARY_CREDENTIAL_FILE_LOCK)",
            "        return True",
            "    except OSError:",
            "        logger.debug(\"Temporary cache file lock no longer exists.\")",
            "        return False",
            "",
            "",
            "def delete_temporary_credential(host, user, cred_type) -> None:",
            "    if (IS_MACOS or IS_WINDOWS) and installed_keyring:",
            "        try:",
            "            keyring.delete_password(",
            "                build_temporary_credential_name(host, user, cred_type), user.upper()",
            "            )",
            "        except Exception as ex:",
            "            logger.error(\"Failed to delete credential in the keyring: err=[%s]\", ex)",
            "    elif IS_LINUX:",
            "        temporary_credential_file_delete_password(host, user, cred_type)",
            "",
            "",
            "def temporary_credential_file_delete_password(host, user, cred_type) -> None:",
            "    \"\"\"Remove credential from temporary credential file when OS is Linux.\"\"\"",
            "    if not CACHE_DIR:",
            "        # no cache is enabled",
            "        return",
            "    global TEMPORARY_CREDENTIAL",
            "    global TEMPORARY_CREDENTIAL_LOCK",
            "    with TEMPORARY_CREDENTIAL_LOCK:",
            "        # update the cache",
            "        host_data = TEMPORARY_CREDENTIAL.get(host.upper(), {})",
            "        host_data.pop(build_temporary_credential_name(host, user, cred_type), None)",
            "        if not host_data:",
            "            TEMPORARY_CREDENTIAL.pop(host.upper(), None)",
            "        else:",
            "            TEMPORARY_CREDENTIAL[host.upper()] = host_data",
            "        flush_temporary_credentials()",
            "",
            "",
            "def delete_temporary_credential_file() -> None:",
            "    \"\"\"Deletes temporary credential file and its lock file.\"\"\"",
            "    global TEMPORARY_CREDENTIAL_FILE",
            "    try:",
            "        remove(TEMPORARY_CREDENTIAL_FILE)",
            "    except Exception as ex:",
            "        logger.debug(",
            "            \"Failed to delete a credential file: \" \"file=[%s], err=[%s]\",",
            "            TEMPORARY_CREDENTIAL_FILE,",
            "            ex,",
            "        )",
            "    try:",
            "        removedirs(TEMPORARY_CREDENTIAL_FILE_LOCK)",
            "    except Exception as ex:",
            "        logger.debug(\"Failed to delete credential lock file: err=[%s]\", ex)",
            "",
            "",
            "def build_temporary_credential_name(host, user, cred_type) -> str:",
            "    return \"{host}:{user}:{driver}:{cred}\".format(",
            "        host=host.upper(), user=user.upper(), driver=KEYRING_DRIVER_NAME, cred=cred_type",
            "    )",
            "",
            "",
            "def get_token_from_private_key(",
            "    user: str, account: str, privatekey_path: str, key_password: str | None",
            ") -> str:",
            "    encoded_password = key_password.encode() if key_password is not None else None",
            "    with open(privatekey_path, \"rb\") as key:",
            "        p_key = load_pem_private_key(",
            "            key.read(), password=encoded_password, backend=default_backend()",
            "        )",
            "",
            "    private_key = p_key.private_bytes(",
            "        encoding=Encoding.DER,",
            "        format=PrivateFormat.PKCS8,",
            "        encryption_algorithm=NoEncryption(),",
            "    )",
            "    from . import AuthByKeyPair",
            "",
            "    auth_instance = AuthByKeyPair(",
            "        private_key,",
            "        DAY_IN_SECONDS,",
            "    )  # token valid for 24 hours",
            "    return auth_instance.prepare(account=account, user=user)",
            "",
            "",
            "def get_public_key_fingerprint(private_key_file: str, password: str) -> str:",
            "    \"\"\"Helper function to generate the public key fingerprint from the private key file\"\"\"",
            "    with open(private_key_file, \"rb\") as key:",
            "        p_key = load_pem_private_key(",
            "            key.read(), password=password.encode(), backend=default_backend()",
            "        )",
            "    private_key = p_key.private_bytes(",
            "        encoding=Encoding.DER,",
            "        format=PrivateFormat.PKCS8,",
            "        encryption_algorithm=NoEncryption(),",
            "    )",
            "    private_key = load_der_private_key(",
            "        data=private_key, password=None, backend=default_backend()",
            "    )",
            "    from . import AuthByKeyPair",
            "",
            "    return AuthByKeyPair.calculate_public_key_fingerprint(private_key)"
        ],
        "afterPatchFile": [
            "#",
            "# Copyright (c) 2012-2023 Snowflake Computing Inc. All rights reserved.",
            "#",
            "",
            "from __future__ import annotations",
            "",
            "import codecs",
            "import copy",
            "import json",
            "import logging",
            "import tempfile",
            "import time",
            "import uuid",
            "from datetime import datetime, timezone",
            "from os import getenv, makedirs, mkdir, path, remove, removedirs, rmdir",
            "from os.path import expanduser",
            "from threading import Lock, Thread",
            "from typing import TYPE_CHECKING, Any, Callable",
            "",
            "from cryptography.hazmat.backends import default_backend",
            "from cryptography.hazmat.primitives.serialization import (",
            "    Encoding,",
            "    NoEncryption,",
            "    PrivateFormat,",
            "    load_der_private_key,",
            "    load_pem_private_key,",
            ")",
            "",
            "from ..compat import IS_LINUX, IS_MACOS, IS_WINDOWS, urlencode",
            "from ..constants import (",
            "    DAY_IN_SECONDS,",
            "    HTTP_HEADER_ACCEPT,",
            "    HTTP_HEADER_CONTENT_TYPE,",
            "    HTTP_HEADER_SERVICE_NAME,",
            "    HTTP_HEADER_USER_AGENT,",
            "    PARAMETER_CLIENT_REQUEST_MFA_TOKEN,",
            "    PARAMETER_CLIENT_STORE_TEMPORARY_CREDENTIAL,",
            ")",
            "from ..description import (",
            "    COMPILER,",
            "    IMPLEMENTATION,",
            "    OPERATING_SYSTEM,",
            "    PLATFORM,",
            "    PYTHON_VERSION,",
            ")",
            "from ..errorcode import ER_FAILED_TO_CONNECT_TO_DB",
            "from ..errors import (",
            "    BadGatewayError,",
            "    DatabaseError,",
            "    Error,",
            "    ForbiddenError,",
            "    ProgrammingError,",
            "    ServiceUnavailableError,",
            ")",
            "from ..file_util import owner_rw_opener",
            "from ..network import (",
            "    ACCEPT_TYPE_APPLICATION_SNOWFLAKE,",
            "    CONTENT_TYPE_APPLICATION_JSON,",
            "    ID_TOKEN_INVALID_LOGIN_REQUEST_GS_CODE,",
            "    PYTHON_CONNECTOR_USER_AGENT,",
            "    ReauthenticationRequest,",
            ")",
            "from ..options import installed_keyring, keyring",
            "from ..sqlstate import SQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED",
            "from ..version import VERSION",
            "",
            "if TYPE_CHECKING:",
            "    from . import AuthByPlugin",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "# Cache directory",
            "CACHE_ROOT_DIR = (",
            "    getenv(\"SF_TEMPORARY_CREDENTIAL_CACHE_DIR\")",
            "    or expanduser(\"~\")",
            "    or tempfile.gettempdir()",
            ")",
            "if IS_WINDOWS:",
            "    CACHE_DIR = path.join(CACHE_ROOT_DIR, \"AppData\", \"Local\", \"Snowflake\", \"Caches\")",
            "elif IS_MACOS:",
            "    CACHE_DIR = path.join(CACHE_ROOT_DIR, \"Library\", \"Caches\", \"Snowflake\")",
            "else:",
            "    CACHE_DIR = path.join(CACHE_ROOT_DIR, \".cache\", \"snowflake\")",
            "",
            "if not path.exists(CACHE_DIR):",
            "    try:",
            "        makedirs(CACHE_DIR, mode=0o700)",
            "    except Exception as ex:",
            "        logger.debug(\"cannot create a cache directory: [%s], err=[%s]\", CACHE_DIR, ex)",
            "        CACHE_DIR = None",
            "logger.debug(\"cache directory: %s\", CACHE_DIR)",
            "",
            "# temporary credential cache",
            "TEMPORARY_CREDENTIAL: dict[str, dict[str, str | None]] = {}",
            "",
            "TEMPORARY_CREDENTIAL_LOCK = Lock()",
            "",
            "# temporary credential cache file name",
            "TEMPORARY_CREDENTIAL_FILE = \"temporary_credential.json\"",
            "TEMPORARY_CREDENTIAL_FILE = (",
            "    path.join(CACHE_DIR, TEMPORARY_CREDENTIAL_FILE) if CACHE_DIR else \"\"",
            ")",
            "",
            "# temporary credential cache lock directory name",
            "TEMPORARY_CREDENTIAL_FILE_LOCK = TEMPORARY_CREDENTIAL_FILE + \".lck\"",
            "",
            "# keyring",
            "KEYRING_SERVICE_NAME = \"net.snowflake.temporary_token\"",
            "KEYRING_USER = \"temp_token\"",
            "KEYRING_DRIVER_NAME = \"SNOWFLAKE-PYTHON-DRIVER\"",
            "",
            "ID_TOKEN = \"ID_TOKEN\"",
            "MFA_TOKEN = \"MFATOKEN\"",
            "",
            "AUTHENTICATION_REQUEST_KEY_WHITELIST = {",
            "    \"ACCOUNT_NAME\",",
            "    \"AUTHENTICATOR\",",
            "    \"CLIENT_APP_ID\",",
            "    \"CLIENT_APP_VERSION\",",
            "    \"CLIENT_ENVIRONMENT\",",
            "    \"EXT_AUTHN_DUO_METHOD\",",
            "    \"LOGIN_NAME\",",
            "    \"SESSION_PARAMETERS\",",
            "    \"SVN_REVISION\",",
            "}",
            "",
            "",
            "class Auth:",
            "    \"\"\"Snowflake Authenticator.\"\"\"",
            "",
            "    def __init__(self, rest) -> None:",
            "        self._rest = rest",
            "",
            "    @staticmethod",
            "    def base_auth_data(",
            "        user,",
            "        account,",
            "        application,",
            "        internal_application_name,",
            "        internal_application_version,",
            "        ocsp_mode,",
            "        login_timeout: int | None = None,",
            "        network_timeout: int | None = None,",
            "        socket_timeout: int | None = None,",
            "    ):",
            "        return {",
            "            \"data\": {",
            "                \"CLIENT_APP_ID\": internal_application_name,",
            "                \"CLIENT_APP_VERSION\": internal_application_version,",
            "                \"SVN_REVISION\": VERSION[3],",
            "                \"ACCOUNT_NAME\": account,",
            "                \"LOGIN_NAME\": user,",
            "                \"CLIENT_ENVIRONMENT\": {",
            "                    \"APPLICATION\": application,",
            "                    \"OS\": OPERATING_SYSTEM,",
            "                    \"OS_VERSION\": PLATFORM,",
            "                    \"PYTHON_VERSION\": PYTHON_VERSION,",
            "                    \"PYTHON_RUNTIME\": IMPLEMENTATION,",
            "                    \"PYTHON_COMPILER\": COMPILER,",
            "                    \"OCSP_MODE\": ocsp_mode.name,",
            "                    \"TRACING\": logger.getEffectiveLevel(),",
            "                    \"LOGIN_TIMEOUT\": login_timeout,",
            "                    \"NETWORK_TIMEOUT\": network_timeout,",
            "                    \"SOCKET_TIMEOUT\": socket_timeout,",
            "                },",
            "            },",
            "        }",
            "",
            "    def authenticate(",
            "        self,",
            "        auth_instance: AuthByPlugin,",
            "        account: str,",
            "        user: str,",
            "        database: str | None = None,",
            "        schema: str | None = None,",
            "        warehouse: str | None = None,",
            "        role: str | None = None,",
            "        passcode: str | None = None,",
            "        passcode_in_password: bool = False,",
            "        mfa_callback: Callable[[], None] | None = None,",
            "        password_callback: Callable[[], str] | None = None,",
            "        session_parameters: dict[Any, Any] | None = None,",
            "        # max time waiting for MFA response, currently unused",
            "        timeout: int | None = None,",
            "    ) -> dict[str, str | int | bool]:",
            "        logger.debug(\"authenticate\")",
            "",
            "        if timeout is None:",
            "            timeout = auth_instance.timeout",
            "",
            "        if session_parameters is None:",
            "            session_parameters = {}",
            "",
            "        request_id = str(uuid.uuid4())",
            "        headers = {",
            "            HTTP_HEADER_CONTENT_TYPE: CONTENT_TYPE_APPLICATION_JSON,",
            "            HTTP_HEADER_ACCEPT: ACCEPT_TYPE_APPLICATION_SNOWFLAKE,",
            "            HTTP_HEADER_USER_AGENT: PYTHON_CONNECTOR_USER_AGENT,",
            "        }",
            "        if HTTP_HEADER_SERVICE_NAME in session_parameters:",
            "            headers[HTTP_HEADER_SERVICE_NAME] = session_parameters[",
            "                HTTP_HEADER_SERVICE_NAME",
            "            ]",
            "        url = \"/session/v1/login-request\"",
            "",
            "        body_template = Auth.base_auth_data(",
            "            user,",
            "            account,",
            "            self._rest._connection.application,",
            "            self._rest._connection._internal_application_name,",
            "            self._rest._connection._internal_application_version,",
            "            self._rest._connection._ocsp_mode(),",
            "            self._rest._connection.login_timeout,",
            "            self._rest._connection._network_timeout,",
            "            self._rest._connection._socket_timeout,",
            "        )",
            "",
            "        body = copy.deepcopy(body_template)",
            "        # updating request body",
            "        auth_instance.update_body(body)",
            "",
            "        logger.debug(",
            "            \"account=%s, user=%s, database=%s, schema=%s, \"",
            "            \"warehouse=%s, role=%s, request_id=%s\",",
            "            account,",
            "            user,",
            "            database,",
            "            schema,",
            "            warehouse,",
            "            role,",
            "            request_id,",
            "        )",
            "        url_parameters = {\"request_id\": request_id}",
            "        if database is not None:",
            "            url_parameters[\"databaseName\"] = database",
            "        if schema is not None:",
            "            url_parameters[\"schemaName\"] = schema",
            "        if warehouse is not None:",
            "            url_parameters[\"warehouse\"] = warehouse",
            "        if role is not None:",
            "            url_parameters[\"roleName\"] = role",
            "",
            "        url = url + \"?\" + urlencode(url_parameters)",
            "",
            "        # first auth request",
            "        if passcode_in_password:",
            "            body[\"data\"][\"EXT_AUTHN_DUO_METHOD\"] = \"passcode\"",
            "        elif passcode:",
            "            body[\"data\"][\"EXT_AUTHN_DUO_METHOD\"] = \"passcode\"",
            "            body[\"data\"][\"PASSCODE\"] = passcode",
            "",
            "        if session_parameters:",
            "            body[\"data\"][\"SESSION_PARAMETERS\"] = session_parameters",
            "",
            "        logger.debug(",
            "            \"body['data']: %s\",",
            "            {",
            "                k: v if k in AUTHENTICATION_REQUEST_KEY_WHITELIST else \"******\"",
            "                for (k, v) in body[\"data\"].items()",
            "            },",
            "        )",
            "",
            "        try:",
            "            ret = self._rest._post_request(",
            "                url,",
            "                headers,",
            "                json.dumps(body),",
            "                socket_timeout=auth_instance._socket_timeout,",
            "            )",
            "        except ForbiddenError as err:",
            "            # HTTP 403",
            "            raise err.__class__(",
            "                msg=(",
            "                    \"Failed to connect to DB. \"",
            "                    \"Verify the account name is correct: {host}:{port}. \"",
            "                    \"{message}\"",
            "                ).format(",
            "                    host=self._rest._host, port=self._rest._port, message=str(err)",
            "                ),",
            "                errno=ER_FAILED_TO_CONNECT_TO_DB,",
            "                sqlstate=SQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED,",
            "            )",
            "        except (ServiceUnavailableError, BadGatewayError) as err:",
            "            # HTTP 502/504",
            "            raise err.__class__(",
            "                msg=(",
            "                    \"Failed to connect to DB. \"",
            "                    \"Service is unavailable: {host}:{port}. \"",
            "                    \"{message}\"",
            "                ).format(",
            "                    host=self._rest._host, port=self._rest._port, message=str(err)",
            "                ),",
            "                errno=ER_FAILED_TO_CONNECT_TO_DB,",
            "                sqlstate=SQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED,",
            "            )",
            "",
            "        # waiting for MFA authentication",
            "        if ret[\"data\"] and ret[\"data\"].get(\"nextAction\") in (",
            "            \"EXT_AUTHN_DUO_ALL\",",
            "            \"EXT_AUTHN_DUO_PUSH_N_PASSCODE\",",
            "        ):",
            "            body[\"inFlightCtx\"] = ret[\"data\"].get(\"inFlightCtx\")",
            "            body[\"data\"][\"EXT_AUTHN_DUO_METHOD\"] = \"push\"",
            "            self.ret = {\"message\": \"Timeout\", \"data\": {}}",
            "",
            "            def post_request_wrapper(self, url, headers, body) -> None:",
            "                # get the MFA response",
            "                self.ret = self._rest._post_request(",
            "                    url,",
            "                    headers,",
            "                    body,",
            "                    socket_timeout=auth_instance._socket_timeout,",
            "                )",
            "",
            "            # send new request to wait until MFA is approved",
            "            t = Thread(",
            "                target=post_request_wrapper, args=[self, url, headers, json.dumps(body)]",
            "            )",
            "            t.daemon = True",
            "            t.start()",
            "            if callable(mfa_callback):",
            "                c = mfa_callback()",
            "                while not self.ret or self.ret.get(\"message\") == \"Timeout\":",
            "                    next(c)",
            "            else:",
            "                # _post_request should already terminate on timeout, so this is just a safeguard",
            "                t.join(timeout=timeout)",
            "",
            "            ret = self.ret",
            "            if (",
            "                ret",
            "                and ret[\"data\"]",
            "                and ret[\"data\"].get(\"nextAction\") == \"EXT_AUTHN_SUCCESS\"",
            "            ):",
            "                body = copy.deepcopy(body_template)",
            "                body[\"inFlightCtx\"] = ret[\"data\"].get(\"inFlightCtx\")",
            "                # final request to get tokens",
            "                ret = self._rest._post_request(",
            "                    url,",
            "                    headers,",
            "                    json.dumps(body),",
            "                    socket_timeout=auth_instance._socket_timeout,",
            "                )",
            "            elif not ret or not ret[\"data\"] or not ret[\"data\"].get(\"token\"):",
            "                # not token is returned.",
            "                Error.errorhandler_wrapper(",
            "                    self._rest._connection,",
            "                    None,",
            "                    DatabaseError,",
            "                    {",
            "                        \"msg\": (",
            "                            \"Failed to connect to DB. MFA \"",
            "                            \"authentication failed: {\"",
            "                            \"host}:{port}. {message}\"",
            "                        ).format(",
            "                            host=self._rest._host,",
            "                            port=self._rest._port,",
            "                            message=ret[\"message\"],",
            "                        ),",
            "                        \"errno\": ER_FAILED_TO_CONNECT_TO_DB,",
            "                        \"sqlstate\": SQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED,",
            "                    },",
            "                )",
            "                return session_parameters  # required for unit test",
            "",
            "        elif ret[\"data\"] and ret[\"data\"].get(\"nextAction\") == \"PWD_CHANGE\":",
            "            if callable(password_callback):",
            "                body = copy.deepcopy(body_template)",
            "                body[\"inFlightCtx\"] = ret[\"data\"].get(\"inFlightCtx\")",
            "                body[\"data\"][\"LOGIN_NAME\"] = user",
            "                body[\"data\"][\"PASSWORD\"] = (",
            "                    auth_instance.password",
            "                    if hasattr(auth_instance, \"password\")",
            "                    else None",
            "                )",
            "                body[\"data\"][\"CHOSEN_NEW_PASSWORD\"] = password_callback()",
            "                # New Password input",
            "                ret = self._rest._post_request(",
            "                    url,",
            "                    headers,",
            "                    json.dumps(body),",
            "                    socket_timeout=auth_instance._socket_timeout,",
            "                )",
            "",
            "        logger.debug(\"completed authentication\")",
            "        if not ret[\"success\"]:",
            "            errno = ret.get(\"code\", ER_FAILED_TO_CONNECT_TO_DB)",
            "            if errno == ID_TOKEN_INVALID_LOGIN_REQUEST_GS_CODE:",
            "                # clear stored id_token if failed to connect because of id_token",
            "                # raise an exception for reauth without id_token",
            "                self._rest.id_token = None",
            "                delete_temporary_credential(self._rest._host, user, ID_TOKEN)",
            "                raise ReauthenticationRequest(",
            "                    ProgrammingError(",
            "                        msg=ret[\"message\"],",
            "                        errno=int(errno),",
            "                        sqlstate=SQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED,",
            "                    )",
            "                )",
            "",
            "            from . import AuthByKeyPair",
            "",
            "            if isinstance(auth_instance, AuthByKeyPair):",
            "                logger.debug(",
            "                    \"JWT Token authentication failed. \"",
            "                    \"Token expires at: %s. \"",
            "                    \"Current Time: %s\",",
            "                    str(auth_instance._jwt_token_exp),",
            "                    str(datetime.now(timezone.utc).replace(tzinfo=None)),",
            "                )",
            "            from . import AuthByUsrPwdMfa",
            "",
            "            if isinstance(auth_instance, AuthByUsrPwdMfa):",
            "                delete_temporary_credential(self._rest._host, user, MFA_TOKEN)",
            "            Error.errorhandler_wrapper(",
            "                self._rest._connection,",
            "                None,",
            "                DatabaseError,",
            "                {",
            "                    \"msg\": (",
            "                        \"Failed to connect to DB: {host}:{port}. \" \"{message}\"",
            "                    ).format(",
            "                        host=self._rest._host,",
            "                        port=self._rest._port,",
            "                        message=ret[\"message\"],",
            "                    ),",
            "                    \"errno\": ER_FAILED_TO_CONNECT_TO_DB,",
            "                    \"sqlstate\": SQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED,",
            "                },",
            "            )",
            "        else:",
            "            logger.debug(",
            "                \"token = %s\",",
            "                (",
            "                    \"******\"",
            "                    if ret[\"data\"] and ret[\"data\"].get(\"token\") is not None",
            "                    else \"NULL\"",
            "                ),",
            "            )",
            "            logger.debug(",
            "                \"master_token = %s\",",
            "                (",
            "                    \"******\"",
            "                    if ret[\"data\"] and ret[\"data\"].get(\"masterToken\") is not None",
            "                    else \"NULL\"",
            "                ),",
            "            )",
            "            logger.debug(",
            "                \"id_token = %s\",",
            "                (",
            "                    \"******\"",
            "                    if ret[\"data\"] and ret[\"data\"].get(\"idToken\") is not None",
            "                    else \"NULL\"",
            "                ),",
            "            )",
            "            logger.debug(",
            "                \"mfa_token = %s\",",
            "                (",
            "                    \"******\"",
            "                    if ret[\"data\"] and ret[\"data\"].get(\"mfaToken\") is not None",
            "                    else \"NULL\"",
            "                ),",
            "            )",
            "            if not ret[\"data\"]:",
            "                Error.errorhandler_wrapper(",
            "                    None,",
            "                    None,",
            "                    Error,",
            "                    {",
            "                        \"msg\": \"There is no data in the returning response, please retry the operation.\"",
            "                    },",
            "                )",
            "            self._rest.update_tokens(",
            "                ret[\"data\"].get(\"token\"),",
            "                ret[\"data\"].get(\"masterToken\"),",
            "                master_validity_in_seconds=ret[\"data\"].get(\"masterValidityInSeconds\"),",
            "                id_token=ret[\"data\"].get(\"idToken\"),",
            "                mfa_token=ret[\"data\"].get(\"mfaToken\"),",
            "            )",
            "            self.write_temporary_credentials(",
            "                self._rest._host, user, session_parameters, ret",
            "            )",
            "            if ret[\"data\"] and \"sessionId\" in ret[\"data\"]:",
            "                self._rest._connection._session_id = ret[\"data\"].get(\"sessionId\")",
            "            if ret[\"data\"] and \"sessionInfo\" in ret[\"data\"]:",
            "                session_info = ret[\"data\"].get(\"sessionInfo\")",
            "                self._rest._connection._database = session_info.get(\"databaseName\")",
            "                self._rest._connection._schema = session_info.get(\"schemaName\")",
            "                self._rest._connection._warehouse = session_info.get(\"warehouseName\")",
            "                self._rest._connection._role = session_info.get(\"roleName\")",
            "            if ret[\"data\"] and \"parameters\" in ret[\"data\"]:",
            "                session_parameters.update(",
            "                    {p[\"name\"]: p[\"value\"] for p in ret[\"data\"].get(\"parameters\")}",
            "                )",
            "            self._rest._connection._update_parameters(session_parameters)",
            "            return session_parameters",
            "",
            "    def _read_temporary_credential(",
            "        self,",
            "        host: str,",
            "        user: str,",
            "        cred_type: str,",
            "    ) -> str | None:",
            "        cred = None",
            "        if IS_MACOS or IS_WINDOWS:",
            "            if not installed_keyring:",
            "                logger.debug(",
            "                    \"Dependency 'keyring' is not installed, cannot cache id token. You might experience \"",
            "                    \"multiple authentication pop ups while using ExternalBrowser Authenticator. To avoid \"",
            "                    \"this please install keyring module using the following command : pip install \"",
            "                    \"snowflake-connector-python[secure-local-storage]\"",
            "                )",
            "                return None",
            "            try:",
            "                cred = keyring.get_password(",
            "                    build_temporary_credential_name(host, user, cred_type), user.upper()",
            "                )",
            "            except keyring.errors.KeyringError as ke:",
            "                logger.error(",
            "                    \"Could not retrieve {} from secure storage : {}\".format(",
            "                        cred_type, str(ke)",
            "                    )",
            "                )",
            "        elif IS_LINUX:",
            "            read_temporary_credential_file()",
            "            cred = TEMPORARY_CREDENTIAL.get(host.upper(), {}).get(",
            "                build_temporary_credential_name(host, user, cred_type)",
            "            )",
            "        else:",
            "            logger.debug(\"OS not supported for Local Secure Storage\")",
            "        return cred",
            "",
            "    def read_temporary_credentials(",
            "        self,",
            "        host: str,",
            "        user: str,",
            "        session_parameters: dict[str, Any],",
            "    ) -> None:",
            "        if session_parameters.get(PARAMETER_CLIENT_STORE_TEMPORARY_CREDENTIAL, False):",
            "            self._rest.id_token = self._read_temporary_credential(",
            "                host,",
            "                user,",
            "                ID_TOKEN,",
            "            )",
            "",
            "        if session_parameters.get(PARAMETER_CLIENT_REQUEST_MFA_TOKEN, False):",
            "            self._rest.mfa_token = self._read_temporary_credential(",
            "                host,",
            "                user,",
            "                MFA_TOKEN,",
            "            )",
            "",
            "    def _write_temporary_credential(",
            "        self,",
            "        host: str,",
            "        user: str,",
            "        cred_type: str,",
            "        cred: str | None,",
            "    ) -> None:",
            "        if not cred:",
            "            logger.debug(",
            "                \"no credential is given when try to store temporary credential\"",
            "            )",
            "            return",
            "        if IS_MACOS or IS_WINDOWS:",
            "            if not installed_keyring:",
            "                logger.debug(",
            "                    \"Dependency 'keyring' is not installed, cannot cache id token. You might experience \"",
            "                    \"multiple authentication pop ups while using ExternalBrowser Authenticator. To avoid \"",
            "                    \"this please install keyring module using the following command : pip install \"",
            "                    \"snowflake-connector-python[secure-local-storage]\"",
            "                )",
            "                return",
            "            try:",
            "                keyring.set_password(",
            "                    build_temporary_credential_name(host, user, cred_type),",
            "                    user.upper(),",
            "                    cred,",
            "                )",
            "            except keyring.errors.KeyringError as ke:",
            "                logger.error(\"Could not store id_token to keyring, %s\", str(ke))",
            "        elif IS_LINUX:",
            "            write_temporary_credential_file(",
            "                host, build_temporary_credential_name(host, user, cred_type), cred",
            "            )",
            "        else:",
            "            logger.debug(\"OS not supported for Local Secure Storage\")",
            "",
            "    def write_temporary_credentials(",
            "        self,",
            "        host: str,",
            "        user: str,",
            "        session_parameters: dict[str, Any],",
            "        response: dict[str, Any],",
            "    ) -> None:",
            "        if (",
            "            self._rest._connection.auth_class.consent_cache_id_token",
            "            and session_parameters.get(",
            "                PARAMETER_CLIENT_STORE_TEMPORARY_CREDENTIAL, False",
            "            )",
            "        ):",
            "            self._write_temporary_credential(",
            "                host, user, ID_TOKEN, response[\"data\"].get(\"idToken\")",
            "            )",
            "",
            "        if session_parameters.get(PARAMETER_CLIENT_REQUEST_MFA_TOKEN, False):",
            "            self._write_temporary_credential(",
            "                host, user, MFA_TOKEN, response[\"data\"].get(\"mfaToken\")",
            "            )",
            "",
            "",
            "def flush_temporary_credentials() -> None:",
            "    \"\"\"Flush temporary credentials in memory into disk. Need to hold TEMPORARY_CREDENTIAL_LOCK.\"\"\"",
            "    global TEMPORARY_CREDENTIAL",
            "    global TEMPORARY_CREDENTIAL_FILE",
            "    for _ in range(10):",
            "        if lock_temporary_credential_file():",
            "            break",
            "        time.sleep(1)",
            "    else:",
            "        logger.debug(",
            "            \"The lock file still persists after the maximum wait time.\"",
            "            \"Will ignore it and write temporary credential file: %s\",",
            "            TEMPORARY_CREDENTIAL_FILE,",
            "        )",
            "    try:",
            "        with open(",
            "            TEMPORARY_CREDENTIAL_FILE,",
            "            \"w\",",
            "            encoding=\"utf-8\",",
            "            errors=\"ignore\",",
            "            opener=owner_rw_opener,",
            "        ) as f:",
            "            json.dump(TEMPORARY_CREDENTIAL, f)",
            "    except Exception as ex:",
            "        logger.debug(",
            "            \"Failed to write a credential file: \" \"file=[%s], err=[%s]\",",
            "            TEMPORARY_CREDENTIAL_FILE,",
            "            ex,",
            "        )",
            "    finally:",
            "        unlock_temporary_credential_file()",
            "",
            "",
            "def write_temporary_credential_file(host: str, cred_name: str, cred) -> None:",
            "    \"\"\"Writes temporary credential file when OS is Linux.\"\"\"",
            "    if not CACHE_DIR:",
            "        # no cache is enabled",
            "        return",
            "    global TEMPORARY_CREDENTIAL",
            "    global TEMPORARY_CREDENTIAL_LOCK",
            "    with TEMPORARY_CREDENTIAL_LOCK:",
            "        # update the cache",
            "        host_data = TEMPORARY_CREDENTIAL.get(host.upper(), {})",
            "        host_data[cred_name.upper()] = cred",
            "        TEMPORARY_CREDENTIAL[host.upper()] = host_data",
            "        flush_temporary_credentials()",
            "",
            "",
            "def read_temporary_credential_file():",
            "    \"\"\"Reads temporary credential file when OS is Linux.\"\"\"",
            "    if not CACHE_DIR:",
            "        # no cache is enabled",
            "        return",
            "",
            "    global TEMPORARY_CREDENTIAL",
            "    global TEMPORARY_CREDENTIAL_LOCK",
            "    global TEMPORARY_CREDENTIAL_FILE",
            "    with TEMPORARY_CREDENTIAL_LOCK:",
            "        for _ in range(10):",
            "            if lock_temporary_credential_file():",
            "                break",
            "            time.sleep(1)",
            "        else:",
            "            logger.debug(",
            "                \"The lock file still persists. Will ignore and \"",
            "                \"write the temporary credential file: %s\",",
            "                TEMPORARY_CREDENTIAL_FILE,",
            "            )",
            "        try:",
            "            with codecs.open(",
            "                TEMPORARY_CREDENTIAL_FILE, \"r\", encoding=\"utf-8\", errors=\"ignore\"",
            "            ) as f:",
            "                TEMPORARY_CREDENTIAL = json.load(f)",
            "            return TEMPORARY_CREDENTIAL",
            "        except Exception as ex:",
            "            logger.debug(",
            "                \"Failed to read a credential file. The file may not\"",
            "                \"exists: file=[%s], err=[%s]\",",
            "                TEMPORARY_CREDENTIAL_FILE,",
            "                ex,",
            "            )",
            "        finally:",
            "            unlock_temporary_credential_file()",
            "",
            "",
            "def lock_temporary_credential_file() -> bool:",
            "    global TEMPORARY_CREDENTIAL_FILE_LOCK",
            "    try:",
            "        mkdir(TEMPORARY_CREDENTIAL_FILE_LOCK)",
            "        return True",
            "    except OSError:",
            "        logger.debug(",
            "            \"Temporary cache file lock already exists. Other \"",
            "            \"process may be updating the temporary \"",
            "        )",
            "        return False",
            "",
            "",
            "def unlock_temporary_credential_file() -> bool:",
            "    global TEMPORARY_CREDENTIAL_FILE_LOCK",
            "    try:",
            "        rmdir(TEMPORARY_CREDENTIAL_FILE_LOCK)",
            "        return True",
            "    except OSError:",
            "        logger.debug(\"Temporary cache file lock no longer exists.\")",
            "        return False",
            "",
            "",
            "def delete_temporary_credential(host, user, cred_type) -> None:",
            "    if (IS_MACOS or IS_WINDOWS) and installed_keyring:",
            "        try:",
            "            keyring.delete_password(",
            "                build_temporary_credential_name(host, user, cred_type), user.upper()",
            "            )",
            "        except Exception as ex:",
            "            logger.error(\"Failed to delete credential in the keyring: err=[%s]\", ex)",
            "    elif IS_LINUX:",
            "        temporary_credential_file_delete_password(host, user, cred_type)",
            "",
            "",
            "def temporary_credential_file_delete_password(host, user, cred_type) -> None:",
            "    \"\"\"Remove credential from temporary credential file when OS is Linux.\"\"\"",
            "    if not CACHE_DIR:",
            "        # no cache is enabled",
            "        return",
            "    global TEMPORARY_CREDENTIAL",
            "    global TEMPORARY_CREDENTIAL_LOCK",
            "    with TEMPORARY_CREDENTIAL_LOCK:",
            "        # update the cache",
            "        host_data = TEMPORARY_CREDENTIAL.get(host.upper(), {})",
            "        host_data.pop(build_temporary_credential_name(host, user, cred_type), None)",
            "        if not host_data:",
            "            TEMPORARY_CREDENTIAL.pop(host.upper(), None)",
            "        else:",
            "            TEMPORARY_CREDENTIAL[host.upper()] = host_data",
            "        flush_temporary_credentials()",
            "",
            "",
            "def delete_temporary_credential_file() -> None:",
            "    \"\"\"Deletes temporary credential file and its lock file.\"\"\"",
            "    global TEMPORARY_CREDENTIAL_FILE",
            "    try:",
            "        remove(TEMPORARY_CREDENTIAL_FILE)",
            "    except Exception as ex:",
            "        logger.debug(",
            "            \"Failed to delete a credential file: \" \"file=[%s], err=[%s]\",",
            "            TEMPORARY_CREDENTIAL_FILE,",
            "            ex,",
            "        )",
            "    try:",
            "        removedirs(TEMPORARY_CREDENTIAL_FILE_LOCK)",
            "    except Exception as ex:",
            "        logger.debug(\"Failed to delete credential lock file: err=[%s]\", ex)",
            "",
            "",
            "def build_temporary_credential_name(host, user, cred_type) -> str:",
            "    return \"{host}:{user}:{driver}:{cred}\".format(",
            "        host=host.upper(), user=user.upper(), driver=KEYRING_DRIVER_NAME, cred=cred_type",
            "    )",
            "",
            "",
            "def get_token_from_private_key(",
            "    user: str, account: str, privatekey_path: str, key_password: str | None",
            ") -> str:",
            "    encoded_password = key_password.encode() if key_password is not None else None",
            "    with open(privatekey_path, \"rb\") as key:",
            "        p_key = load_pem_private_key(",
            "            key.read(), password=encoded_password, backend=default_backend()",
            "        )",
            "",
            "    private_key = p_key.private_bytes(",
            "        encoding=Encoding.DER,",
            "        format=PrivateFormat.PKCS8,",
            "        encryption_algorithm=NoEncryption(),",
            "    )",
            "    from . import AuthByKeyPair",
            "",
            "    auth_instance = AuthByKeyPair(",
            "        private_key,",
            "        DAY_IN_SECONDS,",
            "    )  # token valid for 24 hours",
            "    return auth_instance.prepare(account=account, user=user)",
            "",
            "",
            "def get_public_key_fingerprint(private_key_file: str, password: str) -> str:",
            "    \"\"\"Helper function to generate the public key fingerprint from the private key file\"\"\"",
            "    with open(private_key_file, \"rb\") as key:",
            "        p_key = load_pem_private_key(",
            "            key.read(), password=password.encode(), backend=default_backend()",
            "        )",
            "    private_key = p_key.private_bytes(",
            "        encoding=Encoding.DER,",
            "        format=PrivateFormat.PKCS8,",
            "        encryption_algorithm=NoEncryption(),",
            "    )",
            "    private_key = load_der_private_key(",
            "        data=private_key, password=None, backend=default_backend()",
            "    )",
            "    from . import AuthByKeyPair",
            "",
            "    return AuthByKeyPair.calculate_public_key_fingerprint(private_key)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "628": [
                "flush_temporary_credentials"
            ]
        },
        "addLocation": []
    },
    "src/snowflake/connector/cache.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 388,
                "afterPatchRowNumber": 388,
                "PatchRowcode": "         file_path: str | dict[str, str],"
            },
            "1": {
                "beforePatchRowNumber": 389,
                "afterPatchRowNumber": 389,
                "PatchRowcode": "         entry_lifetime: int = constants.DAY_IN_SECONDS,"
            },
            "2": {
                "beforePatchRowNumber": 390,
                "afterPatchRowNumber": 390,
                "PatchRowcode": "         file_timeout: int = 0,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 391,
                "PatchRowcode": "+        load_if_file_exists: bool = True,"
            },
            "4": {
                "beforePatchRowNumber": 391,
                "afterPatchRowNumber": 392,
                "PatchRowcode": "     ) -> None:"
            },
            "5": {
                "beforePatchRowNumber": 392,
                "afterPatchRowNumber": 393,
                "PatchRowcode": "         \"\"\"Inits an SFDictFileCache with path, lifetime."
            },
            "6": {
                "beforePatchRowNumber": 393,
                "afterPatchRowNumber": 394,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 445,
                "afterPatchRowNumber": 446,
                "PatchRowcode": "         self._file_lock_path = f\"{self.file_path}.lock\""
            },
            "8": {
                "beforePatchRowNumber": 446,
                "afterPatchRowNumber": 447,
                "PatchRowcode": "         self._file_lock = FileLock(self._file_lock_path, timeout=self.file_timeout)"
            },
            "9": {
                "beforePatchRowNumber": 447,
                "afterPatchRowNumber": 448,
                "PatchRowcode": "         self.last_loaded: datetime.datetime | None = None"
            },
            "10": {
                "beforePatchRowNumber": 448,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if os.path.exists(self.file_path):"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 449,
                "PatchRowcode": "+        if os.path.exists(self.file_path) and load_if_file_exists:"
            },
            "12": {
                "beforePatchRowNumber": 449,
                "afterPatchRowNumber": 450,
                "PatchRowcode": "             with self._lock:"
            },
            "13": {
                "beforePatchRowNumber": 450,
                "afterPatchRowNumber": 451,
                "PatchRowcode": "                 self._load()"
            },
            "14": {
                "beforePatchRowNumber": 451,
                "afterPatchRowNumber": 452,
                "PatchRowcode": "         # indicate whether the cache is modified or not, this variable is for"
            },
            "15": {
                "beforePatchRowNumber": 498,
                "afterPatchRowNumber": 499,
                "PatchRowcode": "         \"\"\"Load cache from disk if possible, returns whether it was able to load.\"\"\""
            },
            "16": {
                "beforePatchRowNumber": 499,
                "afterPatchRowNumber": 500,
                "PatchRowcode": "         try:"
            },
            "17": {
                "beforePatchRowNumber": 500,
                "afterPatchRowNumber": 501,
                "PatchRowcode": "             with open(self.file_path, \"rb\") as r_file:"
            },
            "18": {
                "beforePatchRowNumber": 501,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                other: SFDictFileCache = pickle.load(r_file)"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 502,
                "PatchRowcode": "+                other: SFDictFileCache = self._deserialize(r_file)"
            },
            "20": {
                "beforePatchRowNumber": 502,
                "afterPatchRowNumber": 503,
                "PatchRowcode": "             # Since we want to know whether we are dirty after loading"
            },
            "21": {
                "beforePatchRowNumber": 503,
                "afterPatchRowNumber": 504,
                "PatchRowcode": "             #  we have to know whether the file could learn anything from self"
            },
            "22": {
                "beforePatchRowNumber": 504,
                "afterPatchRowNumber": 505,
                "PatchRowcode": "             #  so instead of calling self.update we call other.update and swap"
            },
            "23": {
                "beforePatchRowNumber": 529,
                "afterPatchRowNumber": 530,
                "PatchRowcode": "         with self._lock:"
            },
            "24": {
                "beforePatchRowNumber": 530,
                "afterPatchRowNumber": 531,
                "PatchRowcode": "             return self._load()"
            },
            "25": {
                "beforePatchRowNumber": 531,
                "afterPatchRowNumber": 532,
                "PatchRowcode": " "
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 533,
                "PatchRowcode": "+    def _serialize(self):"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 534,
                "PatchRowcode": "+        return pickle.dumps(self)"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 535,
                "PatchRowcode": "+"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 536,
                "PatchRowcode": "+    @classmethod"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 537,
                "PatchRowcode": "+    def _deserialize(cls, r_file):"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 538,
                "PatchRowcode": "+        return pickle.load(r_file)"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 539,
                "PatchRowcode": "+"
            },
            "33": {
                "beforePatchRowNumber": 532,
                "afterPatchRowNumber": 540,
                "PatchRowcode": "     def _save(self, load_first: bool = True, force_flush: bool = False) -> bool:"
            },
            "34": {
                "beforePatchRowNumber": 533,
                "afterPatchRowNumber": 541,
                "PatchRowcode": "         \"\"\"Save cache to disk if possible, returns whether it was able to save."
            },
            "35": {
                "beforePatchRowNumber": 534,
                "afterPatchRowNumber": 542,
                "PatchRowcode": " "
            },
            "36": {
                "beforePatchRowNumber": 559,
                "afterPatchRowNumber": 567,
                "PatchRowcode": "                     # python program."
            },
            "37": {
                "beforePatchRowNumber": 560,
                "afterPatchRowNumber": 568,
                "PatchRowcode": "                     # thus we fall back to the approach using the normal open() method to open a file and write."
            },
            "38": {
                "beforePatchRowNumber": 561,
                "afterPatchRowNumber": 569,
                "PatchRowcode": "                     with open(tmp_file, \"wb\") as w_file:"
            },
            "39": {
                "beforePatchRowNumber": 562,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        w_file.write(pickle.dumps(self))"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 570,
                "PatchRowcode": "+                        w_file.write(self._serialize())"
            },
            "41": {
                "beforePatchRowNumber": 563,
                "afterPatchRowNumber": 571,
                "PatchRowcode": "                     # We write to a tmp file and then move it to have atomic write"
            },
            "42": {
                "beforePatchRowNumber": 564,
                "afterPatchRowNumber": 572,
                "PatchRowcode": "                     os.replace(tmp_file_path, self.file_path)"
            },
            "43": {
                "beforePatchRowNumber": 565,
                "afterPatchRowNumber": 573,
                "PatchRowcode": "                     self.last_loaded = datetime.datetime.fromtimestamp("
            }
        },
        "frontPatchFile": [
            "#",
            "# Copyright (c) 2012-2023 Snowflake Computing Inc. All rights reserved.",
            "#",
            "",
            "from __future__ import annotations",
            "",
            "import datetime",
            "import logging",
            "import os",
            "import pickle",
            "import platform",
            "import random",
            "import string",
            "import tempfile",
            "from collections.abc import Iterator",
            "from threading import Lock",
            "from typing import Generic, NoReturn, TypeVar",
            "",
            "from filelock import FileLock, Timeout",
            "from typing_extensions import NamedTuple, Self",
            "",
            "from . import constants",
            "from .constants import ENV_VAR_TEST_MODE",
            "",
            "now = datetime.datetime.now",
            "getmtime = os.path.getmtime",
            "",
            "T = TypeVar(\"T\")",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "test_mode = os.getenv(ENV_VAR_TEST_MODE, \"\").lower() == \"true\"",
            "",
            "",
            "class CacheEntry(NamedTuple, Generic[T]):",
            "    expiry: datetime.datetime",
            "    entry: T",
            "",
            "",
            "K = TypeVar(\"K\")",
            "V = TypeVar(\"V\")",
            "",
            "",
            "def is_expired(d: datetime.datetime) -> bool:",
            "    return now() >= d",
            "",
            "",
            "class SFDictCache(Generic[K, V]):",
            "    \"\"\"A generic in-memory cache that acts somewhat like a dictionary.",
            "",
            "    Unlike normal dictionaries keys(), values() and items() return list materialized",
            "    at call time, unlike normal dictionaries that return a view object.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        entry_lifetime: int = constants.DAY_IN_SECONDS,",
            "    ) -> None:",
            "        \"\"\"Inits a SFDictCache with lifetime.\"\"\"",
            "        self._entry_lifetime = datetime.timedelta(seconds=entry_lifetime)",
            "        self._cache: dict[K, CacheEntry[V]] = {}",
            "        self._lock = Lock()",
            "        self._reset_telemetry()",
            "",
            "    def __len__(self) -> int:",
            "        with self._lock:",
            "            return len(self._cache)",
            "",
            "    @classmethod",
            "    def from_dict(",
            "        cls,",
            "        _dict: dict[K, V],",
            "        **kw,",
            "    ) -> Self:",
            "        \"\"\"Create an dictionary cache from an already existing dictionary.",
            "",
            "        Note that the same references will be stored in the cache than in",
            "        the dictionary provided.",
            "        \"\"\"",
            "        cache = cls(**kw)",
            "        cache.update(_dict)",
            "        return cache",
            "",
            "    def _getitem(",
            "        self,",
            "        k: K,",
            "        *,",
            "        should_record_hits: bool = True,",
            "    ) -> V:",
            "        \"\"\"Non-locking version of __getitem__.",
            "",
            "        This should only be used by internal functions when already",
            "        holding self._lock.",
            "        \"\"\"",
            "        if test_mode:",
            "            assert (",
            "                self._lock.locked()",
            "            ), \"The mutex self._lock should be locked by this thread\"",
            "        try:",
            "            t, v = self._cache[k]",
            "        except KeyError:",
            "            self._miss(k)",
            "            raise",
            "        if is_expired(t):",
            "            self._expire(k)",
            "        if should_record_hits:",
            "            self._hit(k)",
            "        return v",
            "",
            "    # aliasing _getitem to unify the api with SFDictFileCache",
            "    _getitem_non_locking = _getitem",
            "",
            "    def _setitem(self, k: K, v: V) -> None:",
            "        \"\"\"Non-locking version of __setitem__.",
            "",
            "        This should only be used by internal functions when already",
            "        holding self._lock.",
            "        \"\"\"",
            "        if test_mode:",
            "            assert (",
            "                self._lock.locked()",
            "            ), \"The mutex self._lock should be locked by this thread\"",
            "        self._cache[k] = CacheEntry(",
            "            expiry=now() + self._entry_lifetime,",
            "            entry=v,",
            "        )",
            "        self._add_or_remove()",
            "",
            "    def __getitem__(",
            "        self,",
            "        k: K,",
            "    ) -> V:",
            "        \"\"\"Returns an element if it hasn't expired yet in a thread-safe way.\"\"\"",
            "        with self._lock:",
            "            return self._getitem(k, should_record_hits=True)",
            "",
            "    def __setitem__(",
            "        self,",
            "        k: K,",
            "        v: V,",
            "    ) -> None:",
            "        \"\"\"Inserts an element in a thread-safe way.\"\"\"",
            "        with self._lock:",
            "            self._setitem(k, v)",
            "",
            "    def __iter__(self) -> Iterator[K]:",
            "        return iter(self.keys())",
            "",
            "    def keys(self) -> list[K]:",
            "        return [k for k, _ in self.items()]",
            "",
            "    def items(self) -> list[tuple[K, V]]:",
            "        with self._lock:",
            "            values: list[tuple[K, V]] = []",
            "            for k in list(self._cache.keys()):",
            "                try:",
            "                    values.append((k, self._getitem(k, should_record_hits=False)))",
            "                except KeyError:",
            "                    pass",
            "        return values",
            "",
            "    def values(self) -> list[V]:",
            "        return [v for _, v in self.items()]",
            "",
            "    def get(",
            "        self,",
            "        k: K,",
            "        default: V | None = None,",
            "    ) -> V | None:",
            "        try:",
            "            return self[k]",
            "        except KeyError:",
            "            return default",
            "",
            "    def clear(self) -> None:",
            "        with self._lock:",
            "            self._cache.clear()",
            "            self._reset_telemetry()",
            "",
            "    def _delitem(",
            "        self,",
            "        key: K,",
            "    ) -> None:",
            "        \"\"\"Non-locking version of __delitem__.",
            "",
            "        This should only be used by internal functions when already",
            "        holding self._lock.",
            "        \"\"\"",
            "        if test_mode:",
            "            assert (",
            "                self._lock.locked()",
            "            ), \"The mutex self._lock should be locked by this thread\"",
            "        del self._cache[key]",
            "        self._add_or_remove()",
            "",
            "    def __delitem__(",
            "        self,",
            "        key: K,",
            "    ) -> None:",
            "        with self._lock:",
            "            self._delitem(key)",
            "",
            "    def __contains__(",
            "        self,",
            "        key: K,",
            "    ) -> bool:",
            "        with self._lock:",
            "            try:",
            "                self._getitem(key, should_record_hits=True)",
            "                return True",
            "            except KeyError:",
            "                # Fall through",
            "                return False",
            "",
            "    def _update(",
            "        self,",
            "        other: dict[K, V] | list[tuple[K, V]] | SFDictCache[K, V],",
            "        update_newer_only: bool = False,",
            "    ) -> bool:",
            "        \"\"\"Non-locking version of update.",
            "",
            "        This should only be used by internal functions when already",
            "        holding self._lock and other._lock.",
            "        \"\"\"",
            "        if test_mode:",
            "            assert (",
            "                self._lock.locked()",
            "            ), \"The mutex self._lock should be locked by this thread\"",
            "        to_insert: dict[K, CacheEntry[V]]",
            "        self._clear_expired_entries()",
            "        if isinstance(other, (list, dict)):",
            "            expiry = now() + self._entry_lifetime",
            "            if isinstance(other, list):",
            "                g = iter(other)",
            "            elif isinstance(other, dict):",
            "                g = iter(other.items())",
            "            to_insert = {k: CacheEntry(expiry=expiry, entry=v) for k, v in g}",
            "        elif isinstance(other, SFDictCache):",
            "            other.clear_expired_entries()",
            "            others_items = list(other._cache.items())",
            "            # Only accept values from another cache if their key is not in self,",
            "            #  or if expiry is later the self known one",
            "            to_insert = {",
            "                k: v",
            "                for k, v in others_items",
            "                if (",
            "                    # self doesn't have this key",
            "                    k not in self._cache",
            "                    # we should update entries, regardless of whether they are newer",
            "                    or (not update_newer_only)",
            "                    # other has newer expiry time we want to update newer values only",
            "                    or self._cache[k].expiry < v.expiry",
            "                )",
            "            }",
            "        else:",
            "            raise TypeError",
            "        self._cache.update(to_insert)",
            "        if to_insert:",
            "            self._add_or_remove()",
            "        # TODO: this should really save_if_should",
            "        return len(to_insert) > 0",
            "",
            "    def update(",
            "        self,",
            "        other: dict[K, V] | list[tuple[K, V]] | SFDictCache[K, V],",
            "        update_newer_only: bool = False,",
            "    ) -> bool:",
            "        \"\"\"Insert multiple values at the same time, if self could learn from the other.",
            "",
            "        If this function is given a dictionary, or list expiration timestamps",
            "        will be all the same a self._entry_lifetime form now. If it's",
            "        given another SFDictCache then the timestamps will be taken",
            "        from the other cache.",
            "",
            "        Returns a boolean. It describes whether self learnt anything from other.",
            "",
            "        Note that clear_expired_entries will be called on both caches. To",
            "        prevent deadlocks this is done without acquiring other._lock. The",
            "        intended behavior is to use this function with an unpickled/unused cache.",
            "        If live caches are being merged then use .items() on them first and merge those",
            "        into the other caches.",
            "        \"\"\"",
            "        with self._lock:",
            "            return self._update(other, update_newer_only)",
            "",
            "    def update_newer(",
            "        self,",
            "        other: dict[K, V] | list[tuple[K, V]] | SFDictCache[K, V],",
            "    ) -> bool:",
            "        \"\"\"This function is like update, but it only updates newer elements.\"\"\"",
            "        with self._lock:",
            "            return self._update(",
            "                other,",
            "                update_newer_only=True,",
            "            )",
            "",
            "    def _clear_expired_entries(self) -> None:",
            "        if test_mode:",
            "            assert (",
            "                self._lock.locked()",
            "            ), \"The mutex self._lock should be locked by this thread\"",
            "        cache_updated = False",
            "        for k in list(self._cache.keys()):",
            "            try:",
            "                self._getitem(k, should_record_hits=False)",
            "            except KeyError:",
            "                # the only case KeyError raised in this method",
            "                # is that k is expired",
            "                cache_updated = True",
            "        if cache_updated:",
            "            self._add_or_remove()",
            "",
            "    def clear_expired_entries(self) -> None:",
            "        \"\"\"Remove expired entries from the cache.\"\"\"",
            "        with self._lock:",
            "            self._clear_expired_entries()",
            "",
            "    # Telemetry related functions, these can be plugged by child classes",
            "    def _reset_telemetry(self) -> None:",
            "        \"\"\"(Re)set telemetry fields.",
            "",
            "        This function will be called by the initalizer and other functions that should",
            "        reset telemtry entries.",
            "        \"\"\"",
            "        self.telemetry = {",
            "            \"hit\": 0,",
            "            \"miss\": 0,",
            "            \"expiration\": 0,",
            "            \"size\": 0,",
            "        }",
            "",
            "    def _hit(self, k: K) -> None:",
            "        \"\"\"This function gets called when a hit occurs.",
            "",
            "        Functions that hit every entry (like values) is not going to count.",
            "",
            "        Note that while this function does not interact with lock, but it's only",
            "        called from contexts where the lock is already held.",
            "        \"\"\"",
            "        self.telemetry[\"hit\"] += 1",
            "",
            "    def _miss(self, k: K) -> None:",
            "        \"\"\"This function gets called when a miss occurs.",
            "",
            "        Note that while this function does not interact with lock, but it's only",
            "        called from contexts where the lock is already held.",
            "        \"\"\"",
            "        self.telemetry[\"miss\"] += 1",
            "",
            "    def _expiration(self, k: K) -> None:",
            "        \"\"\"This function gets called when an expiration occurs.",
            "",
            "        Note that while this function does not interact with lock, but it's only",
            "        called from contexts where the lock is already held.",
            "        \"\"\"",
            "        self.telemetry[\"expiration\"] += 1",
            "",
            "    def _expire(self, k: K) -> NoReturn:",
            "        \"\"\"Helper function to call _expiration and delete an item.\"\"\"",
            "        self._expiration(k)",
            "        self._delitem(k)",
            "        raise KeyError",
            "",
            "    def _add_or_remove(self) -> None:",
            "        \"\"\"This function gets called when an element is added, or removed.",
            "",
            "        Note that while this function does not interact with lock, but it's only",
            "        called from contexts where the lock is already held.",
            "        \"\"\"",
            "        self.telemetry[\"size\"] = len(self._cache)",
            "",
            "",
            "class SFDictFileCache(SFDictCache):",
            "    # This number decides the chance of saving after writing (probability: 1/n+1)",
            "    MAX_RAND_INT = 9",
            "    _ATTRIBUTES_TO_PICKLE = (",
            "        \"_entry_lifetime\",",
            "        \"_cache\",",
            "        \"telemetry\",",
            "        \"file_path\",",
            "        \"file_timeout\",",
            "        \"_file_lock_path\",",
            "        \"last_loaded\",",
            "    )",
            "",
            "    def __init__(",
            "        self,",
            "        file_path: str | dict[str, str],",
            "        entry_lifetime: int = constants.DAY_IN_SECONDS,",
            "        file_timeout: int = 0,",
            "    ) -> None:",
            "        \"\"\"Inits an SFDictFileCache with path, lifetime.",
            "",
            "        File path can be a dictionary that contains different paths for different OSes,",
            "        possible keys are: 'darwin', 'linux' and 'windows'. If a current platform",
            "        cannot be determined, or is not in the dictionary we'll use the first value.",
            "",
            "        Once we select a location based on file path, we write and read a random",
            "        temporary file to check for read/write permissions. If this fails OSError might",
            "        be thrown.",
            "        \"\"\"",
            "        super().__init__(",
            "            entry_lifetime=entry_lifetime,",
            "        )",
            "        if isinstance(file_path, str):",
            "            self.file_path = os.path.expanduser(file_path)",
            "        else:",
            "            current_platform = platform.system().lower()",
            "            if current_platform is None or current_platform not in file_path:",
            "                self.file_path = next(iter(file_path.values()))",
            "            else:",
            "                self.file_path = os.path.expanduser(file_path[current_platform])",
            "        # Once we decided on where to put the file cache make sure that this",
            "        #  place is readable/writable by us",
            "        random_string = \"\".join(random.choice(string.ascii_letters) for _ in range(5))",
            "        cache_folder = os.path.dirname(self.file_path)",
            "        try:",
            "            tmp_file, tmp_file_path = tempfile.mkstemp(",
            "                dir=cache_folder,",
            "            )",
            "        except OSError as o_err:",
            "            raise PermissionError(",
            "                o_err.errno,",
            "                \"Cache folder is not writeable\",",
            "                cache_folder,",
            "            )",
            "        try:",
            "            with open(tmp_file, \"w\") as w_file:",
            "                # If mkstemp didn't fail this shouldn't throw an error",
            "                w_file.write(random_string)",
            "            try:",
            "                with open(tmp_file_path) as r_file:",
            "                    if r_file.read() != random_string:",
            "                        Exception(\"Temporary file just written has wrong content\")",
            "            except OSError as o_err:",
            "                raise PermissionError(",
            "                    o_err.errno,",
            "                    \"Cache file is not readable\",",
            "                    tmp_file_path,",
            "                )",
            "        finally:",
            "            if os.path.exists(tmp_file_path) and os.path.isfile(tmp_file_path):",
            "                os.unlink(tmp_file_path)",
            "        self.file_timeout = file_timeout",
            "        self._file_lock_path = f\"{self.file_path}.lock\"",
            "        self._file_lock = FileLock(self._file_lock_path, timeout=self.file_timeout)",
            "        self.last_loaded: datetime.datetime | None = None",
            "        if os.path.exists(self.file_path):",
            "            with self._lock:",
            "                self._load()",
            "        # indicate whether the cache is modified or not, this variable is for",
            "        # SFDictFileCache to determine whether to dump cache to file when _save is called",
            "        self._cache_modified = False",
            "",
            "    def _getitem_non_locking(",
            "        self,",
            "        k: K,",
            "        *,",
            "        should_record_hits: bool = True,",
            "    ) -> V:",
            "        \"\"\"Non-locking version of __getitem__ of SFDictFileCache.",
            "",
            "        This should only be used by internal functions when already",
            "        holding self._lock.",
            "",
            "        Note that we do not overwrite _getitem because _getitem is used by",
            "        self._load to clear in-memory expired caches. Overwriting would cause",
            "        infinite recursive call.",
            "        \"\"\"",
            "        if k not in self._cache:",
            "            loaded = self._load_if_should()",
            "            if (not loaded) or k not in self._cache:",
            "                self._miss(k)",
            "                raise KeyError",
            "        t, v = self._cache[k]",
            "        if is_expired(t):",
            "            loaded = self._load_if_should()",
            "            expire_item = True",
            "            if loaded:",
            "                t, v = self._cache[k]",
            "                expire_item = is_expired(t)",
            "            if expire_item:",
            "                # Raises KeyError",
            "                self._expire(k)",
            "        self._hit(k)",
            "        return v",
            "",
            "    def __getitem__(self, k: K) -> V:",
            "        \"\"\"Returns an element if it hasn't expired yet in a thread-safe way.\"\"\"",
            "        with self._lock:",
            "            return self._getitem_non_locking(k)",
            "",
            "    def _setitem(self, k: K, v: V) -> None:",
            "        super()._setitem(k, v)",
            "        self._save_if_should()",
            "",
            "    def _load(self) -> bool:",
            "        \"\"\"Load cache from disk if possible, returns whether it was able to load.\"\"\"",
            "        try:",
            "            with open(self.file_path, \"rb\") as r_file:",
            "                other: SFDictFileCache = pickle.load(r_file)",
            "            # Since we want to know whether we are dirty after loading",
            "            #  we have to know whether the file could learn anything from self",
            "            #  so instead of calling self.update we call other.update and swap",
            "            #  the 2 underlying caches after.",
            "            self._lock.release()",
            "            cache_file_learnt = other.update(",
            "                self,",
            "                update_newer_only=True,",
            "            )",
            "            self._lock.acquire()",
            "            self._cache, other._cache = other._cache, self._cache",
            "            self.telemetry[\"size\"] = other.telemetry[\"size\"]",
            "            self._cache_modified = cache_file_learnt",
            "            self.last_loaded = now()",
            "            return True",
            "        except (AssertionError, RuntimeError):",
            "            raise",
            "        except Exception as e:",
            "            logger.debug(\"Fail to read cache from disk due to error: %s\", e)",
            "            return False",
            "",
            "    def load(self) -> bool:",
            "        \"\"\"Load cache from disk if possible, returns whether it was able to load.",
            "",
            "        This is the public version of _load, it makes sure that all the",
            "        necessary locks are acquired.",
            "        \"\"\"",
            "        with self._lock:",
            "            return self._load()",
            "",
            "    def _save(self, load_first: bool = True, force_flush: bool = False) -> bool:",
            "        \"\"\"Save cache to disk if possible, returns whether it was able to save.",
            "",
            "        This function is non-locking when it comes to self._lock.",
            "        \"\"\"",
            "        if test_mode:",
            "            assert (",
            "                self._lock.locked()",
            "            ), \"The mutex self._lock should be locked by this thread\"",
            "        self._clear_expired_entries()",
            "        if not self._cache_modified and not force_flush:",
            "            # cache is not updated, so there is no need to dump cache to file, we just return",
            "            return False",
            "        try:",
            "            with self._file_lock:",
            "                if load_first:",
            "                    self._load_if_should()",
            "                _dir, fname = os.path.split(self.file_path)",
            "                try:",
            "                    tmp_file, tmp_file_path = tempfile.mkstemp(",
            "                        prefix=fname,",
            "                        dir=_dir,",
            "                    )",
            "                    # tmp_file is an opened OS level handle, which means we need to close it manually.",
            "                    # https://docs.python.org/3/library/tempfile.html#tempfile.mkstemp",
            "                    # ideally we shall just use the tmp_file fd to write,",
            "                    # however, using os.write(tmp_file, bytes) causes seg fault during garbage collection when exiting",
            "                    # python program.",
            "                    # thus we fall back to the approach using the normal open() method to open a file and write.",
            "                    with open(tmp_file, \"wb\") as w_file:",
            "                        w_file.write(pickle.dumps(self))",
            "                    # We write to a tmp file and then move it to have atomic write",
            "                    os.replace(tmp_file_path, self.file_path)",
            "                    self.last_loaded = datetime.datetime.fromtimestamp(",
            "                        getmtime(self.file_path),",
            "                    )",
            "                    # after update, reset self._cache_modified to indicate it's up-to-update to avoid unnecessary flush",
            "                    self._cache_modified = False",
            "                    return True",
            "                except NameError:",
            "                    # note: when exiting python program, garbage collection will kick in",
            "                    # leading to `open` being garbage collected,",
            "                    # calling `open` raises NameError, we close the tmp file fd here to release the tmp file fd",
            "                    try:",
            "                        os.close(tmp_file)",
            "                    except OSError:",
            "                        pass",
            "                except OSError as o_err:",
            "                    raise PermissionError(",
            "                        o_err.errno,",
            "                        \"Cache folder is not writeable\",",
            "                        _dir,",
            "                    )",
            "                finally:",
            "                    if os.path.exists(tmp_file_path) and os.path.isfile(tmp_file_path):",
            "                        os.unlink(tmp_file_path)",
            "        except Timeout:",
            "            logger.debug(",
            "                f\"acquiring {self._file_lock_path} timed out, skipping saving...\"",
            "            )",
            "        except (AssertionError, RuntimeError):",
            "            raise",
            "        except Exception as e:",
            "            logger.debug(\"Fail to write cache to disk due to error: %s\", e)",
            "        return False",
            "",
            "    def save(self, load_first: bool = True) -> bool:",
            "        \"\"\"Save cache to disk if possible, returns whether it was able to save.",
            "",
            "        This is the public version of _save, it makes sure that all the",
            "        necessary locks are acquired.",
            "        \"\"\"",
            "        with self._lock:",
            "            return self._save(load_first)",
            "",
            "    def _save_if_should(self) -> bool:",
            "        \"\"\"Saves file to disk if necessary and returns whether it saved.",
            "",
            "        Uses self._should_save to decide whether to save.",
            "        \"\"\"",
            "        if self._should_save():",
            "            return self._save()",
            "        return False",
            "",
            "    def _load_if_should(self) -> bool:",
            "        \"\"\"Load file to disk if necessary and returns whether it loaded.",
            "",
            "        Uses self._should_load to decide whether to load.",
            "        \"\"\"",
            "        if self._should_load():",
            "            return self._load()",
            "        return False",
            "",
            "    def _should_save(self) -> bool:",
            "        \"\"\"Decide whether we should save.",
            "",
            "        This is a simple random number generator to randomize writes across processes",
            "        that are possibly saving the same values in this cache.",
            "        \"\"\"",
            "        return random.randint(0, self.MAX_RAND_INT) == 0",
            "",
            "    def _should_load(self) -> bool:",
            "        \"\"\"Decide whether we should load.",
            "",
            "        We should load if the file on disk has changed since we have last read it.",
            "        \"\"\"",
            "        if os.path.exists(self.file_path) and os.path.isfile(self.file_path):",
            "            if self.last_loaded is None:",
            "                return True",
            "            return (",
            "                datetime.datetime.fromtimestamp(",
            "                    getmtime(self.file_path),",
            "                )",
            "                > self.last_loaded",
            "            )",
            "        return False",
            "",
            "    def clear(self) -> None:",
            "        super().clear()",
            "        # This unlink prevents us from loading just before saving",
            "        with self._file_lock:",
            "            if os.path.exists(self.file_path) and os.path.isfile(self.file_path):",
            "                os.unlink(self.file_path)",
            "        # TODO: is this necessary?",
            "        with self._lock:",
            "            self._save(load_first=False, force_flush=True)",
            "",
            "    # Custom pickling implementation",
            "",
            "    def __getstate__(self) -> dict:",
            "        return {",
            "            k: v",
            "            for k, v in self.__dict__.items()",
            "            if k in SFDictFileCache._ATTRIBUTES_TO_PICKLE",
            "        }",
            "",
            "    def __setstate__(self, state: dict) -> None:",
            "        self.__dict__.update(state)",
            "        self._cache_modified = False",
            "        self._lock = Lock()",
            "        self._file_lock = FileLock(self._file_lock_path, timeout=self.file_timeout)",
            "",
            "    def _add_or_remove(self) -> None:",
            "        \"\"\"This function gets called when an element is added, or removed.",
            "",
            "        Note that while this function does not interact with lock, but it's only",
            "        called from contexts where the lock is already held.",
            "        \"\"\"",
            "        super()._add_or_remove()",
            "        self._cache_modified = True"
        ],
        "afterPatchFile": [
            "#",
            "# Copyright (c) 2012-2023 Snowflake Computing Inc. All rights reserved.",
            "#",
            "",
            "from __future__ import annotations",
            "",
            "import datetime",
            "import logging",
            "import os",
            "import pickle",
            "import platform",
            "import random",
            "import string",
            "import tempfile",
            "from collections.abc import Iterator",
            "from threading import Lock",
            "from typing import Generic, NoReturn, TypeVar",
            "",
            "from filelock import FileLock, Timeout",
            "from typing_extensions import NamedTuple, Self",
            "",
            "from . import constants",
            "from .constants import ENV_VAR_TEST_MODE",
            "",
            "now = datetime.datetime.now",
            "getmtime = os.path.getmtime",
            "",
            "T = TypeVar(\"T\")",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "test_mode = os.getenv(ENV_VAR_TEST_MODE, \"\").lower() == \"true\"",
            "",
            "",
            "class CacheEntry(NamedTuple, Generic[T]):",
            "    expiry: datetime.datetime",
            "    entry: T",
            "",
            "",
            "K = TypeVar(\"K\")",
            "V = TypeVar(\"V\")",
            "",
            "",
            "def is_expired(d: datetime.datetime) -> bool:",
            "    return now() >= d",
            "",
            "",
            "class SFDictCache(Generic[K, V]):",
            "    \"\"\"A generic in-memory cache that acts somewhat like a dictionary.",
            "",
            "    Unlike normal dictionaries keys(), values() and items() return list materialized",
            "    at call time, unlike normal dictionaries that return a view object.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        entry_lifetime: int = constants.DAY_IN_SECONDS,",
            "    ) -> None:",
            "        \"\"\"Inits a SFDictCache with lifetime.\"\"\"",
            "        self._entry_lifetime = datetime.timedelta(seconds=entry_lifetime)",
            "        self._cache: dict[K, CacheEntry[V]] = {}",
            "        self._lock = Lock()",
            "        self._reset_telemetry()",
            "",
            "    def __len__(self) -> int:",
            "        with self._lock:",
            "            return len(self._cache)",
            "",
            "    @classmethod",
            "    def from_dict(",
            "        cls,",
            "        _dict: dict[K, V],",
            "        **kw,",
            "    ) -> Self:",
            "        \"\"\"Create an dictionary cache from an already existing dictionary.",
            "",
            "        Note that the same references will be stored in the cache than in",
            "        the dictionary provided.",
            "        \"\"\"",
            "        cache = cls(**kw)",
            "        cache.update(_dict)",
            "        return cache",
            "",
            "    def _getitem(",
            "        self,",
            "        k: K,",
            "        *,",
            "        should_record_hits: bool = True,",
            "    ) -> V:",
            "        \"\"\"Non-locking version of __getitem__.",
            "",
            "        This should only be used by internal functions when already",
            "        holding self._lock.",
            "        \"\"\"",
            "        if test_mode:",
            "            assert (",
            "                self._lock.locked()",
            "            ), \"The mutex self._lock should be locked by this thread\"",
            "        try:",
            "            t, v = self._cache[k]",
            "        except KeyError:",
            "            self._miss(k)",
            "            raise",
            "        if is_expired(t):",
            "            self._expire(k)",
            "        if should_record_hits:",
            "            self._hit(k)",
            "        return v",
            "",
            "    # aliasing _getitem to unify the api with SFDictFileCache",
            "    _getitem_non_locking = _getitem",
            "",
            "    def _setitem(self, k: K, v: V) -> None:",
            "        \"\"\"Non-locking version of __setitem__.",
            "",
            "        This should only be used by internal functions when already",
            "        holding self._lock.",
            "        \"\"\"",
            "        if test_mode:",
            "            assert (",
            "                self._lock.locked()",
            "            ), \"The mutex self._lock should be locked by this thread\"",
            "        self._cache[k] = CacheEntry(",
            "            expiry=now() + self._entry_lifetime,",
            "            entry=v,",
            "        )",
            "        self._add_or_remove()",
            "",
            "    def __getitem__(",
            "        self,",
            "        k: K,",
            "    ) -> V:",
            "        \"\"\"Returns an element if it hasn't expired yet in a thread-safe way.\"\"\"",
            "        with self._lock:",
            "            return self._getitem(k, should_record_hits=True)",
            "",
            "    def __setitem__(",
            "        self,",
            "        k: K,",
            "        v: V,",
            "    ) -> None:",
            "        \"\"\"Inserts an element in a thread-safe way.\"\"\"",
            "        with self._lock:",
            "            self._setitem(k, v)",
            "",
            "    def __iter__(self) -> Iterator[K]:",
            "        return iter(self.keys())",
            "",
            "    def keys(self) -> list[K]:",
            "        return [k for k, _ in self.items()]",
            "",
            "    def items(self) -> list[tuple[K, V]]:",
            "        with self._lock:",
            "            values: list[tuple[K, V]] = []",
            "            for k in list(self._cache.keys()):",
            "                try:",
            "                    values.append((k, self._getitem(k, should_record_hits=False)))",
            "                except KeyError:",
            "                    pass",
            "        return values",
            "",
            "    def values(self) -> list[V]:",
            "        return [v for _, v in self.items()]",
            "",
            "    def get(",
            "        self,",
            "        k: K,",
            "        default: V | None = None,",
            "    ) -> V | None:",
            "        try:",
            "            return self[k]",
            "        except KeyError:",
            "            return default",
            "",
            "    def clear(self) -> None:",
            "        with self._lock:",
            "            self._cache.clear()",
            "            self._reset_telemetry()",
            "",
            "    def _delitem(",
            "        self,",
            "        key: K,",
            "    ) -> None:",
            "        \"\"\"Non-locking version of __delitem__.",
            "",
            "        This should only be used by internal functions when already",
            "        holding self._lock.",
            "        \"\"\"",
            "        if test_mode:",
            "            assert (",
            "                self._lock.locked()",
            "            ), \"The mutex self._lock should be locked by this thread\"",
            "        del self._cache[key]",
            "        self._add_or_remove()",
            "",
            "    def __delitem__(",
            "        self,",
            "        key: K,",
            "    ) -> None:",
            "        with self._lock:",
            "            self._delitem(key)",
            "",
            "    def __contains__(",
            "        self,",
            "        key: K,",
            "    ) -> bool:",
            "        with self._lock:",
            "            try:",
            "                self._getitem(key, should_record_hits=True)",
            "                return True",
            "            except KeyError:",
            "                # Fall through",
            "                return False",
            "",
            "    def _update(",
            "        self,",
            "        other: dict[K, V] | list[tuple[K, V]] | SFDictCache[K, V],",
            "        update_newer_only: bool = False,",
            "    ) -> bool:",
            "        \"\"\"Non-locking version of update.",
            "",
            "        This should only be used by internal functions when already",
            "        holding self._lock and other._lock.",
            "        \"\"\"",
            "        if test_mode:",
            "            assert (",
            "                self._lock.locked()",
            "            ), \"The mutex self._lock should be locked by this thread\"",
            "        to_insert: dict[K, CacheEntry[V]]",
            "        self._clear_expired_entries()",
            "        if isinstance(other, (list, dict)):",
            "            expiry = now() + self._entry_lifetime",
            "            if isinstance(other, list):",
            "                g = iter(other)",
            "            elif isinstance(other, dict):",
            "                g = iter(other.items())",
            "            to_insert = {k: CacheEntry(expiry=expiry, entry=v) for k, v in g}",
            "        elif isinstance(other, SFDictCache):",
            "            other.clear_expired_entries()",
            "            others_items = list(other._cache.items())",
            "            # Only accept values from another cache if their key is not in self,",
            "            #  or if expiry is later the self known one",
            "            to_insert = {",
            "                k: v",
            "                for k, v in others_items",
            "                if (",
            "                    # self doesn't have this key",
            "                    k not in self._cache",
            "                    # we should update entries, regardless of whether they are newer",
            "                    or (not update_newer_only)",
            "                    # other has newer expiry time we want to update newer values only",
            "                    or self._cache[k].expiry < v.expiry",
            "                )",
            "            }",
            "        else:",
            "            raise TypeError",
            "        self._cache.update(to_insert)",
            "        if to_insert:",
            "            self._add_or_remove()",
            "        # TODO: this should really save_if_should",
            "        return len(to_insert) > 0",
            "",
            "    def update(",
            "        self,",
            "        other: dict[K, V] | list[tuple[K, V]] | SFDictCache[K, V],",
            "        update_newer_only: bool = False,",
            "    ) -> bool:",
            "        \"\"\"Insert multiple values at the same time, if self could learn from the other.",
            "",
            "        If this function is given a dictionary, or list expiration timestamps",
            "        will be all the same a self._entry_lifetime form now. If it's",
            "        given another SFDictCache then the timestamps will be taken",
            "        from the other cache.",
            "",
            "        Returns a boolean. It describes whether self learnt anything from other.",
            "",
            "        Note that clear_expired_entries will be called on both caches. To",
            "        prevent deadlocks this is done without acquiring other._lock. The",
            "        intended behavior is to use this function with an unpickled/unused cache.",
            "        If live caches are being merged then use .items() on them first and merge those",
            "        into the other caches.",
            "        \"\"\"",
            "        with self._lock:",
            "            return self._update(other, update_newer_only)",
            "",
            "    def update_newer(",
            "        self,",
            "        other: dict[K, V] | list[tuple[K, V]] | SFDictCache[K, V],",
            "    ) -> bool:",
            "        \"\"\"This function is like update, but it only updates newer elements.\"\"\"",
            "        with self._lock:",
            "            return self._update(",
            "                other,",
            "                update_newer_only=True,",
            "            )",
            "",
            "    def _clear_expired_entries(self) -> None:",
            "        if test_mode:",
            "            assert (",
            "                self._lock.locked()",
            "            ), \"The mutex self._lock should be locked by this thread\"",
            "        cache_updated = False",
            "        for k in list(self._cache.keys()):",
            "            try:",
            "                self._getitem(k, should_record_hits=False)",
            "            except KeyError:",
            "                # the only case KeyError raised in this method",
            "                # is that k is expired",
            "                cache_updated = True",
            "        if cache_updated:",
            "            self._add_or_remove()",
            "",
            "    def clear_expired_entries(self) -> None:",
            "        \"\"\"Remove expired entries from the cache.\"\"\"",
            "        with self._lock:",
            "            self._clear_expired_entries()",
            "",
            "    # Telemetry related functions, these can be plugged by child classes",
            "    def _reset_telemetry(self) -> None:",
            "        \"\"\"(Re)set telemetry fields.",
            "",
            "        This function will be called by the initalizer and other functions that should",
            "        reset telemtry entries.",
            "        \"\"\"",
            "        self.telemetry = {",
            "            \"hit\": 0,",
            "            \"miss\": 0,",
            "            \"expiration\": 0,",
            "            \"size\": 0,",
            "        }",
            "",
            "    def _hit(self, k: K) -> None:",
            "        \"\"\"This function gets called when a hit occurs.",
            "",
            "        Functions that hit every entry (like values) is not going to count.",
            "",
            "        Note that while this function does not interact with lock, but it's only",
            "        called from contexts where the lock is already held.",
            "        \"\"\"",
            "        self.telemetry[\"hit\"] += 1",
            "",
            "    def _miss(self, k: K) -> None:",
            "        \"\"\"This function gets called when a miss occurs.",
            "",
            "        Note that while this function does not interact with lock, but it's only",
            "        called from contexts where the lock is already held.",
            "        \"\"\"",
            "        self.telemetry[\"miss\"] += 1",
            "",
            "    def _expiration(self, k: K) -> None:",
            "        \"\"\"This function gets called when an expiration occurs.",
            "",
            "        Note that while this function does not interact with lock, but it's only",
            "        called from contexts where the lock is already held.",
            "        \"\"\"",
            "        self.telemetry[\"expiration\"] += 1",
            "",
            "    def _expire(self, k: K) -> NoReturn:",
            "        \"\"\"Helper function to call _expiration and delete an item.\"\"\"",
            "        self._expiration(k)",
            "        self._delitem(k)",
            "        raise KeyError",
            "",
            "    def _add_or_remove(self) -> None:",
            "        \"\"\"This function gets called when an element is added, or removed.",
            "",
            "        Note that while this function does not interact with lock, but it's only",
            "        called from contexts where the lock is already held.",
            "        \"\"\"",
            "        self.telemetry[\"size\"] = len(self._cache)",
            "",
            "",
            "class SFDictFileCache(SFDictCache):",
            "    # This number decides the chance of saving after writing (probability: 1/n+1)",
            "    MAX_RAND_INT = 9",
            "    _ATTRIBUTES_TO_PICKLE = (",
            "        \"_entry_lifetime\",",
            "        \"_cache\",",
            "        \"telemetry\",",
            "        \"file_path\",",
            "        \"file_timeout\",",
            "        \"_file_lock_path\",",
            "        \"last_loaded\",",
            "    )",
            "",
            "    def __init__(",
            "        self,",
            "        file_path: str | dict[str, str],",
            "        entry_lifetime: int = constants.DAY_IN_SECONDS,",
            "        file_timeout: int = 0,",
            "        load_if_file_exists: bool = True,",
            "    ) -> None:",
            "        \"\"\"Inits an SFDictFileCache with path, lifetime.",
            "",
            "        File path can be a dictionary that contains different paths for different OSes,",
            "        possible keys are: 'darwin', 'linux' and 'windows'. If a current platform",
            "        cannot be determined, or is not in the dictionary we'll use the first value.",
            "",
            "        Once we select a location based on file path, we write and read a random",
            "        temporary file to check for read/write permissions. If this fails OSError might",
            "        be thrown.",
            "        \"\"\"",
            "        super().__init__(",
            "            entry_lifetime=entry_lifetime,",
            "        )",
            "        if isinstance(file_path, str):",
            "            self.file_path = os.path.expanduser(file_path)",
            "        else:",
            "            current_platform = platform.system().lower()",
            "            if current_platform is None or current_platform not in file_path:",
            "                self.file_path = next(iter(file_path.values()))",
            "            else:",
            "                self.file_path = os.path.expanduser(file_path[current_platform])",
            "        # Once we decided on where to put the file cache make sure that this",
            "        #  place is readable/writable by us",
            "        random_string = \"\".join(random.choice(string.ascii_letters) for _ in range(5))",
            "        cache_folder = os.path.dirname(self.file_path)",
            "        try:",
            "            tmp_file, tmp_file_path = tempfile.mkstemp(",
            "                dir=cache_folder,",
            "            )",
            "        except OSError as o_err:",
            "            raise PermissionError(",
            "                o_err.errno,",
            "                \"Cache folder is not writeable\",",
            "                cache_folder,",
            "            )",
            "        try:",
            "            with open(tmp_file, \"w\") as w_file:",
            "                # If mkstemp didn't fail this shouldn't throw an error",
            "                w_file.write(random_string)",
            "            try:",
            "                with open(tmp_file_path) as r_file:",
            "                    if r_file.read() != random_string:",
            "                        Exception(\"Temporary file just written has wrong content\")",
            "            except OSError as o_err:",
            "                raise PermissionError(",
            "                    o_err.errno,",
            "                    \"Cache file is not readable\",",
            "                    tmp_file_path,",
            "                )",
            "        finally:",
            "            if os.path.exists(tmp_file_path) and os.path.isfile(tmp_file_path):",
            "                os.unlink(tmp_file_path)",
            "        self.file_timeout = file_timeout",
            "        self._file_lock_path = f\"{self.file_path}.lock\"",
            "        self._file_lock = FileLock(self._file_lock_path, timeout=self.file_timeout)",
            "        self.last_loaded: datetime.datetime | None = None",
            "        if os.path.exists(self.file_path) and load_if_file_exists:",
            "            with self._lock:",
            "                self._load()",
            "        # indicate whether the cache is modified or not, this variable is for",
            "        # SFDictFileCache to determine whether to dump cache to file when _save is called",
            "        self._cache_modified = False",
            "",
            "    def _getitem_non_locking(",
            "        self,",
            "        k: K,",
            "        *,",
            "        should_record_hits: bool = True,",
            "    ) -> V:",
            "        \"\"\"Non-locking version of __getitem__ of SFDictFileCache.",
            "",
            "        This should only be used by internal functions when already",
            "        holding self._lock.",
            "",
            "        Note that we do not overwrite _getitem because _getitem is used by",
            "        self._load to clear in-memory expired caches. Overwriting would cause",
            "        infinite recursive call.",
            "        \"\"\"",
            "        if k not in self._cache:",
            "            loaded = self._load_if_should()",
            "            if (not loaded) or k not in self._cache:",
            "                self._miss(k)",
            "                raise KeyError",
            "        t, v = self._cache[k]",
            "        if is_expired(t):",
            "            loaded = self._load_if_should()",
            "            expire_item = True",
            "            if loaded:",
            "                t, v = self._cache[k]",
            "                expire_item = is_expired(t)",
            "            if expire_item:",
            "                # Raises KeyError",
            "                self._expire(k)",
            "        self._hit(k)",
            "        return v",
            "",
            "    def __getitem__(self, k: K) -> V:",
            "        \"\"\"Returns an element if it hasn't expired yet in a thread-safe way.\"\"\"",
            "        with self._lock:",
            "            return self._getitem_non_locking(k)",
            "",
            "    def _setitem(self, k: K, v: V) -> None:",
            "        super()._setitem(k, v)",
            "        self._save_if_should()",
            "",
            "    def _load(self) -> bool:",
            "        \"\"\"Load cache from disk if possible, returns whether it was able to load.\"\"\"",
            "        try:",
            "            with open(self.file_path, \"rb\") as r_file:",
            "                other: SFDictFileCache = self._deserialize(r_file)",
            "            # Since we want to know whether we are dirty after loading",
            "            #  we have to know whether the file could learn anything from self",
            "            #  so instead of calling self.update we call other.update and swap",
            "            #  the 2 underlying caches after.",
            "            self._lock.release()",
            "            cache_file_learnt = other.update(",
            "                self,",
            "                update_newer_only=True,",
            "            )",
            "            self._lock.acquire()",
            "            self._cache, other._cache = other._cache, self._cache",
            "            self.telemetry[\"size\"] = other.telemetry[\"size\"]",
            "            self._cache_modified = cache_file_learnt",
            "            self.last_loaded = now()",
            "            return True",
            "        except (AssertionError, RuntimeError):",
            "            raise",
            "        except Exception as e:",
            "            logger.debug(\"Fail to read cache from disk due to error: %s\", e)",
            "            return False",
            "",
            "    def load(self) -> bool:",
            "        \"\"\"Load cache from disk if possible, returns whether it was able to load.",
            "",
            "        This is the public version of _load, it makes sure that all the",
            "        necessary locks are acquired.",
            "        \"\"\"",
            "        with self._lock:",
            "            return self._load()",
            "",
            "    def _serialize(self):",
            "        return pickle.dumps(self)",
            "",
            "    @classmethod",
            "    def _deserialize(cls, r_file):",
            "        return pickle.load(r_file)",
            "",
            "    def _save(self, load_first: bool = True, force_flush: bool = False) -> bool:",
            "        \"\"\"Save cache to disk if possible, returns whether it was able to save.",
            "",
            "        This function is non-locking when it comes to self._lock.",
            "        \"\"\"",
            "        if test_mode:",
            "            assert (",
            "                self._lock.locked()",
            "            ), \"The mutex self._lock should be locked by this thread\"",
            "        self._clear_expired_entries()",
            "        if not self._cache_modified and not force_flush:",
            "            # cache is not updated, so there is no need to dump cache to file, we just return",
            "            return False",
            "        try:",
            "            with self._file_lock:",
            "                if load_first:",
            "                    self._load_if_should()",
            "                _dir, fname = os.path.split(self.file_path)",
            "                try:",
            "                    tmp_file, tmp_file_path = tempfile.mkstemp(",
            "                        prefix=fname,",
            "                        dir=_dir,",
            "                    )",
            "                    # tmp_file is an opened OS level handle, which means we need to close it manually.",
            "                    # https://docs.python.org/3/library/tempfile.html#tempfile.mkstemp",
            "                    # ideally we shall just use the tmp_file fd to write,",
            "                    # however, using os.write(tmp_file, bytes) causes seg fault during garbage collection when exiting",
            "                    # python program.",
            "                    # thus we fall back to the approach using the normal open() method to open a file and write.",
            "                    with open(tmp_file, \"wb\") as w_file:",
            "                        w_file.write(self._serialize())",
            "                    # We write to a tmp file and then move it to have atomic write",
            "                    os.replace(tmp_file_path, self.file_path)",
            "                    self.last_loaded = datetime.datetime.fromtimestamp(",
            "                        getmtime(self.file_path),",
            "                    )",
            "                    # after update, reset self._cache_modified to indicate it's up-to-update to avoid unnecessary flush",
            "                    self._cache_modified = False",
            "                    return True",
            "                except NameError:",
            "                    # note: when exiting python program, garbage collection will kick in",
            "                    # leading to `open` being garbage collected,",
            "                    # calling `open` raises NameError, we close the tmp file fd here to release the tmp file fd",
            "                    try:",
            "                        os.close(tmp_file)",
            "                    except OSError:",
            "                        pass",
            "                except OSError as o_err:",
            "                    raise PermissionError(",
            "                        o_err.errno,",
            "                        \"Cache folder is not writeable\",",
            "                        _dir,",
            "                    )",
            "                finally:",
            "                    if os.path.exists(tmp_file_path) and os.path.isfile(tmp_file_path):",
            "                        os.unlink(tmp_file_path)",
            "        except Timeout:",
            "            logger.debug(",
            "                f\"acquiring {self._file_lock_path} timed out, skipping saving...\"",
            "            )",
            "        except (AssertionError, RuntimeError):",
            "            raise",
            "        except Exception as e:",
            "            logger.debug(\"Fail to write cache to disk due to error: %s\", e)",
            "        return False",
            "",
            "    def save(self, load_first: bool = True) -> bool:",
            "        \"\"\"Save cache to disk if possible, returns whether it was able to save.",
            "",
            "        This is the public version of _save, it makes sure that all the",
            "        necessary locks are acquired.",
            "        \"\"\"",
            "        with self._lock:",
            "            return self._save(load_first)",
            "",
            "    def _save_if_should(self) -> bool:",
            "        \"\"\"Saves file to disk if necessary and returns whether it saved.",
            "",
            "        Uses self._should_save to decide whether to save.",
            "        \"\"\"",
            "        if self._should_save():",
            "            return self._save()",
            "        return False",
            "",
            "    def _load_if_should(self) -> bool:",
            "        \"\"\"Load file to disk if necessary and returns whether it loaded.",
            "",
            "        Uses self._should_load to decide whether to load.",
            "        \"\"\"",
            "        if self._should_load():",
            "            return self._load()",
            "        return False",
            "",
            "    def _should_save(self) -> bool:",
            "        \"\"\"Decide whether we should save.",
            "",
            "        This is a simple random number generator to randomize writes across processes",
            "        that are possibly saving the same values in this cache.",
            "        \"\"\"",
            "        return random.randint(0, self.MAX_RAND_INT) == 0",
            "",
            "    def _should_load(self) -> bool:",
            "        \"\"\"Decide whether we should load.",
            "",
            "        We should load if the file on disk has changed since we have last read it.",
            "        \"\"\"",
            "        if os.path.exists(self.file_path) and os.path.isfile(self.file_path):",
            "            if self.last_loaded is None:",
            "                return True",
            "            return (",
            "                datetime.datetime.fromtimestamp(",
            "                    getmtime(self.file_path),",
            "                )",
            "                > self.last_loaded",
            "            )",
            "        return False",
            "",
            "    def clear(self) -> None:",
            "        super().clear()",
            "        # This unlink prevents us from loading just before saving",
            "        with self._file_lock:",
            "            if os.path.exists(self.file_path) and os.path.isfile(self.file_path):",
            "                os.unlink(self.file_path)",
            "        # TODO: is this necessary?",
            "        with self._lock:",
            "            self._save(load_first=False, force_flush=True)",
            "",
            "    # Custom pickling implementation",
            "",
            "    def __getstate__(self) -> dict:",
            "        return {",
            "            k: v",
            "            for k, v in self.__dict__.items()",
            "            if k in SFDictFileCache._ATTRIBUTES_TO_PICKLE",
            "        }",
            "",
            "    def __setstate__(self, state: dict) -> None:",
            "        self.__dict__.update(state)",
            "        self._cache_modified = False",
            "        self._lock = Lock()",
            "        self._file_lock = FileLock(self._file_lock_path, timeout=self.file_timeout)",
            "",
            "    def _add_or_remove(self) -> None:",
            "        \"\"\"This function gets called when an element is added, or removed.",
            "",
            "        Note that while this function does not interact with lock, but it's only",
            "        called from contexts where the lock is already held.",
            "        \"\"\"",
            "        super()._add_or_remove()",
            "        self._cache_modified = True"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "448": [
                "SFDictFileCache",
                "__init__"
            ],
            "501": [
                "SFDictFileCache",
                "_load"
            ],
            "562": [
                "SFDictFileCache",
                "_save"
            ]
        },
        "addLocation": [
            "jupyter_server.base.handlers.APIHandler.content_security_policy",
            "src.snowflake.connector.cache.SFDictFileCache"
        ]
    },
    "src/snowflake/connector/encryption_util.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from .compat import PKCS5_OFFSET, PKCS5_PAD, PKCS5_UNPAD"
            },
            "2": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " from .constants import UTF8, EncryptionMetadata, MaterialDescriptor, kilobyte"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 20,
                "PatchRowcode": "+from .file_util import owner_rw_opener"
            },
            "4": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from .util_text import random_string"
            },
            "5": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " block_size = int(algorithms.AES.block_size / 8)  # in bytes"
            },
            "7": {
                "beforePatchRowNumber": 213,
                "afterPatchRowNumber": 214,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 214,
                "afterPatchRowNumber": 215,
                "PatchRowcode": "         logger.debug(\"encrypted file: %s, tmp file: %s\", in_filename, temp_output_file)"
            },
            "9": {
                "beforePatchRowNumber": 215,
                "afterPatchRowNumber": 216,
                "PatchRowcode": "         with open(in_filename, \"rb\") as infile:"
            },
            "10": {
                "beforePatchRowNumber": 216,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            with open(temp_output_file, \"wb\") as outfile:"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 217,
                "PatchRowcode": "+            with open(temp_output_file, \"wb\", opener=owner_rw_opener) as outfile:"
            },
            "12": {
                "beforePatchRowNumber": 217,
                "afterPatchRowNumber": 218,
                "PatchRowcode": "                 SnowflakeEncryptionUtil.decrypt_stream("
            },
            "13": {
                "beforePatchRowNumber": 218,
                "afterPatchRowNumber": 219,
                "PatchRowcode": "                     metadata, encryption_material, infile, outfile, chunk_size"
            },
            "14": {
                "beforePatchRowNumber": 219,
                "afterPatchRowNumber": 220,
                "PatchRowcode": "                 )"
            }
        },
        "frontPatchFile": [
            "#!/usr/bin/env python",
            "#",
            "# Copyright (c) 2012-2023 Snowflake Computing Inc. All rights reserved.",
            "#",
            "",
            "from __future__ import annotations",
            "",
            "import base64",
            "import json",
            "import os",
            "import tempfile",
            "from logging import getLogger",
            "from typing import IO, TYPE_CHECKING",
            "",
            "from cryptography.hazmat.backends import default_backend",
            "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes",
            "",
            "from .compat import PKCS5_OFFSET, PKCS5_PAD, PKCS5_UNPAD",
            "from .constants import UTF8, EncryptionMetadata, MaterialDescriptor, kilobyte",
            "from .util_text import random_string",
            "",
            "block_size = int(algorithms.AES.block_size / 8)  # in bytes",
            "",
            "if TYPE_CHECKING:  # pragma: no cover",
            "    from .storage_client import SnowflakeFileEncryptionMaterial",
            "",
            "logger = getLogger(__name__)",
            "",
            "",
            "def matdesc_to_unicode(matdesc: MaterialDescriptor) -> str:",
            "    \"\"\"Convert Material Descriptor to Unicode String.\"\"\"",
            "    return str(",
            "        json.dumps(",
            "            {",
            "                \"queryId\": matdesc.query_id,",
            "                \"smkId\": str(matdesc.smk_id),",
            "                \"keySize\": str(matdesc.key_size),",
            "            },",
            "            separators=(\",\", \":\"),",
            "        )",
            "    )",
            "",
            "",
            "class SnowflakeEncryptionUtil:",
            "    @staticmethod",
            "    def get_secure_random(byte_length: int) -> bytes:",
            "        return os.urandom(byte_length)",
            "",
            "    @staticmethod",
            "    def encrypt_stream(",
            "        encryption_material: SnowflakeFileEncryptionMaterial,",
            "        src: IO[bytes],",
            "        out: IO[bytes],",
            "        chunk_size: int = 64 * kilobyte,  # block_size * 4 * 1024,",
            "    ) -> EncryptionMetadata:",
            "        \"\"\"Reads content from src and write the encrypted content into out.",
            "",
            "        This function is sensitive to current position of src and out.",
            "        It does not seek to position 0 in neither stream objects before or after the encryption.",
            "",
            "        Args:",
            "            encryption_material: The encryption material for file.",
            "            src: The input stream.",
            "            out: The output stream.",
            "            chunk_size: The size of read chunks (Default value = block_size * 4 * 1024",
            "",
            "        Returns:",
            "            The encryption metadata.",
            "        \"\"\"",
            "        logger = getLogger(__name__)",
            "        decoded_key = base64.standard_b64decode(",
            "            encryption_material.query_stage_master_key",
            "        )",
            "        key_size = len(decoded_key)",
            "        logger.debug(\"key_size = %s\", key_size)",
            "",
            "        # Generate key for data encryption",
            "        iv_data = SnowflakeEncryptionUtil.get_secure_random(block_size)",
            "        file_key = SnowflakeEncryptionUtil.get_secure_random(key_size)",
            "        backend = default_backend()",
            "        cipher = Cipher(algorithms.AES(file_key), modes.CBC(iv_data), backend=backend)",
            "        encryptor = cipher.encryptor()",
            "",
            "        padded = False",
            "        while True:",
            "            chunk = src.read(chunk_size)",
            "            if len(chunk) == 0:",
            "                break",
            "            elif len(chunk) % block_size != 0:",
            "                chunk = PKCS5_PAD(chunk, block_size)",
            "                padded = True",
            "            out.write(encryptor.update(chunk))",
            "        if not padded:",
            "            out.write(encryptor.update(block_size * chr(block_size).encode(UTF8)))",
            "        out.write(encryptor.finalize())",
            "",
            "        # encrypt key with QRMK",
            "        cipher = Cipher(algorithms.AES(decoded_key), modes.ECB(), backend=backend)",
            "        encryptor = cipher.encryptor()",
            "        enc_kek = (",
            "            encryptor.update(PKCS5_PAD(file_key, block_size)) + encryptor.finalize()",
            "        )",
            "",
            "        mat_desc = MaterialDescriptor(",
            "            smk_id=encryption_material.smk_id,",
            "            query_id=encryption_material.query_id,",
            "            key_size=key_size * 8,",
            "        )",
            "        metadata = EncryptionMetadata(",
            "            key=base64.b64encode(enc_kek).decode(\"utf-8\"),",
            "            iv=base64.b64encode(iv_data).decode(\"utf-8\"),",
            "            matdesc=matdesc_to_unicode(mat_desc),",
            "        )",
            "        return metadata",
            "",
            "    @staticmethod",
            "    def encrypt_file(",
            "        encryption_material: SnowflakeFileEncryptionMaterial,",
            "        in_filename: str,",
            "        chunk_size: int = 64 * kilobyte,",
            "        tmp_dir: str | None = None,",
            "    ) -> tuple[EncryptionMetadata, str]:",
            "        \"\"\"Encrypts a file in a temporary directory.",
            "",
            "        Args:",
            "            encryption_material: The encryption material for file.",
            "            in_filename: The input file's name.",
            "            chunk_size: The size of read chunks (Default value = block_size * 4 * 1024).",
            "            tmp_dir: Temporary directory to use, optional (Default value = None).",
            "",
            "        Returns:",
            "            The encryption metadata and the encrypted file's location.",
            "        \"\"\"",
            "        logger = getLogger(__name__)",
            "        temp_output_fd, temp_output_file = tempfile.mkstemp(",
            "            text=False, dir=tmp_dir, prefix=os.path.basename(in_filename) + \"#\"",
            "        )",
            "        logger.debug(",
            "            \"unencrypted file: %s, temp file: %s, tmp_dir: %s\",",
            "            in_filename,",
            "            temp_output_file,",
            "            tmp_dir,",
            "        )",
            "        with open(in_filename, \"rb\") as infile:",
            "            with os.fdopen(temp_output_fd, \"wb\") as outfile:",
            "                metadata = SnowflakeEncryptionUtil.encrypt_stream(",
            "                    encryption_material, infile, outfile, chunk_size",
            "                )",
            "        return metadata, temp_output_file",
            "",
            "    @staticmethod",
            "    def decrypt_stream(",
            "        metadata: EncryptionMetadata,",
            "        encryption_material: SnowflakeFileEncryptionMaterial,",
            "        src: IO[bytes],",
            "        out: IO[bytes],",
            "        chunk_size: int = 64 * kilobyte,  # block_size * 4 * 1024,",
            "    ) -> None:",
            "        \"\"\"To read from `src` stream then decrypt to `out` stream.\"\"\"",
            "",
            "        key_base64 = metadata.key",
            "        iv_base64 = metadata.iv",
            "        decoded_key = base64.standard_b64decode(",
            "            encryption_material.query_stage_master_key",
            "        )",
            "        key_bytes = base64.standard_b64decode(key_base64)",
            "        iv_bytes = base64.standard_b64decode(iv_base64)",
            "",
            "        backend = default_backend()",
            "        cipher = Cipher(algorithms.AES(decoded_key), modes.ECB(), backend=backend)",
            "        decryptor = cipher.decryptor()",
            "        file_key = PKCS5_UNPAD(decryptor.update(key_bytes) + decryptor.finalize())",
            "        cipher = Cipher(algorithms.AES(file_key), modes.CBC(iv_bytes), backend=backend)",
            "        decryptor = cipher.decryptor()",
            "",
            "        last_decrypted_chunk = None",
            "        chunk = src.read(chunk_size)",
            "        while len(chunk) != 0:",
            "            if last_decrypted_chunk is not None:",
            "                out.write(last_decrypted_chunk)",
            "            d = decryptor.update(chunk)",
            "            last_decrypted_chunk = d",
            "            chunk = src.read(chunk_size)",
            "",
            "        if last_decrypted_chunk is not None:",
            "            offset = PKCS5_OFFSET(last_decrypted_chunk)",
            "            out.write(last_decrypted_chunk[:-offset])",
            "        out.write(decryptor.finalize())",
            "",
            "    @staticmethod",
            "    def decrypt_file(",
            "        metadata: EncryptionMetadata,",
            "        encryption_material: SnowflakeFileEncryptionMaterial,",
            "        in_filename: str,",
            "        chunk_size: int = 64 * kilobyte,",
            "        tmp_dir: str | None = None,",
            "    ) -> str:",
            "        \"\"\"Decrypts a file and stores the output in the temporary directory.",
            "",
            "        Args:",
            "            metadata: The file's metadata input.",
            "            encryption_material: The file's encryption material.",
            "            in_filename: The name of the input file.",
            "            chunk_size: The size of read chunks (Default value = block_size * 4 * 1024).",
            "            tmp_dir: Temporary directory to use, optional (Default value = None).",
            "",
            "        Returns:",
            "            The decrypted file's location.",
            "        \"\"\"",
            "        temp_output_file = f\"{os.path.basename(in_filename)}#{random_string()}\"",
            "        if tmp_dir:",
            "            temp_output_file = os.path.join(tmp_dir, temp_output_file)",
            "",
            "        logger.debug(\"encrypted file: %s, tmp file: %s\", in_filename, temp_output_file)",
            "        with open(in_filename, \"rb\") as infile:",
            "            with open(temp_output_file, \"wb\") as outfile:",
            "                SnowflakeEncryptionUtil.decrypt_stream(",
            "                    metadata, encryption_material, infile, outfile, chunk_size",
            "                )",
            "        return temp_output_file"
        ],
        "afterPatchFile": [
            "#!/usr/bin/env python",
            "#",
            "# Copyright (c) 2012-2023 Snowflake Computing Inc. All rights reserved.",
            "#",
            "",
            "from __future__ import annotations",
            "",
            "import base64",
            "import json",
            "import os",
            "import tempfile",
            "from logging import getLogger",
            "from typing import IO, TYPE_CHECKING",
            "",
            "from cryptography.hazmat.backends import default_backend",
            "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes",
            "",
            "from .compat import PKCS5_OFFSET, PKCS5_PAD, PKCS5_UNPAD",
            "from .constants import UTF8, EncryptionMetadata, MaterialDescriptor, kilobyte",
            "from .file_util import owner_rw_opener",
            "from .util_text import random_string",
            "",
            "block_size = int(algorithms.AES.block_size / 8)  # in bytes",
            "",
            "if TYPE_CHECKING:  # pragma: no cover",
            "    from .storage_client import SnowflakeFileEncryptionMaterial",
            "",
            "logger = getLogger(__name__)",
            "",
            "",
            "def matdesc_to_unicode(matdesc: MaterialDescriptor) -> str:",
            "    \"\"\"Convert Material Descriptor to Unicode String.\"\"\"",
            "    return str(",
            "        json.dumps(",
            "            {",
            "                \"queryId\": matdesc.query_id,",
            "                \"smkId\": str(matdesc.smk_id),",
            "                \"keySize\": str(matdesc.key_size),",
            "            },",
            "            separators=(\",\", \":\"),",
            "        )",
            "    )",
            "",
            "",
            "class SnowflakeEncryptionUtil:",
            "    @staticmethod",
            "    def get_secure_random(byte_length: int) -> bytes:",
            "        return os.urandom(byte_length)",
            "",
            "    @staticmethod",
            "    def encrypt_stream(",
            "        encryption_material: SnowflakeFileEncryptionMaterial,",
            "        src: IO[bytes],",
            "        out: IO[bytes],",
            "        chunk_size: int = 64 * kilobyte,  # block_size * 4 * 1024,",
            "    ) -> EncryptionMetadata:",
            "        \"\"\"Reads content from src and write the encrypted content into out.",
            "",
            "        This function is sensitive to current position of src and out.",
            "        It does not seek to position 0 in neither stream objects before or after the encryption.",
            "",
            "        Args:",
            "            encryption_material: The encryption material for file.",
            "            src: The input stream.",
            "            out: The output stream.",
            "            chunk_size: The size of read chunks (Default value = block_size * 4 * 1024",
            "",
            "        Returns:",
            "            The encryption metadata.",
            "        \"\"\"",
            "        logger = getLogger(__name__)",
            "        decoded_key = base64.standard_b64decode(",
            "            encryption_material.query_stage_master_key",
            "        )",
            "        key_size = len(decoded_key)",
            "        logger.debug(\"key_size = %s\", key_size)",
            "",
            "        # Generate key for data encryption",
            "        iv_data = SnowflakeEncryptionUtil.get_secure_random(block_size)",
            "        file_key = SnowflakeEncryptionUtil.get_secure_random(key_size)",
            "        backend = default_backend()",
            "        cipher = Cipher(algorithms.AES(file_key), modes.CBC(iv_data), backend=backend)",
            "        encryptor = cipher.encryptor()",
            "",
            "        padded = False",
            "        while True:",
            "            chunk = src.read(chunk_size)",
            "            if len(chunk) == 0:",
            "                break",
            "            elif len(chunk) % block_size != 0:",
            "                chunk = PKCS5_PAD(chunk, block_size)",
            "                padded = True",
            "            out.write(encryptor.update(chunk))",
            "        if not padded:",
            "            out.write(encryptor.update(block_size * chr(block_size).encode(UTF8)))",
            "        out.write(encryptor.finalize())",
            "",
            "        # encrypt key with QRMK",
            "        cipher = Cipher(algorithms.AES(decoded_key), modes.ECB(), backend=backend)",
            "        encryptor = cipher.encryptor()",
            "        enc_kek = (",
            "            encryptor.update(PKCS5_PAD(file_key, block_size)) + encryptor.finalize()",
            "        )",
            "",
            "        mat_desc = MaterialDescriptor(",
            "            smk_id=encryption_material.smk_id,",
            "            query_id=encryption_material.query_id,",
            "            key_size=key_size * 8,",
            "        )",
            "        metadata = EncryptionMetadata(",
            "            key=base64.b64encode(enc_kek).decode(\"utf-8\"),",
            "            iv=base64.b64encode(iv_data).decode(\"utf-8\"),",
            "            matdesc=matdesc_to_unicode(mat_desc),",
            "        )",
            "        return metadata",
            "",
            "    @staticmethod",
            "    def encrypt_file(",
            "        encryption_material: SnowflakeFileEncryptionMaterial,",
            "        in_filename: str,",
            "        chunk_size: int = 64 * kilobyte,",
            "        tmp_dir: str | None = None,",
            "    ) -> tuple[EncryptionMetadata, str]:",
            "        \"\"\"Encrypts a file in a temporary directory.",
            "",
            "        Args:",
            "            encryption_material: The encryption material for file.",
            "            in_filename: The input file's name.",
            "            chunk_size: The size of read chunks (Default value = block_size * 4 * 1024).",
            "            tmp_dir: Temporary directory to use, optional (Default value = None).",
            "",
            "        Returns:",
            "            The encryption metadata and the encrypted file's location.",
            "        \"\"\"",
            "        logger = getLogger(__name__)",
            "        temp_output_fd, temp_output_file = tempfile.mkstemp(",
            "            text=False, dir=tmp_dir, prefix=os.path.basename(in_filename) + \"#\"",
            "        )",
            "        logger.debug(",
            "            \"unencrypted file: %s, temp file: %s, tmp_dir: %s\",",
            "            in_filename,",
            "            temp_output_file,",
            "            tmp_dir,",
            "        )",
            "        with open(in_filename, \"rb\") as infile:",
            "            with os.fdopen(temp_output_fd, \"wb\") as outfile:",
            "                metadata = SnowflakeEncryptionUtil.encrypt_stream(",
            "                    encryption_material, infile, outfile, chunk_size",
            "                )",
            "        return metadata, temp_output_file",
            "",
            "    @staticmethod",
            "    def decrypt_stream(",
            "        metadata: EncryptionMetadata,",
            "        encryption_material: SnowflakeFileEncryptionMaterial,",
            "        src: IO[bytes],",
            "        out: IO[bytes],",
            "        chunk_size: int = 64 * kilobyte,  # block_size * 4 * 1024,",
            "    ) -> None:",
            "        \"\"\"To read from `src` stream then decrypt to `out` stream.\"\"\"",
            "",
            "        key_base64 = metadata.key",
            "        iv_base64 = metadata.iv",
            "        decoded_key = base64.standard_b64decode(",
            "            encryption_material.query_stage_master_key",
            "        )",
            "        key_bytes = base64.standard_b64decode(key_base64)",
            "        iv_bytes = base64.standard_b64decode(iv_base64)",
            "",
            "        backend = default_backend()",
            "        cipher = Cipher(algorithms.AES(decoded_key), modes.ECB(), backend=backend)",
            "        decryptor = cipher.decryptor()",
            "        file_key = PKCS5_UNPAD(decryptor.update(key_bytes) + decryptor.finalize())",
            "        cipher = Cipher(algorithms.AES(file_key), modes.CBC(iv_bytes), backend=backend)",
            "        decryptor = cipher.decryptor()",
            "",
            "        last_decrypted_chunk = None",
            "        chunk = src.read(chunk_size)",
            "        while len(chunk) != 0:",
            "            if last_decrypted_chunk is not None:",
            "                out.write(last_decrypted_chunk)",
            "            d = decryptor.update(chunk)",
            "            last_decrypted_chunk = d",
            "            chunk = src.read(chunk_size)",
            "",
            "        if last_decrypted_chunk is not None:",
            "            offset = PKCS5_OFFSET(last_decrypted_chunk)",
            "            out.write(last_decrypted_chunk[:-offset])",
            "        out.write(decryptor.finalize())",
            "",
            "    @staticmethod",
            "    def decrypt_file(",
            "        metadata: EncryptionMetadata,",
            "        encryption_material: SnowflakeFileEncryptionMaterial,",
            "        in_filename: str,",
            "        chunk_size: int = 64 * kilobyte,",
            "        tmp_dir: str | None = None,",
            "    ) -> str:",
            "        \"\"\"Decrypts a file and stores the output in the temporary directory.",
            "",
            "        Args:",
            "            metadata: The file's metadata input.",
            "            encryption_material: The file's encryption material.",
            "            in_filename: The name of the input file.",
            "            chunk_size: The size of read chunks (Default value = block_size * 4 * 1024).",
            "            tmp_dir: Temporary directory to use, optional (Default value = None).",
            "",
            "        Returns:",
            "            The decrypted file's location.",
            "        \"\"\"",
            "        temp_output_file = f\"{os.path.basename(in_filename)}#{random_string()}\"",
            "        if tmp_dir:",
            "            temp_output_file = os.path.join(tmp_dir, temp_output_file)",
            "",
            "        logger.debug(\"encrypted file: %s, tmp file: %s\", in_filename, temp_output_file)",
            "        with open(in_filename, \"rb\") as infile:",
            "            with open(temp_output_file, \"wb\", opener=owner_rw_opener) as outfile:",
            "                SnowflakeEncryptionUtil.decrypt_stream(",
            "                    metadata, encryption_material, infile, outfile, chunk_size",
            "                )",
            "        return temp_output_file"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "216": [
                "SnowflakeEncryptionUtil",
                "decrypt_file"
            ]
        },
        "addLocation": []
    },
    "src/snowflake/connector/file_util.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " logger = getLogger(__name__)"
            },
            "1": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 24,
                "PatchRowcode": "+def owner_rw_opener(path, flags) -> int:"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+    return os.open(path, flags, mode=0o600)"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 27,
                "PatchRowcode": "+"
            },
            "7": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " class SnowflakeFileUtil:"
            },
            "8": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 29,
                "PatchRowcode": "     @staticmethod"
            },
            "9": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 30,
                "PatchRowcode": "     def get_digest_and_size(src: IO[bytes]) -> tuple[str, int]:"
            }
        },
        "frontPatchFile": [
            "#",
            "# Copyright (c) 2012-2023 Snowflake Computing Inc. All rights reserved.",
            "#",
            "",
            "from __future__ import annotations",
            "",
            "import base64",
            "import gzip",
            "import os",
            "import shutil",
            "import struct",
            "from io import BytesIO",
            "from logging import getLogger",
            "from typing import IO",
            "",
            "from cryptography.hazmat.backends import default_backend",
            "from cryptography.hazmat.primitives import hashes",
            "",
            "from .constants import UTF8, kilobyte",
            "",
            "logger = getLogger(__name__)",
            "",
            "",
            "class SnowflakeFileUtil:",
            "    @staticmethod",
            "    def get_digest_and_size(src: IO[bytes]) -> tuple[str, int]:",
            "        \"\"\"Gets stream digest and size.",
            "",
            "        Args:",
            "            src: The input stream.",
            "",
            "        Returns:",
            "            Tuple of src's digest and src's size in bytes.",
            "        \"\"\"",
            "        CHUNK_SIZE = 64 * kilobyte",
            "        backend = default_backend()",
            "        chosen_hash = hashes.SHA256()",
            "        hasher = hashes.Hash(chosen_hash, backend)",
            "        while True:",
            "            chunk = src.read(CHUNK_SIZE)",
            "            if chunk == b\"\":",
            "                break",
            "            hasher.update(chunk)",
            "",
            "        digest = base64.standard_b64encode(hasher.finalize()).decode(UTF8)",
            "",
            "        size = src.tell()",
            "        src.seek(0)",
            "        return digest, size",
            "",
            "    @staticmethod",
            "    def compress_with_gzip_from_stream(src_stream: IO[bytes]) -> tuple[IO[bytes], int]:",
            "        \"\"\"Compresses a stream of bytes with GZIP.",
            "",
            "        Args:",
            "            src_stream: bytes stream",
            "",
            "        Returns:",
            "            A tuple of byte stream and size.",
            "        \"\"\"",
            "        compressed_data = gzip.compress(src_stream.read())",
            "        src_stream.seek(0)",
            "        return BytesIO(compressed_data), len(compressed_data)",
            "",
            "    @staticmethod",
            "    def compress_file_with_gzip(file_name: str, tmp_dir: str) -> tuple[str, int]:",
            "        \"\"\"Compresses a file with GZIP.",
            "",
            "        Args:",
            "            file_name: Local path to file to be compressed.",
            "            tmp_dir: Temporary directory where an GZIP file will be created.",
            "",
            "        Returns:",
            "            A tuple of gzip file name and size.",
            "        \"\"\"",
            "        base_name = os.path.basename(file_name)",
            "        gzip_file_name = os.path.join(tmp_dir, base_name + \"_c.gz\")",
            "        logger.debug(\"gzip file: %s, original file: %s\", gzip_file_name, file_name)",
            "        with open(file_name, \"rb\") as fr:",
            "            with gzip.GzipFile(gzip_file_name, \"wb\") as fw:",
            "                shutil.copyfileobj(fr, fw, length=64 * kilobyte)",
            "        SnowflakeFileUtil.normalize_gzip_header(gzip_file_name)",
            "",
            "        statinfo = os.stat(gzip_file_name)",
            "        return gzip_file_name, statinfo.st_size",
            "",
            "    @staticmethod",
            "    def normalize_gzip_header(gzip_file_name: str) -> None:",
            "        \"\"\"Normalizes GZIP file header.",
            "",
            "        For consistent file digest, this removes creation timestamp and file name from the header.",
            "        For more information see http://www.zlib.org/rfc-gzip.html#file-format",
            "",
            "        Args:",
            "            gzip_file_name: Local path of gzip file.",
            "        \"\"\"",
            "        with open(gzip_file_name, \"r+b\") as f:",
            "            # reset the timestamp in gzip header",
            "            f.seek(3, 0)",
            "            # Read flags bit",
            "            flag_byte = f.read(1)",
            "            flags = struct.unpack(\"B\", flag_byte)[0]",
            "            f.seek(4, 0)",
            "            f.write(struct.pack(\"<L\", 0))",
            "            # Reset the file name in gzip header if included",
            "            if flags & 8:",
            "                f.seek(10, 0)",
            "                # Skip through xlen bytes and length if included",
            "                if flags & 4:",
            "                    xlen_bytes = f.read(2)",
            "                    xlen = struct.unpack(\"<H\", xlen_bytes)[0]",
            "                    f.seek(10 + 2 + xlen)",
            "                byte = f.read(1)",
            "                while byte:",
            "                    value = struct.unpack(\"B\", byte)[0]",
            "                    # logger.debug('ch=%s, byte=%s', value, byte)",
            "                    if value == 0:",
            "                        break",
            "                    f.seek(-1, 1)  # current_pos - 1",
            "                    f.write(struct.pack(\"B\", 0x20))  # replace with a space",
            "                    byte = f.read(1)",
            "",
            "    @staticmethod",
            "    def get_digest_and_size_for_stream(src_stream: IO[bytes]) -> tuple[str, int]:",
            "        \"\"\"Gets stream digest and size.",
            "",
            "        Args:",
            "            src_stream: The input source stream.",
            "",
            "        Returns:",
            "            Tuple of src_stream's digest and src_stream's size in bytes.",
            "        \"\"\"",
            "        digest, size = SnowflakeFileUtil.get_digest_and_size(src_stream)",
            "        logger.debug(\"getting digest and size for stream: %s, %s\", digest, size)",
            "        return digest, size",
            "",
            "    @staticmethod",
            "    def get_digest_and_size_for_file(file_name: str) -> tuple[str, int]:",
            "        \"\"\"Gets file digest and size.",
            "",
            "        Args:",
            "            file_name: Local path to a file.",
            "",
            "        Returns:",
            "            Tuple of file's digest and file size in bytes.",
            "        \"\"\"",
            "        digest, size = None, None",
            "        with open(file_name, \"rb\") as src:",
            "            digest, size = SnowflakeFileUtil.get_digest_and_size(src)",
            "        logger.debug(",
            "            \"getting digest and size: %s, %s, file=%s\", digest, size, file_name",
            "        )",
            "        return digest, size"
        ],
        "afterPatchFile": [
            "#",
            "# Copyright (c) 2012-2023 Snowflake Computing Inc. All rights reserved.",
            "#",
            "",
            "from __future__ import annotations",
            "",
            "import base64",
            "import gzip",
            "import os",
            "import shutil",
            "import struct",
            "from io import BytesIO",
            "from logging import getLogger",
            "from typing import IO",
            "",
            "from cryptography.hazmat.backends import default_backend",
            "from cryptography.hazmat.primitives import hashes",
            "",
            "from .constants import UTF8, kilobyte",
            "",
            "logger = getLogger(__name__)",
            "",
            "",
            "def owner_rw_opener(path, flags) -> int:",
            "    return os.open(path, flags, mode=0o600)",
            "",
            "",
            "class SnowflakeFileUtil:",
            "    @staticmethod",
            "    def get_digest_and_size(src: IO[bytes]) -> tuple[str, int]:",
            "        \"\"\"Gets stream digest and size.",
            "",
            "        Args:",
            "            src: The input stream.",
            "",
            "        Returns:",
            "            Tuple of src's digest and src's size in bytes.",
            "        \"\"\"",
            "        CHUNK_SIZE = 64 * kilobyte",
            "        backend = default_backend()",
            "        chosen_hash = hashes.SHA256()",
            "        hasher = hashes.Hash(chosen_hash, backend)",
            "        while True:",
            "            chunk = src.read(CHUNK_SIZE)",
            "            if chunk == b\"\":",
            "                break",
            "            hasher.update(chunk)",
            "",
            "        digest = base64.standard_b64encode(hasher.finalize()).decode(UTF8)",
            "",
            "        size = src.tell()",
            "        src.seek(0)",
            "        return digest, size",
            "",
            "    @staticmethod",
            "    def compress_with_gzip_from_stream(src_stream: IO[bytes]) -> tuple[IO[bytes], int]:",
            "        \"\"\"Compresses a stream of bytes with GZIP.",
            "",
            "        Args:",
            "            src_stream: bytes stream",
            "",
            "        Returns:",
            "            A tuple of byte stream and size.",
            "        \"\"\"",
            "        compressed_data = gzip.compress(src_stream.read())",
            "        src_stream.seek(0)",
            "        return BytesIO(compressed_data), len(compressed_data)",
            "",
            "    @staticmethod",
            "    def compress_file_with_gzip(file_name: str, tmp_dir: str) -> tuple[str, int]:",
            "        \"\"\"Compresses a file with GZIP.",
            "",
            "        Args:",
            "            file_name: Local path to file to be compressed.",
            "            tmp_dir: Temporary directory where an GZIP file will be created.",
            "",
            "        Returns:",
            "            A tuple of gzip file name and size.",
            "        \"\"\"",
            "        base_name = os.path.basename(file_name)",
            "        gzip_file_name = os.path.join(tmp_dir, base_name + \"_c.gz\")",
            "        logger.debug(\"gzip file: %s, original file: %s\", gzip_file_name, file_name)",
            "        with open(file_name, \"rb\") as fr:",
            "            with gzip.GzipFile(gzip_file_name, \"wb\") as fw:",
            "                shutil.copyfileobj(fr, fw, length=64 * kilobyte)",
            "        SnowflakeFileUtil.normalize_gzip_header(gzip_file_name)",
            "",
            "        statinfo = os.stat(gzip_file_name)",
            "        return gzip_file_name, statinfo.st_size",
            "",
            "    @staticmethod",
            "    def normalize_gzip_header(gzip_file_name: str) -> None:",
            "        \"\"\"Normalizes GZIP file header.",
            "",
            "        For consistent file digest, this removes creation timestamp and file name from the header.",
            "        For more information see http://www.zlib.org/rfc-gzip.html#file-format",
            "",
            "        Args:",
            "            gzip_file_name: Local path of gzip file.",
            "        \"\"\"",
            "        with open(gzip_file_name, \"r+b\") as f:",
            "            # reset the timestamp in gzip header",
            "            f.seek(3, 0)",
            "            # Read flags bit",
            "            flag_byte = f.read(1)",
            "            flags = struct.unpack(\"B\", flag_byte)[0]",
            "            f.seek(4, 0)",
            "            f.write(struct.pack(\"<L\", 0))",
            "            # Reset the file name in gzip header if included",
            "            if flags & 8:",
            "                f.seek(10, 0)",
            "                # Skip through xlen bytes and length if included",
            "                if flags & 4:",
            "                    xlen_bytes = f.read(2)",
            "                    xlen = struct.unpack(\"<H\", xlen_bytes)[0]",
            "                    f.seek(10 + 2 + xlen)",
            "                byte = f.read(1)",
            "                while byte:",
            "                    value = struct.unpack(\"B\", byte)[0]",
            "                    # logger.debug('ch=%s, byte=%s', value, byte)",
            "                    if value == 0:",
            "                        break",
            "                    f.seek(-1, 1)  # current_pos - 1",
            "                    f.write(struct.pack(\"B\", 0x20))  # replace with a space",
            "                    byte = f.read(1)",
            "",
            "    @staticmethod",
            "    def get_digest_and_size_for_stream(src_stream: IO[bytes]) -> tuple[str, int]:",
            "        \"\"\"Gets stream digest and size.",
            "",
            "        Args:",
            "            src_stream: The input source stream.",
            "",
            "        Returns:",
            "            Tuple of src_stream's digest and src_stream's size in bytes.",
            "        \"\"\"",
            "        digest, size = SnowflakeFileUtil.get_digest_and_size(src_stream)",
            "        logger.debug(\"getting digest and size for stream: %s, %s\", digest, size)",
            "        return digest, size",
            "",
            "    @staticmethod",
            "    def get_digest_and_size_for_file(file_name: str) -> tuple[str, int]:",
            "        \"\"\"Gets file digest and size.",
            "",
            "        Args:",
            "            file_name: Local path to a file.",
            "",
            "        Returns:",
            "            Tuple of file's digest and file size in bytes.",
            "        \"\"\"",
            "        digest, size = None, None",
            "        with open(file_name, \"rb\") as src:",
            "            digest, size = SnowflakeFileUtil.get_digest_and_size(src)",
            "        logger.debug(",
            "            \"getting digest and size: %s, %s, file=%s\", digest, size, file_name",
            "        )",
            "        return digest, size"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "src/snowflake/connector/ocsp_snowflake.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " from __future__ import annotations"
            },
            "1": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " import codecs"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 9,
                "PatchRowcode": "+import importlib"
            },
            "4": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " import json"
            },
            "5": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " import os"
            },
            "6": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " import platform"
            },
            "7": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " from asn1crypto.x509 import Certificate"
            },
            "8": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " from OpenSSL.SSL import Connection"
            },
            "9": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+from snowflake.connector import SNOWFLAKE_CONNECTOR_VERSION"
            },
            "11": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " from snowflake.connector.compat import OK, urlsplit, urlunparse"
            },
            "12": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " from snowflake.connector.constants import HTTP_HEADER_USER_AGENT"
            },
            "13": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 37,
                "PatchRowcode": " from snowflake.connector.errorcode import ("
            },
            "14": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": 60,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 61,
                "PatchRowcode": " from . import constants"
            },
            "16": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 62,
                "PatchRowcode": " from .backoff_policies import exponential_backoff"
            },
            "17": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from .cache import SFDictCache, SFDictFileCache"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 63,
                "PatchRowcode": "+from .cache import CacheEntry, SFDictCache, SFDictFileCache"
            },
            "19": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 64,
                "PatchRowcode": " from .telemetry import TelemetryField, generate_telemetry_data_dict"
            },
            "20": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 65,
                "PatchRowcode": " from .url_util import extract_top_level_domain_from_hostname, url_encode_str"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+from .util_text import _base64_bytes_to_str"
            },
            "22": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 67,
                "PatchRowcode": " "
            },
            "23": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 68,
                "PatchRowcode": " "
            },
            "24": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 69,
                "PatchRowcode": " class OCSPResponseValidationResult(NamedTuple):"
            },
            "25": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 75,
                "PatchRowcode": "     ts: int | None = None"
            },
            "26": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 76,
                "PatchRowcode": "     validated: bool = False"
            },
            "27": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 77,
                "PatchRowcode": " "
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+    def _serialize(self):"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+        def serialize_exception(exc):"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 80,
                "PatchRowcode": "+            # serialization exception is not supported for all exceptions"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+            # in the ocsp_snowflake.py, most exceptions are RevocationCheckError which is easy to serialize."
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 82,
                "PatchRowcode": "+            # however, it would require non-trivial effort to serialize other exceptions especially 3rd part errors"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 83,
                "PatchRowcode": "+            # as there can be un-serializable members and nondeterministic constructor arguments."
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 84,
                "PatchRowcode": "+            # here we do a general best efforts serialization for other exceptions recording only the error message."
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 85,
                "PatchRowcode": "+            if not exc:"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+                return None"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+            exc_type = type(exc)"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 89,
                "PatchRowcode": "+            ret = {\"class\": exc_type.__name__, \"module\": exc_type.__module__}"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 90,
                "PatchRowcode": "+            if isinstance(exc, RevocationCheckError):"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 91,
                "PatchRowcode": "+                ret.update({\"errno\": exc.errno, \"msg\": exc.raw_msg})"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 92,
                "PatchRowcode": "+            else:"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+                ret.update({\"msg\": str(exc)})"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+            return ret"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+        return json.dumps("
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+            {"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+                \"exception\": serialize_exception(self.exception),"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+                \"issuer\": ("
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+                    _base64_bytes_to_str(self.issuer.dump()) if self.issuer else None"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+                ),"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+                \"subject\": ("
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+                    _base64_bytes_to_str(self.subject.dump()) if self.subject else None"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 104,
                "PatchRowcode": "+                ),"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 105,
                "PatchRowcode": "+                \"cert_id\": ("
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+                    _base64_bytes_to_str(self.cert_id.dump()) if self.cert_id else None"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+                ),"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 108,
                "PatchRowcode": "+                \"ocsp_response\": _base64_bytes_to_str(self.ocsp_response),"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+                \"ts\": self.ts,"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 110,
                "PatchRowcode": "+                \"validated\": self.validated,"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 111,
                "PatchRowcode": "+            }"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 112,
                "PatchRowcode": "+        )"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 113,
                "PatchRowcode": "+"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 114,
                "PatchRowcode": "+    @classmethod"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 115,
                "PatchRowcode": "+    def _deserialize(cls, json_str: str) -> OCSPResponseValidationResult:"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+        json_obj = json.loads(json_str)"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 117,
                "PatchRowcode": "+"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 118,
                "PatchRowcode": "+        def deserialize_exception(exception_dict: dict | None) -> Exception | None:"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 119,
                "PatchRowcode": "+            # as pointed out in the serialization method, here we do the best effort deserialization"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 120,
                "PatchRowcode": "+            # for non-RevocationCheckError exceptions. If we can not deserialize the exception, we will"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 121,
                "PatchRowcode": "+            # return a RevocationCheckError with a message indicating the failure."
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 122,
                "PatchRowcode": "+            if not exception_dict:"
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 123,
                "PatchRowcode": "+                return"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 124,
                "PatchRowcode": "+            exc_class = exception_dict.get(\"class\")"
            },
            "75": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 125,
                "PatchRowcode": "+            exc_module = exception_dict.get(\"module\")"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 126,
                "PatchRowcode": "+            try:"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 127,
                "PatchRowcode": "+                if ("
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 128,
                "PatchRowcode": "+                    exc_class == \"RevocationCheckError\""
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 129,
                "PatchRowcode": "+                    and exc_module == \"snowflake.connector.errors\""
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 130,
                "PatchRowcode": "+                ):"
            },
            "81": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 131,
                "PatchRowcode": "+                    return RevocationCheckError("
            },
            "82": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 132,
                "PatchRowcode": "+                        msg=exception_dict[\"msg\"],"
            },
            "83": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 133,
                "PatchRowcode": "+                        errno=exception_dict[\"errno\"],"
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 134,
                "PatchRowcode": "+                    )"
            },
            "85": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 135,
                "PatchRowcode": "+                else:"
            },
            "86": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 136,
                "PatchRowcode": "+                    module = importlib.import_module(exc_module)"
            },
            "87": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 137,
                "PatchRowcode": "+                    exc_cls = getattr(module, exc_class)"
            },
            "88": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 138,
                "PatchRowcode": "+                    return exc_cls(exception_dict[\"msg\"])"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 139,
                "PatchRowcode": "+            except Exception as deserialize_exc:"
            },
            "90": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 140,
                "PatchRowcode": "+                logger.debug("
            },
            "91": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 141,
                "PatchRowcode": "+                    f\"hitting error {str(deserialize_exc)} while deserializing exception,\""
            },
            "92": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 142,
                "PatchRowcode": "+                    f\" the original error error class and message are {exc_class} and {exception_dict['msg']}\""
            },
            "93": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 143,
                "PatchRowcode": "+                )"
            },
            "94": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 144,
                "PatchRowcode": "+                return RevocationCheckError("
            },
            "95": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 145,
                "PatchRowcode": "+                    f\"Got error {str(deserialize_exc)} while deserializing ocsp cache, please try \""
            },
            "96": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 146,
                "PatchRowcode": "+                    f\"cleaning up the \""
            },
            "97": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 147,
                "PatchRowcode": "+                    f\"OCSP cache under directory {OCSP_RESPONSE_VALIDATION_CACHE.file_path}\","
            },
            "98": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 148,
                "PatchRowcode": "+                    errno=ER_OCSP_RESPONSE_LOAD_FAILURE,"
            },
            "99": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 149,
                "PatchRowcode": "+                )"
            },
            "100": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 150,
                "PatchRowcode": "+"
            },
            "101": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 151,
                "PatchRowcode": "+        return OCSPResponseValidationResult("
            },
            "102": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 152,
                "PatchRowcode": "+            exception=deserialize_exception(json_obj.get(\"exception\")),"
            },
            "103": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 153,
                "PatchRowcode": "+            issuer=("
            },
            "104": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 154,
                "PatchRowcode": "+                Certificate.load(b64decode(json_obj.get(\"issuer\")))"
            },
            "105": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 155,
                "PatchRowcode": "+                if json_obj.get(\"issuer\")"
            },
            "106": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 156,
                "PatchRowcode": "+                else None"
            },
            "107": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 157,
                "PatchRowcode": "+            ),"
            },
            "108": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 158,
                "PatchRowcode": "+            subject=("
            },
            "109": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 159,
                "PatchRowcode": "+                Certificate.load(b64decode(json_obj.get(\"subject\")))"
            },
            "110": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 160,
                "PatchRowcode": "+                if json_obj.get(\"subject\")"
            },
            "111": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 161,
                "PatchRowcode": "+                else None"
            },
            "112": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 162,
                "PatchRowcode": "+            ),"
            },
            "113": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 163,
                "PatchRowcode": "+            cert_id=("
            },
            "114": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 164,
                "PatchRowcode": "+                CertId.load(b64decode(json_obj.get(\"cert_id\")))"
            },
            "115": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 165,
                "PatchRowcode": "+                if json_obj.get(\"cert_id\")"
            },
            "116": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 166,
                "PatchRowcode": "+                else None"
            },
            "117": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 167,
                "PatchRowcode": "+            ),"
            },
            "118": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 168,
                "PatchRowcode": "+            ocsp_response=("
            },
            "119": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 169,
                "PatchRowcode": "+                b64decode(json_obj.get(\"ocsp_response\"))"
            },
            "120": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 170,
                "PatchRowcode": "+                if json_obj.get(\"ocsp_response\")"
            },
            "121": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 171,
                "PatchRowcode": "+                else None"
            },
            "122": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 172,
                "PatchRowcode": "+            ),"
            },
            "123": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 173,
                "PatchRowcode": "+            ts=json_obj.get(\"ts\"),"
            },
            "124": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 174,
                "PatchRowcode": "+            validated=json_obj.get(\"validated\"),"
            },
            "125": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 175,
                "PatchRowcode": "+        )"
            },
            "126": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 176,
                "PatchRowcode": "+"
            },
            "127": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 177,
                "PatchRowcode": "+"
            },
            "128": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 178,
                "PatchRowcode": "+class _OCSPResponseValidationResultCache(SFDictFileCache):"
            },
            "129": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 179,
                "PatchRowcode": "+    def _serialize(self) -> bytes:"
            },
            "130": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 180,
                "PatchRowcode": "+        entries = {"
            },
            "131": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 181,
                "PatchRowcode": "+            ("
            },
            "132": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 182,
                "PatchRowcode": "+                _base64_bytes_to_str(k[0]),"
            },
            "133": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 183,
                "PatchRowcode": "+                _base64_bytes_to_str(k[1]),"
            },
            "134": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 184,
                "PatchRowcode": "+                _base64_bytes_to_str(k[2]),"
            },
            "135": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 185,
                "PatchRowcode": "+            ): (v.expiry.isoformat(), v.entry._serialize())"
            },
            "136": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 186,
                "PatchRowcode": "+            for k, v in self._cache.items()"
            },
            "137": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 187,
                "PatchRowcode": "+        }"
            },
            "138": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 188,
                "PatchRowcode": "+"
            },
            "139": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 189,
                "PatchRowcode": "+        return json.dumps("
            },
            "140": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 190,
                "PatchRowcode": "+            {"
            },
            "141": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 191,
                "PatchRowcode": "+                \"cache_keys\": list(entries.keys()),"
            },
            "142": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 192,
                "PatchRowcode": "+                \"cache_items\": list(entries.values()),"
            },
            "143": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 193,
                "PatchRowcode": "+                \"entry_lifetime\": self._entry_lifetime.total_seconds(),"
            },
            "144": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 194,
                "PatchRowcode": "+                \"file_path\": str(self.file_path),"
            },
            "145": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 195,
                "PatchRowcode": "+                \"file_timeout\": self.file_timeout,"
            },
            "146": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 196,
                "PatchRowcode": "+                \"last_loaded\": ("
            },
            "147": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 197,
                "PatchRowcode": "+                    self.last_loaded.isoformat() if self.last_loaded else None"
            },
            "148": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 198,
                "PatchRowcode": "+                ),"
            },
            "149": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 199,
                "PatchRowcode": "+                \"telemetry\": self.telemetry,"
            },
            "150": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 200,
                "PatchRowcode": "+                \"connector_version\": SNOWFLAKE_CONNECTOR_VERSION,  # reserved for schema version control"
            },
            "151": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 201,
                "PatchRowcode": "+            }"
            },
            "152": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 202,
                "PatchRowcode": "+        ).encode()"
            },
            "153": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 203,
                "PatchRowcode": "+"
            },
            "154": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 204,
                "PatchRowcode": "+    @classmethod"
            },
            "155": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 205,
                "PatchRowcode": "+    def _deserialize(cls, opened_fd) -> _OCSPResponseValidationResultCache:"
            },
            "156": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 206,
                "PatchRowcode": "+        data = json.loads(opened_fd.read().decode())"
            },
            "157": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 207,
                "PatchRowcode": "+        cache_instance = cls("
            },
            "158": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 208,
                "PatchRowcode": "+            file_path=data[\"file_path\"],"
            },
            "159": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 209,
                "PatchRowcode": "+            entry_lifetime=int(data[\"entry_lifetime\"]),"
            },
            "160": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 210,
                "PatchRowcode": "+            file_timeout=data[\"file_timeout\"],"
            },
            "161": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 211,
                "PatchRowcode": "+            load_if_file_exists=False,"
            },
            "162": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 212,
                "PatchRowcode": "+        )"
            },
            "163": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 213,
                "PatchRowcode": "+        cache_instance.file_path = os.path.expanduser(data[\"file_path\"])"
            },
            "164": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 214,
                "PatchRowcode": "+        cache_instance.telemetry = data[\"telemetry\"]"
            },
            "165": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 215,
                "PatchRowcode": "+        cache_instance.last_loaded = ("
            },
            "166": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 216,
                "PatchRowcode": "+            datetime.fromisoformat(data[\"last_loaded\"]) if data[\"last_loaded\"] else None"
            },
            "167": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 217,
                "PatchRowcode": "+        )"
            },
            "168": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 218,
                "PatchRowcode": "+        for k, v in zip(data[\"cache_keys\"], data[\"cache_items\"]):"
            },
            "169": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 219,
                "PatchRowcode": "+            cache_instance._cache["
            },
            "170": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 220,
                "PatchRowcode": "+                (b64decode(k[0]), b64decode(k[1]), b64decode(k[2]))"
            },
            "171": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 221,
                "PatchRowcode": "+            ] = CacheEntry("
            },
            "172": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 222,
                "PatchRowcode": "+                datetime.fromisoformat(v[0]),"
            },
            "173": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 223,
                "PatchRowcode": "+                OCSPResponseValidationResult._deserialize(v[1]),"
            },
            "174": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 224,
                "PatchRowcode": "+            )"
            },
            "175": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 225,
                "PatchRowcode": "+        return cache_instance"
            },
            "176": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 226,
                "PatchRowcode": "+"
            },
            "177": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 227,
                "PatchRowcode": " "
            },
            "178": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 228,
                "PatchRowcode": " try:"
            },
            "179": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 229,
                "PatchRowcode": "     OCSP_RESPONSE_VALIDATION_CACHE: SFDictFileCache["
            },
            "180": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 230,
                "PatchRowcode": "         tuple[bytes, bytes, bytes],"
            },
            "181": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 231,
                "PatchRowcode": "         OCSPResponseValidationResult,"
            },
            "182": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    ] = SFDictFileCache("
            },
            "183": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 232,
                "PatchRowcode": "+    ] = _OCSPResponseValidationResultCache("
            },
            "184": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 233,
                "PatchRowcode": "         entry_lifetime=constants.DAY_IN_SECONDS,"
            },
            "185": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 234,
                "PatchRowcode": "         file_path={"
            },
            "186": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 235,
                "PatchRowcode": "             \"linux\": os.path.join("
            },
            "187": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"~\", \".cache\", \"snowflake\", \"ocsp_response_validation_cache\""
            },
            "188": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 236,
                "PatchRowcode": "+                \"~\", \".cache\", \"snowflake\", \"ocsp_response_validation_cache.json\""
            },
            "189": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 237,
                "PatchRowcode": "             ),"
            },
            "190": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 238,
                "PatchRowcode": "             \"darwin\": os.path.join("
            },
            "191": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"~\", \"Library\", \"Caches\", \"Snowflake\", \"ocsp_response_validation_cache\""
            },
            "192": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 239,
                "PatchRowcode": "+                \"~\","
            },
            "193": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 240,
                "PatchRowcode": "+                \"Library\","
            },
            "194": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 241,
                "PatchRowcode": "+                \"Caches\","
            },
            "195": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 242,
                "PatchRowcode": "+                \"Snowflake\","
            },
            "196": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 243,
                "PatchRowcode": "+                \"ocsp_response_validation_cache.json\","
            },
            "197": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": 244,
                "PatchRowcode": "             ),"
            },
            "198": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": 245,
                "PatchRowcode": "             \"windows\": os.path.join("
            },
            "199": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 246,
                "PatchRowcode": "                 \"~\","
            },
            "200": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 247,
                "PatchRowcode": "                 \"AppData\","
            },
            "201": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 248,
                "PatchRowcode": "                 \"Local\","
            },
            "202": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 249,
                "PatchRowcode": "                 \"Snowflake\","
            },
            "203": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": 250,
                "PatchRowcode": "                 \"Caches\","
            },
            "204": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"ocsp_response_validation_cache\","
            },
            "205": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 251,
                "PatchRowcode": "+                \"ocsp_response_validation_cache.json\","
            },
            "206": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 252,
                "PatchRowcode": "             ),"
            },
            "207": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 253,
                "PatchRowcode": "         },"
            },
            "208": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": 254,
                "PatchRowcode": "     )"
            }
        },
        "frontPatchFile": [
            "#!/usr/bin/env python",
            "#",
            "# Copyright (c) 2012-2023 Snowflake Computing Inc. All rights reserved.",
            "#",
            "",
            "from __future__ import annotations",
            "",
            "import codecs",
            "import json",
            "import os",
            "import platform",
            "import re",
            "import sys",
            "import tempfile",
            "import time",
            "from base64 import b64decode, b64encode",
            "from datetime import datetime, timezone",
            "from logging import getLogger",
            "from os import environ, path",
            "from os.path import expanduser",
            "from threading import Lock, RLock",
            "from time import gmtime, strftime",
            "from typing import Any, NamedTuple",
            "",
            "# We use regular requests and urlib3 when we reach out to do OCSP checks, basically in this very narrow",
            "# part of the code where we want to call out to check for revoked certificates,",
            "# we don't want to use our hardened version of requests.",
            "import requests as generic_requests",
            "from asn1crypto.ocsp import CertId, OCSPRequest, SingleResponse",
            "from asn1crypto.x509 import Certificate",
            "from OpenSSL.SSL import Connection",
            "",
            "from snowflake.connector.compat import OK, urlsplit, urlunparse",
            "from snowflake.connector.constants import HTTP_HEADER_USER_AGENT",
            "from snowflake.connector.errorcode import (",
            "    ER_INVALID_OCSP_RESPONSE_SSD,",
            "    ER_INVALID_SSD,",
            "    ER_OCSP_FAILED_TO_CONNECT_CACHE_SERVER,",
            "    ER_OCSP_RESPONSE_ATTACHED_CERT_EXPIRED,",
            "    ER_OCSP_RESPONSE_ATTACHED_CERT_INVALID,",
            "    ER_OCSP_RESPONSE_CACHE_DECODE_FAILED,",
            "    ER_OCSP_RESPONSE_CACHE_DOWNLOAD_FAILED,",
            "    ER_OCSP_RESPONSE_CERT_STATUS_INVALID,",
            "    ER_OCSP_RESPONSE_CERT_STATUS_REVOKED,",
            "    ER_OCSP_RESPONSE_CERT_STATUS_UNKNOWN,",
            "    ER_OCSP_RESPONSE_EXPIRED,",
            "    ER_OCSP_RESPONSE_FETCH_EXCEPTION,",
            "    ER_OCSP_RESPONSE_FETCH_FAILURE,",
            "    ER_OCSP_RESPONSE_INVALID_EXPIRY_INFO_MISSING,",
            "    ER_OCSP_RESPONSE_INVALID_SIGNATURE,",
            "    ER_OCSP_RESPONSE_LOAD_FAILURE,",
            "    ER_OCSP_RESPONSE_STATUS_UNSUCCESSFUL,",
            "    ER_OCSP_RESPONSE_UNAVAILABLE,",
            "    ER_OCSP_URL_INFO_MISSING,",
            ")",
            "from snowflake.connector.errors import RevocationCheckError",
            "from snowflake.connector.network import PYTHON_CONNECTOR_USER_AGENT",
            "",
            "from . import constants",
            "from .backoff_policies import exponential_backoff",
            "from .cache import SFDictCache, SFDictFileCache",
            "from .telemetry import TelemetryField, generate_telemetry_data_dict",
            "from .url_util import extract_top_level_domain_from_hostname, url_encode_str",
            "",
            "",
            "class OCSPResponseValidationResult(NamedTuple):",
            "    exception: Exception | None = None",
            "    issuer: Certificate | None = None",
            "    subject: Certificate | None = None",
            "    cert_id: CertId | None = None",
            "    ocsp_response: bytes | None = None",
            "    ts: int | None = None",
            "    validated: bool = False",
            "",
            "",
            "try:",
            "    OCSP_RESPONSE_VALIDATION_CACHE: SFDictFileCache[",
            "        tuple[bytes, bytes, bytes],",
            "        OCSPResponseValidationResult,",
            "    ] = SFDictFileCache(",
            "        entry_lifetime=constants.DAY_IN_SECONDS,",
            "        file_path={",
            "            \"linux\": os.path.join(",
            "                \"~\", \".cache\", \"snowflake\", \"ocsp_response_validation_cache\"",
            "            ),",
            "            \"darwin\": os.path.join(",
            "                \"~\", \"Library\", \"Caches\", \"Snowflake\", \"ocsp_response_validation_cache\"",
            "            ),",
            "            \"windows\": os.path.join(",
            "                \"~\",",
            "                \"AppData\",",
            "                \"Local\",",
            "                \"Snowflake\",",
            "                \"Caches\",",
            "                \"ocsp_response_validation_cache\",",
            "            ),",
            "        },",
            "    )",
            "except OSError:",
            "    # In case we run into some read/write permission error fall back onto",
            "    #  in memory caching",
            "    OCSP_RESPONSE_VALIDATION_CACHE: SFDictCache[",
            "        tuple[bytes, bytes, bytes],",
            "        OCSPResponseValidationResult,",
            "    ] = SFDictCache(",
            "        entry_lifetime=constants.DAY_IN_SECONDS,",
            "    )",
            "",
            "logger = getLogger(__name__)",
            "",
            "",
            "def generate_cache_key(",
            "    cert_id: CertId,",
            ") -> tuple[bytes, bytes, bytes]:",
            "    return (",
            "        cert_id[\"issuer_name_hash\"].dump(),",
            "        cert_id[\"issuer_key_hash\"].dump(),",
            "        cert_id[\"serial_number\"].dump(),",
            "    )",
            "",
            "",
            "class OCSPTelemetryData:",
            "    CERTIFICATE_EXTRACTION_FAILED = \"CertificateExtractionFailed\"",
            "    OCSP_URL_MISSING = \"OCSPURLMissing\"",
            "    OCSP_RESPONSE_UNAVAILABLE = \"OCSPResponseUnavailable\"",
            "    OCSP_RESPONSE_FETCH_EXCEPTION = \"OCSPResponseFetchException\"",
            "    OCSP_RESPONSE_FAILED_TO_CONNECT_CACHE_SERVER = (",
            "        \"OCSPResponseFailedToConnectCacheServer\"",
            "    )",
            "    OCSP_RESPONSE_CERT_STATUS_INVALID = \"OCSPResponseCertStatusInvalid\"",
            "    OCSP_RESPONSE_CERT_STATUS_REVOKED = \"OCSPResponseCertStatusRevoked\"",
            "    OCSP_RESPONSE_CERT_STATUS_UNKNOWN = \"OCSPResponseCertStatusUnknown\"",
            "    OCSP_RESPONSE_STATUS_UNSUCCESSFUL = \"OCSPResponseStatusUnsuccessful\"",
            "    OCSP_RESPONSE_ATTACHED_CERT_INVALID = \"OCSPResponseAttachedCertInvalid\"",
            "    OCSP_RESPONSE_ATTACHED_CERT_EXPIRED = \"OCSPResponseAttachedCertExpired\"",
            "    OCSP_RESPONSE_INVALID_SIGNATURE = \"OCSPResponseSignatureInvalid\"",
            "    OCSP_RESPONSE_EXPIRY_INFO_MISSING = \"OCSPResponseExpiryInfoMissing\"",
            "    OCSP_RESPONSE_EXPIRED = \"OCSPResponseExpired\"",
            "    OCSP_RESPONSE_FETCH_FAILURE = \"OCSPResponseFetchFailure\"",
            "    OCSP_RESPONSE_CACHE_DOWNLOAD_FAILED = \"OCSPResponseCacheDownloadFailed\"",
            "    OCSP_RESPONSE_CACHE_DECODE_FAILED = \"OCSPResponseCacheDecodeFailed\"",
            "    OCSP_RESPONSE_LOAD_FAILURE = \"OCSPResponseLoadFailure\"",
            "    OCSP_RESPONSE_INVALID_SSD = \"OCSPResponseInvalidSSD\"",
            "",
            "    ERROR_CODE_MAP = {",
            "        ER_OCSP_URL_INFO_MISSING: OCSP_URL_MISSING,",
            "        ER_OCSP_RESPONSE_UNAVAILABLE: OCSP_RESPONSE_UNAVAILABLE,",
            "        ER_OCSP_RESPONSE_FETCH_EXCEPTION: OCSP_RESPONSE_FETCH_EXCEPTION,",
            "        ER_OCSP_FAILED_TO_CONNECT_CACHE_SERVER: OCSP_RESPONSE_FAILED_TO_CONNECT_CACHE_SERVER,",
            "        ER_OCSP_RESPONSE_CERT_STATUS_INVALID: OCSP_RESPONSE_CERT_STATUS_INVALID,",
            "        ER_OCSP_RESPONSE_CERT_STATUS_REVOKED: OCSP_RESPONSE_CERT_STATUS_REVOKED,",
            "        ER_OCSP_RESPONSE_CERT_STATUS_UNKNOWN: OCSP_RESPONSE_CERT_STATUS_UNKNOWN,",
            "        ER_OCSP_RESPONSE_STATUS_UNSUCCESSFUL: OCSP_RESPONSE_STATUS_UNSUCCESSFUL,",
            "        ER_OCSP_RESPONSE_ATTACHED_CERT_INVALID: OCSP_RESPONSE_ATTACHED_CERT_INVALID,",
            "        ER_OCSP_RESPONSE_ATTACHED_CERT_EXPIRED: OCSP_RESPONSE_ATTACHED_CERT_EXPIRED,",
            "        ER_OCSP_RESPONSE_INVALID_SIGNATURE: OCSP_RESPONSE_INVALID_SIGNATURE,",
            "        ER_OCSP_RESPONSE_INVALID_EXPIRY_INFO_MISSING: OCSP_RESPONSE_EXPIRY_INFO_MISSING,",
            "        ER_OCSP_RESPONSE_EXPIRED: OCSP_RESPONSE_EXPIRED,",
            "        ER_OCSP_RESPONSE_FETCH_FAILURE: OCSP_RESPONSE_FETCH_FAILURE,",
            "        ER_OCSP_RESPONSE_LOAD_FAILURE: OCSP_RESPONSE_LOAD_FAILURE,",
            "        ER_OCSP_RESPONSE_CACHE_DOWNLOAD_FAILED: OCSP_RESPONSE_CACHE_DOWNLOAD_FAILED,",
            "        ER_OCSP_RESPONSE_CACHE_DECODE_FAILED: OCSP_RESPONSE_CACHE_DECODE_FAILED,",
            "        ER_INVALID_OCSP_RESPONSE_SSD: OCSP_RESPONSE_INVALID_SSD,",
            "        ER_INVALID_SSD: OCSP_RESPONSE_INVALID_SSD,",
            "    }",
            "",
            "    def __init__(self) -> None:",
            "        self.event_sub_type = None",
            "        self.ocsp_connection_method = None",
            "        self.cert_id = None",
            "        self.sfc_peer_host = None",
            "        self.ocsp_url = None",
            "        self.ocsp_req = None",
            "        self.error_msg = None",
            "        self.cache_enabled = False",
            "        self.cache_hit = False",
            "        self.fail_open = False",
            "        self.insecure_mode = False",
            "",
            "    def set_event_sub_type(self, event_sub_type: str) -> None:",
            "        \"\"\"",
            "        Sets sub type for OCSP Telemetry Event.",
            "",
            "        There can be multiple event_sub_type that could have happened",
            "        during a single connection establishment. Ensure that all of them",
            "        are captured.",
            "        :param event_sub_type:",
            "        :return:",
            "        \"\"\"",
            "        if self.event_sub_type is not None:",
            "            self.event_sub_type = f\"{self.event_sub_type}|{event_sub_type}\"",
            "        else:",
            "            self.event_sub_type = event_sub_type",
            "",
            "    def set_ocsp_connection_method(self, ocsp_conn_method: str) -> None:",
            "        self.ocsp_connection_method = ocsp_conn_method",
            "",
            "    def set_cert_id(self, cert_id) -> None:",
            "        self.cert_id = cert_id",
            "",
            "    def set_sfc_peer_host(self, sfc_peer_host) -> None:",
            "        self.sfc_peer_host = sfc_peer_host",
            "",
            "    def set_ocsp_url(self, ocsp_url) -> None:",
            "        self.ocsp_url = ocsp_url",
            "",
            "    def set_ocsp_req(self, ocsp_req) -> None:",
            "        self.ocsp_req = ocsp_req",
            "",
            "    def set_error_msg(self, error_msg) -> None:",
            "        self.error_msg = error_msg",
            "",
            "    def set_cache_enabled(self, cache_enabled) -> None:",
            "        self.cache_enabled = cache_enabled",
            "        if not cache_enabled:",
            "            self.cache_hit = False",
            "",
            "    def set_cache_hit(self, cache_hit) -> None:",
            "        if not self.cache_enabled:",
            "            self.cache_hit = False",
            "        else:",
            "            self.cache_hit = cache_hit",
            "",
            "    def set_fail_open(self, fail_open) -> None:",
            "        self.fail_open = fail_open",
            "",
            "    def set_insecure_mode(self, insecure_mode) -> None:",
            "        self.insecure_mode = insecure_mode",
            "",
            "    def generate_telemetry_data(",
            "        self, event_type: str, urgent: bool = False",
            "    ) -> dict[str, Any]:",
            "        _, exception, _ = sys.exc_info()",
            "        telemetry_data = generate_telemetry_data_dict(",
            "            from_dict={",
            "                TelemetryField.KEY_OOB_EVENT_TYPE.value: event_type,",
            "                TelemetryField.KEY_OOB_EVENT_SUB_TYPE.value: self.event_sub_type,",
            "                TelemetryField.KEY_OOB_SFC_PEER_HOST.value: self.sfc_peer_host,",
            "                TelemetryField.KEY_OOB_CERT_ID.value: self.cert_id,",
            "                TelemetryField.KEY_OOB_OCSP_REQUEST_BASE64.value: self.ocsp_req,",
            "                TelemetryField.KEY_OOB_OCSP_RESPONDER_URL.value: self.ocsp_url,",
            "                TelemetryField.KEY_OOB_ERROR_MESSAGE.value: self.error_msg,",
            "                TelemetryField.KEY_OOB_INSECURE_MODE.value: self.insecure_mode,",
            "                TelemetryField.KEY_OOB_FAIL_OPEN.value: self.fail_open,",
            "                TelemetryField.KEY_OOB_CACHE_ENABLED.value: self.cache_enabled,",
            "                TelemetryField.KEY_OOB_CACHE_HIT.value: self.cache_hit,",
            "            },",
            "            is_oob_telemetry=True,",
            "        )",
            "",
            "        return telemetry_data",
            "        # To be updated once Python Driver has out of band telemetry.",
            "        # telemetry_client = TelemetryClient()",
            "        # telemetry_client.add_log_to_batch(TelemetryData(telemetry_data, datetime.now(timezone.utc).replace(tzinfo=None))",
            "",
            "",
            "class OCSPServer:",
            "    MAX_RETRY = int(os.getenv(\"OCSP_MAX_RETRY\", \"3\"))",
            "",
            "    def __init__(self, **kwargs) -> None:",
            "        top_level_domain = kwargs.pop(",
            "            \"top_level_domain\", constants._DEFAULT_HOSTNAME_TLD",
            "        )",
            "        self.DEFAULT_CACHE_SERVER_URL = (",
            "            f\"http://ocsp.snowflakecomputing.{top_level_domain}\"",
            "        )",
            "        \"\"\"",
            "        The following will change to something like",
            "        http://ocspssd.snowflakecomputing.com/ocsp/",
            "        once the endpoint is up in the backend",
            "        \"\"\"",
            "        self.NEW_DEFAULT_CACHE_SERVER_BASE_URL = (",
            "            f\"https://ocspssd.snowflakecomputing.{top_level_domain}/ocsp/\"",
            "        )",
            "        if not OCSPServer.is_enabled_new_ocsp_endpoint():",
            "            self.CACHE_SERVER_URL = os.getenv(",
            "                \"SF_OCSP_RESPONSE_CACHE_SERVER_URL\",",
            "                \"{}/{}\".format(",
            "                    self.DEFAULT_CACHE_SERVER_URL,",
            "                    OCSPCache.OCSP_RESPONSE_CACHE_FILE_NAME,",
            "                ),",
            "            )",
            "        else:",
            "            self.CACHE_SERVER_URL = os.getenv(\"SF_OCSP_RESPONSE_CACHE_SERVER_URL\")",
            "",
            "        self.CACHE_SERVER_ENABLED = (",
            "            os.getenv(\"SF_OCSP_RESPONSE_CACHE_SERVER_ENABLED\", \"true\") != \"false\"",
            "        )",
            "        # OCSP dynamic cache server URL pattern",
            "        self.OCSP_RETRY_URL = None",
            "",
            "    @staticmethod",
            "    def is_enabled_new_ocsp_endpoint() -> bool:",
            "        \"\"\"Checks if new OCSP Endpoint has been enabled.\"\"\"",
            "        return os.getenv(\"SF_OCSP_ACTIVATE_NEW_ENDPOINT\", \"false\").lower() == \"true\"",
            "",
            "    def reset_ocsp_endpoint(self, hname) -> None:",
            "        \"\"\"Resets current object members CACHE_SERVER_URL and RETRY_URL_PATTERN.",
            "",
            "        They will point at the new OCSP Fetch and Retry endpoints respectively. The new OCSP Endpoint address is based",
            "        on the hostname the customer is trying to connect to. The deployment or in case of client failover, the",
            "        replication ID is copied from the hostname.",
            "        \"\"\"",
            "        top_level_domain = extract_top_level_domain_from_hostname(hname)",
            "        if \"privatelink.snowflakecomputing.\" in hname:",
            "            temp_ocsp_endpoint = \"\".join([\"https://ocspssd.\", hname, \"/ocsp/\"])",
            "        elif \"global.snowflakecomputing.\" in hname:",
            "            rep_id_begin = hname[hname.find(\"-\") :]",
            "            temp_ocsp_endpoint = \"\".join([\"https://ocspssd\", rep_id_begin, \"/ocsp/\"])",
            "        elif not hname.endswith(f\"snowflakecomputing.{top_level_domain}\"):",
            "            temp_ocsp_endpoint = self.NEW_DEFAULT_CACHE_SERVER_BASE_URL",
            "        else:",
            "            hname_wo_acc = hname[hname.find(\".\") :]",
            "            temp_ocsp_endpoint = \"\".join([\"https://ocspssd\", hname_wo_acc, \"/ocsp/\"])",
            "",
            "        self.CACHE_SERVER_URL = \"\".join([temp_ocsp_endpoint, \"fetch\"])",
            "        self.OCSP_RETRY_URL = \"\".join([temp_ocsp_endpoint, \"retry\"])",
            "",
            "    def reset_ocsp_dynamic_cache_server_url(self, use_ocsp_cache_server) -> None:",
            "        \"\"\"Resets OCSP dynamic cache server url pattern.",
            "",
            "        This is used only when OCSP cache server is updated.",
            "        \"\"\"",
            "        if use_ocsp_cache_server is not None:",
            "            self.CACHE_SERVER_ENABLED = use_ocsp_cache_server",
            "",
            "        if self.CACHE_SERVER_ENABLED:",
            "            logger.debug(",
            "                \"OCSP response cache server is enabled: %s\", self.CACHE_SERVER_URL",
            "            )",
            "        else:",
            "            logger.debug(\"OCSP response cache server is disabled\")",
            "",
            "        if self.OCSP_RETRY_URL is None:",
            "            if self.CACHE_SERVER_URL is not None and (",
            "                not self.CACHE_SERVER_URL.startswith(self.DEFAULT_CACHE_SERVER_URL)",
            "            ):",
            "                # only if custom OCSP cache server is used.",
            "                parsed_url = urlsplit(self.CACHE_SERVER_URL)",
            "                self.OCSP_RETRY_URL = f\"{urlunparse((parsed_url.scheme, parsed_url.netloc, '', '', '', ''))}/retry/{{0}}/{{1}}\"",
            "        logger.debug(\"OCSP dynamic cache server RETRY URL: %s\", self.OCSP_RETRY_URL)",
            "",
            "    def download_cache_from_server(self, ocsp):",
            "        if self.CACHE_SERVER_ENABLED:",
            "            # if any of them is not cache, download the cache file from",
            "            # OCSP response cache server.",
            "            try:",
            "                retval = OCSPServer._download_ocsp_response_cache(",
            "                    ocsp, self.CACHE_SERVER_URL",
            "                )",
            "                if not retval:",
            "                    raise RevocationCheckError(",
            "                        msg=\"OCSP Cache Server Unavailable.\",",
            "                        errno=ER_OCSP_RESPONSE_CACHE_DOWNLOAD_FAILED,",
            "                    )",
            "                logger.debug(",
            "                    \"downloaded OCSP response cache file from %s\", self.CACHE_SERVER_URL",
            "                )",
            "                # len(OCSP_RESPONSE_VALIDATION_CACHE) is thread-safe, however, we do not want to",
            "                # block for logging purpose, thus using len(OCSP_RESPONSE_VALIDATION_CACHE._cache) here.",
            "                logger.debug(",
            "                    \"# of certificates: %u\",",
            "                    len(OCSP_RESPONSE_VALIDATION_CACHE._cache),",
            "                )",
            "            except RevocationCheckError as rce:",
            "                logger.debug(",
            "                    \"OCSP Response cache download failed. The client\"",
            "                    \"will reach out to the OCSP Responder directly for\"",
            "                    \"any missing OCSP responses %s\\n\" % rce.msg",
            "                )",
            "                raise",
            "",
            "    @staticmethod",
            "    def _download_ocsp_response_cache(ocsp, url, do_retry: bool = True) -> bool:",
            "        \"\"\"Downloads OCSP response cache from the cache server.\"\"\"",
            "        headers = {HTTP_HEADER_USER_AGENT: PYTHON_CONNECTOR_USER_AGENT}",
            "        sf_timeout = SnowflakeOCSP.OCSP_CACHE_SERVER_CONNECTION_TIMEOUT",
            "",
            "        try:",
            "            start_time = time.time()",
            "            logger.debug(\"started downloading OCSP response cache file: %s\", url)",
            "",
            "            if ocsp.test_mode is not None:",
            "                test_timeout = os.getenv(",
            "                    \"SF_TEST_OCSP_CACHE_SERVER_CONNECTION_TIMEOUT\", None",
            "                )",
            "                sf_cache_server_url = os.getenv(\"SF_TEST_OCSP_CACHE_SERVER_URL\", None)",
            "                if test_timeout is not None:",
            "                    sf_timeout = int(test_timeout)",
            "                if sf_cache_server_url is not None:",
            "                    url = sf_cache_server_url",
            "",
            "            with generic_requests.Session() as session:",
            "                max_retry = SnowflakeOCSP.OCSP_CACHE_SERVER_MAX_RETRY if do_retry else 1",
            "                sleep_time = 1",
            "                backoff = exponential_backoff()()",
            "                for _ in range(max_retry):",
            "                    response = session.get(",
            "                        url,",
            "                        timeout=sf_timeout,  # socket timeout",
            "                        headers=headers,",
            "                    )",
            "                    if response.status_code == OK:",
            "                        ocsp.decode_ocsp_response_cache(response.json())",
            "                        elapsed_time = time.time() - start_time",
            "                        logger.debug(",
            "                            \"ended downloading OCSP response cache file. \"",
            "                            \"elapsed time: %ss\",",
            "                            elapsed_time,",
            "                        )",
            "                        break",
            "                    elif max_retry > 1:",
            "                        sleep_time = next(backoff)",
            "                        logger.debug(",
            "                            \"OCSP server returned %s. Retrying in %s(s)\",",
            "                            response.status_code,",
            "                            sleep_time,",
            "                        )",
            "                    time.sleep(sleep_time)",
            "                else:",
            "                    logger.error(",
            "                        \"Failed to get OCSP response after %s attempt.\", max_retry",
            "                    )",
            "                    return False",
            "                return True",
            "        except Exception as e:",
            "            logger.debug(\"Failed to get OCSP response cache from %s: %s\", url, e)",
            "            raise RevocationCheckError(",
            "                msg=f\"Failed to get OCSP Response Cache from {url}: {e}\",",
            "                errno=ER_OCSP_FAILED_TO_CONNECT_CACHE_SERVER,",
            "            )",
            "",
            "    def generate_get_url(self, ocsp_url, b64data):",
            "        parsed_url = urlsplit(ocsp_url)",
            "        url_encoded_b64data = url_encode_str(b64data)",
            "        if self.OCSP_RETRY_URL is None:",
            "            target_url = f\"{ocsp_url}/{url_encoded_b64data}\"",
            "        else:",
            "            # values of parsed_url.netloc and parsed_url.path based on oscp_url are as follows:",
            "            # URL                                    NETLOC                         PATH",
            "            # \"http://oneocsp.microsoft.com\"         \"oneocsp.microsoft.com\"        \"\"",
            "            # \"http://oneocsp.microsoft.com:8080\"    \"oneocsp.microsoft.com:8080\"   \"\"",
            "            # \"http://oneocsp.microsoft.com/\"        \"oneocsp.microsoft.com\"        \"/\"",
            "            # \"http://oneocsp.microsoft.com/ocsp\"    \"oneocsp.microsoft.com\"        \"/ocsp\"",
            "            # The check below is to treat first two urls same",
            "            path = parsed_url.path if parsed_url.path != \"/\" else \"\"",
            "            target_url = self.OCSP_RETRY_URL.format(",
            "                parsed_url.netloc + path, url_encoded_b64data",
            "            )",
            "",
            "        logger.debug(\"OCSP Retry URL is - %s\", target_url)",
            "        return target_url",
            "",
            "",
            "class OCSPCache:",
            "    # OCSP cache lock",
            "    CACHE_LOCK = Lock()",
            "",
            "    # OCSP cache update flag",
            "    CACHE_UPDATED = False",
            "",
            "    # Cache Expiration in seconds (120 hours). OCSP validation cache is",
            "    # invalidated every 120 hours (5 days)",
            "    CACHE_EXPIRATION = 432000",
            "",
            "    # OCSP Response Cache URI",
            "    OCSP_RESPONSE_CACHE_URI = None",
            "",
            "    # OCSP response cache file name",
            "    OCSP_RESPONSE_CACHE_FILE_NAME = \"ocsp_response_cache.json\"",
            "",
            "    # Cache directory",
            "    CACHE_DIR = None",
            "",
            "    @staticmethod",
            "    def reset_cache_dir() -> None:",
            "        # Cache directory",
            "        OCSPCache.CACHE_DIR = os.getenv(\"SF_OCSP_RESPONSE_CACHE_DIR\")",
            "        if OCSPCache.CACHE_DIR is None:",
            "            cache_root_dir = expanduser(\"~\") or tempfile.gettempdir()",
            "            if platform.system() == \"Windows\":",
            "                OCSPCache.CACHE_DIR = path.join(",
            "                    cache_root_dir, \"AppData\", \"Local\", \"Snowflake\", \"Caches\"",
            "                )",
            "            elif platform.system() == \"Darwin\":",
            "                OCSPCache.CACHE_DIR = path.join(",
            "                    cache_root_dir, \"Library\", \"Caches\", \"Snowflake\"",
            "                )",
            "            else:",
            "                OCSPCache.CACHE_DIR = path.join(cache_root_dir, \".cache\", \"snowflake\")",
            "        logger.debug(\"cache directory: %s\", OCSPCache.CACHE_DIR)",
            "",
            "        if not path.exists(OCSPCache.CACHE_DIR):",
            "            try:",
            "                os.makedirs(OCSPCache.CACHE_DIR, mode=0o700)",
            "            except Exception as ex:",
            "                logger.debug(",
            "                    \"cannot create a cache directory: [%s], err=[%s]\",",
            "                    OCSPCache.CACHE_DIR,",
            "                    ex,",
            "                )",
            "                OCSPCache.CACHE_DIR = None",
            "",
            "    @staticmethod",
            "    def del_cache_file() -> None:",
            "        \"\"\"Deletes the OCSP response cache file if exists.\"\"\"",
            "        cache_file = path.join(",
            "            OCSPCache.CACHE_DIR, OCSPCache.OCSP_RESPONSE_CACHE_FILE_NAME",
            "        )",
            "        if path.exists(cache_file):",
            "            logger.debug(f\"deleting cache file {cache_file}\")",
            "            os.unlink(cache_file)",
            "",
            "    @staticmethod",
            "    def reset_ocsp_response_cache_uri(ocsp_response_cache_uri) -> None:",
            "        if ocsp_response_cache_uri is None and OCSPCache.CACHE_DIR is not None:",
            "            OCSPCache.OCSP_RESPONSE_CACHE_URI = \"file://\" + path.join(",
            "                OCSPCache.CACHE_DIR, OCSPCache.OCSP_RESPONSE_CACHE_FILE_NAME",
            "            )",
            "        else:",
            "            OCSPCache.OCSP_RESPONSE_CACHE_URI = ocsp_response_cache_uri",
            "",
            "        if OCSPCache.OCSP_RESPONSE_CACHE_URI is not None:",
            "            # normalize URI for Windows",
            "            OCSPCache.OCSP_RESPONSE_CACHE_URI = (",
            "                OCSPCache.OCSP_RESPONSE_CACHE_URI.replace(\"\\\\\", \"/\")",
            "            )",
            "",
            "        logger.debug(\"ocsp_response_cache_uri: %s\", OCSPCache.OCSP_RESPONSE_CACHE_URI)",
            "        # len(OCSP_RESPONSE_VALIDATION_CACHE) is thread-safe, however, we do not want to",
            "        # block for logging purpose, thus using len(OCSP_RESPONSE_VALIDATION_CACHE._cache) here.",
            "        logger.debug(",
            "            \"OCSP_VALIDATION_CACHE size: %u\",",
            "            len(OCSP_RESPONSE_VALIDATION_CACHE._cache),",
            "        )",
            "",
            "    @staticmethod",
            "    def read_file(ocsp):",
            "        \"\"\"Reads OCSP Response cache data from the URI, which is very likely a file.\"\"\"",
            "        try:",
            "            parsed_url = urlsplit(OCSPCache.OCSP_RESPONSE_CACHE_URI)",
            "            if parsed_url.scheme == \"file\":",
            "                OCSPCache.read_ocsp_response_cache_file(",
            "                    ocsp, path.join(parsed_url.netloc, parsed_url.path)",
            "                )",
            "            else:",
            "                msg = \"Unsupported OCSP URI: {}\".format(",
            "                    OCSPCache.OCSP_RESPONSE_CACHE_URI",
            "                )",
            "                raise Exception(msg)",
            "        except (RevocationCheckError, Exception) as rce:",
            "            logger.debug(",
            "                \"Failed to read OCSP response cache file %s: %s, \"",
            "                \"No worry. It will validate with OCSP server. \"",
            "                \"Ignoring...\",",
            "                OCSPCache.OCSP_RESPONSE_CACHE_URI,",
            "                rce,",
            "                exc_info=True,",
            "            )",
            "",
            "    @staticmethod",
            "    def read_ocsp_response_cache_file(ocsp, filename):",
            "        \"\"\"Reads OCSP Response cache.\"\"\"",
            "        try:",
            "            if OCSPCache.check_ocsp_response_cache_lock_dir(filename) and path.exists(",
            "                filename",
            "            ):",
            "                with codecs.open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:",
            "                    ocsp.decode_ocsp_response_cache(json.load(f))",
            "                # len(OCSP_RESPONSE_VALIDATION_CACHE) is thread-safe, however, we do not want to",
            "                # block for logging purpose, thus using len(OCSP_RESPONSE_VALIDATION_CACHE._cache) here.",
            "                logger.debug(",
            "                    \"Read OCSP response cache file: %s, count=%s\",",
            "                    filename,",
            "                    len(OCSP_RESPONSE_VALIDATION_CACHE._cache),",
            "                )",
            "            else:",
            "                logger.debug(",
            "                    \"Failed to locate OCSP response cache file. \"",
            "                    \"No worry. It will validate with OCSP server: %s\",",
            "                    filename,",
            "                )",
            "        except Exception as ex:",
            "            logger.debug(\"Caught - %s\", ex)",
            "            raise ex",
            "",
            "    @staticmethod",
            "    def update_file(ocsp) -> None:",
            "        \"\"\"",
            "        Updates OCSP Response Cache file.",
            "        Two file shall be updated/saved:",
            "            1. file for OCSP_RESPONSE_VALIDATION_CACHE which keeps ocsp response validation result",
            "            2. ocsp_response_cache.json, the file in the same format as the one downloaded from snowflake cache service",
            "        \"\"\"",
            "        if OCSPCache.CACHE_UPDATED:",
            "            if isinstance(OCSP_RESPONSE_VALIDATION_CACHE, SFDictFileCache):",
            "                OCSP_RESPONSE_VALIDATION_CACHE.save()",
            "            OCSPCache.update_ocsp_response_cache_file(",
            "                ocsp, OCSPCache.OCSP_RESPONSE_CACHE_URI",
            "            )",
            "            OCSPCache.CACHE_UPDATED = False",
            "",
            "    @staticmethod",
            "    def update_ocsp_response_cache_file(ocsp, ocsp_response_cache_uri) -> None:",
            "        \"\"\"Updates OCSP Response Cache.\"\"\"",
            "        if ocsp_response_cache_uri is not None:",
            "            try:",
            "                parsed_url = urlsplit(ocsp_response_cache_uri)",
            "                if parsed_url.scheme == \"file\":",
            "                    filename = path.join(parsed_url.netloc, parsed_url.path)",
            "                    lock_dir = filename + \".lck\"",
            "                    for _ in range(100):",
            "                        # wait until the lck file has been removed",
            "                        # or up to 1 second (0.01 x 100)",
            "                        if OCSPCache.lock_cache_file(lock_dir):",
            "                            break",
            "                        time.sleep(0.01)",
            "                    try:",
            "                        OCSPCache.write_ocsp_response_cache_file(ocsp, filename)",
            "                    finally:",
            "                        OCSPCache.unlock_cache_file(lock_dir)",
            "                else:",
            "                    logger.debug(",
            "                        \"No OCSP response cache file is written, because the \"",
            "                        \"given URI is not a file: %s. Ignoring...\",",
            "                        ocsp_response_cache_uri,",
            "                    )",
            "            except Exception as e:",
            "                logger.debug(",
            "                    \"Failed to write OCSP response cache \"",
            "                    \"file. file: %s, error: %s, Ignoring...\",",
            "                    ocsp_response_cache_uri,",
            "                    e,",
            "                    exc_info=True,",
            "                )",
            "",
            "    @staticmethod",
            "    def write_ocsp_response_cache_file(ocsp, filename) -> None:",
            "        \"\"\"Writes OCSP Response Cache.\"\"\"",
            "        logger.debug(f\"writing OCSP response cache file to {filename}\")",
            "        file_cache_data = {}",
            "        ocsp.encode_ocsp_response_cache(file_cache_data)",
            "        with codecs.open(filename, \"w\", encoding=\"utf-8\", errors=\"ignore\") as f:",
            "            json.dump(file_cache_data, f)",
            "",
            "    @staticmethod",
            "    def check_ocsp_response_cache_lock_dir(filename) -> bool:",
            "        \"\"\"Checks if the lock directory exists.",
            "",
            "        Returns:",
            "            True if it can update the cache file or False when some other process may be updating the cache file.",
            "        \"\"\"",
            "        current_time = int(time.time())",
            "        lock_dir = filename + \".lck\"",
            "",
            "        try:",
            "            ts_cache_file = OCSPCache._file_timestamp(filename)",
            "            if (",
            "                not path.exists(lock_dir)",
            "                and current_time - OCSPCache.CACHE_EXPIRATION <= ts_cache_file",
            "            ):",
            "                # use cache only if no lock directory exists and the cache file",
            "                # was created last 24 hours",
            "                return True",
            "",
            "            if path.exists(lock_dir):",
            "                # delete lock directory if older 60 seconds",
            "                ts_lock_dir = OCSPCache._file_timestamp(lock_dir)",
            "                if ts_lock_dir < current_time - 60:",
            "                    OCSPCache.unlock_cache_file(lock_dir)",
            "                    logger.debug(",
            "                        \"The lock directory is older than 60 seconds. \"",
            "                        \"Deleted the lock directory and ignoring the cache: %s\",",
            "                        lock_dir,",
            "                    )",
            "                else:",
            "                    logger.debug(",
            "                        \"The lock directory exists. Other process may be \"",
            "                        \"updating the cache file: %s, %s\",",
            "                        filename,",
            "                        lock_dir,",
            "                    )",
            "            else:",
            "                os.unlink(filename)",
            "                logger.debug(",
            "                    \"The cache is older than 1 day. \" \"Deleted the cache file: %s\",",
            "                    filename,",
            "                )",
            "        except Exception as e:",
            "            logger.debug(",
            "                \"Failed to check OCSP response cache file. No worry. It will \"",
            "                \"validate with OCSP server: file: %s, lock directory: %s, \"",
            "                \"error: %s\",",
            "                filename,",
            "                lock_dir,",
            "                e,",
            "            )",
            "        return False",
            "",
            "    @staticmethod",
            "    def is_cache_fresh(current_time: int, ts: int) -> bool:",
            "        return current_time - OCSPCache.CACHE_EXPIRATION <= ts",
            "",
            "    @staticmethod",
            "    def find_cache(",
            "        ocsp: SnowflakeOCSP, cert_id: CertId, subject: Certificate | None, **kwargs: Any",
            "    ) -> tuple[bool, bytes | None]:",
            "        subject_name = ocsp.subject_name(subject) if subject else None",
            "        current_time = int(time.time())",
            "        cache_key: tuple[bytes, bytes, bytes] = kwargs.get(",
            "            \"cache_key\", ocsp.decode_cert_id_key(cert_id)",
            "        )",
            "        lock_cache: bool = kwargs.get(\"lock_cache\", True)",
            "        try:",
            "            ocsp_response_validation_result = (",
            "                OCSP_RESPONSE_VALIDATION_CACHE[cache_key]",
            "                if lock_cache",
            "                else OCSP_RESPONSE_VALIDATION_CACHE._getitem_non_locking(cache_key)",
            "            )",
            "            try:",
            "                # is_valid_time can raise exception if the cache",
            "                # entry is a SSD.",
            "                if OCSPCache.is_cache_fresh(",
            "                    current_time, ocsp_response_validation_result.ts",
            "                ) and ocsp.is_valid_time(",
            "                    cert_id, ocsp_response_validation_result.ocsp_response",
            "                ):",
            "                    if subject_name:",
            "                        logger.debug(\"hit cache for subject: %s\", subject_name)",
            "                    return True, ocsp_response_validation_result.ocsp_response",
            "                else:",
            "                    OCSPCache.delete_cache(",
            "                        ocsp, cert_id, cache_key=cache_key, lock_cache=lock_cache",
            "                    )",
            "            except Exception as ex:",
            "                logger.debug(f\"Could not validate cache entry {cert_id} {ex}\")",
            "            OCSPCache.CACHE_UPDATED = True",
            "        except KeyError:",
            "            if subject_name:",
            "                logger.debug(f\"cache miss for subject: '{subject_name}'\")",
            "        return False, None",
            "",
            "    @staticmethod",
            "    def delete_cache(ocsp: SnowflakeOCSP, cert_id: CertId, **kwargs: Any) -> None:",
            "        cache_key: tuple[bytes, bytes, bytes] = kwargs.get(",
            "            \"cache_key\", ocsp.decode_cert_id_key(cert_id)",
            "        )",
            "        lock_cache: bool = kwargs.get(\"lock_cache\", True)",
            "        try:",
            "            if lock_cache:",
            "                del OCSP_RESPONSE_VALIDATION_CACHE[cache_key]",
            "            else:",
            "                OCSP_RESPONSE_VALIDATION_CACHE._delitem(cache_key)",
            "            OCSPCache.CACHE_UPDATED = True",
            "        except KeyError:",
            "            pass",
            "",
            "    @staticmethod",
            "    def _file_timestamp(filename):",
            "        \"\"\"Gets the last created timestamp of the file/dir.\"\"\"",
            "        if platform.system() == \"Windows\":",
            "            ts = int(path.getctime(filename))",
            "        else:",
            "            stat = os.stat(filename)",
            "            if hasattr(stat, \"st_birthtime\"):  # odx",
            "                ts = int(stat.st_birthtime)",
            "            else:",
            "                ts = int(stat.st_mtime)  # linux",
            "        return ts",
            "",
            "    @staticmethod",
            "    def lock_cache_file(fname) -> bool:",
            "        \"\"\"Locks a cache file by creating a directory.\"\"\"",
            "        try:",
            "            os.mkdir(fname)",
            "            return True",
            "        except OSError:",
            "            return False",
            "",
            "    @staticmethod",
            "    def unlock_cache_file(fname) -> bool:",
            "        \"\"\"Unlocks a cache file by deleting a directory.\"\"\"",
            "        try:",
            "            os.rmdir(fname)",
            "            return True",
            "        except OSError:",
            "            return False",
            "",
            "    @staticmethod",
            "    def delete_cache_file() -> None:",
            "        \"\"\"Deletes the cache file. Used by tests only.\"\"\"",
            "        parsed_url = urlsplit(OCSPCache.OCSP_RESPONSE_CACHE_URI)",
            "        fname = path.join(parsed_url.netloc, parsed_url.path)",
            "        OCSPCache.lock_cache_file(fname)",
            "        try:",
            "            logger.debug(f\"deleting cache file, used by tests only {fname}\")",
            "            os.unlink(fname)",
            "        finally:",
            "            OCSPCache.unlock_cache_file(fname)",
            "",
            "    @staticmethod",
            "    def clear_cache() -> None:",
            "        \"\"\"Clears cache.\"\"\"",
            "        OCSP_RESPONSE_VALIDATION_CACHE.clear()",
            "",
            "    @staticmethod",
            "    def cache_size():",
            "        \"\"\"Returns the cache's size.\"\"\"",
            "        return len(OCSP_RESPONSE_VALIDATION_CACHE)",
            "",
            "",
            "# Reset OCSP cache directory",
            "OCSPCache.reset_cache_dir()",
            "",
            "",
            "class SnowflakeOCSP:",
            "    \"\"\"OCSP validator using PyOpenSSL and asn1crypto/pyasn1.\"\"\"",
            "",
            "    # root certificate cache",
            "    ROOT_CERTIFICATES_DICT = {}  # root certificates",
            "",
            "    # root certificate cache lock",
            "    ROOT_CERTIFICATES_DICT_LOCK = RLock()",
            "",
            "    # cache object",
            "    OCSP_CACHE = OCSPCache()",
            "",
            "    OCSP_WHITELIST = re.compile(",
            "        r\"^\"",
            "        r\"(.*\\.snowflakecomputing(\\.[a-zA-Z]{1,63}){1,2}$\"",
            "        r\"|(?:|.*\\.)s3.*\\.amazonaws(\\.[a-zA-Z]{1,63}){1,2}$\"  # start with s3 or .s3 in the middle",
            "        r\"|.*\\.okta\\.com$\"",
            "        r\"|(?:|.*\\.)storage\\.googleapis\\.com$\"",
            "        r\"|.*\\.blob\\.core\\.windows\\.net$\"",
            "        r\"|.*\\.blob\\.core\\.usgovcloudapi\\.net$)\"",
            "    )",
            "",
            "    # Tolerable validity date range ratio. The OCSP response is valid up",
            "    # to (next update timestamp) + (next update timestamp -",
            "    # this update timestamp) * TOLERABLE_VALIDITY_RANGE_RATIO. This buffer",
            "    # yields some time for Root CA to update intermediate CA's certificate",
            "    # OCSP response. In fact, they don't update OCSP response in time. In Dec",
            "    # 2016, they left OCSP response expires for 5 hours at least, and it",
            "    # caused the connectivity issues in customers.",
            "    # With this buffer, about 2 days are given for 180 days validity date.",
            "    TOLERABLE_VALIDITY_RANGE_RATIO = 0.01",
            "",
            "    # Maximum clock skew in seconds (15 minutes) allowed when checking",
            "    # validity of OCSP responses",
            "    MAX_CLOCK_SKEW = 900",
            "",
            "    # Epoch time",
            "    ZERO_EPOCH = datetime.fromtimestamp(0, timezone.utc).replace(tzinfo=None)",
            "",
            "    # Timestamp format for logging",
            "    OUTPUT_TIMESTAMP_FORMAT = \"%Y-%m-%d %H:%M:%SZ\"",
            "",
            "    # Connection timeout in seconds for CA OCSP Responder",
            "    CA_OCSP_RESPONDER_CONNECTION_TIMEOUT = 10",
            "",
            "    # Connection timeout in seconds for Cache Server",
            "    OCSP_CACHE_SERVER_CONNECTION_TIMEOUT = 5",
            "",
            "    # MAX number of connection retry attempts with Responder in Fail Open",
            "    CA_OCSP_RESPONDER_MAX_RETRY_FO = 1",
            "",
            "    # MAX number of connection retry attempts with Responder in Fail Close",
            "    CA_OCSP_RESPONDER_MAX_RETRY_FC = 3",
            "",
            "    # MAX number of connection retry attempts with Cache Server",
            "    OCSP_CACHE_SERVER_MAX_RETRY = 1",
            "",
            "    def __init__(",
            "        self,",
            "        ocsp_response_cache_uri=None,",
            "        use_ocsp_cache_server=None,",
            "        use_post_method: bool = True,",
            "        use_fail_open: bool = True,",
            "        **kwargs,",
            "    ) -> None:",
            "        self.test_mode = os.getenv(\"SF_OCSP_TEST_MODE\", None)",
            "",
            "        if self.test_mode == \"true\":",
            "            logger.debug(\"WARNING - DRIVER CONFIGURED IN TEST MODE\")",
            "",
            "        self._use_post_method = use_post_method",
            "        self.OCSP_CACHE_SERVER = OCSPServer(",
            "            top_level_domain=extract_top_level_domain_from_hostname(",
            "                kwargs.pop(\"hostname\", None)",
            "            )",
            "        )",
            "",
            "        self.debug_ocsp_failure_url = None",
            "",
            "        if os.getenv(\"SF_OCSP_FAIL_OPEN\") is not None:",
            "            # failOpen Env Variable is for internal usage/ testing only.",
            "            # Using it in production is not advised and not supported.",
            "            self.FAIL_OPEN = os.getenv(\"SF_OCSP_FAIL_OPEN\").lower() == \"true\"",
            "        else:",
            "            self.FAIL_OPEN = use_fail_open",
            "",
            "        SnowflakeOCSP.OCSP_CACHE.reset_ocsp_response_cache_uri(ocsp_response_cache_uri)",
            "",
            "        if not OCSPServer.is_enabled_new_ocsp_endpoint():",
            "            self.OCSP_CACHE_SERVER.reset_ocsp_dynamic_cache_server_url(",
            "                use_ocsp_cache_server",
            "            )",
            "",
            "        \"\"\"",
            "        Here we have a two-layer cache design:",
            "",
            "        The upper layer is the OCSP_RESPONSE_VALIDATION_CACHE which caches not only the ocsp responses but also",
            "        the validation result of ocsp responses. This will be both in-memory and in-file (if program has the right",
            "        to read and write files).",
            "",
            "        The bottom layer is the ocsp responses in the form of a json file which are either",
            "        retrieved from Snowflake cache service or locally maintained by writing OCSP_RESPONSE_VALIDATION_CACHE back",
            "        to the json file for any updates (certificate revoked, cache expired, etc.). This will be in-file.",
            "",
            "        The cache logic is as following:",
            "        1. The OCSP_RESPONSE_VALIDATION_CACHE will be loaded from disk first during module loading period.",
            "        2. If there's no content loaded either due to no cache file or all cache expired, then we try load ocsp",
            "         response cache file. We will parse the content in the ocsp response cache json file, and",
            "         then update OCSP_RESPONSE_VALIDATION_CACHE.",
            "        3. When validating certs, we will first check OCSP_RESPONSE_VALIDATION_CACHE, if cache is not found,",
            "         when we will validate against the OCSP servers and cache the results.",
            "        4. After validating all the certs, we save OCSP_RESPONSE_VALIDATION_CACHE and ocsp response json",
            "         onto disk.",
            "        \"\"\"",
            "        if not OCSP_RESPONSE_VALIDATION_CACHE:",
            "            SnowflakeOCSP.OCSP_CACHE.read_file(self)",
            "",
            "    def validate_certfile(self, cert_filename, no_exception: bool = False):",
            "        \"\"\"Validates that the certificate is NOT revoked.\"\"\"",
            "        cert_map = {}",
            "        telemetry_data = OCSPTelemetryData()",
            "        telemetry_data.set_cache_enabled(self.OCSP_CACHE_SERVER.CACHE_SERVER_ENABLED)",
            "        telemetry_data.set_insecure_mode(False)",
            "        telemetry_data.set_sfc_peer_host(cert_filename)",
            "        telemetry_data.set_fail_open(self.is_enabled_fail_open())",
            "        try:",
            "            self.read_cert_bundle(cert_filename, cert_map)",
            "            cert_data = self.create_pair_issuer_subject(cert_map)",
            "        except Exception as ex:",
            "            logger.debug(\"Caught exception while validating certfile %s\", str(ex))",
            "            raise ex",
            "",
            "        return self._validate(",
            "            None, cert_data, telemetry_data, do_retry=False, no_exception=no_exception",
            "        )",
            "",
            "    def validate(",
            "        self,",
            "        hostname: str | None,",
            "        connection: Connection,",
            "        no_exception: bool = False,",
            "    ) -> (",
            "        list[",
            "            tuple[",
            "                Exception | None,",
            "                Certificate,",
            "                Certificate,",
            "                CertId,",
            "                str | bytes,",
            "            ]",
            "        ]",
            "        | None",
            "    ):",
            "        \"\"\"Validates the certificate is not revoked using OCSP.\"\"\"",
            "        logger.debug(\"validating certificate: %s\", hostname)",
            "",
            "        do_retry = SnowflakeOCSP.get_ocsp_retry_choice()",
            "",
            "        m = not SnowflakeOCSP.OCSP_WHITELIST.match(hostname)",
            "        if m or hostname.startswith(\"ocspssd\"):",
            "            logger.debug(\"skipping OCSP check: %s\", hostname)",
            "            return [None, None, None, None, None]",
            "",
            "        if OCSPServer.is_enabled_new_ocsp_endpoint():",
            "            self.OCSP_CACHE_SERVER.reset_ocsp_endpoint(hostname)",
            "",
            "        telemetry_data = OCSPTelemetryData()",
            "        telemetry_data.set_cache_enabled(self.OCSP_CACHE_SERVER.CACHE_SERVER_ENABLED)",
            "        telemetry_data.set_insecure_mode(False)",
            "        telemetry_data.set_sfc_peer_host(hostname)",
            "        telemetry_data.set_fail_open(self.is_enabled_fail_open())",
            "",
            "        try:",
            "            cert_data = self.extract_certificate_chain(connection)",
            "        except RevocationCheckError:",
            "            telemetry_data.set_event_sub_type(",
            "                OCSPTelemetryData.CERTIFICATE_EXTRACTION_FAILED",
            "            )",
            "            logger.debug(",
            "                telemetry_data.generate_telemetry_data(\"RevocationCheckFailure\")",
            "            )",
            "            return None",
            "",
            "        return self._validate(",
            "            hostname, cert_data, telemetry_data, do_retry, no_exception",
            "        )",
            "",
            "    def _validate(",
            "        self,",
            "        hostname: str | None,",
            "        cert_data: list[tuple[Certificate, Certificate]],",
            "        telemetry_data: OCSPTelemetryData,",
            "        do_retry: bool = True,",
            "        no_exception: bool = False,",
            "    ) -> list[tuple[Exception | None, Certificate, Certificate, CertId, bytes]]:",
            "        \"\"\"Validate certs sequentially if OCSP response cache server is used.\"\"\"",
            "        results = self._validate_certificates_sequential(",
            "            cert_data, telemetry_data, hostname, do_retry=do_retry",
            "        )",
            "",
            "        SnowflakeOCSP.OCSP_CACHE.update_file(self)",
            "",
            "        any_err = False",
            "        for err, _, _, _, _ in results:",
            "            if isinstance(err, RevocationCheckError):",
            "                err.msg += f\" for {hostname}\"",
            "            if not no_exception and err is not None:",
            "                raise err",
            "            elif err is not None:",
            "                any_err = True",
            "",
            "        logger.debug(\"ok\" if not any_err else \"failed\")",
            "        return results",
            "",
            "    @staticmethod",
            "    def get_ocsp_retry_choice() -> bool:",
            "        return os.getenv(\"SF_OCSP_DO_RETRY\", \"true\") == \"true\"",
            "",
            "    def is_cert_id_in_cache(",
            "        self, cert_id: CertId, subject: Certificate | None, **kwargs: Any",
            "    ):",
            "        \"\"\"Decides whether OCSP CertID is in cache.",
            "",
            "        Args:",
            "            cert_id: OCSP CertID.",
            "            subject: Subject certificate.",
            "",
            "        Returns:",
            "            True if in cache otherwise False, followed by the cached OCSP Response.",
            "        \"\"\"",
            "        found, cache = SnowflakeOCSP.OCSP_CACHE.find_cache(",
            "            self, cert_id, subject, **kwargs",
            "        )",
            "        return found, cache",
            "",
            "    def get_account_from_hostname(self, hostname: str) -> str:",
            "        \"\"\"Extracts the account name part from the hostname.",
            "",
            "        Args:",
            "            hostname: Hostname that account name is in.",
            "",
            "        Returns:",
            "            The extracted account name.",
            "        \"\"\"",
            "        split_hname = hostname.split(\".\")",
            "        if \"global\" in split_hname:",
            "            acc_name = split_hname[0].split(\"-\")[0]",
            "        else:",
            "            acc_name = split_hname[0]",
            "        return acc_name",
            "",
            "    def is_enabled_fail_open(self) -> bool:",
            "        return self.FAIL_OPEN",
            "",
            "    @staticmethod",
            "    def print_fail_open_warning(ocsp_log) -> None:",
            "        static_warning = (",
            "            \"WARNING!!! Using fail-open to connect. Driver is connecting to an \"",
            "            \"HTTPS endpoint without OCSP based Certificate Revocation checking \"",
            "            \"as it could not obtain a valid OCSP Response to use from the CA OCSP \"",
            "            \"responder. Details:\"",
            "        )",
            "        ocsp_warning = f\"{static_warning} \\n {ocsp_log}\"",
            "        logger.warning(ocsp_warning)",
            "",
            "    def validate_by_direct_connection(",
            "        self,",
            "        issuer: Certificate,",
            "        subject: Certificate,",
            "        telemetry_data: OCSPTelemetryData,",
            "        hostname: str = None,",
            "        do_retry: bool = True,",
            "        **kwargs: Any,",
            "    ) -> tuple[Exception | None, Certificate, Certificate, CertId, bytes]:",
            "        cert_id, req = self.create_ocsp_request(issuer, subject)",
            "        cache_status, ocsp_response = self.is_cert_id_in_cache(",
            "            cert_id, subject, **kwargs",
            "        )",
            "",
            "        try:",
            "            if not cache_status:",
            "                telemetry_data.set_cache_hit(False)",
            "                logger.debug(\"getting OCSP response from CA's OCSP server\")",
            "                ocsp_response = self._fetch_ocsp_response(",
            "                    req, subject, cert_id, telemetry_data, hostname, do_retry",
            "                )",
            "            else:",
            "                ocsp_url = self.extract_ocsp_url(subject)",
            "                cert_id_enc = self.encode_cert_id_base64(",
            "                    self.decode_cert_id_key(cert_id)",
            "                )",
            "                telemetry_data.set_cache_hit(True)",
            "                self.debug_ocsp_failure_url = SnowflakeOCSP.create_ocsp_debug_info(",
            "                    self, req, ocsp_url",
            "                )",
            "                telemetry_data.set_ocsp_url(ocsp_url)",
            "                telemetry_data.set_ocsp_req(req)",
            "                telemetry_data.set_cert_id(cert_id_enc)",
            "                logger.debug(\"using OCSP response cache\")",
            "",
            "            if not ocsp_response:",
            "                telemetry_data.set_event_sub_type(",
            "                    OCSPTelemetryData.OCSP_RESPONSE_UNAVAILABLE",
            "                )",
            "                raise RevocationCheckError(",
            "                    msg=\"Could not retrieve OCSP Response. Cannot perform Revocation Check\",",
            "                    errno=ER_OCSP_RESPONSE_UNAVAILABLE,",
            "                )",
            "            try:",
            "                self.process_ocsp_response(issuer, cert_id, ocsp_response)",
            "                err = None",
            "            except RevocationCheckError as op_er:",
            "                telemetry_data.set_event_sub_type(",
            "                    OCSPTelemetryData.ERROR_CODE_MAP[op_er.errno]",
            "                )",
            "                raise op_er",
            "",
            "        except RevocationCheckError as rce:",
            "            telemetry_data.set_error_msg(rce.msg)",
            "            err = self.verify_fail_open(rce, telemetry_data)",
            "",
            "        except Exception as ex:",
            "            logger.debug(\"OCSP Validation failed %s\", str(ex))",
            "            telemetry_data.set_error_msg(str(ex))",
            "            err = self.verify_fail_open(ex, telemetry_data)",
            "            SnowflakeOCSP.OCSP_CACHE.delete_cache(self, cert_id)",
            "",
            "        return err, issuer, subject, cert_id, ocsp_response",
            "",
            "    def verify_fail_open(self, ex_obj, telemetry_data):",
            "        if not self.is_enabled_fail_open():",
            "            if ex_obj.errno is ER_OCSP_RESPONSE_CERT_STATUS_REVOKED:",
            "                logger.debug(",
            "                    telemetry_data.generate_telemetry_data(",
            "                        \"RevokedCertificateError\", True",
            "                    )",
            "                )",
            "            else:",
            "                logger.debug(",
            "                    telemetry_data.generate_telemetry_data(\"RevocationCheckFailure\")",
            "                )",
            "            return ex_obj",
            "        else:",
            "            if ex_obj.errno is ER_OCSP_RESPONSE_CERT_STATUS_REVOKED:",
            "                logger.debug(",
            "                    telemetry_data.generate_telemetry_data(",
            "                        \"RevokedCertificateError\", True",
            "                    )",
            "                )",
            "                return ex_obj",
            "            else:",
            "                SnowflakeOCSP.print_fail_open_warning(",
            "                    telemetry_data.generate_telemetry_data(\"RevocationCheckFailure\")",
            "                )",
            "                return None",
            "",
            "    def _validate_certificates_sequential(",
            "        self,",
            "        cert_data: list[tuple[Certificate, Certificate]],",
            "        telemetry_data: OCSPTelemetryData,",
            "        hostname: str | None = None,",
            "        do_retry: bool = True,",
            "    ) -> list[tuple[Exception | None, Certificate, Certificate, CertId, bytes]]:",
            "        results = []",
            "        try:",
            "            self._check_ocsp_response_cache_server(cert_data)",
            "        except RevocationCheckError as rce:",
            "            telemetry_data.set_event_sub_type(",
            "                OCSPTelemetryData.ERROR_CODE_MAP[rce.errno]",
            "            )",
            "        except Exception as ex:",
            "            logger.debug(",
            "                \"Caught unknown exception - %s. Continue to validate by direct connection\",",
            "                str(ex),",
            "            )",
            "",
            "        to_update_cache_dict = {}",
            "        for issuer, subject in cert_data:",
            "            cert_id, _ = self.create_ocsp_request(issuer=issuer, subject=subject)",
            "            cache_key = self.decode_cert_id_key(cert_id)",
            "            ocsp_response_validation_result = OCSP_RESPONSE_VALIDATION_CACHE.get(",
            "                cache_key",
            "            )",
            "",
            "            if (",
            "                ocsp_response_validation_result is None",
            "                or not ocsp_response_validation_result.validated",
            "            ):",
            "                # r is a tuple of (err, issuer, subject, cert_id, ocsp_response)",
            "                r = self.validate_by_direct_connection(",
            "                    issuer,",
            "                    subject,",
            "                    telemetry_data,",
            "                    hostname,",
            "                    do_retry=do_retry,",
            "                    cache_key=cache_key,",
            "                )",
            "",
            "                # When OCSP server is down, the validation fails and the oscp_response will be None, and in fail open",
            "                # case, we will also reset err to None.",
            "                # In this case we don't need to write the response to cache because there is no information from a",
            "                # connection error.",
            "                if r[0] is not None or r[4] is not None:",
            "                    to_update_cache_dict[cache_key] = OCSPResponseValidationResult(",
            "                        *r,",
            "                        ts=int(time.time()),",
            "                        validated=True,",
            "                    )",
            "                    OCSPCache.CACHE_UPDATED = True",
            "                results.append(r)",
            "            else:",
            "                results.append(",
            "                    (",
            "                        ocsp_response_validation_result.exception,",
            "                        ocsp_response_validation_result.issuer,",
            "                        ocsp_response_validation_result.subject,",
            "                        ocsp_response_validation_result.cert_id,",
            "                        ocsp_response_validation_result.ocsp_response,",
            "                    )",
            "                )",
            "        OCSP_RESPONSE_VALIDATION_CACHE.update(to_update_cache_dict)",
            "        return results",
            "",
            "    def _check_ocsp_response_cache_server(",
            "        self,",
            "        cert_data: list[tuple[Certificate, Certificate]],",
            "    ) -> None:",
            "        \"\"\"Checks if OCSP response is in cache, and if not it downloads the OCSP response cache from the server.",
            "",
            "        Args:",
            "          cert_data: Tuple of issuer and subject certificates.",
            "        \"\"\"",
            "        in_cache = False",
            "        for issuer, subject in cert_data:",
            "            # check if any OCSP response is NOT in cache",
            "            cert_id, _ = self.create_ocsp_request(issuer, subject)",
            "            in_cache, _ = SnowflakeOCSP.OCSP_CACHE.find_cache(self, cert_id, subject)",
            "            if not in_cache:",
            "                # not found any",
            "                break",
            "",
            "        if not in_cache:",
            "            self.OCSP_CACHE_SERVER.download_cache_from_server(self)",
            "",
            "    def _lazy_read_ca_bundle(self) -> None:",
            "        \"\"\"Reads the local cabundle file and cache it in memory.\"\"\"",
            "        with SnowflakeOCSP.ROOT_CERTIFICATES_DICT_LOCK:",
            "            if SnowflakeOCSP.ROOT_CERTIFICATES_DICT:",
            "                # return if already loaded",
            "                return",
            "",
            "            try:",
            "                ca_bundle = environ.get(\"REQUESTS_CA_BUNDLE\") or environ.get(",
            "                    \"CURL_CA_BUNDLE\"",
            "                )",
            "                if ca_bundle and path.exists(ca_bundle):",
            "                    # if the user/application specifies cabundle.",
            "                    self.read_cert_bundle(ca_bundle)",
            "                else:",
            "                    import sys",
            "",
            "                    # This import that depends on these libraries is to import certificates from them,",
            "                    # we would like to have these as up to date as possible.",
            "                    from requests import certs",
            "",
            "                    if (",
            "                        hasattr(certs, \"__file__\")",
            "                        and path.exists(certs.__file__)",
            "                        and path.exists(",
            "                            path.join(path.dirname(certs.__file__), \"cacert.pem\")",
            "                        )",
            "                    ):",
            "                        # if cacert.pem exists next to certs.py in request",
            "                        # package.",
            "                        ca_bundle = path.join(",
            "                            path.dirname(certs.__file__), \"cacert.pem\"",
            "                        )",
            "                        self.read_cert_bundle(ca_bundle)",
            "                    elif hasattr(sys, \"_MEIPASS\"):",
            "                        # if pyinstaller includes cacert.pem",
            "                        cabundle_candidates = [",
            "                            [\"botocore\", \"vendored\", \"requests\", \"cacert.pem\"],",
            "                            [\"requests\", \"cacert.pem\"],",
            "                            [\"cacert.pem\"],",
            "                        ]",
            "                        for filename in cabundle_candidates:",
            "                            ca_bundle = path.join(sys._MEIPASS, *filename)",
            "                            if path.exists(ca_bundle):",
            "                                self.read_cert_bundle(ca_bundle)",
            "                                break",
            "                        else:",
            "                            logger.error(\"No cabundle file is found in _MEIPASS\")",
            "                    try:",
            "                        import certifi",
            "",
            "                        self.read_cert_bundle(certifi.where())",
            "                    except Exception:",
            "                        logger.debug(\"no certifi is installed. ignored.\")",
            "",
            "            except Exception as e:",
            "                logger.error(\"Failed to read ca_bundle: %s\", e)",
            "",
            "            if not SnowflakeOCSP.ROOT_CERTIFICATES_DICT:",
            "                logger.error(",
            "                    \"No CA bundle file is found in the system. \"",
            "                    \"Set REQUESTS_CA_BUNDLE to the file.\"",
            "                )",
            "",
            "    @staticmethod",
            "    def _calculate_tolerable_validity(this_update: float, next_update: float) -> int:",
            "        return max(",
            "            int(",
            "                SnowflakeOCSP.TOLERABLE_VALIDITY_RANGE_RATIO",
            "                * (next_update - this_update)",
            "            ),",
            "            SnowflakeOCSP.MAX_CLOCK_SKEW,",
            "        )",
            "",
            "    @staticmethod",
            "    def _is_validaity_range(",
            "        current_time: int,",
            "        this_update: float,",
            "        next_update: float,",
            "        test_mode: Any | None = None,",
            "    ) -> bool:",
            "        if test_mode is not None:",
            "            force_validity_fail = os.getenv(\"SF_TEST_OCSP_FORCE_BAD_RESPONSE_VALIDITY\")",
            "            if force_validity_fail is not None:",
            "                return False",
            "",
            "        tolerable_validity = SnowflakeOCSP._calculate_tolerable_validity(",
            "            this_update, next_update",
            "        )",
            "        return (",
            "            this_update - SnowflakeOCSP.MAX_CLOCK_SKEW",
            "            <= current_time",
            "            <= next_update + tolerable_validity",
            "        )",
            "",
            "    @staticmethod",
            "    def _validity_error_message(current_time, this_update, next_update) -> str:",
            "        tolerable_validity = SnowflakeOCSP._calculate_tolerable_validity(",
            "            this_update, next_update",
            "        )",
            "        return (",
            "            \"Response is unreliable. Its validity \"",
            "            \"date is out of range: current_time={}, \"",
            "            \"this_update={}, next_update={}, \"",
            "            \"tolerable next_update={}. A potential cause is \"",
            "            \"client clock is skewed, CA fails to update OCSP \"",
            "            \"response in time.\".format(",
            "                strftime(SnowflakeOCSP.OUTPUT_TIMESTAMP_FORMAT, gmtime(current_time)),",
            "                strftime(SnowflakeOCSP.OUTPUT_TIMESTAMP_FORMAT, gmtime(this_update)),",
            "                strftime(SnowflakeOCSP.OUTPUT_TIMESTAMP_FORMAT, gmtime(next_update)),",
            "                strftime(",
            "                    SnowflakeOCSP.OUTPUT_TIMESTAMP_FORMAT,",
            "                    gmtime(next_update + tolerable_validity),",
            "                ),",
            "            )",
            "        )",
            "",
            "    @staticmethod",
            "    def clear_cache() -> None:",
            "        SnowflakeOCSP.OCSP_CACHE.clear_cache()",
            "",
            "    @staticmethod",
            "    def cache_size():",
            "        return SnowflakeOCSP.OCSP_CACHE.cache_size()",
            "",
            "    @staticmethod",
            "    def delete_cache_file() -> None:",
            "        SnowflakeOCSP.OCSP_CACHE.delete_cache_file()",
            "",
            "    @staticmethod",
            "    def create_ocsp_debug_info(ocsp, ocsp_request, ocsp_url):",
            "        b64data = ocsp.decode_ocsp_request_b64(ocsp_request)",
            "        target_url = f\"{ocsp_url}/{b64data}\"",
            "        return target_url",
            "",
            "    def _fetch_ocsp_response(",
            "        self,",
            "        ocsp_request,",
            "        subject,",
            "        cert_id,",
            "        telemetry_data,",
            "        hostname=None,",
            "        do_retry: bool = True,",
            "    ):",
            "        \"\"\"Fetches OCSP response using OCSPRequest.\"\"\"",
            "        sf_timeout = SnowflakeOCSP.CA_OCSP_RESPONDER_CONNECTION_TIMEOUT",
            "        ocsp_url = self.extract_ocsp_url(subject)",
            "        cert_id_enc = self.encode_cert_id_base64(self.decode_cert_id_key(cert_id))",
            "        if not ocsp_url:",
            "            telemetry_data.set_event_sub_type(OCSPTelemetryData.OCSP_URL_MISSING)",
            "            raise RevocationCheckError(",
            "                msg=\"No OCSP URL found in cert. Cannot perform Certificate Revocation check\",",
            "                errno=ER_OCSP_URL_INFO_MISSING,",
            "            )",
            "        headers = {HTTP_HEADER_USER_AGENT: PYTHON_CONNECTOR_USER_AGENT}",
            "",
            "        if not OCSPServer.is_enabled_new_ocsp_endpoint():",
            "            actual_method = \"post\" if self._use_post_method else \"get\"",
            "            if self.OCSP_CACHE_SERVER.OCSP_RETRY_URL:",
            "                # no POST is supported for Retry URL at the moment.",
            "                actual_method = \"get\"",
            "",
            "            if actual_method == \"get\":",
            "                b64data = self.decode_ocsp_request_b64(ocsp_request)",
            "                target_url = self.OCSP_CACHE_SERVER.generate_get_url(ocsp_url, b64data)",
            "                payload = None",
            "            else:",
            "                target_url = ocsp_url",
            "                payload = self.decode_ocsp_request(ocsp_request)",
            "                headers[\"Content-Type\"] = \"application/ocsp-request\"",
            "        else:",
            "            actual_method = \"post\"",
            "            target_url = self.OCSP_CACHE_SERVER.OCSP_RETRY_URL",
            "            ocsp_req_enc = self.decode_ocsp_request_b64(ocsp_request)",
            "",
            "            payload = json.dumps(",
            "                {",
            "                    \"hostname\": hostname,",
            "                    \"ocsp_request\": ocsp_req_enc,",
            "                    \"cert_id\": cert_id_enc,",
            "                    \"ocsp_responder_url\": ocsp_url,",
            "                }",
            "            )",
            "            headers[\"Content-Type\"] = \"application/json\"",
            "",
            "        telemetry_data.set_ocsp_connection_method(actual_method)",
            "        if self.test_mode is not None:",
            "            logger.debug(\"WARNING - DRIVER IS CONFIGURED IN TESTMODE.\")",
            "            test_ocsp_url = os.getenv(\"SF_TEST_OCSP_URL\", None)",
            "            test_timeout = os.getenv(",
            "                \"SF_TEST_CA_OCSP_RESPONDER_CONNECTION_TIMEOUT\", None",
            "            )",
            "            if test_timeout is not None:",
            "                sf_timeout = int(test_timeout)",
            "            if test_ocsp_url is not None:",
            "                target_url = test_ocsp_url",
            "",
            "        self.debug_ocsp_failure_url = SnowflakeOCSP.create_ocsp_debug_info(",
            "            self, ocsp_request, ocsp_url",
            "        )",
            "        telemetry_data.set_ocsp_req(self.decode_ocsp_request_b64(ocsp_request))",
            "        telemetry_data.set_ocsp_url(ocsp_url)",
            "        telemetry_data.set_cert_id(cert_id_enc)",
            "",
            "        ret = None",
            "        logger.debug(\"url: %s\", target_url)",
            "        sf_max_retry = SnowflakeOCSP.CA_OCSP_RESPONDER_MAX_RETRY_FO",
            "        if not self.is_enabled_fail_open():",
            "            sf_max_retry = SnowflakeOCSP.CA_OCSP_RESPONDER_MAX_RETRY_FC",
            "",
            "        with generic_requests.Session() as session:",
            "            max_retry = sf_max_retry if do_retry else 1",
            "            sleep_time = 1",
            "            backoff = exponential_backoff()()",
            "            for _ in range(max_retry):",
            "                try:",
            "                    response = session.request(",
            "                        headers=headers,",
            "                        method=actual_method,",
            "                        url=target_url,",
            "                        timeout=sf_timeout,",
            "                        data=payload,",
            "                    )",
            "                    if response.status_code == OK:",
            "                        logger.debug(",
            "                            \"OCSP response was successfully returned from OCSP \"",
            "                            \"server.\"",
            "                        )",
            "                        ret = response.content",
            "                        break",
            "                    elif max_retry > 1:",
            "                        sleep_time = next(backoff)",
            "                        logger.debug(",
            "                            \"OCSP server returned %s. Retrying in %s(s)\",",
            "                            response.status_code,",
            "                            sleep_time,",
            "                        )",
            "                    time.sleep(sleep_time)",
            "                except Exception as ex:",
            "                    if max_retry > 1:",
            "                        sleep_time = next(backoff)",
            "                        logger.debug(",
            "                            \"Could not fetch OCSP Response from server\"",
            "                            \"Retrying in %s(s)\",",
            "                            sleep_time,",
            "                        )",
            "                        time.sleep(sleep_time)",
            "                    else:",
            "                        telemetry_data.set_event_sub_type(",
            "                            OCSPTelemetryData.OCSP_RESPONSE_FETCH_EXCEPTION",
            "                        )",
            "                        raise RevocationCheckError(",
            "                            msg=\"Could not fetch OCSP Response from server. Consider\"",
            "                            \"checking your whitelists : Exception - {}\".format(str(ex)),",
            "                            errno=ER_OCSP_RESPONSE_FETCH_EXCEPTION,",
            "                        )",
            "            else:",
            "                logger.error(",
            "                    \"Failed to get OCSP response after {} attempt. Consider checking \"",
            "                    \"for OCSP URLs being blocked\".format(max_retry)",
            "                )",
            "                telemetry_data.set_event_sub_type(",
            "                    OCSPTelemetryData.OCSP_RESPONSE_FETCH_FAILURE",
            "                )",
            "                raise RevocationCheckError(",
            "                    msg=\"Failed to get OCSP response after {} attempt.\".format(",
            "                        max_retry",
            "                    ),",
            "                    errno=ER_OCSP_RESPONSE_FETCH_FAILURE,",
            "                )",
            "",
            "        return ret",
            "",
            "    def _process_good_status(",
            "        self, single_response: SingleResponse, cert_id: CertId, ocsp_response: bytes",
            "    ) -> None:",
            "        \"\"\"Processes GOOD status.\"\"\"",
            "        current_time = int(time.time())",
            "        this_update_native, next_update_native = self.extract_good_status(",
            "            single_response",
            "        )",
            "",
            "        if this_update_native is None or next_update_native is None:",
            "            raise RevocationCheckError(",
            "                msg=\"Either this update or next \"",
            "                \"update is None. this_update: {}, next_update: {}\".format(",
            "                    this_update_native, next_update_native",
            "                ),",
            "                errno=ER_OCSP_RESPONSE_INVALID_EXPIRY_INFO_MISSING,",
            "            )",
            "",
            "        this_update = (",
            "            this_update_native.replace(tzinfo=None) - SnowflakeOCSP.ZERO_EPOCH",
            "        ).total_seconds()",
            "        next_update = (",
            "            next_update_native.replace(tzinfo=None) - SnowflakeOCSP.ZERO_EPOCH",
            "        ).total_seconds()",
            "        if not SnowflakeOCSP._is_validaity_range(",
            "            current_time, this_update, next_update, self.test_mode",
            "        ):",
            "            raise RevocationCheckError(",
            "                msg=SnowflakeOCSP._validity_error_message(",
            "                    current_time, this_update, next_update",
            "                ),",
            "                errno=ER_OCSP_RESPONSE_EXPIRED,",
            "            )",
            "",
            "    def _process_revoked_status(self, single_response, cert_id):",
            "        \"\"\"Processes REVOKED status.\"\"\"",
            "        current_time = int(time.time())",
            "        if self.test_mode is not None:",
            "            test_cert_status = os.getenv(\"SF_TEST_OCSP_CERT_STATUS\")",
            "            if test_cert_status == \"revoked\":",
            "                raise RevocationCheckError(",
            "                    msg=\"The certificate has been revoked: current_time={}, \"",
            "                    \"revocation_time={}, reason={}\".format(",
            "                        strftime(",
            "                            SnowflakeOCSP.OUTPUT_TIMESTAMP_FORMAT, gmtime(current_time)",
            "                        ),",
            "                        strftime(",
            "                            SnowflakeOCSP.OUTPUT_TIMESTAMP_FORMAT, gmtime(current_time)",
            "                        ),",
            "                        \"Force Revoke\",",
            "                    ),",
            "                    errno=ER_OCSP_RESPONSE_CERT_STATUS_REVOKED,",
            "                )",
            "",
            "        SnowflakeOCSP.OCSP_CACHE.delete_cache(self, cert_id)",
            "        revocation_time, revocation_reason = self.extract_revoked_status(",
            "            single_response",
            "        )",
            "        raise RevocationCheckError(",
            "            msg=\"The certificate has been revoked: current_time={}, \"",
            "            \"revocation_time={}, reason={}\".format(",
            "                strftime(SnowflakeOCSP.OUTPUT_TIMESTAMP_FORMAT, gmtime(current_time)),",
            "                revocation_time.strftime(SnowflakeOCSP.OUTPUT_TIMESTAMP_FORMAT),",
            "                revocation_reason,",
            "            ),",
            "            errno=ER_OCSP_RESPONSE_CERT_STATUS_REVOKED,",
            "        )",
            "",
            "    def _process_unknown_status(self, cert_id):",
            "        \"\"\"Processes UNKNOWN status.\"\"\"",
            "        SnowflakeOCSP.OCSP_CACHE.delete_cache(self, cert_id)",
            "        raise RevocationCheckError(",
            "            msg=\"The certificate is in UNKNOWN revocation status.\",",
            "            errno=ER_OCSP_RESPONSE_CERT_STATUS_UNKNOWN,",
            "        )",
            "",
            "    def decode_ocsp_response_cache(self, ocsp_response_cache_json):",
            "        \"\"\"Decodes OCSP response cache from JSON.\"\"\"",
            "        try:",
            "            with OCSP_RESPONSE_VALIDATION_CACHE._lock:",
            "                new_cache_dict = {}",
            "                for cert_id_base64, (",
            "                    ts,",
            "                    ocsp_response,",
            "                ) in ocsp_response_cache_json.items():",
            "                    cert_id = self.decode_cert_id_base64(cert_id_base64)",
            "                    b64decoded_ocsp_response = b64decode(ocsp_response)",
            "                    if not self.is_valid_time(cert_id, b64decoded_ocsp_response):",
            "                        continue",
            "                    current_time = int(time.time())",
            "                    cache_key: tuple[bytes, bytes, bytes] = self.decode_cert_id_key(",
            "                        cert_id",
            "                    )",
            "                    found, _ = OCSPCache.find_cache(",
            "                        self, cert_id, None, cache_key=cache_key, lock_cache=False",
            "                    )",
            "                    if OCSPCache.is_cache_fresh(current_time, ts):",
            "                        new_cache_dict[cache_key] = OCSPResponseValidationResult(",
            "                            ocsp_response=b64decoded_ocsp_response,",
            "                            ts=current_time,",
            "                            validated=False,",
            "                        )",
            "                    elif found:",
            "                        OCSPCache.delete_cache(",
            "                            self, cert_id, cache_key=cache_key, lock_cache=False",
            "                        )",
            "            if new_cache_dict:",
            "                OCSP_RESPONSE_VALIDATION_CACHE.update(new_cache_dict)",
            "                OCSPCache.CACHE_UPDATED = True",
            "        except Exception as ex:",
            "            logger.debug(\"Caught here - %s\", ex)",
            "            ermsg = \"Exception raised while decoding OCSP Response Cache {}\".format(",
            "                str(ex)",
            "            )",
            "            raise RevocationCheckError(",
            "                msg=ermsg, errno=ER_OCSP_RESPONSE_CACHE_DECODE_FAILED",
            "            )",
            "",
            "    def encode_ocsp_response_cache(self, ocsp_response_cache_json) -> None:",
            "        \"\"\"Encodes OCSP response cache to JSON.\"\"\"",
            "        logger.debug(\"encoding OCSP response cache to JSON\")",
            "        for (",
            "            cache_key,",
            "            ocsp_response_validation_result,",
            "        ) in OCSP_RESPONSE_VALIDATION_CACHE.items():",
            "            k = self.encode_cert_id_base64(cache_key)",
            "            v = b64encode(ocsp_response_validation_result.ocsp_response).decode(\"ascii\")",
            "            ocsp_response_cache_json[k] = (ocsp_response_validation_result.ts, v)",
            "",
            "    def read_cert_bundle(self, ca_bundle_file, storage=None):",
            "        \"\"\"Reads a certificate file including certificates in PEM format.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def encode_cert_id_key(self, _):",
            "        \"\"\"Encodes Cert ID key to native CertID.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def decode_cert_id_key(self, _):",
            "        \"\"\"Decodes name CertID to Cert ID key.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def encode_cert_id_base64(self, hkey):",
            "        \"\"\"Encodes native CertID to base64 Cert ID.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def decode_cert_id_base64(self, cert_id_base64):",
            "        \"\"\"Decodes base64 Cert ID to native CertID.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def create_ocsp_request(",
            "        self,",
            "        issuer: Certificate,",
            "        subject: Certificate,",
            "    ) -> tuple[CertId, OCSPRequest]:",
            "        \"\"\"Creates CertId and OCSPRequest.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def extract_ocsp_url(self, cert):",
            "        \"\"\"Extracts OCSP URL from Certificate.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def decode_ocsp_request(self, ocsp_request):",
            "        \"\"\"Decodes OCSP request to DER.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def decode_ocsp_request_b64(self, ocsp_request):",
            "        \"\"\"Decodes OCSP Request object to b64.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def extract_good_status(self, single_response):",
            "        \"\"\"Extracts Revocation Status GOOD.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def extract_revoked_status(self, single_response):",
            "        \"\"\"Extracts Revocation Status REVOKED.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def process_ocsp_response(self, issuer, cert_id, ocsp_response):",
            "        \"\"\"Processes OCSP response.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def verify_signature(self, signature_algorithm, signature, cert, data):",
            "        \"\"\"Verifies signature.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def extract_certificate_chain(self, connection):",
            "        \"\"\"Gets certificate chain and extract the key info from OpenSSL connection.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def create_pair_issuer_subject(self, cert_map):",
            "        \"\"\"Creates pairs of issuer and subject certificates.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def subject_name(self, subject):",
            "        \"\"\"Gets human readable Subject name.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def is_valid_time(self, cert_id, ocsp_response):",
            "        \"\"\"Checks whether ocsp_response is in valid time range.\"\"\"",
            "        raise NotImplementedError"
        ],
        "afterPatchFile": [
            "#!/usr/bin/env python",
            "#",
            "# Copyright (c) 2012-2023 Snowflake Computing Inc. All rights reserved.",
            "#",
            "",
            "from __future__ import annotations",
            "",
            "import codecs",
            "import importlib",
            "import json",
            "import os",
            "import platform",
            "import re",
            "import sys",
            "import tempfile",
            "import time",
            "from base64 import b64decode, b64encode",
            "from datetime import datetime, timezone",
            "from logging import getLogger",
            "from os import environ, path",
            "from os.path import expanduser",
            "from threading import Lock, RLock",
            "from time import gmtime, strftime",
            "from typing import Any, NamedTuple",
            "",
            "# We use regular requests and urlib3 when we reach out to do OCSP checks, basically in this very narrow",
            "# part of the code where we want to call out to check for revoked certificates,",
            "# we don't want to use our hardened version of requests.",
            "import requests as generic_requests",
            "from asn1crypto.ocsp import CertId, OCSPRequest, SingleResponse",
            "from asn1crypto.x509 import Certificate",
            "from OpenSSL.SSL import Connection",
            "",
            "from snowflake.connector import SNOWFLAKE_CONNECTOR_VERSION",
            "from snowflake.connector.compat import OK, urlsplit, urlunparse",
            "from snowflake.connector.constants import HTTP_HEADER_USER_AGENT",
            "from snowflake.connector.errorcode import (",
            "    ER_INVALID_OCSP_RESPONSE_SSD,",
            "    ER_INVALID_SSD,",
            "    ER_OCSP_FAILED_TO_CONNECT_CACHE_SERVER,",
            "    ER_OCSP_RESPONSE_ATTACHED_CERT_EXPIRED,",
            "    ER_OCSP_RESPONSE_ATTACHED_CERT_INVALID,",
            "    ER_OCSP_RESPONSE_CACHE_DECODE_FAILED,",
            "    ER_OCSP_RESPONSE_CACHE_DOWNLOAD_FAILED,",
            "    ER_OCSP_RESPONSE_CERT_STATUS_INVALID,",
            "    ER_OCSP_RESPONSE_CERT_STATUS_REVOKED,",
            "    ER_OCSP_RESPONSE_CERT_STATUS_UNKNOWN,",
            "    ER_OCSP_RESPONSE_EXPIRED,",
            "    ER_OCSP_RESPONSE_FETCH_EXCEPTION,",
            "    ER_OCSP_RESPONSE_FETCH_FAILURE,",
            "    ER_OCSP_RESPONSE_INVALID_EXPIRY_INFO_MISSING,",
            "    ER_OCSP_RESPONSE_INVALID_SIGNATURE,",
            "    ER_OCSP_RESPONSE_LOAD_FAILURE,",
            "    ER_OCSP_RESPONSE_STATUS_UNSUCCESSFUL,",
            "    ER_OCSP_RESPONSE_UNAVAILABLE,",
            "    ER_OCSP_URL_INFO_MISSING,",
            ")",
            "from snowflake.connector.errors import RevocationCheckError",
            "from snowflake.connector.network import PYTHON_CONNECTOR_USER_AGENT",
            "",
            "from . import constants",
            "from .backoff_policies import exponential_backoff",
            "from .cache import CacheEntry, SFDictCache, SFDictFileCache",
            "from .telemetry import TelemetryField, generate_telemetry_data_dict",
            "from .url_util import extract_top_level_domain_from_hostname, url_encode_str",
            "from .util_text import _base64_bytes_to_str",
            "",
            "",
            "class OCSPResponseValidationResult(NamedTuple):",
            "    exception: Exception | None = None",
            "    issuer: Certificate | None = None",
            "    subject: Certificate | None = None",
            "    cert_id: CertId | None = None",
            "    ocsp_response: bytes | None = None",
            "    ts: int | None = None",
            "    validated: bool = False",
            "",
            "    def _serialize(self):",
            "        def serialize_exception(exc):",
            "            # serialization exception is not supported for all exceptions",
            "            # in the ocsp_snowflake.py, most exceptions are RevocationCheckError which is easy to serialize.",
            "            # however, it would require non-trivial effort to serialize other exceptions especially 3rd part errors",
            "            # as there can be un-serializable members and nondeterministic constructor arguments.",
            "            # here we do a general best efforts serialization for other exceptions recording only the error message.",
            "            if not exc:",
            "                return None",
            "",
            "            exc_type = type(exc)",
            "            ret = {\"class\": exc_type.__name__, \"module\": exc_type.__module__}",
            "            if isinstance(exc, RevocationCheckError):",
            "                ret.update({\"errno\": exc.errno, \"msg\": exc.raw_msg})",
            "            else:",
            "                ret.update({\"msg\": str(exc)})",
            "            return ret",
            "",
            "        return json.dumps(",
            "            {",
            "                \"exception\": serialize_exception(self.exception),",
            "                \"issuer\": (",
            "                    _base64_bytes_to_str(self.issuer.dump()) if self.issuer else None",
            "                ),",
            "                \"subject\": (",
            "                    _base64_bytes_to_str(self.subject.dump()) if self.subject else None",
            "                ),",
            "                \"cert_id\": (",
            "                    _base64_bytes_to_str(self.cert_id.dump()) if self.cert_id else None",
            "                ),",
            "                \"ocsp_response\": _base64_bytes_to_str(self.ocsp_response),",
            "                \"ts\": self.ts,",
            "                \"validated\": self.validated,",
            "            }",
            "        )",
            "",
            "    @classmethod",
            "    def _deserialize(cls, json_str: str) -> OCSPResponseValidationResult:",
            "        json_obj = json.loads(json_str)",
            "",
            "        def deserialize_exception(exception_dict: dict | None) -> Exception | None:",
            "            # as pointed out in the serialization method, here we do the best effort deserialization",
            "            # for non-RevocationCheckError exceptions. If we can not deserialize the exception, we will",
            "            # return a RevocationCheckError with a message indicating the failure.",
            "            if not exception_dict:",
            "                return",
            "            exc_class = exception_dict.get(\"class\")",
            "            exc_module = exception_dict.get(\"module\")",
            "            try:",
            "                if (",
            "                    exc_class == \"RevocationCheckError\"",
            "                    and exc_module == \"snowflake.connector.errors\"",
            "                ):",
            "                    return RevocationCheckError(",
            "                        msg=exception_dict[\"msg\"],",
            "                        errno=exception_dict[\"errno\"],",
            "                    )",
            "                else:",
            "                    module = importlib.import_module(exc_module)",
            "                    exc_cls = getattr(module, exc_class)",
            "                    return exc_cls(exception_dict[\"msg\"])",
            "            except Exception as deserialize_exc:",
            "                logger.debug(",
            "                    f\"hitting error {str(deserialize_exc)} while deserializing exception,\"",
            "                    f\" the original error error class and message are {exc_class} and {exception_dict['msg']}\"",
            "                )",
            "                return RevocationCheckError(",
            "                    f\"Got error {str(deserialize_exc)} while deserializing ocsp cache, please try \"",
            "                    f\"cleaning up the \"",
            "                    f\"OCSP cache under directory {OCSP_RESPONSE_VALIDATION_CACHE.file_path}\",",
            "                    errno=ER_OCSP_RESPONSE_LOAD_FAILURE,",
            "                )",
            "",
            "        return OCSPResponseValidationResult(",
            "            exception=deserialize_exception(json_obj.get(\"exception\")),",
            "            issuer=(",
            "                Certificate.load(b64decode(json_obj.get(\"issuer\")))",
            "                if json_obj.get(\"issuer\")",
            "                else None",
            "            ),",
            "            subject=(",
            "                Certificate.load(b64decode(json_obj.get(\"subject\")))",
            "                if json_obj.get(\"subject\")",
            "                else None",
            "            ),",
            "            cert_id=(",
            "                CertId.load(b64decode(json_obj.get(\"cert_id\")))",
            "                if json_obj.get(\"cert_id\")",
            "                else None",
            "            ),",
            "            ocsp_response=(",
            "                b64decode(json_obj.get(\"ocsp_response\"))",
            "                if json_obj.get(\"ocsp_response\")",
            "                else None",
            "            ),",
            "            ts=json_obj.get(\"ts\"),",
            "            validated=json_obj.get(\"validated\"),",
            "        )",
            "",
            "",
            "class _OCSPResponseValidationResultCache(SFDictFileCache):",
            "    def _serialize(self) -> bytes:",
            "        entries = {",
            "            (",
            "                _base64_bytes_to_str(k[0]),",
            "                _base64_bytes_to_str(k[1]),",
            "                _base64_bytes_to_str(k[2]),",
            "            ): (v.expiry.isoformat(), v.entry._serialize())",
            "            for k, v in self._cache.items()",
            "        }",
            "",
            "        return json.dumps(",
            "            {",
            "                \"cache_keys\": list(entries.keys()),",
            "                \"cache_items\": list(entries.values()),",
            "                \"entry_lifetime\": self._entry_lifetime.total_seconds(),",
            "                \"file_path\": str(self.file_path),",
            "                \"file_timeout\": self.file_timeout,",
            "                \"last_loaded\": (",
            "                    self.last_loaded.isoformat() if self.last_loaded else None",
            "                ),",
            "                \"telemetry\": self.telemetry,",
            "                \"connector_version\": SNOWFLAKE_CONNECTOR_VERSION,  # reserved for schema version control",
            "            }",
            "        ).encode()",
            "",
            "    @classmethod",
            "    def _deserialize(cls, opened_fd) -> _OCSPResponseValidationResultCache:",
            "        data = json.loads(opened_fd.read().decode())",
            "        cache_instance = cls(",
            "            file_path=data[\"file_path\"],",
            "            entry_lifetime=int(data[\"entry_lifetime\"]),",
            "            file_timeout=data[\"file_timeout\"],",
            "            load_if_file_exists=False,",
            "        )",
            "        cache_instance.file_path = os.path.expanduser(data[\"file_path\"])",
            "        cache_instance.telemetry = data[\"telemetry\"]",
            "        cache_instance.last_loaded = (",
            "            datetime.fromisoformat(data[\"last_loaded\"]) if data[\"last_loaded\"] else None",
            "        )",
            "        for k, v in zip(data[\"cache_keys\"], data[\"cache_items\"]):",
            "            cache_instance._cache[",
            "                (b64decode(k[0]), b64decode(k[1]), b64decode(k[2]))",
            "            ] = CacheEntry(",
            "                datetime.fromisoformat(v[0]),",
            "                OCSPResponseValidationResult._deserialize(v[1]),",
            "            )",
            "        return cache_instance",
            "",
            "",
            "try:",
            "    OCSP_RESPONSE_VALIDATION_CACHE: SFDictFileCache[",
            "        tuple[bytes, bytes, bytes],",
            "        OCSPResponseValidationResult,",
            "    ] = _OCSPResponseValidationResultCache(",
            "        entry_lifetime=constants.DAY_IN_SECONDS,",
            "        file_path={",
            "            \"linux\": os.path.join(",
            "                \"~\", \".cache\", \"snowflake\", \"ocsp_response_validation_cache.json\"",
            "            ),",
            "            \"darwin\": os.path.join(",
            "                \"~\",",
            "                \"Library\",",
            "                \"Caches\",",
            "                \"Snowflake\",",
            "                \"ocsp_response_validation_cache.json\",",
            "            ),",
            "            \"windows\": os.path.join(",
            "                \"~\",",
            "                \"AppData\",",
            "                \"Local\",",
            "                \"Snowflake\",",
            "                \"Caches\",",
            "                \"ocsp_response_validation_cache.json\",",
            "            ),",
            "        },",
            "    )",
            "except OSError:",
            "    # In case we run into some read/write permission error fall back onto",
            "    #  in memory caching",
            "    OCSP_RESPONSE_VALIDATION_CACHE: SFDictCache[",
            "        tuple[bytes, bytes, bytes],",
            "        OCSPResponseValidationResult,",
            "    ] = SFDictCache(",
            "        entry_lifetime=constants.DAY_IN_SECONDS,",
            "    )",
            "",
            "logger = getLogger(__name__)",
            "",
            "",
            "def generate_cache_key(",
            "    cert_id: CertId,",
            ") -> tuple[bytes, bytes, bytes]:",
            "    return (",
            "        cert_id[\"issuer_name_hash\"].dump(),",
            "        cert_id[\"issuer_key_hash\"].dump(),",
            "        cert_id[\"serial_number\"].dump(),",
            "    )",
            "",
            "",
            "class OCSPTelemetryData:",
            "    CERTIFICATE_EXTRACTION_FAILED = \"CertificateExtractionFailed\"",
            "    OCSP_URL_MISSING = \"OCSPURLMissing\"",
            "    OCSP_RESPONSE_UNAVAILABLE = \"OCSPResponseUnavailable\"",
            "    OCSP_RESPONSE_FETCH_EXCEPTION = \"OCSPResponseFetchException\"",
            "    OCSP_RESPONSE_FAILED_TO_CONNECT_CACHE_SERVER = (",
            "        \"OCSPResponseFailedToConnectCacheServer\"",
            "    )",
            "    OCSP_RESPONSE_CERT_STATUS_INVALID = \"OCSPResponseCertStatusInvalid\"",
            "    OCSP_RESPONSE_CERT_STATUS_REVOKED = \"OCSPResponseCertStatusRevoked\"",
            "    OCSP_RESPONSE_CERT_STATUS_UNKNOWN = \"OCSPResponseCertStatusUnknown\"",
            "    OCSP_RESPONSE_STATUS_UNSUCCESSFUL = \"OCSPResponseStatusUnsuccessful\"",
            "    OCSP_RESPONSE_ATTACHED_CERT_INVALID = \"OCSPResponseAttachedCertInvalid\"",
            "    OCSP_RESPONSE_ATTACHED_CERT_EXPIRED = \"OCSPResponseAttachedCertExpired\"",
            "    OCSP_RESPONSE_INVALID_SIGNATURE = \"OCSPResponseSignatureInvalid\"",
            "    OCSP_RESPONSE_EXPIRY_INFO_MISSING = \"OCSPResponseExpiryInfoMissing\"",
            "    OCSP_RESPONSE_EXPIRED = \"OCSPResponseExpired\"",
            "    OCSP_RESPONSE_FETCH_FAILURE = \"OCSPResponseFetchFailure\"",
            "    OCSP_RESPONSE_CACHE_DOWNLOAD_FAILED = \"OCSPResponseCacheDownloadFailed\"",
            "    OCSP_RESPONSE_CACHE_DECODE_FAILED = \"OCSPResponseCacheDecodeFailed\"",
            "    OCSP_RESPONSE_LOAD_FAILURE = \"OCSPResponseLoadFailure\"",
            "    OCSP_RESPONSE_INVALID_SSD = \"OCSPResponseInvalidSSD\"",
            "",
            "    ERROR_CODE_MAP = {",
            "        ER_OCSP_URL_INFO_MISSING: OCSP_URL_MISSING,",
            "        ER_OCSP_RESPONSE_UNAVAILABLE: OCSP_RESPONSE_UNAVAILABLE,",
            "        ER_OCSP_RESPONSE_FETCH_EXCEPTION: OCSP_RESPONSE_FETCH_EXCEPTION,",
            "        ER_OCSP_FAILED_TO_CONNECT_CACHE_SERVER: OCSP_RESPONSE_FAILED_TO_CONNECT_CACHE_SERVER,",
            "        ER_OCSP_RESPONSE_CERT_STATUS_INVALID: OCSP_RESPONSE_CERT_STATUS_INVALID,",
            "        ER_OCSP_RESPONSE_CERT_STATUS_REVOKED: OCSP_RESPONSE_CERT_STATUS_REVOKED,",
            "        ER_OCSP_RESPONSE_CERT_STATUS_UNKNOWN: OCSP_RESPONSE_CERT_STATUS_UNKNOWN,",
            "        ER_OCSP_RESPONSE_STATUS_UNSUCCESSFUL: OCSP_RESPONSE_STATUS_UNSUCCESSFUL,",
            "        ER_OCSP_RESPONSE_ATTACHED_CERT_INVALID: OCSP_RESPONSE_ATTACHED_CERT_INVALID,",
            "        ER_OCSP_RESPONSE_ATTACHED_CERT_EXPIRED: OCSP_RESPONSE_ATTACHED_CERT_EXPIRED,",
            "        ER_OCSP_RESPONSE_INVALID_SIGNATURE: OCSP_RESPONSE_INVALID_SIGNATURE,",
            "        ER_OCSP_RESPONSE_INVALID_EXPIRY_INFO_MISSING: OCSP_RESPONSE_EXPIRY_INFO_MISSING,",
            "        ER_OCSP_RESPONSE_EXPIRED: OCSP_RESPONSE_EXPIRED,",
            "        ER_OCSP_RESPONSE_FETCH_FAILURE: OCSP_RESPONSE_FETCH_FAILURE,",
            "        ER_OCSP_RESPONSE_LOAD_FAILURE: OCSP_RESPONSE_LOAD_FAILURE,",
            "        ER_OCSP_RESPONSE_CACHE_DOWNLOAD_FAILED: OCSP_RESPONSE_CACHE_DOWNLOAD_FAILED,",
            "        ER_OCSP_RESPONSE_CACHE_DECODE_FAILED: OCSP_RESPONSE_CACHE_DECODE_FAILED,",
            "        ER_INVALID_OCSP_RESPONSE_SSD: OCSP_RESPONSE_INVALID_SSD,",
            "        ER_INVALID_SSD: OCSP_RESPONSE_INVALID_SSD,",
            "    }",
            "",
            "    def __init__(self) -> None:",
            "        self.event_sub_type = None",
            "        self.ocsp_connection_method = None",
            "        self.cert_id = None",
            "        self.sfc_peer_host = None",
            "        self.ocsp_url = None",
            "        self.ocsp_req = None",
            "        self.error_msg = None",
            "        self.cache_enabled = False",
            "        self.cache_hit = False",
            "        self.fail_open = False",
            "        self.insecure_mode = False",
            "",
            "    def set_event_sub_type(self, event_sub_type: str) -> None:",
            "        \"\"\"",
            "        Sets sub type for OCSP Telemetry Event.",
            "",
            "        There can be multiple event_sub_type that could have happened",
            "        during a single connection establishment. Ensure that all of them",
            "        are captured.",
            "        :param event_sub_type:",
            "        :return:",
            "        \"\"\"",
            "        if self.event_sub_type is not None:",
            "            self.event_sub_type = f\"{self.event_sub_type}|{event_sub_type}\"",
            "        else:",
            "            self.event_sub_type = event_sub_type",
            "",
            "    def set_ocsp_connection_method(self, ocsp_conn_method: str) -> None:",
            "        self.ocsp_connection_method = ocsp_conn_method",
            "",
            "    def set_cert_id(self, cert_id) -> None:",
            "        self.cert_id = cert_id",
            "",
            "    def set_sfc_peer_host(self, sfc_peer_host) -> None:",
            "        self.sfc_peer_host = sfc_peer_host",
            "",
            "    def set_ocsp_url(self, ocsp_url) -> None:",
            "        self.ocsp_url = ocsp_url",
            "",
            "    def set_ocsp_req(self, ocsp_req) -> None:",
            "        self.ocsp_req = ocsp_req",
            "",
            "    def set_error_msg(self, error_msg) -> None:",
            "        self.error_msg = error_msg",
            "",
            "    def set_cache_enabled(self, cache_enabled) -> None:",
            "        self.cache_enabled = cache_enabled",
            "        if not cache_enabled:",
            "            self.cache_hit = False",
            "",
            "    def set_cache_hit(self, cache_hit) -> None:",
            "        if not self.cache_enabled:",
            "            self.cache_hit = False",
            "        else:",
            "            self.cache_hit = cache_hit",
            "",
            "    def set_fail_open(self, fail_open) -> None:",
            "        self.fail_open = fail_open",
            "",
            "    def set_insecure_mode(self, insecure_mode) -> None:",
            "        self.insecure_mode = insecure_mode",
            "",
            "    def generate_telemetry_data(",
            "        self, event_type: str, urgent: bool = False",
            "    ) -> dict[str, Any]:",
            "        _, exception, _ = sys.exc_info()",
            "        telemetry_data = generate_telemetry_data_dict(",
            "            from_dict={",
            "                TelemetryField.KEY_OOB_EVENT_TYPE.value: event_type,",
            "                TelemetryField.KEY_OOB_EVENT_SUB_TYPE.value: self.event_sub_type,",
            "                TelemetryField.KEY_OOB_SFC_PEER_HOST.value: self.sfc_peer_host,",
            "                TelemetryField.KEY_OOB_CERT_ID.value: self.cert_id,",
            "                TelemetryField.KEY_OOB_OCSP_REQUEST_BASE64.value: self.ocsp_req,",
            "                TelemetryField.KEY_OOB_OCSP_RESPONDER_URL.value: self.ocsp_url,",
            "                TelemetryField.KEY_OOB_ERROR_MESSAGE.value: self.error_msg,",
            "                TelemetryField.KEY_OOB_INSECURE_MODE.value: self.insecure_mode,",
            "                TelemetryField.KEY_OOB_FAIL_OPEN.value: self.fail_open,",
            "                TelemetryField.KEY_OOB_CACHE_ENABLED.value: self.cache_enabled,",
            "                TelemetryField.KEY_OOB_CACHE_HIT.value: self.cache_hit,",
            "            },",
            "            is_oob_telemetry=True,",
            "        )",
            "",
            "        return telemetry_data",
            "        # To be updated once Python Driver has out of band telemetry.",
            "        # telemetry_client = TelemetryClient()",
            "        # telemetry_client.add_log_to_batch(TelemetryData(telemetry_data, datetime.now(timezone.utc).replace(tzinfo=None))",
            "",
            "",
            "class OCSPServer:",
            "    MAX_RETRY = int(os.getenv(\"OCSP_MAX_RETRY\", \"3\"))",
            "",
            "    def __init__(self, **kwargs) -> None:",
            "        top_level_domain = kwargs.pop(",
            "            \"top_level_domain\", constants._DEFAULT_HOSTNAME_TLD",
            "        )",
            "        self.DEFAULT_CACHE_SERVER_URL = (",
            "            f\"http://ocsp.snowflakecomputing.{top_level_domain}\"",
            "        )",
            "        \"\"\"",
            "        The following will change to something like",
            "        http://ocspssd.snowflakecomputing.com/ocsp/",
            "        once the endpoint is up in the backend",
            "        \"\"\"",
            "        self.NEW_DEFAULT_CACHE_SERVER_BASE_URL = (",
            "            f\"https://ocspssd.snowflakecomputing.{top_level_domain}/ocsp/\"",
            "        )",
            "        if not OCSPServer.is_enabled_new_ocsp_endpoint():",
            "            self.CACHE_SERVER_URL = os.getenv(",
            "                \"SF_OCSP_RESPONSE_CACHE_SERVER_URL\",",
            "                \"{}/{}\".format(",
            "                    self.DEFAULT_CACHE_SERVER_URL,",
            "                    OCSPCache.OCSP_RESPONSE_CACHE_FILE_NAME,",
            "                ),",
            "            )",
            "        else:",
            "            self.CACHE_SERVER_URL = os.getenv(\"SF_OCSP_RESPONSE_CACHE_SERVER_URL\")",
            "",
            "        self.CACHE_SERVER_ENABLED = (",
            "            os.getenv(\"SF_OCSP_RESPONSE_CACHE_SERVER_ENABLED\", \"true\") != \"false\"",
            "        )",
            "        # OCSP dynamic cache server URL pattern",
            "        self.OCSP_RETRY_URL = None",
            "",
            "    @staticmethod",
            "    def is_enabled_new_ocsp_endpoint() -> bool:",
            "        \"\"\"Checks if new OCSP Endpoint has been enabled.\"\"\"",
            "        return os.getenv(\"SF_OCSP_ACTIVATE_NEW_ENDPOINT\", \"false\").lower() == \"true\"",
            "",
            "    def reset_ocsp_endpoint(self, hname) -> None:",
            "        \"\"\"Resets current object members CACHE_SERVER_URL and RETRY_URL_PATTERN.",
            "",
            "        They will point at the new OCSP Fetch and Retry endpoints respectively. The new OCSP Endpoint address is based",
            "        on the hostname the customer is trying to connect to. The deployment or in case of client failover, the",
            "        replication ID is copied from the hostname.",
            "        \"\"\"",
            "        top_level_domain = extract_top_level_domain_from_hostname(hname)",
            "        if \"privatelink.snowflakecomputing.\" in hname:",
            "            temp_ocsp_endpoint = \"\".join([\"https://ocspssd.\", hname, \"/ocsp/\"])",
            "        elif \"global.snowflakecomputing.\" in hname:",
            "            rep_id_begin = hname[hname.find(\"-\") :]",
            "            temp_ocsp_endpoint = \"\".join([\"https://ocspssd\", rep_id_begin, \"/ocsp/\"])",
            "        elif not hname.endswith(f\"snowflakecomputing.{top_level_domain}\"):",
            "            temp_ocsp_endpoint = self.NEW_DEFAULT_CACHE_SERVER_BASE_URL",
            "        else:",
            "            hname_wo_acc = hname[hname.find(\".\") :]",
            "            temp_ocsp_endpoint = \"\".join([\"https://ocspssd\", hname_wo_acc, \"/ocsp/\"])",
            "",
            "        self.CACHE_SERVER_URL = \"\".join([temp_ocsp_endpoint, \"fetch\"])",
            "        self.OCSP_RETRY_URL = \"\".join([temp_ocsp_endpoint, \"retry\"])",
            "",
            "    def reset_ocsp_dynamic_cache_server_url(self, use_ocsp_cache_server) -> None:",
            "        \"\"\"Resets OCSP dynamic cache server url pattern.",
            "",
            "        This is used only when OCSP cache server is updated.",
            "        \"\"\"",
            "        if use_ocsp_cache_server is not None:",
            "            self.CACHE_SERVER_ENABLED = use_ocsp_cache_server",
            "",
            "        if self.CACHE_SERVER_ENABLED:",
            "            logger.debug(",
            "                \"OCSP response cache server is enabled: %s\", self.CACHE_SERVER_URL",
            "            )",
            "        else:",
            "            logger.debug(\"OCSP response cache server is disabled\")",
            "",
            "        if self.OCSP_RETRY_URL is None:",
            "            if self.CACHE_SERVER_URL is not None and (",
            "                not self.CACHE_SERVER_URL.startswith(self.DEFAULT_CACHE_SERVER_URL)",
            "            ):",
            "                # only if custom OCSP cache server is used.",
            "                parsed_url = urlsplit(self.CACHE_SERVER_URL)",
            "                self.OCSP_RETRY_URL = f\"{urlunparse((parsed_url.scheme, parsed_url.netloc, '', '', '', ''))}/retry/{{0}}/{{1}}\"",
            "        logger.debug(\"OCSP dynamic cache server RETRY URL: %s\", self.OCSP_RETRY_URL)",
            "",
            "    def download_cache_from_server(self, ocsp):",
            "        if self.CACHE_SERVER_ENABLED:",
            "            # if any of them is not cache, download the cache file from",
            "            # OCSP response cache server.",
            "            try:",
            "                retval = OCSPServer._download_ocsp_response_cache(",
            "                    ocsp, self.CACHE_SERVER_URL",
            "                )",
            "                if not retval:",
            "                    raise RevocationCheckError(",
            "                        msg=\"OCSP Cache Server Unavailable.\",",
            "                        errno=ER_OCSP_RESPONSE_CACHE_DOWNLOAD_FAILED,",
            "                    )",
            "                logger.debug(",
            "                    \"downloaded OCSP response cache file from %s\", self.CACHE_SERVER_URL",
            "                )",
            "                # len(OCSP_RESPONSE_VALIDATION_CACHE) is thread-safe, however, we do not want to",
            "                # block for logging purpose, thus using len(OCSP_RESPONSE_VALIDATION_CACHE._cache) here.",
            "                logger.debug(",
            "                    \"# of certificates: %u\",",
            "                    len(OCSP_RESPONSE_VALIDATION_CACHE._cache),",
            "                )",
            "            except RevocationCheckError as rce:",
            "                logger.debug(",
            "                    \"OCSP Response cache download failed. The client\"",
            "                    \"will reach out to the OCSP Responder directly for\"",
            "                    \"any missing OCSP responses %s\\n\" % rce.msg",
            "                )",
            "                raise",
            "",
            "    @staticmethod",
            "    def _download_ocsp_response_cache(ocsp, url, do_retry: bool = True) -> bool:",
            "        \"\"\"Downloads OCSP response cache from the cache server.\"\"\"",
            "        headers = {HTTP_HEADER_USER_AGENT: PYTHON_CONNECTOR_USER_AGENT}",
            "        sf_timeout = SnowflakeOCSP.OCSP_CACHE_SERVER_CONNECTION_TIMEOUT",
            "",
            "        try:",
            "            start_time = time.time()",
            "            logger.debug(\"started downloading OCSP response cache file: %s\", url)",
            "",
            "            if ocsp.test_mode is not None:",
            "                test_timeout = os.getenv(",
            "                    \"SF_TEST_OCSP_CACHE_SERVER_CONNECTION_TIMEOUT\", None",
            "                )",
            "                sf_cache_server_url = os.getenv(\"SF_TEST_OCSP_CACHE_SERVER_URL\", None)",
            "                if test_timeout is not None:",
            "                    sf_timeout = int(test_timeout)",
            "                if sf_cache_server_url is not None:",
            "                    url = sf_cache_server_url",
            "",
            "            with generic_requests.Session() as session:",
            "                max_retry = SnowflakeOCSP.OCSP_CACHE_SERVER_MAX_RETRY if do_retry else 1",
            "                sleep_time = 1",
            "                backoff = exponential_backoff()()",
            "                for _ in range(max_retry):",
            "                    response = session.get(",
            "                        url,",
            "                        timeout=sf_timeout,  # socket timeout",
            "                        headers=headers,",
            "                    )",
            "                    if response.status_code == OK:",
            "                        ocsp.decode_ocsp_response_cache(response.json())",
            "                        elapsed_time = time.time() - start_time",
            "                        logger.debug(",
            "                            \"ended downloading OCSP response cache file. \"",
            "                            \"elapsed time: %ss\",",
            "                            elapsed_time,",
            "                        )",
            "                        break",
            "                    elif max_retry > 1:",
            "                        sleep_time = next(backoff)",
            "                        logger.debug(",
            "                            \"OCSP server returned %s. Retrying in %s(s)\",",
            "                            response.status_code,",
            "                            sleep_time,",
            "                        )",
            "                    time.sleep(sleep_time)",
            "                else:",
            "                    logger.error(",
            "                        \"Failed to get OCSP response after %s attempt.\", max_retry",
            "                    )",
            "                    return False",
            "                return True",
            "        except Exception as e:",
            "            logger.debug(\"Failed to get OCSP response cache from %s: %s\", url, e)",
            "            raise RevocationCheckError(",
            "                msg=f\"Failed to get OCSP Response Cache from {url}: {e}\",",
            "                errno=ER_OCSP_FAILED_TO_CONNECT_CACHE_SERVER,",
            "            )",
            "",
            "    def generate_get_url(self, ocsp_url, b64data):",
            "        parsed_url = urlsplit(ocsp_url)",
            "        url_encoded_b64data = url_encode_str(b64data)",
            "        if self.OCSP_RETRY_URL is None:",
            "            target_url = f\"{ocsp_url}/{url_encoded_b64data}\"",
            "        else:",
            "            # values of parsed_url.netloc and parsed_url.path based on oscp_url are as follows:",
            "            # URL                                    NETLOC                         PATH",
            "            # \"http://oneocsp.microsoft.com\"         \"oneocsp.microsoft.com\"        \"\"",
            "            # \"http://oneocsp.microsoft.com:8080\"    \"oneocsp.microsoft.com:8080\"   \"\"",
            "            # \"http://oneocsp.microsoft.com/\"        \"oneocsp.microsoft.com\"        \"/\"",
            "            # \"http://oneocsp.microsoft.com/ocsp\"    \"oneocsp.microsoft.com\"        \"/ocsp\"",
            "            # The check below is to treat first two urls same",
            "            path = parsed_url.path if parsed_url.path != \"/\" else \"\"",
            "            target_url = self.OCSP_RETRY_URL.format(",
            "                parsed_url.netloc + path, url_encoded_b64data",
            "            )",
            "",
            "        logger.debug(\"OCSP Retry URL is - %s\", target_url)",
            "        return target_url",
            "",
            "",
            "class OCSPCache:",
            "    # OCSP cache lock",
            "    CACHE_LOCK = Lock()",
            "",
            "    # OCSP cache update flag",
            "    CACHE_UPDATED = False",
            "",
            "    # Cache Expiration in seconds (120 hours). OCSP validation cache is",
            "    # invalidated every 120 hours (5 days)",
            "    CACHE_EXPIRATION = 432000",
            "",
            "    # OCSP Response Cache URI",
            "    OCSP_RESPONSE_CACHE_URI = None",
            "",
            "    # OCSP response cache file name",
            "    OCSP_RESPONSE_CACHE_FILE_NAME = \"ocsp_response_cache.json\"",
            "",
            "    # Cache directory",
            "    CACHE_DIR = None",
            "",
            "    @staticmethod",
            "    def reset_cache_dir() -> None:",
            "        # Cache directory",
            "        OCSPCache.CACHE_DIR = os.getenv(\"SF_OCSP_RESPONSE_CACHE_DIR\")",
            "        if OCSPCache.CACHE_DIR is None:",
            "            cache_root_dir = expanduser(\"~\") or tempfile.gettempdir()",
            "            if platform.system() == \"Windows\":",
            "                OCSPCache.CACHE_DIR = path.join(",
            "                    cache_root_dir, \"AppData\", \"Local\", \"Snowflake\", \"Caches\"",
            "                )",
            "            elif platform.system() == \"Darwin\":",
            "                OCSPCache.CACHE_DIR = path.join(",
            "                    cache_root_dir, \"Library\", \"Caches\", \"Snowflake\"",
            "                )",
            "            else:",
            "                OCSPCache.CACHE_DIR = path.join(cache_root_dir, \".cache\", \"snowflake\")",
            "        logger.debug(\"cache directory: %s\", OCSPCache.CACHE_DIR)",
            "",
            "        if not path.exists(OCSPCache.CACHE_DIR):",
            "            try:",
            "                os.makedirs(OCSPCache.CACHE_DIR, mode=0o700)",
            "            except Exception as ex:",
            "                logger.debug(",
            "                    \"cannot create a cache directory: [%s], err=[%s]\",",
            "                    OCSPCache.CACHE_DIR,",
            "                    ex,",
            "                )",
            "                OCSPCache.CACHE_DIR = None",
            "",
            "    @staticmethod",
            "    def del_cache_file() -> None:",
            "        \"\"\"Deletes the OCSP response cache file if exists.\"\"\"",
            "        cache_file = path.join(",
            "            OCSPCache.CACHE_DIR, OCSPCache.OCSP_RESPONSE_CACHE_FILE_NAME",
            "        )",
            "        if path.exists(cache_file):",
            "            logger.debug(f\"deleting cache file {cache_file}\")",
            "            os.unlink(cache_file)",
            "",
            "    @staticmethod",
            "    def reset_ocsp_response_cache_uri(ocsp_response_cache_uri) -> None:",
            "        if ocsp_response_cache_uri is None and OCSPCache.CACHE_DIR is not None:",
            "            OCSPCache.OCSP_RESPONSE_CACHE_URI = \"file://\" + path.join(",
            "                OCSPCache.CACHE_DIR, OCSPCache.OCSP_RESPONSE_CACHE_FILE_NAME",
            "            )",
            "        else:",
            "            OCSPCache.OCSP_RESPONSE_CACHE_URI = ocsp_response_cache_uri",
            "",
            "        if OCSPCache.OCSP_RESPONSE_CACHE_URI is not None:",
            "            # normalize URI for Windows",
            "            OCSPCache.OCSP_RESPONSE_CACHE_URI = (",
            "                OCSPCache.OCSP_RESPONSE_CACHE_URI.replace(\"\\\\\", \"/\")",
            "            )",
            "",
            "        logger.debug(\"ocsp_response_cache_uri: %s\", OCSPCache.OCSP_RESPONSE_CACHE_URI)",
            "        # len(OCSP_RESPONSE_VALIDATION_CACHE) is thread-safe, however, we do not want to",
            "        # block for logging purpose, thus using len(OCSP_RESPONSE_VALIDATION_CACHE._cache) here.",
            "        logger.debug(",
            "            \"OCSP_VALIDATION_CACHE size: %u\",",
            "            len(OCSP_RESPONSE_VALIDATION_CACHE._cache),",
            "        )",
            "",
            "    @staticmethod",
            "    def read_file(ocsp):",
            "        \"\"\"Reads OCSP Response cache data from the URI, which is very likely a file.\"\"\"",
            "        try:",
            "            parsed_url = urlsplit(OCSPCache.OCSP_RESPONSE_CACHE_URI)",
            "            if parsed_url.scheme == \"file\":",
            "                OCSPCache.read_ocsp_response_cache_file(",
            "                    ocsp, path.join(parsed_url.netloc, parsed_url.path)",
            "                )",
            "            else:",
            "                msg = \"Unsupported OCSP URI: {}\".format(",
            "                    OCSPCache.OCSP_RESPONSE_CACHE_URI",
            "                )",
            "                raise Exception(msg)",
            "        except (RevocationCheckError, Exception) as rce:",
            "            logger.debug(",
            "                \"Failed to read OCSP response cache file %s: %s, \"",
            "                \"No worry. It will validate with OCSP server. \"",
            "                \"Ignoring...\",",
            "                OCSPCache.OCSP_RESPONSE_CACHE_URI,",
            "                rce,",
            "                exc_info=True,",
            "            )",
            "",
            "    @staticmethod",
            "    def read_ocsp_response_cache_file(ocsp, filename):",
            "        \"\"\"Reads OCSP Response cache.\"\"\"",
            "        try:",
            "            if OCSPCache.check_ocsp_response_cache_lock_dir(filename) and path.exists(",
            "                filename",
            "            ):",
            "                with codecs.open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:",
            "                    ocsp.decode_ocsp_response_cache(json.load(f))",
            "                # len(OCSP_RESPONSE_VALIDATION_CACHE) is thread-safe, however, we do not want to",
            "                # block for logging purpose, thus using len(OCSP_RESPONSE_VALIDATION_CACHE._cache) here.",
            "                logger.debug(",
            "                    \"Read OCSP response cache file: %s, count=%s\",",
            "                    filename,",
            "                    len(OCSP_RESPONSE_VALIDATION_CACHE._cache),",
            "                )",
            "            else:",
            "                logger.debug(",
            "                    \"Failed to locate OCSP response cache file. \"",
            "                    \"No worry. It will validate with OCSP server: %s\",",
            "                    filename,",
            "                )",
            "        except Exception as ex:",
            "            logger.debug(\"Caught - %s\", ex)",
            "            raise ex",
            "",
            "    @staticmethod",
            "    def update_file(ocsp) -> None:",
            "        \"\"\"",
            "        Updates OCSP Response Cache file.",
            "        Two file shall be updated/saved:",
            "            1. file for OCSP_RESPONSE_VALIDATION_CACHE which keeps ocsp response validation result",
            "            2. ocsp_response_cache.json, the file in the same format as the one downloaded from snowflake cache service",
            "        \"\"\"",
            "        if OCSPCache.CACHE_UPDATED:",
            "            if isinstance(OCSP_RESPONSE_VALIDATION_CACHE, SFDictFileCache):",
            "                OCSP_RESPONSE_VALIDATION_CACHE.save()",
            "            OCSPCache.update_ocsp_response_cache_file(",
            "                ocsp, OCSPCache.OCSP_RESPONSE_CACHE_URI",
            "            )",
            "            OCSPCache.CACHE_UPDATED = False",
            "",
            "    @staticmethod",
            "    def update_ocsp_response_cache_file(ocsp, ocsp_response_cache_uri) -> None:",
            "        \"\"\"Updates OCSP Response Cache.\"\"\"",
            "        if ocsp_response_cache_uri is not None:",
            "            try:",
            "                parsed_url = urlsplit(ocsp_response_cache_uri)",
            "                if parsed_url.scheme == \"file\":",
            "                    filename = path.join(parsed_url.netloc, parsed_url.path)",
            "                    lock_dir = filename + \".lck\"",
            "                    for _ in range(100):",
            "                        # wait until the lck file has been removed",
            "                        # or up to 1 second (0.01 x 100)",
            "                        if OCSPCache.lock_cache_file(lock_dir):",
            "                            break",
            "                        time.sleep(0.01)",
            "                    try:",
            "                        OCSPCache.write_ocsp_response_cache_file(ocsp, filename)",
            "                    finally:",
            "                        OCSPCache.unlock_cache_file(lock_dir)",
            "                else:",
            "                    logger.debug(",
            "                        \"No OCSP response cache file is written, because the \"",
            "                        \"given URI is not a file: %s. Ignoring...\",",
            "                        ocsp_response_cache_uri,",
            "                    )",
            "            except Exception as e:",
            "                logger.debug(",
            "                    \"Failed to write OCSP response cache \"",
            "                    \"file. file: %s, error: %s, Ignoring...\",",
            "                    ocsp_response_cache_uri,",
            "                    e,",
            "                    exc_info=True,",
            "                )",
            "",
            "    @staticmethod",
            "    def write_ocsp_response_cache_file(ocsp, filename) -> None:",
            "        \"\"\"Writes OCSP Response Cache.\"\"\"",
            "        logger.debug(f\"writing OCSP response cache file to {filename}\")",
            "        file_cache_data = {}",
            "        ocsp.encode_ocsp_response_cache(file_cache_data)",
            "        with codecs.open(filename, \"w\", encoding=\"utf-8\", errors=\"ignore\") as f:",
            "            json.dump(file_cache_data, f)",
            "",
            "    @staticmethod",
            "    def check_ocsp_response_cache_lock_dir(filename) -> bool:",
            "        \"\"\"Checks if the lock directory exists.",
            "",
            "        Returns:",
            "            True if it can update the cache file or False when some other process may be updating the cache file.",
            "        \"\"\"",
            "        current_time = int(time.time())",
            "        lock_dir = filename + \".lck\"",
            "",
            "        try:",
            "            ts_cache_file = OCSPCache._file_timestamp(filename)",
            "            if (",
            "                not path.exists(lock_dir)",
            "                and current_time - OCSPCache.CACHE_EXPIRATION <= ts_cache_file",
            "            ):",
            "                # use cache only if no lock directory exists and the cache file",
            "                # was created last 24 hours",
            "                return True",
            "",
            "            if path.exists(lock_dir):",
            "                # delete lock directory if older 60 seconds",
            "                ts_lock_dir = OCSPCache._file_timestamp(lock_dir)",
            "                if ts_lock_dir < current_time - 60:",
            "                    OCSPCache.unlock_cache_file(lock_dir)",
            "                    logger.debug(",
            "                        \"The lock directory is older than 60 seconds. \"",
            "                        \"Deleted the lock directory and ignoring the cache: %s\",",
            "                        lock_dir,",
            "                    )",
            "                else:",
            "                    logger.debug(",
            "                        \"The lock directory exists. Other process may be \"",
            "                        \"updating the cache file: %s, %s\",",
            "                        filename,",
            "                        lock_dir,",
            "                    )",
            "            else:",
            "                os.unlink(filename)",
            "                logger.debug(",
            "                    \"The cache is older than 1 day. \" \"Deleted the cache file: %s\",",
            "                    filename,",
            "                )",
            "        except Exception as e:",
            "            logger.debug(",
            "                \"Failed to check OCSP response cache file. No worry. It will \"",
            "                \"validate with OCSP server: file: %s, lock directory: %s, \"",
            "                \"error: %s\",",
            "                filename,",
            "                lock_dir,",
            "                e,",
            "            )",
            "        return False",
            "",
            "    @staticmethod",
            "    def is_cache_fresh(current_time: int, ts: int) -> bool:",
            "        return current_time - OCSPCache.CACHE_EXPIRATION <= ts",
            "",
            "    @staticmethod",
            "    def find_cache(",
            "        ocsp: SnowflakeOCSP, cert_id: CertId, subject: Certificate | None, **kwargs: Any",
            "    ) -> tuple[bool, bytes | None]:",
            "        subject_name = ocsp.subject_name(subject) if subject else None",
            "        current_time = int(time.time())",
            "        cache_key: tuple[bytes, bytes, bytes] = kwargs.get(",
            "            \"cache_key\", ocsp.decode_cert_id_key(cert_id)",
            "        )",
            "        lock_cache: bool = kwargs.get(\"lock_cache\", True)",
            "        try:",
            "            ocsp_response_validation_result = (",
            "                OCSP_RESPONSE_VALIDATION_CACHE[cache_key]",
            "                if lock_cache",
            "                else OCSP_RESPONSE_VALIDATION_CACHE._getitem_non_locking(cache_key)",
            "            )",
            "            try:",
            "                # is_valid_time can raise exception if the cache",
            "                # entry is a SSD.",
            "                if OCSPCache.is_cache_fresh(",
            "                    current_time, ocsp_response_validation_result.ts",
            "                ) and ocsp.is_valid_time(",
            "                    cert_id, ocsp_response_validation_result.ocsp_response",
            "                ):",
            "                    if subject_name:",
            "                        logger.debug(\"hit cache for subject: %s\", subject_name)",
            "                    return True, ocsp_response_validation_result.ocsp_response",
            "                else:",
            "                    OCSPCache.delete_cache(",
            "                        ocsp, cert_id, cache_key=cache_key, lock_cache=lock_cache",
            "                    )",
            "            except Exception as ex:",
            "                logger.debug(f\"Could not validate cache entry {cert_id} {ex}\")",
            "            OCSPCache.CACHE_UPDATED = True",
            "        except KeyError:",
            "            if subject_name:",
            "                logger.debug(f\"cache miss for subject: '{subject_name}'\")",
            "        return False, None",
            "",
            "    @staticmethod",
            "    def delete_cache(ocsp: SnowflakeOCSP, cert_id: CertId, **kwargs: Any) -> None:",
            "        cache_key: tuple[bytes, bytes, bytes] = kwargs.get(",
            "            \"cache_key\", ocsp.decode_cert_id_key(cert_id)",
            "        )",
            "        lock_cache: bool = kwargs.get(\"lock_cache\", True)",
            "        try:",
            "            if lock_cache:",
            "                del OCSP_RESPONSE_VALIDATION_CACHE[cache_key]",
            "            else:",
            "                OCSP_RESPONSE_VALIDATION_CACHE._delitem(cache_key)",
            "            OCSPCache.CACHE_UPDATED = True",
            "        except KeyError:",
            "            pass",
            "",
            "    @staticmethod",
            "    def _file_timestamp(filename):",
            "        \"\"\"Gets the last created timestamp of the file/dir.\"\"\"",
            "        if platform.system() == \"Windows\":",
            "            ts = int(path.getctime(filename))",
            "        else:",
            "            stat = os.stat(filename)",
            "            if hasattr(stat, \"st_birthtime\"):  # odx",
            "                ts = int(stat.st_birthtime)",
            "            else:",
            "                ts = int(stat.st_mtime)  # linux",
            "        return ts",
            "",
            "    @staticmethod",
            "    def lock_cache_file(fname) -> bool:",
            "        \"\"\"Locks a cache file by creating a directory.\"\"\"",
            "        try:",
            "            os.mkdir(fname)",
            "            return True",
            "        except OSError:",
            "            return False",
            "",
            "    @staticmethod",
            "    def unlock_cache_file(fname) -> bool:",
            "        \"\"\"Unlocks a cache file by deleting a directory.\"\"\"",
            "        try:",
            "            os.rmdir(fname)",
            "            return True",
            "        except OSError:",
            "            return False",
            "",
            "    @staticmethod",
            "    def delete_cache_file() -> None:",
            "        \"\"\"Deletes the cache file. Used by tests only.\"\"\"",
            "        parsed_url = urlsplit(OCSPCache.OCSP_RESPONSE_CACHE_URI)",
            "        fname = path.join(parsed_url.netloc, parsed_url.path)",
            "        OCSPCache.lock_cache_file(fname)",
            "        try:",
            "            logger.debug(f\"deleting cache file, used by tests only {fname}\")",
            "            os.unlink(fname)",
            "        finally:",
            "            OCSPCache.unlock_cache_file(fname)",
            "",
            "    @staticmethod",
            "    def clear_cache() -> None:",
            "        \"\"\"Clears cache.\"\"\"",
            "        OCSP_RESPONSE_VALIDATION_CACHE.clear()",
            "",
            "    @staticmethod",
            "    def cache_size():",
            "        \"\"\"Returns the cache's size.\"\"\"",
            "        return len(OCSP_RESPONSE_VALIDATION_CACHE)",
            "",
            "",
            "# Reset OCSP cache directory",
            "OCSPCache.reset_cache_dir()",
            "",
            "",
            "class SnowflakeOCSP:",
            "    \"\"\"OCSP validator using PyOpenSSL and asn1crypto/pyasn1.\"\"\"",
            "",
            "    # root certificate cache",
            "    ROOT_CERTIFICATES_DICT = {}  # root certificates",
            "",
            "    # root certificate cache lock",
            "    ROOT_CERTIFICATES_DICT_LOCK = RLock()",
            "",
            "    # cache object",
            "    OCSP_CACHE = OCSPCache()",
            "",
            "    OCSP_WHITELIST = re.compile(",
            "        r\"^\"",
            "        r\"(.*\\.snowflakecomputing(\\.[a-zA-Z]{1,63}){1,2}$\"",
            "        r\"|(?:|.*\\.)s3.*\\.amazonaws(\\.[a-zA-Z]{1,63}){1,2}$\"  # start with s3 or .s3 in the middle",
            "        r\"|.*\\.okta\\.com$\"",
            "        r\"|(?:|.*\\.)storage\\.googleapis\\.com$\"",
            "        r\"|.*\\.blob\\.core\\.windows\\.net$\"",
            "        r\"|.*\\.blob\\.core\\.usgovcloudapi\\.net$)\"",
            "    )",
            "",
            "    # Tolerable validity date range ratio. The OCSP response is valid up",
            "    # to (next update timestamp) + (next update timestamp -",
            "    # this update timestamp) * TOLERABLE_VALIDITY_RANGE_RATIO. This buffer",
            "    # yields some time for Root CA to update intermediate CA's certificate",
            "    # OCSP response. In fact, they don't update OCSP response in time. In Dec",
            "    # 2016, they left OCSP response expires for 5 hours at least, and it",
            "    # caused the connectivity issues in customers.",
            "    # With this buffer, about 2 days are given for 180 days validity date.",
            "    TOLERABLE_VALIDITY_RANGE_RATIO = 0.01",
            "",
            "    # Maximum clock skew in seconds (15 minutes) allowed when checking",
            "    # validity of OCSP responses",
            "    MAX_CLOCK_SKEW = 900",
            "",
            "    # Epoch time",
            "    ZERO_EPOCH = datetime.fromtimestamp(0, timezone.utc).replace(tzinfo=None)",
            "",
            "    # Timestamp format for logging",
            "    OUTPUT_TIMESTAMP_FORMAT = \"%Y-%m-%d %H:%M:%SZ\"",
            "",
            "    # Connection timeout in seconds for CA OCSP Responder",
            "    CA_OCSP_RESPONDER_CONNECTION_TIMEOUT = 10",
            "",
            "    # Connection timeout in seconds for Cache Server",
            "    OCSP_CACHE_SERVER_CONNECTION_TIMEOUT = 5",
            "",
            "    # MAX number of connection retry attempts with Responder in Fail Open",
            "    CA_OCSP_RESPONDER_MAX_RETRY_FO = 1",
            "",
            "    # MAX number of connection retry attempts with Responder in Fail Close",
            "    CA_OCSP_RESPONDER_MAX_RETRY_FC = 3",
            "",
            "    # MAX number of connection retry attempts with Cache Server",
            "    OCSP_CACHE_SERVER_MAX_RETRY = 1",
            "",
            "    def __init__(",
            "        self,",
            "        ocsp_response_cache_uri=None,",
            "        use_ocsp_cache_server=None,",
            "        use_post_method: bool = True,",
            "        use_fail_open: bool = True,",
            "        **kwargs,",
            "    ) -> None:",
            "        self.test_mode = os.getenv(\"SF_OCSP_TEST_MODE\", None)",
            "",
            "        if self.test_mode == \"true\":",
            "            logger.debug(\"WARNING - DRIVER CONFIGURED IN TEST MODE\")",
            "",
            "        self._use_post_method = use_post_method",
            "        self.OCSP_CACHE_SERVER = OCSPServer(",
            "            top_level_domain=extract_top_level_domain_from_hostname(",
            "                kwargs.pop(\"hostname\", None)",
            "            )",
            "        )",
            "",
            "        self.debug_ocsp_failure_url = None",
            "",
            "        if os.getenv(\"SF_OCSP_FAIL_OPEN\") is not None:",
            "            # failOpen Env Variable is for internal usage/ testing only.",
            "            # Using it in production is not advised and not supported.",
            "            self.FAIL_OPEN = os.getenv(\"SF_OCSP_FAIL_OPEN\").lower() == \"true\"",
            "        else:",
            "            self.FAIL_OPEN = use_fail_open",
            "",
            "        SnowflakeOCSP.OCSP_CACHE.reset_ocsp_response_cache_uri(ocsp_response_cache_uri)",
            "",
            "        if not OCSPServer.is_enabled_new_ocsp_endpoint():",
            "            self.OCSP_CACHE_SERVER.reset_ocsp_dynamic_cache_server_url(",
            "                use_ocsp_cache_server",
            "            )",
            "",
            "        \"\"\"",
            "        Here we have a two-layer cache design:",
            "",
            "        The upper layer is the OCSP_RESPONSE_VALIDATION_CACHE which caches not only the ocsp responses but also",
            "        the validation result of ocsp responses. This will be both in-memory and in-file (if program has the right",
            "        to read and write files).",
            "",
            "        The bottom layer is the ocsp responses in the form of a json file which are either",
            "        retrieved from Snowflake cache service or locally maintained by writing OCSP_RESPONSE_VALIDATION_CACHE back",
            "        to the json file for any updates (certificate revoked, cache expired, etc.). This will be in-file.",
            "",
            "        The cache logic is as following:",
            "        1. The OCSP_RESPONSE_VALIDATION_CACHE will be loaded from disk first during module loading period.",
            "        2. If there's no content loaded either due to no cache file or all cache expired, then we try load ocsp",
            "         response cache file. We will parse the content in the ocsp response cache json file, and",
            "         then update OCSP_RESPONSE_VALIDATION_CACHE.",
            "        3. When validating certs, we will first check OCSP_RESPONSE_VALIDATION_CACHE, if cache is not found,",
            "         when we will validate against the OCSP servers and cache the results.",
            "        4. After validating all the certs, we save OCSP_RESPONSE_VALIDATION_CACHE and ocsp response json",
            "         onto disk.",
            "        \"\"\"",
            "        if not OCSP_RESPONSE_VALIDATION_CACHE:",
            "            SnowflakeOCSP.OCSP_CACHE.read_file(self)",
            "",
            "    def validate_certfile(self, cert_filename, no_exception: bool = False):",
            "        \"\"\"Validates that the certificate is NOT revoked.\"\"\"",
            "        cert_map = {}",
            "        telemetry_data = OCSPTelemetryData()",
            "        telemetry_data.set_cache_enabled(self.OCSP_CACHE_SERVER.CACHE_SERVER_ENABLED)",
            "        telemetry_data.set_insecure_mode(False)",
            "        telemetry_data.set_sfc_peer_host(cert_filename)",
            "        telemetry_data.set_fail_open(self.is_enabled_fail_open())",
            "        try:",
            "            self.read_cert_bundle(cert_filename, cert_map)",
            "            cert_data = self.create_pair_issuer_subject(cert_map)",
            "        except Exception as ex:",
            "            logger.debug(\"Caught exception while validating certfile %s\", str(ex))",
            "            raise ex",
            "",
            "        return self._validate(",
            "            None, cert_data, telemetry_data, do_retry=False, no_exception=no_exception",
            "        )",
            "",
            "    def validate(",
            "        self,",
            "        hostname: str | None,",
            "        connection: Connection,",
            "        no_exception: bool = False,",
            "    ) -> (",
            "        list[",
            "            tuple[",
            "                Exception | None,",
            "                Certificate,",
            "                Certificate,",
            "                CertId,",
            "                str | bytes,",
            "            ]",
            "        ]",
            "        | None",
            "    ):",
            "        \"\"\"Validates the certificate is not revoked using OCSP.\"\"\"",
            "        logger.debug(\"validating certificate: %s\", hostname)",
            "",
            "        do_retry = SnowflakeOCSP.get_ocsp_retry_choice()",
            "",
            "        m = not SnowflakeOCSP.OCSP_WHITELIST.match(hostname)",
            "        if m or hostname.startswith(\"ocspssd\"):",
            "            logger.debug(\"skipping OCSP check: %s\", hostname)",
            "            return [None, None, None, None, None]",
            "",
            "        if OCSPServer.is_enabled_new_ocsp_endpoint():",
            "            self.OCSP_CACHE_SERVER.reset_ocsp_endpoint(hostname)",
            "",
            "        telemetry_data = OCSPTelemetryData()",
            "        telemetry_data.set_cache_enabled(self.OCSP_CACHE_SERVER.CACHE_SERVER_ENABLED)",
            "        telemetry_data.set_insecure_mode(False)",
            "        telemetry_data.set_sfc_peer_host(hostname)",
            "        telemetry_data.set_fail_open(self.is_enabled_fail_open())",
            "",
            "        try:",
            "            cert_data = self.extract_certificate_chain(connection)",
            "        except RevocationCheckError:",
            "            telemetry_data.set_event_sub_type(",
            "                OCSPTelemetryData.CERTIFICATE_EXTRACTION_FAILED",
            "            )",
            "            logger.debug(",
            "                telemetry_data.generate_telemetry_data(\"RevocationCheckFailure\")",
            "            )",
            "            return None",
            "",
            "        return self._validate(",
            "            hostname, cert_data, telemetry_data, do_retry, no_exception",
            "        )",
            "",
            "    def _validate(",
            "        self,",
            "        hostname: str | None,",
            "        cert_data: list[tuple[Certificate, Certificate]],",
            "        telemetry_data: OCSPTelemetryData,",
            "        do_retry: bool = True,",
            "        no_exception: bool = False,",
            "    ) -> list[tuple[Exception | None, Certificate, Certificate, CertId, bytes]]:",
            "        \"\"\"Validate certs sequentially if OCSP response cache server is used.\"\"\"",
            "        results = self._validate_certificates_sequential(",
            "            cert_data, telemetry_data, hostname, do_retry=do_retry",
            "        )",
            "",
            "        SnowflakeOCSP.OCSP_CACHE.update_file(self)",
            "",
            "        any_err = False",
            "        for err, _, _, _, _ in results:",
            "            if isinstance(err, RevocationCheckError):",
            "                err.msg += f\" for {hostname}\"",
            "            if not no_exception and err is not None:",
            "                raise err",
            "            elif err is not None:",
            "                any_err = True",
            "",
            "        logger.debug(\"ok\" if not any_err else \"failed\")",
            "        return results",
            "",
            "    @staticmethod",
            "    def get_ocsp_retry_choice() -> bool:",
            "        return os.getenv(\"SF_OCSP_DO_RETRY\", \"true\") == \"true\"",
            "",
            "    def is_cert_id_in_cache(",
            "        self, cert_id: CertId, subject: Certificate | None, **kwargs: Any",
            "    ):",
            "        \"\"\"Decides whether OCSP CertID is in cache.",
            "",
            "        Args:",
            "            cert_id: OCSP CertID.",
            "            subject: Subject certificate.",
            "",
            "        Returns:",
            "            True if in cache otherwise False, followed by the cached OCSP Response.",
            "        \"\"\"",
            "        found, cache = SnowflakeOCSP.OCSP_CACHE.find_cache(",
            "            self, cert_id, subject, **kwargs",
            "        )",
            "        return found, cache",
            "",
            "    def get_account_from_hostname(self, hostname: str) -> str:",
            "        \"\"\"Extracts the account name part from the hostname.",
            "",
            "        Args:",
            "            hostname: Hostname that account name is in.",
            "",
            "        Returns:",
            "            The extracted account name.",
            "        \"\"\"",
            "        split_hname = hostname.split(\".\")",
            "        if \"global\" in split_hname:",
            "            acc_name = split_hname[0].split(\"-\")[0]",
            "        else:",
            "            acc_name = split_hname[0]",
            "        return acc_name",
            "",
            "    def is_enabled_fail_open(self) -> bool:",
            "        return self.FAIL_OPEN",
            "",
            "    @staticmethod",
            "    def print_fail_open_warning(ocsp_log) -> None:",
            "        static_warning = (",
            "            \"WARNING!!! Using fail-open to connect. Driver is connecting to an \"",
            "            \"HTTPS endpoint without OCSP based Certificate Revocation checking \"",
            "            \"as it could not obtain a valid OCSP Response to use from the CA OCSP \"",
            "            \"responder. Details:\"",
            "        )",
            "        ocsp_warning = f\"{static_warning} \\n {ocsp_log}\"",
            "        logger.warning(ocsp_warning)",
            "",
            "    def validate_by_direct_connection(",
            "        self,",
            "        issuer: Certificate,",
            "        subject: Certificate,",
            "        telemetry_data: OCSPTelemetryData,",
            "        hostname: str = None,",
            "        do_retry: bool = True,",
            "        **kwargs: Any,",
            "    ) -> tuple[Exception | None, Certificate, Certificate, CertId, bytes]:",
            "        cert_id, req = self.create_ocsp_request(issuer, subject)",
            "        cache_status, ocsp_response = self.is_cert_id_in_cache(",
            "            cert_id, subject, **kwargs",
            "        )",
            "",
            "        try:",
            "            if not cache_status:",
            "                telemetry_data.set_cache_hit(False)",
            "                logger.debug(\"getting OCSP response from CA's OCSP server\")",
            "                ocsp_response = self._fetch_ocsp_response(",
            "                    req, subject, cert_id, telemetry_data, hostname, do_retry",
            "                )",
            "            else:",
            "                ocsp_url = self.extract_ocsp_url(subject)",
            "                cert_id_enc = self.encode_cert_id_base64(",
            "                    self.decode_cert_id_key(cert_id)",
            "                )",
            "                telemetry_data.set_cache_hit(True)",
            "                self.debug_ocsp_failure_url = SnowflakeOCSP.create_ocsp_debug_info(",
            "                    self, req, ocsp_url",
            "                )",
            "                telemetry_data.set_ocsp_url(ocsp_url)",
            "                telemetry_data.set_ocsp_req(req)",
            "                telemetry_data.set_cert_id(cert_id_enc)",
            "                logger.debug(\"using OCSP response cache\")",
            "",
            "            if not ocsp_response:",
            "                telemetry_data.set_event_sub_type(",
            "                    OCSPTelemetryData.OCSP_RESPONSE_UNAVAILABLE",
            "                )",
            "                raise RevocationCheckError(",
            "                    msg=\"Could not retrieve OCSP Response. Cannot perform Revocation Check\",",
            "                    errno=ER_OCSP_RESPONSE_UNAVAILABLE,",
            "                )",
            "            try:",
            "                self.process_ocsp_response(issuer, cert_id, ocsp_response)",
            "                err = None",
            "            except RevocationCheckError as op_er:",
            "                telemetry_data.set_event_sub_type(",
            "                    OCSPTelemetryData.ERROR_CODE_MAP[op_er.errno]",
            "                )",
            "                raise op_er",
            "",
            "        except RevocationCheckError as rce:",
            "            telemetry_data.set_error_msg(rce.msg)",
            "            err = self.verify_fail_open(rce, telemetry_data)",
            "",
            "        except Exception as ex:",
            "            logger.debug(\"OCSP Validation failed %s\", str(ex))",
            "            telemetry_data.set_error_msg(str(ex))",
            "            err = self.verify_fail_open(ex, telemetry_data)",
            "            SnowflakeOCSP.OCSP_CACHE.delete_cache(self, cert_id)",
            "",
            "        return err, issuer, subject, cert_id, ocsp_response",
            "",
            "    def verify_fail_open(self, ex_obj, telemetry_data):",
            "        if not self.is_enabled_fail_open():",
            "            if ex_obj.errno is ER_OCSP_RESPONSE_CERT_STATUS_REVOKED:",
            "                logger.debug(",
            "                    telemetry_data.generate_telemetry_data(",
            "                        \"RevokedCertificateError\", True",
            "                    )",
            "                )",
            "            else:",
            "                logger.debug(",
            "                    telemetry_data.generate_telemetry_data(\"RevocationCheckFailure\")",
            "                )",
            "            return ex_obj",
            "        else:",
            "            if ex_obj.errno is ER_OCSP_RESPONSE_CERT_STATUS_REVOKED:",
            "                logger.debug(",
            "                    telemetry_data.generate_telemetry_data(",
            "                        \"RevokedCertificateError\", True",
            "                    )",
            "                )",
            "                return ex_obj",
            "            else:",
            "                SnowflakeOCSP.print_fail_open_warning(",
            "                    telemetry_data.generate_telemetry_data(\"RevocationCheckFailure\")",
            "                )",
            "                return None",
            "",
            "    def _validate_certificates_sequential(",
            "        self,",
            "        cert_data: list[tuple[Certificate, Certificate]],",
            "        telemetry_data: OCSPTelemetryData,",
            "        hostname: str | None = None,",
            "        do_retry: bool = True,",
            "    ) -> list[tuple[Exception | None, Certificate, Certificate, CertId, bytes]]:",
            "        results = []",
            "        try:",
            "            self._check_ocsp_response_cache_server(cert_data)",
            "        except RevocationCheckError as rce:",
            "            telemetry_data.set_event_sub_type(",
            "                OCSPTelemetryData.ERROR_CODE_MAP[rce.errno]",
            "            )",
            "        except Exception as ex:",
            "            logger.debug(",
            "                \"Caught unknown exception - %s. Continue to validate by direct connection\",",
            "                str(ex),",
            "            )",
            "",
            "        to_update_cache_dict = {}",
            "        for issuer, subject in cert_data:",
            "            cert_id, _ = self.create_ocsp_request(issuer=issuer, subject=subject)",
            "            cache_key = self.decode_cert_id_key(cert_id)",
            "            ocsp_response_validation_result = OCSP_RESPONSE_VALIDATION_CACHE.get(",
            "                cache_key",
            "            )",
            "",
            "            if (",
            "                ocsp_response_validation_result is None",
            "                or not ocsp_response_validation_result.validated",
            "            ):",
            "                # r is a tuple of (err, issuer, subject, cert_id, ocsp_response)",
            "                r = self.validate_by_direct_connection(",
            "                    issuer,",
            "                    subject,",
            "                    telemetry_data,",
            "                    hostname,",
            "                    do_retry=do_retry,",
            "                    cache_key=cache_key,",
            "                )",
            "",
            "                # When OCSP server is down, the validation fails and the oscp_response will be None, and in fail open",
            "                # case, we will also reset err to None.",
            "                # In this case we don't need to write the response to cache because there is no information from a",
            "                # connection error.",
            "                if r[0] is not None or r[4] is not None:",
            "                    to_update_cache_dict[cache_key] = OCSPResponseValidationResult(",
            "                        *r,",
            "                        ts=int(time.time()),",
            "                        validated=True,",
            "                    )",
            "                    OCSPCache.CACHE_UPDATED = True",
            "                results.append(r)",
            "            else:",
            "                results.append(",
            "                    (",
            "                        ocsp_response_validation_result.exception,",
            "                        ocsp_response_validation_result.issuer,",
            "                        ocsp_response_validation_result.subject,",
            "                        ocsp_response_validation_result.cert_id,",
            "                        ocsp_response_validation_result.ocsp_response,",
            "                    )",
            "                )",
            "        OCSP_RESPONSE_VALIDATION_CACHE.update(to_update_cache_dict)",
            "        return results",
            "",
            "    def _check_ocsp_response_cache_server(",
            "        self,",
            "        cert_data: list[tuple[Certificate, Certificate]],",
            "    ) -> None:",
            "        \"\"\"Checks if OCSP response is in cache, and if not it downloads the OCSP response cache from the server.",
            "",
            "        Args:",
            "          cert_data: Tuple of issuer and subject certificates.",
            "        \"\"\"",
            "        in_cache = False",
            "        for issuer, subject in cert_data:",
            "            # check if any OCSP response is NOT in cache",
            "            cert_id, _ = self.create_ocsp_request(issuer, subject)",
            "            in_cache, _ = SnowflakeOCSP.OCSP_CACHE.find_cache(self, cert_id, subject)",
            "            if not in_cache:",
            "                # not found any",
            "                break",
            "",
            "        if not in_cache:",
            "            self.OCSP_CACHE_SERVER.download_cache_from_server(self)",
            "",
            "    def _lazy_read_ca_bundle(self) -> None:",
            "        \"\"\"Reads the local cabundle file and cache it in memory.\"\"\"",
            "        with SnowflakeOCSP.ROOT_CERTIFICATES_DICT_LOCK:",
            "            if SnowflakeOCSP.ROOT_CERTIFICATES_DICT:",
            "                # return if already loaded",
            "                return",
            "",
            "            try:",
            "                ca_bundle = environ.get(\"REQUESTS_CA_BUNDLE\") or environ.get(",
            "                    \"CURL_CA_BUNDLE\"",
            "                )",
            "                if ca_bundle and path.exists(ca_bundle):",
            "                    # if the user/application specifies cabundle.",
            "                    self.read_cert_bundle(ca_bundle)",
            "                else:",
            "                    import sys",
            "",
            "                    # This import that depends on these libraries is to import certificates from them,",
            "                    # we would like to have these as up to date as possible.",
            "                    from requests import certs",
            "",
            "                    if (",
            "                        hasattr(certs, \"__file__\")",
            "                        and path.exists(certs.__file__)",
            "                        and path.exists(",
            "                            path.join(path.dirname(certs.__file__), \"cacert.pem\")",
            "                        )",
            "                    ):",
            "                        # if cacert.pem exists next to certs.py in request",
            "                        # package.",
            "                        ca_bundle = path.join(",
            "                            path.dirname(certs.__file__), \"cacert.pem\"",
            "                        )",
            "                        self.read_cert_bundle(ca_bundle)",
            "                    elif hasattr(sys, \"_MEIPASS\"):",
            "                        # if pyinstaller includes cacert.pem",
            "                        cabundle_candidates = [",
            "                            [\"botocore\", \"vendored\", \"requests\", \"cacert.pem\"],",
            "                            [\"requests\", \"cacert.pem\"],",
            "                            [\"cacert.pem\"],",
            "                        ]",
            "                        for filename in cabundle_candidates:",
            "                            ca_bundle = path.join(sys._MEIPASS, *filename)",
            "                            if path.exists(ca_bundle):",
            "                                self.read_cert_bundle(ca_bundle)",
            "                                break",
            "                        else:",
            "                            logger.error(\"No cabundle file is found in _MEIPASS\")",
            "                    try:",
            "                        import certifi",
            "",
            "                        self.read_cert_bundle(certifi.where())",
            "                    except Exception:",
            "                        logger.debug(\"no certifi is installed. ignored.\")",
            "",
            "            except Exception as e:",
            "                logger.error(\"Failed to read ca_bundle: %s\", e)",
            "",
            "            if not SnowflakeOCSP.ROOT_CERTIFICATES_DICT:",
            "                logger.error(",
            "                    \"No CA bundle file is found in the system. \"",
            "                    \"Set REQUESTS_CA_BUNDLE to the file.\"",
            "                )",
            "",
            "    @staticmethod",
            "    def _calculate_tolerable_validity(this_update: float, next_update: float) -> int:",
            "        return max(",
            "            int(",
            "                SnowflakeOCSP.TOLERABLE_VALIDITY_RANGE_RATIO",
            "                * (next_update - this_update)",
            "            ),",
            "            SnowflakeOCSP.MAX_CLOCK_SKEW,",
            "        )",
            "",
            "    @staticmethod",
            "    def _is_validaity_range(",
            "        current_time: int,",
            "        this_update: float,",
            "        next_update: float,",
            "        test_mode: Any | None = None,",
            "    ) -> bool:",
            "        if test_mode is not None:",
            "            force_validity_fail = os.getenv(\"SF_TEST_OCSP_FORCE_BAD_RESPONSE_VALIDITY\")",
            "            if force_validity_fail is not None:",
            "                return False",
            "",
            "        tolerable_validity = SnowflakeOCSP._calculate_tolerable_validity(",
            "            this_update, next_update",
            "        )",
            "        return (",
            "            this_update - SnowflakeOCSP.MAX_CLOCK_SKEW",
            "            <= current_time",
            "            <= next_update + tolerable_validity",
            "        )",
            "",
            "    @staticmethod",
            "    def _validity_error_message(current_time, this_update, next_update) -> str:",
            "        tolerable_validity = SnowflakeOCSP._calculate_tolerable_validity(",
            "            this_update, next_update",
            "        )",
            "        return (",
            "            \"Response is unreliable. Its validity \"",
            "            \"date is out of range: current_time={}, \"",
            "            \"this_update={}, next_update={}, \"",
            "            \"tolerable next_update={}. A potential cause is \"",
            "            \"client clock is skewed, CA fails to update OCSP \"",
            "            \"response in time.\".format(",
            "                strftime(SnowflakeOCSP.OUTPUT_TIMESTAMP_FORMAT, gmtime(current_time)),",
            "                strftime(SnowflakeOCSP.OUTPUT_TIMESTAMP_FORMAT, gmtime(this_update)),",
            "                strftime(SnowflakeOCSP.OUTPUT_TIMESTAMP_FORMAT, gmtime(next_update)),",
            "                strftime(",
            "                    SnowflakeOCSP.OUTPUT_TIMESTAMP_FORMAT,",
            "                    gmtime(next_update + tolerable_validity),",
            "                ),",
            "            )",
            "        )",
            "",
            "    @staticmethod",
            "    def clear_cache() -> None:",
            "        SnowflakeOCSP.OCSP_CACHE.clear_cache()",
            "",
            "    @staticmethod",
            "    def cache_size():",
            "        return SnowflakeOCSP.OCSP_CACHE.cache_size()",
            "",
            "    @staticmethod",
            "    def delete_cache_file() -> None:",
            "        SnowflakeOCSP.OCSP_CACHE.delete_cache_file()",
            "",
            "    @staticmethod",
            "    def create_ocsp_debug_info(ocsp, ocsp_request, ocsp_url):",
            "        b64data = ocsp.decode_ocsp_request_b64(ocsp_request)",
            "        target_url = f\"{ocsp_url}/{b64data}\"",
            "        return target_url",
            "",
            "    def _fetch_ocsp_response(",
            "        self,",
            "        ocsp_request,",
            "        subject,",
            "        cert_id,",
            "        telemetry_data,",
            "        hostname=None,",
            "        do_retry: bool = True,",
            "    ):",
            "        \"\"\"Fetches OCSP response using OCSPRequest.\"\"\"",
            "        sf_timeout = SnowflakeOCSP.CA_OCSP_RESPONDER_CONNECTION_TIMEOUT",
            "        ocsp_url = self.extract_ocsp_url(subject)",
            "        cert_id_enc = self.encode_cert_id_base64(self.decode_cert_id_key(cert_id))",
            "        if not ocsp_url:",
            "            telemetry_data.set_event_sub_type(OCSPTelemetryData.OCSP_URL_MISSING)",
            "            raise RevocationCheckError(",
            "                msg=\"No OCSP URL found in cert. Cannot perform Certificate Revocation check\",",
            "                errno=ER_OCSP_URL_INFO_MISSING,",
            "            )",
            "        headers = {HTTP_HEADER_USER_AGENT: PYTHON_CONNECTOR_USER_AGENT}",
            "",
            "        if not OCSPServer.is_enabled_new_ocsp_endpoint():",
            "            actual_method = \"post\" if self._use_post_method else \"get\"",
            "            if self.OCSP_CACHE_SERVER.OCSP_RETRY_URL:",
            "                # no POST is supported for Retry URL at the moment.",
            "                actual_method = \"get\"",
            "",
            "            if actual_method == \"get\":",
            "                b64data = self.decode_ocsp_request_b64(ocsp_request)",
            "                target_url = self.OCSP_CACHE_SERVER.generate_get_url(ocsp_url, b64data)",
            "                payload = None",
            "            else:",
            "                target_url = ocsp_url",
            "                payload = self.decode_ocsp_request(ocsp_request)",
            "                headers[\"Content-Type\"] = \"application/ocsp-request\"",
            "        else:",
            "            actual_method = \"post\"",
            "            target_url = self.OCSP_CACHE_SERVER.OCSP_RETRY_URL",
            "            ocsp_req_enc = self.decode_ocsp_request_b64(ocsp_request)",
            "",
            "            payload = json.dumps(",
            "                {",
            "                    \"hostname\": hostname,",
            "                    \"ocsp_request\": ocsp_req_enc,",
            "                    \"cert_id\": cert_id_enc,",
            "                    \"ocsp_responder_url\": ocsp_url,",
            "                }",
            "            )",
            "            headers[\"Content-Type\"] = \"application/json\"",
            "",
            "        telemetry_data.set_ocsp_connection_method(actual_method)",
            "        if self.test_mode is not None:",
            "            logger.debug(\"WARNING - DRIVER IS CONFIGURED IN TESTMODE.\")",
            "            test_ocsp_url = os.getenv(\"SF_TEST_OCSP_URL\", None)",
            "            test_timeout = os.getenv(",
            "                \"SF_TEST_CA_OCSP_RESPONDER_CONNECTION_TIMEOUT\", None",
            "            )",
            "            if test_timeout is not None:",
            "                sf_timeout = int(test_timeout)",
            "            if test_ocsp_url is not None:",
            "                target_url = test_ocsp_url",
            "",
            "        self.debug_ocsp_failure_url = SnowflakeOCSP.create_ocsp_debug_info(",
            "            self, ocsp_request, ocsp_url",
            "        )",
            "        telemetry_data.set_ocsp_req(self.decode_ocsp_request_b64(ocsp_request))",
            "        telemetry_data.set_ocsp_url(ocsp_url)",
            "        telemetry_data.set_cert_id(cert_id_enc)",
            "",
            "        ret = None",
            "        logger.debug(\"url: %s\", target_url)",
            "        sf_max_retry = SnowflakeOCSP.CA_OCSP_RESPONDER_MAX_RETRY_FO",
            "        if not self.is_enabled_fail_open():",
            "            sf_max_retry = SnowflakeOCSP.CA_OCSP_RESPONDER_MAX_RETRY_FC",
            "",
            "        with generic_requests.Session() as session:",
            "            max_retry = sf_max_retry if do_retry else 1",
            "            sleep_time = 1",
            "            backoff = exponential_backoff()()",
            "            for _ in range(max_retry):",
            "                try:",
            "                    response = session.request(",
            "                        headers=headers,",
            "                        method=actual_method,",
            "                        url=target_url,",
            "                        timeout=sf_timeout,",
            "                        data=payload,",
            "                    )",
            "                    if response.status_code == OK:",
            "                        logger.debug(",
            "                            \"OCSP response was successfully returned from OCSP \"",
            "                            \"server.\"",
            "                        )",
            "                        ret = response.content",
            "                        break",
            "                    elif max_retry > 1:",
            "                        sleep_time = next(backoff)",
            "                        logger.debug(",
            "                            \"OCSP server returned %s. Retrying in %s(s)\",",
            "                            response.status_code,",
            "                            sleep_time,",
            "                        )",
            "                    time.sleep(sleep_time)",
            "                except Exception as ex:",
            "                    if max_retry > 1:",
            "                        sleep_time = next(backoff)",
            "                        logger.debug(",
            "                            \"Could not fetch OCSP Response from server\"",
            "                            \"Retrying in %s(s)\",",
            "                            sleep_time,",
            "                        )",
            "                        time.sleep(sleep_time)",
            "                    else:",
            "                        telemetry_data.set_event_sub_type(",
            "                            OCSPTelemetryData.OCSP_RESPONSE_FETCH_EXCEPTION",
            "                        )",
            "                        raise RevocationCheckError(",
            "                            msg=\"Could not fetch OCSP Response from server. Consider\"",
            "                            \"checking your whitelists : Exception - {}\".format(str(ex)),",
            "                            errno=ER_OCSP_RESPONSE_FETCH_EXCEPTION,",
            "                        )",
            "            else:",
            "                logger.error(",
            "                    \"Failed to get OCSP response after {} attempt. Consider checking \"",
            "                    \"for OCSP URLs being blocked\".format(max_retry)",
            "                )",
            "                telemetry_data.set_event_sub_type(",
            "                    OCSPTelemetryData.OCSP_RESPONSE_FETCH_FAILURE",
            "                )",
            "                raise RevocationCheckError(",
            "                    msg=\"Failed to get OCSP response after {} attempt.\".format(",
            "                        max_retry",
            "                    ),",
            "                    errno=ER_OCSP_RESPONSE_FETCH_FAILURE,",
            "                )",
            "",
            "        return ret",
            "",
            "    def _process_good_status(",
            "        self, single_response: SingleResponse, cert_id: CertId, ocsp_response: bytes",
            "    ) -> None:",
            "        \"\"\"Processes GOOD status.\"\"\"",
            "        current_time = int(time.time())",
            "        this_update_native, next_update_native = self.extract_good_status(",
            "            single_response",
            "        )",
            "",
            "        if this_update_native is None or next_update_native is None:",
            "            raise RevocationCheckError(",
            "                msg=\"Either this update or next \"",
            "                \"update is None. this_update: {}, next_update: {}\".format(",
            "                    this_update_native, next_update_native",
            "                ),",
            "                errno=ER_OCSP_RESPONSE_INVALID_EXPIRY_INFO_MISSING,",
            "            )",
            "",
            "        this_update = (",
            "            this_update_native.replace(tzinfo=None) - SnowflakeOCSP.ZERO_EPOCH",
            "        ).total_seconds()",
            "        next_update = (",
            "            next_update_native.replace(tzinfo=None) - SnowflakeOCSP.ZERO_EPOCH",
            "        ).total_seconds()",
            "        if not SnowflakeOCSP._is_validaity_range(",
            "            current_time, this_update, next_update, self.test_mode",
            "        ):",
            "            raise RevocationCheckError(",
            "                msg=SnowflakeOCSP._validity_error_message(",
            "                    current_time, this_update, next_update",
            "                ),",
            "                errno=ER_OCSP_RESPONSE_EXPIRED,",
            "            )",
            "",
            "    def _process_revoked_status(self, single_response, cert_id):",
            "        \"\"\"Processes REVOKED status.\"\"\"",
            "        current_time = int(time.time())",
            "        if self.test_mode is not None:",
            "            test_cert_status = os.getenv(\"SF_TEST_OCSP_CERT_STATUS\")",
            "            if test_cert_status == \"revoked\":",
            "                raise RevocationCheckError(",
            "                    msg=\"The certificate has been revoked: current_time={}, \"",
            "                    \"revocation_time={}, reason={}\".format(",
            "                        strftime(",
            "                            SnowflakeOCSP.OUTPUT_TIMESTAMP_FORMAT, gmtime(current_time)",
            "                        ),",
            "                        strftime(",
            "                            SnowflakeOCSP.OUTPUT_TIMESTAMP_FORMAT, gmtime(current_time)",
            "                        ),",
            "                        \"Force Revoke\",",
            "                    ),",
            "                    errno=ER_OCSP_RESPONSE_CERT_STATUS_REVOKED,",
            "                )",
            "",
            "        SnowflakeOCSP.OCSP_CACHE.delete_cache(self, cert_id)",
            "        revocation_time, revocation_reason = self.extract_revoked_status(",
            "            single_response",
            "        )",
            "        raise RevocationCheckError(",
            "            msg=\"The certificate has been revoked: current_time={}, \"",
            "            \"revocation_time={}, reason={}\".format(",
            "                strftime(SnowflakeOCSP.OUTPUT_TIMESTAMP_FORMAT, gmtime(current_time)),",
            "                revocation_time.strftime(SnowflakeOCSP.OUTPUT_TIMESTAMP_FORMAT),",
            "                revocation_reason,",
            "            ),",
            "            errno=ER_OCSP_RESPONSE_CERT_STATUS_REVOKED,",
            "        )",
            "",
            "    def _process_unknown_status(self, cert_id):",
            "        \"\"\"Processes UNKNOWN status.\"\"\"",
            "        SnowflakeOCSP.OCSP_CACHE.delete_cache(self, cert_id)",
            "        raise RevocationCheckError(",
            "            msg=\"The certificate is in UNKNOWN revocation status.\",",
            "            errno=ER_OCSP_RESPONSE_CERT_STATUS_UNKNOWN,",
            "        )",
            "",
            "    def decode_ocsp_response_cache(self, ocsp_response_cache_json):",
            "        \"\"\"Decodes OCSP response cache from JSON.\"\"\"",
            "        try:",
            "            with OCSP_RESPONSE_VALIDATION_CACHE._lock:",
            "                new_cache_dict = {}",
            "                for cert_id_base64, (",
            "                    ts,",
            "                    ocsp_response,",
            "                ) in ocsp_response_cache_json.items():",
            "                    cert_id = self.decode_cert_id_base64(cert_id_base64)",
            "                    b64decoded_ocsp_response = b64decode(ocsp_response)",
            "                    if not self.is_valid_time(cert_id, b64decoded_ocsp_response):",
            "                        continue",
            "                    current_time = int(time.time())",
            "                    cache_key: tuple[bytes, bytes, bytes] = self.decode_cert_id_key(",
            "                        cert_id",
            "                    )",
            "                    found, _ = OCSPCache.find_cache(",
            "                        self, cert_id, None, cache_key=cache_key, lock_cache=False",
            "                    )",
            "                    if OCSPCache.is_cache_fresh(current_time, ts):",
            "                        new_cache_dict[cache_key] = OCSPResponseValidationResult(",
            "                            ocsp_response=b64decoded_ocsp_response,",
            "                            ts=current_time,",
            "                            validated=False,",
            "                        )",
            "                    elif found:",
            "                        OCSPCache.delete_cache(",
            "                            self, cert_id, cache_key=cache_key, lock_cache=False",
            "                        )",
            "            if new_cache_dict:",
            "                OCSP_RESPONSE_VALIDATION_CACHE.update(new_cache_dict)",
            "                OCSPCache.CACHE_UPDATED = True",
            "        except Exception as ex:",
            "            logger.debug(\"Caught here - %s\", ex)",
            "            ermsg = \"Exception raised while decoding OCSP Response Cache {}\".format(",
            "                str(ex)",
            "            )",
            "            raise RevocationCheckError(",
            "                msg=ermsg, errno=ER_OCSP_RESPONSE_CACHE_DECODE_FAILED",
            "            )",
            "",
            "    def encode_ocsp_response_cache(self, ocsp_response_cache_json) -> None:",
            "        \"\"\"Encodes OCSP response cache to JSON.\"\"\"",
            "        logger.debug(\"encoding OCSP response cache to JSON\")",
            "        for (",
            "            cache_key,",
            "            ocsp_response_validation_result,",
            "        ) in OCSP_RESPONSE_VALIDATION_CACHE.items():",
            "            k = self.encode_cert_id_base64(cache_key)",
            "            v = b64encode(ocsp_response_validation_result.ocsp_response).decode(\"ascii\")",
            "            ocsp_response_cache_json[k] = (ocsp_response_validation_result.ts, v)",
            "",
            "    def read_cert_bundle(self, ca_bundle_file, storage=None):",
            "        \"\"\"Reads a certificate file including certificates in PEM format.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def encode_cert_id_key(self, _):",
            "        \"\"\"Encodes Cert ID key to native CertID.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def decode_cert_id_key(self, _):",
            "        \"\"\"Decodes name CertID to Cert ID key.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def encode_cert_id_base64(self, hkey):",
            "        \"\"\"Encodes native CertID to base64 Cert ID.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def decode_cert_id_base64(self, cert_id_base64):",
            "        \"\"\"Decodes base64 Cert ID to native CertID.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def create_ocsp_request(",
            "        self,",
            "        issuer: Certificate,",
            "        subject: Certificate,",
            "    ) -> tuple[CertId, OCSPRequest]:",
            "        \"\"\"Creates CertId and OCSPRequest.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def extract_ocsp_url(self, cert):",
            "        \"\"\"Extracts OCSP URL from Certificate.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def decode_ocsp_request(self, ocsp_request):",
            "        \"\"\"Decodes OCSP request to DER.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def decode_ocsp_request_b64(self, ocsp_request):",
            "        \"\"\"Decodes OCSP Request object to b64.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def extract_good_status(self, single_response):",
            "        \"\"\"Extracts Revocation Status GOOD.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def extract_revoked_status(self, single_response):",
            "        \"\"\"Extracts Revocation Status REVOKED.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def process_ocsp_response(self, issuer, cert_id, ocsp_response):",
            "        \"\"\"Processes OCSP response.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def verify_signature(self, signature_algorithm, signature, cert, data):",
            "        \"\"\"Verifies signature.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def extract_certificate_chain(self, connection):",
            "        \"\"\"Gets certificate chain and extract the key info from OpenSSL connection.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def create_pair_issuer_subject(self, cert_map):",
            "        \"\"\"Creates pairs of issuer and subject certificates.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def subject_name(self, subject):",
            "        \"\"\"Gets human readable Subject name.\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def is_valid_time(self, cert_id, ocsp_response):",
            "        \"\"\"Checks whether ocsp_response is in valid time range.\"\"\"",
            "        raise NotImplementedError"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "61": [],
            "80": [],
            "84": [],
            "87": [],
            "95": []
        },
        "addLocation": []
    },
    "src/snowflake/connector/storage_client.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 329,
                "afterPatchRowNumber": 329,
                "PatchRowcode": "                 f\"{verb} with url {url} failed for exceeding maximum retries.\""
            },
            "1": {
                "beforePatchRowNumber": 330,
                "afterPatchRowNumber": 330,
                "PatchRowcode": "             )"
            },
            "2": {
                "beforePatchRowNumber": 331,
                "afterPatchRowNumber": 331,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 332,
                "PatchRowcode": "+    def _open_intermediate_dst_path(self, mode):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 333,
                "PatchRowcode": "+        if not self.intermediate_dst_path.exists():"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 334,
                "PatchRowcode": "+            self.intermediate_dst_path.touch(mode=0o600)"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 335,
                "PatchRowcode": "+        return self.intermediate_dst_path.open(mode)"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 336,
                "PatchRowcode": "+"
            },
            "8": {
                "beforePatchRowNumber": 332,
                "afterPatchRowNumber": 337,
                "PatchRowcode": "     def prepare_download(self) -> None:"
            },
            "9": {
                "beforePatchRowNumber": 333,
                "afterPatchRowNumber": 338,
                "PatchRowcode": "         # TODO: add nicer error message for when target directory is not writeable"
            },
            "10": {
                "beforePatchRowNumber": 334,
                "afterPatchRowNumber": 339,
                "PatchRowcode": "         #  but this should be done before we get here"
            },
            "11": {
                "beforePatchRowNumber": 352,
                "afterPatchRowNumber": 357,
                "PatchRowcode": "                 self.num_of_chunks = ceil(file_header.content_length / self.chunk_size)"
            },
            "12": {
                "beforePatchRowNumber": 353,
                "afterPatchRowNumber": 358,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 354,
                "afterPatchRowNumber": 359,
                "PatchRowcode": "         # Preallocate encrypted file."
            },
            "14": {
                "beforePatchRowNumber": 355,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        with self.intermediate_dst_path.open(\"wb+\") as fd:"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 360,
                "PatchRowcode": "+        with self._open_intermediate_dst_path(\"wb+\") as fd:"
            },
            "16": {
                "beforePatchRowNumber": 356,
                "afterPatchRowNumber": 361,
                "PatchRowcode": "             fd.truncate(self.meta.src_file_size)"
            },
            "17": {
                "beforePatchRowNumber": 357,
                "afterPatchRowNumber": 362,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 358,
                "afterPatchRowNumber": 363,
                "PatchRowcode": "     def write_downloaded_chunk(self, chunk_id: int, data: bytes) -> None:"
            },
            "19": {
                "beforePatchRowNumber": 359,
                "afterPatchRowNumber": 364,
                "PatchRowcode": "         \"\"\"Writes given data to the temp location starting at chunk_id * chunk_size.\"\"\""
            },
            "20": {
                "beforePatchRowNumber": 360,
                "afterPatchRowNumber": 365,
                "PatchRowcode": "         # TODO: should we use chunking and write content in smaller chunks?"
            },
            "21": {
                "beforePatchRowNumber": 361,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        with self.intermediate_dst_path.open(\"rb+\") as fd:"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 366,
                "PatchRowcode": "+        with self._open_intermediate_dst_path(\"rb+\") as fd:"
            },
            "23": {
                "beforePatchRowNumber": 362,
                "afterPatchRowNumber": 367,
                "PatchRowcode": "             fd.seek(self.chunk_size * chunk_id)"
            },
            "24": {
                "beforePatchRowNumber": 363,
                "afterPatchRowNumber": 368,
                "PatchRowcode": "             fd.write(data)"
            },
            "25": {
                "beforePatchRowNumber": 364,
                "afterPatchRowNumber": 369,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "#",
            "# Copyright (c) 2012-2023 Snowflake Computing Inc. All rights reserved.",
            "#",
            "",
            "from __future__ import annotations",
            "",
            "import os",
            "import shutil",
            "import tempfile",
            "import threading",
            "import time",
            "from abc import ABC, abstractmethod",
            "from collections import defaultdict",
            "from io import BytesIO",
            "from logging import getLogger",
            "from math import ceil",
            "from pathlib import Path",
            "from typing import TYPE_CHECKING, Any, Callable, NamedTuple",
            "",
            "import OpenSSL",
            "",
            "from .constants import (",
            "    HTTP_HEADER_CONTENT_ENCODING,",
            "    REQUEST_CONNECTION_TIMEOUT,",
            "    REQUEST_READ_TIMEOUT,",
            "    FileHeader,",
            "    ResultStatus,",
            ")",
            "from .encryption_util import EncryptionMetadata, SnowflakeEncryptionUtil",
            "from .errors import RequestExceedMaxRetryError",
            "from .file_util import SnowflakeFileUtil",
            "from .vendored import requests",
            "from .vendored.requests import ConnectionError, Timeout",
            "from .vendored.urllib3 import HTTPResponse",
            "",
            "if TYPE_CHECKING:  # pragma: no cover",
            "    from .file_transfer_agent import SnowflakeFileMeta, StorageCredential",
            "",
            "logger = getLogger(__name__)",
            "",
            "",
            "class SnowflakeFileEncryptionMaterial(NamedTuple):",
            "    query_stage_master_key: str  # query stage master key",
            "    query_id: str  # query id",
            "    smk_id: int  # SMK id",
            "",
            "",
            "METHODS = {",
            "    \"GET\": requests.get,",
            "    \"PUT\": requests.put,",
            "    \"POST\": requests.post,",
            "    \"HEAD\": requests.head,",
            "    \"DELETE\": requests.delete,",
            "}",
            "",
            "",
            "def remove_content_encoding(resp: requests.Response, **kwargs) -> None:",
            "    \"\"\"Remove content-encoding header and decoder so decompression is not triggered\"\"\"",
            "    if HTTP_HEADER_CONTENT_ENCODING in resp.headers:",
            "        if isinstance(resp.raw, HTTPResponse):",
            "            resp.raw._decoder = None",
            "            resp.raw.headers.pop(HTTP_HEADER_CONTENT_ENCODING)",
            "",
            "",
            "class SnowflakeStorageClient(ABC):",
            "    TRANSIENT_HTTP_ERR = (408, 429, 500, 502, 503, 504)",
            "",
            "    TRANSIENT_ERRORS = (OpenSSL.SSL.SysCallError, Timeout, ConnectionError)",
            "    SLEEP_MAX = 16.0",
            "    SLEEP_UNIT = 1.0",
            "",
            "    def __init__(",
            "        self,",
            "        meta: SnowflakeFileMeta,",
            "        stage_info: dict[str, Any],",
            "        chunk_size: int,",
            "        chunked_transfer: bool | None = True,",
            "        credentials: StorageCredential | None = None,",
            "        max_retry: int = 5,",
            "    ) -> None:",
            "        self.meta = meta",
            "        self.stage_info = stage_info",
            "        self.retry_count: dict[int | str, int] = defaultdict(int)",
            "        self.tmp_dir = tempfile.mkdtemp()",
            "        self.data_file: str | None = None",
            "        self.encryption_metadata: EncryptionMetadata | None = None",
            "",
            "        self.max_retry = max_retry  # TODO",
            "        self.credentials = credentials",
            "        # UPLOAD",
            "        meta.real_src_file_name = meta.src_file_name",
            "        meta.upload_size = meta.src_file_size",
            "        self.preprocessed = (",
            "            False  # so we don't repeat compression/file digest when re-encrypting",
            "        )",
            "        # DOWNLOAD",
            "        self.full_dst_file_name: str | None = (",
            "            os.path.join(",
            "                self.meta.local_location, os.path.basename(self.meta.dst_file_name)",
            "            )",
            "            if self.meta.local_location",
            "            else None",
            "        )",
            "        self.intermediate_dst_path: Path | None = (",
            "            Path(self.full_dst_file_name + \".part\")",
            "            if self.meta.local_location",
            "            else None",
            "        )",
            "        # CHUNK",
            "        self.chunked_transfer = chunked_transfer  # only true for GCS",
            "        self.chunk_size = chunk_size",
            "        self.num_of_chunks = 0",
            "        self.lock = threading.Lock()",
            "        self.successful_transfers: int = 0",
            "        self.failed_transfers: int = 0",
            "        # only used when PRESIGNED_URL expires",
            "        self.last_err_is_presigned_url = False",
            "",
            "    def compress(self) -> None:",
            "        if self.meta.require_compress:",
            "            meta = self.meta",
            "            logger.debug(f\"compressing file={meta.src_file_name}\")",
            "            if meta.intermediate_stream:",
            "                (",
            "                    meta.src_stream,",
            "                    upload_size,",
            "                ) = SnowflakeFileUtil.compress_with_gzip_from_stream(",
            "                    meta.intermediate_stream",
            "                )",
            "            else:",
            "                (",
            "                    meta.real_src_file_name,",
            "                    upload_size,",
            "                ) = SnowflakeFileUtil.compress_file_with_gzip(",
            "                    meta.src_file_name, self.tmp_dir",
            "                )",
            "",
            "    def get_digest(self) -> None:",
            "        meta = self.meta",
            "        logger.debug(f\"getting digest file={meta.real_src_file_name}\")",
            "        if meta.intermediate_stream is None:",
            "            (",
            "                meta.sha256_digest,",
            "                meta.upload_size,",
            "            ) = SnowflakeFileUtil.get_digest_and_size_for_file(meta.real_src_file_name)",
            "        else:",
            "            (",
            "                meta.sha256_digest,",
            "                meta.upload_size,",
            "            ) = SnowflakeFileUtil.get_digest_and_size_for_stream(",
            "                meta.src_stream or meta.intermediate_stream",
            "            )",
            "",
            "    def encrypt(self) -> None:",
            "        meta = self.meta",
            "        logger.debug(f\"encrypting file={meta.real_src_file_name}\")",
            "        if meta.intermediate_stream is None:",
            "            (",
            "                self.encryption_metadata,",
            "                self.data_file,",
            "            ) = SnowflakeEncryptionUtil.encrypt_file(",
            "                meta.encryption_material,",
            "                meta.real_src_file_name,",
            "                tmp_dir=self.tmp_dir,",
            "            )",
            "            meta.upload_size = os.path.getsize(self.data_file)",
            "        else:",
            "            encrypted_stream = BytesIO()",
            "            src_stream = meta.src_stream or meta.intermediate_stream",
            "            src_stream.seek(0)",
            "            self.encryption_metadata = SnowflakeEncryptionUtil.encrypt_stream(",
            "                meta.encryption_material, src_stream, encrypted_stream",
            "            )",
            "            src_stream.seek(0)",
            "            meta.upload_size = encrypted_stream.seek(0, os.SEEK_END)",
            "            encrypted_stream.seek(0)",
            "            if meta.src_stream is not None:",
            "                meta.src_stream.close()",
            "            meta.src_stream = encrypted_stream",
            "            self.data_file = meta.real_src_file_name",
            "",
            "    @abstractmethod",
            "    def get_file_header(self, filename: str) -> FileHeader | None:",
            "        \"\"\"Check if file exists in target location and obtain file metadata if exists.",
            "",
            "        Notes:",
            "            Updates meta.result_status.",
            "        \"\"\"",
            "        pass",
            "",
            "    def preprocess(self) -> None:",
            "        meta = self.meta",
            "        logger.debug(f\"Preprocessing {meta.src_file_name}\")",
            "",
            "        file_header = self.get_file_header(",
            "            meta.dst_file_name",
            "        )  # check if file exists on remote",
            "        if not meta.overwrite:",
            "            self.get_digest()  # self.get_file_header needs digest for multiparts upload when aws is used.",
            "            if meta.result_status == ResultStatus.UPLOADED:",
            "                # Skipped",
            "                logger.debug(",
            "                    f'file already exists location=\"{self.stage_info[\"location\"]}\", '",
            "                    f'file_name=\"{meta.dst_file_name}\"'",
            "                )",
            "                meta.dst_file_size = 0",
            "                meta.result_status = ResultStatus.SKIPPED",
            "                self.preprocessed = True",
            "                return",
            "        # Uploading",
            "        if meta.require_compress:",
            "            self.compress()",
            "        self.get_digest()",
            "",
            "        if (",
            "            meta.skip_upload_on_content_match",
            "            and file_header",
            "            and meta.sha256_digest == file_header.digest",
            "        ):",
            "            logger.debug(f\"same file contents for {meta.name}, skipping upload\")",
            "            meta.result_status = ResultStatus.SKIPPED",
            "",
            "        self.preprocessed = True",
            "",
            "    def prepare_upload(self) -> None:",
            "        meta = self.meta",
            "",
            "        if not self.preprocessed:",
            "            self.preprocess()",
            "        elif meta.encryption_material:",
            "            # need to clean up previous encrypted file",
            "            os.remove(self.data_file)",
            "",
            "        logger.debug(f\"Preparing to upload {meta.src_file_name}\")",
            "",
            "        if meta.encryption_material:",
            "            self.encrypt()",
            "        else:",
            "            self.data_file = meta.real_src_file_name",
            "        logger.debug(\"finished preprocessing\")",
            "        if meta.upload_size < meta.multipart_threshold or not self.chunked_transfer:",
            "            self.num_of_chunks = 1",
            "        else:",
            "            self.num_of_chunks = ceil(meta.upload_size / self.chunk_size)",
            "        logger.debug(f\"number of chunks {self.num_of_chunks}\")",
            "        # clean up",
            "        self.retry_count = {}",
            "",
            "        for chunk_id in range(self.num_of_chunks):",
            "            self.retry_count[chunk_id] = 0",
            "        if self.chunked_transfer and self.num_of_chunks > 1:",
            "            self._initiate_multipart_upload()",
            "",
            "    def finish_upload(self) -> None:",
            "        meta = self.meta",
            "        if self.successful_transfers == self.num_of_chunks and self.num_of_chunks != 0:",
            "            if self.num_of_chunks > 1:",
            "                self._complete_multipart_upload()",
            "            meta.result_status = ResultStatus.UPLOADED",
            "            meta.dst_file_size = meta.upload_size",
            "            logger.debug(f\"{meta.src_file_name} upload is completed.\")",
            "        else:",
            "            # TODO: add more error details to result/meta",
            "            meta.dst_file_size = 0",
            "            logger.debug(f\"{meta.src_file_name} upload is aborted.\")",
            "            if self.num_of_chunks > 1:",
            "                self._abort_multipart_upload()",
            "            meta.result_status = ResultStatus.ERROR",
            "",
            "    @abstractmethod",
            "    def _has_expired_token(self, response: requests.Response) -> bool:",
            "        pass",
            "",
            "    def _send_request_with_retry(",
            "        self,",
            "        verb: str,",
            "        get_request_args: Callable[[], tuple[bytes, dict[str, Any]]],",
            "        retry_id: int,",
            "    ) -> requests.Response:",
            "        rest_call = METHODS[verb]",
            "        url = b\"\"",
            "        conn = None",
            "        if self.meta.sfagent and self.meta.sfagent._cursor.connection:",
            "            conn = self.meta.sfagent._cursor.connection",
            "",
            "        while self.retry_count[retry_id] < self.max_retry:",
            "            cur_timestamp = self.credentials.timestamp",
            "            url, rest_kwargs = get_request_args()",
            "            rest_kwargs[\"timeout\"] = (REQUEST_CONNECTION_TIMEOUT, REQUEST_READ_TIMEOUT)",
            "            try:",
            "                if conn:",
            "                    with conn._rest._use_requests_session(url) as session:",
            "                        logger.debug(f\"storage client request with session {session}\")",
            "                        response = session.request(verb, url, **rest_kwargs)",
            "                else:",
            "                    logger.debug(\"storage client request with new session\")",
            "                    response = rest_call(url, **rest_kwargs)",
            "",
            "                if self._has_expired_presigned_url(response):",
            "                    self._update_presigned_url()",
            "                else:",
            "                    self.last_err_is_presigned_url = False",
            "                    if response.status_code in self.TRANSIENT_HTTP_ERR:",
            "                        time.sleep(",
            "                            min(",
            "                                # TODO should SLEEP_UNIT come from the parent",
            "                                #  SnowflakeConnection and be customizable by users?",
            "                                (2 ** self.retry_count[retry_id]) * self.SLEEP_UNIT,",
            "                                self.SLEEP_MAX,",
            "                            )",
            "                        )",
            "                        self.retry_count[retry_id] += 1",
            "                    elif self._has_expired_token(response):",
            "                        self.credentials.update(cur_timestamp)",
            "                    else:",
            "                        return response",
            "            except self.TRANSIENT_ERRORS as e:",
            "                self.last_err_is_presigned_url = False",
            "                time.sleep(",
            "                    min(",
            "                        (2 ** self.retry_count[retry_id]) * self.SLEEP_UNIT,",
            "                        self.SLEEP_MAX,",
            "                    )",
            "                )",
            "                logger.warning(f\"{verb} with url {url} failed for transient error: {e}\")",
            "                self.retry_count[retry_id] += 1",
            "        else:",
            "            raise RequestExceedMaxRetryError(",
            "                f\"{verb} with url {url} failed for exceeding maximum retries.\"",
            "            )",
            "",
            "    def prepare_download(self) -> None:",
            "        # TODO: add nicer error message for when target directory is not writeable",
            "        #  but this should be done before we get here",
            "        base_dir = os.path.dirname(self.full_dst_file_name)",
            "        if not os.path.exists(base_dir):",
            "            os.makedirs(base_dir)",
            "",
            "        # HEAD",
            "        file_header = self.get_file_header(self.meta.real_src_file_name)",
            "",
            "        if file_header and file_header.encryption_metadata:",
            "            self.encryption_metadata = file_header.encryption_metadata",
            "",
            "        self.num_of_chunks = 1",
            "        if file_header and file_header.content_length:",
            "            self.meta.src_file_size = file_header.content_length",
            "            if (",
            "                self.chunked_transfer",
            "                and self.meta.src_file_size > self.meta.multipart_threshold",
            "            ):",
            "                self.num_of_chunks = ceil(file_header.content_length / self.chunk_size)",
            "",
            "        # Preallocate encrypted file.",
            "        with self.intermediate_dst_path.open(\"wb+\") as fd:",
            "            fd.truncate(self.meta.src_file_size)",
            "",
            "    def write_downloaded_chunk(self, chunk_id: int, data: bytes) -> None:",
            "        \"\"\"Writes given data to the temp location starting at chunk_id * chunk_size.\"\"\"",
            "        # TODO: should we use chunking and write content in smaller chunks?",
            "        with self.intermediate_dst_path.open(\"rb+\") as fd:",
            "            fd.seek(self.chunk_size * chunk_id)",
            "            fd.write(data)",
            "",
            "    def finish_download(self) -> None:",
            "        meta = self.meta",
            "        if self.num_of_chunks != 0 and self.successful_transfers == self.num_of_chunks:",
            "            meta.result_status = ResultStatus.DOWNLOADED",
            "            if meta.encryption_material:",
            "                logger.debug(f\"encrypted data file={self.full_dst_file_name}\")",
            "                # For storage utils that do not have the privilege of",
            "                # getting the metadata early, both object and metadata",
            "                # are downloaded at once. In which case, the file meta will",
            "                # be updated with all the metadata that we need and",
            "                # then we can call get_file_header to get just that and also",
            "                # preserve the idea of getting metadata in the first place.",
            "                # One example of this is the utils that use presigned url",
            "                # for upload/download and not the storage client library.",
            "                if meta.presigned_url is not None:",
            "                    file_header = self.get_file_header(meta.src_file_name)",
            "                    self.encryption_metadata = file_header.encryption_metadata",
            "",
            "                tmp_dst_file_name = SnowflakeEncryptionUtil.decrypt_file(",
            "                    self.encryption_metadata,",
            "                    meta.encryption_material,",
            "                    str(self.intermediate_dst_path),",
            "                    tmp_dir=self.tmp_dir,",
            "                )",
            "                shutil.move(tmp_dst_file_name, self.full_dst_file_name)",
            "                self.intermediate_dst_path.unlink()",
            "            else:",
            "                logger.debug(f\"not encrypted data file={self.full_dst_file_name}\")",
            "                shutil.move(str(self.intermediate_dst_path), self.full_dst_file_name)",
            "            stat_info = os.stat(self.full_dst_file_name)",
            "            meta.dst_file_size = stat_info.st_size",
            "        else:",
            "            # TODO: add more error details to result/meta",
            "            if os.path.isfile(self.full_dst_file_name):",
            "                os.unlink(self.full_dst_file_name)",
            "            logger.exception(f\"Failed to download a file: {self.full_dst_file_name}\")",
            "            meta.dst_file_size = -1",
            "            meta.result_status = ResultStatus.ERROR",
            "",
            "    def upload_chunk(self, chunk_id: int) -> None:",
            "        new_stream = not bool(self.meta.src_stream or self.meta.intermediate_stream)",
            "        fd = (",
            "            self.meta.src_stream",
            "            or self.meta.intermediate_stream",
            "            or open(self.data_file, \"rb\")",
            "        )",
            "        try:",
            "            if self.num_of_chunks == 1:",
            "                _data = fd.read()",
            "            else:",
            "                fd.seek(chunk_id * self.chunk_size)",
            "                _data = fd.read(self.chunk_size)",
            "        finally:",
            "            if new_stream:",
            "                fd.close()",
            "        logger.debug(f\"Uploading chunk {chunk_id} of file {self.data_file}\")",
            "        self._upload_chunk(chunk_id, _data)",
            "        logger.debug(f\"Successfully uploaded chunk {chunk_id} of file {self.data_file}\")",
            "",
            "    @abstractmethod",
            "    def _upload_chunk(self, chunk_id: int, chunk: bytes) -> None:",
            "        pass",
            "",
            "    @abstractmethod",
            "    def download_chunk(self, chunk_id: int) -> None:",
            "        pass",
            "",
            "    # Override in GCS",
            "    def _has_expired_presigned_url(self, response: requests.Response) -> bool:",
            "        return False",
            "",
            "    # Override in GCS",
            "    def _update_presigned_url(self) -> None:",
            "        return",
            "",
            "    # Override in S3",
            "    def _initiate_multipart_upload(self) -> None:",
            "        return",
            "",
            "    # Override in S3",
            "    def _complete_multipart_upload(self) -> None:",
            "        return",
            "",
            "    # Override in S3",
            "    def _abort_multipart_upload(self) -> None:",
            "        return",
            "",
            "    def delete_client_data(self) -> None:",
            "        \"\"\"Deletes the tmp_dir and closes the source stream belonging to this client.",
            "        This function is idempotent.\"\"\"",
            "        if os.path.exists(self.tmp_dir):",
            "            logger.debug(f\"cleaning up tmp dir: {self.tmp_dir}\")",
            "            try:",
            "                shutil.rmtree(self.tmp_dir)",
            "            except OSError as ex:",
            "                # it's ok to ignore the exception here because another thread might",
            "                # have cleaned up the temp directory",
            "                logger.debug(f\"Failed to delete {self.tmp_dir}: {ex}\")",
            "        if self.meta.src_stream and not self.meta.src_stream.closed:",
            "            self.meta.src_stream.close()",
            "",
            "    def __del__(self) -> None:",
            "        self.delete_client_data()"
        ],
        "afterPatchFile": [
            "#",
            "# Copyright (c) 2012-2023 Snowflake Computing Inc. All rights reserved.",
            "#",
            "",
            "from __future__ import annotations",
            "",
            "import os",
            "import shutil",
            "import tempfile",
            "import threading",
            "import time",
            "from abc import ABC, abstractmethod",
            "from collections import defaultdict",
            "from io import BytesIO",
            "from logging import getLogger",
            "from math import ceil",
            "from pathlib import Path",
            "from typing import TYPE_CHECKING, Any, Callable, NamedTuple",
            "",
            "import OpenSSL",
            "",
            "from .constants import (",
            "    HTTP_HEADER_CONTENT_ENCODING,",
            "    REQUEST_CONNECTION_TIMEOUT,",
            "    REQUEST_READ_TIMEOUT,",
            "    FileHeader,",
            "    ResultStatus,",
            ")",
            "from .encryption_util import EncryptionMetadata, SnowflakeEncryptionUtil",
            "from .errors import RequestExceedMaxRetryError",
            "from .file_util import SnowflakeFileUtil",
            "from .vendored import requests",
            "from .vendored.requests import ConnectionError, Timeout",
            "from .vendored.urllib3 import HTTPResponse",
            "",
            "if TYPE_CHECKING:  # pragma: no cover",
            "    from .file_transfer_agent import SnowflakeFileMeta, StorageCredential",
            "",
            "logger = getLogger(__name__)",
            "",
            "",
            "class SnowflakeFileEncryptionMaterial(NamedTuple):",
            "    query_stage_master_key: str  # query stage master key",
            "    query_id: str  # query id",
            "    smk_id: int  # SMK id",
            "",
            "",
            "METHODS = {",
            "    \"GET\": requests.get,",
            "    \"PUT\": requests.put,",
            "    \"POST\": requests.post,",
            "    \"HEAD\": requests.head,",
            "    \"DELETE\": requests.delete,",
            "}",
            "",
            "",
            "def remove_content_encoding(resp: requests.Response, **kwargs) -> None:",
            "    \"\"\"Remove content-encoding header and decoder so decompression is not triggered\"\"\"",
            "    if HTTP_HEADER_CONTENT_ENCODING in resp.headers:",
            "        if isinstance(resp.raw, HTTPResponse):",
            "            resp.raw._decoder = None",
            "            resp.raw.headers.pop(HTTP_HEADER_CONTENT_ENCODING)",
            "",
            "",
            "class SnowflakeStorageClient(ABC):",
            "    TRANSIENT_HTTP_ERR = (408, 429, 500, 502, 503, 504)",
            "",
            "    TRANSIENT_ERRORS = (OpenSSL.SSL.SysCallError, Timeout, ConnectionError)",
            "    SLEEP_MAX = 16.0",
            "    SLEEP_UNIT = 1.0",
            "",
            "    def __init__(",
            "        self,",
            "        meta: SnowflakeFileMeta,",
            "        stage_info: dict[str, Any],",
            "        chunk_size: int,",
            "        chunked_transfer: bool | None = True,",
            "        credentials: StorageCredential | None = None,",
            "        max_retry: int = 5,",
            "    ) -> None:",
            "        self.meta = meta",
            "        self.stage_info = stage_info",
            "        self.retry_count: dict[int | str, int] = defaultdict(int)",
            "        self.tmp_dir = tempfile.mkdtemp()",
            "        self.data_file: str | None = None",
            "        self.encryption_metadata: EncryptionMetadata | None = None",
            "",
            "        self.max_retry = max_retry  # TODO",
            "        self.credentials = credentials",
            "        # UPLOAD",
            "        meta.real_src_file_name = meta.src_file_name",
            "        meta.upload_size = meta.src_file_size",
            "        self.preprocessed = (",
            "            False  # so we don't repeat compression/file digest when re-encrypting",
            "        )",
            "        # DOWNLOAD",
            "        self.full_dst_file_name: str | None = (",
            "            os.path.join(",
            "                self.meta.local_location, os.path.basename(self.meta.dst_file_name)",
            "            )",
            "            if self.meta.local_location",
            "            else None",
            "        )",
            "        self.intermediate_dst_path: Path | None = (",
            "            Path(self.full_dst_file_name + \".part\")",
            "            if self.meta.local_location",
            "            else None",
            "        )",
            "        # CHUNK",
            "        self.chunked_transfer = chunked_transfer  # only true for GCS",
            "        self.chunk_size = chunk_size",
            "        self.num_of_chunks = 0",
            "        self.lock = threading.Lock()",
            "        self.successful_transfers: int = 0",
            "        self.failed_transfers: int = 0",
            "        # only used when PRESIGNED_URL expires",
            "        self.last_err_is_presigned_url = False",
            "",
            "    def compress(self) -> None:",
            "        if self.meta.require_compress:",
            "            meta = self.meta",
            "            logger.debug(f\"compressing file={meta.src_file_name}\")",
            "            if meta.intermediate_stream:",
            "                (",
            "                    meta.src_stream,",
            "                    upload_size,",
            "                ) = SnowflakeFileUtil.compress_with_gzip_from_stream(",
            "                    meta.intermediate_stream",
            "                )",
            "            else:",
            "                (",
            "                    meta.real_src_file_name,",
            "                    upload_size,",
            "                ) = SnowflakeFileUtil.compress_file_with_gzip(",
            "                    meta.src_file_name, self.tmp_dir",
            "                )",
            "",
            "    def get_digest(self) -> None:",
            "        meta = self.meta",
            "        logger.debug(f\"getting digest file={meta.real_src_file_name}\")",
            "        if meta.intermediate_stream is None:",
            "            (",
            "                meta.sha256_digest,",
            "                meta.upload_size,",
            "            ) = SnowflakeFileUtil.get_digest_and_size_for_file(meta.real_src_file_name)",
            "        else:",
            "            (",
            "                meta.sha256_digest,",
            "                meta.upload_size,",
            "            ) = SnowflakeFileUtil.get_digest_and_size_for_stream(",
            "                meta.src_stream or meta.intermediate_stream",
            "            )",
            "",
            "    def encrypt(self) -> None:",
            "        meta = self.meta",
            "        logger.debug(f\"encrypting file={meta.real_src_file_name}\")",
            "        if meta.intermediate_stream is None:",
            "            (",
            "                self.encryption_metadata,",
            "                self.data_file,",
            "            ) = SnowflakeEncryptionUtil.encrypt_file(",
            "                meta.encryption_material,",
            "                meta.real_src_file_name,",
            "                tmp_dir=self.tmp_dir,",
            "            )",
            "            meta.upload_size = os.path.getsize(self.data_file)",
            "        else:",
            "            encrypted_stream = BytesIO()",
            "            src_stream = meta.src_stream or meta.intermediate_stream",
            "            src_stream.seek(0)",
            "            self.encryption_metadata = SnowflakeEncryptionUtil.encrypt_stream(",
            "                meta.encryption_material, src_stream, encrypted_stream",
            "            )",
            "            src_stream.seek(0)",
            "            meta.upload_size = encrypted_stream.seek(0, os.SEEK_END)",
            "            encrypted_stream.seek(0)",
            "            if meta.src_stream is not None:",
            "                meta.src_stream.close()",
            "            meta.src_stream = encrypted_stream",
            "            self.data_file = meta.real_src_file_name",
            "",
            "    @abstractmethod",
            "    def get_file_header(self, filename: str) -> FileHeader | None:",
            "        \"\"\"Check if file exists in target location and obtain file metadata if exists.",
            "",
            "        Notes:",
            "            Updates meta.result_status.",
            "        \"\"\"",
            "        pass",
            "",
            "    def preprocess(self) -> None:",
            "        meta = self.meta",
            "        logger.debug(f\"Preprocessing {meta.src_file_name}\")",
            "",
            "        file_header = self.get_file_header(",
            "            meta.dst_file_name",
            "        )  # check if file exists on remote",
            "        if not meta.overwrite:",
            "            self.get_digest()  # self.get_file_header needs digest for multiparts upload when aws is used.",
            "            if meta.result_status == ResultStatus.UPLOADED:",
            "                # Skipped",
            "                logger.debug(",
            "                    f'file already exists location=\"{self.stage_info[\"location\"]}\", '",
            "                    f'file_name=\"{meta.dst_file_name}\"'",
            "                )",
            "                meta.dst_file_size = 0",
            "                meta.result_status = ResultStatus.SKIPPED",
            "                self.preprocessed = True",
            "                return",
            "        # Uploading",
            "        if meta.require_compress:",
            "            self.compress()",
            "        self.get_digest()",
            "",
            "        if (",
            "            meta.skip_upload_on_content_match",
            "            and file_header",
            "            and meta.sha256_digest == file_header.digest",
            "        ):",
            "            logger.debug(f\"same file contents for {meta.name}, skipping upload\")",
            "            meta.result_status = ResultStatus.SKIPPED",
            "",
            "        self.preprocessed = True",
            "",
            "    def prepare_upload(self) -> None:",
            "        meta = self.meta",
            "",
            "        if not self.preprocessed:",
            "            self.preprocess()",
            "        elif meta.encryption_material:",
            "            # need to clean up previous encrypted file",
            "            os.remove(self.data_file)",
            "",
            "        logger.debug(f\"Preparing to upload {meta.src_file_name}\")",
            "",
            "        if meta.encryption_material:",
            "            self.encrypt()",
            "        else:",
            "            self.data_file = meta.real_src_file_name",
            "        logger.debug(\"finished preprocessing\")",
            "        if meta.upload_size < meta.multipart_threshold or not self.chunked_transfer:",
            "            self.num_of_chunks = 1",
            "        else:",
            "            self.num_of_chunks = ceil(meta.upload_size / self.chunk_size)",
            "        logger.debug(f\"number of chunks {self.num_of_chunks}\")",
            "        # clean up",
            "        self.retry_count = {}",
            "",
            "        for chunk_id in range(self.num_of_chunks):",
            "            self.retry_count[chunk_id] = 0",
            "        if self.chunked_transfer and self.num_of_chunks > 1:",
            "            self._initiate_multipart_upload()",
            "",
            "    def finish_upload(self) -> None:",
            "        meta = self.meta",
            "        if self.successful_transfers == self.num_of_chunks and self.num_of_chunks != 0:",
            "            if self.num_of_chunks > 1:",
            "                self._complete_multipart_upload()",
            "            meta.result_status = ResultStatus.UPLOADED",
            "            meta.dst_file_size = meta.upload_size",
            "            logger.debug(f\"{meta.src_file_name} upload is completed.\")",
            "        else:",
            "            # TODO: add more error details to result/meta",
            "            meta.dst_file_size = 0",
            "            logger.debug(f\"{meta.src_file_name} upload is aborted.\")",
            "            if self.num_of_chunks > 1:",
            "                self._abort_multipart_upload()",
            "            meta.result_status = ResultStatus.ERROR",
            "",
            "    @abstractmethod",
            "    def _has_expired_token(self, response: requests.Response) -> bool:",
            "        pass",
            "",
            "    def _send_request_with_retry(",
            "        self,",
            "        verb: str,",
            "        get_request_args: Callable[[], tuple[bytes, dict[str, Any]]],",
            "        retry_id: int,",
            "    ) -> requests.Response:",
            "        rest_call = METHODS[verb]",
            "        url = b\"\"",
            "        conn = None",
            "        if self.meta.sfagent and self.meta.sfagent._cursor.connection:",
            "            conn = self.meta.sfagent._cursor.connection",
            "",
            "        while self.retry_count[retry_id] < self.max_retry:",
            "            cur_timestamp = self.credentials.timestamp",
            "            url, rest_kwargs = get_request_args()",
            "            rest_kwargs[\"timeout\"] = (REQUEST_CONNECTION_TIMEOUT, REQUEST_READ_TIMEOUT)",
            "            try:",
            "                if conn:",
            "                    with conn._rest._use_requests_session(url) as session:",
            "                        logger.debug(f\"storage client request with session {session}\")",
            "                        response = session.request(verb, url, **rest_kwargs)",
            "                else:",
            "                    logger.debug(\"storage client request with new session\")",
            "                    response = rest_call(url, **rest_kwargs)",
            "",
            "                if self._has_expired_presigned_url(response):",
            "                    self._update_presigned_url()",
            "                else:",
            "                    self.last_err_is_presigned_url = False",
            "                    if response.status_code in self.TRANSIENT_HTTP_ERR:",
            "                        time.sleep(",
            "                            min(",
            "                                # TODO should SLEEP_UNIT come from the parent",
            "                                #  SnowflakeConnection and be customizable by users?",
            "                                (2 ** self.retry_count[retry_id]) * self.SLEEP_UNIT,",
            "                                self.SLEEP_MAX,",
            "                            )",
            "                        )",
            "                        self.retry_count[retry_id] += 1",
            "                    elif self._has_expired_token(response):",
            "                        self.credentials.update(cur_timestamp)",
            "                    else:",
            "                        return response",
            "            except self.TRANSIENT_ERRORS as e:",
            "                self.last_err_is_presigned_url = False",
            "                time.sleep(",
            "                    min(",
            "                        (2 ** self.retry_count[retry_id]) * self.SLEEP_UNIT,",
            "                        self.SLEEP_MAX,",
            "                    )",
            "                )",
            "                logger.warning(f\"{verb} with url {url} failed for transient error: {e}\")",
            "                self.retry_count[retry_id] += 1",
            "        else:",
            "            raise RequestExceedMaxRetryError(",
            "                f\"{verb} with url {url} failed for exceeding maximum retries.\"",
            "            )",
            "",
            "    def _open_intermediate_dst_path(self, mode):",
            "        if not self.intermediate_dst_path.exists():",
            "            self.intermediate_dst_path.touch(mode=0o600)",
            "        return self.intermediate_dst_path.open(mode)",
            "",
            "    def prepare_download(self) -> None:",
            "        # TODO: add nicer error message for when target directory is not writeable",
            "        #  but this should be done before we get here",
            "        base_dir = os.path.dirname(self.full_dst_file_name)",
            "        if not os.path.exists(base_dir):",
            "            os.makedirs(base_dir)",
            "",
            "        # HEAD",
            "        file_header = self.get_file_header(self.meta.real_src_file_name)",
            "",
            "        if file_header and file_header.encryption_metadata:",
            "            self.encryption_metadata = file_header.encryption_metadata",
            "",
            "        self.num_of_chunks = 1",
            "        if file_header and file_header.content_length:",
            "            self.meta.src_file_size = file_header.content_length",
            "            if (",
            "                self.chunked_transfer",
            "                and self.meta.src_file_size > self.meta.multipart_threshold",
            "            ):",
            "                self.num_of_chunks = ceil(file_header.content_length / self.chunk_size)",
            "",
            "        # Preallocate encrypted file.",
            "        with self._open_intermediate_dst_path(\"wb+\") as fd:",
            "            fd.truncate(self.meta.src_file_size)",
            "",
            "    def write_downloaded_chunk(self, chunk_id: int, data: bytes) -> None:",
            "        \"\"\"Writes given data to the temp location starting at chunk_id * chunk_size.\"\"\"",
            "        # TODO: should we use chunking and write content in smaller chunks?",
            "        with self._open_intermediate_dst_path(\"rb+\") as fd:",
            "            fd.seek(self.chunk_size * chunk_id)",
            "            fd.write(data)",
            "",
            "    def finish_download(self) -> None:",
            "        meta = self.meta",
            "        if self.num_of_chunks != 0 and self.successful_transfers == self.num_of_chunks:",
            "            meta.result_status = ResultStatus.DOWNLOADED",
            "            if meta.encryption_material:",
            "                logger.debug(f\"encrypted data file={self.full_dst_file_name}\")",
            "                # For storage utils that do not have the privilege of",
            "                # getting the metadata early, both object and metadata",
            "                # are downloaded at once. In which case, the file meta will",
            "                # be updated with all the metadata that we need and",
            "                # then we can call get_file_header to get just that and also",
            "                # preserve the idea of getting metadata in the first place.",
            "                # One example of this is the utils that use presigned url",
            "                # for upload/download and not the storage client library.",
            "                if meta.presigned_url is not None:",
            "                    file_header = self.get_file_header(meta.src_file_name)",
            "                    self.encryption_metadata = file_header.encryption_metadata",
            "",
            "                tmp_dst_file_name = SnowflakeEncryptionUtil.decrypt_file(",
            "                    self.encryption_metadata,",
            "                    meta.encryption_material,",
            "                    str(self.intermediate_dst_path),",
            "                    tmp_dir=self.tmp_dir,",
            "                )",
            "                shutil.move(tmp_dst_file_name, self.full_dst_file_name)",
            "                self.intermediate_dst_path.unlink()",
            "            else:",
            "                logger.debug(f\"not encrypted data file={self.full_dst_file_name}\")",
            "                shutil.move(str(self.intermediate_dst_path), self.full_dst_file_name)",
            "            stat_info = os.stat(self.full_dst_file_name)",
            "            meta.dst_file_size = stat_info.st_size",
            "        else:",
            "            # TODO: add more error details to result/meta",
            "            if os.path.isfile(self.full_dst_file_name):",
            "                os.unlink(self.full_dst_file_name)",
            "            logger.exception(f\"Failed to download a file: {self.full_dst_file_name}\")",
            "            meta.dst_file_size = -1",
            "            meta.result_status = ResultStatus.ERROR",
            "",
            "    def upload_chunk(self, chunk_id: int) -> None:",
            "        new_stream = not bool(self.meta.src_stream or self.meta.intermediate_stream)",
            "        fd = (",
            "            self.meta.src_stream",
            "            or self.meta.intermediate_stream",
            "            or open(self.data_file, \"rb\")",
            "        )",
            "        try:",
            "            if self.num_of_chunks == 1:",
            "                _data = fd.read()",
            "            else:",
            "                fd.seek(chunk_id * self.chunk_size)",
            "                _data = fd.read(self.chunk_size)",
            "        finally:",
            "            if new_stream:",
            "                fd.close()",
            "        logger.debug(f\"Uploading chunk {chunk_id} of file {self.data_file}\")",
            "        self._upload_chunk(chunk_id, _data)",
            "        logger.debug(f\"Successfully uploaded chunk {chunk_id} of file {self.data_file}\")",
            "",
            "    @abstractmethod",
            "    def _upload_chunk(self, chunk_id: int, chunk: bytes) -> None:",
            "        pass",
            "",
            "    @abstractmethod",
            "    def download_chunk(self, chunk_id: int) -> None:",
            "        pass",
            "",
            "    # Override in GCS",
            "    def _has_expired_presigned_url(self, response: requests.Response) -> bool:",
            "        return False",
            "",
            "    # Override in GCS",
            "    def _update_presigned_url(self) -> None:",
            "        return",
            "",
            "    # Override in S3",
            "    def _initiate_multipart_upload(self) -> None:",
            "        return",
            "",
            "    # Override in S3",
            "    def _complete_multipart_upload(self) -> None:",
            "        return",
            "",
            "    # Override in S3",
            "    def _abort_multipart_upload(self) -> None:",
            "        return",
            "",
            "    def delete_client_data(self) -> None:",
            "        \"\"\"Deletes the tmp_dir and closes the source stream belonging to this client.",
            "        This function is idempotent.\"\"\"",
            "        if os.path.exists(self.tmp_dir):",
            "            logger.debug(f\"cleaning up tmp dir: {self.tmp_dir}\")",
            "            try:",
            "                shutil.rmtree(self.tmp_dir)",
            "            except OSError as ex:",
            "                # it's ok to ignore the exception here because another thread might",
            "                # have cleaned up the temp directory",
            "                logger.debug(f\"Failed to delete {self.tmp_dir}: {ex}\")",
            "        if self.meta.src_stream and not self.meta.src_stream.closed:",
            "            self.meta.src_stream.close()",
            "",
            "    def __del__(self) -> None:",
            "        self.delete_client_data()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "355": [
                "SnowflakeStorageClient",
                "prepare_download"
            ],
            "361": [
                "SnowflakeStorageClient",
                "write_downloaded_chunk"
            ]
        },
        "addLocation": [
            "jupyter_server.base.handlers.APIHandler.content_security_policy",
            "src.snowflake.connector.storage_client.SnowflakeStorageClient.write_downloaded_chunk",
            "src.snowflake.connector.storage_client.SnowflakeStorageClient.self",
            "src.snowflake.connector.storage_client.SnowflakeStorageClient.prepare_download"
        ]
    },
    "src/snowflake/connector/util_text.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " from __future__ import annotations"
            },
            "2": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 8,
                "PatchRowcode": "+import base64"
            },
            "4": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " import hashlib"
            },
            "5": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " import logging"
            },
            "6": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " import random"
            },
            "7": {
                "beforePatchRowNumber": 292,
                "afterPatchRowNumber": 293,
                "PatchRowcode": "     return \"\".join([prefix, random_part, suffix])"
            },
            "8": {
                "beforePatchRowNumber": 293,
                "afterPatchRowNumber": 294,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 294,
                "afterPatchRowNumber": 295,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 296,
                "PatchRowcode": "+def _base64_bytes_to_str(x) -> str | None:"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 297,
                "PatchRowcode": "+    return base64.b64encode(x).decode(\"utf-8\") if x else None"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 298,
                "PatchRowcode": "+"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 299,
                "PatchRowcode": "+"
            },
            "14": {
                "beforePatchRowNumber": 295,
                "afterPatchRowNumber": 300,
                "PatchRowcode": " def get_md5(text: str | bytes) -> bytes:"
            },
            "15": {
                "beforePatchRowNumber": 296,
                "afterPatchRowNumber": 301,
                "PatchRowcode": "     if isinstance(text, str):"
            },
            "16": {
                "beforePatchRowNumber": 297,
                "afterPatchRowNumber": 302,
                "PatchRowcode": "         text = text.encode(\"utf-8\")"
            }
        },
        "frontPatchFile": [
            "#!/usr/bin/env python",
            "#",
            "# Copyright (c) 2012-2023 Snowflake Computing Inc. All rights reserved.",
            "#",
            "",
            "from __future__ import annotations",
            "",
            "import hashlib",
            "import logging",
            "import random",
            "import re",
            "import string",
            "from io import StringIO",
            "from typing import Sequence",
            "",
            "COMMENT_PATTERN_RE = re.compile(r\"^\\s*\\-\\-\")",
            "EMPTY_LINE_RE = re.compile(r\"^\\s*$\")",
            "",
            "_logger = logging.getLogger(__name__)",
            "",
            "",
            "class SQLDelimiter:",
            "    \"\"\"Class that wraps a SQL delimiter string.",
            "",
            "    Since split_statements is a generator this mutable object will allow it change while executing.",
            "    \"\"\"",
            "",
            "    def __str__(self) -> str:",
            "        return self.sql_delimiter",
            "",
            "    def __init__(self, sql_delimiter: str = \";\") -> None:",
            "        \"\"\"Initializes SQLDelimiter with a string.\"\"\"",
            "        self.sql_delimiter = sql_delimiter",
            "",
            "",
            "def split_statements(",
            "    buf: StringIO,",
            "    remove_comments: bool = False,",
            "    delimiter: SQLDelimiter | None = None,",
            "):",
            "    \"\"\"Splits a stream into SQL statements (ends with a semicolon) or commands (!...).",
            "",
            "    Args:",
            "        buf: Unicode data stream.",
            "        remove_comments: Whether or not to remove all comments (Default value = False).",
            "        delimiter: The delimiter string that separates SQL commands from each other.",
            "",
            "    Yields:",
            "        A SQL statement or a command.",
            "    \"\"\"",
            "    if delimiter is None:",
            "        delimiter = SQLDelimiter()  # Use default delimiter if none was given.",
            "    in_quote = False",
            "    ch_quote = None",
            "    in_comment = False",
            "    in_double_dollars = False",
            "    previous_delimiter = None",
            "",
            "    line = buf.readline()",
            "    if isinstance(line, bytes):",
            "        raise TypeError(\"Input data must not be binary type.\")",
            "",
            "    statement = []",
            "    while line != \"\":",
            "        col = 0",
            "        col0 = 0",
            "        len_line = len(line)",
            "        sql_delimiter = delimiter.sql_delimiter",
            "        if not previous_delimiter or sql_delimiter != previous_delimiter:",
            "            # Only (re)compile new Regexes if they should be",
            "            escaped_delim = re.escape(sql_delimiter)",
            "            # Special characters possible in the sql delimiter are '_', '/' and ';'. If a delimiter does not end, or",
            "            # start with a special character then look for word separation with \\b regex.",
            "            if re.match(r\"\\w\", sql_delimiter[0]):",
            "                RE_START = re.compile(rf\"^[^\\w$]?{escaped_delim}\")",
            "            else:",
            "                RE_START = re.compile(rf\"^.?{escaped_delim}\")",
            "            if re.match(r\"\\w\", sql_delimiter[-1]):",
            "                RE_END = re.compile(rf\"{escaped_delim}[^\\w$]?$\")",
            "            else:",
            "                RE_END = re.compile(rf\"{escaped_delim}.?$\")",
            "            previous_delimiter = sql_delimiter",
            "        while True:",
            "            if col >= len_line:",
            "                if col0 < col:",
            "                    if not in_comment and not in_quote and not in_double_dollars:",
            "                        statement.append((line[col0:col], True))",
            "                        if len(statement) == 1 and statement[0][0] == \"\":",
            "                            statement = []",
            "                        break",
            "                    elif not in_comment and (in_quote or in_double_dollars):",
            "                        statement.append((line[col0:col], True))",
            "                    elif not remove_comments:",
            "                        statement.append((line[col0:col], False))",
            "                break",
            "            elif in_comment:",
            "                if line[col:].startswith(\"*/\"):",
            "                    in_comment = False",
            "                    if not remove_comments:",
            "                        statement.append((line[col0 : col + 2], False))",
            "                    col += 2",
            "                    col0 = col",
            "                else:",
            "                    col += 1",
            "            elif in_double_dollars:",
            "                if line[col:].startswith(\"$$\"):",
            "                    in_double_dollars = False",
            "                    statement.append((line[col0 : col + 2], False))",
            "                    col += 2",
            "                    col0 = col",
            "                else:",
            "                    col += 1",
            "            elif in_quote:",
            "                if (",
            "                    line[col] == \"\\\\\"",
            "                    and col < len_line - 1",
            "                    and line[col + 1] in (ch_quote, \"\\\\\")",
            "                ):",
            "                    col += 2",
            "                elif line[col] == ch_quote:",
            "                    if (",
            "                        col < len_line - 1",
            "                        and line[col + 1] != ch_quote",
            "                        or col == len_line - 1",
            "                    ):",
            "                        # exits quote",
            "                        in_quote = False",
            "                        statement.append((line[col0 : col + 1], True))",
            "                        col += 1",
            "                        col0 = col",
            "                    else:",
            "                        # escaped quote and still in quote",
            "                        col += 2",
            "                else:",
            "                    col += 1",
            "            else:",
            "                if line[col] in (\"'\", '\"'):",
            "                    in_quote = True",
            "                    ch_quote = line[col]",
            "                    col += 1",
            "                elif line[col] in (\" \", \"\\t\"):",
            "                    statement.append((line[col0 : col + 1], True))",
            "                    col += 1",
            "                    col0 = col",
            "                elif line[col:].startswith(\"--\"):",
            "                    statement.append((line[col0:col], True))",
            "                    if not remove_comments:",
            "                        # keep the comment",
            "                        statement.append((line[col:], False))",
            "                    else:",
            "                        statement.append((\"\\n\", True))",
            "                    col = len_line + 1",
            "                    col0 = col",
            "                elif line[col:].startswith(\"/*\") and not line[col0:].startswith(",
            "                    \"file://\"",
            "                ):",
            "                    if not remove_comments:",
            "                        statement.append((line[col0 : col + 2], False))",
            "                    else:",
            "                        statement.append((line[col0:col], False))",
            "                    col += 2",
            "                    col0 = col",
            "                    in_comment = True",
            "                elif line[col:].startswith(\"$$\"):",
            "                    statement.append((line[col0 : col + 2], True))",
            "                    col += 2",
            "                    col0 = col",
            "                    in_double_dollars = True",
            "                elif (",
            "                    RE_START.match(line[col - 1 : col + len(sql_delimiter)])",
            "                    if col > 0",
            "                    else (RE_START.match(line[col : col + len(sql_delimiter)]))",
            "                ) and (RE_END.match(line[col : col + len(sql_delimiter) + 1])):",
            "                    statement.append((line[col0:col] + \";\", True))",
            "                    col += len(sql_delimiter)",
            "                    try:",
            "                        if line[col] == \">\":",
            "                            col += 1",
            "                            statement[-1] = (statement[-1][0] + \">\", statement[-1][1])",
            "                    except IndexError:",
            "                        pass",
            "                    if COMMENT_PATTERN_RE.match(line[col:]) or EMPTY_LINE_RE.match(",
            "                        line[col:]",
            "                    ):",
            "                        if not remove_comments:",
            "                            # keep the comment",
            "                            statement.append((line[col:], False))",
            "                        col = len_line",
            "                    while col < len_line and line[col] in (\" \", \"\\t\"):",
            "                        col += 1",
            "                    yield _concatenate_statements(statement)",
            "                    col0 = col",
            "                    statement = []",
            "                elif col == 0 and line[col] == \"!\":  # command",
            "                    if len(statement) > 0:",
            "                        yield _concatenate_statements(statement)",
            "                        statement = []",
            "                    yield (",
            "                        line.strip()[: -len(sql_delimiter)]",
            "                        if line.strip().endswith(sql_delimiter)",
            "                        else line.strip()",
            "                    ).strip(), False",
            "                    break",
            "                else:",
            "                    col += 1",
            "        line = buf.readline()",
            "",
            "    if len(statement) > 0:",
            "        yield _concatenate_statements(statement)",
            "",
            "",
            "def _concatenate_statements(",
            "    statement_list: list[tuple[str, bool]]",
            ") -> tuple[str, bool | None]:",
            "    \"\"\"Concatenate statements.",
            "",
            "    Each statement should be a tuple of statement and is_put_or_get.",
            "",
            "    The is_put_or_get is set to True if the statement is PUT or GET otherwise False for valid statement.",
            "    None is set if the statement is empty or comment only.",
            "",
            "    Args:",
            "        statement_list: List of statement parts.",
            "",
            "    Returns:",
            "        Tuple of statements and whether they are PUT or GET.",
            "    \"\"\"",
            "    valid_statement_list = []",
            "    is_put_or_get = None",
            "    for text, is_statement in statement_list:",
            "        valid_statement_list.append(text)",
            "        if is_put_or_get is None and is_statement and len(text.strip()) >= 3:",
            "            is_put_or_get = text[:3].upper() in (\"PUT\", \"GET\")",
            "    return \"\".join(valid_statement_list).strip(), is_put_or_get",
            "",
            "",
            "def construct_hostname(region: str | None, account: str) -> str:",
            "    \"\"\"Constructs hostname from region and account.\"\"\"",
            "",
            "    def _is_china_region(r: str) -> bool:",
            "        # This is consistent with the Go driver:",
            "        # https://github.com/snowflakedb/gosnowflake/blob/f20a46475dce322f3f6b97b4a72f2807571e750b/dsn.go#L535",
            "        return r.lower().startswith(\"cn-\")",
            "",
            "    if region == \"us-west-2\":",
            "        region = \"\"",
            "    if region:",
            "        if account.find(\".\") > 0:",
            "            account = account[0 : account.find(\".\")]",
            "        top_level_domain = \"cn\" if _is_china_region(region) else \"com\"",
            "        host = f\"{account}.{region}.snowflakecomputing.{top_level_domain}\"",
            "    else:",
            "        top_level_domain = \"com\"",
            "        if account.find(\".\") > 0 and _is_china_region(account.split(\".\")[1]):",
            "            top_level_domain = \"cn\"",
            "        host = f\"{account}.snowflakecomputing.{top_level_domain}\"",
            "    return host",
            "",
            "",
            "def parse_account(account):",
            "    url_parts = account.split(\".\")",
            "    # if this condition is true, then we have some extra",
            "    # stuff in the account field.",
            "    if len(url_parts) > 1:",
            "        if url_parts[1] == \"global\":",
            "            # remove external ID from account",
            "            parsed_account = url_parts[0][0 : url_parts[0].rfind(\"-\")]",
            "        else:",
            "            # remove region subdomain",
            "            parsed_account = url_parts[0]",
            "    else:",
            "        parsed_account = account",
            "",
            "    return parsed_account",
            "",
            "",
            "def random_string(",
            "    length: int = 10,",
            "    prefix: str = \"\",",
            "    suffix: str = \"\",",
            "    choices: Sequence[str] = string.ascii_lowercase,",
            ") -> str:",
            "    \"\"\"Our convenience function to generate random string for object names.",
            "",
            "    Args:",
            "        length: How many random characters to choose from choices.",
            "        prefix: Prefix to add to random string generated.",
            "        suffix: Suffix to add to random string generated.",
            "        choices: A generator of things to choose from.",
            "    \"\"\"",
            "    random_part = \"\".join([random.Random().choice(choices) for _ in range(length)])",
            "    return \"\".join([prefix, random_part, suffix])",
            "",
            "",
            "def get_md5(text: str | bytes) -> bytes:",
            "    if isinstance(text, str):",
            "        text = text.encode(\"utf-8\")",
            "    md5 = hashlib.md5()",
            "    md5.update(text)",
            "    return md5.digest()"
        ],
        "afterPatchFile": [
            "#!/usr/bin/env python",
            "#",
            "# Copyright (c) 2012-2023 Snowflake Computing Inc. All rights reserved.",
            "#",
            "",
            "from __future__ import annotations",
            "",
            "import base64",
            "import hashlib",
            "import logging",
            "import random",
            "import re",
            "import string",
            "from io import StringIO",
            "from typing import Sequence",
            "",
            "COMMENT_PATTERN_RE = re.compile(r\"^\\s*\\-\\-\")",
            "EMPTY_LINE_RE = re.compile(r\"^\\s*$\")",
            "",
            "_logger = logging.getLogger(__name__)",
            "",
            "",
            "class SQLDelimiter:",
            "    \"\"\"Class that wraps a SQL delimiter string.",
            "",
            "    Since split_statements is a generator this mutable object will allow it change while executing.",
            "    \"\"\"",
            "",
            "    def __str__(self) -> str:",
            "        return self.sql_delimiter",
            "",
            "    def __init__(self, sql_delimiter: str = \";\") -> None:",
            "        \"\"\"Initializes SQLDelimiter with a string.\"\"\"",
            "        self.sql_delimiter = sql_delimiter",
            "",
            "",
            "def split_statements(",
            "    buf: StringIO,",
            "    remove_comments: bool = False,",
            "    delimiter: SQLDelimiter | None = None,",
            "):",
            "    \"\"\"Splits a stream into SQL statements (ends with a semicolon) or commands (!...).",
            "",
            "    Args:",
            "        buf: Unicode data stream.",
            "        remove_comments: Whether or not to remove all comments (Default value = False).",
            "        delimiter: The delimiter string that separates SQL commands from each other.",
            "",
            "    Yields:",
            "        A SQL statement or a command.",
            "    \"\"\"",
            "    if delimiter is None:",
            "        delimiter = SQLDelimiter()  # Use default delimiter if none was given.",
            "    in_quote = False",
            "    ch_quote = None",
            "    in_comment = False",
            "    in_double_dollars = False",
            "    previous_delimiter = None",
            "",
            "    line = buf.readline()",
            "    if isinstance(line, bytes):",
            "        raise TypeError(\"Input data must not be binary type.\")",
            "",
            "    statement = []",
            "    while line != \"\":",
            "        col = 0",
            "        col0 = 0",
            "        len_line = len(line)",
            "        sql_delimiter = delimiter.sql_delimiter",
            "        if not previous_delimiter or sql_delimiter != previous_delimiter:",
            "            # Only (re)compile new Regexes if they should be",
            "            escaped_delim = re.escape(sql_delimiter)",
            "            # Special characters possible in the sql delimiter are '_', '/' and ';'. If a delimiter does not end, or",
            "            # start with a special character then look for word separation with \\b regex.",
            "            if re.match(r\"\\w\", sql_delimiter[0]):",
            "                RE_START = re.compile(rf\"^[^\\w$]?{escaped_delim}\")",
            "            else:",
            "                RE_START = re.compile(rf\"^.?{escaped_delim}\")",
            "            if re.match(r\"\\w\", sql_delimiter[-1]):",
            "                RE_END = re.compile(rf\"{escaped_delim}[^\\w$]?$\")",
            "            else:",
            "                RE_END = re.compile(rf\"{escaped_delim}.?$\")",
            "            previous_delimiter = sql_delimiter",
            "        while True:",
            "            if col >= len_line:",
            "                if col0 < col:",
            "                    if not in_comment and not in_quote and not in_double_dollars:",
            "                        statement.append((line[col0:col], True))",
            "                        if len(statement) == 1 and statement[0][0] == \"\":",
            "                            statement = []",
            "                        break",
            "                    elif not in_comment and (in_quote or in_double_dollars):",
            "                        statement.append((line[col0:col], True))",
            "                    elif not remove_comments:",
            "                        statement.append((line[col0:col], False))",
            "                break",
            "            elif in_comment:",
            "                if line[col:].startswith(\"*/\"):",
            "                    in_comment = False",
            "                    if not remove_comments:",
            "                        statement.append((line[col0 : col + 2], False))",
            "                    col += 2",
            "                    col0 = col",
            "                else:",
            "                    col += 1",
            "            elif in_double_dollars:",
            "                if line[col:].startswith(\"$$\"):",
            "                    in_double_dollars = False",
            "                    statement.append((line[col0 : col + 2], False))",
            "                    col += 2",
            "                    col0 = col",
            "                else:",
            "                    col += 1",
            "            elif in_quote:",
            "                if (",
            "                    line[col] == \"\\\\\"",
            "                    and col < len_line - 1",
            "                    and line[col + 1] in (ch_quote, \"\\\\\")",
            "                ):",
            "                    col += 2",
            "                elif line[col] == ch_quote:",
            "                    if (",
            "                        col < len_line - 1",
            "                        and line[col + 1] != ch_quote",
            "                        or col == len_line - 1",
            "                    ):",
            "                        # exits quote",
            "                        in_quote = False",
            "                        statement.append((line[col0 : col + 1], True))",
            "                        col += 1",
            "                        col0 = col",
            "                    else:",
            "                        # escaped quote and still in quote",
            "                        col += 2",
            "                else:",
            "                    col += 1",
            "            else:",
            "                if line[col] in (\"'\", '\"'):",
            "                    in_quote = True",
            "                    ch_quote = line[col]",
            "                    col += 1",
            "                elif line[col] in (\" \", \"\\t\"):",
            "                    statement.append((line[col0 : col + 1], True))",
            "                    col += 1",
            "                    col0 = col",
            "                elif line[col:].startswith(\"--\"):",
            "                    statement.append((line[col0:col], True))",
            "                    if not remove_comments:",
            "                        # keep the comment",
            "                        statement.append((line[col:], False))",
            "                    else:",
            "                        statement.append((\"\\n\", True))",
            "                    col = len_line + 1",
            "                    col0 = col",
            "                elif line[col:].startswith(\"/*\") and not line[col0:].startswith(",
            "                    \"file://\"",
            "                ):",
            "                    if not remove_comments:",
            "                        statement.append((line[col0 : col + 2], False))",
            "                    else:",
            "                        statement.append((line[col0:col], False))",
            "                    col += 2",
            "                    col0 = col",
            "                    in_comment = True",
            "                elif line[col:].startswith(\"$$\"):",
            "                    statement.append((line[col0 : col + 2], True))",
            "                    col += 2",
            "                    col0 = col",
            "                    in_double_dollars = True",
            "                elif (",
            "                    RE_START.match(line[col - 1 : col + len(sql_delimiter)])",
            "                    if col > 0",
            "                    else (RE_START.match(line[col : col + len(sql_delimiter)]))",
            "                ) and (RE_END.match(line[col : col + len(sql_delimiter) + 1])):",
            "                    statement.append((line[col0:col] + \";\", True))",
            "                    col += len(sql_delimiter)",
            "                    try:",
            "                        if line[col] == \">\":",
            "                            col += 1",
            "                            statement[-1] = (statement[-1][0] + \">\", statement[-1][1])",
            "                    except IndexError:",
            "                        pass",
            "                    if COMMENT_PATTERN_RE.match(line[col:]) or EMPTY_LINE_RE.match(",
            "                        line[col:]",
            "                    ):",
            "                        if not remove_comments:",
            "                            # keep the comment",
            "                            statement.append((line[col:], False))",
            "                        col = len_line",
            "                    while col < len_line and line[col] in (\" \", \"\\t\"):",
            "                        col += 1",
            "                    yield _concatenate_statements(statement)",
            "                    col0 = col",
            "                    statement = []",
            "                elif col == 0 and line[col] == \"!\":  # command",
            "                    if len(statement) > 0:",
            "                        yield _concatenate_statements(statement)",
            "                        statement = []",
            "                    yield (",
            "                        line.strip()[: -len(sql_delimiter)]",
            "                        if line.strip().endswith(sql_delimiter)",
            "                        else line.strip()",
            "                    ).strip(), False",
            "                    break",
            "                else:",
            "                    col += 1",
            "        line = buf.readline()",
            "",
            "    if len(statement) > 0:",
            "        yield _concatenate_statements(statement)",
            "",
            "",
            "def _concatenate_statements(",
            "    statement_list: list[tuple[str, bool]]",
            ") -> tuple[str, bool | None]:",
            "    \"\"\"Concatenate statements.",
            "",
            "    Each statement should be a tuple of statement and is_put_or_get.",
            "",
            "    The is_put_or_get is set to True if the statement is PUT or GET otherwise False for valid statement.",
            "    None is set if the statement is empty or comment only.",
            "",
            "    Args:",
            "        statement_list: List of statement parts.",
            "",
            "    Returns:",
            "        Tuple of statements and whether they are PUT or GET.",
            "    \"\"\"",
            "    valid_statement_list = []",
            "    is_put_or_get = None",
            "    for text, is_statement in statement_list:",
            "        valid_statement_list.append(text)",
            "        if is_put_or_get is None and is_statement and len(text.strip()) >= 3:",
            "            is_put_or_get = text[:3].upper() in (\"PUT\", \"GET\")",
            "    return \"\".join(valid_statement_list).strip(), is_put_or_get",
            "",
            "",
            "def construct_hostname(region: str | None, account: str) -> str:",
            "    \"\"\"Constructs hostname from region and account.\"\"\"",
            "",
            "    def _is_china_region(r: str) -> bool:",
            "        # This is consistent with the Go driver:",
            "        # https://github.com/snowflakedb/gosnowflake/blob/f20a46475dce322f3f6b97b4a72f2807571e750b/dsn.go#L535",
            "        return r.lower().startswith(\"cn-\")",
            "",
            "    if region == \"us-west-2\":",
            "        region = \"\"",
            "    if region:",
            "        if account.find(\".\") > 0:",
            "            account = account[0 : account.find(\".\")]",
            "        top_level_domain = \"cn\" if _is_china_region(region) else \"com\"",
            "        host = f\"{account}.{region}.snowflakecomputing.{top_level_domain}\"",
            "    else:",
            "        top_level_domain = \"com\"",
            "        if account.find(\".\") > 0 and _is_china_region(account.split(\".\")[1]):",
            "            top_level_domain = \"cn\"",
            "        host = f\"{account}.snowflakecomputing.{top_level_domain}\"",
            "    return host",
            "",
            "",
            "def parse_account(account):",
            "    url_parts = account.split(\".\")",
            "    # if this condition is true, then we have some extra",
            "    # stuff in the account field.",
            "    if len(url_parts) > 1:",
            "        if url_parts[1] == \"global\":",
            "            # remove external ID from account",
            "            parsed_account = url_parts[0][0 : url_parts[0].rfind(\"-\")]",
            "        else:",
            "            # remove region subdomain",
            "            parsed_account = url_parts[0]",
            "    else:",
            "        parsed_account = account",
            "",
            "    return parsed_account",
            "",
            "",
            "def random_string(",
            "    length: int = 10,",
            "    prefix: str = \"\",",
            "    suffix: str = \"\",",
            "    choices: Sequence[str] = string.ascii_lowercase,",
            ") -> str:",
            "    \"\"\"Our convenience function to generate random string for object names.",
            "",
            "    Args:",
            "        length: How many random characters to choose from choices.",
            "        prefix: Prefix to add to random string generated.",
            "        suffix: Suffix to add to random string generated.",
            "        choices: A generator of things to choose from.",
            "    \"\"\"",
            "    random_part = \"\".join([random.Random().choice(choices) for _ in range(length)])",
            "    return \"\".join([prefix, random_part, suffix])",
            "",
            "",
            "def _base64_bytes_to_str(x) -> str | None:",
            "    return base64.b64encode(x).decode(\"utf-8\") if x else None",
            "",
            "",
            "def get_md5(text: str | bytes) -> bytes:",
            "    if isinstance(text, str):",
            "        text = text.encode(\"utf-8\")",
            "    md5 = hashlib.md5()",
            "    md5.update(text)",
            "    return md5.digest()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "jupyter_server.base.handlers.APIHandler.content_security_policy"
        ]
    }
}