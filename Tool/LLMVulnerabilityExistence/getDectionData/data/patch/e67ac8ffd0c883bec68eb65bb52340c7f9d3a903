{
    "keras/src/models/functional.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " from keras.src.ops.function import make_node_key"
            },
            "1": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " from keras.src.ops.node import KerasHistory"
            },
            "2": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from keras.src.ops.node import Node"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 22,
                "PatchRowcode": "+from keras.src.ops.operation import Operation"
            },
            "4": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " from keras.src.saving import serialization_lib"
            },
            "5": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " from keras.src.utils import tracking"
            },
            "6": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 523,
                "afterPatchRowNumber": 524,
                "PatchRowcode": "             layer = serialization_lib.deserialize_keras_object("
            },
            "8": {
                "beforePatchRowNumber": 524,
                "afterPatchRowNumber": 525,
                "PatchRowcode": "                 layer_data, custom_objects=custom_objects"
            },
            "9": {
                "beforePatchRowNumber": 525,
                "afterPatchRowNumber": 526,
                "PatchRowcode": "             )"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 527,
                "PatchRowcode": "+        if not isinstance(layer, Operation):"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 528,
                "PatchRowcode": "+            raise ValueError("
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 529,
                "PatchRowcode": "+                \"Unexpected object from deserialization, expected a layer or \""
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 530,
                "PatchRowcode": "+                f\"operation, got a {type(layer)}\""
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 531,
                "PatchRowcode": "+            )"
            },
            "15": {
                "beforePatchRowNumber": 526,
                "afterPatchRowNumber": 532,
                "PatchRowcode": "         created_layers[layer_name] = layer"
            },
            "16": {
                "beforePatchRowNumber": 527,
                "afterPatchRowNumber": 533,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 528,
                "afterPatchRowNumber": 534,
                "PatchRowcode": "         # Gather layer inputs."
            }
        },
        "frontPatchFile": [
            "import copy",
            "import inspect",
            "import typing",
            "import warnings",
            "",
            "from keras.src import backend",
            "from keras.src import ops",
            "from keras.src import tree",
            "from keras.src.backend.common import global_state",
            "from keras.src.layers.core.input_layer import Input",
            "from keras.src.layers.core.input_layer import InputLayer",
            "from keras.src.layers.input_spec import InputSpec",
            "from keras.src.layers.layer import Layer",
            "from keras.src.legacy.saving import saving_utils",
            "from keras.src.legacy.saving import serialization as legacy_serialization",
            "from keras.src.models.model import Model",
            "from keras.src.ops.function import Function",
            "from keras.src.ops.function import _build_map",
            "from keras.src.ops.function import make_node_key",
            "from keras.src.ops.node import KerasHistory",
            "from keras.src.ops.node import Node",
            "from keras.src.saving import serialization_lib",
            "from keras.src.utils import tracking",
            "",
            "",
            "class Functional(Function, Model):",
            "    \"\"\"A `Functional` model is a `Model` defined as a directed graph of layers.",
            "",
            "    Three types of `Model` exist: subclassed `Model`, `Functional` model,",
            "    and `Sequential` (a special case of `Functional`).",
            "",
            "    A `Functional` model can be instantiated by passing two arguments to",
            "    `__init__()`. The first argument is the `keras.Input` objects",
            "    that represent the inputs to the model.",
            "    The second argument specifies the output tensors that represent",
            "    the outputs of this model. Both arguments can be a nested structure",
            "    of tensors.",
            "",
            "    Example:",
            "",
            "    ```",
            "    inputs = {'x1': keras.Input(shape=(10,), name='x1'),",
            "              'x2': keras.Input(shape=(1,), name='x2')}",
            "    t = keras.layers.Dense(1, activation='relu')(inputs['x1'])",
            "    outputs = keras.layers.Add()([t, inputs['x2']])",
            "    model = keras.Model(inputs, outputs)",
            "    ```",
            "",
            "    A `Functional` model constructed using the Functional API can also",
            "    include raw Keras 3 ops.",
            "",
            "    Example:",
            "",
            "    ```python",
            "    inputs = keras.Input(shape=(10,))",
            "    x = keras.layers.Dense(1)(inputs)",
            "    outputs = ops.nn.relu(x)",
            "    model = keras.Model(inputs, outputs)",
            "    ```",
            "",
            "    A new `Functional` model can also be created by using the",
            "    intermediate tensors. This enables you to quickly extract sub-components",
            "    of the model.",
            "",
            "    Example:",
            "",
            "    ```python",
            "    inputs = keras.Input(shape=(None, None, 3))",
            "    processed = keras.layers.RandomCrop(width=32, height=32)(inputs)",
            "    conv = keras.layers.Conv2D(filters=2, kernel_size=3)(processed)",
            "    pooling = keras.layers.GlobalAveragePooling2D()(conv)",
            "    feature = keras.layers.Dense(10)(pooling)",
            "",
            "    full_model = keras.Model(inputs, feature)",
            "    backbone = keras.Model(processed, conv)",
            "    activations = keras.Model(conv, feature)",
            "    ```",
            "",
            "    Note that the `backbone` and `activations` models are not",
            "    created with `keras.Input` objects, but with the tensors",
            "    that are originated from `keras.Input` objects.",
            "    Under the hood, the layers and weights will",
            "    be shared across these models, so that user can train the `full_model`, and",
            "    use `backbone` or `activations` to do feature extraction.",
            "    The inputs and outputs of the model can be nested structures of tensors as",
            "    well, and the created models are standard `Functional` model that support",
            "    all the existing API.",
            "",
            "    Args:",
            "        inputs: List of input tensors (must be created via `keras.Input()`",
            "            or originated from `keras.Input()`).",
            "        outputs: List of output tensors.",
            "        name: String, optional. Name of the model.",
            "        trainable: Boolean, optional. If the model's variables should be",
            "            trainable.",
            "    \"\"\"",
            "",
            "    def __new__(cls, *args, **kwargs):",
            "        return typing.cast(cls, super().__new__(cls))",
            "",
            "    @tracking.no_automatic_dependency_tracking",
            "    def __init__(self, inputs, outputs, name=None, **kwargs):",
            "        if isinstance(inputs, dict):",
            "            for k, v in inputs.items():",
            "                if isinstance(v, backend.KerasTensor) and k != v.name:",
            "                    warnings.warn(",
            "                        \"When providing `inputs` as a dict, all keys in the \"",
            "                        \"dict must match the names of the corresponding \"",
            "                        f\"tensors. Received key '{k}' mapping to value {v} \"",
            "                        f\"which has name '{v.name}'. Change the tensor name to \"",
            "                        f\"'{k}' (via `Input(..., name='{k}')`)\"",
            "                    )",
            "",
            "        trainable = kwargs.pop(\"trainable\", None)",
            "        flat_inputs = tree.flatten(inputs)",
            "        flat_outputs = tree.flatten(outputs)",
            "        for x in flat_inputs:",
            "            if not isinstance(x, backend.KerasTensor):",
            "                raise ValueError(",
            "                    \"All `inputs` values must be KerasTensors. Received: \"",
            "                    f\"inputs={inputs} including invalid value {x} of \"",
            "                    f\"type {type(x)}\"",
            "                )",
            "        for x in flat_outputs:",
            "            if not isinstance(x, backend.KerasTensor):",
            "                raise ValueError(",
            "                    \"All `outputs` values must be KerasTensors. Received: \"",
            "                    f\"outputs={outputs} including invalid value {x} of \"",
            "                    f\"type {type(x)}\"",
            "                )",
            "",
            "        if not all(is_input_keras_tensor(t) for t in flat_inputs):",
            "            inputs, outputs = clone_graph_nodes(inputs, outputs)",
            "",
            "        Function.__init__(self, inputs, outputs, name=name)",
            "",
            "        if trainable is not None:",
            "            self.trainable = trainable",
            "",
            "        self._layers = self.layers",
            "        self.build(None)",
            "        # We will convert directly (to the correct dtype per input).",
            "        self._convert_input_args = False",
            "        self._allow_non_tensor_positional_args = True",
            "        output_layers = [x._keras_history[0] for x in self.outputs]",
            "        self.output_names = [x.name for x in output_layers]",
            "",
            "    def _lock_state(self):",
            "        # Unlike other layers, we allow Functional state to be mutable after",
            "        # build. E.g. to attach a layer to a model that is not part of the",
            "        # functional DAG.",
            "        pass",
            "",
            "    def _obj_type(self):",
            "        return \"Functional\"",
            "",
            "    @property",
            "    def layers(self):",
            "        layers = []",
            "        for operation in self._operations:",
            "            if isinstance(operation, Layer):",
            "                layers.append(operation)",
            "        return layers",
            "",
            "    @layers.setter",
            "    def layers(self, _):",
            "        raise AttributeError(",
            "            \"`Model.layers` attribute is reserved and should not be used. \"",
            "            \"Please use another name.\"",
            "        )",
            "",
            "    def call(self, inputs, training=None, mask=None):",
            "        # Add support for training, masking",
            "        inputs = self._standardize_inputs(inputs)",
            "        if mask is None:",
            "            masks = [None] * len(inputs)",
            "        else:",
            "            masks = tree.flatten(mask)",
            "            for x, mask in zip(inputs, masks):",
            "                if mask is not None:",
            "                    backend.set_keras_mask(x, mask)",
            "        outputs = self._run_through_graph(",
            "            inputs, operation_fn=lambda op: operation_fn(op, training=training)",
            "        )",
            "        return unpack_singleton(outputs)",
            "",
            "    def compute_output_spec(self, inputs, training=None, mask=None):",
            "        # From Function",
            "        return super().compute_output_spec(inputs)",
            "",
            "    def compute_output_shape(self, input_shape):",
            "        # From Function",
            "        return super().compute_output_shape(input_shape)",
            "",
            "    def build(self, input_shape):",
            "        self.built = True",
            "",
            "    @property",
            "    def input_shape(self):",
            "        input_shapes = tree.map_structure(lambda x: x.shape, self.inputs)",
            "        if isinstance(input_shapes, list) and len(input_shapes) == 1:",
            "            return input_shapes[0]",
            "        return input_shapes",
            "",
            "    @property",
            "    def output_shape(self):",
            "        output_shapes = tree.map_structure(lambda x: x.shape, self.outputs)",
            "        if isinstance(output_shapes, list) and len(output_shapes) == 1:",
            "            return output_shapes[0]",
            "        return output_shapes",
            "",
            "    def _assert_input_compatibility(self, *args):",
            "        return super(Model, self)._assert_input_compatibility(*args)",
            "",
            "    def _maybe_warn_inputs_struct_mismatch(self, inputs, raise_exception=False):",
            "        try:",
            "            # We first normalize to tuples before performing the check to",
            "            # suppress warnings when encountering mismatched tuples and lists.",
            "            tree.assert_same_structure(",
            "                tree.lists_to_tuples(inputs),",
            "                tree.lists_to_tuples(self._inputs_struct),",
            "            )",
            "        except:",
            "            model_inputs_struct = tree.map_structure(",
            "                lambda x: x.name, self._inputs_struct",
            "            )",
            "            inputs_struct = tree.map_structure(",
            "                lambda x: f\"Tensor(shape={x.shape})\", inputs",
            "            )",
            "            msg = (",
            "                \"The structure of `inputs` doesn't match the expected \"",
            "                f\"structure.\\nExpected: {model_inputs_struct}\\n\"",
            "                f\"Received: inputs={inputs_struct}\"",
            "            )",
            "            if raise_exception:",
            "                raise ValueError(msg)",
            "            warnings.warn(msg)",
            "",
            "    def _convert_inputs_to_tensors(self, flat_inputs):",
            "        converted = []",
            "        for x, input in zip(flat_inputs, self._inputs):",
            "            if x is None:  # TODO: check if optional",
            "                converted.append(x)",
            "            else:",
            "                converted.append(",
            "                    ops.convert_to_tensor(",
            "                        x, dtype=input.dtype, sparse=input.sparse",
            "                    )",
            "                )",
            "        return converted",
            "",
            "    def _adjust_input_rank(self, flat_inputs):",
            "        flat_ref_shapes = [x.shape for x in self._inputs]",
            "        adjusted = []",
            "        for x, ref_shape in zip(flat_inputs, flat_ref_shapes):",
            "            if x is None:",
            "                adjusted.append(x)",
            "                continue",
            "            x_rank = len(x.shape)",
            "            ref_rank = len(ref_shape)",
            "            if x_rank == ref_rank:",
            "                adjusted.append(x)",
            "                continue",
            "            if x_rank == ref_rank + 1:",
            "                if x.shape[-1] == 1:",
            "                    adjusted.append(ops.squeeze(x, axis=-1))",
            "                    continue",
            "            if x_rank == ref_rank - 1:",
            "                if ref_shape[-1] == 1:",
            "                    adjusted.append(ops.expand_dims(x, axis=-1))",
            "                    continue",
            "            raise ValueError(",
            "                f\"Invalid input shape for input {x}. Expected shape \"",
            "                f\"{ref_shape}, but input has incompatible shape {x.shape}\"",
            "            )",
            "        # Add back metadata.",
            "        for i in range(len(flat_inputs)):",
            "            if hasattr(flat_inputs[i], \"_keras_history\"):",
            "                adjusted[i]._keras_history = flat_inputs[i]._keras_history",
            "            mask = backend.get_keras_mask(flat_inputs[i])",
            "            if mask is not None:",
            "                backend.set_keras_mask(adjusted[i], mask)",
            "        return adjusted",
            "",
            "    def _standardize_inputs(self, inputs):",
            "        raise_exception = False",
            "        if isinstance(inputs, dict) and not isinstance(",
            "            self._inputs_struct, dict",
            "        ):",
            "            # This is to avoid warning",
            "            # when we have reconciable dict/list structs",
            "            if hasattr(self._inputs_struct, \"__len__\") and all(",
            "                isinstance(i, backend.KerasTensor) for i in self._inputs_struct",
            "            ):",
            "                expected_keys = set(i.name for i in self._inputs_struct)",
            "                keys = set(inputs.keys())",
            "                if expected_keys.issubset(keys):",
            "                    inputs = [inputs[i.name] for i in self._inputs_struct]",
            "                else:",
            "                    raise_exception = True",
            "            elif isinstance(self._inputs_struct, backend.KerasTensor):",
            "                if self._inputs_struct.name in inputs:",
            "                    inputs = [inputs[self._inputs_struct.name]]",
            "                else:",
            "                    raise_exception = True",
            "            else:",
            "                raise_exception = True",
            "        if (",
            "            isinstance(self._inputs_struct, dict)",
            "            and not isinstance(inputs, dict)",
            "            and list(self._inputs_struct.keys())",
            "            != sorted(self._inputs_struct.keys())",
            "        ):",
            "            raise_exception = True",
            "        self._maybe_warn_inputs_struct_mismatch(",
            "            inputs, raise_exception=raise_exception",
            "        )",
            "",
            "        flat_inputs = tree.flatten(inputs)",
            "        flat_inputs = self._convert_inputs_to_tensors(flat_inputs)",
            "        return self._adjust_input_rank(flat_inputs)",
            "",
            "    @property",
            "    def input(self):",
            "        # For backwards compatibility,",
            "        # override `input` to retrieve the used-provided",
            "        # constructor inputs",
            "        return self._inputs_struct",
            "",
            "    @property",
            "    def output(self):",
            "        return self._outputs_struct",
            "",
            "    def add_loss(self, loss):",
            "        # Symbolic only. TODO",
            "        raise NotImplementedError",
            "",
            "    @property",
            "    def input_spec(self):",
            "        if hasattr(self, \"_manual_input_spec\"):",
            "            return self._manual_input_spec",
            "",
            "        def shape_with_no_batch_size(x):",
            "            x = list(x)",
            "            if x:",
            "                x[0] = None",
            "            return tuple(x)",
            "",
            "        def make_spec_for_tensor(x):",
            "            optional = False",
            "            if isinstance(x._keras_history[0], InputLayer):",
            "                if x._keras_history[0].optional:",
            "                    optional = True",
            "            return InputSpec(",
            "                shape=shape_with_no_batch_size(x.shape),",
            "                allow_last_axis_squeeze=True,",
            "                name=x._keras_history[0].name,",
            "                optional=optional,",
            "            )",
            "",
            "        if isinstance(self._inputs_struct, dict):",
            "            if all(",
            "                isinstance(x, backend.KerasTensor)",
            "                for x in self._inputs_struct.values()",
            "            ):",
            "                # Case where `_nested_inputs` is a plain dict of Inputs.",
            "                names = sorted(self._inputs_struct.keys())",
            "                return [",
            "                    InputSpec(",
            "                        shape=shape_with_no_batch_size(",
            "                            self._inputs_struct[name].shape",
            "                        ),",
            "                        allow_last_axis_squeeze=True,",
            "                        name=name,",
            "                    )",
            "                    for name in names",
            "                ]",
            "            return None  # Deeply nested dict: skip checks.",
            "        return [make_spec_for_tensor(x) for x in self.inputs]",
            "",
            "    @input_spec.setter",
            "    def input_spec(self, value):",
            "        self._manual_input_spec = value",
            "",
            "    def get_config(self):",
            "        if not functional_like_constructor(self.__class__):",
            "            # Subclassed networks are not serializable",
            "            # (unless serialization is implemented by",
            "            # the author of the subclassed network).",
            "            return Model.get_config(self)",
            "",
            "        config = {",
            "            \"name\": self.name,",
            "            \"trainable\": self.trainable,",
            "        }",
            "        # Build a map from a layer unique name (make_node_key)",
            "        # to the index of the nodes that are saved in the config.",
            "        # Only nodes in network_nodes are saved.",
            "        node_reindexing_map = {}",
            "        for operation in self.operations:",
            "            if issubclass(operation.__class__, Functional):",
            "                # Functional models start with a pre-existing node",
            "                # linking their input to output.",
            "                kept_nodes = 1",
            "            else:",
            "                kept_nodes = 0",
            "            for original_node_index, node in enumerate(",
            "                operation._inbound_nodes",
            "            ):",
            "                node_key = make_node_key(operation, original_node_index)",
            "                if node_key in self._nodes:",
            "                    # i.e. we mark it to be saved",
            "                    node_reindexing_map[node_key] = kept_nodes",
            "                    kept_nodes += 1",
            "",
            "        # serialize and save the layers in layer_configs",
            "        layer_configs = []",
            "        for operation in self.operations:  # From the earliest layers on.",
            "            filtered_inbound_nodes = []",
            "            for original_node_index, node in enumerate(",
            "                operation._inbound_nodes",
            "            ):",
            "                node_key = make_node_key(operation, original_node_index)",
            "                if node_key in self._nodes:",
            "                    # The node is relevant to the model:",
            "                    # add to filtered_inbound_nodes.",
            "                    node_data = serialize_node(node, own_nodes=self._nodes)",
            "                    if node_data is not None:",
            "                        filtered_inbound_nodes.append(node_data)",
            "",
            "            serialize_obj_fn = serialization_lib.serialize_keras_object",
            "            if global_state.get_global_attribute(\"use_legacy_config\", False):",
            "                # Legacy format serialization used for H5 and SavedModel",
            "                serialize_obj_fn = legacy_serialization.serialize_keras_object",
            "            layer_config = serialize_obj_fn(operation)",
            "            layer_config[\"name\"] = operation.name",
            "            layer_config[\"inbound_nodes\"] = filtered_inbound_nodes",
            "            layer_configs.append(layer_config)",
            "        config[\"layers\"] = layer_configs",
            "",
            "        # Gather info about inputs and outputs.",
            "        def get_tensor_config(tensor):",
            "            operation = tensor._keras_history[0]",
            "            node_index = tensor._keras_history[1]",
            "            tensor_index = tensor._keras_history[2]",
            "            node_key = make_node_key(operation, node_index)",
            "            assert node_key in self._nodes",
            "            new_node_index = node_reindexing_map[node_key]",
            "            return [operation.name, new_node_index, tensor_index]",
            "",
            "        def map_tensors(tensors):",
            "            if isinstance(tensors, backend.KerasTensor):",
            "                return [get_tensor_config(tensors)]",
            "            return tree.map_structure(get_tensor_config, tensors)",
            "",
            "        config[\"input_layers\"] = map_tensors(self._inputs_struct)",
            "        config[\"output_layers\"] = map_tensors(self._outputs_struct)",
            "        return copy.deepcopy(config)",
            "",
            "",
            "def functional_from_config(cls, config, custom_objects=None):",
            "    \"\"\"Instantiates a Functional model from its config (from `get_config()`).",
            "",
            "    Args:",
            "        cls: Class of the model, e.g. a custom subclass of `Model`.",
            "        config: Output of `get_config()` for the original model instance.",
            "        custom_objects: Optional dict of custom objects.",
            "",
            "    Returns:",
            "        An instance of `cls`.",
            "    \"\"\"",
            "    # Layer instances created during",
            "    # the graph reconstruction process",
            "    created_layers = {}",
            "",
            "    # Dictionary mapping layer instances to",
            "    # node data that specifies a layer call.",
            "    # It acts as a queue that maintains any unprocessed",
            "    # layer call until it becomes possible to process it",
            "    # (i.e. until the input tensors to the call all exist).",
            "    unprocessed_nodes = {}",
            "",
            "    def add_unprocessed_node(layer, node_data):",
            "        \"\"\"Add node to layer list",
            "",
            "        Arg:",
            "            layer: layer object",
            "            node_data: Node data specifying layer call",
            "        \"\"\"",
            "        if layer not in unprocessed_nodes:",
            "            unprocessed_nodes[layer] = [node_data]",
            "        else:",
            "            unprocessed_nodes[layer].append(node_data)",
            "",
            "    def process_node(layer, node_data):",
            "        \"\"\"Reconstruct node by linking to inbound layers",
            "",
            "        Args:",
            "            layer: Layer to process",
            "            node_data: List of layer configs",
            "        \"\"\"",
            "        args, kwargs = deserialize_node(node_data, created_layers)",
            "        # Call layer on its inputs, thus creating the node",
            "        # and building the layer if needed.",
            "        layer(*args, **kwargs)",
            "",
            "    def process_layer(layer_data):",
            "        \"\"\"Deserializes a layer and index its inbound nodes.",
            "",
            "        Args:",
            "            layer_data: layer config dict.",
            "        \"\"\"",
            "        layer_name = layer_data[\"name\"]",
            "",
            "        # Instantiate layer.",
            "        if \"module\" not in layer_data:",
            "            # Legacy format deserialization (no \"module\" key)",
            "            # used for H5 and SavedModel formats",
            "            layer = saving_utils.model_from_config(",
            "                layer_data, custom_objects=custom_objects",
            "            )",
            "        else:",
            "            layer = serialization_lib.deserialize_keras_object(",
            "                layer_data, custom_objects=custom_objects",
            "            )",
            "        created_layers[layer_name] = layer",
            "",
            "        # Gather layer inputs.",
            "        inbound_nodes_data = layer_data[\"inbound_nodes\"]",
            "        for node_data in inbound_nodes_data:",
            "            # We don't process nodes (i.e. make layer calls)",
            "            # on the fly because the inbound node may not yet exist,",
            "            # in case of layer shared at different topological depths",
            "            # (e.g. a model such as A(B(A(B(x)))))",
            "            add_unprocessed_node(layer, node_data)",
            "",
            "    # Extract config used to instantiate Functional model from the config. The",
            "    # remaining config will be passed as keyword arguments to the Model",
            "    # constructor.",
            "    functional_config = {}",
            "    for key in [\"layers\", \"input_layers\", \"output_layers\"]:",
            "        functional_config[key] = config.pop(key)",
            "    for key in [\"name\", \"trainable\"]:",
            "        if key in config:",
            "            functional_config[key] = config.pop(key)",
            "        else:",
            "            functional_config[key] = None",
            "",
            "    # First, we create all layers and enqueue nodes to be processed",
            "    for layer_data in functional_config[\"layers\"]:",
            "        process_layer(layer_data)",
            "",
            "    # Then we process nodes in order of layer depth.",
            "    # Nodes that cannot yet be processed (if the inbound node",
            "    # does not yet exist) are re-enqueued, and the process",
            "    # is repeated until all nodes are processed.",
            "    while unprocessed_nodes:",
            "        for layer_data in functional_config[\"layers\"]:",
            "            layer = created_layers[layer_data[\"name\"]]",
            "",
            "            # Process all nodes in layer, if not yet processed",
            "            if layer in unprocessed_nodes:",
            "                node_data_list = unprocessed_nodes[layer]",
            "",
            "                # Process nodes in order",
            "                node_index = 0",
            "                while node_index < len(node_data_list):",
            "                    node_data = node_data_list[node_index]",
            "                    try:",
            "                        process_node(layer, node_data)",
            "",
            "                    # If the node does not have all inbound layers",
            "                    # available, stop processing and continue later",
            "                    except IndexError:",
            "                        break",
            "",
            "                    node_index += 1",
            "",
            "                # If not all nodes processed then store unprocessed nodes",
            "                if node_index < len(node_data_list):",
            "                    unprocessed_nodes[layer] = node_data_list[node_index:]",
            "                # If all nodes processed remove the layer",
            "                else:",
            "                    del unprocessed_nodes[layer]",
            "",
            "    # Create list of input and output tensors and return new class",
            "    name = functional_config[\"name\"]",
            "    trainable = functional_config[\"trainable\"]",
            "",
            "    def get_tensor(layer_name, node_index, tensor_index):",
            "        assert layer_name in created_layers",
            "        layer = created_layers[layer_name]",
            "        if isinstance(layer, Functional):",
            "            # Functional models start out with a built-in node.",
            "            node_index -= 1",
            "        layer_output_tensors = layer._inbound_nodes[node_index].output_tensors",
            "        return layer_output_tensors[tensor_index]",
            "",
            "    def map_tensors(tensors):",
            "        if (",
            "            isinstance(tensors, list)",
            "            and len(tensors) == 3",
            "            and isinstance(tensors[0], str)",
            "        ):",
            "            # Leaf",
            "            return get_tensor(*tensors)",
            "        if isinstance(tensors, dict):",
            "            return {k: map_tensors(v) for k, v in tensors.items()}",
            "        if isinstance(tensors, tuple):",
            "            return tuple([map_tensors(v) for v in tensors])",
            "        return [map_tensors(v) for v in tensors]",
            "",
            "    input_tensors = map_tensors(functional_config[\"input_layers\"])",
            "    output_tensors = map_tensors(functional_config[\"output_layers\"])",
            "    if isinstance(input_tensors, list) and len(input_tensors) == 1:",
            "        input_tensors = input_tensors[0]",
            "    if isinstance(output_tensors, list) and len(output_tensors) == 1:",
            "        output_tensors = output_tensors[0]",
            "",
            "    return cls(",
            "        inputs=input_tensors,",
            "        outputs=output_tensors,",
            "        name=name,",
            "        trainable=trainable,",
            "        **config,",
            "    )",
            "",
            "",
            "def operation_fn(operation, training):",
            "    def call(*args, **kwargs):",
            "        if (",
            "            hasattr(operation, \"_call_has_training_arg\")",
            "            and operation._call_has_training_arg",
            "            and training is not None",
            "        ):",
            "            kwargs[\"training\"] = training",
            "        return operation(*args, **kwargs)",
            "",
            "    return call",
            "",
            "",
            "def functional_like_constructor(cls):",
            "    init_args = inspect.getfullargspec(cls.__init__).args[1:]",
            "    functional_init_args = inspect.getfullargspec(Functional.__init__).args[1:]",
            "    if init_args == functional_init_args:",
            "        return True",
            "    return False",
            "",
            "",
            "def unpack_singleton(x):",
            "    if isinstance(x, (list, tuple)) and len(x) == 1:",
            "        return x[0]",
            "    return x",
            "",
            "",
            "def serialize_node(node, own_nodes=()):",
            "    if not node.input_tensors:",
            "        # Does not need to be serialized.",
            "        return",
            "",
            "    def serialize_keras_tensor(x):",
            "        # Serialize KerasTensor while converting",
            "        # node indices to only include nodes relevant to `own_nodes`.",
            "        if isinstance(x, backend.KerasTensor):",
            "            operation, node_index, tensor_index = x._keras_history",
            "            irrelevant_node_count = 0",
            "            for i, node in enumerate(operation._inbound_nodes[:node_index]):",
            "                node_key = make_node_key(operation, i)",
            "                if node_key not in own_nodes:",
            "                    irrelevant_node_count += 1",
            "            x._keras_history = KerasHistory(",
            "                operation, node_index - irrelevant_node_count, tensor_index",
            "            )",
            "            serialized = serialization_lib.serialize_keras_object(x)",
            "            x._keras_history = KerasHistory(operation, node_index, tensor_index)",
            "            return serialized",
            "        return x",
            "",
            "    args = node.arguments.args",
            "    kwargs = node.arguments.kwargs",
            "",
            "    args = tree.map_structure(serialize_keras_tensor, args)",
            "    kwargs = tree.map_structure(serialize_keras_tensor, kwargs)",
            "    return {",
            "        \"args\": serialization_lib.serialize_keras_object(args),",
            "        \"kwargs\": serialization_lib.serialize_keras_object(kwargs),",
            "    }",
            "",
            "",
            "def deserialize_node(node_data, created_layers):",
            "    \"\"\"Return (args, kwargs) for calling the node layer.\"\"\"",
            "    if not node_data:",
            "        return [], {}",
            "",
            "    if isinstance(node_data, list):",
            "        # Legacy case.",
            "        input_tensors = []",
            "        for input_data in node_data:",
            "            inbound_layer_name = input_data[0]",
            "            inbound_node_index = input_data[1]",
            "            inbound_tensor_index = input_data[2]",
            "            if len(input_data) == 3:",
            "                kwargs = {}",
            "            elif len(input_data) == 4:",
            "                kwargs = input_data[3]",
            "            else:",
            "                raise ValueError(",
            "                    \"Cannot deserialize the model (invalid config data?)\"",
            "                )",
            "            inbound_layer = created_layers[inbound_layer_name]",
            "",
            "            # Raise an error if the corresponding layer node",
            "            # has not yet been created",
            "            if len(inbound_layer._inbound_nodes) <= inbound_node_index:",
            "                raise IndexError(",
            "                    \"Layer node index out of bounds.\\n\"",
            "                    f\"inbound_layer = {inbound_layer}\\n\"",
            "                    \"inbound_layer._inbound_nodes = \"",
            "                    f\"{inbound_layer._inbound_nodes}\\n\"",
            "                    f\"inbound_node_index = {inbound_node_index}\"",
            "                )",
            "            inbound_node = inbound_layer._inbound_nodes[inbound_node_index]",
            "            input_tensors.append(",
            "                inbound_node.output_tensors[inbound_tensor_index]",
            "            )",
            "        return [unpack_singleton(input_tensors)], kwargs",
            "",
            "    args = serialization_lib.deserialize_keras_object(node_data[\"args\"])",
            "    kwargs = serialization_lib.deserialize_keras_object(node_data[\"kwargs\"])",
            "",
            "    def convert_revived_tensor(x):",
            "        if isinstance(x, backend.KerasTensor):",
            "            history = x._pre_serialization_keras_history",
            "            if history is None:",
            "                return x",
            "            layer = created_layers.get(history[0], None)",
            "            if layer is None:",
            "                raise ValueError(f\"Unknown layer: {history[0]}\")",
            "            inbound_node_index = history[1]",
            "            inbound_tensor_index = history[2]",
            "            if len(layer._inbound_nodes) <= inbound_node_index:",
            "                raise IndexError(",
            "                    \"Layer node index out of bounds.\\n\"",
            "                    f\"inbound_layer = {layer}\\n\"",
            "                    f\"inbound_layer._inbound_nodes = {layer._inbound_nodes}\\n\"",
            "                    f\"inbound_node_index = {inbound_node_index}\"",
            "                )",
            "            inbound_node = layer._inbound_nodes[inbound_node_index]",
            "            return inbound_node.output_tensors[inbound_tensor_index]",
            "        return x",
            "",
            "    args = tree.map_structure(convert_revived_tensor, args)",
            "    kwargs = tree.map_structure(convert_revived_tensor, kwargs)",
            "    return args, kwargs",
            "",
            "",
            "def is_input_keras_tensor(x):",
            "    (",
            "        operation,",
            "        node_index,",
            "        _,",
            "    ) = x._keras_history",
            "    node = operation._inbound_nodes[node_index]",
            "    return node.is_input",
            "",
            "",
            "def clone_single_keras_tensor(x):",
            "    return backend.KerasTensor(",
            "        shape=x.shape, dtype=x.dtype, sparse=x.sparse, name=x.name + \"_clone\"",
            "    )",
            "",
            "",
            "def clone_keras_tensors(tensors, kt_id_mapping):",
            "    def swap(x):",
            "        if not isinstance(x, backend.KerasTensor):",
            "            return x",
            "        if id(x) in kt_id_mapping:",
            "            return kt_id_mapping[id(x)]",
            "        new_x = clone_single_keras_tensor(x)",
            "        kt_id_mapping[id(x)] = new_x",
            "        return new_x",
            "",
            "    return tree.map_structure(swap, tensors)",
            "",
            "",
            "def find_nodes_by_inputs_and_outputs(inputs, outputs):",
            "    nodes, _ = _build_map(inputs, outputs)",
            "    return nodes",
            "",
            "",
            "def clone_graph_nodes(inputs, outputs):",
            "    \"\"\"Clone the `Node` between the inputs and output tensors.",
            "",
            "    This function is used to create a new functional model from any intermediate",
            "    Keras tensors. The clone of the nodes mimic the behavior of reconstructing",
            "    the functional graph network by re-executing all the `__call__()` methods.",
            "    The cloned nodes will be appended to the layers.",
            "",
            "    Note that a new `keras.Input` will be created for any items in the",
            "    `inputs`",
            "",
            "    Args:",
            "    inputs: A nested structure of `KerasTensor` instances.",
            "    outputs: A nested structure of `KerasTensor` instances.",
            "",
            "    Returns:",
            "        A pair of inputs and outputs, with cloned `KerasTensor` instances.",
            "        They can be used to create a new functional model.",
            "    \"\"\"",
            "    nodes_to_clone = find_nodes_by_inputs_and_outputs(inputs, outputs)",
            "    cloned_inputs = []",
            "    cloned_outputs = []",
            "    # We not only need to create copies of Nodes (mimic the calls), also need to",
            "    # clone Keras tensors to avoid the override of _keras_history attached on",
            "    # the Keras tensor. The following dict is used to track any keras tensor we",
            "    # cloned The key is the string ID of the original keras tensor, and value is",
            "    # the cloned Keras tensor instance.",
            "    kt_id_mapping = {}",
            "    op_id_mapping = {}",
            "",
            "    for kt_input in tree.flatten(inputs):",
            "        if is_input_keras_tensor(kt_input):",
            "            # For any existing Keras tensor from keras.Input, leave them as is.",
            "            cloned_inputs.append(kt_input)",
            "            kt_id_mapping[id(kt_input)] = kt_input",
            "        else:",
            "            # We need to create a new Keras tensor for any intermediate tensor",
            "            cloned_input = Input(",
            "                batch_shape=kt_input.shape,",
            "                dtype=kt_input.dtype,",
            "                sparse=kt_input.sparse,",
            "                name=kt_input.name + \"CLONE\",",
            "            )",
            "            cloned_inputs.append(cloned_input)",
            "            kt_id_mapping[id(kt_input)] = cloned_input",
            "            op_id_mapping[id(kt_input._keras_history[0])] = (",
            "                cloned_input._keras_history[0]",
            "            )",
            "    cloned_inputs = tree.pack_sequence_as(inputs, cloned_inputs)",
            "",
            "    for kt_output in tree.flatten(outputs):",
            "        cpy = clone_single_keras_tensor(kt_output)",
            "        # We reuse the _keras_history here, which contains the old information.",
            "        cpy._keras_history = kt_output._keras_history",
            "        cloned_outputs.append(cpy)",
            "        kt_id_mapping[id(kt_output)] = cpy",
            "    cloned_outputs = tree.pack_sequence_as(outputs, cloned_outputs)",
            "",
            "    for node in nodes_to_clone:",
            "        if id(node.operation) in op_id_mapping:",
            "            operation = op_id_mapping[id(node.operation)]",
            "        else:",
            "            operation = node.operation",
            "        # Clone any Keras tensor to avoid override of _keras_history",
            "        # Or reuse an existing Keras tensor if it has already been cloned.",
            "        output_copy = clone_keras_tensors(node.output_tensors, kt_id_mapping)",
            "        if not isinstance(operation, InputLayer):",
            "            call_args_copy = clone_keras_tensors(",
            "                node.arguments.args, kt_id_mapping",
            "            )",
            "            call_kwargs_copy = clone_keras_tensors(",
            "                node.arguments.kwargs, kt_id_mapping",
            "            )",
            "        else:",
            "            call_args_copy = ()",
            "            call_kwargs_copy = {}",
            "        # Creating new nodes based on the existing node information.  Node wires",
            "        # itself to inbound and outbound layers.  The Node constructor actually",
            "        # updates this layer's self._inbound_nodes, sets _keras_history on the",
            "        # outputs, and adds itself to the `_outbound_nodes` of the layers that",
            "        # produced the inputs to this layer call.",
            "        Node(",
            "            operation,",
            "            call_args=call_args_copy,",
            "            call_kwargs=call_kwargs_copy,",
            "            outputs=output_copy,",
            "        )",
            "    return cloned_inputs, cloned_outputs"
        ],
        "afterPatchFile": [
            "import copy",
            "import inspect",
            "import typing",
            "import warnings",
            "",
            "from keras.src import backend",
            "from keras.src import ops",
            "from keras.src import tree",
            "from keras.src.backend.common import global_state",
            "from keras.src.layers.core.input_layer import Input",
            "from keras.src.layers.core.input_layer import InputLayer",
            "from keras.src.layers.input_spec import InputSpec",
            "from keras.src.layers.layer import Layer",
            "from keras.src.legacy.saving import saving_utils",
            "from keras.src.legacy.saving import serialization as legacy_serialization",
            "from keras.src.models.model import Model",
            "from keras.src.ops.function import Function",
            "from keras.src.ops.function import _build_map",
            "from keras.src.ops.function import make_node_key",
            "from keras.src.ops.node import KerasHistory",
            "from keras.src.ops.node import Node",
            "from keras.src.ops.operation import Operation",
            "from keras.src.saving import serialization_lib",
            "from keras.src.utils import tracking",
            "",
            "",
            "class Functional(Function, Model):",
            "    \"\"\"A `Functional` model is a `Model` defined as a directed graph of layers.",
            "",
            "    Three types of `Model` exist: subclassed `Model`, `Functional` model,",
            "    and `Sequential` (a special case of `Functional`).",
            "",
            "    A `Functional` model can be instantiated by passing two arguments to",
            "    `__init__()`. The first argument is the `keras.Input` objects",
            "    that represent the inputs to the model.",
            "    The second argument specifies the output tensors that represent",
            "    the outputs of this model. Both arguments can be a nested structure",
            "    of tensors.",
            "",
            "    Example:",
            "",
            "    ```",
            "    inputs = {'x1': keras.Input(shape=(10,), name='x1'),",
            "              'x2': keras.Input(shape=(1,), name='x2')}",
            "    t = keras.layers.Dense(1, activation='relu')(inputs['x1'])",
            "    outputs = keras.layers.Add()([t, inputs['x2']])",
            "    model = keras.Model(inputs, outputs)",
            "    ```",
            "",
            "    A `Functional` model constructed using the Functional API can also",
            "    include raw Keras 3 ops.",
            "",
            "    Example:",
            "",
            "    ```python",
            "    inputs = keras.Input(shape=(10,))",
            "    x = keras.layers.Dense(1)(inputs)",
            "    outputs = ops.nn.relu(x)",
            "    model = keras.Model(inputs, outputs)",
            "    ```",
            "",
            "    A new `Functional` model can also be created by using the",
            "    intermediate tensors. This enables you to quickly extract sub-components",
            "    of the model.",
            "",
            "    Example:",
            "",
            "    ```python",
            "    inputs = keras.Input(shape=(None, None, 3))",
            "    processed = keras.layers.RandomCrop(width=32, height=32)(inputs)",
            "    conv = keras.layers.Conv2D(filters=2, kernel_size=3)(processed)",
            "    pooling = keras.layers.GlobalAveragePooling2D()(conv)",
            "    feature = keras.layers.Dense(10)(pooling)",
            "",
            "    full_model = keras.Model(inputs, feature)",
            "    backbone = keras.Model(processed, conv)",
            "    activations = keras.Model(conv, feature)",
            "    ```",
            "",
            "    Note that the `backbone` and `activations` models are not",
            "    created with `keras.Input` objects, but with the tensors",
            "    that are originated from `keras.Input` objects.",
            "    Under the hood, the layers and weights will",
            "    be shared across these models, so that user can train the `full_model`, and",
            "    use `backbone` or `activations` to do feature extraction.",
            "    The inputs and outputs of the model can be nested structures of tensors as",
            "    well, and the created models are standard `Functional` model that support",
            "    all the existing API.",
            "",
            "    Args:",
            "        inputs: List of input tensors (must be created via `keras.Input()`",
            "            or originated from `keras.Input()`).",
            "        outputs: List of output tensors.",
            "        name: String, optional. Name of the model.",
            "        trainable: Boolean, optional. If the model's variables should be",
            "            trainable.",
            "    \"\"\"",
            "",
            "    def __new__(cls, *args, **kwargs):",
            "        return typing.cast(cls, super().__new__(cls))",
            "",
            "    @tracking.no_automatic_dependency_tracking",
            "    def __init__(self, inputs, outputs, name=None, **kwargs):",
            "        if isinstance(inputs, dict):",
            "            for k, v in inputs.items():",
            "                if isinstance(v, backend.KerasTensor) and k != v.name:",
            "                    warnings.warn(",
            "                        \"When providing `inputs` as a dict, all keys in the \"",
            "                        \"dict must match the names of the corresponding \"",
            "                        f\"tensors. Received key '{k}' mapping to value {v} \"",
            "                        f\"which has name '{v.name}'. Change the tensor name to \"",
            "                        f\"'{k}' (via `Input(..., name='{k}')`)\"",
            "                    )",
            "",
            "        trainable = kwargs.pop(\"trainable\", None)",
            "        flat_inputs = tree.flatten(inputs)",
            "        flat_outputs = tree.flatten(outputs)",
            "        for x in flat_inputs:",
            "            if not isinstance(x, backend.KerasTensor):",
            "                raise ValueError(",
            "                    \"All `inputs` values must be KerasTensors. Received: \"",
            "                    f\"inputs={inputs} including invalid value {x} of \"",
            "                    f\"type {type(x)}\"",
            "                )",
            "        for x in flat_outputs:",
            "            if not isinstance(x, backend.KerasTensor):",
            "                raise ValueError(",
            "                    \"All `outputs` values must be KerasTensors. Received: \"",
            "                    f\"outputs={outputs} including invalid value {x} of \"",
            "                    f\"type {type(x)}\"",
            "                )",
            "",
            "        if not all(is_input_keras_tensor(t) for t in flat_inputs):",
            "            inputs, outputs = clone_graph_nodes(inputs, outputs)",
            "",
            "        Function.__init__(self, inputs, outputs, name=name)",
            "",
            "        if trainable is not None:",
            "            self.trainable = trainable",
            "",
            "        self._layers = self.layers",
            "        self.build(None)",
            "        # We will convert directly (to the correct dtype per input).",
            "        self._convert_input_args = False",
            "        self._allow_non_tensor_positional_args = True",
            "        output_layers = [x._keras_history[0] for x in self.outputs]",
            "        self.output_names = [x.name for x in output_layers]",
            "",
            "    def _lock_state(self):",
            "        # Unlike other layers, we allow Functional state to be mutable after",
            "        # build. E.g. to attach a layer to a model that is not part of the",
            "        # functional DAG.",
            "        pass",
            "",
            "    def _obj_type(self):",
            "        return \"Functional\"",
            "",
            "    @property",
            "    def layers(self):",
            "        layers = []",
            "        for operation in self._operations:",
            "            if isinstance(operation, Layer):",
            "                layers.append(operation)",
            "        return layers",
            "",
            "    @layers.setter",
            "    def layers(self, _):",
            "        raise AttributeError(",
            "            \"`Model.layers` attribute is reserved and should not be used. \"",
            "            \"Please use another name.\"",
            "        )",
            "",
            "    def call(self, inputs, training=None, mask=None):",
            "        # Add support for training, masking",
            "        inputs = self._standardize_inputs(inputs)",
            "        if mask is None:",
            "            masks = [None] * len(inputs)",
            "        else:",
            "            masks = tree.flatten(mask)",
            "            for x, mask in zip(inputs, masks):",
            "                if mask is not None:",
            "                    backend.set_keras_mask(x, mask)",
            "        outputs = self._run_through_graph(",
            "            inputs, operation_fn=lambda op: operation_fn(op, training=training)",
            "        )",
            "        return unpack_singleton(outputs)",
            "",
            "    def compute_output_spec(self, inputs, training=None, mask=None):",
            "        # From Function",
            "        return super().compute_output_spec(inputs)",
            "",
            "    def compute_output_shape(self, input_shape):",
            "        # From Function",
            "        return super().compute_output_shape(input_shape)",
            "",
            "    def build(self, input_shape):",
            "        self.built = True",
            "",
            "    @property",
            "    def input_shape(self):",
            "        input_shapes = tree.map_structure(lambda x: x.shape, self.inputs)",
            "        if isinstance(input_shapes, list) and len(input_shapes) == 1:",
            "            return input_shapes[0]",
            "        return input_shapes",
            "",
            "    @property",
            "    def output_shape(self):",
            "        output_shapes = tree.map_structure(lambda x: x.shape, self.outputs)",
            "        if isinstance(output_shapes, list) and len(output_shapes) == 1:",
            "            return output_shapes[0]",
            "        return output_shapes",
            "",
            "    def _assert_input_compatibility(self, *args):",
            "        return super(Model, self)._assert_input_compatibility(*args)",
            "",
            "    def _maybe_warn_inputs_struct_mismatch(self, inputs, raise_exception=False):",
            "        try:",
            "            # We first normalize to tuples before performing the check to",
            "            # suppress warnings when encountering mismatched tuples and lists.",
            "            tree.assert_same_structure(",
            "                tree.lists_to_tuples(inputs),",
            "                tree.lists_to_tuples(self._inputs_struct),",
            "            )",
            "        except:",
            "            model_inputs_struct = tree.map_structure(",
            "                lambda x: x.name, self._inputs_struct",
            "            )",
            "            inputs_struct = tree.map_structure(",
            "                lambda x: f\"Tensor(shape={x.shape})\", inputs",
            "            )",
            "            msg = (",
            "                \"The structure of `inputs` doesn't match the expected \"",
            "                f\"structure.\\nExpected: {model_inputs_struct}\\n\"",
            "                f\"Received: inputs={inputs_struct}\"",
            "            )",
            "            if raise_exception:",
            "                raise ValueError(msg)",
            "            warnings.warn(msg)",
            "",
            "    def _convert_inputs_to_tensors(self, flat_inputs):",
            "        converted = []",
            "        for x, input in zip(flat_inputs, self._inputs):",
            "            if x is None:  # TODO: check if optional",
            "                converted.append(x)",
            "            else:",
            "                converted.append(",
            "                    ops.convert_to_tensor(",
            "                        x, dtype=input.dtype, sparse=input.sparse",
            "                    )",
            "                )",
            "        return converted",
            "",
            "    def _adjust_input_rank(self, flat_inputs):",
            "        flat_ref_shapes = [x.shape for x in self._inputs]",
            "        adjusted = []",
            "        for x, ref_shape in zip(flat_inputs, flat_ref_shapes):",
            "            if x is None:",
            "                adjusted.append(x)",
            "                continue",
            "            x_rank = len(x.shape)",
            "            ref_rank = len(ref_shape)",
            "            if x_rank == ref_rank:",
            "                adjusted.append(x)",
            "                continue",
            "            if x_rank == ref_rank + 1:",
            "                if x.shape[-1] == 1:",
            "                    adjusted.append(ops.squeeze(x, axis=-1))",
            "                    continue",
            "            if x_rank == ref_rank - 1:",
            "                if ref_shape[-1] == 1:",
            "                    adjusted.append(ops.expand_dims(x, axis=-1))",
            "                    continue",
            "            raise ValueError(",
            "                f\"Invalid input shape for input {x}. Expected shape \"",
            "                f\"{ref_shape}, but input has incompatible shape {x.shape}\"",
            "            )",
            "        # Add back metadata.",
            "        for i in range(len(flat_inputs)):",
            "            if hasattr(flat_inputs[i], \"_keras_history\"):",
            "                adjusted[i]._keras_history = flat_inputs[i]._keras_history",
            "            mask = backend.get_keras_mask(flat_inputs[i])",
            "            if mask is not None:",
            "                backend.set_keras_mask(adjusted[i], mask)",
            "        return adjusted",
            "",
            "    def _standardize_inputs(self, inputs):",
            "        raise_exception = False",
            "        if isinstance(inputs, dict) and not isinstance(",
            "            self._inputs_struct, dict",
            "        ):",
            "            # This is to avoid warning",
            "            # when we have reconciable dict/list structs",
            "            if hasattr(self._inputs_struct, \"__len__\") and all(",
            "                isinstance(i, backend.KerasTensor) for i in self._inputs_struct",
            "            ):",
            "                expected_keys = set(i.name for i in self._inputs_struct)",
            "                keys = set(inputs.keys())",
            "                if expected_keys.issubset(keys):",
            "                    inputs = [inputs[i.name] for i in self._inputs_struct]",
            "                else:",
            "                    raise_exception = True",
            "            elif isinstance(self._inputs_struct, backend.KerasTensor):",
            "                if self._inputs_struct.name in inputs:",
            "                    inputs = [inputs[self._inputs_struct.name]]",
            "                else:",
            "                    raise_exception = True",
            "            else:",
            "                raise_exception = True",
            "        if (",
            "            isinstance(self._inputs_struct, dict)",
            "            and not isinstance(inputs, dict)",
            "            and list(self._inputs_struct.keys())",
            "            != sorted(self._inputs_struct.keys())",
            "        ):",
            "            raise_exception = True",
            "        self._maybe_warn_inputs_struct_mismatch(",
            "            inputs, raise_exception=raise_exception",
            "        )",
            "",
            "        flat_inputs = tree.flatten(inputs)",
            "        flat_inputs = self._convert_inputs_to_tensors(flat_inputs)",
            "        return self._adjust_input_rank(flat_inputs)",
            "",
            "    @property",
            "    def input(self):",
            "        # For backwards compatibility,",
            "        # override `input` to retrieve the used-provided",
            "        # constructor inputs",
            "        return self._inputs_struct",
            "",
            "    @property",
            "    def output(self):",
            "        return self._outputs_struct",
            "",
            "    def add_loss(self, loss):",
            "        # Symbolic only. TODO",
            "        raise NotImplementedError",
            "",
            "    @property",
            "    def input_spec(self):",
            "        if hasattr(self, \"_manual_input_spec\"):",
            "            return self._manual_input_spec",
            "",
            "        def shape_with_no_batch_size(x):",
            "            x = list(x)",
            "            if x:",
            "                x[0] = None",
            "            return tuple(x)",
            "",
            "        def make_spec_for_tensor(x):",
            "            optional = False",
            "            if isinstance(x._keras_history[0], InputLayer):",
            "                if x._keras_history[0].optional:",
            "                    optional = True",
            "            return InputSpec(",
            "                shape=shape_with_no_batch_size(x.shape),",
            "                allow_last_axis_squeeze=True,",
            "                name=x._keras_history[0].name,",
            "                optional=optional,",
            "            )",
            "",
            "        if isinstance(self._inputs_struct, dict):",
            "            if all(",
            "                isinstance(x, backend.KerasTensor)",
            "                for x in self._inputs_struct.values()",
            "            ):",
            "                # Case where `_nested_inputs` is a plain dict of Inputs.",
            "                names = sorted(self._inputs_struct.keys())",
            "                return [",
            "                    InputSpec(",
            "                        shape=shape_with_no_batch_size(",
            "                            self._inputs_struct[name].shape",
            "                        ),",
            "                        allow_last_axis_squeeze=True,",
            "                        name=name,",
            "                    )",
            "                    for name in names",
            "                ]",
            "            return None  # Deeply nested dict: skip checks.",
            "        return [make_spec_for_tensor(x) for x in self.inputs]",
            "",
            "    @input_spec.setter",
            "    def input_spec(self, value):",
            "        self._manual_input_spec = value",
            "",
            "    def get_config(self):",
            "        if not functional_like_constructor(self.__class__):",
            "            # Subclassed networks are not serializable",
            "            # (unless serialization is implemented by",
            "            # the author of the subclassed network).",
            "            return Model.get_config(self)",
            "",
            "        config = {",
            "            \"name\": self.name,",
            "            \"trainable\": self.trainable,",
            "        }",
            "        # Build a map from a layer unique name (make_node_key)",
            "        # to the index of the nodes that are saved in the config.",
            "        # Only nodes in network_nodes are saved.",
            "        node_reindexing_map = {}",
            "        for operation in self.operations:",
            "            if issubclass(operation.__class__, Functional):",
            "                # Functional models start with a pre-existing node",
            "                # linking their input to output.",
            "                kept_nodes = 1",
            "            else:",
            "                kept_nodes = 0",
            "            for original_node_index, node in enumerate(",
            "                operation._inbound_nodes",
            "            ):",
            "                node_key = make_node_key(operation, original_node_index)",
            "                if node_key in self._nodes:",
            "                    # i.e. we mark it to be saved",
            "                    node_reindexing_map[node_key] = kept_nodes",
            "                    kept_nodes += 1",
            "",
            "        # serialize and save the layers in layer_configs",
            "        layer_configs = []",
            "        for operation in self.operations:  # From the earliest layers on.",
            "            filtered_inbound_nodes = []",
            "            for original_node_index, node in enumerate(",
            "                operation._inbound_nodes",
            "            ):",
            "                node_key = make_node_key(operation, original_node_index)",
            "                if node_key in self._nodes:",
            "                    # The node is relevant to the model:",
            "                    # add to filtered_inbound_nodes.",
            "                    node_data = serialize_node(node, own_nodes=self._nodes)",
            "                    if node_data is not None:",
            "                        filtered_inbound_nodes.append(node_data)",
            "",
            "            serialize_obj_fn = serialization_lib.serialize_keras_object",
            "            if global_state.get_global_attribute(\"use_legacy_config\", False):",
            "                # Legacy format serialization used for H5 and SavedModel",
            "                serialize_obj_fn = legacy_serialization.serialize_keras_object",
            "            layer_config = serialize_obj_fn(operation)",
            "            layer_config[\"name\"] = operation.name",
            "            layer_config[\"inbound_nodes\"] = filtered_inbound_nodes",
            "            layer_configs.append(layer_config)",
            "        config[\"layers\"] = layer_configs",
            "",
            "        # Gather info about inputs and outputs.",
            "        def get_tensor_config(tensor):",
            "            operation = tensor._keras_history[0]",
            "            node_index = tensor._keras_history[1]",
            "            tensor_index = tensor._keras_history[2]",
            "            node_key = make_node_key(operation, node_index)",
            "            assert node_key in self._nodes",
            "            new_node_index = node_reindexing_map[node_key]",
            "            return [operation.name, new_node_index, tensor_index]",
            "",
            "        def map_tensors(tensors):",
            "            if isinstance(tensors, backend.KerasTensor):",
            "                return [get_tensor_config(tensors)]",
            "            return tree.map_structure(get_tensor_config, tensors)",
            "",
            "        config[\"input_layers\"] = map_tensors(self._inputs_struct)",
            "        config[\"output_layers\"] = map_tensors(self._outputs_struct)",
            "        return copy.deepcopy(config)",
            "",
            "",
            "def functional_from_config(cls, config, custom_objects=None):",
            "    \"\"\"Instantiates a Functional model from its config (from `get_config()`).",
            "",
            "    Args:",
            "        cls: Class of the model, e.g. a custom subclass of `Model`.",
            "        config: Output of `get_config()` for the original model instance.",
            "        custom_objects: Optional dict of custom objects.",
            "",
            "    Returns:",
            "        An instance of `cls`.",
            "    \"\"\"",
            "    # Layer instances created during",
            "    # the graph reconstruction process",
            "    created_layers = {}",
            "",
            "    # Dictionary mapping layer instances to",
            "    # node data that specifies a layer call.",
            "    # It acts as a queue that maintains any unprocessed",
            "    # layer call until it becomes possible to process it",
            "    # (i.e. until the input tensors to the call all exist).",
            "    unprocessed_nodes = {}",
            "",
            "    def add_unprocessed_node(layer, node_data):",
            "        \"\"\"Add node to layer list",
            "",
            "        Arg:",
            "            layer: layer object",
            "            node_data: Node data specifying layer call",
            "        \"\"\"",
            "        if layer not in unprocessed_nodes:",
            "            unprocessed_nodes[layer] = [node_data]",
            "        else:",
            "            unprocessed_nodes[layer].append(node_data)",
            "",
            "    def process_node(layer, node_data):",
            "        \"\"\"Reconstruct node by linking to inbound layers",
            "",
            "        Args:",
            "            layer: Layer to process",
            "            node_data: List of layer configs",
            "        \"\"\"",
            "        args, kwargs = deserialize_node(node_data, created_layers)",
            "        # Call layer on its inputs, thus creating the node",
            "        # and building the layer if needed.",
            "        layer(*args, **kwargs)",
            "",
            "    def process_layer(layer_data):",
            "        \"\"\"Deserializes a layer and index its inbound nodes.",
            "",
            "        Args:",
            "            layer_data: layer config dict.",
            "        \"\"\"",
            "        layer_name = layer_data[\"name\"]",
            "",
            "        # Instantiate layer.",
            "        if \"module\" not in layer_data:",
            "            # Legacy format deserialization (no \"module\" key)",
            "            # used for H5 and SavedModel formats",
            "            layer = saving_utils.model_from_config(",
            "                layer_data, custom_objects=custom_objects",
            "            )",
            "        else:",
            "            layer = serialization_lib.deserialize_keras_object(",
            "                layer_data, custom_objects=custom_objects",
            "            )",
            "        if not isinstance(layer, Operation):",
            "            raise ValueError(",
            "                \"Unexpected object from deserialization, expected a layer or \"",
            "                f\"operation, got a {type(layer)}\"",
            "            )",
            "        created_layers[layer_name] = layer",
            "",
            "        # Gather layer inputs.",
            "        inbound_nodes_data = layer_data[\"inbound_nodes\"]",
            "        for node_data in inbound_nodes_data:",
            "            # We don't process nodes (i.e. make layer calls)",
            "            # on the fly because the inbound node may not yet exist,",
            "            # in case of layer shared at different topological depths",
            "            # (e.g. a model such as A(B(A(B(x)))))",
            "            add_unprocessed_node(layer, node_data)",
            "",
            "    # Extract config used to instantiate Functional model from the config. The",
            "    # remaining config will be passed as keyword arguments to the Model",
            "    # constructor.",
            "    functional_config = {}",
            "    for key in [\"layers\", \"input_layers\", \"output_layers\"]:",
            "        functional_config[key] = config.pop(key)",
            "    for key in [\"name\", \"trainable\"]:",
            "        if key in config:",
            "            functional_config[key] = config.pop(key)",
            "        else:",
            "            functional_config[key] = None",
            "",
            "    # First, we create all layers and enqueue nodes to be processed",
            "    for layer_data in functional_config[\"layers\"]:",
            "        process_layer(layer_data)",
            "",
            "    # Then we process nodes in order of layer depth.",
            "    # Nodes that cannot yet be processed (if the inbound node",
            "    # does not yet exist) are re-enqueued, and the process",
            "    # is repeated until all nodes are processed.",
            "    while unprocessed_nodes:",
            "        for layer_data in functional_config[\"layers\"]:",
            "            layer = created_layers[layer_data[\"name\"]]",
            "",
            "            # Process all nodes in layer, if not yet processed",
            "            if layer in unprocessed_nodes:",
            "                node_data_list = unprocessed_nodes[layer]",
            "",
            "                # Process nodes in order",
            "                node_index = 0",
            "                while node_index < len(node_data_list):",
            "                    node_data = node_data_list[node_index]",
            "                    try:",
            "                        process_node(layer, node_data)",
            "",
            "                    # If the node does not have all inbound layers",
            "                    # available, stop processing and continue later",
            "                    except IndexError:",
            "                        break",
            "",
            "                    node_index += 1",
            "",
            "                # If not all nodes processed then store unprocessed nodes",
            "                if node_index < len(node_data_list):",
            "                    unprocessed_nodes[layer] = node_data_list[node_index:]",
            "                # If all nodes processed remove the layer",
            "                else:",
            "                    del unprocessed_nodes[layer]",
            "",
            "    # Create list of input and output tensors and return new class",
            "    name = functional_config[\"name\"]",
            "    trainable = functional_config[\"trainable\"]",
            "",
            "    def get_tensor(layer_name, node_index, tensor_index):",
            "        assert layer_name in created_layers",
            "        layer = created_layers[layer_name]",
            "        if isinstance(layer, Functional):",
            "            # Functional models start out with a built-in node.",
            "            node_index -= 1",
            "        layer_output_tensors = layer._inbound_nodes[node_index].output_tensors",
            "        return layer_output_tensors[tensor_index]",
            "",
            "    def map_tensors(tensors):",
            "        if (",
            "            isinstance(tensors, list)",
            "            and len(tensors) == 3",
            "            and isinstance(tensors[0], str)",
            "        ):",
            "            # Leaf",
            "            return get_tensor(*tensors)",
            "        if isinstance(tensors, dict):",
            "            return {k: map_tensors(v) for k, v in tensors.items()}",
            "        if isinstance(tensors, tuple):",
            "            return tuple([map_tensors(v) for v in tensors])",
            "        return [map_tensors(v) for v in tensors]",
            "",
            "    input_tensors = map_tensors(functional_config[\"input_layers\"])",
            "    output_tensors = map_tensors(functional_config[\"output_layers\"])",
            "    if isinstance(input_tensors, list) and len(input_tensors) == 1:",
            "        input_tensors = input_tensors[0]",
            "    if isinstance(output_tensors, list) and len(output_tensors) == 1:",
            "        output_tensors = output_tensors[0]",
            "",
            "    return cls(",
            "        inputs=input_tensors,",
            "        outputs=output_tensors,",
            "        name=name,",
            "        trainable=trainable,",
            "        **config,",
            "    )",
            "",
            "",
            "def operation_fn(operation, training):",
            "    def call(*args, **kwargs):",
            "        if (",
            "            hasattr(operation, \"_call_has_training_arg\")",
            "            and operation._call_has_training_arg",
            "            and training is not None",
            "        ):",
            "            kwargs[\"training\"] = training",
            "        return operation(*args, **kwargs)",
            "",
            "    return call",
            "",
            "",
            "def functional_like_constructor(cls):",
            "    init_args = inspect.getfullargspec(cls.__init__).args[1:]",
            "    functional_init_args = inspect.getfullargspec(Functional.__init__).args[1:]",
            "    if init_args == functional_init_args:",
            "        return True",
            "    return False",
            "",
            "",
            "def unpack_singleton(x):",
            "    if isinstance(x, (list, tuple)) and len(x) == 1:",
            "        return x[0]",
            "    return x",
            "",
            "",
            "def serialize_node(node, own_nodes=()):",
            "    if not node.input_tensors:",
            "        # Does not need to be serialized.",
            "        return",
            "",
            "    def serialize_keras_tensor(x):",
            "        # Serialize KerasTensor while converting",
            "        # node indices to only include nodes relevant to `own_nodes`.",
            "        if isinstance(x, backend.KerasTensor):",
            "            operation, node_index, tensor_index = x._keras_history",
            "            irrelevant_node_count = 0",
            "            for i, node in enumerate(operation._inbound_nodes[:node_index]):",
            "                node_key = make_node_key(operation, i)",
            "                if node_key not in own_nodes:",
            "                    irrelevant_node_count += 1",
            "            x._keras_history = KerasHistory(",
            "                operation, node_index - irrelevant_node_count, tensor_index",
            "            )",
            "            serialized = serialization_lib.serialize_keras_object(x)",
            "            x._keras_history = KerasHistory(operation, node_index, tensor_index)",
            "            return serialized",
            "        return x",
            "",
            "    args = node.arguments.args",
            "    kwargs = node.arguments.kwargs",
            "",
            "    args = tree.map_structure(serialize_keras_tensor, args)",
            "    kwargs = tree.map_structure(serialize_keras_tensor, kwargs)",
            "    return {",
            "        \"args\": serialization_lib.serialize_keras_object(args),",
            "        \"kwargs\": serialization_lib.serialize_keras_object(kwargs),",
            "    }",
            "",
            "",
            "def deserialize_node(node_data, created_layers):",
            "    \"\"\"Return (args, kwargs) for calling the node layer.\"\"\"",
            "    if not node_data:",
            "        return [], {}",
            "",
            "    if isinstance(node_data, list):",
            "        # Legacy case.",
            "        input_tensors = []",
            "        for input_data in node_data:",
            "            inbound_layer_name = input_data[0]",
            "            inbound_node_index = input_data[1]",
            "            inbound_tensor_index = input_data[2]",
            "            if len(input_data) == 3:",
            "                kwargs = {}",
            "            elif len(input_data) == 4:",
            "                kwargs = input_data[3]",
            "            else:",
            "                raise ValueError(",
            "                    \"Cannot deserialize the model (invalid config data?)\"",
            "                )",
            "            inbound_layer = created_layers[inbound_layer_name]",
            "",
            "            # Raise an error if the corresponding layer node",
            "            # has not yet been created",
            "            if len(inbound_layer._inbound_nodes) <= inbound_node_index:",
            "                raise IndexError(",
            "                    \"Layer node index out of bounds.\\n\"",
            "                    f\"inbound_layer = {inbound_layer}\\n\"",
            "                    \"inbound_layer._inbound_nodes = \"",
            "                    f\"{inbound_layer._inbound_nodes}\\n\"",
            "                    f\"inbound_node_index = {inbound_node_index}\"",
            "                )",
            "            inbound_node = inbound_layer._inbound_nodes[inbound_node_index]",
            "            input_tensors.append(",
            "                inbound_node.output_tensors[inbound_tensor_index]",
            "            )",
            "        return [unpack_singleton(input_tensors)], kwargs",
            "",
            "    args = serialization_lib.deserialize_keras_object(node_data[\"args\"])",
            "    kwargs = serialization_lib.deserialize_keras_object(node_data[\"kwargs\"])",
            "",
            "    def convert_revived_tensor(x):",
            "        if isinstance(x, backend.KerasTensor):",
            "            history = x._pre_serialization_keras_history",
            "            if history is None:",
            "                return x",
            "            layer = created_layers.get(history[0], None)",
            "            if layer is None:",
            "                raise ValueError(f\"Unknown layer: {history[0]}\")",
            "            inbound_node_index = history[1]",
            "            inbound_tensor_index = history[2]",
            "            if len(layer._inbound_nodes) <= inbound_node_index:",
            "                raise IndexError(",
            "                    \"Layer node index out of bounds.\\n\"",
            "                    f\"inbound_layer = {layer}\\n\"",
            "                    f\"inbound_layer._inbound_nodes = {layer._inbound_nodes}\\n\"",
            "                    f\"inbound_node_index = {inbound_node_index}\"",
            "                )",
            "            inbound_node = layer._inbound_nodes[inbound_node_index]",
            "            return inbound_node.output_tensors[inbound_tensor_index]",
            "        return x",
            "",
            "    args = tree.map_structure(convert_revived_tensor, args)",
            "    kwargs = tree.map_structure(convert_revived_tensor, kwargs)",
            "    return args, kwargs",
            "",
            "",
            "def is_input_keras_tensor(x):",
            "    (",
            "        operation,",
            "        node_index,",
            "        _,",
            "    ) = x._keras_history",
            "    node = operation._inbound_nodes[node_index]",
            "    return node.is_input",
            "",
            "",
            "def clone_single_keras_tensor(x):",
            "    return backend.KerasTensor(",
            "        shape=x.shape, dtype=x.dtype, sparse=x.sparse, name=x.name + \"_clone\"",
            "    )",
            "",
            "",
            "def clone_keras_tensors(tensors, kt_id_mapping):",
            "    def swap(x):",
            "        if not isinstance(x, backend.KerasTensor):",
            "            return x",
            "        if id(x) in kt_id_mapping:",
            "            return kt_id_mapping[id(x)]",
            "        new_x = clone_single_keras_tensor(x)",
            "        kt_id_mapping[id(x)] = new_x",
            "        return new_x",
            "",
            "    return tree.map_structure(swap, tensors)",
            "",
            "",
            "def find_nodes_by_inputs_and_outputs(inputs, outputs):",
            "    nodes, _ = _build_map(inputs, outputs)",
            "    return nodes",
            "",
            "",
            "def clone_graph_nodes(inputs, outputs):",
            "    \"\"\"Clone the `Node` between the inputs and output tensors.",
            "",
            "    This function is used to create a new functional model from any intermediate",
            "    Keras tensors. The clone of the nodes mimic the behavior of reconstructing",
            "    the functional graph network by re-executing all the `__call__()` methods.",
            "    The cloned nodes will be appended to the layers.",
            "",
            "    Note that a new `keras.Input` will be created for any items in the",
            "    `inputs`",
            "",
            "    Args:",
            "    inputs: A nested structure of `KerasTensor` instances.",
            "    outputs: A nested structure of `KerasTensor` instances.",
            "",
            "    Returns:",
            "        A pair of inputs and outputs, with cloned `KerasTensor` instances.",
            "        They can be used to create a new functional model.",
            "    \"\"\"",
            "    nodes_to_clone = find_nodes_by_inputs_and_outputs(inputs, outputs)",
            "    cloned_inputs = []",
            "    cloned_outputs = []",
            "    # We not only need to create copies of Nodes (mimic the calls), also need to",
            "    # clone Keras tensors to avoid the override of _keras_history attached on",
            "    # the Keras tensor. The following dict is used to track any keras tensor we",
            "    # cloned The key is the string ID of the original keras tensor, and value is",
            "    # the cloned Keras tensor instance.",
            "    kt_id_mapping = {}",
            "    op_id_mapping = {}",
            "",
            "    for kt_input in tree.flatten(inputs):",
            "        if is_input_keras_tensor(kt_input):",
            "            # For any existing Keras tensor from keras.Input, leave them as is.",
            "            cloned_inputs.append(kt_input)",
            "            kt_id_mapping[id(kt_input)] = kt_input",
            "        else:",
            "            # We need to create a new Keras tensor for any intermediate tensor",
            "            cloned_input = Input(",
            "                batch_shape=kt_input.shape,",
            "                dtype=kt_input.dtype,",
            "                sparse=kt_input.sparse,",
            "                name=kt_input.name + \"CLONE\",",
            "            )",
            "            cloned_inputs.append(cloned_input)",
            "            kt_id_mapping[id(kt_input)] = cloned_input",
            "            op_id_mapping[id(kt_input._keras_history[0])] = (",
            "                cloned_input._keras_history[0]",
            "            )",
            "    cloned_inputs = tree.pack_sequence_as(inputs, cloned_inputs)",
            "",
            "    for kt_output in tree.flatten(outputs):",
            "        cpy = clone_single_keras_tensor(kt_output)",
            "        # We reuse the _keras_history here, which contains the old information.",
            "        cpy._keras_history = kt_output._keras_history",
            "        cloned_outputs.append(cpy)",
            "        kt_id_mapping[id(kt_output)] = cpy",
            "    cloned_outputs = tree.pack_sequence_as(outputs, cloned_outputs)",
            "",
            "    for node in nodes_to_clone:",
            "        if id(node.operation) in op_id_mapping:",
            "            operation = op_id_mapping[id(node.operation)]",
            "        else:",
            "            operation = node.operation",
            "        # Clone any Keras tensor to avoid override of _keras_history",
            "        # Or reuse an existing Keras tensor if it has already been cloned.",
            "        output_copy = clone_keras_tensors(node.output_tensors, kt_id_mapping)",
            "        if not isinstance(operation, InputLayer):",
            "            call_args_copy = clone_keras_tensors(",
            "                node.arguments.args, kt_id_mapping",
            "            )",
            "            call_kwargs_copy = clone_keras_tensors(",
            "                node.arguments.kwargs, kt_id_mapping",
            "            )",
            "        else:",
            "            call_args_copy = ()",
            "            call_kwargs_copy = {}",
            "        # Creating new nodes based on the existing node information.  Node wires",
            "        # itself to inbound and outbound layers.  The Node constructor actually",
            "        # updates this layer's self._inbound_nodes, sets _keras_history on the",
            "        # outputs, and adds itself to the `_outbound_nodes` of the layers that",
            "        # produced the inputs to this layer call.",
            "        Node(",
            "            operation,",
            "            call_args=call_args_copy,",
            "            call_kwargs=call_kwargs_copy,",
            "            outputs=output_copy,",
            "        )",
            "    return cloned_inputs, cloned_outputs"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "keras.src.models.functional.functional_from_config"
        ]
    },
    "keras/src/saving/serialization_lib.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 783,
                "afterPatchRowNumber": 783,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 784,
                "afterPatchRowNumber": 784,
                "PatchRowcode": "         # Otherwise, attempt to retrieve the class object given the `module`"
            },
            "2": {
                "beforePatchRowNumber": 785,
                "afterPatchRowNumber": 785,
                "PatchRowcode": "         # and `class_name`. Import the module, find the class."
            },
            "3": {
                "beforePatchRowNumber": 786,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        try:"
            },
            "4": {
                "beforePatchRowNumber": 787,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            mod = importlib.import_module(module)"
            },
            "5": {
                "beforePatchRowNumber": 788,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        except ModuleNotFoundError:"
            },
            "6": {
                "beforePatchRowNumber": 789,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            raise TypeError("
            },
            "7": {
                "beforePatchRowNumber": 790,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                f\"Could not deserialize {obj_type} '{name}' because \""
            },
            "8": {
                "beforePatchRowNumber": 791,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                f\"its parent module {module} cannot be imported. \""
            },
            "9": {
                "beforePatchRowNumber": 792,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                f\"Full object config: {full_config}\""
            },
            "10": {
                "beforePatchRowNumber": 793,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "11": {
                "beforePatchRowNumber": 794,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        obj = vars(mod).get(name, None)"
            },
            "12": {
                "beforePatchRowNumber": 795,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "13": {
                "beforePatchRowNumber": 796,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # Special case for keras.metrics.metrics"
            },
            "14": {
                "beforePatchRowNumber": 797,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if obj is None and registered_name is not None:"
            },
            "15": {
                "beforePatchRowNumber": 798,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            obj = vars(mod).get(registered_name, None)"
            },
            "16": {
                "beforePatchRowNumber": 799,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "17": {
                "beforePatchRowNumber": 800,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if obj is not None:"
            },
            "18": {
                "beforePatchRowNumber": 801,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return obj"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 786,
                "PatchRowcode": "+        if module == \"keras.src\" or module.startswith(\"keras.src.\"):"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 787,
                "PatchRowcode": "+            try:"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 788,
                "PatchRowcode": "+                mod = importlib.import_module(module)"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 789,
                "PatchRowcode": "+                obj = vars(mod).get(name, None)"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 790,
                "PatchRowcode": "+                if obj is not None:"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 791,
                "PatchRowcode": "+                    return obj"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 792,
                "PatchRowcode": "+            except ModuleNotFoundError:"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 793,
                "PatchRowcode": "+                raise TypeError("
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 794,
                "PatchRowcode": "+                    f\"Could not deserialize {obj_type} '{name}' because \""
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 795,
                "PatchRowcode": "+                    f\"its parent module {module} cannot be imported. \""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 796,
                "PatchRowcode": "+                    f\"Full object config: {full_config}\""
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 797,
                "PatchRowcode": "+                )"
            },
            "31": {
                "beforePatchRowNumber": 802,
                "afterPatchRowNumber": 798,
                "PatchRowcode": " "
            },
            "32": {
                "beforePatchRowNumber": 803,
                "afterPatchRowNumber": 799,
                "PatchRowcode": "     raise TypeError("
            },
            "33": {
                "beforePatchRowNumber": 804,
                "afterPatchRowNumber": 800,
                "PatchRowcode": "         f\"Could not locate {obj_type} '{name}'. \""
            }
        },
        "frontPatchFile": [
            "\"\"\"Object config serialization and deserialization logic.\"\"\"",
            "",
            "import importlib",
            "import inspect",
            "import types",
            "import warnings",
            "",
            "import numpy as np",
            "",
            "from keras.src import api_export",
            "from keras.src import backend",
            "from keras.src.api_export import keras_export",
            "from keras.src.backend.common import global_state",
            "from keras.src.saving import object_registration",
            "from keras.src.utils import python_utils",
            "from keras.src.utils.module_utils import tensorflow as tf",
            "",
            "PLAIN_TYPES = (str, int, float, bool)",
            "",
            "# List of Keras modules with built-in string representations for Keras defaults",
            "BUILTIN_MODULES = (",
            "    \"activations\",",
            "    \"constraints\",",
            "    \"initializers\",",
            "    \"losses\",",
            "    \"metrics\",",
            "    \"optimizers\",",
            "    \"regularizers\",",
            ")",
            "",
            "",
            "class SerializableDict:",
            "    def __init__(self, **config):",
            "        self.config = config",
            "",
            "    def serialize(self):",
            "        return serialize_keras_object(self.config)",
            "",
            "",
            "class SafeModeScope:",
            "    \"\"\"Scope to propagate safe mode flag to nested deserialization calls.\"\"\"",
            "",
            "    def __init__(self, safe_mode=True):",
            "        self.safe_mode = safe_mode",
            "",
            "    def __enter__(self):",
            "        self.original_value = in_safe_mode()",
            "        global_state.set_global_attribute(\"safe_mode_saving\", self.safe_mode)",
            "",
            "    def __exit__(self, *args, **kwargs):",
            "        global_state.set_global_attribute(",
            "            \"safe_mode_saving\", self.original_value",
            "        )",
            "",
            "",
            "@keras_export(\"keras.config.enable_unsafe_deserialization\")",
            "def enable_unsafe_deserialization():",
            "    \"\"\"Disables safe mode globally, allowing deserialization of lambdas.\"\"\"",
            "    global_state.set_global_attribute(\"safe_mode_saving\", False)",
            "",
            "",
            "def in_safe_mode():",
            "    return global_state.get_global_attribute(\"safe_mode_saving\")",
            "",
            "",
            "class ObjectSharingScope:",
            "    \"\"\"Scope to enable detection and reuse of previously seen objects.\"\"\"",
            "",
            "    def __enter__(self):",
            "        global_state.set_global_attribute(\"shared_objects/id_to_obj_map\", {})",
            "        global_state.set_global_attribute(\"shared_objects/id_to_config_map\", {})",
            "",
            "    def __exit__(self, *args, **kwargs):",
            "        global_state.set_global_attribute(\"shared_objects/id_to_obj_map\", None)",
            "        global_state.set_global_attribute(",
            "            \"shared_objects/id_to_config_map\", None",
            "        )",
            "",
            "",
            "def get_shared_object(obj_id):",
            "    \"\"\"Retrieve an object previously seen during deserialization.\"\"\"",
            "    id_to_obj_map = global_state.get_global_attribute(",
            "        \"shared_objects/id_to_obj_map\"",
            "    )",
            "    if id_to_obj_map is not None:",
            "        return id_to_obj_map.get(obj_id, None)",
            "",
            "",
            "def record_object_after_serialization(obj, config):",
            "    \"\"\"Call after serializing an object, to keep track of its config.\"\"\"",
            "    if config[\"module\"] == \"__main__\":",
            "        config[\"module\"] = None  # Ensures module is None when no module found",
            "    id_to_config_map = global_state.get_global_attribute(",
            "        \"shared_objects/id_to_config_map\"",
            "    )",
            "    if id_to_config_map is None:",
            "        return  # Not in a sharing scope",
            "    obj_id = int(id(obj))",
            "    if obj_id not in id_to_config_map:",
            "        id_to_config_map[obj_id] = config",
            "    else:",
            "        config[\"shared_object_id\"] = obj_id",
            "        prev_config = id_to_config_map[obj_id]",
            "        prev_config[\"shared_object_id\"] = obj_id",
            "",
            "",
            "def record_object_after_deserialization(obj, obj_id):",
            "    \"\"\"Call after deserializing an object, to keep track of it in the future.\"\"\"",
            "    id_to_obj_map = global_state.get_global_attribute(",
            "        \"shared_objects/id_to_obj_map\"",
            "    )",
            "    if id_to_obj_map is None:",
            "        return  # Not in a sharing scope",
            "    id_to_obj_map[obj_id] = obj",
            "",
            "",
            "@keras_export(",
            "    [",
            "        \"keras.saving.serialize_keras_object\",",
            "        \"keras.utils.serialize_keras_object\",",
            "    ]",
            ")",
            "def serialize_keras_object(obj):",
            "    \"\"\"Retrieve the config dict by serializing the Keras object.",
            "",
            "    `serialize_keras_object()` serializes a Keras object to a python dictionary",
            "    that represents the object, and is a reciprocal function of",
            "    `deserialize_keras_object()`. See `deserialize_keras_object()` for more",
            "    information about the config format.",
            "",
            "    Args:",
            "        obj: the Keras object to serialize.",
            "",
            "    Returns:",
            "        A python dict that represents the object. The python dict can be",
            "        deserialized via `deserialize_keras_object()`.",
            "    \"\"\"",
            "    if obj is None:",
            "        return obj",
            "",
            "    if isinstance(obj, PLAIN_TYPES):",
            "        return obj",
            "",
            "    if isinstance(obj, (list, tuple)):",
            "        config_arr = [serialize_keras_object(x) for x in obj]",
            "        return tuple(config_arr) if isinstance(obj, tuple) else config_arr",
            "    if isinstance(obj, dict):",
            "        return serialize_dict(obj)",
            "",
            "    # Special cases:",
            "    if isinstance(obj, bytes):",
            "        return {",
            "            \"class_name\": \"__bytes__\",",
            "            \"config\": {\"value\": obj.decode(\"utf-8\")},",
            "        }",
            "    if isinstance(obj, slice):",
            "        return {",
            "            \"class_name\": \"__slice__\",",
            "            \"config\": {",
            "                \"start\": serialize_keras_object(obj.start),",
            "                \"stop\": serialize_keras_object(obj.stop),",
            "                \"step\": serialize_keras_object(obj.step),",
            "            },",
            "        }",
            "    # Ellipsis is an instance, and ellipsis class is not in global scope.",
            "    # checking equality also fails elsewhere in the library, so we have",
            "    # to dynamically get the type.",
            "    if isinstance(obj, type(Ellipsis)):",
            "        return {\"class_name\": \"__ellipsis__\", \"config\": {}}",
            "    if isinstance(obj, backend.KerasTensor):",
            "        history = getattr(obj, \"_keras_history\", None)",
            "        if history:",
            "            history = list(history)",
            "            history[0] = history[0].name",
            "        return {",
            "            \"class_name\": \"__keras_tensor__\",",
            "            \"config\": {",
            "                \"shape\": obj.shape,",
            "                \"dtype\": obj.dtype,",
            "                \"keras_history\": history,",
            "            },",
            "        }",
            "    if tf.available and isinstance(obj, tf.TensorShape):",
            "        return obj.as_list() if obj._dims is not None else None",
            "    if backend.is_tensor(obj):",
            "        return {",
            "            \"class_name\": \"__tensor__\",",
            "            \"config\": {",
            "                \"value\": backend.convert_to_numpy(obj).tolist(),",
            "                \"dtype\": backend.standardize_dtype(obj.dtype),",
            "            },",
            "        }",
            "    if type(obj).__module__ == np.__name__:",
            "        if isinstance(obj, np.ndarray) and obj.ndim > 0:",
            "            return {",
            "                \"class_name\": \"__numpy__\",",
            "                \"config\": {",
            "                    \"value\": obj.tolist(),",
            "                    \"dtype\": backend.standardize_dtype(obj.dtype),",
            "                },",
            "            }",
            "        else:",
            "            # Treat numpy floats / etc as plain types.",
            "            return obj.item()",
            "    if tf.available and isinstance(obj, tf.DType):",
            "        return obj.name",
            "    if isinstance(obj, types.FunctionType) and obj.__name__ == \"<lambda>\":",
            "        warnings.warn(",
            "            \"The object being serialized includes a `lambda`. This is unsafe. \"",
            "            \"In order to reload the object, you will have to pass \"",
            "            \"`safe_mode=False` to the loading function. \"",
            "            \"Please avoid using `lambda` in the \"",
            "            \"future, and use named Python functions instead. \"",
            "            f\"This is the `lambda` being serialized: {inspect.getsource(obj)}\",",
            "            stacklevel=2,",
            "        )",
            "        return {",
            "            \"class_name\": \"__lambda__\",",
            "            \"config\": {",
            "                \"value\": python_utils.func_dump(obj),",
            "            },",
            "        }",
            "    if tf.available and isinstance(obj, tf.TypeSpec):",
            "        ts_config = obj._serialize()",
            "        # TensorShape and tf.DType conversion",
            "        ts_config = list(",
            "            map(",
            "                lambda x: (",
            "                    x.as_list()",
            "                    if isinstance(x, tf.TensorShape)",
            "                    else (x.name if isinstance(x, tf.DType) else x)",
            "                ),",
            "                ts_config,",
            "            )",
            "        )",
            "        return {",
            "            \"class_name\": \"__typespec__\",",
            "            \"spec_name\": obj.__class__.__name__,",
            "            \"module\": obj.__class__.__module__,",
            "            \"config\": ts_config,",
            "            \"registered_name\": None,",
            "        }",
            "",
            "    inner_config = _get_class_or_fn_config(obj)",
            "    config_with_public_class = serialize_with_public_class(",
            "        obj.__class__, inner_config",
            "    )",
            "",
            "    if config_with_public_class is not None:",
            "        get_build_and_compile_config(obj, config_with_public_class)",
            "        record_object_after_serialization(obj, config_with_public_class)",
            "        return config_with_public_class",
            "",
            "    # Any custom object or otherwise non-exported object",
            "    if isinstance(obj, types.FunctionType):",
            "        module = obj.__module__",
            "    else:",
            "        module = obj.__class__.__module__",
            "    class_name = obj.__class__.__name__",
            "",
            "    if module == \"builtins\":",
            "        registered_name = None",
            "    else:",
            "        if isinstance(obj, types.FunctionType):",
            "            registered_name = object_registration.get_registered_name(obj)",
            "        else:",
            "            registered_name = object_registration.get_registered_name(",
            "                obj.__class__",
            "            )",
            "",
            "    config = {",
            "        \"module\": module,",
            "        \"class_name\": class_name,",
            "        \"config\": inner_config,",
            "        \"registered_name\": registered_name,",
            "    }",
            "    get_build_and_compile_config(obj, config)",
            "    record_object_after_serialization(obj, config)",
            "    return config",
            "",
            "",
            "def get_build_and_compile_config(obj, config):",
            "    if hasattr(obj, \"get_build_config\"):",
            "        build_config = obj.get_build_config()",
            "        if build_config is not None:",
            "            config[\"build_config\"] = serialize_dict(build_config)",
            "    if hasattr(obj, \"get_compile_config\"):",
            "        compile_config = obj.get_compile_config()",
            "        if compile_config is not None:",
            "            config[\"compile_config\"] = serialize_dict(compile_config)",
            "    return",
            "",
            "",
            "def serialize_with_public_class(cls, inner_config=None):",
            "    \"\"\"Serializes classes from public Keras API or object registration.",
            "",
            "    Called to check and retrieve the config of any class that has a public",
            "    Keras API or has been registered as serializable via",
            "    `keras.saving.register_keras_serializable()`.",
            "    \"\"\"",
            "    # This gets the `keras.*` exported name, such as",
            "    # \"keras.optimizers.Adam\".",
            "    keras_api_name = api_export.get_name_from_symbol(cls)",
            "",
            "    # Case of custom or unknown class object",
            "    if keras_api_name is None:",
            "        registered_name = object_registration.get_registered_name(cls)",
            "        if registered_name is None:",
            "            return None",
            "",
            "        # Return custom object config with corresponding registration name",
            "        return {",
            "            \"module\": cls.__module__,",
            "            \"class_name\": cls.__name__,",
            "            \"config\": inner_config,",
            "            \"registered_name\": registered_name,",
            "        }",
            "",
            "    # Split the canonical Keras API name into a Keras module and class name.",
            "    parts = keras_api_name.split(\".\")",
            "    return {",
            "        \"module\": \".\".join(parts[:-1]),",
            "        \"class_name\": parts[-1],",
            "        \"config\": inner_config,",
            "        \"registered_name\": None,",
            "    }",
            "",
            "",
            "def serialize_with_public_fn(fn, config, fn_module_name=None):",
            "    \"\"\"Serializes functions from public Keras API or object registration.",
            "",
            "    Called to check and retrieve the config of any function that has a public",
            "    Keras API or has been registered as serializable via",
            "    `keras.saving.register_keras_serializable()`. If function's module name",
            "    is already known, returns corresponding config.",
            "    \"\"\"",
            "    if fn_module_name:",
            "        return {",
            "            \"module\": fn_module_name,",
            "            \"class_name\": \"function\",",
            "            \"config\": config,",
            "            \"registered_name\": config,",
            "        }",
            "    keras_api_name = api_export.get_name_from_symbol(fn)",
            "    if keras_api_name:",
            "        parts = keras_api_name.split(\".\")",
            "        return {",
            "            \"module\": \".\".join(parts[:-1]),",
            "            \"class_name\": \"function\",",
            "            \"config\": config,",
            "            \"registered_name\": config,",
            "        }",
            "    else:",
            "        registered_name = object_registration.get_registered_name(fn)",
            "        if not registered_name and not fn.__module__ == \"builtins\":",
            "            return None",
            "        return {",
            "            \"module\": fn.__module__,",
            "            \"class_name\": \"function\",",
            "            \"config\": config,",
            "            \"registered_name\": registered_name,",
            "        }",
            "",
            "",
            "def _get_class_or_fn_config(obj):",
            "    \"\"\"Return the object's config depending on its type.\"\"\"",
            "    # Functions / lambdas:",
            "    if isinstance(obj, types.FunctionType):",
            "        return object_registration.get_registered_name(obj)",
            "    # All classes:",
            "    if hasattr(obj, \"get_config\"):",
            "        config = obj.get_config()",
            "        if not isinstance(config, dict):",
            "            raise TypeError(",
            "                f\"The `get_config()` method of {obj} should return \"",
            "                f\"a dict. It returned: {config}\"",
            "            )",
            "        return serialize_dict(config)",
            "    elif hasattr(obj, \"__name__\"):",
            "        return object_registration.get_registered_name(obj)",
            "    else:",
            "        raise TypeError(",
            "            f\"Cannot serialize object {obj} of type {type(obj)}. \"",
            "            \"To be serializable, \"",
            "            \"a class must implement the `get_config()` method.\"",
            "        )",
            "",
            "",
            "def serialize_dict(obj):",
            "    return {key: serialize_keras_object(value) for key, value in obj.items()}",
            "",
            "",
            "@keras_export(",
            "    [",
            "        \"keras.saving.deserialize_keras_object\",",
            "        \"keras.utils.deserialize_keras_object\",",
            "    ]",
            ")",
            "def deserialize_keras_object(",
            "    config, custom_objects=None, safe_mode=True, **kwargs",
            "):",
            "    \"\"\"Retrieve the object by deserializing the config dict.",
            "",
            "    The config dict is a Python dictionary that consists of a set of key-value",
            "    pairs, and represents a Keras object, such as an `Optimizer`, `Layer`,",
            "    `Metrics`, etc. The saving and loading library uses the following keys to",
            "    record information of a Keras object:",
            "",
            "    - `class_name`: String. This is the name of the class,",
            "      as exactly defined in the source",
            "      code, such as \"LossesContainer\".",
            "    - `config`: Dict. Library-defined or user-defined key-value pairs that store",
            "      the configuration of the object, as obtained by `object.get_config()`.",
            "    - `module`: String. The path of the python module. Built-in Keras classes",
            "      expect to have prefix `keras`.",
            "    - `registered_name`: String. The key the class is registered under via",
            "      `keras.saving.register_keras_serializable(package, name)` API. The",
            "      key has the format of '{package}>{name}', where `package` and `name` are",
            "      the arguments passed to `register_keras_serializable()`. If `name` is not",
            "      provided, it uses the class name. If `registered_name` successfully",
            "      resolves to a class (that was registered), the `class_name` and `config`",
            "      values in the dict will not be used. `registered_name` is only used for",
            "      non-built-in classes.",
            "",
            "    For example, the following dictionary represents the built-in Adam optimizer",
            "    with the relevant config:",
            "",
            "    ```python",
            "    dict_structure = {",
            "        \"class_name\": \"Adam\",",
            "        \"config\": {",
            "            \"amsgrad\": false,",
            "            \"beta_1\": 0.8999999761581421,",
            "            \"beta_2\": 0.9990000128746033,",
            "            \"decay\": 0.0,",
            "            \"epsilon\": 1e-07,",
            "            \"learning_rate\": 0.0010000000474974513,",
            "            \"name\": \"Adam\"",
            "        },",
            "        \"module\": \"keras.optimizers\",",
            "        \"registered_name\": None",
            "    }",
            "    # Returns an `Adam` instance identical to the original one.",
            "    deserialize_keras_object(dict_structure)",
            "    ```",
            "",
            "    If the class does not have an exported Keras namespace, the library tracks",
            "    it by its `module` and `class_name`. For example:",
            "",
            "    ```python",
            "    dict_structure = {",
            "      \"class_name\": \"MetricsList\",",
            "      \"config\": {",
            "          ...",
            "      },",
            "      \"module\": \"keras.trainers.compile_utils\",",
            "      \"registered_name\": \"MetricsList\"",
            "    }",
            "",
            "    # Returns a `MetricsList` instance identical to the original one.",
            "    deserialize_keras_object(dict_structure)",
            "    ```",
            "",
            "    And the following dictionary represents a user-customized `MeanSquaredError`",
            "    loss:",
            "",
            "    ```python",
            "    @keras.saving.register_keras_serializable(package='my_package')",
            "    class ModifiedMeanSquaredError(keras.losses.MeanSquaredError):",
            "      ...",
            "",
            "    dict_structure = {",
            "        \"class_name\": \"ModifiedMeanSquaredError\",",
            "        \"config\": {",
            "            \"fn\": \"mean_squared_error\",",
            "            \"name\": \"mean_squared_error\",",
            "            \"reduction\": \"auto\"",
            "        },",
            "        \"registered_name\": \"my_package>ModifiedMeanSquaredError\"",
            "    }",
            "    # Returns the `ModifiedMeanSquaredError` object",
            "    deserialize_keras_object(dict_structure)",
            "    ```",
            "",
            "    Args:",
            "        config: Python dict describing the object.",
            "        custom_objects: Python dict containing a mapping between custom",
            "            object names the corresponding classes or functions.",
            "        safe_mode: Boolean, whether to disallow unsafe `lambda` deserialization.",
            "            When `safe_mode=False`, loading an object has the potential to",
            "            trigger arbitrary code execution. This argument is only",
            "            applicable to the Keras v3 model format. Defaults to `True`.",
            "",
            "    Returns:",
            "        The object described by the `config` dictionary.",
            "    \"\"\"",
            "    safe_scope_arg = in_safe_mode()  # Enforces SafeModeScope",
            "    safe_mode = safe_scope_arg if safe_scope_arg is not None else safe_mode",
            "",
            "    module_objects = kwargs.pop(\"module_objects\", None)",
            "    custom_objects = custom_objects or {}",
            "    tlco = global_state.get_global_attribute(\"custom_objects_scope_dict\", {})",
            "    gco = object_registration.GLOBAL_CUSTOM_OBJECTS",
            "    custom_objects = {**custom_objects, **tlco, **gco}",
            "",
            "    if config is None:",
            "        return None",
            "",
            "    if (",
            "        isinstance(config, str)",
            "        and custom_objects",
            "        and custom_objects.get(config) is not None",
            "    ):",
            "        # This is to deserialize plain functions which are serialized as",
            "        # string names by legacy saving formats.",
            "        return custom_objects[config]",
            "",
            "    if isinstance(config, (list, tuple)):",
            "        return [",
            "            deserialize_keras_object(",
            "                x, custom_objects=custom_objects, safe_mode=safe_mode",
            "            )",
            "            for x in config",
            "        ]",
            "",
            "    if module_objects is not None:",
            "        inner_config, fn_module_name, has_custom_object = None, None, False",
            "",
            "        if isinstance(config, dict):",
            "            if \"config\" in config:",
            "                inner_config = config[\"config\"]",
            "            if \"class_name\" not in config:",
            "                raise ValueError(",
            "                    f\"Unknown `config` as a `dict`, config={config}\"",
            "                )",
            "",
            "            # Check case where config is function or class and in custom objects",
            "            if custom_objects and (",
            "                config[\"class_name\"] in custom_objects",
            "                or config.get(\"registered_name\") in custom_objects",
            "                or (",
            "                    isinstance(inner_config, str)",
            "                    and inner_config in custom_objects",
            "                )",
            "            ):",
            "                has_custom_object = True",
            "",
            "            # Case where config is function but not in custom objects",
            "            elif config[\"class_name\"] == \"function\":",
            "                fn_module_name = config[\"module\"]",
            "                if fn_module_name == \"builtins\":",
            "                    config = config[\"config\"]",
            "                else:",
            "                    config = config[\"registered_name\"]",
            "",
            "            # Case where config is class but not in custom objects",
            "            else:",
            "                if config.get(\"module\", \"_\") is None:",
            "                    raise TypeError(",
            "                        \"Cannot deserialize object of type \"",
            "                        f\"`{config['class_name']}`. If \"",
            "                        f\"`{config['class_name']}` is a custom class, please \"",
            "                        \"register it using the \"",
            "                        \"`@keras.saving.register_keras_serializable()` \"",
            "                        \"decorator.\"",
            "                    )",
            "                config = config[\"class_name\"]",
            "",
            "        if not has_custom_object:",
            "            # Return if not found in either module objects or custom objects",
            "            if config not in module_objects:",
            "                # Object has already been deserialized",
            "                return config",
            "            if isinstance(module_objects[config], types.FunctionType):",
            "                return deserialize_keras_object(",
            "                    serialize_with_public_fn(",
            "                        module_objects[config], config, fn_module_name",
            "                    ),",
            "                    custom_objects=custom_objects,",
            "                )",
            "            return deserialize_keras_object(",
            "                serialize_with_public_class(",
            "                    module_objects[config], inner_config=inner_config",
            "                ),",
            "                custom_objects=custom_objects,",
            "            )",
            "",
            "    if isinstance(config, PLAIN_TYPES):",
            "        return config",
            "    if not isinstance(config, dict):",
            "        raise TypeError(f\"Could not parse config: {config}\")",
            "",
            "    if \"class_name\" not in config or \"config\" not in config:",
            "        return {",
            "            key: deserialize_keras_object(",
            "                value, custom_objects=custom_objects, safe_mode=safe_mode",
            "            )",
            "            for key, value in config.items()",
            "        }",
            "",
            "    class_name = config[\"class_name\"]",
            "    inner_config = config[\"config\"] or {}",
            "    custom_objects = custom_objects or {}",
            "",
            "    # Special cases:",
            "    if class_name == \"__keras_tensor__\":",
            "        obj = backend.KerasTensor(",
            "            inner_config[\"shape\"], dtype=inner_config[\"dtype\"]",
            "        )",
            "        obj._pre_serialization_keras_history = inner_config[\"keras_history\"]",
            "        return obj",
            "",
            "    if class_name == \"__tensor__\":",
            "        return backend.convert_to_tensor(",
            "            inner_config[\"value\"], dtype=inner_config[\"dtype\"]",
            "        )",
            "    if class_name == \"__numpy__\":",
            "        return np.array(inner_config[\"value\"], dtype=inner_config[\"dtype\"])",
            "    if config[\"class_name\"] == \"__bytes__\":",
            "        return inner_config[\"value\"].encode(\"utf-8\")",
            "    if config[\"class_name\"] == \"__ellipsis__\":",
            "        return Ellipsis",
            "    if config[\"class_name\"] == \"__slice__\":",
            "        return slice(",
            "            deserialize_keras_object(",
            "                inner_config[\"start\"],",
            "                custom_objects=custom_objects,",
            "                safe_mode=safe_mode,",
            "            ),",
            "            deserialize_keras_object(",
            "                inner_config[\"stop\"],",
            "                custom_objects=custom_objects,",
            "                safe_mode=safe_mode,",
            "            ),",
            "            deserialize_keras_object(",
            "                inner_config[\"step\"],",
            "                custom_objects=custom_objects,",
            "                safe_mode=safe_mode,",
            "            ),",
            "        )",
            "    if config[\"class_name\"] == \"__lambda__\":",
            "        if safe_mode:",
            "            raise ValueError(",
            "                \"Requested the deserialization of a `lambda` object. \"",
            "                \"This carries a potential risk of arbitrary code execution \"",
            "                \"and thus it is disallowed by default. If you trust the \"",
            "                \"source of the saved model, you can pass `safe_mode=False` to \"",
            "                \"the loading function in order to allow `lambda` loading, \"",
            "                \"or call `keras.config.enable_unsafe_deserialization()`.\"",
            "            )",
            "        return python_utils.func_load(inner_config[\"value\"])",
            "    if tf is not None and config[\"class_name\"] == \"__typespec__\":",
            "        obj = _retrieve_class_or_fn(",
            "            config[\"spec_name\"],",
            "            config[\"registered_name\"],",
            "            config[\"module\"],",
            "            obj_type=\"class\",",
            "            full_config=config,",
            "            custom_objects=custom_objects,",
            "        )",
            "        # Conversion to TensorShape and DType",
            "        inner_config = map(",
            "            lambda x: (",
            "                tf.TensorShape(x)",
            "                if isinstance(x, list)",
            "                else (getattr(tf, x) if hasattr(tf.dtypes, str(x)) else x)",
            "            ),",
            "            inner_config,",
            "        )",
            "        return obj._deserialize(tuple(inner_config))",
            "",
            "    # Below: classes and functions.",
            "    module = config.get(\"module\", None)",
            "    registered_name = config.get(\"registered_name\", class_name)",
            "",
            "    if class_name == \"function\":",
            "        fn_name = inner_config",
            "        return _retrieve_class_or_fn(",
            "            fn_name,",
            "            registered_name,",
            "            module,",
            "            obj_type=\"function\",",
            "            full_config=config,",
            "            custom_objects=custom_objects,",
            "        )",
            "",
            "    # Below, handling of all classes.",
            "    # First, is it a shared object?",
            "    if \"shared_object_id\" in config:",
            "        obj = get_shared_object(config[\"shared_object_id\"])",
            "        if obj is not None:",
            "            return obj",
            "",
            "    cls = _retrieve_class_or_fn(",
            "        class_name,",
            "        registered_name,",
            "        module,",
            "        obj_type=\"class\",",
            "        full_config=config,",
            "        custom_objects=custom_objects,",
            "    )",
            "",
            "    if isinstance(cls, types.FunctionType):",
            "        return cls",
            "    if not hasattr(cls, \"from_config\"):",
            "        raise TypeError(",
            "            f\"Unable to reconstruct an instance of '{class_name}' because \"",
            "            f\"the class is missing a `from_config()` method. \"",
            "            f\"Full object config: {config}\"",
            "        )",
            "",
            "    # Instantiate the class from its config inside a custom object scope",
            "    # so that we can catch any custom objects that the config refers to.",
            "    custom_obj_scope = object_registration.CustomObjectScope(custom_objects)",
            "    safe_mode_scope = SafeModeScope(safe_mode)",
            "    with custom_obj_scope, safe_mode_scope:",
            "        try:",
            "            instance = cls.from_config(inner_config)",
            "        except TypeError as e:",
            "            raise TypeError(",
            "                f\"{cls} could not be deserialized properly. Please\"",
            "                \" ensure that components that are Python object\"",
            "                \" instances (layers, models, etc.) returned by\"",
            "                \" `get_config()` are explicitly deserialized in the\"",
            "                \" model's `from_config()` method.\"",
            "                f\"\\n\\nconfig={config}.\\n\\nException encountered: {e}\"",
            "            )",
            "        build_config = config.get(\"build_config\", None)",
            "        if build_config and not instance.built:",
            "            instance.build_from_config(build_config)",
            "            instance.built = True",
            "        compile_config = config.get(\"compile_config\", None)",
            "        if compile_config:",
            "            instance.compile_from_config(compile_config)",
            "            instance.compiled = True",
            "",
            "    if \"shared_object_id\" in config:",
            "        record_object_after_deserialization(",
            "            instance, config[\"shared_object_id\"]",
            "        )",
            "    return instance",
            "",
            "",
            "def _retrieve_class_or_fn(",
            "    name, registered_name, module, obj_type, full_config, custom_objects=None",
            "):",
            "    # If there is a custom object registered via",
            "    # `register_keras_serializable()`, that takes precedence.",
            "    if obj_type == \"function\":",
            "        custom_obj = object_registration.get_registered_object(",
            "            name, custom_objects=custom_objects",
            "        )",
            "    else:",
            "        custom_obj = object_registration.get_registered_object(",
            "            registered_name, custom_objects=custom_objects",
            "        )",
            "    if custom_obj is not None:",
            "        return custom_obj",
            "",
            "    if module:",
            "        # If it's a Keras built-in object,",
            "        # we cannot always use direct import, because the exported",
            "        # module name might not match the package structure",
            "        # (e.g. experimental symbols).",
            "        if module == \"keras\" or module.startswith(\"keras.\"):",
            "            api_name = module + \".\" + name",
            "",
            "            obj = api_export.get_symbol_from_name(api_name)",
            "            if obj is not None:",
            "                return obj",
            "",
            "        # Configs of Keras built-in functions do not contain identifying",
            "        # information other than their name (e.g. 'acc' or 'tanh'). This special",
            "        # case searches the Keras modules that contain built-ins to retrieve",
            "        # the corresponding function from the identifying string.",
            "        if obj_type == \"function\" and module == \"builtins\":",
            "            for mod in BUILTIN_MODULES:",
            "                obj = api_export.get_symbol_from_name(",
            "                    \"keras.\" + mod + \".\" + name",
            "                )",
            "                if obj is not None:",
            "                    return obj",
            "",
            "        # Otherwise, attempt to retrieve the class object given the `module`",
            "        # and `class_name`. Import the module, find the class.",
            "        try:",
            "            mod = importlib.import_module(module)",
            "        except ModuleNotFoundError:",
            "            raise TypeError(",
            "                f\"Could not deserialize {obj_type} '{name}' because \"",
            "                f\"its parent module {module} cannot be imported. \"",
            "                f\"Full object config: {full_config}\"",
            "            )",
            "        obj = vars(mod).get(name, None)",
            "",
            "        # Special case for keras.metrics.metrics",
            "        if obj is None and registered_name is not None:",
            "            obj = vars(mod).get(registered_name, None)",
            "",
            "        if obj is not None:",
            "            return obj",
            "",
            "    raise TypeError(",
            "        f\"Could not locate {obj_type} '{name}'. \"",
            "        \"Make sure custom classes are decorated with \"",
            "        \"`@keras.saving.register_keras_serializable()`. \"",
            "        f\"Full object config: {full_config}\"",
            "    )"
        ],
        "afterPatchFile": [
            "\"\"\"Object config serialization and deserialization logic.\"\"\"",
            "",
            "import importlib",
            "import inspect",
            "import types",
            "import warnings",
            "",
            "import numpy as np",
            "",
            "from keras.src import api_export",
            "from keras.src import backend",
            "from keras.src.api_export import keras_export",
            "from keras.src.backend.common import global_state",
            "from keras.src.saving import object_registration",
            "from keras.src.utils import python_utils",
            "from keras.src.utils.module_utils import tensorflow as tf",
            "",
            "PLAIN_TYPES = (str, int, float, bool)",
            "",
            "# List of Keras modules with built-in string representations for Keras defaults",
            "BUILTIN_MODULES = (",
            "    \"activations\",",
            "    \"constraints\",",
            "    \"initializers\",",
            "    \"losses\",",
            "    \"metrics\",",
            "    \"optimizers\",",
            "    \"regularizers\",",
            ")",
            "",
            "",
            "class SerializableDict:",
            "    def __init__(self, **config):",
            "        self.config = config",
            "",
            "    def serialize(self):",
            "        return serialize_keras_object(self.config)",
            "",
            "",
            "class SafeModeScope:",
            "    \"\"\"Scope to propagate safe mode flag to nested deserialization calls.\"\"\"",
            "",
            "    def __init__(self, safe_mode=True):",
            "        self.safe_mode = safe_mode",
            "",
            "    def __enter__(self):",
            "        self.original_value = in_safe_mode()",
            "        global_state.set_global_attribute(\"safe_mode_saving\", self.safe_mode)",
            "",
            "    def __exit__(self, *args, **kwargs):",
            "        global_state.set_global_attribute(",
            "            \"safe_mode_saving\", self.original_value",
            "        )",
            "",
            "",
            "@keras_export(\"keras.config.enable_unsafe_deserialization\")",
            "def enable_unsafe_deserialization():",
            "    \"\"\"Disables safe mode globally, allowing deserialization of lambdas.\"\"\"",
            "    global_state.set_global_attribute(\"safe_mode_saving\", False)",
            "",
            "",
            "def in_safe_mode():",
            "    return global_state.get_global_attribute(\"safe_mode_saving\")",
            "",
            "",
            "class ObjectSharingScope:",
            "    \"\"\"Scope to enable detection and reuse of previously seen objects.\"\"\"",
            "",
            "    def __enter__(self):",
            "        global_state.set_global_attribute(\"shared_objects/id_to_obj_map\", {})",
            "        global_state.set_global_attribute(\"shared_objects/id_to_config_map\", {})",
            "",
            "    def __exit__(self, *args, **kwargs):",
            "        global_state.set_global_attribute(\"shared_objects/id_to_obj_map\", None)",
            "        global_state.set_global_attribute(",
            "            \"shared_objects/id_to_config_map\", None",
            "        )",
            "",
            "",
            "def get_shared_object(obj_id):",
            "    \"\"\"Retrieve an object previously seen during deserialization.\"\"\"",
            "    id_to_obj_map = global_state.get_global_attribute(",
            "        \"shared_objects/id_to_obj_map\"",
            "    )",
            "    if id_to_obj_map is not None:",
            "        return id_to_obj_map.get(obj_id, None)",
            "",
            "",
            "def record_object_after_serialization(obj, config):",
            "    \"\"\"Call after serializing an object, to keep track of its config.\"\"\"",
            "    if config[\"module\"] == \"__main__\":",
            "        config[\"module\"] = None  # Ensures module is None when no module found",
            "    id_to_config_map = global_state.get_global_attribute(",
            "        \"shared_objects/id_to_config_map\"",
            "    )",
            "    if id_to_config_map is None:",
            "        return  # Not in a sharing scope",
            "    obj_id = int(id(obj))",
            "    if obj_id not in id_to_config_map:",
            "        id_to_config_map[obj_id] = config",
            "    else:",
            "        config[\"shared_object_id\"] = obj_id",
            "        prev_config = id_to_config_map[obj_id]",
            "        prev_config[\"shared_object_id\"] = obj_id",
            "",
            "",
            "def record_object_after_deserialization(obj, obj_id):",
            "    \"\"\"Call after deserializing an object, to keep track of it in the future.\"\"\"",
            "    id_to_obj_map = global_state.get_global_attribute(",
            "        \"shared_objects/id_to_obj_map\"",
            "    )",
            "    if id_to_obj_map is None:",
            "        return  # Not in a sharing scope",
            "    id_to_obj_map[obj_id] = obj",
            "",
            "",
            "@keras_export(",
            "    [",
            "        \"keras.saving.serialize_keras_object\",",
            "        \"keras.utils.serialize_keras_object\",",
            "    ]",
            ")",
            "def serialize_keras_object(obj):",
            "    \"\"\"Retrieve the config dict by serializing the Keras object.",
            "",
            "    `serialize_keras_object()` serializes a Keras object to a python dictionary",
            "    that represents the object, and is a reciprocal function of",
            "    `deserialize_keras_object()`. See `deserialize_keras_object()` for more",
            "    information about the config format.",
            "",
            "    Args:",
            "        obj: the Keras object to serialize.",
            "",
            "    Returns:",
            "        A python dict that represents the object. The python dict can be",
            "        deserialized via `deserialize_keras_object()`.",
            "    \"\"\"",
            "    if obj is None:",
            "        return obj",
            "",
            "    if isinstance(obj, PLAIN_TYPES):",
            "        return obj",
            "",
            "    if isinstance(obj, (list, tuple)):",
            "        config_arr = [serialize_keras_object(x) for x in obj]",
            "        return tuple(config_arr) if isinstance(obj, tuple) else config_arr",
            "    if isinstance(obj, dict):",
            "        return serialize_dict(obj)",
            "",
            "    # Special cases:",
            "    if isinstance(obj, bytes):",
            "        return {",
            "            \"class_name\": \"__bytes__\",",
            "            \"config\": {\"value\": obj.decode(\"utf-8\")},",
            "        }",
            "    if isinstance(obj, slice):",
            "        return {",
            "            \"class_name\": \"__slice__\",",
            "            \"config\": {",
            "                \"start\": serialize_keras_object(obj.start),",
            "                \"stop\": serialize_keras_object(obj.stop),",
            "                \"step\": serialize_keras_object(obj.step),",
            "            },",
            "        }",
            "    # Ellipsis is an instance, and ellipsis class is not in global scope.",
            "    # checking equality also fails elsewhere in the library, so we have",
            "    # to dynamically get the type.",
            "    if isinstance(obj, type(Ellipsis)):",
            "        return {\"class_name\": \"__ellipsis__\", \"config\": {}}",
            "    if isinstance(obj, backend.KerasTensor):",
            "        history = getattr(obj, \"_keras_history\", None)",
            "        if history:",
            "            history = list(history)",
            "            history[0] = history[0].name",
            "        return {",
            "            \"class_name\": \"__keras_tensor__\",",
            "            \"config\": {",
            "                \"shape\": obj.shape,",
            "                \"dtype\": obj.dtype,",
            "                \"keras_history\": history,",
            "            },",
            "        }",
            "    if tf.available and isinstance(obj, tf.TensorShape):",
            "        return obj.as_list() if obj._dims is not None else None",
            "    if backend.is_tensor(obj):",
            "        return {",
            "            \"class_name\": \"__tensor__\",",
            "            \"config\": {",
            "                \"value\": backend.convert_to_numpy(obj).tolist(),",
            "                \"dtype\": backend.standardize_dtype(obj.dtype),",
            "            },",
            "        }",
            "    if type(obj).__module__ == np.__name__:",
            "        if isinstance(obj, np.ndarray) and obj.ndim > 0:",
            "            return {",
            "                \"class_name\": \"__numpy__\",",
            "                \"config\": {",
            "                    \"value\": obj.tolist(),",
            "                    \"dtype\": backend.standardize_dtype(obj.dtype),",
            "                },",
            "            }",
            "        else:",
            "            # Treat numpy floats / etc as plain types.",
            "            return obj.item()",
            "    if tf.available and isinstance(obj, tf.DType):",
            "        return obj.name",
            "    if isinstance(obj, types.FunctionType) and obj.__name__ == \"<lambda>\":",
            "        warnings.warn(",
            "            \"The object being serialized includes a `lambda`. This is unsafe. \"",
            "            \"In order to reload the object, you will have to pass \"",
            "            \"`safe_mode=False` to the loading function. \"",
            "            \"Please avoid using `lambda` in the \"",
            "            \"future, and use named Python functions instead. \"",
            "            f\"This is the `lambda` being serialized: {inspect.getsource(obj)}\",",
            "            stacklevel=2,",
            "        )",
            "        return {",
            "            \"class_name\": \"__lambda__\",",
            "            \"config\": {",
            "                \"value\": python_utils.func_dump(obj),",
            "            },",
            "        }",
            "    if tf.available and isinstance(obj, tf.TypeSpec):",
            "        ts_config = obj._serialize()",
            "        # TensorShape and tf.DType conversion",
            "        ts_config = list(",
            "            map(",
            "                lambda x: (",
            "                    x.as_list()",
            "                    if isinstance(x, tf.TensorShape)",
            "                    else (x.name if isinstance(x, tf.DType) else x)",
            "                ),",
            "                ts_config,",
            "            )",
            "        )",
            "        return {",
            "            \"class_name\": \"__typespec__\",",
            "            \"spec_name\": obj.__class__.__name__,",
            "            \"module\": obj.__class__.__module__,",
            "            \"config\": ts_config,",
            "            \"registered_name\": None,",
            "        }",
            "",
            "    inner_config = _get_class_or_fn_config(obj)",
            "    config_with_public_class = serialize_with_public_class(",
            "        obj.__class__, inner_config",
            "    )",
            "",
            "    if config_with_public_class is not None:",
            "        get_build_and_compile_config(obj, config_with_public_class)",
            "        record_object_after_serialization(obj, config_with_public_class)",
            "        return config_with_public_class",
            "",
            "    # Any custom object or otherwise non-exported object",
            "    if isinstance(obj, types.FunctionType):",
            "        module = obj.__module__",
            "    else:",
            "        module = obj.__class__.__module__",
            "    class_name = obj.__class__.__name__",
            "",
            "    if module == \"builtins\":",
            "        registered_name = None",
            "    else:",
            "        if isinstance(obj, types.FunctionType):",
            "            registered_name = object_registration.get_registered_name(obj)",
            "        else:",
            "            registered_name = object_registration.get_registered_name(",
            "                obj.__class__",
            "            )",
            "",
            "    config = {",
            "        \"module\": module,",
            "        \"class_name\": class_name,",
            "        \"config\": inner_config,",
            "        \"registered_name\": registered_name,",
            "    }",
            "    get_build_and_compile_config(obj, config)",
            "    record_object_after_serialization(obj, config)",
            "    return config",
            "",
            "",
            "def get_build_and_compile_config(obj, config):",
            "    if hasattr(obj, \"get_build_config\"):",
            "        build_config = obj.get_build_config()",
            "        if build_config is not None:",
            "            config[\"build_config\"] = serialize_dict(build_config)",
            "    if hasattr(obj, \"get_compile_config\"):",
            "        compile_config = obj.get_compile_config()",
            "        if compile_config is not None:",
            "            config[\"compile_config\"] = serialize_dict(compile_config)",
            "    return",
            "",
            "",
            "def serialize_with_public_class(cls, inner_config=None):",
            "    \"\"\"Serializes classes from public Keras API or object registration.",
            "",
            "    Called to check and retrieve the config of any class that has a public",
            "    Keras API or has been registered as serializable via",
            "    `keras.saving.register_keras_serializable()`.",
            "    \"\"\"",
            "    # This gets the `keras.*` exported name, such as",
            "    # \"keras.optimizers.Adam\".",
            "    keras_api_name = api_export.get_name_from_symbol(cls)",
            "",
            "    # Case of custom or unknown class object",
            "    if keras_api_name is None:",
            "        registered_name = object_registration.get_registered_name(cls)",
            "        if registered_name is None:",
            "            return None",
            "",
            "        # Return custom object config with corresponding registration name",
            "        return {",
            "            \"module\": cls.__module__,",
            "            \"class_name\": cls.__name__,",
            "            \"config\": inner_config,",
            "            \"registered_name\": registered_name,",
            "        }",
            "",
            "    # Split the canonical Keras API name into a Keras module and class name.",
            "    parts = keras_api_name.split(\".\")",
            "    return {",
            "        \"module\": \".\".join(parts[:-1]),",
            "        \"class_name\": parts[-1],",
            "        \"config\": inner_config,",
            "        \"registered_name\": None,",
            "    }",
            "",
            "",
            "def serialize_with_public_fn(fn, config, fn_module_name=None):",
            "    \"\"\"Serializes functions from public Keras API or object registration.",
            "",
            "    Called to check and retrieve the config of any function that has a public",
            "    Keras API or has been registered as serializable via",
            "    `keras.saving.register_keras_serializable()`. If function's module name",
            "    is already known, returns corresponding config.",
            "    \"\"\"",
            "    if fn_module_name:",
            "        return {",
            "            \"module\": fn_module_name,",
            "            \"class_name\": \"function\",",
            "            \"config\": config,",
            "            \"registered_name\": config,",
            "        }",
            "    keras_api_name = api_export.get_name_from_symbol(fn)",
            "    if keras_api_name:",
            "        parts = keras_api_name.split(\".\")",
            "        return {",
            "            \"module\": \".\".join(parts[:-1]),",
            "            \"class_name\": \"function\",",
            "            \"config\": config,",
            "            \"registered_name\": config,",
            "        }",
            "    else:",
            "        registered_name = object_registration.get_registered_name(fn)",
            "        if not registered_name and not fn.__module__ == \"builtins\":",
            "            return None",
            "        return {",
            "            \"module\": fn.__module__,",
            "            \"class_name\": \"function\",",
            "            \"config\": config,",
            "            \"registered_name\": registered_name,",
            "        }",
            "",
            "",
            "def _get_class_or_fn_config(obj):",
            "    \"\"\"Return the object's config depending on its type.\"\"\"",
            "    # Functions / lambdas:",
            "    if isinstance(obj, types.FunctionType):",
            "        return object_registration.get_registered_name(obj)",
            "    # All classes:",
            "    if hasattr(obj, \"get_config\"):",
            "        config = obj.get_config()",
            "        if not isinstance(config, dict):",
            "            raise TypeError(",
            "                f\"The `get_config()` method of {obj} should return \"",
            "                f\"a dict. It returned: {config}\"",
            "            )",
            "        return serialize_dict(config)",
            "    elif hasattr(obj, \"__name__\"):",
            "        return object_registration.get_registered_name(obj)",
            "    else:",
            "        raise TypeError(",
            "            f\"Cannot serialize object {obj} of type {type(obj)}. \"",
            "            \"To be serializable, \"",
            "            \"a class must implement the `get_config()` method.\"",
            "        )",
            "",
            "",
            "def serialize_dict(obj):",
            "    return {key: serialize_keras_object(value) for key, value in obj.items()}",
            "",
            "",
            "@keras_export(",
            "    [",
            "        \"keras.saving.deserialize_keras_object\",",
            "        \"keras.utils.deserialize_keras_object\",",
            "    ]",
            ")",
            "def deserialize_keras_object(",
            "    config, custom_objects=None, safe_mode=True, **kwargs",
            "):",
            "    \"\"\"Retrieve the object by deserializing the config dict.",
            "",
            "    The config dict is a Python dictionary that consists of a set of key-value",
            "    pairs, and represents a Keras object, such as an `Optimizer`, `Layer`,",
            "    `Metrics`, etc. The saving and loading library uses the following keys to",
            "    record information of a Keras object:",
            "",
            "    - `class_name`: String. This is the name of the class,",
            "      as exactly defined in the source",
            "      code, such as \"LossesContainer\".",
            "    - `config`: Dict. Library-defined or user-defined key-value pairs that store",
            "      the configuration of the object, as obtained by `object.get_config()`.",
            "    - `module`: String. The path of the python module. Built-in Keras classes",
            "      expect to have prefix `keras`.",
            "    - `registered_name`: String. The key the class is registered under via",
            "      `keras.saving.register_keras_serializable(package, name)` API. The",
            "      key has the format of '{package}>{name}', where `package` and `name` are",
            "      the arguments passed to `register_keras_serializable()`. If `name` is not",
            "      provided, it uses the class name. If `registered_name` successfully",
            "      resolves to a class (that was registered), the `class_name` and `config`",
            "      values in the dict will not be used. `registered_name` is only used for",
            "      non-built-in classes.",
            "",
            "    For example, the following dictionary represents the built-in Adam optimizer",
            "    with the relevant config:",
            "",
            "    ```python",
            "    dict_structure = {",
            "        \"class_name\": \"Adam\",",
            "        \"config\": {",
            "            \"amsgrad\": false,",
            "            \"beta_1\": 0.8999999761581421,",
            "            \"beta_2\": 0.9990000128746033,",
            "            \"decay\": 0.0,",
            "            \"epsilon\": 1e-07,",
            "            \"learning_rate\": 0.0010000000474974513,",
            "            \"name\": \"Adam\"",
            "        },",
            "        \"module\": \"keras.optimizers\",",
            "        \"registered_name\": None",
            "    }",
            "    # Returns an `Adam` instance identical to the original one.",
            "    deserialize_keras_object(dict_structure)",
            "    ```",
            "",
            "    If the class does not have an exported Keras namespace, the library tracks",
            "    it by its `module` and `class_name`. For example:",
            "",
            "    ```python",
            "    dict_structure = {",
            "      \"class_name\": \"MetricsList\",",
            "      \"config\": {",
            "          ...",
            "      },",
            "      \"module\": \"keras.trainers.compile_utils\",",
            "      \"registered_name\": \"MetricsList\"",
            "    }",
            "",
            "    # Returns a `MetricsList` instance identical to the original one.",
            "    deserialize_keras_object(dict_structure)",
            "    ```",
            "",
            "    And the following dictionary represents a user-customized `MeanSquaredError`",
            "    loss:",
            "",
            "    ```python",
            "    @keras.saving.register_keras_serializable(package='my_package')",
            "    class ModifiedMeanSquaredError(keras.losses.MeanSquaredError):",
            "      ...",
            "",
            "    dict_structure = {",
            "        \"class_name\": \"ModifiedMeanSquaredError\",",
            "        \"config\": {",
            "            \"fn\": \"mean_squared_error\",",
            "            \"name\": \"mean_squared_error\",",
            "            \"reduction\": \"auto\"",
            "        },",
            "        \"registered_name\": \"my_package>ModifiedMeanSquaredError\"",
            "    }",
            "    # Returns the `ModifiedMeanSquaredError` object",
            "    deserialize_keras_object(dict_structure)",
            "    ```",
            "",
            "    Args:",
            "        config: Python dict describing the object.",
            "        custom_objects: Python dict containing a mapping between custom",
            "            object names the corresponding classes or functions.",
            "        safe_mode: Boolean, whether to disallow unsafe `lambda` deserialization.",
            "            When `safe_mode=False`, loading an object has the potential to",
            "            trigger arbitrary code execution. This argument is only",
            "            applicable to the Keras v3 model format. Defaults to `True`.",
            "",
            "    Returns:",
            "        The object described by the `config` dictionary.",
            "    \"\"\"",
            "    safe_scope_arg = in_safe_mode()  # Enforces SafeModeScope",
            "    safe_mode = safe_scope_arg if safe_scope_arg is not None else safe_mode",
            "",
            "    module_objects = kwargs.pop(\"module_objects\", None)",
            "    custom_objects = custom_objects or {}",
            "    tlco = global_state.get_global_attribute(\"custom_objects_scope_dict\", {})",
            "    gco = object_registration.GLOBAL_CUSTOM_OBJECTS",
            "    custom_objects = {**custom_objects, **tlco, **gco}",
            "",
            "    if config is None:",
            "        return None",
            "",
            "    if (",
            "        isinstance(config, str)",
            "        and custom_objects",
            "        and custom_objects.get(config) is not None",
            "    ):",
            "        # This is to deserialize plain functions which are serialized as",
            "        # string names by legacy saving formats.",
            "        return custom_objects[config]",
            "",
            "    if isinstance(config, (list, tuple)):",
            "        return [",
            "            deserialize_keras_object(",
            "                x, custom_objects=custom_objects, safe_mode=safe_mode",
            "            )",
            "            for x in config",
            "        ]",
            "",
            "    if module_objects is not None:",
            "        inner_config, fn_module_name, has_custom_object = None, None, False",
            "",
            "        if isinstance(config, dict):",
            "            if \"config\" in config:",
            "                inner_config = config[\"config\"]",
            "            if \"class_name\" not in config:",
            "                raise ValueError(",
            "                    f\"Unknown `config` as a `dict`, config={config}\"",
            "                )",
            "",
            "            # Check case where config is function or class and in custom objects",
            "            if custom_objects and (",
            "                config[\"class_name\"] in custom_objects",
            "                or config.get(\"registered_name\") in custom_objects",
            "                or (",
            "                    isinstance(inner_config, str)",
            "                    and inner_config in custom_objects",
            "                )",
            "            ):",
            "                has_custom_object = True",
            "",
            "            # Case where config is function but not in custom objects",
            "            elif config[\"class_name\"] == \"function\":",
            "                fn_module_name = config[\"module\"]",
            "                if fn_module_name == \"builtins\":",
            "                    config = config[\"config\"]",
            "                else:",
            "                    config = config[\"registered_name\"]",
            "",
            "            # Case where config is class but not in custom objects",
            "            else:",
            "                if config.get(\"module\", \"_\") is None:",
            "                    raise TypeError(",
            "                        \"Cannot deserialize object of type \"",
            "                        f\"`{config['class_name']}`. If \"",
            "                        f\"`{config['class_name']}` is a custom class, please \"",
            "                        \"register it using the \"",
            "                        \"`@keras.saving.register_keras_serializable()` \"",
            "                        \"decorator.\"",
            "                    )",
            "                config = config[\"class_name\"]",
            "",
            "        if not has_custom_object:",
            "            # Return if not found in either module objects or custom objects",
            "            if config not in module_objects:",
            "                # Object has already been deserialized",
            "                return config",
            "            if isinstance(module_objects[config], types.FunctionType):",
            "                return deserialize_keras_object(",
            "                    serialize_with_public_fn(",
            "                        module_objects[config], config, fn_module_name",
            "                    ),",
            "                    custom_objects=custom_objects,",
            "                )",
            "            return deserialize_keras_object(",
            "                serialize_with_public_class(",
            "                    module_objects[config], inner_config=inner_config",
            "                ),",
            "                custom_objects=custom_objects,",
            "            )",
            "",
            "    if isinstance(config, PLAIN_TYPES):",
            "        return config",
            "    if not isinstance(config, dict):",
            "        raise TypeError(f\"Could not parse config: {config}\")",
            "",
            "    if \"class_name\" not in config or \"config\" not in config:",
            "        return {",
            "            key: deserialize_keras_object(",
            "                value, custom_objects=custom_objects, safe_mode=safe_mode",
            "            )",
            "            for key, value in config.items()",
            "        }",
            "",
            "    class_name = config[\"class_name\"]",
            "    inner_config = config[\"config\"] or {}",
            "    custom_objects = custom_objects or {}",
            "",
            "    # Special cases:",
            "    if class_name == \"__keras_tensor__\":",
            "        obj = backend.KerasTensor(",
            "            inner_config[\"shape\"], dtype=inner_config[\"dtype\"]",
            "        )",
            "        obj._pre_serialization_keras_history = inner_config[\"keras_history\"]",
            "        return obj",
            "",
            "    if class_name == \"__tensor__\":",
            "        return backend.convert_to_tensor(",
            "            inner_config[\"value\"], dtype=inner_config[\"dtype\"]",
            "        )",
            "    if class_name == \"__numpy__\":",
            "        return np.array(inner_config[\"value\"], dtype=inner_config[\"dtype\"])",
            "    if config[\"class_name\"] == \"__bytes__\":",
            "        return inner_config[\"value\"].encode(\"utf-8\")",
            "    if config[\"class_name\"] == \"__ellipsis__\":",
            "        return Ellipsis",
            "    if config[\"class_name\"] == \"__slice__\":",
            "        return slice(",
            "            deserialize_keras_object(",
            "                inner_config[\"start\"],",
            "                custom_objects=custom_objects,",
            "                safe_mode=safe_mode,",
            "            ),",
            "            deserialize_keras_object(",
            "                inner_config[\"stop\"],",
            "                custom_objects=custom_objects,",
            "                safe_mode=safe_mode,",
            "            ),",
            "            deserialize_keras_object(",
            "                inner_config[\"step\"],",
            "                custom_objects=custom_objects,",
            "                safe_mode=safe_mode,",
            "            ),",
            "        )",
            "    if config[\"class_name\"] == \"__lambda__\":",
            "        if safe_mode:",
            "            raise ValueError(",
            "                \"Requested the deserialization of a `lambda` object. \"",
            "                \"This carries a potential risk of arbitrary code execution \"",
            "                \"and thus it is disallowed by default. If you trust the \"",
            "                \"source of the saved model, you can pass `safe_mode=False` to \"",
            "                \"the loading function in order to allow `lambda` loading, \"",
            "                \"or call `keras.config.enable_unsafe_deserialization()`.\"",
            "            )",
            "        return python_utils.func_load(inner_config[\"value\"])",
            "    if tf is not None and config[\"class_name\"] == \"__typespec__\":",
            "        obj = _retrieve_class_or_fn(",
            "            config[\"spec_name\"],",
            "            config[\"registered_name\"],",
            "            config[\"module\"],",
            "            obj_type=\"class\",",
            "            full_config=config,",
            "            custom_objects=custom_objects,",
            "        )",
            "        # Conversion to TensorShape and DType",
            "        inner_config = map(",
            "            lambda x: (",
            "                tf.TensorShape(x)",
            "                if isinstance(x, list)",
            "                else (getattr(tf, x) if hasattr(tf.dtypes, str(x)) else x)",
            "            ),",
            "            inner_config,",
            "        )",
            "        return obj._deserialize(tuple(inner_config))",
            "",
            "    # Below: classes and functions.",
            "    module = config.get(\"module\", None)",
            "    registered_name = config.get(\"registered_name\", class_name)",
            "",
            "    if class_name == \"function\":",
            "        fn_name = inner_config",
            "        return _retrieve_class_or_fn(",
            "            fn_name,",
            "            registered_name,",
            "            module,",
            "            obj_type=\"function\",",
            "            full_config=config,",
            "            custom_objects=custom_objects,",
            "        )",
            "",
            "    # Below, handling of all classes.",
            "    # First, is it a shared object?",
            "    if \"shared_object_id\" in config:",
            "        obj = get_shared_object(config[\"shared_object_id\"])",
            "        if obj is not None:",
            "            return obj",
            "",
            "    cls = _retrieve_class_or_fn(",
            "        class_name,",
            "        registered_name,",
            "        module,",
            "        obj_type=\"class\",",
            "        full_config=config,",
            "        custom_objects=custom_objects,",
            "    )",
            "",
            "    if isinstance(cls, types.FunctionType):",
            "        return cls",
            "    if not hasattr(cls, \"from_config\"):",
            "        raise TypeError(",
            "            f\"Unable to reconstruct an instance of '{class_name}' because \"",
            "            f\"the class is missing a `from_config()` method. \"",
            "            f\"Full object config: {config}\"",
            "        )",
            "",
            "    # Instantiate the class from its config inside a custom object scope",
            "    # so that we can catch any custom objects that the config refers to.",
            "    custom_obj_scope = object_registration.CustomObjectScope(custom_objects)",
            "    safe_mode_scope = SafeModeScope(safe_mode)",
            "    with custom_obj_scope, safe_mode_scope:",
            "        try:",
            "            instance = cls.from_config(inner_config)",
            "        except TypeError as e:",
            "            raise TypeError(",
            "                f\"{cls} could not be deserialized properly. Please\"",
            "                \" ensure that components that are Python object\"",
            "                \" instances (layers, models, etc.) returned by\"",
            "                \" `get_config()` are explicitly deserialized in the\"",
            "                \" model's `from_config()` method.\"",
            "                f\"\\n\\nconfig={config}.\\n\\nException encountered: {e}\"",
            "            )",
            "        build_config = config.get(\"build_config\", None)",
            "        if build_config and not instance.built:",
            "            instance.build_from_config(build_config)",
            "            instance.built = True",
            "        compile_config = config.get(\"compile_config\", None)",
            "        if compile_config:",
            "            instance.compile_from_config(compile_config)",
            "            instance.compiled = True",
            "",
            "    if \"shared_object_id\" in config:",
            "        record_object_after_deserialization(",
            "            instance, config[\"shared_object_id\"]",
            "        )",
            "    return instance",
            "",
            "",
            "def _retrieve_class_or_fn(",
            "    name, registered_name, module, obj_type, full_config, custom_objects=None",
            "):",
            "    # If there is a custom object registered via",
            "    # `register_keras_serializable()`, that takes precedence.",
            "    if obj_type == \"function\":",
            "        custom_obj = object_registration.get_registered_object(",
            "            name, custom_objects=custom_objects",
            "        )",
            "    else:",
            "        custom_obj = object_registration.get_registered_object(",
            "            registered_name, custom_objects=custom_objects",
            "        )",
            "    if custom_obj is not None:",
            "        return custom_obj",
            "",
            "    if module:",
            "        # If it's a Keras built-in object,",
            "        # we cannot always use direct import, because the exported",
            "        # module name might not match the package structure",
            "        # (e.g. experimental symbols).",
            "        if module == \"keras\" or module.startswith(\"keras.\"):",
            "            api_name = module + \".\" + name",
            "",
            "            obj = api_export.get_symbol_from_name(api_name)",
            "            if obj is not None:",
            "                return obj",
            "",
            "        # Configs of Keras built-in functions do not contain identifying",
            "        # information other than their name (e.g. 'acc' or 'tanh'). This special",
            "        # case searches the Keras modules that contain built-ins to retrieve",
            "        # the corresponding function from the identifying string.",
            "        if obj_type == \"function\" and module == \"builtins\":",
            "            for mod in BUILTIN_MODULES:",
            "                obj = api_export.get_symbol_from_name(",
            "                    \"keras.\" + mod + \".\" + name",
            "                )",
            "                if obj is not None:",
            "                    return obj",
            "",
            "        # Otherwise, attempt to retrieve the class object given the `module`",
            "        # and `class_name`. Import the module, find the class.",
            "        if module == \"keras.src\" or module.startswith(\"keras.src.\"):",
            "            try:",
            "                mod = importlib.import_module(module)",
            "                obj = vars(mod).get(name, None)",
            "                if obj is not None:",
            "                    return obj",
            "            except ModuleNotFoundError:",
            "                raise TypeError(",
            "                    f\"Could not deserialize {obj_type} '{name}' because \"",
            "                    f\"its parent module {module} cannot be imported. \"",
            "                    f\"Full object config: {full_config}\"",
            "                )",
            "",
            "    raise TypeError(",
            "        f\"Could not locate {obj_type} '{name}'. \"",
            "        \"Make sure custom classes are decorated with \"",
            "        \"`@keras.saving.register_keras_serializable()`. \"",
            "        f\"Full object config: {full_config}\"",
            "    )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "786": [
                "_retrieve_class_or_fn"
            ],
            "787": [
                "_retrieve_class_or_fn"
            ],
            "788": [
                "_retrieve_class_or_fn"
            ],
            "789": [
                "_retrieve_class_or_fn"
            ],
            "790": [
                "_retrieve_class_or_fn"
            ],
            "791": [
                "_retrieve_class_or_fn"
            ],
            "792": [
                "_retrieve_class_or_fn"
            ],
            "793": [
                "_retrieve_class_or_fn"
            ],
            "794": [
                "_retrieve_class_or_fn"
            ],
            "795": [
                "_retrieve_class_or_fn"
            ],
            "796": [
                "_retrieve_class_or_fn"
            ],
            "797": [
                "_retrieve_class_or_fn"
            ],
            "798": [
                "_retrieve_class_or_fn"
            ],
            "799": [
                "_retrieve_class_or_fn"
            ],
            "800": [
                "_retrieve_class_or_fn"
            ],
            "801": [
                "_retrieve_class_or_fn"
            ]
        },
        "addLocation": []
    }
}