{
    "bigflow/_version.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-__version__ = '1.5.4'"
            },
            "1": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1,
                "PatchRowcode": "+__version__ = '1.6.0.dev1'"
            }
        },
        "frontPatchFile": [
            "__version__ = '1.5.4'"
        ],
        "afterPatchFile": [
            "__version__ = '1.6.0.dev1'"
        ],
        "action": [
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "1": [
                "__version__"
            ]
        },
        "addLocation": []
    },
    "bigflow/build/operate.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 95,
                "PatchRowcode": "             auth_method=cache_params.auth_method or bigflow.deploy.AuthorizationType.LOCAL_ACCOUNT,"
            },
            "1": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 96,
                "PatchRowcode": "             vault_endpoint=cache_params.vault_endpoint,"
            },
            "2": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 97,
                "PatchRowcode": "             vault_secret=cache_params.vault_secret,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+            vault_endpoint_verify=cache_params.vault_endpoint_verify"
            },
            "4": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": 99,
                "PatchRowcode": "         )"
            },
            "5": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 100,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": 101,
                "PatchRowcode": "         for image in (cache_params.cache_from_image or []):"
            },
            "7": {
                "beforePatchRowNumber": 120,
                "afterPatchRowNumber": 121,
                "PatchRowcode": "     vault_secret: str | None = None"
            },
            "8": {
                "beforePatchRowNumber": 121,
                "afterPatchRowNumber": 122,
                "PatchRowcode": "     cache_from_version: list[str] | None = None"
            },
            "9": {
                "beforePatchRowNumber": 122,
                "afterPatchRowNumber": 123,
                "PatchRowcode": "     cache_from_image: list[str] | None = None"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 124,
                "PatchRowcode": "+    vault_endpoint_verify: str | bool | None = None"
            },
            "11": {
                "beforePatchRowNumber": 123,
                "afterPatchRowNumber": 125,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 124,
                "afterPatchRowNumber": 126,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 125,
                "afterPatchRowNumber": 127,
                "PatchRowcode": " def build_image("
            }
        },
        "frontPatchFile": [
            "\"\"\"Actual implementaion of buid/distribution operations\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import os",
            "import subprocess",
            "import shutil",
            "import logging",
            "import typing",
            "import textwrap",
            "import sys",
            "",
            "from datetime import datetime",
            "from pathlib import Path",
            "from dataclasses import dataclass",
            "",
            "import toml",
            "",
            "import bigflow.resources",
            "import bigflow.dagbuilder",
            "import bigflow.deploy",
            "import bigflow.version",
            "import bigflow.build.pip",
            "import bigflow.build.dev",
            "import bigflow.build.dist",
            "import bigflow.build.dataflow.dependency_checker",
            "import bigflow.commons as bf_commons",
            "",
            "from bigflow.build.spec import BigflowProjectSpec",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "def run_tests(project_spec: BigflowProjectSpec):",
            "",
            "    test_fn = {",
            "        'pytest': run_tests_pytest,",
            "        'unittest': run_tests_unittest,",
            "    }[project_spec.test_framework]",
            "",
            "    logger.info(\"Runing tests with %s...\", project_spec.test_framework)",
            "    try:",
            "        test_fn(project_spec)",
            "    except subprocess.CalledProcessError:",
            "        logger.error(\"Test suite was FAILED\")",
            "        exit(1)",
            "    logger.info(\"Test suite was PASSED\")",
            "",
            "",
            "def run_tests_pytest(project_spec: BigflowProjectSpec):",
            "    junit_xml = Path(\"./build/junit-reports/report.xml\").absolute()",
            "    junit_xml.parent.mkdir(parents=True, exist_ok=True)",
            "    color = sys.stdout.isatty()",
            "    bf_commons.run_process(",
            "        [",
            "            \"python\",",
            "            \"-u\",  # disable buffering",
            "            \"-m\", \"pytest\",",
            "            \"--color\", (\"yes\" if color else \"auto\"),",
            "            \"--junit-xml\", str(junit_xml),",
            "        ],",
            "    )",
            "",
            "",
            "def run_tests_unittest(project_spec: BigflowProjectSpec):",
            "    output_dir = \"build/junit-reports\"",
            "    bf_commons.run_process([",
            "        \"python\", \"-m\", \"xmlrunner\", \"discover\",",
            "        \"-s\", \".\",",
            "        \"-t\", project_spec.project_dir,",
            "        \"-o\", output_dir,",
            "    ])",
            "",
            "",
            "def _export_docker_image_to_file(tag: str, target_dir: Path, version: str):",
            "    image_target_path = target_dir / f\"image-{version}.tar\"",
            "    logger.info(\"Exporting the image to %s ...\", image_target_path)",
            "    bf_commons.run_process([\"docker\", \"image\", \"save\", \"-o\", image_target_path, bf_commons.get_docker_image_id(tag)])",
            "",
            "",
            "def _build_docker_image(",
            "    project_spec: BigflowProjectSpec,",
            "    tag: str,",
            "    cache_params: BuildImageCacheParams | None,",
            "):",
            "",
            "    logger.debug(\"Run docker build...\")",
            "    cmd = [\"docker\", \"build\", project_spec.project_dir, \"--tag\", tag]",
            "",
            "    if cache_params:",
            "",
            "        logger.debug(\"Authenticate to docker registry\")",
            "        bigflow.deploy.authenticate_to_registry(",
            "            auth_method=cache_params.auth_method or bigflow.deploy.AuthorizationType.LOCAL_ACCOUNT,",
            "            vault_endpoint=cache_params.vault_endpoint,",
            "            vault_secret=cache_params.vault_secret,",
            "        )",
            "",
            "        for image in (cache_params.cache_from_image or []):",
            "            logger.debug(\"Add --cache-from=%s to `docker build`\", image)",
            "            cmd.extend([\"--cache-from\", image])",
            "",
            "        for version in (cache_params.cache_from_version or []):",
            "            image = f\"{project_spec.docker_repository}:{version}\"",
            "            logger.debug(\"Add --cache-from=%s to `docker build`\", image)",
            "            cmd.extend([\"--cache-from\", image])",
            "",
            "        # noop when building backend is not a buildkit",
            "        logger.debug(\"Enable buildkit inline cache\")",
            "        cmd.extend([\"--build-arg\", \"BUILDKIT_INLINE_CACHE=1\"])",
            "",
            "    return bf_commons.run_process(cmd)",
            "",
            "",
            "@dataclass()",
            "class BuildImageCacheParams:",
            "    auth_method: bigflow.deploy.AuthorizationType",
            "    vault_endpoint: str | None = None",
            "    vault_secret: str | None = None",
            "    cache_from_version: list[str] | None = None",
            "    cache_from_image: list[str] | None = None",
            "",
            "",
            "def build_image(",
            "    project_spec: BigflowProjectSpec,",
            "    export_image_tar: bool | None = None,",
            "    cache_params: BuildImageCacheParams | None = None,",
            "):",
            "    if export_image_tar is None:",
            "        export_image_tar = project_spec.export_image_tar",
            "",
            "    logger.info(\"Building docker image...\")",
            "    clear_image_leftovers(project_spec)",
            "",
            "    image_dir = project_spec.project_dir / \".image\"",
            "    os.mkdir(image_dir)",
            "",
            "    dconf_file = Path(project_spec.deployment_config_file)",
            "    shutil.copyfile(dconf_file, image_dir / dconf_file.name)",
            "",
            "    tag = bf_commons.build_docker_image_tag(project_spec.docker_repository, project_spec.version)",
            "    logger.info(\"Generated image tag: %s\", tag)",
            "    _build_docker_image(project_spec, tag, cache_params)",
            "",
            "    if export_image_tar:",
            "        _export_image_as_tar(project_spec, image_dir, tag)",
            "    else:",
            "        _export_image_as_tag(project_spec, image_dir, tag)",
            "    logger.info(\"Docker image was built\")",
            "",
            "",
            "def _export_image_as_tag(project_spec, image_dir, tag):",
            "    infofile = Path(image_dir) / f\"imageinfo-{project_spec.version}.toml\"",
            "    image_id = bf_commons.get_docker_image_id(tag)",
            "    info = toml.dumps({",
            "            'created': datetime.now(),",
            "            'project_version': project_spec.version,",
            "            'project_name': project_spec.name,",
            "            'docker_image_id': image_id,",
            "            'docker_image_tag': tag,",
            "        })",
            "    logger.debug(\"Create 'image-info' marker %s\", infofile)",
            "    infofile.write_text(",
            "        textwrap.dedent(f\"\"\"",
            "            # This file is a marker indicating that docker image",
            "            # was built by bigflow but wasn't exported to a tar.",
            "            # Instead it kept inside local docker repo",
            "            # and tagged with `{tag}`.",
            "        \"\"\") + info",
            "    )",
            "",
            "",
            "def _export_image_as_tar(project_spec, image_dir, tag):",
            "    try:",
            "        _export_docker_image_to_file(tag, image_dir, project_spec.version)",
            "    finally:",
            "        logger.info(",
            "                \"Trying to remove the docker image. Tag: %s, image ID: %s\",",
            "                tag,",
            "                bf_commons.get_docker_image_id(tag),",
            "            )",
            "        try:",
            "            bf_commons.remove_docker_image_from_local_registry(tag)",
            "        except Exception:",
            "            logger.exception(\"Couldn't remove the docker image. Tag: %s, image ID: %s\", tag, bf_commons.get_docker_image_id(tag))",
            "",
            "",
            "def create_image_version_file(dags_dir: str, image_version: str):",
            "    dags_path = bigflow.dagbuilder.get_dags_output_dir(dags_dir) / \"image_version.txt\"",
            "    dags_path.write_text(image_version)",
            "",
            "",
            "def build_dags(",
            "    project_spec: BigflowProjectSpec,",
            "    start_time: str,",
            "    workflow_id: typing.Optional[str] = None,",
            "):",
            "    # TODO: Move common functions from bigflow.cli to bigflow.commons (or other shared module)",
            "    from bigflow.cli import walk_workflows",
            "",
            "    logger.debug('Loading workflow(s)...')",
            "    workflows = []",
            "    for root_package in project_spec.packages:",
            "        if \".\" in root_package:",
            "            # leaf package",
            "            continue",
            "",
            "        for workflow in walk_workflows(project_spec.project_dir / root_package):",
            "            if workflow_id is not None and workflow_id != workflow.workflow_id:",
            "                continue",
            "            workflows.append((workflow, root_package))",
            "",
            "    if not workflows:",
            "        if not workflow_id:",
            "            raise Exception('No workflow found')",
            "        else:",
            "            raise Exception(\"Workflow '{}' not found\".format(workflow_id))",
            "",
            "    logger.info(\"Building airflow DAGs...\")",
            "    clear_dags_leftovers(project_spec)",
            "",
            "    image_version = bf_commons.build_docker_image_tag(project_spec.docker_repository, project_spec.version)",
            "    create_image_version_file(str(project_spec.project_dir), image_version)",
            "",
            "    for (workflow, package) in workflows:",
            "        logger.info(\"Generating DAG file for %s\", workflow.workflow_id)",
            "        bigflow.dagbuilder.generate_dag_file(",
            "            str(project_spec.project_dir),",
            "            image_version,",
            "            workflow,",
            "            start_time,",
            "            project_spec.version,",
            "            package,",
            "        )",
            "",
            "    logger.info(\"Generated %d DAG files\", len(workflows))",
            "",
            "",
            "def _rmtree(p: Path):",
            "    logger.info(\"Removing directory %s\", p)",
            "    shutil.rmtree(p, ignore_errors=True)",
            "",
            "",
            "def clear_image_leftovers(project_spec: BigflowProjectSpec):",
            "    _rmtree(project_spec.project_dir / \".image\")",
            "",
            "",
            "def clear_dags_leftovers(project_spec: BigflowProjectSpec):",
            "    _rmtree(project_spec.project_dir / \".dags\")",
            "",
            "",
            "def build_package(project_spec: BigflowProjectSpec):",
            "    logger.info('Building python package')",
            "",
            "    req_in = Path(project_spec.project_requirements_file)",
            "    recompiled = bigflow.build.pip.maybe_recompile_requirements_file(req_in)",
            "    if recompiled:",
            "        req_txt = req_in.with_suffix(\".txt\")",
            "        logger.warning(textwrap.dedent(f\"\"\"",
            "            !!! Requirements file was recompiled, you need to reinstall packages.",
            "            !!! Run this command from your virtualenv:",
            "            pip install -r {req_txt}",
            "        \"\"\"))",
            "        project_spec.requries = bigflow.build.pip.read_requirements(req_in)",
            "",
            "    bigflow.build.dataflow.dependency_checker.check_beam_worker_dependencies_conflict(req_in)",
            "",
            "    clear_package_leftovers(project_spec)",
            "    run_tests(project_spec)",
            "",
            "    bigflow.build.dist.run_setup_command(project_spec, 'bdist_wheel')",
            "",
            "",
            "def clear_package_leftovers(project_spec: BigflowProjectSpec):",
            "    _rmtree(project_spec.project_dir / \"build\")",
            "    _rmtree(project_spec.project_dir / \"dist\")",
            "    _rmtree(project_spec.project_dir / f\"{project_spec.name}.egg\")",
            "",
            "",
            "def build_project(",
            "    project_spec: BigflowProjectSpec,",
            "    start_time: str,",
            "    workflow_id: str | None = None,",
            "    export_image_tar: bool | None = None,",
            "    cache_params: BuildImageCacheParams | None = None,",
            "):",
            "    logger.info(\"Build the project\")",
            "    build_dags(project_spec, start_time, workflow_id=workflow_id)",
            "    build_package(project_spec)",
            "    build_image(",
            "        project_spec,",
            "        export_image_tar=export_image_tar,",
            "        cache_params=cache_params,",
            "    )",
            "    logger.info(\"Project was built\")"
        ],
        "afterPatchFile": [
            "\"\"\"Actual implementaion of buid/distribution operations\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import os",
            "import subprocess",
            "import shutil",
            "import logging",
            "import typing",
            "import textwrap",
            "import sys",
            "",
            "from datetime import datetime",
            "from pathlib import Path",
            "from dataclasses import dataclass",
            "",
            "import toml",
            "",
            "import bigflow.resources",
            "import bigflow.dagbuilder",
            "import bigflow.deploy",
            "import bigflow.version",
            "import bigflow.build.pip",
            "import bigflow.build.dev",
            "import bigflow.build.dist",
            "import bigflow.build.dataflow.dependency_checker",
            "import bigflow.commons as bf_commons",
            "",
            "from bigflow.build.spec import BigflowProjectSpec",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "def run_tests(project_spec: BigflowProjectSpec):",
            "",
            "    test_fn = {",
            "        'pytest': run_tests_pytest,",
            "        'unittest': run_tests_unittest,",
            "    }[project_spec.test_framework]",
            "",
            "    logger.info(\"Runing tests with %s...\", project_spec.test_framework)",
            "    try:",
            "        test_fn(project_spec)",
            "    except subprocess.CalledProcessError:",
            "        logger.error(\"Test suite was FAILED\")",
            "        exit(1)",
            "    logger.info(\"Test suite was PASSED\")",
            "",
            "",
            "def run_tests_pytest(project_spec: BigflowProjectSpec):",
            "    junit_xml = Path(\"./build/junit-reports/report.xml\").absolute()",
            "    junit_xml.parent.mkdir(parents=True, exist_ok=True)",
            "    color = sys.stdout.isatty()",
            "    bf_commons.run_process(",
            "        [",
            "            \"python\",",
            "            \"-u\",  # disable buffering",
            "            \"-m\", \"pytest\",",
            "            \"--color\", (\"yes\" if color else \"auto\"),",
            "            \"--junit-xml\", str(junit_xml),",
            "        ],",
            "    )",
            "",
            "",
            "def run_tests_unittest(project_spec: BigflowProjectSpec):",
            "    output_dir = \"build/junit-reports\"",
            "    bf_commons.run_process([",
            "        \"python\", \"-m\", \"xmlrunner\", \"discover\",",
            "        \"-s\", \".\",",
            "        \"-t\", project_spec.project_dir,",
            "        \"-o\", output_dir,",
            "    ])",
            "",
            "",
            "def _export_docker_image_to_file(tag: str, target_dir: Path, version: str):",
            "    image_target_path = target_dir / f\"image-{version}.tar\"",
            "    logger.info(\"Exporting the image to %s ...\", image_target_path)",
            "    bf_commons.run_process([\"docker\", \"image\", \"save\", \"-o\", image_target_path, bf_commons.get_docker_image_id(tag)])",
            "",
            "",
            "def _build_docker_image(",
            "    project_spec: BigflowProjectSpec,",
            "    tag: str,",
            "    cache_params: BuildImageCacheParams | None,",
            "):",
            "",
            "    logger.debug(\"Run docker build...\")",
            "    cmd = [\"docker\", \"build\", project_spec.project_dir, \"--tag\", tag]",
            "",
            "    if cache_params:",
            "",
            "        logger.debug(\"Authenticate to docker registry\")",
            "        bigflow.deploy.authenticate_to_registry(",
            "            auth_method=cache_params.auth_method or bigflow.deploy.AuthorizationType.LOCAL_ACCOUNT,",
            "            vault_endpoint=cache_params.vault_endpoint,",
            "            vault_secret=cache_params.vault_secret,",
            "            vault_endpoint_verify=cache_params.vault_endpoint_verify",
            "        )",
            "",
            "        for image in (cache_params.cache_from_image or []):",
            "            logger.debug(\"Add --cache-from=%s to `docker build`\", image)",
            "            cmd.extend([\"--cache-from\", image])",
            "",
            "        for version in (cache_params.cache_from_version or []):",
            "            image = f\"{project_spec.docker_repository}:{version}\"",
            "            logger.debug(\"Add --cache-from=%s to `docker build`\", image)",
            "            cmd.extend([\"--cache-from\", image])",
            "",
            "        # noop when building backend is not a buildkit",
            "        logger.debug(\"Enable buildkit inline cache\")",
            "        cmd.extend([\"--build-arg\", \"BUILDKIT_INLINE_CACHE=1\"])",
            "",
            "    return bf_commons.run_process(cmd)",
            "",
            "",
            "@dataclass()",
            "class BuildImageCacheParams:",
            "    auth_method: bigflow.deploy.AuthorizationType",
            "    vault_endpoint: str | None = None",
            "    vault_secret: str | None = None",
            "    cache_from_version: list[str] | None = None",
            "    cache_from_image: list[str] | None = None",
            "    vault_endpoint_verify: str | bool | None = None",
            "",
            "",
            "def build_image(",
            "    project_spec: BigflowProjectSpec,",
            "    export_image_tar: bool | None = None,",
            "    cache_params: BuildImageCacheParams | None = None,",
            "):",
            "    if export_image_tar is None:",
            "        export_image_tar = project_spec.export_image_tar",
            "",
            "    logger.info(\"Building docker image...\")",
            "    clear_image_leftovers(project_spec)",
            "",
            "    image_dir = project_spec.project_dir / \".image\"",
            "    os.mkdir(image_dir)",
            "",
            "    dconf_file = Path(project_spec.deployment_config_file)",
            "    shutil.copyfile(dconf_file, image_dir / dconf_file.name)",
            "",
            "    tag = bf_commons.build_docker_image_tag(project_spec.docker_repository, project_spec.version)",
            "    logger.info(\"Generated image tag: %s\", tag)",
            "    _build_docker_image(project_spec, tag, cache_params)",
            "",
            "    if export_image_tar:",
            "        _export_image_as_tar(project_spec, image_dir, tag)",
            "    else:",
            "        _export_image_as_tag(project_spec, image_dir, tag)",
            "    logger.info(\"Docker image was built\")",
            "",
            "",
            "def _export_image_as_tag(project_spec, image_dir, tag):",
            "    infofile = Path(image_dir) / f\"imageinfo-{project_spec.version}.toml\"",
            "    image_id = bf_commons.get_docker_image_id(tag)",
            "    info = toml.dumps({",
            "            'created': datetime.now(),",
            "            'project_version': project_spec.version,",
            "            'project_name': project_spec.name,",
            "            'docker_image_id': image_id,",
            "            'docker_image_tag': tag,",
            "        })",
            "    logger.debug(\"Create 'image-info' marker %s\", infofile)",
            "    infofile.write_text(",
            "        textwrap.dedent(f\"\"\"",
            "            # This file is a marker indicating that docker image",
            "            # was built by bigflow but wasn't exported to a tar.",
            "            # Instead it kept inside local docker repo",
            "            # and tagged with `{tag}`.",
            "        \"\"\") + info",
            "    )",
            "",
            "",
            "def _export_image_as_tar(project_spec, image_dir, tag):",
            "    try:",
            "        _export_docker_image_to_file(tag, image_dir, project_spec.version)",
            "    finally:",
            "        logger.info(",
            "                \"Trying to remove the docker image. Tag: %s, image ID: %s\",",
            "                tag,",
            "                bf_commons.get_docker_image_id(tag),",
            "            )",
            "        try:",
            "            bf_commons.remove_docker_image_from_local_registry(tag)",
            "        except Exception:",
            "            logger.exception(\"Couldn't remove the docker image. Tag: %s, image ID: %s\", tag, bf_commons.get_docker_image_id(tag))",
            "",
            "",
            "def create_image_version_file(dags_dir: str, image_version: str):",
            "    dags_path = bigflow.dagbuilder.get_dags_output_dir(dags_dir) / \"image_version.txt\"",
            "    dags_path.write_text(image_version)",
            "",
            "",
            "def build_dags(",
            "    project_spec: BigflowProjectSpec,",
            "    start_time: str,",
            "    workflow_id: typing.Optional[str] = None,",
            "):",
            "    # TODO: Move common functions from bigflow.cli to bigflow.commons (or other shared module)",
            "    from bigflow.cli import walk_workflows",
            "",
            "    logger.debug('Loading workflow(s)...')",
            "    workflows = []",
            "    for root_package in project_spec.packages:",
            "        if \".\" in root_package:",
            "            # leaf package",
            "            continue",
            "",
            "        for workflow in walk_workflows(project_spec.project_dir / root_package):",
            "            if workflow_id is not None and workflow_id != workflow.workflow_id:",
            "                continue",
            "            workflows.append((workflow, root_package))",
            "",
            "    if not workflows:",
            "        if not workflow_id:",
            "            raise Exception('No workflow found')",
            "        else:",
            "            raise Exception(\"Workflow '{}' not found\".format(workflow_id))",
            "",
            "    logger.info(\"Building airflow DAGs...\")",
            "    clear_dags_leftovers(project_spec)",
            "",
            "    image_version = bf_commons.build_docker_image_tag(project_spec.docker_repository, project_spec.version)",
            "    create_image_version_file(str(project_spec.project_dir), image_version)",
            "",
            "    for (workflow, package) in workflows:",
            "        logger.info(\"Generating DAG file for %s\", workflow.workflow_id)",
            "        bigflow.dagbuilder.generate_dag_file(",
            "            str(project_spec.project_dir),",
            "            image_version,",
            "            workflow,",
            "            start_time,",
            "            project_spec.version,",
            "            package,",
            "        )",
            "",
            "    logger.info(\"Generated %d DAG files\", len(workflows))",
            "",
            "",
            "def _rmtree(p: Path):",
            "    logger.info(\"Removing directory %s\", p)",
            "    shutil.rmtree(p, ignore_errors=True)",
            "",
            "",
            "def clear_image_leftovers(project_spec: BigflowProjectSpec):",
            "    _rmtree(project_spec.project_dir / \".image\")",
            "",
            "",
            "def clear_dags_leftovers(project_spec: BigflowProjectSpec):",
            "    _rmtree(project_spec.project_dir / \".dags\")",
            "",
            "",
            "def build_package(project_spec: BigflowProjectSpec):",
            "    logger.info('Building python package')",
            "",
            "    req_in = Path(project_spec.project_requirements_file)",
            "    recompiled = bigflow.build.pip.maybe_recompile_requirements_file(req_in)",
            "    if recompiled:",
            "        req_txt = req_in.with_suffix(\".txt\")",
            "        logger.warning(textwrap.dedent(f\"\"\"",
            "            !!! Requirements file was recompiled, you need to reinstall packages.",
            "            !!! Run this command from your virtualenv:",
            "            pip install -r {req_txt}",
            "        \"\"\"))",
            "        project_spec.requries = bigflow.build.pip.read_requirements(req_in)",
            "",
            "    bigflow.build.dataflow.dependency_checker.check_beam_worker_dependencies_conflict(req_in)",
            "",
            "    clear_package_leftovers(project_spec)",
            "    run_tests(project_spec)",
            "",
            "    bigflow.build.dist.run_setup_command(project_spec, 'bdist_wheel')",
            "",
            "",
            "def clear_package_leftovers(project_spec: BigflowProjectSpec):",
            "    _rmtree(project_spec.project_dir / \"build\")",
            "    _rmtree(project_spec.project_dir / \"dist\")",
            "    _rmtree(project_spec.project_dir / f\"{project_spec.name}.egg\")",
            "",
            "",
            "def build_project(",
            "    project_spec: BigflowProjectSpec,",
            "    start_time: str,",
            "    workflow_id: str | None = None,",
            "    export_image_tar: bool | None = None,",
            "    cache_params: BuildImageCacheParams | None = None,",
            "):",
            "    logger.info(\"Build the project\")",
            "    build_dags(project_spec, start_time, workflow_id=workflow_id)",
            "    build_package(project_spec)",
            "    build_image(",
            "        project_spec,",
            "        export_image_tar=export_image_tar,",
            "        cache_params=cache_params,",
            "    )",
            "    logger.info(\"Project was built\")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "bigflow.build.operate.BuildImageCacheParams.self",
            "src.pyload.core",
            "bigflow.build.operate.build_image",
            "bigflow.build.operate._build_docker_image.cmd"
        ]
    },
    "bigflow/cli.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " from importlib import import_module"
            },
            "1": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " from pathlib import Path"
            },
            "2": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " from types import ModuleType"
            },
            "3": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from typing import Tuple, Iterator"
            },
            "4": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from typing import Optional"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+from typing import Tuple, Iterator, Optional"
            },
            "6": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " import fnmatch"
            },
            "7": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " import bigflow as bf"
            },
            "9": {
                "beforePatchRowNumber": 384,
                "afterPatchRowNumber": 383,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 385,
                "afterPatchRowNumber": 384,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 386,
                "afterPatchRowNumber": 385,
                "PatchRowcode": " def _add_auth_parsers_arguments(parser):"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 386,
                "PatchRowcode": "+    class VaultEndpointVerifyAction(argparse.Action):"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 387,
                "PatchRowcode": "+        def __call__(self, parser, args, values, option_string=None):"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 388,
                "PatchRowcode": "+            if values in ['true', 'false']:"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 389,
                "PatchRowcode": "+                setattr(args, self.dest, values == 'true')"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 390,
                "PatchRowcode": "+            else:"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 391,
                "PatchRowcode": "+                setattr(args, self.dest, str(values))"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 392,
                "PatchRowcode": "+"
            },
            "19": {
                "beforePatchRowNumber": 387,
                "afterPatchRowNumber": 393,
                "PatchRowcode": "     parser.add_argument('-a', '--auth-method',"
            },
            "20": {
                "beforePatchRowNumber": 388,
                "afterPatchRowNumber": 394,
                "PatchRowcode": "                         type=bigflow.deploy.AuthorizationType,"
            },
            "21": {
                "beforePatchRowNumber": 389,
                "afterPatchRowNumber": 395,
                "PatchRowcode": "                         default='local_account',"
            },
            "22": {
                "beforePatchRowNumber": 399,
                "afterPatchRowNumber": 405,
                "PatchRowcode": "                              'Required if auth-method is vault. '"
            },
            "23": {
                "beforePatchRowNumber": 400,
                "afterPatchRowNumber": 406,
                "PatchRowcode": "                              'If not set, will be read from deployment_config.py.'"
            },
            "24": {
                "beforePatchRowNumber": 401,
                "afterPatchRowNumber": 407,
                "PatchRowcode": "                         )"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 408,
                "PatchRowcode": "+    parser.add_argument('-vev', '--vault-endpoint-verify',"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 409,
                "PatchRowcode": "+                        type=str,"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 410,
                "PatchRowcode": "+                        action=VaultEndpointVerifyAction,"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 411,
                "PatchRowcode": "+                        help='Can be \"true\", \"false\", a path to certificate PEM file or a path to directory with PEM files. '"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 412,
                "PatchRowcode": "+                             'Enables/disables vault endpoint TLS certificate verification. Enabled by default. '"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 413,
                "PatchRowcode": "+                             'Disabling makes execution vulnerable for MITM attacks - do it only when justified and in trusted environments. '"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 414,
                "PatchRowcode": "+                             'For details see: https://requests.readthedocs.io/en/latest/user/advanced/#ssl-cert-verification',"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 415,
                "PatchRowcode": "+                        dest='vault_endpoint_verify',"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 416,
                "PatchRowcode": "+                        default=True"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 417,
                "PatchRowcode": "+                        )"
            },
            "35": {
                "beforePatchRowNumber": 402,
                "afterPatchRowNumber": 418,
                "PatchRowcode": "     parser.add_argument('-vs', '--vault-secret',"
            },
            "36": {
                "beforePatchRowNumber": 403,
                "afterPatchRowNumber": 419,
                "PatchRowcode": "                         type=str,"
            },
            "37": {
                "beforePatchRowNumber": 404,
                "afterPatchRowNumber": 420,
                "PatchRowcode": "                         help='Vault secret token. '"
            },
            "38": {
                "beforePatchRowNumber": 514,
                "afterPatchRowNumber": 530,
                "PatchRowcode": " def _resolve_property(args, property_name, ignore_value_error=False):"
            },
            "39": {
                "beforePatchRowNumber": 515,
                "afterPatchRowNumber": 531,
                "PatchRowcode": "     try:"
            },
            "40": {
                "beforePatchRowNumber": 516,
                "afterPatchRowNumber": 532,
                "PatchRowcode": "         cli_atr = getattr(args, property_name)"
            },
            "41": {
                "beforePatchRowNumber": 517,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if cli_atr:"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 533,
                "PatchRowcode": "+        if cli_atr or cli_atr is False:"
            },
            "43": {
                "beforePatchRowNumber": 518,
                "afterPatchRowNumber": 534,
                "PatchRowcode": "             return cli_atr"
            },
            "44": {
                "beforePatchRowNumber": 519,
                "afterPatchRowNumber": 535,
                "PatchRowcode": "         else:"
            },
            "45": {
                "beforePatchRowNumber": 520,
                "afterPatchRowNumber": 536,
                "PatchRowcode": "             config = import_deployment_config(_resolve_deployment_config_path(args), property_name)"
            },
            "46": {
                "beforePatchRowNumber": 533,
                "afterPatchRowNumber": 549,
                "PatchRowcode": "                        clear_dags_folder=args.clear_dags_folder,"
            },
            "47": {
                "beforePatchRowNumber": 534,
                "afterPatchRowNumber": 550,
                "PatchRowcode": "                        auth_method=args.auth_method,"
            },
            "48": {
                "beforePatchRowNumber": 535,
                "afterPatchRowNumber": 551,
                "PatchRowcode": "                        vault_endpoint=_resolve_vault_endpoint(args),"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 552,
                "PatchRowcode": "+                       vault_endpoint_verify=_resolve_property(args, 'vault_endpoint_verify', ignore_value_error=True),"
            },
            "50": {
                "beforePatchRowNumber": 536,
                "afterPatchRowNumber": 553,
                "PatchRowcode": "                        vault_secret=vault_secret,"
            },
            "51": {
                "beforePatchRowNumber": 537,
                "afterPatchRowNumber": 554,
                "PatchRowcode": "                        project_id=_resolve_property(args, 'gcp_project_id')"
            },
            "52": {
                "beforePatchRowNumber": 538,
                "afterPatchRowNumber": 555,
                "PatchRowcode": "                        )"
            },
            "53": {
                "beforePatchRowNumber": 543,
                "afterPatchRowNumber": 560,
                "PatchRowcode": "     docker_repository = _resolve_property(args, 'docker_repository')"
            },
            "54": {
                "beforePatchRowNumber": 544,
                "afterPatchRowNumber": 561,
                "PatchRowcode": "     vault_secret = _resolve_property(args, 'vault_secret', ignore_value_error=True)"
            },
            "55": {
                "beforePatchRowNumber": 545,
                "afterPatchRowNumber": 562,
                "PatchRowcode": "     vault_endpoint = _resolve_vault_endpoint(args)"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 563,
                "PatchRowcode": "+    vault_endpoint_verify = _resolve_property(args, 'vault_endpoint_verify', ignore_value_error=True)"
            },
            "57": {
                "beforePatchRowNumber": 546,
                "afterPatchRowNumber": 564,
                "PatchRowcode": "     image_tar_path = args.image_tar_path if args.image_tar_path else find_image_file()"
            },
            "58": {
                "beforePatchRowNumber": 547,
                "afterPatchRowNumber": 565,
                "PatchRowcode": " "
            },
            "59": {
                "beforePatchRowNumber": 548,
                "afterPatchRowNumber": 566,
                "PatchRowcode": "     bigflow.deploy.deploy_docker_image("
            },
            "60": {
                "beforePatchRowNumber": 549,
                "afterPatchRowNumber": 567,
                "PatchRowcode": "         image_tar_path=image_tar_path,"
            },
            "61": {
                "beforePatchRowNumber": 550,
                "afterPatchRowNumber": 568,
                "PatchRowcode": "         auth_method=args.auth_method,"
            },
            "62": {
                "beforePatchRowNumber": 551,
                "afterPatchRowNumber": 569,
                "PatchRowcode": "         docker_repository=docker_repository,"
            },
            "63": {
                "beforePatchRowNumber": 552,
                "afterPatchRowNumber": 570,
                "PatchRowcode": "         vault_endpoint=vault_endpoint,"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 571,
                "PatchRowcode": "+        vault_endpoint_verify=vault_endpoint_verify,"
            },
            "65": {
                "beforePatchRowNumber": 553,
                "afterPatchRowNumber": 572,
                "PatchRowcode": "         vault_secret=vault_secret,"
            },
            "66": {
                "beforePatchRowNumber": 554,
                "afterPatchRowNumber": 573,
                "PatchRowcode": "     )"
            },
            "67": {
                "beforePatchRowNumber": 555,
                "afterPatchRowNumber": 574,
                "PatchRowcode": " "
            },
            "68": {
                "beforePatchRowNumber": 579,
                "afterPatchRowNumber": 598,
                "PatchRowcode": "         logger.debug(\"Image caching is requested - create build image cache params obj\")"
            },
            "69": {
                "beforePatchRowNumber": 580,
                "afterPatchRowNumber": 599,
                "PatchRowcode": "         vault_secret = _resolve_property(args, 'vault_secret', ignore_value_error=True)"
            },
            "70": {
                "beforePatchRowNumber": 581,
                "afterPatchRowNumber": 600,
                "PatchRowcode": "         vault_endpoint = _resolve_vault_endpoint(args)"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 601,
                "PatchRowcode": "+        vault_endpoint_verify = _resolve_property(args, 'vault_endpoint_verify', ignore_value_error=True)"
            },
            "72": {
                "beforePatchRowNumber": 582,
                "afterPatchRowNumber": 602,
                "PatchRowcode": "         return bigflow.build.operate.BuildImageCacheParams("
            },
            "73": {
                "beforePatchRowNumber": 583,
                "afterPatchRowNumber": 603,
                "PatchRowcode": "             auth_method=args.auth_method,"
            },
            "74": {
                "beforePatchRowNumber": 584,
                "afterPatchRowNumber": 604,
                "PatchRowcode": "             vault_endpoint=vault_endpoint,"
            },
            "75": {
                "beforePatchRowNumber": 585,
                "afterPatchRowNumber": 605,
                "PatchRowcode": "             vault_secret=vault_secret,"
            },
            "76": {
                "beforePatchRowNumber": 586,
                "afterPatchRowNumber": 606,
                "PatchRowcode": "             cache_from_version=args.cache_from_version,"
            },
            "77": {
                "beforePatchRowNumber": 587,
                "afterPatchRowNumber": 607,
                "PatchRowcode": "             cache_from_image=args.cache_from_image,"
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 608,
                "PatchRowcode": "+            vault_endpoint_verify=vault_endpoint_verify"
            },
            "79": {
                "beforePatchRowNumber": 588,
                "afterPatchRowNumber": 609,
                "PatchRowcode": "         )"
            },
            "80": {
                "beforePatchRowNumber": 589,
                "afterPatchRowNumber": 610,
                "PatchRowcode": "     else:"
            },
            "81": {
                "beforePatchRowNumber": 590,
                "afterPatchRowNumber": 611,
                "PatchRowcode": "         logger.debug(\"No caching is requested - so just disable it completly\")"
            }
        },
        "frontPatchFile": [
            "import argparse",
            "import importlib",
            "import os",
            "import pathlib",
            "import subprocess",
            "import sys",
            "import logging",
            "",
            "import importlib.util",
            "",
            "from argparse import Namespace",
            "from datetime import datetime",
            "from importlib import import_module",
            "from pathlib import Path",
            "from types import ModuleType",
            "from typing import Tuple, Iterator",
            "from typing import Optional",
            "import fnmatch",
            "",
            "import bigflow as bf",
            "import bigflow.build.pip",
            "import bigflow.resources",
            "import bigflow.commons as bf_commons",
            "import bigflow.build.dist",
            "import bigflow.build.dev",
            "import bigflow.build.operate",
            "import bigflow.build.spec",
            "import bigflow.migrate",
            "import bigflow.deploy",
            "import bigflow.scaffold",
            "import bigflow.version",
            "",
            "from bigflow import Config",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "def walk_module_files(root_package: Path) -> Iterator[Tuple[str, str]]:",
            "    \"\"\"",
            "    Returning all the Python files in the `root_package`",
            "",
            "    Example:",
            "    walk_module_files(Path(\"fld\")) -> [(\"/path/to/fld\", \"file1\"), (\"/path/to/fld\", \"file2\")]",
            "",
            "    @return: (absolute_path: str, name: str)",
            "    \"\"\"",
            "    logger.debug(\"walk module files %r\", root_package)",
            "    for subdir, dirs, files in os.walk(str(root_package)):",
            "        for file in files:",
            "            if file.endswith('.py'):",
            "                logger.debug(\"found python file %s/%s\", subdir, file)",
            "                yield subdir, file",
            "",
            "",
            "def _removesuffix(s, suffix):",
            "    return s[:-len(suffix)] if s.endswith(suffix) else s",
            "",
            "",
            "def build_module_path(root_package: Path, module_dir: Path, module_file: str) -> str:",
            "    \"\"\"",
            "    Returns module path that can be imported using `import_module`",
            "    \"\"\"",
            "    full_module_file_path = str(module_dir.absolute() / module_file)",
            "    full_module_file_path = full_module_file_path.replace(str(root_package.parent.absolute()), '')",
            "",
            "    res = full_module_file_path",
            "    res = _removesuffix(res, \".py\")",
            "    res = _removesuffix(res, \"/__init__\")",
            "    res = res.lstrip(\"/\").replace(os.sep, \".\")",
            "    return res",
            "",
            "",
            "def walk_module_paths(root_package: Path) -> Iterator[str]:",
            "    \"\"\"",
            "    Returning all the module paths in the `root_package`",
            "    \"\"\"",
            "    logger.debug(\"walk module paths, root %r\", root_package)",
            "    for module_dir, module_file in walk_module_files(root_package):",
            "        mpath = build_module_path(root_package, Path(module_dir), module_file)",
            "        logger.debug(\"%s / %s / %s resolved to module %r\", root_package, module_dir, module_file, mpath)",
            "        logger.debug(\"path %r\", mpath)",
            "        yield mpath",
            "",
            "",
            "def walk_modules(root_package: Path) -> Iterator[ModuleType]:",
            "    \"\"\"",
            "    Imports all the modules in the path and returns",
            "    \"\"\"",
            "    logger.debug(\"walk modules, root %r\", root_package)",
            "    for module_path in walk_module_paths(root_package):",
            "        try:",
            "            logger.debug(\"import module %r\", module_path)",
            "            logger.debug(\"%r\", sys.path)",
            "            yield import_module(module_path)",
            "        except ValueError as e:",
            "            print(f\"Skipping module {module_path}. Can't import due to exception {str(e)}.\")",
            "",
            "",
            "def walk_module_objects(module: ModuleType, expect_type: type) -> Iterator[Tuple[str, type]]:",
            "    \"\"\"",
            "    Returns module items of the set type",
            "    \"\"\"",
            "    logger.debug(\"scan module %r for object of type %r\", module, expect_type)",
            "    for name, obj in module.__dict__.items():",
            "        if isinstance(obj, expect_type):",
            "            yield name, obj",
            "",
            "",
            "def walk_workflows(root_package: Path) -> Iterator[bf.Workflow]:",
            "    \"\"\"",
            "    Imports modules in the `root_package` and returns all the elements of the type bf.Workflow",
            "    \"\"\"",
            "    logger.debug(\"walk workflows, root %s\", root_package)",
            "    for module in walk_modules(root_package):",
            "        for name, workflow in walk_module_objects(module, bf.Workflow):",
            "            yield workflow",
            "",
            "",
            "def find_workflow(root_package: Path, workflow_id: str) -> bf.Workflow:",
            "    \"\"\"",
            "    Imports modules and finds the workflow with id workflow_id",
            "    \"\"\"",
            "    logger.debug(\"find workflow, root %s, workflow_id %r\", root_package, workflow_id)",
            "    for workflow in walk_workflows(root_package):",
            "        if workflow.workflow_id == workflow_id:",
            "            return workflow",
            "    raise ValueError('Workflow with id {} not found in package {}'.format(workflow_id, root_package))",
            "",
            "",
            "def set_configuration_env(env):",
            "    \"\"\"",
            "    Sets 'bf_env' env variable",
            "    \"\"\"",
            "    if env is not None:",
            "        os.environ['bf_env'] = env",
            "        print(f\"bf_env is : {os.environ.get('bf_env', None)}\")",
            "",
            "",
            "def execute_job(root_package: Path, workflow_id: str, job_id: str, runtime=None):",
            "    \"\"\"",
            "    Executes the job with the `workflow_id`, with job id `job_id`",
            "",
            "    @param runtime: str determine partition that will be used for write operations.",
            "    \"\"\"",
            "    w = find_workflow(root_package, workflow_id)",
            "    w.run_job(job_id, runtime)",
            "",
            "",
            "def execute_workflow(root_package: Path, workflow_id: str, runtime=None):",
            "    \"\"\"",
            "    Executes the workflow with the `workflow_id`",
            "",
            "    @param runtime: str determine partition that will be used for write operations.",
            "    \"\"\"",
            "    w = find_workflow(root_package, workflow_id)",
            "    w.run(runtime)",
            "",
            "",
            "def read_project_name_from_setup() -> Optional[str]:",
            "    logger.debug(\"Read project name from project spec\")",
            "    try:",
            "        return bigflow.build.spec.get_project_spec().name",
            "    except Exception as e:",
            "        logger.warning(\"Unable to read project name: %s\", e)",
            "        return None",
            "",
            "",
            "def find_root_package(project_name: Optional[str], project_dir: Optional[str]) -> Path:",
            "    \"\"\"",
            "    Finds project package path. Tries first to find location in project_setup.PROJECT_NAME,",
            "    and if not found then by making a path to the `root` module",
            "",
            "    @param project_dir: Path to the root package of a project, used only when PROJECT_NAME not set",
            "    @return: Path",
            "    \"\"\"",
            "    if project_name is not None:",
            "        project_name = project_name.replace(\"-\", \"_\")",
            "        return Path(project_name)",
            "    else:",
            "        root_module = import_module(project_dir)",
            "        return Path(root_module.__file__.replace('__init__.py', ''))",
            "",
            "",
            "def import_deployment_config(deployment_config_path: str, property_name: str):",
            "    if not Path(deployment_config_path).exists():",
            "        raise ValueError(f\"Can't find deployment_config.py at '{deployment_config_path}'. \"",
            "                         f\"Property '{property_name}' can't be resolved. \"",
            "                          \"If your deployment_config.py is elswhere, \"",
            "                          \"you can set path to it using --deployment-config-path. If you are not using deployment_config.py -- \"",
            "                         f\"set '{property_name}' property as a command line argument.\")",
            "    spec = importlib.util.spec_from_file_location('deployment_config', deployment_config_path)",
            "",
            "    if not spec:",
            "        raise ValueError(f'Failed to load deployment_config from {deployment_config_path}. '",
            "        'Create a proper deployment_config.py file'",
            "        'or set all the properties via command line arguments.')",
            "",
            "    deployment_config_module = importlib.util.module_from_spec(spec)",
            "    spec.loader.exec_module(deployment_config_module)",
            "",
            "    if not isinstance(deployment_config_module.deployment_config, Config):",
            "        raise ValueError('deployment_config attribute in deployment_config.py should be instance of bigflow.Config')",
            "",
            "    return deployment_config_module.deployment_config",
            "",
            "",
            "def cli_run(project_package: str,",
            "            runtime: Optional[str] = None,",
            "            full_job_id: Optional[str] = None,",
            "            workflow_id: Optional[str] = None) -> None:",
            "    \"\"\"",
            "    Runs the specified job or workflow",
            "",
            "    @param project_package: str The main package of a user's project",
            "    @param runtime: Optional[str] Date of XXX in format \"%Y-%m-%d %H:%M:%S\"",
            "    @param full_job_id: Optional[str] Represents both workflow_id and job_id in a string in format \"<workflow_id>.<job_id>\"",
            "    @param workflow_id: Optional[str] The id of the workflow that should be executed",
            "    @return:",
            "    \"\"\"",
            "",
            "    # TODO: Check that installed libs in sync with `requirements.txt`",
            "    bigflow.build.pip.check_requirements_needs_recompile(Path(\"resources/requirements.txt\"))",
            "",
            "    if full_job_id is not None:",
            "        try:",
            "            workflow_id, job_id = full_job_id.split('.')",
            "        except ValueError:",
            "            raise ValueError(",
            "                'You should specify job using the workflow_id and job_id parameters - --job <workflow_id>.<job_id>.')",
            "        execute_job(project_package, workflow_id, job_id, runtime=runtime)",
            "    elif workflow_id is not None:",
            "        execute_workflow(project_package, workflow_id, runtime=runtime)",
            "    else:",
            "        raise ValueError('You must provide the --job or --workflow for the run command.')",
            "",
            "",
            "def _parse_args(project_name: Optional[str], args) -> Namespace:",
            "    parser = argparse.ArgumentParser(description=f'Welcome to BigFlow CLI.'",
            "                                                  '\\nType: bigflow {command} -h to print detailed help for a selected command.')",
            "    parser.add_argument(",
            "        \"-v\",",
            "        \"--verbose\",",
            "        action='store_true',",
            "        default=False,",
            "        help=\"Print verbose output for debugging\",",
            "    )",
            "",
            "    subparsers = parser.add_subparsers(dest='operation',",
            "                                       required=True,",
            "                                       help='BigFlow command to execute')",
            "",
            "    _create_run_parser(subparsers, project_name)",
            "    _create_deploy_dags_parser(subparsers)",
            "    _create_deploy_image_parser(subparsers)",
            "    _create_deploy_parser(subparsers)",
            "",
            "    _create_build_dags_parser(subparsers)",
            "    _create_build_image_parser(subparsers)",
            "    _create_build_package_parser(subparsers)",
            "    _create_build_parser(subparsers)",
            "",
            "    _create_project_version_parser(subparsers)",
            "    _create_release_parser(subparsers)",
            "    _create_start_project_parser(subparsers)",
            "    _create_logs_parser(subparsers)",
            "",
            "    _create_build_requirements_parser(subparsers)",
            "",
            "    _create_codegen_parser(subparsers)",
            "",
            "    return parser.parse_args(args)",
            "",
            "",
            "def _create_logs_parser(subparsers):",
            "    subparsers.add_parser('logs', description='Returns a link leading to a workflow logs in GCP Logging.')",
            "",
            "",
            "def _create_start_project_parser(subparsers):",
            "    subparsers.add_parser('start-project', description='Creates a scaffolding project in a current directory.')",
            "",
            "",
            "def _create_build_parser(subparsers):",
            "    parser = subparsers.add_parser('build', description='Builds a Docker image, DAG files and .whl package from local sources.')",
            "    _add_build_dags_parser_arguments(parser)",
            "    _add_build_image_parser_arguments(parser)",
            "    _add_parsers_common_arguments(parser)",
            "",
            "",
            "def _create_build_package_parser(subparsers):",
            "    subparsers.add_parser('build-package', description='Builds .whl package from local sources.')",
            "",
            "",
            "def _add_build_dags_parser_arguments(parser):",
            "    parser.add_argument('-w', '--workflow',",
            "                        type=str,",
            "                        help=\"Skip or set to ALL to build DAGs from all workflows. \"",
            "                             \"Set a workflow Id to build selected workflow only. \"",
            "                             \"For example to build only this workflow: bigflow.Workflow(workflow_id='workflow1',\"",
            "                             \" definition=[ExampleJob('job1')]) you should use --workflow workflow1\")",
            "    parser.add_argument('-t', '--start-time',",
            "                        help='The first runtime of a workflow. '",
            "                             'For workflows triggered hourly -- datetime in format: Y-m-d H:M:S, for example 2020-01-01 00:00:00. '",
            "                             'For workflows triggered daily -- date in format: Y-m-d, for example 2020-01-01. '",
            "                             'If empty or set as NOW, current hour is used.',",
            "                        type=bf_commons.valid_datetime)",
            "",
            "",
            "def _add_build_image_parser_arguments(parser: argparse.ArgumentParser):",
            "    parser.add_argument(",
            "        '--export-image-tar', dest='export_image_tar', action='store_true',",
            "        help=\"Export built docker image into .tar file\",",
            "    )",
            "    parser.add_argument(",
            "        '--no-export-image-tar', dest='export_image_tar', action='store_false',",
            "        help=\"Don't export built docker image into .tar file (keep image in local docker registry)\",",
            "    )",
            "    parser.set_defaults(export_image_tar=None)",
            "",
            "    parser.add_argument(",
            "        '--cache-from-image',",
            "        dest='cache_from_image',",
            "        action='append',",
            "        help=\"Docker images to consider as cache sources\",",
            "    )",
            "    parser.add_argument(",
            "        '--cache-from-version',",
            "        dest='cache_from_version',",
            "        action='append',",
            "        help=\"Use previous version of the project as cache source\",",
            "    )",
            "    _add_auth_parsers_arguments(parser)",
            "",
            "",
            "def _create_build_dags_parser(subparsers):",
            "    parser = subparsers.add_parser('build-dags',",
            "                                   description='Builds DAG files from local sources to {current_dir}/.dags')",
            "    _add_build_dags_parser_arguments(parser)",
            "",
            "",
            "def _create_build_image_parser(subparsers):",
            "    parser = subparsers.add_parser(",
            "        'build-image',",
            "        description='Builds a docker image from local files.',",
            "    )",
            "    _add_build_image_parser_arguments(parser)",
            "    _add_parsers_common_arguments(parser)",
            "",
            "",
            "def _create_run_parser(subparsers, project_name):",
            "    parser = subparsers.add_parser('run',",
            "                                   description='BigFlow CLI run command -- run a workflow or job')",
            "",
            "    group = parser.add_mutually_exclusive_group()",
            "    group.required = True",
            "    group.add_argument('-j', '--job',",
            "                       type=str,",
            "                       help='The job to start, identified by workflow id and job id in format \"<workflow_id>.<job_id>\".')",
            "    group.add_argument('-w', '--workflow',",
            "                       type=str,",
            "                       help='The id of the workflow to start.')",
            "    parser.add_argument('-r', '--runtime',",
            "                        type=str, default=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),",
            "                        help='The date and time when this job or workflow should be started. '",
            "                             'The default is now (%(default)s). '",
            "                             'Examples: 2019-01-01, 2020-01-01 01:00:00')",
            "    _add_parsers_common_arguments(parser)",
            "",
            "    if project_name is None:",
            "        parser.add_argument('--project-package',",
            "                            required=True,",
            "                            type=str,",
            "                            help='The main package of your project. '",
            "                                 'Should contain `setup.py`')",
            "",
            "",
            "def _add_parsers_common_arguments(parser):",
            "    parser.add_argument('-c', '--config',",
            "                        type=str,",
            "                        help='Config environment name that should be used. For example: dev, prod.'",
            "                             ' If not set, default Config name will be used.'",
            "                             ' This env name is applied to all bigflow.Config objects that are defined by'",
            "                             ' individual workflows as well as to deployment_config.py.')",
            "",
            "",
            "def _add_auth_parsers_arguments(parser):",
            "    parser.add_argument('-a', '--auth-method',",
            "                        type=bigflow.deploy.AuthorizationType,",
            "                        default='local_account',",
            "                        help='One of two authentication method: '",
            "                             'local_account -- you are using credentials of your local user authenticated in gcloud; '",
            "                             'vault -- credentials for service account are obtained from Vault. '",
            "                             'Default: local_account',",
            "                        choices=list(bigflow.deploy.AuthorizationType),",
            "    )",
            "    parser.add_argument('-ve', '--vault-endpoint',",
            "                        type=str,",
            "                        help='URL of a Vault endpoint to get OAuth token for service account. '",
            "                             'Required if auth-method is vault. '",
            "                             'If not set, will be read from deployment_config.py.'",
            "                        )",
            "    parser.add_argument('-vs', '--vault-secret',",
            "                        type=str,",
            "                        help='Vault secret token. '",
            "                             'Required if auth-method is vault.'",
            "                        )",
            "    parser.add_argument('-dc', '--deployment-config-path',",
            "                        type=str,",
            "                        help='Path to the deployment_config.py file. '",
            "                             'If not set, {current_dir}/deployment_config.py will be used.')",
            "",
            "",
            "def _add_deploy_parsers_common_arguments(parser):",
            "    _add_auth_parsers_arguments(parser)",
            "    _add_parsers_common_arguments(parser)",
            "",
            "",
            "def _create_deploy_parser(subparsers):",
            "    parser = subparsers.add_parser('deploy',",
            "                                   description='Performs complete deployment. Uploads DAG files from local DAGs folder '",
            "                                               'to Composer and uploads Docker image to Container Registry.')",
            "",
            "    _add_deploy_dags_parser_arguments(parser)",
            "    _add_deploy_image_parser_arguments(parser)",
            "    _add_deploy_parsers_common_arguments(parser)",
            "",
            "",
            "def _create_deploy_image_parser(subparsers):",
            "    parser = subparsers.add_parser('deploy-image',",
            "                                   description='Uploads Docker image to Container Registry.'",
            "                                   )",
            "",
            "    _add_deploy_image_parser_arguments(parser)",
            "    _add_deploy_parsers_common_arguments(parser)",
            "",
            "",
            "def _create_deploy_dags_parser(subparsers):",
            "    parser = subparsers.add_parser('deploy-dags',",
            "                                   description='Uploads DAG files from local DAGs folder to Composer.')",
            "",
            "    _add_deploy_dags_parser_arguments(parser)",
            "    _add_deploy_parsers_common_arguments(parser)",
            "",
            "",
            "def _create_project_version_parser(subparsers):",
            "    parser = subparsers.add_parser('project-version', aliases=['pv'], description='Prints project version')",
            "    parser.add_argument(",
            "        '--git-commit',",
            "        type=str,",
            "        help=\"Return project version of specifid git commit\",",
            "    )",
            "",
            "",
            "def _create_release_parser(subparsers):",
            "    parser = subparsers.add_parser('release', description='Creates a new release tag')",
            "    parser.add_argument('-i', '--ssh-identity-file',",
            "                        type=str,",
            "                        help=\"Path to the identity file, used to authorize push to remote repository\"",
            "                             \" If not specified, default ssh configuration will be used.\")",
            "",
            "",
            "def _add_deploy_image_parser_arguments(parser):",
            "    parser.add_argument('-i', '--image-tar-path',",
            "                        type=str,",
            "                        help='Path to a Docker image file. The file name must contain version number with the following naming schema: image-{version}.tar')",
            "    parser.add_argument('-r', '--docker-repository',",
            "                        type=str,",
            "                        help='Name of a local and target Docker repository. Typically, a target repository is hosted by Google Cloud Container Registry.'",
            "                             ' If so, with the following naming schema: {HOSTNAME}/{PROJECT-ID}/{IMAGE}.'",
            "                        )",
            "",
            "def _add_deploy_dags_parser_arguments(parser):",
            "    parser.add_argument('-dd', '--dags-dir',",
            "                        type=str,",
            "                        help=\"Path to the folder with DAGs to deploy.\"",
            "                             \" If not set, {current_dir}/.dags will be used.\")",
            "    parser.add_argument('-cdf', '--clear-dags-folder',",
            "                        action='store_true',",
            "                        help=\"Clears the DAGs bucket before uploading fresh DAG files. \"",
            "                             \"Default: False\")",
            "",
            "    parser.add_argument('-p', '--gcp-project-id',",
            "                        help=\"Name of your Google Cloud Platform project.\"",
            "                             \" If not set, will be read from deployment_config.py\")",
            "",
            "    parser.add_argument('-b', '--dags-bucket',",
            "                        help=\"Name of the target Google Cloud Storage bucket which underlies DAGs folder of your Composer.\"",
            "                             \" If not set, will be read from deployment_config.py\")",
            "",
            "",
            "def read_project_package(args):",
            "    return args.project_package if hasattr(args, 'project_package') else None",
            "",
            "",
            "def _resolve_deployment_config_path(args):",
            "    if args.deployment_config_path:",
            "        return args.deployment_config_path",
            "    return os.path.join(os.getcwd(), 'deployment_config.py')",
            "",
            "",
            "def _resolve_dags_dir(args):",
            "    if args.dags_dir:",
            "        return args.dags_dir",
            "    return os.path.join(os.getcwd(), '.dags')",
            "",
            "",
            "def _resolve_vault_endpoint(args):",
            "    if args.auth_method == bigflow.deploy.AuthorizationType.VAULT:",
            "        return _resolve_property(args, 'vault_endpoint')",
            "    else:",
            "        return None",
            "",
            "",
            "def _resolve_property(args, property_name, ignore_value_error=False):",
            "    try:",
            "        cli_atr = getattr(args, property_name)",
            "        if cli_atr:",
            "            return cli_atr",
            "        else:",
            "            config = import_deployment_config(_resolve_deployment_config_path(args), property_name)",
            "            return config.resolve_property(property_name, args.config)",
            "    except ValueError:",
            "        if ignore_value_error:",
            "            return None",
            "        else:",
            "            raise",
            "",
            "",
            "def _cli_deploy_dags(args):",
            "    vault_secret = _resolve_property(args, 'vault_secret', ignore_value_error=True)",
            "    bigflow.deploy.deploy_dags_folder(dags_dir=_resolve_dags_dir(args),",
            "                       dags_bucket=_resolve_property(args, 'dags_bucket'),",
            "                       clear_dags_folder=args.clear_dags_folder,",
            "                       auth_method=args.auth_method,",
            "                       vault_endpoint=_resolve_vault_endpoint(args),",
            "                       vault_secret=vault_secret,",
            "                       project_id=_resolve_property(args, 'gcp_project_id')",
            "                       )",
            "",
            "",
            "def _cli_deploy_image(args):",
            "",
            "    docker_repository = _resolve_property(args, 'docker_repository')",
            "    vault_secret = _resolve_property(args, 'vault_secret', ignore_value_error=True)",
            "    vault_endpoint = _resolve_vault_endpoint(args)",
            "    image_tar_path = args.image_tar_path if args.image_tar_path else find_image_file()",
            "",
            "    bigflow.deploy.deploy_docker_image(",
            "        image_tar_path=image_tar_path,",
            "        auth_method=args.auth_method,",
            "        docker_repository=docker_repository,",
            "        vault_endpoint=vault_endpoint,",
            "        vault_secret=vault_secret,",
            "    )",
            "",
            "",
            "def find_image_file():",
            "",
            "    logger.debug(\"Scan folder .image\")",
            "    if not os.path.isdir(\".image\"):",
            "        raise ValueError(\"Directory .image does not exist\")",
            "",
            "    for f in os.listdir(\".image\"):",
            "        logger.debug(\"Found file %s\", f)",
            "",
            "        if fnmatch.fnmatch(f, \"*-*.tar\"):",
            "            logger.info(\"Found image located at .image/%s\", f)",
            "            return f\".image/{f}\"",
            "",
            "        if fnmatch.fnmatch(f, \"imageinfo-*.toml\"):",
            "            logger.info(\"Found image info file located at .image/%s\", f)",
            "            return f\".image/{f}\"",
            "",
            "    raise ValueError('File containing image to deploy not found')",
            "",
            "",
            "def _grab_image_cache_params(args):",
            "    if args.cache_from_image or args.cache_from_version:",
            "        logger.debug(\"Image caching is requested - create build image cache params obj\")",
            "        vault_secret = _resolve_property(args, 'vault_secret', ignore_value_error=True)",
            "        vault_endpoint = _resolve_vault_endpoint(args)",
            "        return bigflow.build.operate.BuildImageCacheParams(",
            "            auth_method=args.auth_method,",
            "            vault_endpoint=vault_endpoint,",
            "            vault_secret=vault_secret,",
            "            cache_from_version=args.cache_from_version,",
            "            cache_from_image=args.cache_from_image,",
            "        )",
            "    else:",
            "        logger.debug(\"No caching is requested - so just disable it completly\")",
            "        return None",
            "",
            "",
            "def _cli_build_image(args):",
            "    prj = bigflow.build.spec.get_project_spec()",
            "    bigflow.build.operate.build_image(",
            "        prj,",
            "        export_image_tar=args.export_image_tar,",
            "        cache_params=_grab_image_cache_params(args),",
            "    )",
            "",
            "",
            "def _cli_build_package():",
            "    prj = bigflow.build.spec.get_project_spec()",
            "    bigflow.build.operate.build_package(prj)",
            "",
            "",
            "def _cli_build_dags(args):",
            "    prj = bigflow.build.spec.get_project_spec()",
            "    bigflow.build.operate.build_dags(",
            "        prj,",
            "        start_time=args.start_time if _is_starttime_selected(args) else datetime.now().strftime(\"%Y-%m-%d %H:00:00\"),",
            "        workflow_id=args.workflow if _is_workflow_selected(args) else None,",
            "    )",
            "",
            "",
            "def _cli_build(args):",
            "    prj = bigflow.build.spec.get_project_spec()",
            "    bigflow.build.operate.build_project(",
            "        prj,",
            "        start_time=args.start_time if _is_starttime_selected(args) else datetime.now().strftime(\"%Y-%m-%d %H:00:00\"),",
            "        workflow_id=args.workflow if _is_workflow_selected(args) else None,",
            "        export_image_tar=args.export_image_tar,",
            "        cache_params=_grab_image_cache_params(args),",
            "    )",
            "",
            "",
            "def _create_build_requirements_parser(subparsers):",
            "    parser = subparsers.add_parser(",
            "        'build-requirements',",
            "        description=\"Compiles requirements.txt from *.in specs\",",
            "    )",
            "    parser.add_argument(",
            "        'in_file',",
            "        type=str,",
            "        nargs='?',",
            "        default=\"resources/requirements.in\",  # FIXME: read 'project_setup.py'",
            "    )",
            "",
            "",
            "def _create_codegen_parser(subparsers: argparse._SubParsersAction):",
            "    parser = subparsers.add_parser('codegen', description=\"Various codegeneration tools\")",
            "    ss = parser.add_subparsers()",
            "",
            "    p = ss.add_parser('pin-dataflow-requirements')",
            "    p.set_defaults(func=_cli_codegen_pin_dataflow_requirements)",
            "",
            "",
            "def _cli_build_requirements(args):",
            "    in_file = pathlib.Path(args.in_file)",
            "    bigflow.build.pip.pip_compile(in_file)",
            "",
            "",
            "def _cli_codegen(args):",
            "    args.func(args)",
            "",
            "",
            "def _cli_codegen_pin_dataflow_requirements(args):",
            "    import bigflow.build.dataflow.dependency_checker as dc",
            "    dc.sync_requirements_with_dataflow_workers()",
            "",
            "",
            "def _is_workflow_selected(args):",
            "    return args.workflow and args.workflow != 'ALL'",
            "",
            "",
            "def _is_starttime_selected(args):",
            "    return args.start_time and args.start_time != 'NOW'",
            "",
            "",
            "def project_type_input():",
            "    project_type = input(\"Would you like to create basic or advanced project? Default basic. Type 'a' for advanced.\\n\")",
            "    return project_type if project_type else 'b'",
            "",
            "",
            "def project_number_input():",
            "    project_number = input('How many GCP projects would you like to use? '",
            "                           'It allows to deploy your workflows to more than one project. Default 2\\n')",
            "    return project_number if project_number else '2'",
            "",
            "",
            "def gcloud_project_list():",
            "    return subprocess.getoutput('gcloud projects list')",
            "",
            "",
            "def get_default_project_from_gcloud():",
            "    return subprocess.getoutput('gcloud config get-value project')",
            "",
            "",
            "def project_id_input(n):",
            "    if n == 0:",
            "        project = input(f'Enter a GCP project ID that you are going to use in your BigFlow project. '",
            "                        f'Choose a project from the list above. '",
            "                        f'If not provided default project: {get_default_project_from_gcloud()} will be used.\\n')",
            "    else:",
            "        project = input(f'Enter a #{n} GCP project ID that you are going to use in your BigFlow project. '",
            "                        f'Choose a project from the list above.'",
            "                        f' If not provided default project: {get_default_project_from_gcloud()} will be used.\\n')",
            "    return project",
            "",
            "",
            "def gcp_project_flow(n):",
            "    projects_list = gcloud_project_list()",
            "    print(projects_list)",
            "    return gcp_project_input(n, projects_list)",
            "",
            "",
            "def gcp_project_input(n, projects_list):",
            "    project = project_id_input(n)",
            "    if project == '':",
            "        return get_default_project_from_gcloud()",
            "    if project not in projects_list:",
            "        print(f'You do not have access to {project}. Try another project from the list.\\n')",
            "        return gcp_project_input(n, projects_list)",
            "    return project",
            "",
            "",
            "def gcp_bucket_input():",
            "    return input('Enter a Cloud Composer Bucket name where DAG files will be stored.\\n')",
            "",
            "",
            "def environment_name_input(envs):",
            "    environment_name = input('Enter an environment name. Default dev\\n')",
            "    if environment_name in envs:",
            "        print(f'Environment with name{environment_name} is already defined. Try another name.\\n')",
            "        return environment_name_input(envs)",
            "    return environment_name if environment_name else 'dev'",
            "",
            "",
            "def project_name_input():",
            "    return input('Enter the project name. It should be valid python package name. '",
            "                 'It will be used as a main directory of your project and bucket name used by dataflow to run jobs.\\n')",
            "",
            "",
            "def _cli_start_project():",
            "    config = {'is_basic': False, 'project_name': project_name_input(), 'projects_id': [], 'composers_bucket': [], 'envs': []}",
            "    if False:",
            "        for n in range(0, int(project_number_input())):",
            "            config['projects_id'].append(gcp_project_flow(n))",
            "            config['composers_bucket'].append(gcp_bucket_input())",
            "            config['envs'].append(environment_name_input(config['envs']))",
            "    else:",
            "        config['is_basic'] = True",
            "        config['projects_id'].append(gcp_project_flow(0))",
            "        config['composers_bucket'].append(gcp_bucket_input())",
            "",
            "        config['pyspark_job'] = True",
            "",
            "    bigflow.scaffold.start_project(**config)",
            "    print('Bigflow project created successfully.')",
            "",
            "",
            "def _cli_project_version(args):",
            "    commit_ish = args.git_commit or \"HEAD\"",
            "    print(bigflow.version.get_version(commit_ish))",
            "",
            "",
            "def _cli_release(args):",
            "    bigflow.version.release(args.ssh_identity_file)",
            "",
            "",
            "class _ConsoleStreamLogHandler(logging.Handler):",
            "    \"\"\"",
            "    A handler class which writes logging records to `sys.stderr`.",
            "",
            "    When `sys.stderr` is a TTY device a new line is not appended to log",
            "    records with `incomplete_line` attribute set to true.  This allows to",
            "    propagate progress bars / dynamic prints produced by child processes.",
            "    See `bigflow.commons.run_process` for more details.",
            "    \"\"\"",
            "",
            "    # TODO: consider moving this class & `init_console_logging` fn to \"log.py\" module",
            "    # Not it is not poss`ible, as 'log.py' *must* be",
            "    # non-imporable when 'google.cloud.logging' is not installed",
            "    # making 'log.py' always importable will require refactoring",
            "",
            "    def __init__(self):",
            "        logging.Handler.__init__(self)",
            "        self.last_incomplete_msg = \"\"",
            "        self.stream = sys.stderr",
            "        self.isatty = self.stream.isatty()",
            "",
            "    def emit(self, record: logging.LogRecord):",
            "        incomplete_line = getattr(record, 'incomplete_line', False)",
            "",
            "        try:",
            "            last_msg = self.last_incomplete_msg",
            "            msg = self.format(record)",
            "",
            "            if msg.startswith(last_msg):",
            "                msg = msg[len(last_msg):]",
            "            elif self.isatty:",
            "                msg = \"\\r\\033[K\" + msg",
            "            else:",
            "                msg = \"\\n\" + msg",
            "",
            "            with self.lock:",
            "                if not incomplete_line:",
            "                    msg += \"\\n\"",
            "                    self.last_incomplete_msg = \"\"",
            "                else:",
            "                    self.last_incomplete_msg = msg",
            "                self.stream.write(msg)",
            "                self.stream.flush()",
            "",
            "        except RecursionError:  # See issue 36272",
            "            raise",
            "        except Exception:",
            "            self.handleError(record)",
            "",
            "",
            "def init_console_logging(verbose):",
            "    verbose = verbose or os.environ.get('BIGFLOW_VERBOSE', \"\")",
            "    if verbose:",
            "        logging.basicConfig(",
            "            level=logging.DEBUG,",
            "            format=\"%(asctime)s| %(message)s\",",
            "            handlers=[_ConsoleStreamLogHandler()],",
            "        )",
            "    else:",
            "        logging.basicConfig(",
            "            level=logging.INFO,",
            "            format=\"%(message)s\",",
            "            handlers=[_ConsoleStreamLogHandler()],",
            "        )",
            "",
            "",
            "def cli_logs(root_package):",
            "    import bigflow.log as log",
            "",
            "    projects_id = []",
            "    workflows_links = {}",
            "    for workflow in walk_workflows(root_package):",
            "        if workflow.log_config:",
            "            projects_id.append((workflow.log_config['gcp_project_id'], workflow.workflow_id))",
            "            workflows_links[workflow.workflow_id] = log.workflow_logs_link_for_cli(workflow.log_config, workflow.workflow_id)",
            "    if not projects_id:",
            "        raise Exception(\"Found no workflows with configured logging.\")",
            "    deduplicated_projects_id = sorted(set(projects_id), key=lambda x: projects_id.index(x))",
            "    infra_links = log.infrastructure_logs_link_for_cli(deduplicated_projects_id)",
            "    log.print_log_links_message(workflows_links, infra_links)",
            "",
            "",
            "def _is_log_module_installed():",
            "    try:",
            "        import bigflow.log",
            "        return True",
            "    except ImportError:",
            "        raise Exception(\"bigflow.log module not found. You need install bigflow with 'log' extras.\")",
            "",
            "",
            "def cli(raw_args) -> None:",
            "    bigflow.build.dev.install_syspath()",
            "    bigflow.migrate.check_migrate()",
            "",
            "    project_name = read_project_name_from_setup()",
            "    parsed_args = _parse_args(project_name, raw_args)",
            "    init_console_logging(parsed_args.verbose)",
            "",
            "    operation: str = parsed_args.operation",
            "",
            "    if operation == 'run':",
            "        set_configuration_env(parsed_args.config)",
            "        root_package = find_root_package(project_name, read_project_package(parsed_args))",
            "        cli_run(root_package, parsed_args.runtime, parsed_args.job, parsed_args.workflow)",
            "    elif operation == 'deploy-image':",
            "        _cli_deploy_image(parsed_args)",
            "    elif operation == 'deploy-dags':",
            "        _cli_deploy_dags(parsed_args)",
            "    elif operation == 'deploy':",
            "        _cli_deploy_image(parsed_args)",
            "        _cli_deploy_dags(parsed_args)",
            "    elif operation == 'build-dags':",
            "        _cli_build_dags(parsed_args)",
            "    elif operation == 'build-image':",
            "        _cli_build_image(parsed_args)",
            "    elif operation == 'build-package':",
            "        _cli_build_package()",
            "    elif operation == 'build':",
            "        _cli_build(parsed_args)",
            "    elif operation == 'start-project':",
            "        _cli_start_project()",
            "    elif operation == 'project-version' or operation == 'pv':",
            "        _cli_project_version(parsed_args)",
            "    elif operation == 'release':",
            "        _cli_release(parsed_args)",
            "    elif operation == 'build-requirements':",
            "        _cli_build_requirements(parsed_args)",
            "    elif operation == 'codegen':",
            "        _cli_codegen(parsed_args)",
            "    else:",
            "        raise ValueError(f'Operation unknown - {operation}')",
            "",
            "    logger.debug(\"bigflow cli finished\")"
        ],
        "afterPatchFile": [
            "import argparse",
            "import importlib",
            "import os",
            "import pathlib",
            "import subprocess",
            "import sys",
            "import logging",
            "",
            "import importlib.util",
            "",
            "from argparse import Namespace",
            "from datetime import datetime",
            "from importlib import import_module",
            "from pathlib import Path",
            "from types import ModuleType",
            "from typing import Tuple, Iterator, Optional",
            "import fnmatch",
            "",
            "import bigflow as bf",
            "import bigflow.build.pip",
            "import bigflow.resources",
            "import bigflow.commons as bf_commons",
            "import bigflow.build.dist",
            "import bigflow.build.dev",
            "import bigflow.build.operate",
            "import bigflow.build.spec",
            "import bigflow.migrate",
            "import bigflow.deploy",
            "import bigflow.scaffold",
            "import bigflow.version",
            "",
            "from bigflow import Config",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "def walk_module_files(root_package: Path) -> Iterator[Tuple[str, str]]:",
            "    \"\"\"",
            "    Returning all the Python files in the `root_package`",
            "",
            "    Example:",
            "    walk_module_files(Path(\"fld\")) -> [(\"/path/to/fld\", \"file1\"), (\"/path/to/fld\", \"file2\")]",
            "",
            "    @return: (absolute_path: str, name: str)",
            "    \"\"\"",
            "    logger.debug(\"walk module files %r\", root_package)",
            "    for subdir, dirs, files in os.walk(str(root_package)):",
            "        for file in files:",
            "            if file.endswith('.py'):",
            "                logger.debug(\"found python file %s/%s\", subdir, file)",
            "                yield subdir, file",
            "",
            "",
            "def _removesuffix(s, suffix):",
            "    return s[:-len(suffix)] if s.endswith(suffix) else s",
            "",
            "",
            "def build_module_path(root_package: Path, module_dir: Path, module_file: str) -> str:",
            "    \"\"\"",
            "    Returns module path that can be imported using `import_module`",
            "    \"\"\"",
            "    full_module_file_path = str(module_dir.absolute() / module_file)",
            "    full_module_file_path = full_module_file_path.replace(str(root_package.parent.absolute()), '')",
            "",
            "    res = full_module_file_path",
            "    res = _removesuffix(res, \".py\")",
            "    res = _removesuffix(res, \"/__init__\")",
            "    res = res.lstrip(\"/\").replace(os.sep, \".\")",
            "    return res",
            "",
            "",
            "def walk_module_paths(root_package: Path) -> Iterator[str]:",
            "    \"\"\"",
            "    Returning all the module paths in the `root_package`",
            "    \"\"\"",
            "    logger.debug(\"walk module paths, root %r\", root_package)",
            "    for module_dir, module_file in walk_module_files(root_package):",
            "        mpath = build_module_path(root_package, Path(module_dir), module_file)",
            "        logger.debug(\"%s / %s / %s resolved to module %r\", root_package, module_dir, module_file, mpath)",
            "        logger.debug(\"path %r\", mpath)",
            "        yield mpath",
            "",
            "",
            "def walk_modules(root_package: Path) -> Iterator[ModuleType]:",
            "    \"\"\"",
            "    Imports all the modules in the path and returns",
            "    \"\"\"",
            "    logger.debug(\"walk modules, root %r\", root_package)",
            "    for module_path in walk_module_paths(root_package):",
            "        try:",
            "            logger.debug(\"import module %r\", module_path)",
            "            logger.debug(\"%r\", sys.path)",
            "            yield import_module(module_path)",
            "        except ValueError as e:",
            "            print(f\"Skipping module {module_path}. Can't import due to exception {str(e)}.\")",
            "",
            "",
            "def walk_module_objects(module: ModuleType, expect_type: type) -> Iterator[Tuple[str, type]]:",
            "    \"\"\"",
            "    Returns module items of the set type",
            "    \"\"\"",
            "    logger.debug(\"scan module %r for object of type %r\", module, expect_type)",
            "    for name, obj in module.__dict__.items():",
            "        if isinstance(obj, expect_type):",
            "            yield name, obj",
            "",
            "",
            "def walk_workflows(root_package: Path) -> Iterator[bf.Workflow]:",
            "    \"\"\"",
            "    Imports modules in the `root_package` and returns all the elements of the type bf.Workflow",
            "    \"\"\"",
            "    logger.debug(\"walk workflows, root %s\", root_package)",
            "    for module in walk_modules(root_package):",
            "        for name, workflow in walk_module_objects(module, bf.Workflow):",
            "            yield workflow",
            "",
            "",
            "def find_workflow(root_package: Path, workflow_id: str) -> bf.Workflow:",
            "    \"\"\"",
            "    Imports modules and finds the workflow with id workflow_id",
            "    \"\"\"",
            "    logger.debug(\"find workflow, root %s, workflow_id %r\", root_package, workflow_id)",
            "    for workflow in walk_workflows(root_package):",
            "        if workflow.workflow_id == workflow_id:",
            "            return workflow",
            "    raise ValueError('Workflow with id {} not found in package {}'.format(workflow_id, root_package))",
            "",
            "",
            "def set_configuration_env(env):",
            "    \"\"\"",
            "    Sets 'bf_env' env variable",
            "    \"\"\"",
            "    if env is not None:",
            "        os.environ['bf_env'] = env",
            "        print(f\"bf_env is : {os.environ.get('bf_env', None)}\")",
            "",
            "",
            "def execute_job(root_package: Path, workflow_id: str, job_id: str, runtime=None):",
            "    \"\"\"",
            "    Executes the job with the `workflow_id`, with job id `job_id`",
            "",
            "    @param runtime: str determine partition that will be used for write operations.",
            "    \"\"\"",
            "    w = find_workflow(root_package, workflow_id)",
            "    w.run_job(job_id, runtime)",
            "",
            "",
            "def execute_workflow(root_package: Path, workflow_id: str, runtime=None):",
            "    \"\"\"",
            "    Executes the workflow with the `workflow_id`",
            "",
            "    @param runtime: str determine partition that will be used for write operations.",
            "    \"\"\"",
            "    w = find_workflow(root_package, workflow_id)",
            "    w.run(runtime)",
            "",
            "",
            "def read_project_name_from_setup() -> Optional[str]:",
            "    logger.debug(\"Read project name from project spec\")",
            "    try:",
            "        return bigflow.build.spec.get_project_spec().name",
            "    except Exception as e:",
            "        logger.warning(\"Unable to read project name: %s\", e)",
            "        return None",
            "",
            "",
            "def find_root_package(project_name: Optional[str], project_dir: Optional[str]) -> Path:",
            "    \"\"\"",
            "    Finds project package path. Tries first to find location in project_setup.PROJECT_NAME,",
            "    and if not found then by making a path to the `root` module",
            "",
            "    @param project_dir: Path to the root package of a project, used only when PROJECT_NAME not set",
            "    @return: Path",
            "    \"\"\"",
            "    if project_name is not None:",
            "        project_name = project_name.replace(\"-\", \"_\")",
            "        return Path(project_name)",
            "    else:",
            "        root_module = import_module(project_dir)",
            "        return Path(root_module.__file__.replace('__init__.py', ''))",
            "",
            "",
            "def import_deployment_config(deployment_config_path: str, property_name: str):",
            "    if not Path(deployment_config_path).exists():",
            "        raise ValueError(f\"Can't find deployment_config.py at '{deployment_config_path}'. \"",
            "                         f\"Property '{property_name}' can't be resolved. \"",
            "                          \"If your deployment_config.py is elswhere, \"",
            "                          \"you can set path to it using --deployment-config-path. If you are not using deployment_config.py -- \"",
            "                         f\"set '{property_name}' property as a command line argument.\")",
            "    spec = importlib.util.spec_from_file_location('deployment_config', deployment_config_path)",
            "",
            "    if not spec:",
            "        raise ValueError(f'Failed to load deployment_config from {deployment_config_path}. '",
            "        'Create a proper deployment_config.py file'",
            "        'or set all the properties via command line arguments.')",
            "",
            "    deployment_config_module = importlib.util.module_from_spec(spec)",
            "    spec.loader.exec_module(deployment_config_module)",
            "",
            "    if not isinstance(deployment_config_module.deployment_config, Config):",
            "        raise ValueError('deployment_config attribute in deployment_config.py should be instance of bigflow.Config')",
            "",
            "    return deployment_config_module.deployment_config",
            "",
            "",
            "def cli_run(project_package: str,",
            "            runtime: Optional[str] = None,",
            "            full_job_id: Optional[str] = None,",
            "            workflow_id: Optional[str] = None) -> None:",
            "    \"\"\"",
            "    Runs the specified job or workflow",
            "",
            "    @param project_package: str The main package of a user's project",
            "    @param runtime: Optional[str] Date of XXX in format \"%Y-%m-%d %H:%M:%S\"",
            "    @param full_job_id: Optional[str] Represents both workflow_id and job_id in a string in format \"<workflow_id>.<job_id>\"",
            "    @param workflow_id: Optional[str] The id of the workflow that should be executed",
            "    @return:",
            "    \"\"\"",
            "",
            "    # TODO: Check that installed libs in sync with `requirements.txt`",
            "    bigflow.build.pip.check_requirements_needs_recompile(Path(\"resources/requirements.txt\"))",
            "",
            "    if full_job_id is not None:",
            "        try:",
            "            workflow_id, job_id = full_job_id.split('.')",
            "        except ValueError:",
            "            raise ValueError(",
            "                'You should specify job using the workflow_id and job_id parameters - --job <workflow_id>.<job_id>.')",
            "        execute_job(project_package, workflow_id, job_id, runtime=runtime)",
            "    elif workflow_id is not None:",
            "        execute_workflow(project_package, workflow_id, runtime=runtime)",
            "    else:",
            "        raise ValueError('You must provide the --job or --workflow for the run command.')",
            "",
            "",
            "def _parse_args(project_name: Optional[str], args) -> Namespace:",
            "    parser = argparse.ArgumentParser(description=f'Welcome to BigFlow CLI.'",
            "                                                  '\\nType: bigflow {command} -h to print detailed help for a selected command.')",
            "    parser.add_argument(",
            "        \"-v\",",
            "        \"--verbose\",",
            "        action='store_true',",
            "        default=False,",
            "        help=\"Print verbose output for debugging\",",
            "    )",
            "",
            "    subparsers = parser.add_subparsers(dest='operation',",
            "                                       required=True,",
            "                                       help='BigFlow command to execute')",
            "",
            "    _create_run_parser(subparsers, project_name)",
            "    _create_deploy_dags_parser(subparsers)",
            "    _create_deploy_image_parser(subparsers)",
            "    _create_deploy_parser(subparsers)",
            "",
            "    _create_build_dags_parser(subparsers)",
            "    _create_build_image_parser(subparsers)",
            "    _create_build_package_parser(subparsers)",
            "    _create_build_parser(subparsers)",
            "",
            "    _create_project_version_parser(subparsers)",
            "    _create_release_parser(subparsers)",
            "    _create_start_project_parser(subparsers)",
            "    _create_logs_parser(subparsers)",
            "",
            "    _create_build_requirements_parser(subparsers)",
            "",
            "    _create_codegen_parser(subparsers)",
            "",
            "    return parser.parse_args(args)",
            "",
            "",
            "def _create_logs_parser(subparsers):",
            "    subparsers.add_parser('logs', description='Returns a link leading to a workflow logs in GCP Logging.')",
            "",
            "",
            "def _create_start_project_parser(subparsers):",
            "    subparsers.add_parser('start-project', description='Creates a scaffolding project in a current directory.')",
            "",
            "",
            "def _create_build_parser(subparsers):",
            "    parser = subparsers.add_parser('build', description='Builds a Docker image, DAG files and .whl package from local sources.')",
            "    _add_build_dags_parser_arguments(parser)",
            "    _add_build_image_parser_arguments(parser)",
            "    _add_parsers_common_arguments(parser)",
            "",
            "",
            "def _create_build_package_parser(subparsers):",
            "    subparsers.add_parser('build-package', description='Builds .whl package from local sources.')",
            "",
            "",
            "def _add_build_dags_parser_arguments(parser):",
            "    parser.add_argument('-w', '--workflow',",
            "                        type=str,",
            "                        help=\"Skip or set to ALL to build DAGs from all workflows. \"",
            "                             \"Set a workflow Id to build selected workflow only. \"",
            "                             \"For example to build only this workflow: bigflow.Workflow(workflow_id='workflow1',\"",
            "                             \" definition=[ExampleJob('job1')]) you should use --workflow workflow1\")",
            "    parser.add_argument('-t', '--start-time',",
            "                        help='The first runtime of a workflow. '",
            "                             'For workflows triggered hourly -- datetime in format: Y-m-d H:M:S, for example 2020-01-01 00:00:00. '",
            "                             'For workflows triggered daily -- date in format: Y-m-d, for example 2020-01-01. '",
            "                             'If empty or set as NOW, current hour is used.',",
            "                        type=bf_commons.valid_datetime)",
            "",
            "",
            "def _add_build_image_parser_arguments(parser: argparse.ArgumentParser):",
            "    parser.add_argument(",
            "        '--export-image-tar', dest='export_image_tar', action='store_true',",
            "        help=\"Export built docker image into .tar file\",",
            "    )",
            "    parser.add_argument(",
            "        '--no-export-image-tar', dest='export_image_tar', action='store_false',",
            "        help=\"Don't export built docker image into .tar file (keep image in local docker registry)\",",
            "    )",
            "    parser.set_defaults(export_image_tar=None)",
            "",
            "    parser.add_argument(",
            "        '--cache-from-image',",
            "        dest='cache_from_image',",
            "        action='append',",
            "        help=\"Docker images to consider as cache sources\",",
            "    )",
            "    parser.add_argument(",
            "        '--cache-from-version',",
            "        dest='cache_from_version',",
            "        action='append',",
            "        help=\"Use previous version of the project as cache source\",",
            "    )",
            "    _add_auth_parsers_arguments(parser)",
            "",
            "",
            "def _create_build_dags_parser(subparsers):",
            "    parser = subparsers.add_parser('build-dags',",
            "                                   description='Builds DAG files from local sources to {current_dir}/.dags')",
            "    _add_build_dags_parser_arguments(parser)",
            "",
            "",
            "def _create_build_image_parser(subparsers):",
            "    parser = subparsers.add_parser(",
            "        'build-image',",
            "        description='Builds a docker image from local files.',",
            "    )",
            "    _add_build_image_parser_arguments(parser)",
            "    _add_parsers_common_arguments(parser)",
            "",
            "",
            "def _create_run_parser(subparsers, project_name):",
            "    parser = subparsers.add_parser('run',",
            "                                   description='BigFlow CLI run command -- run a workflow or job')",
            "",
            "    group = parser.add_mutually_exclusive_group()",
            "    group.required = True",
            "    group.add_argument('-j', '--job',",
            "                       type=str,",
            "                       help='The job to start, identified by workflow id and job id in format \"<workflow_id>.<job_id>\".')",
            "    group.add_argument('-w', '--workflow',",
            "                       type=str,",
            "                       help='The id of the workflow to start.')",
            "    parser.add_argument('-r', '--runtime',",
            "                        type=str, default=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),",
            "                        help='The date and time when this job or workflow should be started. '",
            "                             'The default is now (%(default)s). '",
            "                             'Examples: 2019-01-01, 2020-01-01 01:00:00')",
            "    _add_parsers_common_arguments(parser)",
            "",
            "    if project_name is None:",
            "        parser.add_argument('--project-package',",
            "                            required=True,",
            "                            type=str,",
            "                            help='The main package of your project. '",
            "                                 'Should contain `setup.py`')",
            "",
            "",
            "def _add_parsers_common_arguments(parser):",
            "    parser.add_argument('-c', '--config',",
            "                        type=str,",
            "                        help='Config environment name that should be used. For example: dev, prod.'",
            "                             ' If not set, default Config name will be used.'",
            "                             ' This env name is applied to all bigflow.Config objects that are defined by'",
            "                             ' individual workflows as well as to deployment_config.py.')",
            "",
            "",
            "def _add_auth_parsers_arguments(parser):",
            "    class VaultEndpointVerifyAction(argparse.Action):",
            "        def __call__(self, parser, args, values, option_string=None):",
            "            if values in ['true', 'false']:",
            "                setattr(args, self.dest, values == 'true')",
            "            else:",
            "                setattr(args, self.dest, str(values))",
            "",
            "    parser.add_argument('-a', '--auth-method',",
            "                        type=bigflow.deploy.AuthorizationType,",
            "                        default='local_account',",
            "                        help='One of two authentication method: '",
            "                             'local_account -- you are using credentials of your local user authenticated in gcloud; '",
            "                             'vault -- credentials for service account are obtained from Vault. '",
            "                             'Default: local_account',",
            "                        choices=list(bigflow.deploy.AuthorizationType),",
            "    )",
            "    parser.add_argument('-ve', '--vault-endpoint',",
            "                        type=str,",
            "                        help='URL of a Vault endpoint to get OAuth token for service account. '",
            "                             'Required if auth-method is vault. '",
            "                             'If not set, will be read from deployment_config.py.'",
            "                        )",
            "    parser.add_argument('-vev', '--vault-endpoint-verify',",
            "                        type=str,",
            "                        action=VaultEndpointVerifyAction,",
            "                        help='Can be \"true\", \"false\", a path to certificate PEM file or a path to directory with PEM files. '",
            "                             'Enables/disables vault endpoint TLS certificate verification. Enabled by default. '",
            "                             'Disabling makes execution vulnerable for MITM attacks - do it only when justified and in trusted environments. '",
            "                             'For details see: https://requests.readthedocs.io/en/latest/user/advanced/#ssl-cert-verification',",
            "                        dest='vault_endpoint_verify',",
            "                        default=True",
            "                        )",
            "    parser.add_argument('-vs', '--vault-secret',",
            "                        type=str,",
            "                        help='Vault secret token. '",
            "                             'Required if auth-method is vault.'",
            "                        )",
            "    parser.add_argument('-dc', '--deployment-config-path',",
            "                        type=str,",
            "                        help='Path to the deployment_config.py file. '",
            "                             'If not set, {current_dir}/deployment_config.py will be used.')",
            "",
            "",
            "def _add_deploy_parsers_common_arguments(parser):",
            "    _add_auth_parsers_arguments(parser)",
            "    _add_parsers_common_arguments(parser)",
            "",
            "",
            "def _create_deploy_parser(subparsers):",
            "    parser = subparsers.add_parser('deploy',",
            "                                   description='Performs complete deployment. Uploads DAG files from local DAGs folder '",
            "                                               'to Composer and uploads Docker image to Container Registry.')",
            "",
            "    _add_deploy_dags_parser_arguments(parser)",
            "    _add_deploy_image_parser_arguments(parser)",
            "    _add_deploy_parsers_common_arguments(parser)",
            "",
            "",
            "def _create_deploy_image_parser(subparsers):",
            "    parser = subparsers.add_parser('deploy-image',",
            "                                   description='Uploads Docker image to Container Registry.'",
            "                                   )",
            "",
            "    _add_deploy_image_parser_arguments(parser)",
            "    _add_deploy_parsers_common_arguments(parser)",
            "",
            "",
            "def _create_deploy_dags_parser(subparsers):",
            "    parser = subparsers.add_parser('deploy-dags',",
            "                                   description='Uploads DAG files from local DAGs folder to Composer.')",
            "",
            "    _add_deploy_dags_parser_arguments(parser)",
            "    _add_deploy_parsers_common_arguments(parser)",
            "",
            "",
            "def _create_project_version_parser(subparsers):",
            "    parser = subparsers.add_parser('project-version', aliases=['pv'], description='Prints project version')",
            "    parser.add_argument(",
            "        '--git-commit',",
            "        type=str,",
            "        help=\"Return project version of specifid git commit\",",
            "    )",
            "",
            "",
            "def _create_release_parser(subparsers):",
            "    parser = subparsers.add_parser('release', description='Creates a new release tag')",
            "    parser.add_argument('-i', '--ssh-identity-file',",
            "                        type=str,",
            "                        help=\"Path to the identity file, used to authorize push to remote repository\"",
            "                             \" If not specified, default ssh configuration will be used.\")",
            "",
            "",
            "def _add_deploy_image_parser_arguments(parser):",
            "    parser.add_argument('-i', '--image-tar-path',",
            "                        type=str,",
            "                        help='Path to a Docker image file. The file name must contain version number with the following naming schema: image-{version}.tar')",
            "    parser.add_argument('-r', '--docker-repository',",
            "                        type=str,",
            "                        help='Name of a local and target Docker repository. Typically, a target repository is hosted by Google Cloud Container Registry.'",
            "                             ' If so, with the following naming schema: {HOSTNAME}/{PROJECT-ID}/{IMAGE}.'",
            "                        )",
            "",
            "def _add_deploy_dags_parser_arguments(parser):",
            "    parser.add_argument('-dd', '--dags-dir',",
            "                        type=str,",
            "                        help=\"Path to the folder with DAGs to deploy.\"",
            "                             \" If not set, {current_dir}/.dags will be used.\")",
            "    parser.add_argument('-cdf', '--clear-dags-folder',",
            "                        action='store_true',",
            "                        help=\"Clears the DAGs bucket before uploading fresh DAG files. \"",
            "                             \"Default: False\")",
            "",
            "    parser.add_argument('-p', '--gcp-project-id',",
            "                        help=\"Name of your Google Cloud Platform project.\"",
            "                             \" If not set, will be read from deployment_config.py\")",
            "",
            "    parser.add_argument('-b', '--dags-bucket',",
            "                        help=\"Name of the target Google Cloud Storage bucket which underlies DAGs folder of your Composer.\"",
            "                             \" If not set, will be read from deployment_config.py\")",
            "",
            "",
            "def read_project_package(args):",
            "    return args.project_package if hasattr(args, 'project_package') else None",
            "",
            "",
            "def _resolve_deployment_config_path(args):",
            "    if args.deployment_config_path:",
            "        return args.deployment_config_path",
            "    return os.path.join(os.getcwd(), 'deployment_config.py')",
            "",
            "",
            "def _resolve_dags_dir(args):",
            "    if args.dags_dir:",
            "        return args.dags_dir",
            "    return os.path.join(os.getcwd(), '.dags')",
            "",
            "",
            "def _resolve_vault_endpoint(args):",
            "    if args.auth_method == bigflow.deploy.AuthorizationType.VAULT:",
            "        return _resolve_property(args, 'vault_endpoint')",
            "    else:",
            "        return None",
            "",
            "",
            "def _resolve_property(args, property_name, ignore_value_error=False):",
            "    try:",
            "        cli_atr = getattr(args, property_name)",
            "        if cli_atr or cli_atr is False:",
            "            return cli_atr",
            "        else:",
            "            config = import_deployment_config(_resolve_deployment_config_path(args), property_name)",
            "            return config.resolve_property(property_name, args.config)",
            "    except ValueError:",
            "        if ignore_value_error:",
            "            return None",
            "        else:",
            "            raise",
            "",
            "",
            "def _cli_deploy_dags(args):",
            "    vault_secret = _resolve_property(args, 'vault_secret', ignore_value_error=True)",
            "    bigflow.deploy.deploy_dags_folder(dags_dir=_resolve_dags_dir(args),",
            "                       dags_bucket=_resolve_property(args, 'dags_bucket'),",
            "                       clear_dags_folder=args.clear_dags_folder,",
            "                       auth_method=args.auth_method,",
            "                       vault_endpoint=_resolve_vault_endpoint(args),",
            "                       vault_endpoint_verify=_resolve_property(args, 'vault_endpoint_verify', ignore_value_error=True),",
            "                       vault_secret=vault_secret,",
            "                       project_id=_resolve_property(args, 'gcp_project_id')",
            "                       )",
            "",
            "",
            "def _cli_deploy_image(args):",
            "",
            "    docker_repository = _resolve_property(args, 'docker_repository')",
            "    vault_secret = _resolve_property(args, 'vault_secret', ignore_value_error=True)",
            "    vault_endpoint = _resolve_vault_endpoint(args)",
            "    vault_endpoint_verify = _resolve_property(args, 'vault_endpoint_verify', ignore_value_error=True)",
            "    image_tar_path = args.image_tar_path if args.image_tar_path else find_image_file()",
            "",
            "    bigflow.deploy.deploy_docker_image(",
            "        image_tar_path=image_tar_path,",
            "        auth_method=args.auth_method,",
            "        docker_repository=docker_repository,",
            "        vault_endpoint=vault_endpoint,",
            "        vault_endpoint_verify=vault_endpoint_verify,",
            "        vault_secret=vault_secret,",
            "    )",
            "",
            "",
            "def find_image_file():",
            "",
            "    logger.debug(\"Scan folder .image\")",
            "    if not os.path.isdir(\".image\"):",
            "        raise ValueError(\"Directory .image does not exist\")",
            "",
            "    for f in os.listdir(\".image\"):",
            "        logger.debug(\"Found file %s\", f)",
            "",
            "        if fnmatch.fnmatch(f, \"*-*.tar\"):",
            "            logger.info(\"Found image located at .image/%s\", f)",
            "            return f\".image/{f}\"",
            "",
            "        if fnmatch.fnmatch(f, \"imageinfo-*.toml\"):",
            "            logger.info(\"Found image info file located at .image/%s\", f)",
            "            return f\".image/{f}\"",
            "",
            "    raise ValueError('File containing image to deploy not found')",
            "",
            "",
            "def _grab_image_cache_params(args):",
            "    if args.cache_from_image or args.cache_from_version:",
            "        logger.debug(\"Image caching is requested - create build image cache params obj\")",
            "        vault_secret = _resolve_property(args, 'vault_secret', ignore_value_error=True)",
            "        vault_endpoint = _resolve_vault_endpoint(args)",
            "        vault_endpoint_verify = _resolve_property(args, 'vault_endpoint_verify', ignore_value_error=True)",
            "        return bigflow.build.operate.BuildImageCacheParams(",
            "            auth_method=args.auth_method,",
            "            vault_endpoint=vault_endpoint,",
            "            vault_secret=vault_secret,",
            "            cache_from_version=args.cache_from_version,",
            "            cache_from_image=args.cache_from_image,",
            "            vault_endpoint_verify=vault_endpoint_verify",
            "        )",
            "    else:",
            "        logger.debug(\"No caching is requested - so just disable it completly\")",
            "        return None",
            "",
            "",
            "def _cli_build_image(args):",
            "    prj = bigflow.build.spec.get_project_spec()",
            "    bigflow.build.operate.build_image(",
            "        prj,",
            "        export_image_tar=args.export_image_tar,",
            "        cache_params=_grab_image_cache_params(args),",
            "    )",
            "",
            "",
            "def _cli_build_package():",
            "    prj = bigflow.build.spec.get_project_spec()",
            "    bigflow.build.operate.build_package(prj)",
            "",
            "",
            "def _cli_build_dags(args):",
            "    prj = bigflow.build.spec.get_project_spec()",
            "    bigflow.build.operate.build_dags(",
            "        prj,",
            "        start_time=args.start_time if _is_starttime_selected(args) else datetime.now().strftime(\"%Y-%m-%d %H:00:00\"),",
            "        workflow_id=args.workflow if _is_workflow_selected(args) else None,",
            "    )",
            "",
            "",
            "def _cli_build(args):",
            "    prj = bigflow.build.spec.get_project_spec()",
            "    bigflow.build.operate.build_project(",
            "        prj,",
            "        start_time=args.start_time if _is_starttime_selected(args) else datetime.now().strftime(\"%Y-%m-%d %H:00:00\"),",
            "        workflow_id=args.workflow if _is_workflow_selected(args) else None,",
            "        export_image_tar=args.export_image_tar,",
            "        cache_params=_grab_image_cache_params(args),",
            "    )",
            "",
            "",
            "def _create_build_requirements_parser(subparsers):",
            "    parser = subparsers.add_parser(",
            "        'build-requirements',",
            "        description=\"Compiles requirements.txt from *.in specs\",",
            "    )",
            "    parser.add_argument(",
            "        'in_file',",
            "        type=str,",
            "        nargs='?',",
            "        default=\"resources/requirements.in\",  # FIXME: read 'project_setup.py'",
            "    )",
            "",
            "",
            "def _create_codegen_parser(subparsers: argparse._SubParsersAction):",
            "    parser = subparsers.add_parser('codegen', description=\"Various codegeneration tools\")",
            "    ss = parser.add_subparsers()",
            "",
            "    p = ss.add_parser('pin-dataflow-requirements')",
            "    p.set_defaults(func=_cli_codegen_pin_dataflow_requirements)",
            "",
            "",
            "def _cli_build_requirements(args):",
            "    in_file = pathlib.Path(args.in_file)",
            "    bigflow.build.pip.pip_compile(in_file)",
            "",
            "",
            "def _cli_codegen(args):",
            "    args.func(args)",
            "",
            "",
            "def _cli_codegen_pin_dataflow_requirements(args):",
            "    import bigflow.build.dataflow.dependency_checker as dc",
            "    dc.sync_requirements_with_dataflow_workers()",
            "",
            "",
            "def _is_workflow_selected(args):",
            "    return args.workflow and args.workflow != 'ALL'",
            "",
            "",
            "def _is_starttime_selected(args):",
            "    return args.start_time and args.start_time != 'NOW'",
            "",
            "",
            "def project_type_input():",
            "    project_type = input(\"Would you like to create basic or advanced project? Default basic. Type 'a' for advanced.\\n\")",
            "    return project_type if project_type else 'b'",
            "",
            "",
            "def project_number_input():",
            "    project_number = input('How many GCP projects would you like to use? '",
            "                           'It allows to deploy your workflows to more than one project. Default 2\\n')",
            "    return project_number if project_number else '2'",
            "",
            "",
            "def gcloud_project_list():",
            "    return subprocess.getoutput('gcloud projects list')",
            "",
            "",
            "def get_default_project_from_gcloud():",
            "    return subprocess.getoutput('gcloud config get-value project')",
            "",
            "",
            "def project_id_input(n):",
            "    if n == 0:",
            "        project = input(f'Enter a GCP project ID that you are going to use in your BigFlow project. '",
            "                        f'Choose a project from the list above. '",
            "                        f'If not provided default project: {get_default_project_from_gcloud()} will be used.\\n')",
            "    else:",
            "        project = input(f'Enter a #{n} GCP project ID that you are going to use in your BigFlow project. '",
            "                        f'Choose a project from the list above.'",
            "                        f' If not provided default project: {get_default_project_from_gcloud()} will be used.\\n')",
            "    return project",
            "",
            "",
            "def gcp_project_flow(n):",
            "    projects_list = gcloud_project_list()",
            "    print(projects_list)",
            "    return gcp_project_input(n, projects_list)",
            "",
            "",
            "def gcp_project_input(n, projects_list):",
            "    project = project_id_input(n)",
            "    if project == '':",
            "        return get_default_project_from_gcloud()",
            "    if project not in projects_list:",
            "        print(f'You do not have access to {project}. Try another project from the list.\\n')",
            "        return gcp_project_input(n, projects_list)",
            "    return project",
            "",
            "",
            "def gcp_bucket_input():",
            "    return input('Enter a Cloud Composer Bucket name where DAG files will be stored.\\n')",
            "",
            "",
            "def environment_name_input(envs):",
            "    environment_name = input('Enter an environment name. Default dev\\n')",
            "    if environment_name in envs:",
            "        print(f'Environment with name{environment_name} is already defined. Try another name.\\n')",
            "        return environment_name_input(envs)",
            "    return environment_name if environment_name else 'dev'",
            "",
            "",
            "def project_name_input():",
            "    return input('Enter the project name. It should be valid python package name. '",
            "                 'It will be used as a main directory of your project and bucket name used by dataflow to run jobs.\\n')",
            "",
            "",
            "def _cli_start_project():",
            "    config = {'is_basic': False, 'project_name': project_name_input(), 'projects_id': [], 'composers_bucket': [], 'envs': []}",
            "    if False:",
            "        for n in range(0, int(project_number_input())):",
            "            config['projects_id'].append(gcp_project_flow(n))",
            "            config['composers_bucket'].append(gcp_bucket_input())",
            "            config['envs'].append(environment_name_input(config['envs']))",
            "    else:",
            "        config['is_basic'] = True",
            "        config['projects_id'].append(gcp_project_flow(0))",
            "        config['composers_bucket'].append(gcp_bucket_input())",
            "",
            "        config['pyspark_job'] = True",
            "",
            "    bigflow.scaffold.start_project(**config)",
            "    print('Bigflow project created successfully.')",
            "",
            "",
            "def _cli_project_version(args):",
            "    commit_ish = args.git_commit or \"HEAD\"",
            "    print(bigflow.version.get_version(commit_ish))",
            "",
            "",
            "def _cli_release(args):",
            "    bigflow.version.release(args.ssh_identity_file)",
            "",
            "",
            "class _ConsoleStreamLogHandler(logging.Handler):",
            "    \"\"\"",
            "    A handler class which writes logging records to `sys.stderr`.",
            "",
            "    When `sys.stderr` is a TTY device a new line is not appended to log",
            "    records with `incomplete_line` attribute set to true.  This allows to",
            "    propagate progress bars / dynamic prints produced by child processes.",
            "    See `bigflow.commons.run_process` for more details.",
            "    \"\"\"",
            "",
            "    # TODO: consider moving this class & `init_console_logging` fn to \"log.py\" module",
            "    # Not it is not poss`ible, as 'log.py' *must* be",
            "    # non-imporable when 'google.cloud.logging' is not installed",
            "    # making 'log.py' always importable will require refactoring",
            "",
            "    def __init__(self):",
            "        logging.Handler.__init__(self)",
            "        self.last_incomplete_msg = \"\"",
            "        self.stream = sys.stderr",
            "        self.isatty = self.stream.isatty()",
            "",
            "    def emit(self, record: logging.LogRecord):",
            "        incomplete_line = getattr(record, 'incomplete_line', False)",
            "",
            "        try:",
            "            last_msg = self.last_incomplete_msg",
            "            msg = self.format(record)",
            "",
            "            if msg.startswith(last_msg):",
            "                msg = msg[len(last_msg):]",
            "            elif self.isatty:",
            "                msg = \"\\r\\033[K\" + msg",
            "            else:",
            "                msg = \"\\n\" + msg",
            "",
            "            with self.lock:",
            "                if not incomplete_line:",
            "                    msg += \"\\n\"",
            "                    self.last_incomplete_msg = \"\"",
            "                else:",
            "                    self.last_incomplete_msg = msg",
            "                self.stream.write(msg)",
            "                self.stream.flush()",
            "",
            "        except RecursionError:  # See issue 36272",
            "            raise",
            "        except Exception:",
            "            self.handleError(record)",
            "",
            "",
            "def init_console_logging(verbose):",
            "    verbose = verbose or os.environ.get('BIGFLOW_VERBOSE', \"\")",
            "    if verbose:",
            "        logging.basicConfig(",
            "            level=logging.DEBUG,",
            "            format=\"%(asctime)s| %(message)s\",",
            "            handlers=[_ConsoleStreamLogHandler()],",
            "        )",
            "    else:",
            "        logging.basicConfig(",
            "            level=logging.INFO,",
            "            format=\"%(message)s\",",
            "            handlers=[_ConsoleStreamLogHandler()],",
            "        )",
            "",
            "",
            "def cli_logs(root_package):",
            "    import bigflow.log as log",
            "",
            "    projects_id = []",
            "    workflows_links = {}",
            "    for workflow in walk_workflows(root_package):",
            "        if workflow.log_config:",
            "            projects_id.append((workflow.log_config['gcp_project_id'], workflow.workflow_id))",
            "            workflows_links[workflow.workflow_id] = log.workflow_logs_link_for_cli(workflow.log_config, workflow.workflow_id)",
            "    if not projects_id:",
            "        raise Exception(\"Found no workflows with configured logging.\")",
            "    deduplicated_projects_id = sorted(set(projects_id), key=lambda x: projects_id.index(x))",
            "    infra_links = log.infrastructure_logs_link_for_cli(deduplicated_projects_id)",
            "    log.print_log_links_message(workflows_links, infra_links)",
            "",
            "",
            "def _is_log_module_installed():",
            "    try:",
            "        import bigflow.log",
            "        return True",
            "    except ImportError:",
            "        raise Exception(\"bigflow.log module not found. You need install bigflow with 'log' extras.\")",
            "",
            "",
            "def cli(raw_args) -> None:",
            "    bigflow.build.dev.install_syspath()",
            "    bigflow.migrate.check_migrate()",
            "",
            "    project_name = read_project_name_from_setup()",
            "    parsed_args = _parse_args(project_name, raw_args)",
            "    init_console_logging(parsed_args.verbose)",
            "",
            "    operation: str = parsed_args.operation",
            "",
            "    if operation == 'run':",
            "        set_configuration_env(parsed_args.config)",
            "        root_package = find_root_package(project_name, read_project_package(parsed_args))",
            "        cli_run(root_package, parsed_args.runtime, parsed_args.job, parsed_args.workflow)",
            "    elif operation == 'deploy-image':",
            "        _cli_deploy_image(parsed_args)",
            "    elif operation == 'deploy-dags':",
            "        _cli_deploy_dags(parsed_args)",
            "    elif operation == 'deploy':",
            "        _cli_deploy_image(parsed_args)",
            "        _cli_deploy_dags(parsed_args)",
            "    elif operation == 'build-dags':",
            "        _cli_build_dags(parsed_args)",
            "    elif operation == 'build-image':",
            "        _cli_build_image(parsed_args)",
            "    elif operation == 'build-package':",
            "        _cli_build_package()",
            "    elif operation == 'build':",
            "        _cli_build(parsed_args)",
            "    elif operation == 'start-project':",
            "        _cli_start_project()",
            "    elif operation == 'project-version' or operation == 'pv':",
            "        _cli_project_version(parsed_args)",
            "    elif operation == 'release':",
            "        _cli_release(parsed_args)",
            "    elif operation == 'build-requirements':",
            "        _cli_build_requirements(parsed_args)",
            "    elif operation == 'codegen':",
            "        _cli_codegen(parsed_args)",
            "    else:",
            "        raise ValueError(f'Operation unknown - {operation}')",
            "",
            "    logger.debug(\"bigflow cli finished\")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "16": [],
            "17": [],
            "517": [
                "_resolve_property"
            ]
        },
        "addLocation": []
    },
    "bigflow/deploy.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 42,
                "PatchRowcode": "     docker_repository: str,"
            },
            "1": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 43,
                "PatchRowcode": "     auth_method: AuthorizationType = AuthorizationType.LOCAL_ACCOUNT,"
            },
            "2": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 44,
                "PatchRowcode": "     vault_endpoint: T.Optional[str] = None,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+    vault_endpoint_verify: str | bool | None = None,"
            },
            "4": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": 46,
                "PatchRowcode": "     vault_secret: T.Optional[str] = None,"
            },
            "5": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 47,
                "PatchRowcode": " ) -> str:"
            },
            "6": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 48,
                "PatchRowcode": "     if image_tar_path.endswith(\".toml\"):"
            },
            "7": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 54,
                "PatchRowcode": "         docker_repository,"
            },
            "8": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 55,
                "PatchRowcode": "         auth_method,"
            },
            "9": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": 56,
                "PatchRowcode": "         vault_endpoint,"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 57,
                "PatchRowcode": "+        vault_endpoint_verify,"
            },
            "11": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 58,
                "PatchRowcode": "         vault_secret,"
            },
            "12": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 59,
                "PatchRowcode": "     )"
            },
            "13": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": 60,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 64,
                "PatchRowcode": "     docker_repository: str,"
            },
            "15": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 65,
                "PatchRowcode": "     auth_method: AuthorizationType = AuthorizationType.LOCAL_ACCOUNT,"
            },
            "16": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 66,
                "PatchRowcode": "     vault_endpoint: str | None = None,"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+    vault_endpoint_verify: str | bool | None = None,"
            },
            "18": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 68,
                "PatchRowcode": "     vault_secret: str | None = None,"
            },
            "19": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 69,
                "PatchRowcode": " ) -> str:"
            },
            "20": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 70,
                "PatchRowcode": " "
            },
            "21": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 84,
                "PatchRowcode": "         docker_repository=docker_repository,"
            },
            "22": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 85,
                "PatchRowcode": "         auth_method=auth_method,"
            },
            "23": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 86,
                "PatchRowcode": "         vault_endpoint=vault_endpoint,"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+        vault_endpoint_verify=vault_endpoint_verify,"
            },
            "25": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 88,
                "PatchRowcode": "         vault_secret=vault_secret,"
            },
            "26": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 89,
                "PatchRowcode": "         image_id=image_id,"
            },
            "27": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 90,
                "PatchRowcode": "         build_ver=build_ver,"
            },
            "28": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 96,
                "PatchRowcode": "     docker_repository: str,"
            },
            "29": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 97,
                "PatchRowcode": "     auth_method: AuthorizationType = AuthorizationType.LOCAL_ACCOUNT,"
            },
            "30": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": 98,
                "PatchRowcode": "     vault_endpoint: str | None = None,"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+    vault_endpoint_verify: str | bool | None = None,"
            },
            "32": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 100,
                "PatchRowcode": "     vault_secret: str | None = None,"
            },
            "33": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 101,
                "PatchRowcode": " ) -> str:"
            },
            "34": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 102,
                "PatchRowcode": "     build_ver = bf_commons.decode_version_number_from_file_name(Path(image_tar_path))"
            },
            "35": {
                "beforePatchRowNumber": 105,
                "afterPatchRowNumber": 110,
                "PatchRowcode": "             image_id=image_id,"
            },
            "36": {
                "beforePatchRowNumber": 106,
                "afterPatchRowNumber": 111,
                "PatchRowcode": "             auth_method=auth_method,"
            },
            "37": {
                "beforePatchRowNumber": 107,
                "afterPatchRowNumber": 112,
                "PatchRowcode": "             vault_endpoint=vault_endpoint,"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 113,
                "PatchRowcode": "+            vault_endpoint_verify=vault_endpoint_verify,"
            },
            "39": {
                "beforePatchRowNumber": 108,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "             vault_secret=vault_secret,"
            },
            "40": {
                "beforePatchRowNumber": 109,
                "afterPatchRowNumber": 115,
                "PatchRowcode": "         )"
            },
            "41": {
                "beforePatchRowNumber": 110,
                "afterPatchRowNumber": 116,
                "PatchRowcode": "     finally:"
            },
            "42": {
                "beforePatchRowNumber": 118,
                "afterPatchRowNumber": 124,
                "PatchRowcode": "     image_id: str,"
            },
            "43": {
                "beforePatchRowNumber": 119,
                "afterPatchRowNumber": 125,
                "PatchRowcode": "     auth_method: AuthorizationType,"
            },
            "44": {
                "beforePatchRowNumber": 120,
                "afterPatchRowNumber": 126,
                "PatchRowcode": "     vault_endpoint: str | None = None,"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 127,
                "PatchRowcode": "+    vault_endpoint_verify: str | bool | None = None,"
            },
            "46": {
                "beforePatchRowNumber": 121,
                "afterPatchRowNumber": 128,
                "PatchRowcode": "     vault_secret: str | None = None,"
            },
            "47": {
                "beforePatchRowNumber": 122,
                "afterPatchRowNumber": 129,
                "PatchRowcode": " ) -> str:"
            },
            "48": {
                "beforePatchRowNumber": 123,
                "afterPatchRowNumber": 130,
                "PatchRowcode": "     docker_image = docker_repository + \":\" + build_ver"
            },
            "49": {
                "beforePatchRowNumber": 124,
                "afterPatchRowNumber": 131,
                "PatchRowcode": "     docker_image_latest = docker_repository + \":latest\""
            },
            "50": {
                "beforePatchRowNumber": 125,
                "afterPatchRowNumber": 132,
                "PatchRowcode": "     tag_image(image_id, docker_repository, \"latest\")"
            },
            "51": {
                "beforePatchRowNumber": 126,
                "afterPatchRowNumber": 133,
                "PatchRowcode": " "
            },
            "52": {
                "beforePatchRowNumber": 127,
                "afterPatchRowNumber": 134,
                "PatchRowcode": "     logger.info(\"Deploying docker image tag=%s auth_method=%s\", docker_image, auth_method)"
            },
            "53": {
                "beforePatchRowNumber": 128,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    authenticate_to_registry(auth_method, vault_endpoint, vault_secret)"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 135,
                "PatchRowcode": "+    authenticate_to_registry(auth_method, vault_endpoint, vault_secret, vault_endpoint_verify)"
            },
            "55": {
                "beforePatchRowNumber": 129,
                "afterPatchRowNumber": 136,
                "PatchRowcode": "     bf_commons.run_process(['docker', 'push', docker_image])"
            },
            "56": {
                "beforePatchRowNumber": 130,
                "afterPatchRowNumber": 137,
                "PatchRowcode": "     bf_commons.run_process(['docker', 'push', docker_image_latest])"
            },
            "57": {
                "beforePatchRowNumber": 131,
                "afterPatchRowNumber": 138,
                "PatchRowcode": " "
            },
            "58": {
                "beforePatchRowNumber": 136,
                "afterPatchRowNumber": 143,
                "PatchRowcode": "         auth_method: AuthorizationType,"
            },
            "59": {
                "beforePatchRowNumber": 137,
                "afterPatchRowNumber": 144,
                "PatchRowcode": "         vault_endpoint: T.Optional[str] = None,"
            },
            "60": {
                "beforePatchRowNumber": 138,
                "afterPatchRowNumber": 145,
                "PatchRowcode": "         vault_secret: T.Optional[str] = None,"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 146,
                "PatchRowcode": "+        vault_endpoint_verify: str | bool | None = None,"
            },
            "62": {
                "beforePatchRowNumber": 139,
                "afterPatchRowNumber": 147,
                "PatchRowcode": " ):"
            },
            "63": {
                "beforePatchRowNumber": 140,
                "afterPatchRowNumber": 148,
                "PatchRowcode": "     logger.info(\"Authenticating to registry with auth_method=%s\", auth_method)"
            },
            "64": {
                "beforePatchRowNumber": 141,
                "afterPatchRowNumber": 149,
                "PatchRowcode": " "
            },
            "65": {
                "beforePatchRowNumber": 142,
                "afterPatchRowNumber": 150,
                "PatchRowcode": "     if auth_method == AuthorizationType.LOCAL_ACCOUNT:"
            },
            "66": {
                "beforePatchRowNumber": 143,
                "afterPatchRowNumber": 151,
                "PatchRowcode": "         bf_commons.run_process(['gcloud', 'auth', 'configure-docker'])"
            },
            "67": {
                "beforePatchRowNumber": 144,
                "afterPatchRowNumber": 152,
                "PatchRowcode": "     elif auth_method == AuthorizationType.VAULT:"
            },
            "68": {
                "beforePatchRowNumber": 145,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        oauthtoken = get_vault_token(vault_endpoint, vault_secret)"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 153,
                "PatchRowcode": "+        oauthtoken = get_vault_token(vault_endpoint, vault_secret, vault_endpoint_verify)"
            },
            "70": {
                "beforePatchRowNumber": 146,
                "afterPatchRowNumber": 154,
                "PatchRowcode": "         bf_commons.run_process("
            },
            "71": {
                "beforePatchRowNumber": 147,
                "afterPatchRowNumber": 155,
                "PatchRowcode": "             ['docker', 'login', '-u', 'oauth2accesstoken', '--password-stdin', 'https://eu.gcr.io'],"
            },
            "72": {
                "beforePatchRowNumber": 148,
                "afterPatchRowNumber": 156,
                "PatchRowcode": "             input=oauthtoken,"
            },
            "73": {
                "beforePatchRowNumber": 156,
                "afterPatchRowNumber": 164,
                "PatchRowcode": "         auth_method: AuthorizationType,"
            },
            "74": {
                "beforePatchRowNumber": 157,
                "afterPatchRowNumber": 165,
                "PatchRowcode": "         vault_endpoint: T.Optional[str] = None,"
            },
            "75": {
                "beforePatchRowNumber": 158,
                "afterPatchRowNumber": 166,
                "PatchRowcode": "         vault_secret: T.Optional[str] = None,"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 167,
                "PatchRowcode": "+        vault_endpoint_verify: str | bool | None = None"
            },
            "77": {
                "beforePatchRowNumber": 159,
                "afterPatchRowNumber": 168,
                "PatchRowcode": " ):"
            },
            "78": {
                "beforePatchRowNumber": 160,
                "afterPatchRowNumber": 169,
                "PatchRowcode": "     logger.info(\"Checking if images used in DAGs exist in the registry\")"
            },
            "79": {
                "beforePatchRowNumber": 161,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    authenticate_to_registry(auth_method, vault_endpoint, vault_secret)"
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 170,
                "PatchRowcode": "+    authenticate_to_registry(auth_method, vault_endpoint, vault_secret, vault_endpoint_verify)"
            },
            "81": {
                "beforePatchRowNumber": 162,
                "afterPatchRowNumber": 171,
                "PatchRowcode": "     missing_images = set()"
            },
            "82": {
                "beforePatchRowNumber": 163,
                "afterPatchRowNumber": 172,
                "PatchRowcode": "     for image in images:"
            },
            "83": {
                "beforePatchRowNumber": 164,
                "afterPatchRowNumber": 173,
                "PatchRowcode": "         found_images = bf_commons.run_process(['docker', 'manifest', 'inspect', image], check=False, verbose=False)"
            },
            "84": {
                "beforePatchRowNumber": 189,
                "afterPatchRowNumber": 198,
                "PatchRowcode": "         clear_dags_folder: bool = False,"
            },
            "85": {
                "beforePatchRowNumber": 190,
                "afterPatchRowNumber": 199,
                "PatchRowcode": "         auth_method: AuthorizationType = AuthorizationType.LOCAL_ACCOUNT,"
            },
            "86": {
                "beforePatchRowNumber": 191,
                "afterPatchRowNumber": 200,
                "PatchRowcode": "         vault_endpoint: T.Optional[str] = None,"
            },
            "87": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 201,
                "PatchRowcode": "+        vault_endpoint_verify: str | bool | None = None,"
            },
            "88": {
                "beforePatchRowNumber": 192,
                "afterPatchRowNumber": 202,
                "PatchRowcode": "         vault_secret: T.Optional[str] = None,"
            },
            "89": {
                "beforePatchRowNumber": 193,
                "afterPatchRowNumber": 203,
                "PatchRowcode": "         gs_client: T.Optional[storage.Client] = None,"
            },
            "90": {
                "beforePatchRowNumber": 194,
                "afterPatchRowNumber": 204,
                "PatchRowcode": " ) -> str:"
            },
            "91": {
                "beforePatchRowNumber": 195,
                "afterPatchRowNumber": 205,
                "PatchRowcode": "     images = get_image_tags_from_image_version_file(dags_dir)"
            },
            "92": {
                "beforePatchRowNumber": 196,
                "afterPatchRowNumber": 206,
                "PatchRowcode": "     if images:"
            },
            "93": {
                "beforePatchRowNumber": 197,
                "afterPatchRowNumber": 207,
                "PatchRowcode": "         check_images_exist(auth_method=auth_method,"
            },
            "94": {
                "beforePatchRowNumber": 198,
                "afterPatchRowNumber": 208,
                "PatchRowcode": "                            vault_endpoint=vault_endpoint,"
            },
            "95": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 209,
                "PatchRowcode": "+                           vault_endpoint_verify=vault_endpoint_verify,"
            },
            "96": {
                "beforePatchRowNumber": 199,
                "afterPatchRowNumber": 210,
                "PatchRowcode": "                            vault_secret=vault_secret,"
            },
            "97": {
                "beforePatchRowNumber": 200,
                "afterPatchRowNumber": 211,
                "PatchRowcode": "                            images=images)"
            },
            "98": {
                "beforePatchRowNumber": 201,
                "afterPatchRowNumber": 212,
                "PatchRowcode": " "
            },
            "99": {
                "beforePatchRowNumber": 202,
                "afterPatchRowNumber": 213,
                "PatchRowcode": "     logger.info(\"Deploying DAGs folder, auth_method=%s, clear_dags_folder=%s, dags_dir=%s\", auth_method, clear_dags_folder, dags_dir)"
            },
            "100": {
                "beforePatchRowNumber": 203,
                "afterPatchRowNumber": 214,
                "PatchRowcode": " "
            },
            "101": {
                "beforePatchRowNumber": 204,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    client = gs_client or create_storage_client(auth_method, project_id, vault_endpoint, vault_secret)"
            },
            "102": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 215,
                "PatchRowcode": "+    client = gs_client or create_storage_client(auth_method, project_id, vault_endpoint, vault_secret, vault_endpoint_verify)"
            },
            "103": {
                "beforePatchRowNumber": 205,
                "afterPatchRowNumber": 216,
                "PatchRowcode": "     bucket = client.bucket(dags_bucket)"
            },
            "104": {
                "beforePatchRowNumber": 206,
                "afterPatchRowNumber": 217,
                "PatchRowcode": " "
            },
            "105": {
                "beforePatchRowNumber": 207,
                "afterPatchRowNumber": 218,
                "PatchRowcode": "     if clear_dags_folder:"
            },
            "106": {
                "beforePatchRowNumber": 246,
                "afterPatchRowNumber": 257,
                "PatchRowcode": "         project_id: str,"
            },
            "107": {
                "beforePatchRowNumber": 247,
                "afterPatchRowNumber": 258,
                "PatchRowcode": "         vault_endpoint: str,"
            },
            "108": {
                "beforePatchRowNumber": 248,
                "afterPatchRowNumber": 259,
                "PatchRowcode": "         vault_secret: str,"
            },
            "109": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 260,
                "PatchRowcode": "+        vault_endpoint_verify: str | bool | None = None"
            },
            "110": {
                "beforePatchRowNumber": 249,
                "afterPatchRowNumber": 261,
                "PatchRowcode": " ) -> storage.Client:"
            },
            "111": {
                "beforePatchRowNumber": 250,
                "afterPatchRowNumber": 262,
                "PatchRowcode": "     if auth_method == AuthorizationType.LOCAL_ACCOUNT:"
            },
            "112": {
                "beforePatchRowNumber": 251,
                "afterPatchRowNumber": 263,
                "PatchRowcode": "         return storage.Client(project=project_id)"
            },
            "113": {
                "beforePatchRowNumber": 252,
                "afterPatchRowNumber": 264,
                "PatchRowcode": "     elif auth_method == AuthorizationType.VAULT:"
            },
            "114": {
                "beforePatchRowNumber": 253,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        oauthtoken = get_vault_token(vault_endpoint, vault_secret)"
            },
            "115": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 265,
                "PatchRowcode": "+        oauthtoken = get_vault_token(vault_endpoint, vault_secret, vault_endpoint_verify)"
            },
            "116": {
                "beforePatchRowNumber": 254,
                "afterPatchRowNumber": 266,
                "PatchRowcode": "         return storage.Client(project=project_id, credentials=credentials.Credentials(oauthtoken))"
            },
            "117": {
                "beforePatchRowNumber": 255,
                "afterPatchRowNumber": 267,
                "PatchRowcode": "     else:"
            },
            "118": {
                "beforePatchRowNumber": 256,
                "afterPatchRowNumber": 268,
                "PatchRowcode": "         raise ValueError(f\"unsupported auth_method: {auth_method!r}\")"
            },
            "119": {
                "beforePatchRowNumber": 257,
                "afterPatchRowNumber": 269,
                "PatchRowcode": " "
            },
            "120": {
                "beforePatchRowNumber": 258,
                "afterPatchRowNumber": 270,
                "PatchRowcode": " "
            },
            "121": {
                "beforePatchRowNumber": 259,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def get_vault_token(vault_endpoint: str, vault_secret: str) -> str:"
            },
            "122": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 271,
                "PatchRowcode": "+def get_vault_token(vault_endpoint: str, vault_secret: str, vault_endpoint_verify: str | bool | None = True) -> str:"
            },
            "123": {
                "beforePatchRowNumber": 260,
                "afterPatchRowNumber": 272,
                "PatchRowcode": "     if not vault_endpoint:"
            },
            "124": {
                "beforePatchRowNumber": 261,
                "afterPatchRowNumber": 273,
                "PatchRowcode": "         raise ValueError('vault_endpoint is required')"
            },
            "125": {
                "beforePatchRowNumber": 262,
                "afterPatchRowNumber": 274,
                "PatchRowcode": "     if not vault_secret:"
            },
            "126": {
                "beforePatchRowNumber": 263,
                "afterPatchRowNumber": 275,
                "PatchRowcode": "         raise ValueError('vault_secret is required')"
            },
            "127": {
                "beforePatchRowNumber": 264,
                "afterPatchRowNumber": 276,
                "PatchRowcode": " "
            },
            "128": {
                "beforePatchRowNumber": 265,
                "afterPatchRowNumber": 277,
                "PatchRowcode": "     headers = {'X-Vault-Token': vault_secret}"
            },
            "129": {
                "beforePatchRowNumber": 266,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    response = requests.get(vault_endpoint, headers=headers, verify=False)"
            },
            "130": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 278,
                "PatchRowcode": "+    response = requests.get(vault_endpoint, headers=headers, verify=vault_endpoint_verify)"
            },
            "131": {
                "beforePatchRowNumber": 267,
                "afterPatchRowNumber": 279,
                "PatchRowcode": " "
            },
            "132": {
                "beforePatchRowNumber": 268,
                "afterPatchRowNumber": 280,
                "PatchRowcode": "     if response.status_code != 200:"
            },
            "133": {
                "beforePatchRowNumber": 269,
                "afterPatchRowNumber": 281,
                "PatchRowcode": "         logger.info(response.text)"
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import logging",
            "import typing as T",
            "",
            "from enum import Enum",
            "from pathlib import Path",
            "",
            "import requests",
            "import toml",
            "",
            "from google.cloud.storage import Bucket",
            "from google.oauth2 import credentials",
            "from google.cloud import storage",
            "from bigflow.build.spec import get_project_spec",
            "",
            "import bigflow.commons as bf_commons",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class AuthorizationType(Enum):",
            "    LOCAL_ACCOUNT = 'local_account'",
            "    VAULT = 'vault'",
            "",
            "",
            "def load_image_from_tar(image_tar_path: str) -> str:",
            "    logger.info(\"Load docker image from %s...\", image_tar_path)",
            "    for line in bf_commons.run_process(['docker', 'load', '-i', image_tar_path]).split('\\n'):",
            "        if 'Loaded image ID:' in line:",
            "            return line.split()[-1].split(':')[-1]",
            "    raise ValueError(f\"Can't load image: {image_tar_path}\")",
            "",
            "",
            "def tag_image(image_id: str, repository: str, tag: str) -> str:",
            "    return bf_commons.run_process([\"docker\", \"tag\", image_id,  f\"{repository}:{tag}\"])",
            "",
            "",
            "def deploy_docker_image(",
            "    image_tar_path: str,",
            "    docker_repository: str,",
            "    auth_method: AuthorizationType = AuthorizationType.LOCAL_ACCOUNT,",
            "    vault_endpoint: T.Optional[str] = None,",
            "    vault_secret: T.Optional[str] = None,",
            ") -> str:",
            "    if image_tar_path.endswith(\".toml\"):",
            "        deploy_fn = _deploy_docker_image_from_local_repo",
            "    else:",
            "        deploy_fn = _deploy_docker_image_from_fs",
            "    return deploy_fn(",
            "        image_tar_path,",
            "        docker_repository,",
            "        auth_method,",
            "        vault_endpoint,",
            "        vault_secret,",
            "    )",
            "",
            "",
            "def _deploy_docker_image_from_local_repo(",
            "    image_info_path: str,",
            "    docker_repository: str,",
            "    auth_method: AuthorizationType = AuthorizationType.LOCAL_ACCOUNT,",
            "    vault_endpoint: str | None = None,",
            "    vault_secret: str | None = None,",
            ") -> str:",
            "",
            "    spec = get_project_spec()",
            "    info = toml.load(image_info_path)",
            "    image_project_version = info['project_version']",
            "    image_id = info['docker_image_id']",
            "    build_ver = image_project_version.replace(\"+\", \"-\")",
            "",
            "    if spec.version != image_project_version:",
            "        logger.warning(",
            "            \"Project version is %r, but image was built for %r\",",
            "            spec.version, image_project_version,",
            "        )",
            "",
            "    return _deploy_image_loaded_to_local_registry(",
            "        docker_repository=docker_repository,",
            "        auth_method=auth_method,",
            "        vault_endpoint=vault_endpoint,",
            "        vault_secret=vault_secret,",
            "        image_id=image_id,",
            "        build_ver=build_ver,",
            "    )",
            "",
            "",
            "def _deploy_docker_image_from_fs(",
            "    image_tar_path: str,",
            "    docker_repository: str,",
            "    auth_method: AuthorizationType = AuthorizationType.LOCAL_ACCOUNT,",
            "    vault_endpoint: str | None = None,",
            "    vault_secret: str | None = None,",
            ") -> str:",
            "    build_ver = bf_commons.decode_version_number_from_file_name(Path(image_tar_path))",
            "    build_ver = build_ver.replace(\"+\", \"-\")  # fix local version separator",
            "    image_id = load_image_from_tar(image_tar_path)",
            "    try:",
            "        tag_image(image_id, docker_repository, build_ver)",
            "        return _deploy_image_loaded_to_local_registry(",
            "            build_ver=build_ver,",
            "            docker_repository=docker_repository,",
            "            image_id=image_id,",
            "            auth_method=auth_method,",
            "            vault_endpoint=vault_endpoint,",
            "            vault_secret=vault_secret,",
            "        )",
            "    finally:",
            "        image_tag = bf_commons.build_docker_image_tag(docker_repository, build_ver)",
            "        bf_commons.remove_docker_image_from_local_registry(image_tag)",
            "",
            "",
            "def _deploy_image_loaded_to_local_registry(",
            "    build_ver: str,",
            "    docker_repository: str,",
            "    image_id: str,",
            "    auth_method: AuthorizationType,",
            "    vault_endpoint: str | None = None,",
            "    vault_secret: str | None = None,",
            ") -> str:",
            "    docker_image = docker_repository + \":\" + build_ver",
            "    docker_image_latest = docker_repository + \":latest\"",
            "    tag_image(image_id, docker_repository, \"latest\")",
            "",
            "    logger.info(\"Deploying docker image tag=%s auth_method=%s\", docker_image, auth_method)",
            "    authenticate_to_registry(auth_method, vault_endpoint, vault_secret)",
            "    bf_commons.run_process(['docker', 'push', docker_image])",
            "    bf_commons.run_process(['docker', 'push', docker_image_latest])",
            "",
            "    return docker_image",
            "",
            "",
            "def authenticate_to_registry(",
            "        auth_method: AuthorizationType,",
            "        vault_endpoint: T.Optional[str] = None,",
            "        vault_secret: T.Optional[str] = None,",
            "):",
            "    logger.info(\"Authenticating to registry with auth_method=%s\", auth_method)",
            "",
            "    if auth_method == AuthorizationType.LOCAL_ACCOUNT:",
            "        bf_commons.run_process(['gcloud', 'auth', 'configure-docker'])",
            "    elif auth_method == AuthorizationType.VAULT:",
            "        oauthtoken = get_vault_token(vault_endpoint, vault_secret)",
            "        bf_commons.run_process(",
            "            ['docker', 'login', '-u', 'oauth2accesstoken', '--password-stdin', 'https://eu.gcr.io'],",
            "            input=oauthtoken,",
            "        )",
            "    else:",
            "        raise ValueError(f\"unsupported auth_method: {auth_method!r}\")",
            "",
            "",
            "def check_images_exist(",
            "        images: T.Set[str],",
            "        auth_method: AuthorizationType,",
            "        vault_endpoint: T.Optional[str] = None,",
            "        vault_secret: T.Optional[str] = None,",
            "):",
            "    logger.info(\"Checking if images used in DAGs exist in the registry\")",
            "    authenticate_to_registry(auth_method, vault_endpoint, vault_secret)",
            "    missing_images = set()",
            "    for image in images:",
            "        found_images = bf_commons.run_process(['docker', 'manifest', 'inspect', image], check=False, verbose=False)",
            "        if not found_images:",
            "            missing_images.add(image)",
            "    if missing_images:",
            "        raise ValueError(f\"Some of the images used in dags do not exist, images: {missing_images}\")",
            "",
            "",
            "def get_image_tags_from_image_version_file(dags_dir: str) -> T.Set[str]:",
            "    image_version_file = Path(dags_dir) / \"image_version.txt\"",
            "    versions = set()",
            "    if image_version_file.exists():",
            "        with image_version_file.open() as f:",
            "            versions = set(f.read().splitlines())",
            "        return versions",
            "    else:",
            "        logger.error(\"The image_version.txt file not found, \"",
            "                     \"the image check will not be performed. To perform the check regenerate \"",
            "                     \"DAG files with the new version of BigFlow\")",
            "    return versions",
            "",
            "",
            "def deploy_dags_folder(",
            "        dags_dir: str,",
            "        dags_bucket: str,",
            "        project_id: str,",
            "        clear_dags_folder: bool = False,",
            "        auth_method: AuthorizationType = AuthorizationType.LOCAL_ACCOUNT,",
            "        vault_endpoint: T.Optional[str] = None,",
            "        vault_secret: T.Optional[str] = None,",
            "        gs_client: T.Optional[storage.Client] = None,",
            ") -> str:",
            "    images = get_image_tags_from_image_version_file(dags_dir)",
            "    if images:",
            "        check_images_exist(auth_method=auth_method,",
            "                           vault_endpoint=vault_endpoint,",
            "                           vault_secret=vault_secret,",
            "                           images=images)",
            "",
            "    logger.info(\"Deploying DAGs folder, auth_method=%s, clear_dags_folder=%s, dags_dir=%s\", auth_method, clear_dags_folder, dags_dir)",
            "",
            "    client = gs_client or create_storage_client(auth_method, project_id, vault_endpoint, vault_secret)",
            "    bucket = client.bucket(dags_bucket)",
            "",
            "    if clear_dags_folder:",
            "        clear_remote_dags_bucket(bucket)",
            "",
            "    upload_dags_folder(dags_dir, bucket)",
            "    return dags_bucket",
            "",
            "",
            "def clear_remote_dags_bucket(bucket: Bucket) -> None:",
            "    i = 0",
            "    for blob in bucket.list_blobs(prefix='dags'):",
            "        if not blob.name in ['dags/', 'dags/airflow_monitoring.py']:",
            "            logger.info(\"deleting file %s\", _blob_uri(blob))",
            "            blob.delete()",
            "            i += 1",
            "",
            "    logger.info(\"%s files deleted\", i)",
            "",
            "",
            "def _blob_uri(blob: storage.Blob) -> str:",
            "    return f\"gs://{blob.bucket.name}/{blob.name}\"",
            "",
            "",
            "def upload_dags_folder(dags_dir: str, bucket: Bucket) -> None:",
            "    dags_dir_path = Path(dags_dir)",
            "",
            "    def upload_file(local_file_path, target_file_name):",
            "        blob = bucket.blob(target_file_name)",
            "        blob.upload_from_filename(local_file_path, content_type='application/octet-stream')",
            "        logger.info(\"uploading file %s to %s\", local_file_path, _blob_uri(blob))",
            "",
            "    files = list(filter(Path.is_file, dags_dir_path.iterdir()))",
            "    for f in files:",
            "        upload_file(f.as_posix(), 'dags/' + f.name)",
            "",
            "    logger.info(\"%s files uploaded\", len(files))",
            "",
            "",
            "def create_storage_client(",
            "        auth_method: AuthorizationType,",
            "        project_id: str,",
            "        vault_endpoint: str,",
            "        vault_secret: str,",
            ") -> storage.Client:",
            "    if auth_method == AuthorizationType.LOCAL_ACCOUNT:",
            "        return storage.Client(project=project_id)",
            "    elif auth_method == AuthorizationType.VAULT:",
            "        oauthtoken = get_vault_token(vault_endpoint, vault_secret)",
            "        return storage.Client(project=project_id, credentials=credentials.Credentials(oauthtoken))",
            "    else:",
            "        raise ValueError(f\"unsupported auth_method: {auth_method!r}\")",
            "",
            "",
            "def get_vault_token(vault_endpoint: str, vault_secret: str) -> str:",
            "    if not vault_endpoint:",
            "        raise ValueError('vault_endpoint is required')",
            "    if not vault_secret:",
            "        raise ValueError('vault_secret is required')",
            "",
            "    headers = {'X-Vault-Token': vault_secret}",
            "    response = requests.get(vault_endpoint, headers=headers, verify=False)",
            "",
            "    if response.status_code != 200:",
            "        logger.info(response.text)",
            "        raise ValueError(",
            "            'Could not get vault token, response code: {}'.format(",
            "                response.status_code))",
            "",
            "    logger.info(\"get oauth token from %s status_code=%s\", vault_endpoint, response.status_code)",
            "    return response.json()['data']['token']"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import logging",
            "import typing as T",
            "",
            "from enum import Enum",
            "from pathlib import Path",
            "",
            "import requests",
            "import toml",
            "",
            "from google.cloud.storage import Bucket",
            "from google.oauth2 import credentials",
            "from google.cloud import storage",
            "from bigflow.build.spec import get_project_spec",
            "",
            "import bigflow.commons as bf_commons",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class AuthorizationType(Enum):",
            "    LOCAL_ACCOUNT = 'local_account'",
            "    VAULT = 'vault'",
            "",
            "",
            "def load_image_from_tar(image_tar_path: str) -> str:",
            "    logger.info(\"Load docker image from %s...\", image_tar_path)",
            "    for line in bf_commons.run_process(['docker', 'load', '-i', image_tar_path]).split('\\n'):",
            "        if 'Loaded image ID:' in line:",
            "            return line.split()[-1].split(':')[-1]",
            "    raise ValueError(f\"Can't load image: {image_tar_path}\")",
            "",
            "",
            "def tag_image(image_id: str, repository: str, tag: str) -> str:",
            "    return bf_commons.run_process([\"docker\", \"tag\", image_id,  f\"{repository}:{tag}\"])",
            "",
            "",
            "def deploy_docker_image(",
            "    image_tar_path: str,",
            "    docker_repository: str,",
            "    auth_method: AuthorizationType = AuthorizationType.LOCAL_ACCOUNT,",
            "    vault_endpoint: T.Optional[str] = None,",
            "    vault_endpoint_verify: str | bool | None = None,",
            "    vault_secret: T.Optional[str] = None,",
            ") -> str:",
            "    if image_tar_path.endswith(\".toml\"):",
            "        deploy_fn = _deploy_docker_image_from_local_repo",
            "    else:",
            "        deploy_fn = _deploy_docker_image_from_fs",
            "    return deploy_fn(",
            "        image_tar_path,",
            "        docker_repository,",
            "        auth_method,",
            "        vault_endpoint,",
            "        vault_endpoint_verify,",
            "        vault_secret,",
            "    )",
            "",
            "",
            "def _deploy_docker_image_from_local_repo(",
            "    image_info_path: str,",
            "    docker_repository: str,",
            "    auth_method: AuthorizationType = AuthorizationType.LOCAL_ACCOUNT,",
            "    vault_endpoint: str | None = None,",
            "    vault_endpoint_verify: str | bool | None = None,",
            "    vault_secret: str | None = None,",
            ") -> str:",
            "",
            "    spec = get_project_spec()",
            "    info = toml.load(image_info_path)",
            "    image_project_version = info['project_version']",
            "    image_id = info['docker_image_id']",
            "    build_ver = image_project_version.replace(\"+\", \"-\")",
            "",
            "    if spec.version != image_project_version:",
            "        logger.warning(",
            "            \"Project version is %r, but image was built for %r\",",
            "            spec.version, image_project_version,",
            "        )",
            "",
            "    return _deploy_image_loaded_to_local_registry(",
            "        docker_repository=docker_repository,",
            "        auth_method=auth_method,",
            "        vault_endpoint=vault_endpoint,",
            "        vault_endpoint_verify=vault_endpoint_verify,",
            "        vault_secret=vault_secret,",
            "        image_id=image_id,",
            "        build_ver=build_ver,",
            "    )",
            "",
            "",
            "def _deploy_docker_image_from_fs(",
            "    image_tar_path: str,",
            "    docker_repository: str,",
            "    auth_method: AuthorizationType = AuthorizationType.LOCAL_ACCOUNT,",
            "    vault_endpoint: str | None = None,",
            "    vault_endpoint_verify: str | bool | None = None,",
            "    vault_secret: str | None = None,",
            ") -> str:",
            "    build_ver = bf_commons.decode_version_number_from_file_name(Path(image_tar_path))",
            "    build_ver = build_ver.replace(\"+\", \"-\")  # fix local version separator",
            "    image_id = load_image_from_tar(image_tar_path)",
            "    try:",
            "        tag_image(image_id, docker_repository, build_ver)",
            "        return _deploy_image_loaded_to_local_registry(",
            "            build_ver=build_ver,",
            "            docker_repository=docker_repository,",
            "            image_id=image_id,",
            "            auth_method=auth_method,",
            "            vault_endpoint=vault_endpoint,",
            "            vault_endpoint_verify=vault_endpoint_verify,",
            "            vault_secret=vault_secret,",
            "        )",
            "    finally:",
            "        image_tag = bf_commons.build_docker_image_tag(docker_repository, build_ver)",
            "        bf_commons.remove_docker_image_from_local_registry(image_tag)",
            "",
            "",
            "def _deploy_image_loaded_to_local_registry(",
            "    build_ver: str,",
            "    docker_repository: str,",
            "    image_id: str,",
            "    auth_method: AuthorizationType,",
            "    vault_endpoint: str | None = None,",
            "    vault_endpoint_verify: str | bool | None = None,",
            "    vault_secret: str | None = None,",
            ") -> str:",
            "    docker_image = docker_repository + \":\" + build_ver",
            "    docker_image_latest = docker_repository + \":latest\"",
            "    tag_image(image_id, docker_repository, \"latest\")",
            "",
            "    logger.info(\"Deploying docker image tag=%s auth_method=%s\", docker_image, auth_method)",
            "    authenticate_to_registry(auth_method, vault_endpoint, vault_secret, vault_endpoint_verify)",
            "    bf_commons.run_process(['docker', 'push', docker_image])",
            "    bf_commons.run_process(['docker', 'push', docker_image_latest])",
            "",
            "    return docker_image",
            "",
            "",
            "def authenticate_to_registry(",
            "        auth_method: AuthorizationType,",
            "        vault_endpoint: T.Optional[str] = None,",
            "        vault_secret: T.Optional[str] = None,",
            "        vault_endpoint_verify: str | bool | None = None,",
            "):",
            "    logger.info(\"Authenticating to registry with auth_method=%s\", auth_method)",
            "",
            "    if auth_method == AuthorizationType.LOCAL_ACCOUNT:",
            "        bf_commons.run_process(['gcloud', 'auth', 'configure-docker'])",
            "    elif auth_method == AuthorizationType.VAULT:",
            "        oauthtoken = get_vault_token(vault_endpoint, vault_secret, vault_endpoint_verify)",
            "        bf_commons.run_process(",
            "            ['docker', 'login', '-u', 'oauth2accesstoken', '--password-stdin', 'https://eu.gcr.io'],",
            "            input=oauthtoken,",
            "        )",
            "    else:",
            "        raise ValueError(f\"unsupported auth_method: {auth_method!r}\")",
            "",
            "",
            "def check_images_exist(",
            "        images: T.Set[str],",
            "        auth_method: AuthorizationType,",
            "        vault_endpoint: T.Optional[str] = None,",
            "        vault_secret: T.Optional[str] = None,",
            "        vault_endpoint_verify: str | bool | None = None",
            "):",
            "    logger.info(\"Checking if images used in DAGs exist in the registry\")",
            "    authenticate_to_registry(auth_method, vault_endpoint, vault_secret, vault_endpoint_verify)",
            "    missing_images = set()",
            "    for image in images:",
            "        found_images = bf_commons.run_process(['docker', 'manifest', 'inspect', image], check=False, verbose=False)",
            "        if not found_images:",
            "            missing_images.add(image)",
            "    if missing_images:",
            "        raise ValueError(f\"Some of the images used in dags do not exist, images: {missing_images}\")",
            "",
            "",
            "def get_image_tags_from_image_version_file(dags_dir: str) -> T.Set[str]:",
            "    image_version_file = Path(dags_dir) / \"image_version.txt\"",
            "    versions = set()",
            "    if image_version_file.exists():",
            "        with image_version_file.open() as f:",
            "            versions = set(f.read().splitlines())",
            "        return versions",
            "    else:",
            "        logger.error(\"The image_version.txt file not found, \"",
            "                     \"the image check will not be performed. To perform the check regenerate \"",
            "                     \"DAG files with the new version of BigFlow\")",
            "    return versions",
            "",
            "",
            "def deploy_dags_folder(",
            "        dags_dir: str,",
            "        dags_bucket: str,",
            "        project_id: str,",
            "        clear_dags_folder: bool = False,",
            "        auth_method: AuthorizationType = AuthorizationType.LOCAL_ACCOUNT,",
            "        vault_endpoint: T.Optional[str] = None,",
            "        vault_endpoint_verify: str | bool | None = None,",
            "        vault_secret: T.Optional[str] = None,",
            "        gs_client: T.Optional[storage.Client] = None,",
            ") -> str:",
            "    images = get_image_tags_from_image_version_file(dags_dir)",
            "    if images:",
            "        check_images_exist(auth_method=auth_method,",
            "                           vault_endpoint=vault_endpoint,",
            "                           vault_endpoint_verify=vault_endpoint_verify,",
            "                           vault_secret=vault_secret,",
            "                           images=images)",
            "",
            "    logger.info(\"Deploying DAGs folder, auth_method=%s, clear_dags_folder=%s, dags_dir=%s\", auth_method, clear_dags_folder, dags_dir)",
            "",
            "    client = gs_client or create_storage_client(auth_method, project_id, vault_endpoint, vault_secret, vault_endpoint_verify)",
            "    bucket = client.bucket(dags_bucket)",
            "",
            "    if clear_dags_folder:",
            "        clear_remote_dags_bucket(bucket)",
            "",
            "    upload_dags_folder(dags_dir, bucket)",
            "    return dags_bucket",
            "",
            "",
            "def clear_remote_dags_bucket(bucket: Bucket) -> None:",
            "    i = 0",
            "    for blob in bucket.list_blobs(prefix='dags'):",
            "        if not blob.name in ['dags/', 'dags/airflow_monitoring.py']:",
            "            logger.info(\"deleting file %s\", _blob_uri(blob))",
            "            blob.delete()",
            "            i += 1",
            "",
            "    logger.info(\"%s files deleted\", i)",
            "",
            "",
            "def _blob_uri(blob: storage.Blob) -> str:",
            "    return f\"gs://{blob.bucket.name}/{blob.name}\"",
            "",
            "",
            "def upload_dags_folder(dags_dir: str, bucket: Bucket) -> None:",
            "    dags_dir_path = Path(dags_dir)",
            "",
            "    def upload_file(local_file_path, target_file_name):",
            "        blob = bucket.blob(target_file_name)",
            "        blob.upload_from_filename(local_file_path, content_type='application/octet-stream')",
            "        logger.info(\"uploading file %s to %s\", local_file_path, _blob_uri(blob))",
            "",
            "    files = list(filter(Path.is_file, dags_dir_path.iterdir()))",
            "    for f in files:",
            "        upload_file(f.as_posix(), 'dags/' + f.name)",
            "",
            "    logger.info(\"%s files uploaded\", len(files))",
            "",
            "",
            "def create_storage_client(",
            "        auth_method: AuthorizationType,",
            "        project_id: str,",
            "        vault_endpoint: str,",
            "        vault_secret: str,",
            "        vault_endpoint_verify: str | bool | None = None",
            ") -> storage.Client:",
            "    if auth_method == AuthorizationType.LOCAL_ACCOUNT:",
            "        return storage.Client(project=project_id)",
            "    elif auth_method == AuthorizationType.VAULT:",
            "        oauthtoken = get_vault_token(vault_endpoint, vault_secret, vault_endpoint_verify)",
            "        return storage.Client(project=project_id, credentials=credentials.Credentials(oauthtoken))",
            "    else:",
            "        raise ValueError(f\"unsupported auth_method: {auth_method!r}\")",
            "",
            "",
            "def get_vault_token(vault_endpoint: str, vault_secret: str, vault_endpoint_verify: str | bool | None = True) -> str:",
            "    if not vault_endpoint:",
            "        raise ValueError('vault_endpoint is required')",
            "    if not vault_secret:",
            "        raise ValueError('vault_secret is required')",
            "",
            "    headers = {'X-Vault-Token': vault_secret}",
            "    response = requests.get(vault_endpoint, headers=headers, verify=vault_endpoint_verify)",
            "",
            "    if response.status_code != 200:",
            "        logger.info(response.text)",
            "        raise ValueError(",
            "            'Could not get vault token, response code: {}'.format(",
            "                response.status_code))",
            "",
            "    logger.info(\"get oauth token from %s status_code=%s\", vault_endpoint, response.status_code)",
            "    return response.json()['data']['token']"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "128": [
                "_deploy_image_loaded_to_local_registry"
            ],
            "145": [
                "authenticate_to_registry"
            ],
            "161": [
                "check_images_exist"
            ],
            "204": [
                "deploy_dags_folder"
            ],
            "253": [
                "create_storage_client"
            ],
            "259": [
                "get_vault_token"
            ],
            "266": [
                "get_vault_token"
            ]
        },
        "addLocation": [
            "bigflow.deploy._deploy_docker_image_from_local_repo",
            "bigflow.deploy.deploy_docker_image",
            "src.pyload.core",
            "bigflow.deploy.deploy_docker_image.deploy_fn",
            "bigflow.deploy._deploy_docker_image_from_fs"
        ]
    }
}