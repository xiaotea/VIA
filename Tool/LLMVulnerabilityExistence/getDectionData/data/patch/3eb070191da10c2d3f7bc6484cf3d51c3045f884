{
    "src/borg/archive.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 493,
                "afterPatchRowNumber": 493,
                "PatchRowcode": "         self.name = name  # overwritten later with name from archive metadata"
            },
            "1": {
                "beforePatchRowNumber": 494,
                "afterPatchRowNumber": 494,
                "PatchRowcode": "         self.name_in_manifest = name  # can differ from .name later (if borg check fixed duplicate archive names)"
            },
            "2": {
                "beforePatchRowNumber": 495,
                "afterPatchRowNumber": 495,
                "PatchRowcode": "         self.comment = None"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 496,
                "PatchRowcode": "+        self.tam_verified = False"
            },
            "4": {
                "beforePatchRowNumber": 496,
                "afterPatchRowNumber": 497,
                "PatchRowcode": "         self.numeric_ids = numeric_ids"
            },
            "5": {
                "beforePatchRowNumber": 497,
                "afterPatchRowNumber": 498,
                "PatchRowcode": "         self.noatime = noatime"
            },
            "6": {
                "beforePatchRowNumber": 498,
                "afterPatchRowNumber": 499,
                "PatchRowcode": "         self.noctime = noctime"
            },
            "7": {
                "beforePatchRowNumber": 532,
                "afterPatchRowNumber": 533,
                "PatchRowcode": "     def _load_meta(self, id):"
            },
            "8": {
                "beforePatchRowNumber": 533,
                "afterPatchRowNumber": 534,
                "PatchRowcode": "         cdata = self.repository.get(id)"
            },
            "9": {
                "beforePatchRowNumber": 534,
                "afterPatchRowNumber": 535,
                "PatchRowcode": "         _, data = self.repo_objs.parse(id, cdata)"
            },
            "10": {
                "beforePatchRowNumber": 535,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        metadata = ArchiveItem(internal_dict=msgpack.unpackb(data))"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 536,
                "PatchRowcode": "+        # we do not require TAM for archives, otherwise we can not even borg list a repo with old archives."
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 537,
                "PatchRowcode": "+        archive, self.tam_verified, _ = self.key.unpack_and_verify_archive(data, force_tam_not_required=True)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 538,
                "PatchRowcode": "+        metadata = ArchiveItem(internal_dict=archive)"
            },
            "14": {
                "beforePatchRowNumber": 536,
                "afterPatchRowNumber": 539,
                "PatchRowcode": "         if metadata.version not in (1, 2):  # legacy: still need to read v1 archives"
            },
            "15": {
                "beforePatchRowNumber": 537,
                "afterPatchRowNumber": 540,
                "PatchRowcode": "             raise Exception(\"Unknown archive metadata version\")"
            },
            "16": {
                "beforePatchRowNumber": 538,
                "afterPatchRowNumber": 541,
                "PatchRowcode": "         # note: metadata.items must not get written to disk!"
            },
            "17": {
                "beforePatchRowNumber": 1024,
                "afterPatchRowNumber": 1027,
                "PatchRowcode": "         setattr(metadata, key, value)"
            },
            "18": {
                "beforePatchRowNumber": 1025,
                "afterPatchRowNumber": 1028,
                "PatchRowcode": "         if \"items\" in metadata:"
            },
            "19": {
                "beforePatchRowNumber": 1026,
                "afterPatchRowNumber": 1029,
                "PatchRowcode": "             del metadata.items"
            },
            "20": {
                "beforePatchRowNumber": 1027,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        data = msgpack.packb(metadata.as_dict())"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1030,
                "PatchRowcode": "+        data = self.key.pack_and_authenticate_metadata(metadata.as_dict(), context=b\"archive\")"
            },
            "22": {
                "beforePatchRowNumber": 1028,
                "afterPatchRowNumber": 1031,
                "PatchRowcode": "         new_id = self.key.id_hash(data)"
            },
            "23": {
                "beforePatchRowNumber": 1029,
                "afterPatchRowNumber": 1032,
                "PatchRowcode": "         self.cache.add_chunk(new_id, {}, data, stats=self.stats)"
            },
            "24": {
                "beforePatchRowNumber": 1030,
                "afterPatchRowNumber": 1033,
                "PatchRowcode": "         self.manifest.archives[self.name] = (new_id, metadata.time)"
            },
            "25": {
                "beforePatchRowNumber": 1992,
                "afterPatchRowNumber": 1995,
                "PatchRowcode": "             except msgpack.UnpackException:"
            },
            "26": {
                "beforePatchRowNumber": 1993,
                "afterPatchRowNumber": 1996,
                "PatchRowcode": "                 continue"
            },
            "27": {
                "beforePatchRowNumber": 1994,
                "afterPatchRowNumber": 1997,
                "PatchRowcode": "             if valid_archive(archive):"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1998,
                "PatchRowcode": "+                # **after** doing the low-level checks and having a strong indication that we"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1999,
                "PatchRowcode": "+                # are likely looking at an archive item here, also check the TAM authentication:"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2000,
                "PatchRowcode": "+                try:"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2001,
                "PatchRowcode": "+                    archive, verified, _ = self.key.unpack_and_verify_archive(data, force_tam_not_required=False)"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2002,
                "PatchRowcode": "+                except IntegrityError:"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2003,
                "PatchRowcode": "+                    # TAM issues - do not accept this archive!"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2004,
                "PatchRowcode": "+                    # either somebody is trying to attack us with a fake archive data or"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2005,
                "PatchRowcode": "+                    # we have an ancient archive made before TAM was a thing (borg < 1.0.9) **and** this repo"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2006,
                "PatchRowcode": "+                    # was not correctly upgraded to borg 1.2.5 (see advisory at top of the changelog)."
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2007,
                "PatchRowcode": "+                    # borg can't tell the difference, so it has to assume this archive might be an attack"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2008,
                "PatchRowcode": "+                    # and drops this archive."
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2009,
                "PatchRowcode": "+                    continue"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2010,
                "PatchRowcode": "+                # note: if we get here and verified is False, a TAM is not required."
            },
            "41": {
                "beforePatchRowNumber": 1995,
                "afterPatchRowNumber": 2011,
                "PatchRowcode": "                 archive = ArchiveItem(internal_dict=archive)"
            },
            "42": {
                "beforePatchRowNumber": 1996,
                "afterPatchRowNumber": 2012,
                "PatchRowcode": "                 name = archive.name"
            },
            "43": {
                "beforePatchRowNumber": 1997,
                "afterPatchRowNumber": 2013,
                "PatchRowcode": "                 logger.info(\"Found archive %s\", name)"
            },
            "44": {
                "beforePatchRowNumber": 2248,
                "afterPatchRowNumber": 2264,
                "PatchRowcode": "                     self.error_found = True"
            },
            "45": {
                "beforePatchRowNumber": 2249,
                "afterPatchRowNumber": 2265,
                "PatchRowcode": "                     del self.manifest.archives[info.name]"
            },
            "46": {
                "beforePatchRowNumber": 2250,
                "afterPatchRowNumber": 2266,
                "PatchRowcode": "                     continue"
            },
            "47": {
                "beforePatchRowNumber": 2251,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                archive = ArchiveItem(internal_dict=msgpack.unpackb(data))"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2267,
                "PatchRowcode": "+                try:"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2268,
                "PatchRowcode": "+                    archive, verified, salt = self.key.unpack_and_verify_archive(data, force_tam_not_required=False)"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2269,
                "PatchRowcode": "+                except IntegrityError as integrity_error:"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2270,
                "PatchRowcode": "+                    # looks like there is a TAM issue with this archive, this might be an attack!"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2271,
                "PatchRowcode": "+                    # when upgrading to borg 1.2.5, users are expected to TAM-authenticate all archives they"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2272,
                "PatchRowcode": "+                    # trust, so there shouldn't be any without TAM."
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2273,
                "PatchRowcode": "+                    logger.error(\"Archive TAM authentication issue for archive %s: %s\", info.name, integrity_error)"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2274,
                "PatchRowcode": "+                    self.error_found = True"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2275,
                "PatchRowcode": "+                    del self.manifest.archives[info.name]"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2276,
                "PatchRowcode": "+                    continue"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2277,
                "PatchRowcode": "+                archive = ArchiveItem(internal_dict=archive)"
            },
            "59": {
                "beforePatchRowNumber": 2252,
                "afterPatchRowNumber": 2278,
                "PatchRowcode": "                 if archive.version != 2:"
            },
            "60": {
                "beforePatchRowNumber": 2253,
                "afterPatchRowNumber": 2279,
                "PatchRowcode": "                     raise Exception(\"Unknown archive metadata version\")"
            },
            "61": {
                "beforePatchRowNumber": 2254,
                "afterPatchRowNumber": 2280,
                "PatchRowcode": "                 items_buffer = ChunkBuffer(self.key)"
            },
            "62": {
                "beforePatchRowNumber": 2267,
                "afterPatchRowNumber": 2293,
                "PatchRowcode": "                 archive.item_ptrs = archive_put_items("
            },
            "63": {
                "beforePatchRowNumber": 2268,
                "afterPatchRowNumber": 2294,
                "PatchRowcode": "                     items_buffer.chunks, repo_objs=self.repo_objs, add_reference=add_reference"
            },
            "64": {
                "beforePatchRowNumber": 2269,
                "afterPatchRowNumber": 2295,
                "PatchRowcode": "                 )"
            },
            "65": {
                "beforePatchRowNumber": 2270,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                data = msgpack.packb(archive.as_dict())"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2296,
                "PatchRowcode": "+                data = self.key.pack_and_authenticate_metadata(archive.as_dict(), context=b\"archive\", salt=salt)"
            },
            "67": {
                "beforePatchRowNumber": 2271,
                "afterPatchRowNumber": 2297,
                "PatchRowcode": "                 new_archive_id = self.key.id_hash(data)"
            },
            "68": {
                "beforePatchRowNumber": 2272,
                "afterPatchRowNumber": 2298,
                "PatchRowcode": "                 cdata = self.repo_objs.format(new_archive_id, {}, data)"
            },
            "69": {
                "beforePatchRowNumber": 2273,
                "afterPatchRowNumber": 2299,
                "PatchRowcode": "                 add_reference(new_archive_id, len(data), cdata)"
            }
        },
        "frontPatchFile": [
            "import base64",
            "import json",
            "import os",
            "import stat",
            "import sys",
            "import time",
            "from collections import OrderedDict, defaultdict",
            "from contextlib import contextmanager",
            "from datetime import timedelta",
            "from functools import partial",
            "from getpass import getuser",
            "from io import BytesIO",
            "from itertools import groupby, zip_longest",
            "from typing import Iterator",
            "from shutil import get_terminal_size",
            "",
            "from .platformflags import is_win32",
            "from .logger import create_logger",
            "",
            "logger = create_logger()",
            "",
            "from . import xattr",
            "from .chunker import get_chunker, Chunk",
            "from .cache import ChunkListEntry",
            "from .crypto.key import key_factory, UnsupportedPayloadError",
            "from .compress import Compressor, CompressionSpec",
            "from .constants import *  # NOQA",
            "from .crypto.low_level import IntegrityError as IntegrityErrorBase",
            "from .hashindex import ChunkIndex, ChunkIndexEntry, CacheSynchronizer",
            "from .helpers import HardLinkManager",
            "from .helpers import ChunkIteratorFileWrapper, open_item",
            "from .helpers import Error, IntegrityError, set_ec",
            "from .platform import uid2user, user2uid, gid2group, group2gid",
            "from .helpers import parse_timestamp, archive_ts_now",
            "from .helpers import OutputTimestamp, format_timedelta, format_file_size, file_status, FileSize",
            "from .helpers import safe_encode, make_path_safe, remove_surrogates, text_to_json, join_cmd, remove_dotdot_prefixes",
            "from .helpers import StableDict",
            "from .helpers import bin_to_hex",
            "from .helpers import safe_ns",
            "from .helpers import ellipsis_truncate, ProgressIndicatorPercent, log_multi",
            "from .helpers import os_open, flags_normal, flags_dir",
            "from .helpers import os_stat",
            "from .helpers import msgpack",
            "from .helpers import sig_int",
            "from .helpers.lrucache import LRUCache",
            "from .manifest import Manifest",
            "from .patterns import PathPrefixPattern, FnmatchPattern, IECommand",
            "from .item import Item, ArchiveItem, ItemDiff",
            "from .platform import acl_get, acl_set, set_flags, get_flags, swidth, hostname",
            "from .remote import cache_if_remote",
            "from .repository import Repository, LIST_SCAN_LIMIT",
            "from .repoobj import RepoObj",
            "",
            "has_link = hasattr(os, \"link\")",
            "",
            "",
            "class Statistics:",
            "    def __init__(self, output_json=False, iec=False):",
            "        self.output_json = output_json",
            "        self.iec = iec",
            "        self.osize = self.usize = self.nfiles = 0",
            "        self.last_progress = 0  # timestamp when last progress was shown",
            "        self.files_stats = defaultdict(int)",
            "        self.chunking_time = 0.0",
            "        self.hashing_time = 0.0",
            "        self.rx_bytes = 0",
            "        self.tx_bytes = 0",
            "",
            "    def update(self, size, unique):",
            "        self.osize += size",
            "        if unique:",
            "            self.usize += size",
            "",
            "    def __add__(self, other):",
            "        if not isinstance(other, Statistics):",
            "            raise TypeError(\"can only add Statistics objects\")",
            "        stats = Statistics(self.output_json, self.iec)",
            "        stats.osize = self.osize + other.osize",
            "        stats.usize = self.usize + other.usize",
            "        stats.nfiles = self.nfiles + other.nfiles",
            "        stats.chunking_time = self.chunking_time + other.chunking_time",
            "        stats.hashing_time = self.hashing_time + other.hashing_time",
            "        st1, st2 = self.files_stats, other.files_stats",
            "        stats.files_stats = defaultdict(int, {key: (st1[key] + st2[key]) for key in st1.keys() | st2.keys()})",
            "",
            "        return stats",
            "",
            "    def __str__(self):",
            "        hashing_time = format_timedelta(timedelta(seconds=self.hashing_time))",
            "        chunking_time = format_timedelta(timedelta(seconds=self.chunking_time))",
            "        return \"\"\"\\",
            "Number of files: {stats.nfiles}",
            "Original size: {stats.osize_fmt}",
            "Deduplicated size: {stats.usize_fmt}",
            "Time spent in hashing: {hashing_time}",
            "Time spent in chunking: {chunking_time}",
            "Added files: {added_files}",
            "Unchanged files: {unchanged_files}",
            "Modified files: {modified_files}",
            "Error files: {error_files}",
            "Files changed while reading: {files_changed_while_reading}",
            "Bytes read from remote: {stats.rx_bytes}",
            "Bytes sent to remote: {stats.tx_bytes}",
            "\"\"\".format(",
            "            stats=self,",
            "            hashing_time=hashing_time,",
            "            chunking_time=chunking_time,",
            "            added_files=self.files_stats[\"A\"],",
            "            unchanged_files=self.files_stats[\"U\"],",
            "            modified_files=self.files_stats[\"M\"],",
            "            error_files=self.files_stats[\"E\"],",
            "            files_changed_while_reading=self.files_stats[\"C\"],",
            "        )",
            "",
            "    def __repr__(self):",
            "        return \"<{cls} object at {hash:#x} ({self.osize}, {self.usize})>\".format(",
            "            cls=type(self).__name__, hash=id(self), self=self",
            "        )",
            "",
            "    def as_dict(self):",
            "        return {",
            "            \"original_size\": FileSize(self.osize, iec=self.iec),",
            "            \"deduplicated_size\": FileSize(self.usize, iec=self.iec),",
            "            \"nfiles\": self.nfiles,",
            "            \"hashing_time\": self.hashing_time,",
            "            \"chunking_time\": self.chunking_time,",
            "            \"files_stats\": self.files_stats,",
            "        }",
            "",
            "    def as_raw_dict(self):",
            "        return {\"size\": self.osize, \"nfiles\": self.nfiles}",
            "",
            "    @classmethod",
            "    def from_raw_dict(cls, **kw):",
            "        self = cls()",
            "        self.osize = kw[\"size\"]",
            "        self.nfiles = kw[\"nfiles\"]",
            "        return self",
            "",
            "    @property",
            "    def osize_fmt(self):",
            "        return format_file_size(self.osize, iec=self.iec)",
            "",
            "    @property",
            "    def usize_fmt(self):",
            "        return format_file_size(self.usize, iec=self.iec)",
            "",
            "    def show_progress(self, item=None, final=False, stream=None, dt=None):",
            "        now = time.monotonic()",
            "        if dt is None or now - self.last_progress > dt:",
            "            self.last_progress = now",
            "            if self.output_json:",
            "                if not final:",
            "                    data = self.as_dict()",
            "                    if item:",
            "                        data.update(text_to_json(\"path\", item.path))",
            "                else:",
            "                    data = {}",
            "                data.update({\"time\": time.time(), \"type\": \"archive_progress\", \"finished\": final})",
            "                msg = json.dumps(data)",
            "                end = \"\\n\"",
            "            else:",
            "                columns, lines = get_terminal_size()",
            "                if not final:",
            "                    msg = \"{0.osize_fmt} O {0.usize_fmt} U {0.nfiles} N \".format(self)",
            "                    path = remove_surrogates(item.path) if item else \"\"",
            "                    space = columns - swidth(msg)",
            "                    if space < 12:",
            "                        msg = \"\"",
            "                        space = columns - swidth(msg)",
            "                    if space >= 8:",
            "                        msg += ellipsis_truncate(path, space)",
            "                else:",
            "                    msg = \" \" * columns",
            "                end = \"\\r\"",
            "            print(msg, end=end, file=stream or sys.stderr, flush=True)",
            "",
            "",
            "def is_special(mode):",
            "    # file types that get special treatment in --read-special mode",
            "    return stat.S_ISBLK(mode) or stat.S_ISCHR(mode) or stat.S_ISFIFO(mode)",
            "",
            "",
            "class BackupError(Exception):",
            "    \"\"\"",
            "    Exception raised for non-OSError-based exceptions while accessing backup files.",
            "    \"\"\"",
            "",
            "",
            "class BackupOSError(Exception):",
            "    \"\"\"",
            "    Wrapper for OSError raised while accessing backup files.",
            "",
            "    Borg does different kinds of IO, and IO failures have different consequences.",
            "    This wrapper represents failures of input file or extraction IO.",
            "    These are non-critical and are only reported (exit code = 1, warning).",
            "",
            "    Any unwrapped IO error is critical and aborts execution (for example repository IO failure).",
            "    \"\"\"",
            "",
            "    def __init__(self, op, os_error):",
            "        self.op = op",
            "        self.os_error = os_error",
            "        self.errno = os_error.errno",
            "        self.strerror = os_error.strerror",
            "        self.filename = os_error.filename",
            "",
            "    def __str__(self):",
            "        if self.op:",
            "            return f\"{self.op}: {self.os_error}\"",
            "        else:",
            "            return str(self.os_error)",
            "",
            "",
            "class BackupIO:",
            "    op = \"\"",
            "",
            "    def __call__(self, op=\"\"):",
            "        self.op = op",
            "        return self",
            "",
            "    def __enter__(self):",
            "        pass",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        if exc_type and issubclass(exc_type, OSError):",
            "            raise BackupOSError(self.op, exc_val) from exc_val",
            "",
            "",
            "backup_io = BackupIO()",
            "",
            "",
            "def backup_io_iter(iterator):",
            "    backup_io.op = \"read\"",
            "    while True:",
            "        with backup_io:",
            "            try:",
            "                item = next(iterator)",
            "            except StopIteration:",
            "                return",
            "        yield item",
            "",
            "",
            "def stat_update_check(st_old, st_curr):",
            "    \"\"\"",
            "    this checks for some race conditions between the first filename-based stat()",
            "    we did before dispatching to the (hopefully correct) file type backup handler",
            "    and the (hopefully) fd-based fstat() we did in the handler.",
            "",
            "    if there is a problematic difference (e.g. file type changed), we rather",
            "    skip the file than being tricked into a security problem.",
            "",
            "    such races should only happen if:",
            "    - we are backing up a live filesystem (no snapshot, not inactive)",
            "    - if files change due to normal fs activity at an unfortunate time",
            "    - if somebody is doing an attack against us",
            "    \"\"\"",
            "    # assuming that a file type change implicates a different inode change AND that inode numbers",
            "    # are not duplicate in a short timeframe, this check is redundant and solved by the ino check:",
            "    if stat.S_IFMT(st_old.st_mode) != stat.S_IFMT(st_curr.st_mode):",
            "        # in this case, we dispatched to wrong handler - abort",
            "        raise BackupError(\"file type changed (race condition), skipping file\")",
            "    if st_old.st_ino != st_curr.st_ino:",
            "        # in this case, the hardlinks-related code in create_helper has the wrong inode - abort!",
            "        raise BackupError(\"file inode changed (race condition), skipping file\")",
            "    # looks ok, we are still dealing with the same thing - return current stat:",
            "    return st_curr",
            "",
            "",
            "@contextmanager",
            "def OsOpen(*, flags, path=None, parent_fd=None, name=None, noatime=False, op=\"open\"):",
            "    with backup_io(op):",
            "        fd = os_open(path=path, parent_fd=parent_fd, name=name, flags=flags, noatime=noatime)",
            "    try:",
            "        yield fd",
            "    finally:",
            "        # On windows fd is None for directories.",
            "        if fd is not None:",
            "            os.close(fd)",
            "",
            "",
            "class DownloadPipeline:",
            "    def __init__(self, repository, repo_objs):",
            "        self.repository = repository",
            "        self.repo_objs = repo_objs",
            "",
            "    def unpack_many(self, ids, *, filter=None, preload=False):",
            "        \"\"\"",
            "        Return iterator of items.",
            "",
            "        *ids* is a chunk ID list of an item stream. *filter* is a callable",
            "        to decide whether an item will be yielded. *preload* preloads the data chunks of every yielded item.",
            "",
            "        Warning: if *preload* is True then all data chunks of every yielded item have to be retrieved,",
            "        otherwise preloaded chunks will accumulate in RemoteRepository and create a memory leak.",
            "        \"\"\"",
            "        hlids_preloaded = set()",
            "        unpacker = msgpack.Unpacker(use_list=False)",
            "        for data in self.fetch_many(ids):",
            "            unpacker.feed(data)",
            "            for _item in unpacker:",
            "                item = Item(internal_dict=_item)",
            "                if \"chunks\" in item:",
            "                    item.chunks = [ChunkListEntry(*e) for e in item.chunks]",
            "                if filter and not filter(item):",
            "                    continue",
            "                if preload and \"chunks\" in item:",
            "                    hlid = item.get(\"hlid\", None)",
            "                    if hlid is None:",
            "                        preload_chunks = True",
            "                    elif hlid in hlids_preloaded:",
            "                        preload_chunks = False",
            "                    else:",
            "                        # not having the hardlink's chunks already preloaded for other hardlink to same inode",
            "                        preload_chunks = True",
            "                        hlids_preloaded.add(hlid)",
            "                    if preload_chunks:",
            "                        self.repository.preload([c.id for c in item.chunks])",
            "                yield item",
            "",
            "    def fetch_many(self, ids, is_preloaded=False):",
            "        for id_, cdata in zip(ids, self.repository.get_many(ids, is_preloaded=is_preloaded)):",
            "            _, data = self.repo_objs.parse(id_, cdata)",
            "            yield data",
            "",
            "",
            "class ChunkBuffer:",
            "    BUFFER_SIZE = 8 * 1024 * 1024",
            "",
            "    def __init__(self, key, chunker_params=ITEMS_CHUNKER_PARAMS):",
            "        self.buffer = BytesIO()",
            "        self.packer = msgpack.Packer()",
            "        self.chunks = []",
            "        self.key = key",
            "        self.chunker = get_chunker(*chunker_params, seed=self.key.chunk_seed, sparse=False)",
            "        self.saved_chunks_len = None",
            "",
            "    def add(self, item):",
            "        self.buffer.write(self.packer.pack(item.as_dict()))",
            "        if self.is_full():",
            "            self.flush()",
            "",
            "    def write_chunk(self, chunk):",
            "        raise NotImplementedError",
            "",
            "    def flush(self, flush=False):",
            "        if self.buffer.tell() == 0:",
            "            return",
            "        self.buffer.seek(0)",
            "        # The chunker returns a memoryview to its internal buffer,",
            "        # thus a copy is needed before resuming the chunker iterator.",
            "        # the metadata stream may produce all-zero chunks, so deal",
            "        # with CH_ALLOC (and CH_HOLE, for completeness) here.",
            "        chunks = []",
            "        for chunk in self.chunker.chunkify(self.buffer):",
            "            alloc = chunk.meta[\"allocation\"]",
            "            if alloc == CH_DATA:",
            "                data = bytes(chunk.data)",
            "            elif alloc in (CH_ALLOC, CH_HOLE):",
            "                data = zeros[: chunk.meta[\"size\"]]",
            "            else:",
            "                raise ValueError(\"chunk allocation has unsupported value of %r\" % alloc)",
            "            chunks.append(data)",
            "        self.buffer.seek(0)",
            "        self.buffer.truncate(0)",
            "        # Leave the last partial chunk in the buffer unless flush is True",
            "        end = None if flush or len(chunks) == 1 else -1",
            "        for chunk in chunks[:end]:",
            "            self.chunks.append(self.write_chunk(chunk))",
            "        if end == -1:",
            "            self.buffer.write(chunks[-1])",
            "",
            "    def is_full(self):",
            "        return self.buffer.tell() > self.BUFFER_SIZE",
            "",
            "    def save_chunks_state(self):",
            "        # as we only append to self.chunks, remembering the current length is good enough",
            "        self.saved_chunks_len = len(self.chunks)",
            "",
            "    def restore_chunks_state(self):",
            "        scl = self.saved_chunks_len",
            "        assert scl is not None, \"forgot to call save_chunks_state?\"",
            "        tail_chunks = self.chunks[scl:]",
            "        del self.chunks[scl:]",
            "        self.saved_chunks_len = None",
            "        return tail_chunks",
            "",
            "",
            "class CacheChunkBuffer(ChunkBuffer):",
            "    def __init__(self, cache, key, stats, chunker_params=ITEMS_CHUNKER_PARAMS):",
            "        super().__init__(key, chunker_params)",
            "        self.cache = cache",
            "        self.stats = stats",
            "",
            "    def write_chunk(self, chunk):",
            "        id_, _ = self.cache.add_chunk(self.key.id_hash(chunk), {}, chunk, stats=self.stats, wait=False)",
            "        logger.debug(f\"writing item metadata stream chunk {bin_to_hex(id_)}\")",
            "        self.cache.repository.async_response(wait=False)",
            "        return id_",
            "",
            "",
            "def get_item_uid_gid(item, *, numeric, uid_forced=None, gid_forced=None, uid_default=0, gid_default=0):",
            "    if uid_forced is not None:",
            "        uid = uid_forced",
            "    else:",
            "        uid = None if numeric else user2uid(item.get(\"user\"))",
            "        uid = item.get(\"uid\") if uid is None else uid",
            "        if uid is None or uid < 0:",
            "            uid = uid_default",
            "    if gid_forced is not None:",
            "        gid = gid_forced",
            "    else:",
            "        gid = None if numeric else group2gid(item.get(\"group\"))",
            "        gid = item.get(\"gid\") if gid is None else gid",
            "        if gid is None or gid < 0:",
            "            gid = gid_default",
            "    return uid, gid",
            "",
            "",
            "def archive_get_items(metadata, *, repo_objs, repository):",
            "    if \"item_ptrs\" in metadata:  # looks like a v2+ archive",
            "        assert \"items\" not in metadata",
            "        items = []",
            "        for id, cdata in zip(metadata.item_ptrs, repository.get_many(metadata.item_ptrs)):",
            "            _, data = repo_objs.parse(id, cdata)",
            "            ids = msgpack.unpackb(data)",
            "            items.extend(ids)",
            "        return items",
            "",
            "    if \"items\" in metadata:  # legacy, v1 archive",
            "        assert \"item_ptrs\" not in metadata",
            "        return metadata.items",
            "",
            "",
            "def archive_put_items(chunk_ids, *, repo_objs, cache=None, stats=None, add_reference=None):",
            "    \"\"\"gets a (potentially large) list of archive metadata stream chunk ids and writes them to repo objects\"\"\"",
            "    item_ptrs = []",
            "    for i in range(0, len(chunk_ids), IDS_PER_CHUNK):",
            "        data = msgpack.packb(chunk_ids[i : i + IDS_PER_CHUNK])",
            "        id = repo_objs.id_hash(data)",
            "        logger.debug(f\"writing item_ptrs chunk {bin_to_hex(id)}\")",
            "        if cache is not None and stats is not None:",
            "            cache.add_chunk(id, {}, data, stats=stats)",
            "        elif add_reference is not None:",
            "            cdata = repo_objs.format(id, {}, data)",
            "            add_reference(id, len(data), cdata)",
            "        else:",
            "            raise NotImplementedError",
            "        item_ptrs.append(id)",
            "    return item_ptrs",
            "",
            "",
            "class Archive:",
            "    class DoesNotExist(Error):",
            "        \"\"\"Archive {} does not exist\"\"\"",
            "",
            "    class AlreadyExists(Error):",
            "        \"\"\"Archive {} already exists\"\"\"",
            "",
            "    class IncompatibleFilesystemEncodingError(Error):",
            "        \"\"\"Failed to encode filename \"{}\" into file system encoding \"{}\". Consider configuring the LANG environment variable.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        manifest,",
            "        name,",
            "        cache=None,",
            "        create=False,",
            "        numeric_ids=False,",
            "        noatime=False,",
            "        noctime=False,",
            "        noflags=False,",
            "        noacls=False,",
            "        noxattrs=False,",
            "        progress=False,",
            "        chunker_params=CHUNKER_PARAMS,",
            "        start=None,",
            "        start_monotonic=None,",
            "        end=None,",
            "        log_json=False,",
            "        iec=False,",
            "    ):",
            "        self.cwd = os.getcwd()",
            "        assert isinstance(manifest, Manifest)",
            "        self.manifest = manifest",
            "        self.key = manifest.repo_objs.key",
            "        self.repo_objs = manifest.repo_objs",
            "        self.repository = manifest.repository",
            "        self.cache = cache",
            "        self.stats = Statistics(output_json=log_json, iec=iec)",
            "        self.iec = iec",
            "        self.show_progress = progress",
            "        self.name = name  # overwritten later with name from archive metadata",
            "        self.name_in_manifest = name  # can differ from .name later (if borg check fixed duplicate archive names)",
            "        self.comment = None",
            "        self.numeric_ids = numeric_ids",
            "        self.noatime = noatime",
            "        self.noctime = noctime",
            "        self.noflags = noflags",
            "        self.noacls = noacls",
            "        self.noxattrs = noxattrs",
            "        assert (start is None) == (",
            "            start_monotonic is None",
            "        ), \"Logic error: if start is given, start_monotonic must be given as well and vice versa.\"",
            "        if start is None:",
            "            start = archive_ts_now()",
            "            start_monotonic = time.monotonic()",
            "        self.chunker_params = chunker_params",
            "        self.start = start",
            "        self.start_monotonic = start_monotonic",
            "        if end is None:",
            "            end = archive_ts_now()",
            "        self.end = end",
            "        self.pipeline = DownloadPipeline(self.repository, self.repo_objs)",
            "        self.create = create",
            "        if self.create:",
            "            self.items_buffer = CacheChunkBuffer(self.cache, self.key, self.stats)",
            "            if name in manifest.archives:",
            "                raise self.AlreadyExists(name)",
            "            i = 0",
            "            while True:",
            "                self.checkpoint_name = \"{}.checkpoint{}\".format(name, i and (\".%d\" % i) or \"\")",
            "                if self.checkpoint_name not in manifest.archives:",
            "                    break",
            "                i += 1",
            "        else:",
            "            info = self.manifest.archives.get(name)",
            "            if info is None:",
            "                raise self.DoesNotExist(name)",
            "            self.load(info.id)",
            "",
            "    def _load_meta(self, id):",
            "        cdata = self.repository.get(id)",
            "        _, data = self.repo_objs.parse(id, cdata)",
            "        metadata = ArchiveItem(internal_dict=msgpack.unpackb(data))",
            "        if metadata.version not in (1, 2):  # legacy: still need to read v1 archives",
            "            raise Exception(\"Unknown archive metadata version\")",
            "        # note: metadata.items must not get written to disk!",
            "        metadata.items = archive_get_items(metadata, repo_objs=self.repo_objs, repository=self.repository)",
            "        return metadata",
            "",
            "    def load(self, id):",
            "        self.id = id",
            "        self.metadata = self._load_meta(self.id)",
            "        self.name = self.metadata.name",
            "        self.comment = self.metadata.get(\"comment\", \"\")",
            "",
            "    @property",
            "    def ts(self):",
            "        \"\"\"Timestamp of archive creation (start) in UTC\"\"\"",
            "        ts = self.metadata.time",
            "        return parse_timestamp(ts)",
            "",
            "    @property",
            "    def ts_end(self):",
            "        \"\"\"Timestamp of archive creation (end) in UTC\"\"\"",
            "        # fall back to time if there is no time_end present in metadata",
            "        ts = self.metadata.get(\"time_end\") or self.metadata.time",
            "        return parse_timestamp(ts)",
            "",
            "    @property",
            "    def fpr(self):",
            "        return bin_to_hex(self.id)",
            "",
            "    @property",
            "    def duration(self):",
            "        return format_timedelta(self.end - self.start)",
            "",
            "    @property",
            "    def duration_from_meta(self):",
            "        return format_timedelta(self.ts_end - self.ts)",
            "",
            "    def info(self):",
            "        if self.create:",
            "            stats = self.stats",
            "            start = self.start",
            "            end = self.end",
            "        else:",
            "            stats = self.calc_stats(self.cache)",
            "            start = self.ts",
            "            end = self.ts_end",
            "        info = {",
            "            \"name\": self.name,",
            "            \"id\": self.fpr,",
            "            \"start\": OutputTimestamp(start),",
            "            \"end\": OutputTimestamp(end),",
            "            \"duration\": (end - start).total_seconds(),",
            "            \"stats\": stats.as_dict(),",
            "        }",
            "        if self.create:",
            "            info[\"command_line\"] = join_cmd(sys.argv)",
            "        else:",
            "            info.update(",
            "                {",
            "                    \"command_line\": self.metadata.command_line,",
            "                    \"hostname\": self.metadata.hostname,",
            "                    \"username\": self.metadata.username,",
            "                    \"comment\": self.metadata.get(\"comment\", \"\"),",
            "                    \"chunker_params\": self.metadata.get(\"chunker_params\", \"\"),",
            "                }",
            "            )",
            "        return info",
            "",
            "    def __str__(self):",
            "        return \"\"\"\\",
            "Repository: {location}",
            "Archive name: {0.name}",
            "Archive fingerprint: {0.fpr}",
            "Time (start): {start}",
            "Time (end):   {end}",
            "Duration: {0.duration}",
            "\"\"\".format(",
            "            self,",
            "            start=OutputTimestamp(self.start),",
            "            end=OutputTimestamp(self.end),",
            "            location=self.repository._location.canonical_path(),",
            "        )",
            "",
            "    def __repr__(self):",
            "        return \"Archive(%r)\" % self.name",
            "",
            "    def item_filter(self, item, filter=None):",
            "        return filter(item) if filter else True",
            "",
            "    def iter_items(self, filter=None, preload=False):",
            "        # note: when calling this with preload=True, later fetch_many() must be called with",
            "        # is_preloaded=True or the RemoteRepository code will leak memory!",
            "        yield from self.pipeline.unpack_many(",
            "            self.metadata.items, preload=preload, filter=lambda item: self.item_filter(item, filter)",
            "        )",
            "",
            "    def add_item(self, item, show_progress=True, stats=None):",
            "        if show_progress and self.show_progress:",
            "            if stats is None:",
            "                stats = self.stats",
            "            stats.show_progress(item=item, dt=0.2)",
            "        self.items_buffer.add(item)",
            "",
            "    def prepare_checkpoint(self):",
            "        # we need to flush the archive metadata stream to repo chunks, so that",
            "        # we have the metadata stream chunks WITHOUT the part file item we add later.",
            "        # The part file item will then get into its own metadata stream chunk, which we",
            "        # can easily NOT include into the next checkpoint or the final archive.",
            "        self.items_buffer.flush(flush=True)",
            "        # remember the current state of self.chunks, which corresponds to the flushed chunks",
            "        self.items_buffer.save_chunks_state()",
            "",
            "    def write_checkpoint(self):",
            "        metadata = self.save(self.checkpoint_name)",
            "        # that .save() has committed the repo.",
            "        # at next commit, we won't need this checkpoint archive any more because we will then",
            "        # have either a newer checkpoint archive or the final archive.",
            "        # so we can already remove it here, the next .save() will then commit this cleanup.",
            "        # remove its manifest entry, remove its ArchiveItem chunk, remove its item_ptrs chunks:",
            "        del self.manifest.archives[self.checkpoint_name]",
            "        self.cache.chunk_decref(self.id, self.stats)",
            "        for id in metadata.item_ptrs:",
            "            self.cache.chunk_decref(id, self.stats)",
            "        # also get rid of that part item, we do not want to have it in next checkpoint or final archive",
            "        tail_chunks = self.items_buffer.restore_chunks_state()",
            "        # tail_chunks contain the tail of the archive items metadata stream, not needed for next commit.",
            "        for id in tail_chunks:",
            "            self.cache.chunk_decref(id, self.stats)",
            "",
            "    def save(self, name=None, comment=None, timestamp=None, stats=None, additional_metadata=None):",
            "        name = name or self.name",
            "        if name in self.manifest.archives:",
            "            raise self.AlreadyExists(name)",
            "        self.items_buffer.flush(flush=True)",
            "        item_ptrs = archive_put_items(",
            "            self.items_buffer.chunks, repo_objs=self.repo_objs, cache=self.cache, stats=self.stats",
            "        )",
            "        duration = timedelta(seconds=time.monotonic() - self.start_monotonic)",
            "        if timestamp is None:",
            "            end = archive_ts_now()",
            "            start = end - duration",
            "        else:",
            "            start = timestamp",
            "            end = start + duration",
            "        self.start = start",
            "        self.end = end",
            "        metadata = {",
            "            \"version\": 2,",
            "            \"name\": name,",
            "            \"comment\": comment or \"\",",
            "            \"item_ptrs\": item_ptrs,  # see #1473",
            "            \"command_line\": join_cmd(sys.argv),",
            "            \"hostname\": hostname,",
            "            \"username\": getuser(),",
            "            \"time\": start.isoformat(timespec=\"microseconds\"),",
            "            \"time_end\": end.isoformat(timespec=\"microseconds\"),",
            "            \"chunker_params\": self.chunker_params,",
            "        }",
            "        # we always want to create archives with the addtl. metadata (nfiles, etc.),",
            "        # because borg info relies on them. so, either use the given stats (from args)",
            "        # or fall back to self.stats if it was not given.",
            "        stats = stats or self.stats",
            "        metadata.update({\"size\": stats.osize, \"nfiles\": stats.nfiles})",
            "        metadata.update(additional_metadata or {})",
            "        metadata = ArchiveItem(metadata)",
            "        data = self.key.pack_and_authenticate_metadata(metadata.as_dict(), context=b\"archive\")",
            "        self.id = self.repo_objs.id_hash(data)",
            "        try:",
            "            self.cache.add_chunk(self.id, {}, data, stats=self.stats)",
            "        except IntegrityError as err:",
            "            err_msg = str(err)",
            "            # hack to avoid changing the RPC protocol by introducing new (more specific) exception class",
            "            if \"More than allowed put data\" in err_msg:",
            "                raise Error(\"%s - archive too big (issue #1473)!\" % err_msg)",
            "            else:",
            "                raise",
            "        while self.repository.async_response(wait=True) is not None:",
            "            pass",
            "        self.manifest.archives[name] = (self.id, metadata.time)",
            "        self.manifest.write()",
            "        self.repository.commit(compact=False)",
            "        self.cache.commit()",
            "        return metadata",
            "",
            "    def calc_stats(self, cache, want_unique=True):",
            "        if not want_unique:",
            "            unique_size = 0",
            "        else:",
            "",
            "            def add(id):",
            "                entry = cache.chunks[id]",
            "                archive_index.add(id, 1, entry.size)",
            "",
            "            archive_index = ChunkIndex()",
            "            sync = CacheSynchronizer(archive_index)",
            "            add(self.id)",
            "            # we must escape any % char in the archive name, because we use it in a format string, see #6500",
            "            arch_name_escd = self.name.replace(\"%\", \"%%\")",
            "            pi = ProgressIndicatorPercent(",
            "                total=len(self.metadata.items),",
            "                msg=\"Calculating statistics for archive %s ... %%3.0f%%%%\" % arch_name_escd,",
            "                msgid=\"archive.calc_stats\",",
            "            )",
            "            for id, chunk in zip(self.metadata.items, self.repository.get_many(self.metadata.items)):",
            "                pi.show(increase=1)",
            "                add(id)",
            "                _, data = self.repo_objs.parse(id, chunk)",
            "                sync.feed(data)",
            "            unique_size = archive_index.stats_against(cache.chunks)[1]",
            "            pi.finish()",
            "",
            "        stats = Statistics(iec=self.iec)",
            "        stats.usize = unique_size",
            "        stats.nfiles = self.metadata.nfiles",
            "        stats.osize = self.metadata.size",
            "        return stats",
            "",
            "    @contextmanager",
            "    def extract_helper(self, item, path, hlm, *, dry_run=False):",
            "        hardlink_set = False",
            "        # Hard link?",
            "        if \"hlid\" in item:",
            "            link_target = hlm.retrieve(id=item.hlid)",
            "            if link_target is not None and has_link:",
            "                if not dry_run:",
            "                    # another hardlink to same inode (same hlid) was extracted previously, just link to it",
            "                    with backup_io(\"link\"):",
            "                        os.link(link_target, path, follow_symlinks=False)",
            "                hardlink_set = True",
            "        yield hardlink_set",
            "        if not hardlink_set:",
            "            if \"hlid\" in item and has_link:",
            "                # Update entry with extracted item path, so that following hardlinks don't extract twice.",
            "                # We have hardlinking support, so we will hardlink not extract.",
            "                hlm.remember(id=item.hlid, info=path)",
            "            else:",
            "                # Broken platform with no hardlinking support.",
            "                # In this case, we *want* to extract twice, because there is no other way.",
            "                pass",
            "",
            "    def extract_item(",
            "        self,",
            "        item,",
            "        *,",
            "        restore_attrs=True,",
            "        dry_run=False,",
            "        stdout=False,",
            "        sparse=False,",
            "        hlm=None,",
            "        pi=None,",
            "        continue_extraction=False,",
            "    ):",
            "        \"\"\"",
            "        Extract archive item.",
            "",
            "        :param item: the item to extract",
            "        :param restore_attrs: restore file attributes",
            "        :param dry_run: do not write any data",
            "        :param stdout: write extracted data to stdout",
            "        :param sparse: write sparse files (chunk-granularity, independent of the original being sparse)",
            "        :param hlm: maps hlid to link_target for extracting subtrees with hardlinks correctly",
            "        :param pi: ProgressIndicatorPercent (or similar) for file extraction progress (in bytes)",
            "        :param continue_extraction: continue a previously interrupted extraction of same archive",
            "        \"\"\"",
            "",
            "        def same_item(item, st):",
            "            \"\"\"is the archived item the same as the fs item at same path with stat st?\"\"\"",
            "            if not stat.S_ISREG(st.st_mode):",
            "                # we only \"optimize\" for regular files.",
            "                # other file types are less frequent and have no content extraction we could \"optimize away\".",
            "                return False",
            "            if item.mode != st.st_mode or item.size != st.st_size:",
            "                # the size check catches incomplete previous file extraction",
            "                return False",
            "            if item.get(\"mtime\") != st.st_mtime_ns:",
            "                # note: mtime is \"extracted\" late, after xattrs and ACLs, but before flags.",
            "                return False",
            "            # this is good enough for the intended use case:",
            "            # continuing an extraction of same archive that initially started in an empty directory.",
            "            # there is a very small risk that \"bsdflags\" of one file are wrong:",
            "            # if a previous extraction was interrupted between setting the mtime and setting non-default flags.",
            "            return True",
            "",
            "        has_damaged_chunks = \"chunks_healthy\" in item",
            "        if dry_run or stdout:",
            "            with self.extract_helper(item, \"\", hlm, dry_run=dry_run or stdout) as hardlink_set:",
            "                if not hardlink_set:",
            "                    # it does not really set hardlinks due to dry_run, but we need to behave same",
            "                    # as non-dry_run concerning fetching preloaded chunks from the pipeline or",
            "                    # it would get stuck.",
            "                    if \"chunks\" in item:",
            "                        item_chunks_size = 0",
            "                        for data in self.pipeline.fetch_many([c.id for c in item.chunks], is_preloaded=True):",
            "                            if pi:",
            "                                pi.show(increase=len(data), info=[remove_surrogates(item.path)])",
            "                            if stdout:",
            "                                sys.stdout.buffer.write(data)",
            "                            item_chunks_size += len(data)",
            "                        if stdout:",
            "                            sys.stdout.buffer.flush()",
            "                        if \"size\" in item:",
            "                            item_size = item.size",
            "                            if item_size != item_chunks_size:",
            "                                raise BackupError(",
            "                                    \"Size inconsistency detected: size {}, chunks size {}\".format(",
            "                                        item_size, item_chunks_size",
            "                                    )",
            "                                )",
            "            if has_damaged_chunks:",
            "                raise BackupError(\"File has damaged (all-zero) chunks. Try running borg check --repair.\")",
            "            return",
            "",
            "        dest = self.cwd",
            "        path = os.path.join(dest, item.path)",
            "        # Attempt to remove existing files, ignore errors on failure",
            "        try:",
            "            st = os.stat(path, follow_symlinks=False)",
            "            if continue_extraction and same_item(item, st):",
            "                return  # done! we already have fully extracted this file in a previous run.",
            "            elif stat.S_ISDIR(st.st_mode):",
            "                os.rmdir(path)",
            "            else:",
            "                os.unlink(path)",
            "        except UnicodeEncodeError:",
            "            raise self.IncompatibleFilesystemEncodingError(path, sys.getfilesystemencoding()) from None",
            "        except OSError:",
            "            pass",
            "",
            "        def make_parent(path):",
            "            parent_dir = os.path.dirname(path)",
            "            if not os.path.exists(parent_dir):",
            "                os.makedirs(parent_dir)",
            "",
            "        mode = item.mode",
            "        if stat.S_ISREG(mode):",
            "            with backup_io(\"makedirs\"):",
            "                make_parent(path)",
            "            with self.extract_helper(item, path, hlm) as hardlink_set:",
            "                if hardlink_set:",
            "                    return",
            "                with backup_io(\"open\"):",
            "                    fd = open(path, \"wb\")",
            "                with fd:",
            "                    ids = [c.id for c in item.chunks]",
            "                    for data in self.pipeline.fetch_many(ids, is_preloaded=True):",
            "                        if pi:",
            "                            pi.show(increase=len(data), info=[remove_surrogates(item.path)])",
            "                        with backup_io(\"write\"):",
            "                            if sparse and zeros.startswith(data):",
            "                                # all-zero chunk: create a hole in a sparse file",
            "                                fd.seek(len(data), 1)",
            "                            else:",
            "                                fd.write(data)",
            "                    with backup_io(\"truncate_and_attrs\"):",
            "                        pos = item_chunks_size = fd.tell()",
            "                        fd.truncate(pos)",
            "                        fd.flush()",
            "                        self.restore_attrs(path, item, fd=fd.fileno())",
            "                if \"size\" in item:",
            "                    item_size = item.size",
            "                    if item_size != item_chunks_size:",
            "                        raise BackupError(",
            "                            f\"Size inconsistency detected: size {item_size}, chunks size {item_chunks_size}\"",
            "                        )",
            "                if has_damaged_chunks:",
            "                    raise BackupError(\"File has damaged (all-zero) chunks. Try running borg check --repair.\")",
            "            return",
            "        with backup_io:",
            "            # No repository access beyond this point.",
            "            if stat.S_ISDIR(mode):",
            "                make_parent(path)",
            "                if not os.path.exists(path):",
            "                    os.mkdir(path)",
            "                if restore_attrs:",
            "                    self.restore_attrs(path, item)",
            "            elif stat.S_ISLNK(mode):",
            "                make_parent(path)",
            "                with self.extract_helper(item, path, hlm) as hardlink_set:",
            "                    if hardlink_set:",
            "                        # unusual, but possible: this is a hardlinked symlink.",
            "                        return",
            "                    target = item.target",
            "                    try:",
            "                        os.symlink(target, path)",
            "                    except UnicodeEncodeError:",
            "                        raise self.IncompatibleFilesystemEncodingError(target, sys.getfilesystemencoding()) from None",
            "                    self.restore_attrs(path, item, symlink=True)",
            "            elif stat.S_ISFIFO(mode):",
            "                make_parent(path)",
            "                with self.extract_helper(item, path, hlm) as hardlink_set:",
            "                    if hardlink_set:",
            "                        return",
            "                    os.mkfifo(path)",
            "                    self.restore_attrs(path, item)",
            "            elif stat.S_ISCHR(mode) or stat.S_ISBLK(mode):",
            "                make_parent(path)",
            "                with self.extract_helper(item, path, hlm) as hardlink_set:",
            "                    if hardlink_set:",
            "                        return",
            "                    os.mknod(path, item.mode, item.rdev)",
            "                    self.restore_attrs(path, item)",
            "            else:",
            "                raise Exception(\"Unknown archive item type %r\" % item.mode)",
            "",
            "    def restore_attrs(self, path, item, symlink=False, fd=None):",
            "        \"\"\"",
            "        Restore filesystem attributes on *path* (*fd*) from *item*.",
            "",
            "        Does not access the repository.",
            "        \"\"\"",
            "        backup_io.op = \"attrs\"",
            "        # This code is a bit of a mess due to OS specific differences.",
            "        if not is_win32:",
            "            # by using uid_default = -1 and gid_default = -1, they will not be restored if",
            "            # the archived item has no information about them.",
            "            uid, gid = get_item_uid_gid(item, numeric=self.numeric_ids, uid_default=-1, gid_default=-1)",
            "            # if uid and/or gid is -1, chown will keep it as is and not change it.",
            "            try:",
            "                if fd:",
            "                    os.fchown(fd, uid, gid)",
            "                else:",
            "                    os.chown(path, uid, gid, follow_symlinks=False)",
            "            except OSError:",
            "                pass",
            "            if fd:",
            "                os.fchmod(fd, item.mode)",
            "            else:",
            "                # To check whether a particular function in the os module accepts False for its",
            "                # follow_symlinks parameter, the in operator on supports_follow_symlinks should be",
            "                # used. However, os.chmod is special as some platforms without a working lchmod() do",
            "                # have fchmodat(), which has a flag that makes it behave like lchmod(). fchmodat()",
            "                # is ignored when deciding whether or not os.chmod should be set in",
            "                # os.supports_follow_symlinks. Work around this by using try/except.",
            "                try:",
            "                    os.chmod(path, item.mode, follow_symlinks=False)",
            "                except NotImplementedError:",
            "                    if not symlink:",
            "                        os.chmod(path, item.mode)",
            "            if not self.noacls:",
            "                acl_set(path, item, self.numeric_ids, fd=fd)",
            "            if not self.noxattrs and \"xattrs\" in item:",
            "                # chown removes Linux capabilities, so set the extended attributes at the end, after chown,",
            "                # since they include the Linux capabilities in the \"security.capability\" attribute.",
            "                warning = xattr.set_all(fd or path, item.xattrs, follow_symlinks=False)",
            "                if warning:",
            "                    set_ec(EXIT_WARNING)",
            "            # set timestamps rather late",
            "            mtime = item.mtime",
            "            atime = item.atime if \"atime\" in item else mtime",
            "            if \"birthtime\" in item:",
            "                birthtime = item.birthtime",
            "                try:",
            "                    # This should work on FreeBSD, NetBSD, and Darwin and be harmless on other platforms.",
            "                    # See utimes(2) on either of the BSDs for details.",
            "                    if fd:",
            "                        os.utime(fd, None, ns=(atime, birthtime))",
            "                    else:",
            "                        os.utime(path, None, ns=(atime, birthtime), follow_symlinks=False)",
            "                except OSError:",
            "                    # some systems don't support calling utime on a symlink",
            "                    pass",
            "            try:",
            "                if fd:",
            "                    os.utime(fd, None, ns=(atime, mtime))",
            "                else:",
            "                    os.utime(path, None, ns=(atime, mtime), follow_symlinks=False)",
            "            except OSError:",
            "                # some systems don't support calling utime on a symlink",
            "                pass",
            "            # bsdflags include the immutable flag and need to be set last:",
            "            if not self.noflags and \"bsdflags\" in item:",
            "                try:",
            "                    set_flags(path, item.bsdflags, fd=fd)",
            "                except OSError:",
            "                    pass",
            "        else:  # win32",
            "            # set timestamps rather late",
            "            mtime = item.mtime",
            "            atime = item.atime if \"atime\" in item else mtime",
            "            try:",
            "                # note: no fd support on win32",
            "                os.utime(path, None, ns=(atime, mtime))",
            "            except OSError:",
            "                # some systems don't support calling utime on a symlink",
            "                pass",
            "",
            "    def set_meta(self, key, value):",
            "        metadata = self._load_meta(self.id)",
            "        setattr(metadata, key, value)",
            "        if \"items\" in metadata:",
            "            del metadata.items",
            "        data = msgpack.packb(metadata.as_dict())",
            "        new_id = self.key.id_hash(data)",
            "        self.cache.add_chunk(new_id, {}, data, stats=self.stats)",
            "        self.manifest.archives[self.name] = (new_id, metadata.time)",
            "        self.cache.chunk_decref(self.id, self.stats)",
            "        self.id = new_id",
            "",
            "    def rename(self, name):",
            "        if name in self.manifest.archives:",
            "            raise self.AlreadyExists(name)",
            "        oldname = self.name",
            "        self.name = name",
            "        self.set_meta(\"name\", name)",
            "        del self.manifest.archives[oldname]",
            "",
            "    def delete(self, stats, progress=False, forced=False):",
            "        class ChunksIndexError(Error):",
            "            \"\"\"Chunk ID {} missing from chunks index, corrupted chunks index - aborting transaction.\"\"\"",
            "",
            "        exception_ignored = object()",
            "",
            "        def fetch_async_response(wait=True):",
            "            try:",
            "                return self.repository.async_response(wait=wait)",
            "            except Repository.ObjectNotFound:",
            "                nonlocal error",
            "                # object not in repo - strange, but we wanted to delete it anyway.",
            "                if forced == 0:",
            "                    raise",
            "                error = True",
            "                return exception_ignored  # must not return None here",
            "",
            "        def chunk_decref(id, stats):",
            "            try:",
            "                self.cache.chunk_decref(id, stats, wait=False)",
            "            except KeyError:",
            "                cid = bin_to_hex(id)",
            "                raise ChunksIndexError(cid)",
            "            else:",
            "                fetch_async_response(wait=False)",
            "",
            "        error = False",
            "        try:",
            "            unpacker = msgpack.Unpacker(use_list=False)",
            "            items_ids = self.metadata.items",
            "            pi = ProgressIndicatorPercent(",
            "                total=len(items_ids), msg=\"Decrementing references %3.0f%%\", msgid=\"archive.delete\"",
            "            )",
            "            for i, (items_id, data) in enumerate(zip(items_ids, self.repository.get_many(items_ids))):",
            "                if progress:",
            "                    pi.show(i)",
            "                _, data = self.repo_objs.parse(items_id, data)",
            "                unpacker.feed(data)",
            "                chunk_decref(items_id, stats)",
            "                try:",
            "                    for item in unpacker:",
            "                        item = Item(internal_dict=item)",
            "                        if \"chunks\" in item:",
            "                            for chunk_id, size in item.chunks:",
            "                                chunk_decref(chunk_id, stats)",
            "                except (TypeError, ValueError):",
            "                    # if items metadata spans multiple chunks and one chunk got dropped somehow,",
            "                    # it could be that unpacker yields bad types",
            "                    if forced == 0:",
            "                        raise",
            "                    error = True",
            "            if progress:",
            "                pi.finish()",
            "        except (msgpack.UnpackException, Repository.ObjectNotFound):",
            "            # items metadata corrupted",
            "            if forced == 0:",
            "                raise",
            "            error = True",
            "",
            "        # delete the blocks that store all the references that end up being loaded into metadata.items:",
            "        for id in self.metadata.item_ptrs:",
            "            chunk_decref(id, stats)",
            "",
            "        # in forced delete mode, we try hard to delete at least the manifest entry,",
            "        # if possible also the archive superblock, even if processing the items raises",
            "        # some harmless exception.",
            "        chunk_decref(self.id, stats)",
            "        del self.manifest.archives[self.name]",
            "        while fetch_async_response(wait=True) is not None:",
            "            # we did async deletes, process outstanding results (== exceptions),",
            "            # so there is nothing pending when we return and our caller wants to commit.",
            "            pass",
            "        if error:",
            "            logger.warning(\"forced deletion succeeded, but the deleted archive was corrupted.\")",
            "            logger.warning(\"borg check --repair is required to free all space.\")",
            "",
            "    @staticmethod",
            "    def compare_archives_iter(",
            "        archive1: \"Archive\", archive2: \"Archive\", matcher=None, can_compare_chunk_ids=False",
            "    ) -> Iterator[ItemDiff]:",
            "        \"\"\"",
            "        Yields an ItemDiff instance describing changes/indicating equality.",
            "",
            "        :param matcher: PatternMatcher class to restrict results to only matching paths.",
            "        :param can_compare_chunk_ids: Whether --chunker-params are the same for both archives.",
            "        \"\"\"",
            "",
            "        def compare_items(path: str, item1: Item, item2: Item):",
            "            return ItemDiff(",
            "                path,",
            "                item1,",
            "                item2,",
            "                archive1.pipeline.fetch_many([c.id for c in item1.get(\"chunks\", [])]),",
            "                archive2.pipeline.fetch_many([c.id for c in item2.get(\"chunks\", [])]),",
            "                can_compare_chunk_ids=can_compare_chunk_ids,",
            "            )",
            "",
            "        orphans_archive1: OrderedDict[str, Item] = OrderedDict()",
            "        orphans_archive2: OrderedDict[str, Item] = OrderedDict()",
            "",
            "        assert matcher is not None, \"matcher must be set\"",
            "",
            "        for item1, item2 in zip_longest(",
            "            archive1.iter_items(lambda item: matcher.match(item.path)),",
            "            archive2.iter_items(lambda item: matcher.match(item.path)),",
            "        ):",
            "            if item1 and item2 and item1.path == item2.path:",
            "                yield compare_items(item1.path, item1, item2)",
            "                continue",
            "            if item1:",
            "                matching_orphan = orphans_archive2.pop(item1.path, None)",
            "                if matching_orphan:",
            "                    yield compare_items(item1.path, item1, matching_orphan)",
            "                else:",
            "                    orphans_archive1[item1.path] = item1",
            "            if item2:",
            "                matching_orphan = orphans_archive1.pop(item2.path, None)",
            "                if matching_orphan:",
            "                    yield compare_items(matching_orphan.path, matching_orphan, item2)",
            "                else:",
            "                    orphans_archive2[item2.path] = item2",
            "        # At this point orphans_* contain items that had no matching partner in the other archive",
            "        for added in orphans_archive2.values():",
            "            path = added.path",
            "            deleted_item = Item.create_deleted(path)",
            "            yield compare_items(path, deleted_item, added)",
            "        for deleted in orphans_archive1.values():",
            "            path = deleted.path",
            "            deleted_item = Item.create_deleted(path)",
            "            yield compare_items(path, deleted, deleted_item)",
            "",
            "",
            "class MetadataCollector:",
            "    def __init__(self, *, noatime, noctime, nobirthtime, numeric_ids, noflags, noacls, noxattrs):",
            "        self.noatime = noatime",
            "        self.noctime = noctime",
            "        self.numeric_ids = numeric_ids",
            "        self.noflags = noflags",
            "        self.noacls = noacls",
            "        self.noxattrs = noxattrs",
            "        self.nobirthtime = nobirthtime",
            "",
            "    def stat_simple_attrs(self, st):",
            "        attrs = {}",
            "        attrs[\"mode\"] = st.st_mode",
            "        # borg can work with archives only having mtime (very old borg archives do not have",
            "        # atime/ctime). it can be useful to omit atime/ctime, if they change without the",
            "        # file content changing - e.g. to get better metadata deduplication.",
            "        attrs[\"mtime\"] = safe_ns(st.st_mtime_ns)",
            "        if not self.noatime:",
            "            attrs[\"atime\"] = safe_ns(st.st_atime_ns)",
            "        if not self.noctime:",
            "            attrs[\"ctime\"] = safe_ns(st.st_ctime_ns)",
            "        if not self.nobirthtime and hasattr(st, \"st_birthtime\"):",
            "            # sadly, there's no stat_result.st_birthtime_ns",
            "            attrs[\"birthtime\"] = safe_ns(int(st.st_birthtime * 10**9))",
            "        attrs[\"uid\"] = st.st_uid",
            "        attrs[\"gid\"] = st.st_gid",
            "        if not self.numeric_ids:",
            "            user = uid2user(st.st_uid)",
            "            if user is not None:",
            "                attrs[\"user\"] = user",
            "            group = gid2group(st.st_gid)",
            "            if group is not None:",
            "                attrs[\"group\"] = group",
            "        return attrs",
            "",
            "    def stat_ext_attrs(self, st, path, fd=None):",
            "        attrs = {}",
            "        if not self.noflags:",
            "            with backup_io(\"extended stat (flags)\"):",
            "                flags = get_flags(path, st, fd=fd)",
            "            attrs[\"bsdflags\"] = flags",
            "        if not self.noxattrs:",
            "            with backup_io(\"extended stat (xattrs)\"):",
            "                xattrs = xattr.get_all(fd or path, follow_symlinks=False)",
            "            attrs[\"xattrs\"] = StableDict(xattrs)",
            "        if not self.noacls:",
            "            with backup_io(\"extended stat (ACLs)\"):",
            "                acl_get(path, attrs, st, self.numeric_ids, fd=fd)",
            "        return attrs",
            "",
            "    def stat_attrs(self, st, path, fd=None):",
            "        attrs = self.stat_simple_attrs(st)",
            "        attrs.update(self.stat_ext_attrs(st, path, fd=fd))",
            "        return attrs",
            "",
            "",
            "# remember a few recently used all-zero chunk hashes in this mapping.",
            "# (hash_func, chunk_length) -> chunk_hash",
            "# we play safe and have the hash_func in the mapping key, in case we",
            "# have different hash_funcs within the same borg run.",
            "zero_chunk_ids = LRUCache(10)  # type: ignore[var-annotated]",
            "",
            "",
            "def cached_hash(chunk, id_hash):",
            "    allocation = chunk.meta[\"allocation\"]",
            "    if allocation == CH_DATA:",
            "        data = chunk.data",
            "        chunk_id = id_hash(data)",
            "    elif allocation in (CH_HOLE, CH_ALLOC):",
            "        size = chunk.meta[\"size\"]",
            "        assert size <= len(zeros)",
            "        data = memoryview(zeros)[:size]",
            "        try:",
            "            chunk_id = zero_chunk_ids[(id_hash, size)]",
            "        except KeyError:",
            "            chunk_id = id_hash(data)",
            "            zero_chunk_ids[(id_hash, size)] = chunk_id",
            "    else:",
            "        raise ValueError(\"unexpected allocation type\")",
            "    return chunk_id, data",
            "",
            "",
            "class ChunksProcessor:",
            "    # Processes an iterator of chunks for an Item",
            "",
            "    def __init__(",
            "        self,",
            "        *,",
            "        key,",
            "        cache,",
            "        add_item,",
            "        prepare_checkpoint,",
            "        write_checkpoint,",
            "        checkpoint_interval,",
            "        checkpoint_volume,",
            "        rechunkify,",
            "    ):",
            "        self.key = key",
            "        self.cache = cache",
            "        self.add_item = add_item",
            "        self.prepare_checkpoint = prepare_checkpoint",
            "        self.write_checkpoint = write_checkpoint",
            "        self.rechunkify = rechunkify",
            "        # time interval based checkpointing",
            "        self.checkpoint_interval = checkpoint_interval",
            "        self.last_checkpoint = time.monotonic()",
            "        # file content volume based checkpointing",
            "        self.checkpoint_volume = checkpoint_volume",
            "        self.current_volume = 0",
            "        self.last_volume_checkpoint = 0",
            "",
            "    def write_part_file(self, item):",
            "        self.prepare_checkpoint()",
            "        item = Item(internal_dict=item.as_dict())",
            "        # for borg recreate, we already have a size member in the source item (giving the total file size),",
            "        # but we consider only a part of the file here, thus we must recompute the size from the chunks:",
            "        item.get_size(memorize=True, from_chunks=True)",
            "        item.path += \".borg_part\"",
            "        self.add_item(item, show_progress=False)",
            "        self.write_checkpoint()",
            "",
            "    def maybe_checkpoint(self, item):",
            "        checkpoint_done = False",
            "        sig_int_triggered = sig_int and sig_int.action_triggered()",
            "        if (",
            "            sig_int_triggered",
            "            or (self.checkpoint_interval and time.monotonic() - self.last_checkpoint > self.checkpoint_interval)",
            "            or (self.checkpoint_volume and self.current_volume - self.last_volume_checkpoint >= self.checkpoint_volume)",
            "        ):",
            "            if sig_int_triggered:",
            "                logger.info(\"checkpoint requested: starting checkpoint creation...\")",
            "            self.write_part_file(item)",
            "            checkpoint_done = True",
            "            self.last_checkpoint = time.monotonic()",
            "            self.last_volume_checkpoint = self.current_volume",
            "            if sig_int_triggered:",
            "                sig_int.action_completed()",
            "                logger.info(\"checkpoint requested: finished checkpoint creation!\")",
            "        return checkpoint_done  # whether a checkpoint archive was created",
            "",
            "    def process_file_chunks(self, item, cache, stats, show_progress, chunk_iter, chunk_processor=None):",
            "        if not chunk_processor:",
            "",
            "            def chunk_processor(chunk):",
            "                started_hashing = time.monotonic()",
            "                chunk_id, data = cached_hash(chunk, self.key.id_hash)",
            "                stats.hashing_time += time.monotonic() - started_hashing",
            "                chunk_entry = cache.add_chunk(chunk_id, {}, data, stats=stats, wait=False)",
            "                self.cache.repository.async_response(wait=False)",
            "                return chunk_entry",
            "",
            "        item.chunks = []",
            "        # if we rechunkify, we'll get a fundamentally different chunks list, thus we need",
            "        # to get rid of .chunks_healthy, as it might not correspond to .chunks any more.",
            "        if self.rechunkify and \"chunks_healthy\" in item:",
            "            del item.chunks_healthy",
            "        for chunk in chunk_iter:",
            "            chunk_entry = chunk_processor(chunk)",
            "            item.chunks.append(chunk_entry)",
            "            self.current_volume += chunk_entry[1]",
            "            if show_progress:",
            "                stats.show_progress(item=item, dt=0.2)",
            "            self.maybe_checkpoint(item)",
            "",
            "",
            "class FilesystemObjectProcessors:",
            "    # When ported to threading, then this doesn't need chunker, cache, key any more.",
            "    # write_checkpoint should then be in the item buffer,",
            "    # and process_file becomes a callback passed to __init__.",
            "",
            "    def __init__(",
            "        self,",
            "        *,",
            "        metadata_collector,",
            "        cache,",
            "        key,",
            "        add_item,",
            "        process_file_chunks,",
            "        chunker_params,",
            "        show_progress,",
            "        sparse,",
            "        log_json,",
            "        iec,",
            "        file_status_printer=None,",
            "    ):",
            "        self.metadata_collector = metadata_collector",
            "        self.cache = cache",
            "        self.key = key",
            "        self.add_item = add_item",
            "        self.process_file_chunks = process_file_chunks",
            "        self.show_progress = show_progress",
            "        self.print_file_status = file_status_printer or (lambda *args: None)",
            "",
            "        self.hlm = HardLinkManager(id_type=tuple, info_type=(list, type(None)))  # (dev, ino) -> chunks or None",
            "        self.stats = Statistics(output_json=log_json, iec=iec)  # threading: done by cache (including progress)",
            "        self.cwd = os.getcwd()",
            "        self.chunker = get_chunker(*chunker_params, seed=key.chunk_seed, sparse=sparse)",
            "",
            "    @contextmanager",
            "    def create_helper(self, path, st, status=None, hardlinkable=True):",
            "        sanitized_path = remove_dotdot_prefixes(path)",
            "        item = Item(path=sanitized_path)",
            "        hardlinked = hardlinkable and st.st_nlink > 1",
            "        hl_chunks = None",
            "        update_map = False",
            "        if hardlinked:",
            "            status = \"h\"  # hardlink",
            "            nothing = object()",
            "            chunks = self.hlm.retrieve(id=(st.st_ino, st.st_dev), default=nothing)",
            "            if chunks is nothing:",
            "                update_map = True",
            "            elif chunks is not None:",
            "                hl_chunks = chunks",
            "            item.hlid = self.hlm.hardlink_id_from_inode(ino=st.st_ino, dev=st.st_dev)",
            "        yield item, status, hardlinked, hl_chunks",
            "        self.add_item(item, stats=self.stats)",
            "        if update_map:",
            "            # remember the hlid of this fs object and if the item has chunks,",
            "            # also remember them, so we do not have to re-chunk a hardlink.",
            "            chunks = item.chunks if \"chunks\" in item else None",
            "            self.hlm.remember(id=(st.st_ino, st.st_dev), info=chunks)",
            "",
            "    def process_dir_with_fd(self, *, path, fd, st):",
            "        with self.create_helper(path, st, \"d\", hardlinkable=False) as (item, status, hardlinked, hl_chunks):",
            "            item.update(self.metadata_collector.stat_attrs(st, path, fd=fd))",
            "            return status",
            "",
            "    def process_dir(self, *, path, parent_fd, name, st):",
            "        with self.create_helper(path, st, \"d\", hardlinkable=False) as (item, status, hardlinked, hl_chunks):",
            "            with OsOpen(path=path, parent_fd=parent_fd, name=name, flags=flags_dir, noatime=True, op=\"dir_open\") as fd:",
            "                # fd is None for directories on windows, in that case a race condition check is not possible.",
            "                if fd is not None:",
            "                    with backup_io(\"fstat\"):",
            "                        st = stat_update_check(st, os.fstat(fd))",
            "                item.update(self.metadata_collector.stat_attrs(st, path, fd=fd))",
            "                return status",
            "",
            "    def process_fifo(self, *, path, parent_fd, name, st):",
            "        with self.create_helper(path, st, \"f\") as (item, status, hardlinked, hl_chunks):  # fifo",
            "            with OsOpen(path=path, parent_fd=parent_fd, name=name, flags=flags_normal, noatime=True) as fd:",
            "                with backup_io(\"fstat\"):",
            "                    st = stat_update_check(st, os.fstat(fd))",
            "                item.update(self.metadata_collector.stat_attrs(st, path, fd=fd))",
            "                return status",
            "",
            "    def process_dev(self, *, path, parent_fd, name, st, dev_type):",
            "        with self.create_helper(path, st, dev_type) as (item, status, hardlinked, hl_chunks):  # char/block device",
            "            # looks like we can not work fd-based here without causing issues when trying to open/close the device",
            "            with backup_io(\"stat\"):",
            "                st = stat_update_check(st, os_stat(path=path, parent_fd=parent_fd, name=name, follow_symlinks=False))",
            "            item.rdev = st.st_rdev",
            "            item.update(self.metadata_collector.stat_attrs(st, path))",
            "            return status",
            "",
            "    def process_symlink(self, *, path, parent_fd, name, st):",
            "        with self.create_helper(path, st, \"s\", hardlinkable=True) as (item, status, hardlinked, hl_chunks):",
            "            fname = name if name is not None and parent_fd is not None else path",
            "            with backup_io(\"readlink\"):",
            "                target = os.readlink(fname, dir_fd=parent_fd)",
            "            item.target = target",
            "            item.update(self.metadata_collector.stat_attrs(st, path))  # can't use FD here?",
            "            return status",
            "",
            "    def process_pipe(self, *, path, cache, fd, mode, user=None, group=None):",
            "        status = \"i\"  # stdin (or other pipe)",
            "        self.print_file_status(status, path)",
            "        status = None  # we already printed the status",
            "        if user is not None:",
            "            uid = user2uid(user)",
            "            if uid is None:",
            "                raise Error(\"no such user: %s\" % user)",
            "        else:",
            "            uid = None",
            "        if group is not None:",
            "            gid = group2gid(group)",
            "            if gid is None:",
            "                raise Error(\"no such group: %s\" % group)",
            "        else:",
            "            gid = None",
            "        t = int(time.time()) * 1000000000",
            "        item = Item(path=path, mode=mode & 0o107777 | 0o100000, mtime=t, atime=t, ctime=t)  # forcing regular file mode",
            "        if user is not None:",
            "            item.user = user",
            "        if group is not None:",
            "            item.group = group",
            "        if uid is not None:",
            "            item.uid = uid",
            "        if gid is not None:",
            "            item.gid = gid",
            "        try:",
            "            self.process_file_chunks(",
            "                item, cache, self.stats, self.show_progress, backup_io_iter(self.chunker.chunkify(fd))",
            "            )",
            "        except BackupOSError:",
            "            # see comments in process_file's exception handler, same issue here.",
            "            for chunk in item.get(\"chunks\", []):",
            "                cache.chunk_decref(chunk.id, self.stats, wait=False)",
            "            raise",
            "        else:",
            "            item.get_size(memorize=True)",
            "            self.stats.nfiles += 1",
            "            self.add_item(item, stats=self.stats)",
            "            return status",
            "",
            "    def process_file(self, *, path, parent_fd, name, st, cache, flags=flags_normal, last_try=False):",
            "        with self.create_helper(path, st, None) as (item, status, hardlinked, hl_chunks):  # no status yet",
            "            with OsOpen(path=path, parent_fd=parent_fd, name=name, flags=flags, noatime=True) as fd:",
            "                with backup_io(\"fstat\"):",
            "                    st = stat_update_check(st, os.fstat(fd))",
            "                item.update(self.metadata_collector.stat_simple_attrs(st))",
            "                is_special_file = is_special(st.st_mode)",
            "                if is_special_file:",
            "                    # we process a special file like a regular file. reflect that in mode,",
            "                    # so it can be extracted / accessed in FUSE mount like a regular file.",
            "                    # this needs to be done early, so that part files also get the patched mode.",
            "                    item.mode = stat.S_IFREG | stat.S_IMODE(item.mode)",
            "                # we begin processing chunks now (writing or incref'ing them to the repository),",
            "                # which might require cleanup (see except-branch):",
            "                try:",
            "                    if hl_chunks is not None:  # create_helper gave us chunks from a previous hardlink",
            "                        item.chunks = []",
            "                        for chunk_id, chunk_size in hl_chunks:",
            "                            # process one-by-one, so we will know in item.chunks how far we got",
            "                            chunk_entry = cache.chunk_incref(chunk_id, self.stats)",
            "                            item.chunks.append(chunk_entry)",
            "                    else:  # normal case, no \"2nd+\" hardlink",
            "                        if not is_special_file:",
            "                            hashed_path = safe_encode(os.path.join(self.cwd, path))",
            "                            started_hashing = time.monotonic()",
            "                            path_hash = self.key.id_hash(hashed_path)",
            "                            self.stats.hashing_time += time.monotonic() - started_hashing",
            "                            known, ids = cache.file_known_and_unchanged(hashed_path, path_hash, st)",
            "                        else:",
            "                            # in --read-special mode, we may be called for special files.",
            "                            # there should be no information in the cache about special files processed in",
            "                            # read-special mode, but we better play safe as this was wrong in the past:",
            "                            hashed_path = path_hash = None",
            "                            known, ids = False, None",
            "                        if ids is not None:",
            "                            # Make sure all ids are available",
            "                            for id_ in ids:",
            "                                if not cache.seen_chunk(id_):",
            "                                    # cache said it is unmodified, but we lost a chunk: process file like modified",
            "                                    status = \"M\"",
            "                                    break",
            "                            else:",
            "                                item.chunks = []",
            "                                for chunk_id in ids:",
            "                                    # process one-by-one, so we will know in item.chunks how far we got",
            "                                    chunk_entry = cache.chunk_incref(chunk_id, self.stats)",
            "                                    item.chunks.append(chunk_entry)",
            "                                status = \"U\"  # regular file, unchanged",
            "                        else:",
            "                            status = \"M\" if known else \"A\"  # regular file, modified or added",
            "                        self.print_file_status(status, path)",
            "                        # Only chunkify the file if needed",
            "                        changed_while_backup = False",
            "                        if \"chunks\" not in item:",
            "                            with backup_io(\"read\"):",
            "                                self.process_file_chunks(",
            "                                    item,",
            "                                    cache,",
            "                                    self.stats,",
            "                                    self.show_progress,",
            "                                    backup_io_iter(self.chunker.chunkify(None, fd)),",
            "                                )",
            "                                self.stats.chunking_time = self.chunker.chunking_time",
            "                            if not is_win32:  # TODO for win32",
            "                                with backup_io(\"fstat2\"):",
            "                                    st2 = os.fstat(fd)",
            "                                # special files:",
            "                                # - fifos change naturally, because they are fed from the other side. no problem.",
            "                                # - blk/chr devices don't change ctime anyway.",
            "                                changed_while_backup = not is_special_file and st.st_ctime_ns != st2.st_ctime_ns",
            "                            if changed_while_backup:",
            "                                # regular file changed while we backed it up, might be inconsistent/corrupt!",
            "                                if last_try:",
            "                                    status = \"C\"  # crap! retries did not help.",
            "                                else:",
            "                                    raise BackupError(\"file changed while we read it!\")",
            "                            if not is_special_file and not changed_while_backup:",
            "                                # we must not memorize special files, because the contents of e.g. a",
            "                                # block or char device will change without its mtime/size/inode changing.",
            "                                # also, we must not memorize a potentially inconsistent/corrupt file that",
            "                                # changed while we backed it up.",
            "                                cache.memorize_file(hashed_path, path_hash, st, [c.id for c in item.chunks])",
            "                        self.stats.files_stats[status] += 1  # must be done late",
            "                        if not changed_while_backup:",
            "                            status = None  # we already called print_file_status",
            "                    self.stats.nfiles += 1",
            "                    item.update(self.metadata_collector.stat_ext_attrs(st, path, fd=fd))",
            "                    item.get_size(memorize=True)",
            "                    return status",
            "                except BackupOSError:",
            "                    # Something went wrong and we might need to clean up a bit.",
            "                    # Maybe we have already incref'ed some file content chunks in the repo -",
            "                    # but we will not add an item (see add_item in create_helper) and thus",
            "                    # they would be orphaned chunks in case that we commit the transaction.",
            "                    for chunk in item.get(\"chunks\", []):",
            "                        cache.chunk_decref(chunk.id, self.stats, wait=False)",
            "                    # Now that we have cleaned up the chunk references, we can re-raise the exception.",
            "                    # This will skip processing of this file, but might retry or continue with the next one.",
            "                    raise",
            "",
            "",
            "class TarfileObjectProcessors:",
            "    def __init__(",
            "        self,",
            "        *,",
            "        cache,",
            "        key,",
            "        add_item,",
            "        process_file_chunks,",
            "        chunker_params,",
            "        show_progress,",
            "        log_json,",
            "        iec,",
            "        file_status_printer=None,",
            "    ):",
            "        self.cache = cache",
            "        self.key = key",
            "        self.add_item = add_item",
            "        self.process_file_chunks = process_file_chunks",
            "        self.show_progress = show_progress",
            "        self.print_file_status = file_status_printer or (lambda *args: None)",
            "",
            "        self.stats = Statistics(output_json=log_json, iec=iec)  # threading: done by cache (including progress)",
            "        self.chunker = get_chunker(*chunker_params, seed=key.chunk_seed, sparse=False)",
            "        self.hlm = HardLinkManager(id_type=str, info_type=list)  # path -> chunks",
            "",
            "    @contextmanager",
            "    def create_helper(self, tarinfo, status=None, type=None):",
            "        ph = tarinfo.pax_headers",
            "        if ph and \"BORG.item.version\" in ph:",
            "            assert ph[\"BORG.item.version\"] == \"1\"",
            "            meta_bin = base64.b64decode(ph[\"BORG.item.meta\"])",
            "            meta_dict = msgpack.unpackb(meta_bin, object_hook=StableDict)",
            "            item = Item(internal_dict=meta_dict)",
            "        else:",
            "",
            "            def s_to_ns(s):",
            "                return safe_ns(int(float(s) * 1e9))",
            "",
            "            item = Item(",
            "                path=make_path_safe(tarinfo.name),",
            "                mode=tarinfo.mode | type,",
            "                uid=tarinfo.uid,",
            "                gid=tarinfo.gid,",
            "                mtime=s_to_ns(tarinfo.mtime),",
            "            )",
            "            if tarinfo.uname:",
            "                item.user = tarinfo.uname",
            "            if tarinfo.gname:",
            "                item.group = tarinfo.gname",
            "            if ph:",
            "                # note: for mtime this is a bit redundant as it is already done by tarfile module,",
            "                #       but we just do it in our way to be consistent for sure.",
            "                for name in \"atime\", \"ctime\", \"mtime\":",
            "                    if name in ph:",
            "                        ns = s_to_ns(ph[name])",
            "                        setattr(item, name, ns)",
            "        yield item, status",
            "        # if we get here, \"with\"-block worked ok without error/exception, the item was processed ok...",
            "        self.add_item(item, stats=self.stats)",
            "",
            "    def process_dir(self, *, tarinfo, status, type):",
            "        with self.create_helper(tarinfo, status, type) as (item, status):",
            "            return status",
            "",
            "    def process_fifo(self, *, tarinfo, status, type):",
            "        with self.create_helper(tarinfo, status, type) as (item, status):",
            "            return status",
            "",
            "    def process_dev(self, *, tarinfo, status, type):",
            "        with self.create_helper(tarinfo, status, type) as (item, status):",
            "            item.rdev = os.makedev(tarinfo.devmajor, tarinfo.devminor)",
            "            return status",
            "",
            "    def process_symlink(self, *, tarinfo, status, type):",
            "        with self.create_helper(tarinfo, status, type) as (item, status):",
            "            item.target = tarinfo.linkname",
            "            return status",
            "",
            "    def process_hardlink(self, *, tarinfo, status, type):",
            "        with self.create_helper(tarinfo, status, type) as (item, status):",
            "            # create a not hardlinked borg item, reusing the chunks, see HardLinkManager.__doc__",
            "            chunks = self.hlm.retrieve(tarinfo.linkname)",
            "            if chunks is not None:",
            "                item.chunks = chunks",
            "            item.get_size(memorize=True, from_chunks=True)",
            "            self.stats.nfiles += 1",
            "            return status",
            "",
            "    def process_file(self, *, tarinfo, status, type, tar):",
            "        with self.create_helper(tarinfo, status, type) as (item, status):",
            "            self.print_file_status(status, tarinfo.name)",
            "            status = None  # we already printed the status",
            "            try:",
            "                fd = tar.extractfile(tarinfo)",
            "                self.process_file_chunks(",
            "                    item, self.cache, self.stats, self.show_progress, backup_io_iter(self.chunker.chunkify(fd))",
            "                )",
            "                item.get_size(memorize=True, from_chunks=True)",
            "                self.stats.nfiles += 1",
            "                # we need to remember ALL files, see HardLinkManager.__doc__",
            "                self.hlm.remember(id=tarinfo.name, info=item.chunks)",
            "                return status",
            "            except BackupOSError:",
            "                # see comment in FilesystemObjectProcessors.process_file, same issue here.",
            "                for chunk in item.get(\"chunks\", []):",
            "                    self.cache.chunk_decref(chunk.id, self.stats, wait=False)",
            "                raise",
            "",
            "",
            "def valid_msgpacked_dict(d, keys_serialized):",
            "    \"\"\"check if the data <d> looks like a msgpacked dict\"\"\"",
            "    d_len = len(d)",
            "    if d_len == 0:",
            "        return False",
            "    if d[0] & 0xF0 == 0x80:  # object is a fixmap (up to 15 elements)",
            "        offs = 1",
            "    elif d[0] == 0xDE:  # object is a map16 (up to 2^16-1 elements)",
            "        offs = 3",
            "    else:",
            "        # object is not a map (dict)",
            "        # note: we must not have dicts with > 2^16-1 elements",
            "        return False",
            "    if d_len <= offs:",
            "        return False",
            "    # is the first dict key a bytestring?",
            "    if d[offs] & 0xE0 == 0xA0:  # key is a small bytestring (up to 31 chars)",
            "        pass",
            "    elif d[offs] in (0xD9, 0xDA, 0xDB):  # key is a str8, str16 or str32",
            "        pass",
            "    else:",
            "        # key is not a bytestring",
            "        return False",
            "    # is the bytestring any of the expected key names?",
            "    key_serialized = d[offs:]",
            "    return any(key_serialized.startswith(pattern) for pattern in keys_serialized)",
            "",
            "",
            "class RobustUnpacker:",
            "    \"\"\"A restartable/robust version of the streaming msgpack unpacker\"\"\"",
            "",
            "    def __init__(self, validator, item_keys):",
            "        super().__init__()",
            "        self.item_keys = [msgpack.packb(name) for name in item_keys]",
            "        self.validator = validator",
            "        self._buffered_data = []",
            "        self._resync = False",
            "        self._unpacker = msgpack.Unpacker(object_hook=StableDict)",
            "",
            "    def resync(self):",
            "        self._buffered_data = []",
            "        self._resync = True",
            "",
            "    def feed(self, data):",
            "        if self._resync:",
            "            self._buffered_data.append(data)",
            "        else:",
            "            self._unpacker.feed(data)",
            "",
            "    def __iter__(self):",
            "        return self",
            "",
            "    def __next__(self):",
            "        if self._resync:",
            "            data = b\"\".join(self._buffered_data)",
            "            while self._resync:",
            "                if not data:",
            "                    raise StopIteration",
            "                # Abort early if the data does not look like a serialized item dict",
            "                if not valid_msgpacked_dict(data, self.item_keys):",
            "                    data = data[1:]",
            "                    continue",
            "                self._unpacker = msgpack.Unpacker(object_hook=StableDict)",
            "                self._unpacker.feed(data)",
            "                try:",
            "                    item = next(self._unpacker)",
            "                except (msgpack.UnpackException, StopIteration):",
            "                    # as long as we are resyncing, we also ignore StopIteration",
            "                    pass",
            "                else:",
            "                    if self.validator(item):",
            "                        self._resync = False",
            "                        return item",
            "                data = data[1:]",
            "        else:",
            "            return next(self._unpacker)",
            "",
            "",
            "class ArchiveChecker:",
            "    def __init__(self):",
            "        self.error_found = False",
            "        self.possibly_superseded = set()",
            "",
            "    def check(",
            "        self,",
            "        repository,",
            "        *,",
            "        verify_data=False,",
            "        repair=False,",
            "        match=None,",
            "        sort_by=\"\",",
            "        first=0,",
            "        last=0,",
            "        older=None,",
            "        newer=None,",
            "        oldest=None,",
            "        newest=None,",
            "    ):",
            "        \"\"\"Perform a set of checks on 'repository'",
            "",
            "        :param repair: enable repair mode, write updated or corrected data into repository",
            "        :param first/last/sort_by: only check this number of first/last archives ordered by sort_by",
            "        :param match: only check archives matching this pattern",
            "        :param older/newer: only check archives older/newer than timedelta from now",
            "        :param oldest/newest: only check archives older/newer than timedelta from oldest/newest archive timestamp",
            "        :param verify_data: integrity verification of data referenced by archives",
            "        \"\"\"",
            "        logger.info(\"Starting archive consistency check...\")",
            "        self.check_all = not any((first, last, match, older, newer, oldest, newest))",
            "        self.repair = repair",
            "        self.repository = repository",
            "        self.init_chunks()",
            "        if not self.chunks:",
            "            logger.error(\"Repository contains no apparent data at all, cannot continue check/repair.\")",
            "            return False",
            "        self.key = self.make_key(repository)",
            "        self.repo_objs = RepoObj(self.key)",
            "        if verify_data:",
            "            self.verify_data()",
            "        if Manifest.MANIFEST_ID not in self.chunks:",
            "            logger.error(\"Repository manifest not found!\")",
            "            self.error_found = True",
            "            self.manifest = self.rebuild_manifest()",
            "        else:",
            "            try:",
            "                self.manifest = Manifest.load(repository, (Manifest.Operation.CHECK,), key=self.key)",
            "            except IntegrityErrorBase as exc:",
            "                logger.error(\"Repository manifest is corrupted: %s\", exc)",
            "                self.error_found = True",
            "                del self.chunks[Manifest.MANIFEST_ID]",
            "                self.manifest = self.rebuild_manifest()",
            "        self.rebuild_refcounts(",
            "            match=match, first=first, last=last, sort_by=sort_by, older=older, oldest=oldest, newer=newer, newest=newest",
            "        )",
            "        self.orphan_chunks_check()",
            "        self.finish()",
            "        if self.error_found:",
            "            logger.error(\"Archive consistency check complete, problems found.\")",
            "        else:",
            "            logger.info(\"Archive consistency check complete, no problems found.\")",
            "        return self.repair or not self.error_found",
            "",
            "    def init_chunks(self):",
            "        \"\"\"Fetch a list of all object keys from repository\"\"\"",
            "        # Explicitly set the initial usable hash table capacity to avoid performance issues",
            "        # due to hash table \"resonance\".",
            "        # Since reconstruction of archive items can add some new chunks, add 10 % headroom.",
            "        self.chunks = ChunkIndex(usable=len(self.repository) * 1.1)",
            "        marker = None",
            "        while True:",
            "            result = self.repository.list(limit=LIST_SCAN_LIMIT, marker=marker)",
            "            if not result:",
            "                break",
            "            marker = result[-1]",
            "            init_entry = ChunkIndexEntry(refcount=0, size=0)",
            "            for id_ in result:",
            "                self.chunks[id_] = init_entry",
            "",
            "    def make_key(self, repository):",
            "        attempt = 0",
            "        for chunkid, _ in self.chunks.iteritems():",
            "            attempt += 1",
            "            if attempt > 999:",
            "                # we did a lot of attempts, but could not create the key via key_factory, give up.",
            "                break",
            "            cdata = repository.get(chunkid)",
            "            try:",
            "                return key_factory(repository, cdata)",
            "            except UnsupportedPayloadError:",
            "                # we get here, if the cdata we got has a corrupted key type byte",
            "                pass  # ignore it, just try the next chunk",
            "        if attempt == 0:",
            "            msg = \"make_key: repository has no chunks at all!\"",
            "        else:",
            "            msg = \"make_key: failed to create the key (tried %d chunks)\" % attempt",
            "        raise IntegrityError(msg)",
            "",
            "    def verify_data(self):",
            "        logger.info(\"Starting cryptographic data integrity verification...\")",
            "        chunks_count_index = len(self.chunks)",
            "        chunks_count_segments = 0",
            "        errors = 0",
            "        defect_chunks = []",
            "        pi = ProgressIndicatorPercent(",
            "            total=chunks_count_index, msg=\"Verifying data %6.2f%%\", step=0.01, msgid=\"check.verify_data\"",
            "        )",
            "        state = None",
            "        while True:",
            "            chunk_ids, state = self.repository.scan(limit=100, state=state)",
            "            if not chunk_ids:",
            "                break",
            "            chunks_count_segments += len(chunk_ids)",
            "            chunk_data_iter = self.repository.get_many(chunk_ids)",
            "            chunk_ids_revd = list(reversed(chunk_ids))",
            "            while chunk_ids_revd:",
            "                pi.show()",
            "                chunk_id = chunk_ids_revd.pop(-1)  # better efficiency",
            "                try:",
            "                    encrypted_data = next(chunk_data_iter)",
            "                except (Repository.ObjectNotFound, IntegrityErrorBase) as err:",
            "                    self.error_found = True",
            "                    errors += 1",
            "                    logger.error(\"chunk %s: %s\", bin_to_hex(chunk_id), err)",
            "                    if isinstance(err, IntegrityErrorBase):",
            "                        defect_chunks.append(chunk_id)",
            "                    # as the exception killed our generator, make a new one for remaining chunks:",
            "                    if chunk_ids_revd:",
            "                        chunk_ids = list(reversed(chunk_ids_revd))",
            "                        chunk_data_iter = self.repository.get_many(chunk_ids)",
            "                else:",
            "                    try:",
            "                        # we must decompress, so it'll call assert_id() in there:",
            "                        self.repo_objs.parse(chunk_id, encrypted_data, decompress=True)",
            "                    except IntegrityErrorBase as integrity_error:",
            "                        self.error_found = True",
            "                        errors += 1",
            "                        logger.error(\"chunk %s, integrity error: %s\", bin_to_hex(chunk_id), integrity_error)",
            "                        defect_chunks.append(chunk_id)",
            "        pi.finish()",
            "        if chunks_count_index != chunks_count_segments:",
            "            logger.error(\"Repo/Chunks index object count vs. segment files object count mismatch.\")",
            "            logger.error(",
            "                \"Repo/Chunks index: %d objects != segment files: %d objects\", chunks_count_index, chunks_count_segments",
            "            )",
            "        if defect_chunks:",
            "            if self.repair:",
            "                # if we kill the defect chunk here, subsequent actions within this \"borg check\"",
            "                # run will find missing chunks and replace them with all-zero replacement",
            "                # chunks and flag the files as \"repaired\".",
            "                # if another backup is done later and the missing chunks get backed up again,",
            "                # a \"borg check\" afterwards can heal all files where this chunk was missing.",
            "                logger.warning(",
            "                    \"Found defect chunks. They will be deleted now, so affected files can \"",
            "                    \"get repaired now and maybe healed later.\"",
            "                )",
            "                for defect_chunk in defect_chunks:",
            "                    # remote repo (ssh): retry might help for strange network / NIC / RAM errors",
            "                    # as the chunk will be retransmitted from remote server.",
            "                    # local repo (fs): as chunks.iteritems loop usually pumps a lot of data through,",
            "                    # a defect chunk is likely not in the fs cache any more and really gets re-read",
            "                    # from the underlying media.",
            "                    try:",
            "                        encrypted_data = self.repository.get(defect_chunk)",
            "                        # we must decompress, so it'll call assert_id() in there:",
            "                        self.repo_objs.parse(defect_chunk, encrypted_data, decompress=True)",
            "                    except IntegrityErrorBase:",
            "                        # failed twice -> get rid of this chunk",
            "                        del self.chunks[defect_chunk]",
            "                        self.repository.delete(defect_chunk)",
            "                        logger.debug(\"chunk %s deleted.\", bin_to_hex(defect_chunk))",
            "                    else:",
            "                        logger.warning(\"chunk %s not deleted, did not consistently fail.\", bin_to_hex(defect_chunk))",
            "            else:",
            "                logger.warning(",
            "                    \"Found defect chunks. With --repair, they would get deleted, so affected \"",
            "                    \"files could get repaired then and maybe healed later.\"",
            "                )",
            "                for defect_chunk in defect_chunks:",
            "                    logger.debug(\"chunk %s is defect.\", bin_to_hex(defect_chunk))",
            "        log = logger.error if errors else logger.info",
            "        log(",
            "            \"Finished cryptographic data integrity verification, verified %d chunks with %d integrity errors.\",",
            "            chunks_count_segments,",
            "            errors,",
            "        )",
            "",
            "    def rebuild_manifest(self):",
            "        \"\"\"Rebuild the manifest object if it is missing",
            "",
            "        Iterates through all objects in the repository looking for archive metadata blocks.",
            "        \"\"\"",
            "",
            "        def valid_archive(obj):",
            "            if not isinstance(obj, dict):",
            "                return False",
            "            return REQUIRED_ARCHIVE_KEYS.issubset(obj)",
            "",
            "        logger.info(\"Rebuilding missing manifest, this might take some time...\")",
            "        # as we have lost the manifest, we do not know any more what valid item keys we had.",
            "        # collecting any key we encounter in a damaged repo seems unwise, thus we just use",
            "        # the hardcoded list from the source code. thus, it is not recommended to rebuild a",
            "        # lost manifest on a older borg version than the most recent one that was ever used",
            "        # within this repository (assuming that newer borg versions support more item keys).",
            "        manifest = Manifest(self.key, self.repository)",
            "        archive_keys_serialized = [msgpack.packb(name) for name in ARCHIVE_KEYS]",
            "        pi = ProgressIndicatorPercent(",
            "            total=len(self.chunks), msg=\"Rebuilding manifest %6.2f%%\", step=0.01, msgid=\"check.rebuild_manifest\"",
            "        )",
            "        for chunk_id, _ in self.chunks.iteritems():",
            "            pi.show()",
            "            cdata = self.repository.get(chunk_id)",
            "            try:",
            "                _, data = self.repo_objs.parse(chunk_id, cdata)",
            "            except IntegrityErrorBase as exc:",
            "                logger.error(\"Skipping corrupted chunk: %s\", exc)",
            "                self.error_found = True",
            "                continue",
            "            if not valid_msgpacked_dict(data, archive_keys_serialized):",
            "                continue",
            "            if b\"command_line\" not in data or b\"\\xa7version\\x02\" not in data:",
            "                continue",
            "            try:",
            "                archive = msgpack.unpackb(data)",
            "            # Ignore exceptions that might be raised when feeding msgpack with invalid data",
            "            except msgpack.UnpackException:",
            "                continue",
            "            if valid_archive(archive):",
            "                archive = ArchiveItem(internal_dict=archive)",
            "                name = archive.name",
            "                logger.info(\"Found archive %s\", name)",
            "                if name in manifest.archives:",
            "                    i = 1",
            "                    while True:",
            "                        new_name = \"%s.%d\" % (name, i)",
            "                        if new_name not in manifest.archives:",
            "                            break",
            "                        i += 1",
            "                    logger.warning(\"Duplicate archive name %s, storing as %s\", name, new_name)",
            "                    name = new_name",
            "                manifest.archives[name] = (chunk_id, archive.time)",
            "        pi.finish()",
            "        logger.info(\"Manifest rebuild complete.\")",
            "        return manifest",
            "",
            "    def rebuild_refcounts(",
            "        self, first=0, last=0, sort_by=\"\", match=None, older=None, newer=None, oldest=None, newest=None",
            "    ):",
            "        \"\"\"Rebuild object reference counts by walking the metadata",
            "",
            "        Missing and/or incorrect data is repaired when detected",
            "        \"\"\"",
            "        # Exclude the manifest from chunks (manifest entry might be already deleted from self.chunks)",
            "        self.chunks.pop(Manifest.MANIFEST_ID, None)",
            "",
            "        def mark_as_possibly_superseded(id_):",
            "            if self.chunks.get(id_, ChunkIndexEntry(0, 0)).refcount == 0:",
            "                self.possibly_superseded.add(id_)",
            "",
            "        def add_callback(chunk):",
            "            id_ = self.key.id_hash(chunk)",
            "            cdata = self.repo_objs.format(id_, {}, chunk)",
            "            add_reference(id_, len(chunk), cdata)",
            "            return id_",
            "",
            "        def add_reference(id_, size, cdata=None):",
            "            try:",
            "                self.chunks.incref(id_)",
            "            except KeyError:",
            "                assert cdata is not None",
            "                self.chunks[id_] = ChunkIndexEntry(refcount=1, size=size)",
            "                if self.repair:",
            "                    self.repository.put(id_, cdata)",
            "",
            "        def verify_file_chunks(archive_name, item):",
            "            \"\"\"Verifies that all file chunks are present.",
            "",
            "            Missing file chunks will be replaced with new chunks of the same length containing all zeros.",
            "            If a previously missing file chunk re-appears, the replacement chunk is replaced by the correct one.",
            "            \"\"\"",
            "",
            "            def replacement_chunk(size):",
            "                chunk = Chunk(None, allocation=CH_ALLOC, size=size)",
            "                chunk_id, data = cached_hash(chunk, self.key.id_hash)",
            "                cdata = self.repo_objs.format(chunk_id, {}, data)",
            "                return chunk_id, size, cdata",
            "",
            "            offset = 0",
            "            chunk_list = []",
            "            chunks_replaced = False",
            "            has_chunks_healthy = \"chunks_healthy\" in item",
            "            chunks_current = item.chunks",
            "            chunks_healthy = item.chunks_healthy if has_chunks_healthy else chunks_current",
            "            if has_chunks_healthy and len(chunks_current) != len(chunks_healthy):",
            "                # should never happen, but there was issue #3218.",
            "                logger.warning(f\"{archive_name}: {item.path}: Invalid chunks_healthy metadata removed!\")",
            "                del item.chunks_healthy",
            "                has_chunks_healthy = False",
            "                chunks_healthy = chunks_current",
            "            for chunk_current, chunk_healthy in zip(chunks_current, chunks_healthy):",
            "                chunk_id, size = chunk_healthy",
            "                if chunk_id not in self.chunks:",
            "                    # a chunk of the healthy list is missing",
            "                    if chunk_current == chunk_healthy:",
            "                        logger.error(",
            "                            \"{}: {}: New missing file chunk detected (Byte {}-{}, Chunk {}). \"",
            "                            \"Replacing with all-zero chunk.\".format(",
            "                                archive_name, item.path, offset, offset + size, bin_to_hex(chunk_id)",
            "                            )",
            "                        )",
            "                        self.error_found = chunks_replaced = True",
            "                        chunk_id, size, cdata = replacement_chunk(size)",
            "                        add_reference(chunk_id, size, cdata)",
            "                    else:",
            "                        logger.info(",
            "                            \"{}: {}: Previously missing file chunk is still missing (Byte {}-{}, Chunk {}). \"",
            "                            \"It has an all-zero replacement chunk already.\".format(",
            "                                archive_name, item.path, offset, offset + size, bin_to_hex(chunk_id)",
            "                            )",
            "                        )",
            "                        chunk_id, size = chunk_current",
            "                        if chunk_id in self.chunks:",
            "                            add_reference(chunk_id, size)",
            "                        else:",
            "                            logger.warning(",
            "                                \"{}: {}: Missing all-zero replacement chunk detected (Byte {}-{}, Chunk {}). \"",
            "                                \"Generating new replacement chunk.\".format(",
            "                                    archive_name, item.path, offset, offset + size, bin_to_hex(chunk_id)",
            "                                )",
            "                            )",
            "                            self.error_found = chunks_replaced = True",
            "                            chunk_id, size, cdata = replacement_chunk(size)",
            "                            add_reference(chunk_id, size, cdata)",
            "                else:",
            "                    if chunk_current == chunk_healthy:",
            "                        # normal case, all fine.",
            "                        add_reference(chunk_id, size)",
            "                    else:",
            "                        logger.info(",
            "                            \"{}: {}: Healed previously missing file chunk! (Byte {}-{}, Chunk {}).\".format(",
            "                                archive_name, item.path, offset, offset + size, bin_to_hex(chunk_id)",
            "                            )",
            "                        )",
            "                        add_reference(chunk_id, size)",
            "                        mark_as_possibly_superseded(chunk_current[0])  # maybe orphaned the all-zero replacement chunk",
            "                chunk_list.append([chunk_id, size])  # list-typed element as chunks_healthy is list-of-lists",
            "                offset += size",
            "            if chunks_replaced and not has_chunks_healthy:",
            "                # if this is first repair, remember the correct chunk IDs, so we can maybe heal the file later",
            "                item.chunks_healthy = item.chunks",
            "            if has_chunks_healthy and chunk_list == chunks_healthy:",
            "                logger.info(f\"{archive_name}: {item.path}: Completely healed previously damaged file!\")",
            "                del item.chunks_healthy",
            "            item.chunks = chunk_list",
            "            if \"size\" in item:",
            "                item_size = item.size",
            "                item_chunks_size = item.get_size(from_chunks=True)",
            "                if item_size != item_chunks_size:",
            "                    # just warn, but keep the inconsistency, so that borg extract can warn about it.",
            "                    logger.warning(",
            "                        \"{}: {}: size inconsistency detected: size {}, chunks size {}\".format(",
            "                            archive_name, item.path, item_size, item_chunks_size",
            "                        )",
            "                    )",
            "",
            "        def robust_iterator(archive):",
            "            \"\"\"Iterates through all archive items",
            "",
            "            Missing item chunks will be skipped and the msgpack stream will be restarted",
            "            \"\"\"",
            "            item_keys = self.manifest.item_keys",
            "            required_item_keys = REQUIRED_ITEM_KEYS",
            "            unpacker = RobustUnpacker(",
            "                lambda item: isinstance(item, StableDict) and \"path\" in item, self.manifest.item_keys",
            "            )",
            "            _state = 0",
            "",
            "            def missing_chunk_detector(chunk_id):",
            "                nonlocal _state",
            "                if _state % 2 != int(chunk_id not in self.chunks):",
            "                    _state += 1",
            "                return _state",
            "",
            "            def report(msg, chunk_id, chunk_no):",
            "                cid = bin_to_hex(chunk_id)",
            "                msg += \" [chunk: %06d_%s]\" % (chunk_no, cid)  # see \"debug dump-archive-items\"",
            "                self.error_found = True",
            "                logger.error(msg)",
            "",
            "            def list_keys_safe(keys):",
            "                return \", \".join(k.decode(errors=\"replace\") if isinstance(k, bytes) else str(k) for k in keys)",
            "",
            "            def valid_item(obj):",
            "                if not isinstance(obj, StableDict):",
            "                    return False, \"not a dictionary\"",
            "                keys = set(obj)",
            "                if not required_item_keys.issubset(keys):",
            "                    return False, \"missing required keys: \" + list_keys_safe(required_item_keys - keys)",
            "                if not keys.issubset(item_keys):",
            "                    return False, \"invalid keys: \" + list_keys_safe(keys - item_keys)",
            "                return True, \"\"",
            "",
            "            i = 0",
            "            archive_items = archive_get_items(archive, repo_objs=self.repo_objs, repository=repository)",
            "            for state, items in groupby(archive_items, missing_chunk_detector):",
            "                items = list(items)",
            "                if state % 2:",
            "                    for chunk_id in items:",
            "                        report(\"item metadata chunk missing\", chunk_id, i)",
            "                        i += 1",
            "                    continue",
            "                if state > 0:",
            "                    unpacker.resync()",
            "                for chunk_id, cdata in zip(items, repository.get_many(items)):",
            "                    try:",
            "                        _, data = self.repo_objs.parse(chunk_id, cdata)",
            "                        unpacker.feed(data)",
            "                        for item in unpacker:",
            "                            valid, reason = valid_item(item)",
            "                            if valid:",
            "                                yield Item(internal_dict=item)",
            "                            else:",
            "                                report(",
            "                                    \"Did not get expected metadata dict when unpacking item metadata (%s)\" % reason,",
            "                                    chunk_id,",
            "                                    i,",
            "                                )",
            "                    except IntegrityError as integrity_error:",
            "                        # repo_objs.parse() detected integrity issues.",
            "                        # maybe the repo gave us a valid cdata, but not for the chunk_id we wanted.",
            "                        # or the authentication of cdata failed, meaning the encrypted data was corrupted.",
            "                        report(str(integrity_error), chunk_id, i)",
            "                    except msgpack.UnpackException:",
            "                        report(\"Unpacker crashed while unpacking item metadata, trying to resync...\", chunk_id, i)",
            "                        unpacker.resync()",
            "                    except Exception:",
            "                        report(\"Exception while decrypting or unpacking item metadata\", chunk_id, i)",
            "                        raise",
            "                    i += 1",
            "",
            "        sort_by = sort_by.split(\",\")",
            "        if any((first, last, match, older, newer, newest, oldest)):",
            "            archive_infos = self.manifest.archives.list(",
            "                sort_by=sort_by,",
            "                match=match,",
            "                first=first,",
            "                last=last,",
            "                oldest=oldest,",
            "                newest=newest,",
            "                older=older,",
            "                newer=newer,",
            "            )",
            "            if match and not archive_infos:",
            "                logger.warning(\"--match-archives %s does not match any archives\", match)",
            "            if first and len(archive_infos) < first:",
            "                logger.warning(\"--first %d archives: only found %d archives\", first, len(archive_infos))",
            "            if last and len(archive_infos) < last:",
            "                logger.warning(\"--last %d archives: only found %d archives\", last, len(archive_infos))",
            "        else:",
            "            archive_infos = self.manifest.archives.list(sort_by=sort_by, consider_checkpoints=True)",
            "        num_archives = len(archive_infos)",
            "",
            "        pi = ProgressIndicatorPercent(",
            "            total=num_archives, msg=\"Checking archives %3.1f%%\", step=0.1, msgid=\"check.rebuild_refcounts\"",
            "        )",
            "        with cache_if_remote(self.repository) as repository:",
            "            for i, info in enumerate(archive_infos):",
            "                pi.show(i)",
            "                logger.info(f\"Analyzing archive {info.name} ({i + 1}/{num_archives})\")",
            "                archive_id = info.id",
            "                if archive_id not in self.chunks:",
            "                    logger.error(\"Archive metadata block %s is missing!\", bin_to_hex(archive_id))",
            "                    self.error_found = True",
            "                    del self.manifest.archives[info.name]",
            "                    continue",
            "                mark_as_possibly_superseded(archive_id)",
            "                cdata = self.repository.get(archive_id)",
            "                try:",
            "                    _, data = self.repo_objs.parse(archive_id, cdata)",
            "                except IntegrityError as integrity_error:",
            "                    logger.error(\"Archive metadata block %s is corrupted: %s\", bin_to_hex(archive_id), integrity_error)",
            "                    self.error_found = True",
            "                    del self.manifest.archives[info.name]",
            "                    continue",
            "                archive = ArchiveItem(internal_dict=msgpack.unpackb(data))",
            "                if archive.version != 2:",
            "                    raise Exception(\"Unknown archive metadata version\")",
            "                items_buffer = ChunkBuffer(self.key)",
            "                items_buffer.write_chunk = add_callback",
            "                for item in robust_iterator(archive):",
            "                    if \"chunks\" in item:",
            "                        verify_file_chunks(info.name, item)",
            "                    items_buffer.add(item)",
            "                items_buffer.flush(flush=True)",
            "                for previous_item_id in archive_get_items(",
            "                    archive, repo_objs=self.repo_objs, repository=self.repository",
            "                ):",
            "                    mark_as_possibly_superseded(previous_item_id)",
            "                for previous_item_ptr in archive.item_ptrs:",
            "                    mark_as_possibly_superseded(previous_item_ptr)",
            "                archive.item_ptrs = archive_put_items(",
            "                    items_buffer.chunks, repo_objs=self.repo_objs, add_reference=add_reference",
            "                )",
            "                data = msgpack.packb(archive.as_dict())",
            "                new_archive_id = self.key.id_hash(data)",
            "                cdata = self.repo_objs.format(new_archive_id, {}, data)",
            "                add_reference(new_archive_id, len(data), cdata)",
            "                self.manifest.archives[info.name] = (new_archive_id, info.ts)",
            "            pi.finish()",
            "",
            "    def orphan_chunks_check(self):",
            "        if self.check_all:",
            "            unused = {id_ for id_, entry in self.chunks.iteritems() if entry.refcount == 0}",
            "            orphaned = unused - self.possibly_superseded",
            "            if orphaned:",
            "                logger.error(f\"{len(orphaned)} orphaned objects found!\")",
            "                for chunk_id in orphaned:",
            "                    logger.debug(f\"chunk {bin_to_hex(chunk_id)} is orphaned.\")",
            "                self.error_found = True",
            "            if self.repair and unused:",
            "                logger.info(",
            "                    \"Deleting %d orphaned and %d superseded objects...\" % (len(orphaned), len(self.possibly_superseded))",
            "                )",
            "                for id_ in unused:",
            "                    self.repository.delete(id_)",
            "                logger.info(\"Finished deleting orphaned/superseded objects.\")",
            "        else:",
            "            logger.info(\"Orphaned objects check skipped (needs all archives checked).\")",
            "",
            "    def finish(self):",
            "        if self.repair:",
            "            logger.info(\"Writing Manifest.\")",
            "            self.manifest.write()",
            "            logger.info(\"Committing repo.\")",
            "            self.repository.commit(compact=False)",
            "",
            "",
            "class ArchiveRecreater:",
            "    class Interrupted(Exception):",
            "        def __init__(self, metadata=None):",
            "            self.metadata = metadata or {}",
            "",
            "    @staticmethod",
            "    def is_temporary_archive(archive_name):",
            "        return archive_name.endswith(\".recreate\")",
            "",
            "    def __init__(",
            "        self,",
            "        manifest,",
            "        cache,",
            "        matcher,",
            "        exclude_caches=False,",
            "        exclude_if_present=None,",
            "        keep_exclude_tags=False,",
            "        chunker_params=None,",
            "        compression=None,",
            "        recompress=False,",
            "        always_recompress=False,",
            "        dry_run=False,",
            "        stats=False,",
            "        progress=False,",
            "        file_status_printer=None,",
            "        timestamp=None,",
            "        checkpoint_interval=1800,",
            "        checkpoint_volume=0,",
            "    ):",
            "        self.manifest = manifest",
            "        self.repository = manifest.repository",
            "        self.key = manifest.key",
            "        self.repo_objs = manifest.repo_objs",
            "        self.cache = cache",
            "",
            "        self.matcher = matcher",
            "        self.exclude_caches = exclude_caches",
            "        self.exclude_if_present = exclude_if_present or []",
            "        self.keep_exclude_tags = keep_exclude_tags",
            "",
            "        self.rechunkify = chunker_params is not None",
            "        if self.rechunkify:",
            "            logger.debug(\"Rechunking archives to %s\", chunker_params)",
            "        self.chunker_params = chunker_params or CHUNKER_PARAMS",
            "        self.recompress = recompress",
            "        self.always_recompress = always_recompress",
            "        self.compression = compression or CompressionSpec(\"none\")",
            "        self.seen_chunks = set()",
            "",
            "        self.timestamp = timestamp",
            "        self.dry_run = dry_run",
            "        self.stats = stats",
            "        self.progress = progress",
            "        self.print_file_status = file_status_printer or (lambda *args: None)",
            "        self.checkpoint_interval = None if dry_run else checkpoint_interval",
            "        self.checkpoint_volume = None if dry_run else checkpoint_volume",
            "",
            "    def recreate(self, archive_name, comment=None, target_name=None):",
            "        assert not self.is_temporary_archive(archive_name)",
            "        archive = self.open_archive(archive_name)",
            "        target = self.create_target(archive, target_name)",
            "        if self.exclude_if_present or self.exclude_caches:",
            "            self.matcher_add_tagged_dirs(archive)",
            "        if (",
            "            self.matcher.empty()",
            "            and not self.recompress",
            "            and not target.recreate_rechunkify",
            "            and comment is None",
            "            and target_name is None",
            "        ):",
            "            # nothing to do",
            "            return False",
            "        self.process_items(archive, target)",
            "        replace_original = target_name is None",
            "        self.save(archive, target, comment, replace_original=replace_original)",
            "        return True",
            "",
            "    def process_items(self, archive, target):",
            "        matcher = self.matcher",
            "",
            "        for item in archive.iter_items():",
            "            if not matcher.match(item.path):",
            "                self.print_file_status(\"-\", item.path)  # excluded (either by \"-\" or by \"!\")",
            "                continue",
            "            if self.dry_run:",
            "                self.print_file_status(\"+\", item.path)  # included",
            "            else:",
            "                self.process_item(archive, target, item)",
            "        if self.progress:",
            "            target.stats.show_progress(final=True)",
            "",
            "    def process_item(self, archive, target, item):",
            "        status = file_status(item.mode)",
            "        if \"chunks\" in item:",
            "            self.print_file_status(status, item.path)",
            "            status = None",
            "            self.process_chunks(archive, target, item)",
            "            target.stats.nfiles += 1",
            "        target.add_item(item, stats=target.stats)",
            "        self.print_file_status(status, item.path)",
            "",
            "    def process_chunks(self, archive, target, item):",
            "        if not self.recompress and not target.recreate_rechunkify:",
            "            for chunk_id, size in item.chunks:",
            "                self.cache.chunk_incref(chunk_id, target.stats)",
            "            return item.chunks",
            "        chunk_iterator = self.iter_chunks(archive, target, list(item.chunks))",
            "        chunk_processor = partial(self.chunk_processor, target)",
            "        target.process_file_chunks(item, self.cache, target.stats, self.progress, chunk_iterator, chunk_processor)",
            "",
            "    def chunk_processor(self, target, chunk):",
            "        chunk_id, data = cached_hash(chunk, self.key.id_hash)",
            "        if chunk_id in self.seen_chunks:",
            "            return self.cache.chunk_incref(chunk_id, target.stats)",
            "        overwrite = self.recompress",
            "        if self.recompress and not self.always_recompress and chunk_id in self.cache.chunks:",
            "            # Check if this chunk is already compressed the way we want it",
            "            old_meta = self.repo_objs.parse_meta(chunk_id, self.repository.get(chunk_id, read_data=False))",
            "            compr_hdr = bytes((old_meta[\"ctype\"], old_meta[\"clevel\"]))",
            "            compressor_cls, level = Compressor.detect(compr_hdr)",
            "            if (",
            "                compressor_cls.name == self.repo_objs.compressor.decide({}, data).name",
            "                and level == self.repo_objs.compressor.level",
            "            ):",
            "                # Stored chunk has the same compression method and level as we wanted",
            "                overwrite = False",
            "        chunk_entry = self.cache.add_chunk(chunk_id, {}, data, stats=target.stats, overwrite=overwrite, wait=False)",
            "        self.cache.repository.async_response(wait=False)",
            "        self.seen_chunks.add(chunk_entry.id)",
            "        return chunk_entry",
            "",
            "    def iter_chunks(self, archive, target, chunks):",
            "        chunk_iterator = archive.pipeline.fetch_many([chunk_id for chunk_id, _ in chunks])",
            "        if target.recreate_rechunkify:",
            "            # The target.chunker will read the file contents through ChunkIteratorFileWrapper chunk-by-chunk",
            "            # (does not load the entire file into memory)",
            "            file = ChunkIteratorFileWrapper(chunk_iterator)",
            "            yield from target.chunker.chunkify(file)",
            "        else:",
            "            for chunk in chunk_iterator:",
            "                yield Chunk(chunk, size=len(chunk), allocation=CH_DATA)",
            "",
            "    def save(self, archive, target, comment=None, replace_original=True):",
            "        if self.dry_run:",
            "            return",
            "        if comment is None:",
            "            comment = archive.metadata.get(\"comment\", \"\")",
            "",
            "        # Keep for the statistics if necessary",
            "        if self.stats:",
            "            _start = target.start",
            "",
            "        if self.timestamp is None:",
            "            additional_metadata = {",
            "                \"time\": archive.metadata.time,",
            "                \"time_end\": archive.metadata.get(\"time_end\") or archive.metadata.time,",
            "                \"command_line\": archive.metadata.command_line,",
            "                # but also remember recreate metadata:",
            "                \"recreate_command_line\": join_cmd(sys.argv),",
            "            }",
            "        else:",
            "            additional_metadata = {",
            "                \"command_line\": archive.metadata.command_line,",
            "                # but also remember recreate metadata:",
            "                \"recreate_command_line\": join_cmd(sys.argv),",
            "            }",
            "",
            "        target.save(comment=comment, timestamp=self.timestamp, additional_metadata=additional_metadata)",
            "        if replace_original:",
            "            archive.delete(Statistics(), progress=self.progress)",
            "            target.rename(archive.name)",
            "        if self.stats:",
            "            target.start = _start",
            "            target.end = archive_ts_now()",
            "            log_multi(str(target), str(target.stats))",
            "",
            "    def matcher_add_tagged_dirs(self, archive):",
            "        \"\"\"Add excludes to the matcher created by exclude_cache and exclude_if_present.\"\"\"",
            "",
            "        def exclude(dir, tag_item):",
            "            if self.keep_exclude_tags:",
            "                tag_files.append(PathPrefixPattern(tag_item.path, recurse_dir=False))",
            "                tagged_dirs.append(FnmatchPattern(dir + \"/\", recurse_dir=False))",
            "            else:",
            "                tagged_dirs.append(PathPrefixPattern(dir, recurse_dir=False))",
            "",
            "        matcher = self.matcher",
            "        tag_files = []",
            "        tagged_dirs = []",
            "",
            "        for item in archive.iter_items(",
            "            filter=lambda item: os.path.basename(item.path) == CACHE_TAG_NAME or matcher.match(item.path)",
            "        ):",
            "            dir, tag_file = os.path.split(item.path)",
            "            if tag_file in self.exclude_if_present:",
            "                exclude(dir, item)",
            "            elif self.exclude_caches and tag_file == CACHE_TAG_NAME and stat.S_ISREG(item.mode):",
            "                file = open_item(archive, item)",
            "                if file.read(len(CACHE_TAG_CONTENTS)) == CACHE_TAG_CONTENTS:",
            "                    exclude(dir, item)",
            "        matcher.add(tag_files, IECommand.Include)",
            "        matcher.add(tagged_dirs, IECommand.ExcludeNoRecurse)",
            "",
            "    def create_target(self, archive, target_name=None):",
            "        \"\"\"Create target archive.\"\"\"",
            "        target_name = target_name or archive.name + \".recreate\"",
            "        target = self.create_target_archive(target_name)",
            "        # If the archives use the same chunker params, then don't rechunkify",
            "        source_chunker_params = tuple(archive.metadata.get(\"chunker_params\", []))",
            "        if len(source_chunker_params) == 4 and isinstance(source_chunker_params[0], int):",
            "            # this is a borg < 1.2 chunker_params tuple, no chunker algo specified, but we only had buzhash:",
            "            source_chunker_params = (CH_BUZHASH,) + source_chunker_params",
            "        target.recreate_rechunkify = self.rechunkify and source_chunker_params != target.chunker_params",
            "        if target.recreate_rechunkify:",
            "            logger.debug(",
            "                \"Rechunking archive from %s to %s\", source_chunker_params or \"(unknown)\", target.chunker_params",
            "            )",
            "        target.process_file_chunks = ChunksProcessor(",
            "            cache=self.cache,",
            "            key=self.key,",
            "            add_item=target.add_item,",
            "            prepare_checkpoint=target.prepare_checkpoint,",
            "            write_checkpoint=target.write_checkpoint,",
            "            checkpoint_interval=self.checkpoint_interval,",
            "            checkpoint_volume=self.checkpoint_volume,",
            "            rechunkify=target.recreate_rechunkify,",
            "        ).process_file_chunks",
            "        target.chunker = get_chunker(*target.chunker_params, seed=self.key.chunk_seed, sparse=False)",
            "        return target",
            "",
            "    def create_target_archive(self, name):",
            "        target = Archive(",
            "            self.manifest,",
            "            name,",
            "            create=True,",
            "            progress=self.progress,",
            "            chunker_params=self.chunker_params,",
            "            cache=self.cache,",
            "        )",
            "        return target",
            "",
            "    def open_archive(self, name, **kwargs):",
            "        return Archive(self.manifest, name, cache=self.cache, **kwargs)"
        ],
        "afterPatchFile": [
            "import base64",
            "import json",
            "import os",
            "import stat",
            "import sys",
            "import time",
            "from collections import OrderedDict, defaultdict",
            "from contextlib import contextmanager",
            "from datetime import timedelta",
            "from functools import partial",
            "from getpass import getuser",
            "from io import BytesIO",
            "from itertools import groupby, zip_longest",
            "from typing import Iterator",
            "from shutil import get_terminal_size",
            "",
            "from .platformflags import is_win32",
            "from .logger import create_logger",
            "",
            "logger = create_logger()",
            "",
            "from . import xattr",
            "from .chunker import get_chunker, Chunk",
            "from .cache import ChunkListEntry",
            "from .crypto.key import key_factory, UnsupportedPayloadError",
            "from .compress import Compressor, CompressionSpec",
            "from .constants import *  # NOQA",
            "from .crypto.low_level import IntegrityError as IntegrityErrorBase",
            "from .hashindex import ChunkIndex, ChunkIndexEntry, CacheSynchronizer",
            "from .helpers import HardLinkManager",
            "from .helpers import ChunkIteratorFileWrapper, open_item",
            "from .helpers import Error, IntegrityError, set_ec",
            "from .platform import uid2user, user2uid, gid2group, group2gid",
            "from .helpers import parse_timestamp, archive_ts_now",
            "from .helpers import OutputTimestamp, format_timedelta, format_file_size, file_status, FileSize",
            "from .helpers import safe_encode, make_path_safe, remove_surrogates, text_to_json, join_cmd, remove_dotdot_prefixes",
            "from .helpers import StableDict",
            "from .helpers import bin_to_hex",
            "from .helpers import safe_ns",
            "from .helpers import ellipsis_truncate, ProgressIndicatorPercent, log_multi",
            "from .helpers import os_open, flags_normal, flags_dir",
            "from .helpers import os_stat",
            "from .helpers import msgpack",
            "from .helpers import sig_int",
            "from .helpers.lrucache import LRUCache",
            "from .manifest import Manifest",
            "from .patterns import PathPrefixPattern, FnmatchPattern, IECommand",
            "from .item import Item, ArchiveItem, ItemDiff",
            "from .platform import acl_get, acl_set, set_flags, get_flags, swidth, hostname",
            "from .remote import cache_if_remote",
            "from .repository import Repository, LIST_SCAN_LIMIT",
            "from .repoobj import RepoObj",
            "",
            "has_link = hasattr(os, \"link\")",
            "",
            "",
            "class Statistics:",
            "    def __init__(self, output_json=False, iec=False):",
            "        self.output_json = output_json",
            "        self.iec = iec",
            "        self.osize = self.usize = self.nfiles = 0",
            "        self.last_progress = 0  # timestamp when last progress was shown",
            "        self.files_stats = defaultdict(int)",
            "        self.chunking_time = 0.0",
            "        self.hashing_time = 0.0",
            "        self.rx_bytes = 0",
            "        self.tx_bytes = 0",
            "",
            "    def update(self, size, unique):",
            "        self.osize += size",
            "        if unique:",
            "            self.usize += size",
            "",
            "    def __add__(self, other):",
            "        if not isinstance(other, Statistics):",
            "            raise TypeError(\"can only add Statistics objects\")",
            "        stats = Statistics(self.output_json, self.iec)",
            "        stats.osize = self.osize + other.osize",
            "        stats.usize = self.usize + other.usize",
            "        stats.nfiles = self.nfiles + other.nfiles",
            "        stats.chunking_time = self.chunking_time + other.chunking_time",
            "        stats.hashing_time = self.hashing_time + other.hashing_time",
            "        st1, st2 = self.files_stats, other.files_stats",
            "        stats.files_stats = defaultdict(int, {key: (st1[key] + st2[key]) for key in st1.keys() | st2.keys()})",
            "",
            "        return stats",
            "",
            "    def __str__(self):",
            "        hashing_time = format_timedelta(timedelta(seconds=self.hashing_time))",
            "        chunking_time = format_timedelta(timedelta(seconds=self.chunking_time))",
            "        return \"\"\"\\",
            "Number of files: {stats.nfiles}",
            "Original size: {stats.osize_fmt}",
            "Deduplicated size: {stats.usize_fmt}",
            "Time spent in hashing: {hashing_time}",
            "Time spent in chunking: {chunking_time}",
            "Added files: {added_files}",
            "Unchanged files: {unchanged_files}",
            "Modified files: {modified_files}",
            "Error files: {error_files}",
            "Files changed while reading: {files_changed_while_reading}",
            "Bytes read from remote: {stats.rx_bytes}",
            "Bytes sent to remote: {stats.tx_bytes}",
            "\"\"\".format(",
            "            stats=self,",
            "            hashing_time=hashing_time,",
            "            chunking_time=chunking_time,",
            "            added_files=self.files_stats[\"A\"],",
            "            unchanged_files=self.files_stats[\"U\"],",
            "            modified_files=self.files_stats[\"M\"],",
            "            error_files=self.files_stats[\"E\"],",
            "            files_changed_while_reading=self.files_stats[\"C\"],",
            "        )",
            "",
            "    def __repr__(self):",
            "        return \"<{cls} object at {hash:#x} ({self.osize}, {self.usize})>\".format(",
            "            cls=type(self).__name__, hash=id(self), self=self",
            "        )",
            "",
            "    def as_dict(self):",
            "        return {",
            "            \"original_size\": FileSize(self.osize, iec=self.iec),",
            "            \"deduplicated_size\": FileSize(self.usize, iec=self.iec),",
            "            \"nfiles\": self.nfiles,",
            "            \"hashing_time\": self.hashing_time,",
            "            \"chunking_time\": self.chunking_time,",
            "            \"files_stats\": self.files_stats,",
            "        }",
            "",
            "    def as_raw_dict(self):",
            "        return {\"size\": self.osize, \"nfiles\": self.nfiles}",
            "",
            "    @classmethod",
            "    def from_raw_dict(cls, **kw):",
            "        self = cls()",
            "        self.osize = kw[\"size\"]",
            "        self.nfiles = kw[\"nfiles\"]",
            "        return self",
            "",
            "    @property",
            "    def osize_fmt(self):",
            "        return format_file_size(self.osize, iec=self.iec)",
            "",
            "    @property",
            "    def usize_fmt(self):",
            "        return format_file_size(self.usize, iec=self.iec)",
            "",
            "    def show_progress(self, item=None, final=False, stream=None, dt=None):",
            "        now = time.monotonic()",
            "        if dt is None or now - self.last_progress > dt:",
            "            self.last_progress = now",
            "            if self.output_json:",
            "                if not final:",
            "                    data = self.as_dict()",
            "                    if item:",
            "                        data.update(text_to_json(\"path\", item.path))",
            "                else:",
            "                    data = {}",
            "                data.update({\"time\": time.time(), \"type\": \"archive_progress\", \"finished\": final})",
            "                msg = json.dumps(data)",
            "                end = \"\\n\"",
            "            else:",
            "                columns, lines = get_terminal_size()",
            "                if not final:",
            "                    msg = \"{0.osize_fmt} O {0.usize_fmt} U {0.nfiles} N \".format(self)",
            "                    path = remove_surrogates(item.path) if item else \"\"",
            "                    space = columns - swidth(msg)",
            "                    if space < 12:",
            "                        msg = \"\"",
            "                        space = columns - swidth(msg)",
            "                    if space >= 8:",
            "                        msg += ellipsis_truncate(path, space)",
            "                else:",
            "                    msg = \" \" * columns",
            "                end = \"\\r\"",
            "            print(msg, end=end, file=stream or sys.stderr, flush=True)",
            "",
            "",
            "def is_special(mode):",
            "    # file types that get special treatment in --read-special mode",
            "    return stat.S_ISBLK(mode) or stat.S_ISCHR(mode) or stat.S_ISFIFO(mode)",
            "",
            "",
            "class BackupError(Exception):",
            "    \"\"\"",
            "    Exception raised for non-OSError-based exceptions while accessing backup files.",
            "    \"\"\"",
            "",
            "",
            "class BackupOSError(Exception):",
            "    \"\"\"",
            "    Wrapper for OSError raised while accessing backup files.",
            "",
            "    Borg does different kinds of IO, and IO failures have different consequences.",
            "    This wrapper represents failures of input file or extraction IO.",
            "    These are non-critical and are only reported (exit code = 1, warning).",
            "",
            "    Any unwrapped IO error is critical and aborts execution (for example repository IO failure).",
            "    \"\"\"",
            "",
            "    def __init__(self, op, os_error):",
            "        self.op = op",
            "        self.os_error = os_error",
            "        self.errno = os_error.errno",
            "        self.strerror = os_error.strerror",
            "        self.filename = os_error.filename",
            "",
            "    def __str__(self):",
            "        if self.op:",
            "            return f\"{self.op}: {self.os_error}\"",
            "        else:",
            "            return str(self.os_error)",
            "",
            "",
            "class BackupIO:",
            "    op = \"\"",
            "",
            "    def __call__(self, op=\"\"):",
            "        self.op = op",
            "        return self",
            "",
            "    def __enter__(self):",
            "        pass",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        if exc_type and issubclass(exc_type, OSError):",
            "            raise BackupOSError(self.op, exc_val) from exc_val",
            "",
            "",
            "backup_io = BackupIO()",
            "",
            "",
            "def backup_io_iter(iterator):",
            "    backup_io.op = \"read\"",
            "    while True:",
            "        with backup_io:",
            "            try:",
            "                item = next(iterator)",
            "            except StopIteration:",
            "                return",
            "        yield item",
            "",
            "",
            "def stat_update_check(st_old, st_curr):",
            "    \"\"\"",
            "    this checks for some race conditions between the first filename-based stat()",
            "    we did before dispatching to the (hopefully correct) file type backup handler",
            "    and the (hopefully) fd-based fstat() we did in the handler.",
            "",
            "    if there is a problematic difference (e.g. file type changed), we rather",
            "    skip the file than being tricked into a security problem.",
            "",
            "    such races should only happen if:",
            "    - we are backing up a live filesystem (no snapshot, not inactive)",
            "    - if files change due to normal fs activity at an unfortunate time",
            "    - if somebody is doing an attack against us",
            "    \"\"\"",
            "    # assuming that a file type change implicates a different inode change AND that inode numbers",
            "    # are not duplicate in a short timeframe, this check is redundant and solved by the ino check:",
            "    if stat.S_IFMT(st_old.st_mode) != stat.S_IFMT(st_curr.st_mode):",
            "        # in this case, we dispatched to wrong handler - abort",
            "        raise BackupError(\"file type changed (race condition), skipping file\")",
            "    if st_old.st_ino != st_curr.st_ino:",
            "        # in this case, the hardlinks-related code in create_helper has the wrong inode - abort!",
            "        raise BackupError(\"file inode changed (race condition), skipping file\")",
            "    # looks ok, we are still dealing with the same thing - return current stat:",
            "    return st_curr",
            "",
            "",
            "@contextmanager",
            "def OsOpen(*, flags, path=None, parent_fd=None, name=None, noatime=False, op=\"open\"):",
            "    with backup_io(op):",
            "        fd = os_open(path=path, parent_fd=parent_fd, name=name, flags=flags, noatime=noatime)",
            "    try:",
            "        yield fd",
            "    finally:",
            "        # On windows fd is None for directories.",
            "        if fd is not None:",
            "            os.close(fd)",
            "",
            "",
            "class DownloadPipeline:",
            "    def __init__(self, repository, repo_objs):",
            "        self.repository = repository",
            "        self.repo_objs = repo_objs",
            "",
            "    def unpack_many(self, ids, *, filter=None, preload=False):",
            "        \"\"\"",
            "        Return iterator of items.",
            "",
            "        *ids* is a chunk ID list of an item stream. *filter* is a callable",
            "        to decide whether an item will be yielded. *preload* preloads the data chunks of every yielded item.",
            "",
            "        Warning: if *preload* is True then all data chunks of every yielded item have to be retrieved,",
            "        otherwise preloaded chunks will accumulate in RemoteRepository and create a memory leak.",
            "        \"\"\"",
            "        hlids_preloaded = set()",
            "        unpacker = msgpack.Unpacker(use_list=False)",
            "        for data in self.fetch_many(ids):",
            "            unpacker.feed(data)",
            "            for _item in unpacker:",
            "                item = Item(internal_dict=_item)",
            "                if \"chunks\" in item:",
            "                    item.chunks = [ChunkListEntry(*e) for e in item.chunks]",
            "                if filter and not filter(item):",
            "                    continue",
            "                if preload and \"chunks\" in item:",
            "                    hlid = item.get(\"hlid\", None)",
            "                    if hlid is None:",
            "                        preload_chunks = True",
            "                    elif hlid in hlids_preloaded:",
            "                        preload_chunks = False",
            "                    else:",
            "                        # not having the hardlink's chunks already preloaded for other hardlink to same inode",
            "                        preload_chunks = True",
            "                        hlids_preloaded.add(hlid)",
            "                    if preload_chunks:",
            "                        self.repository.preload([c.id for c in item.chunks])",
            "                yield item",
            "",
            "    def fetch_many(self, ids, is_preloaded=False):",
            "        for id_, cdata in zip(ids, self.repository.get_many(ids, is_preloaded=is_preloaded)):",
            "            _, data = self.repo_objs.parse(id_, cdata)",
            "            yield data",
            "",
            "",
            "class ChunkBuffer:",
            "    BUFFER_SIZE = 8 * 1024 * 1024",
            "",
            "    def __init__(self, key, chunker_params=ITEMS_CHUNKER_PARAMS):",
            "        self.buffer = BytesIO()",
            "        self.packer = msgpack.Packer()",
            "        self.chunks = []",
            "        self.key = key",
            "        self.chunker = get_chunker(*chunker_params, seed=self.key.chunk_seed, sparse=False)",
            "        self.saved_chunks_len = None",
            "",
            "    def add(self, item):",
            "        self.buffer.write(self.packer.pack(item.as_dict()))",
            "        if self.is_full():",
            "            self.flush()",
            "",
            "    def write_chunk(self, chunk):",
            "        raise NotImplementedError",
            "",
            "    def flush(self, flush=False):",
            "        if self.buffer.tell() == 0:",
            "            return",
            "        self.buffer.seek(0)",
            "        # The chunker returns a memoryview to its internal buffer,",
            "        # thus a copy is needed before resuming the chunker iterator.",
            "        # the metadata stream may produce all-zero chunks, so deal",
            "        # with CH_ALLOC (and CH_HOLE, for completeness) here.",
            "        chunks = []",
            "        for chunk in self.chunker.chunkify(self.buffer):",
            "            alloc = chunk.meta[\"allocation\"]",
            "            if alloc == CH_DATA:",
            "                data = bytes(chunk.data)",
            "            elif alloc in (CH_ALLOC, CH_HOLE):",
            "                data = zeros[: chunk.meta[\"size\"]]",
            "            else:",
            "                raise ValueError(\"chunk allocation has unsupported value of %r\" % alloc)",
            "            chunks.append(data)",
            "        self.buffer.seek(0)",
            "        self.buffer.truncate(0)",
            "        # Leave the last partial chunk in the buffer unless flush is True",
            "        end = None if flush or len(chunks) == 1 else -1",
            "        for chunk in chunks[:end]:",
            "            self.chunks.append(self.write_chunk(chunk))",
            "        if end == -1:",
            "            self.buffer.write(chunks[-1])",
            "",
            "    def is_full(self):",
            "        return self.buffer.tell() > self.BUFFER_SIZE",
            "",
            "    def save_chunks_state(self):",
            "        # as we only append to self.chunks, remembering the current length is good enough",
            "        self.saved_chunks_len = len(self.chunks)",
            "",
            "    def restore_chunks_state(self):",
            "        scl = self.saved_chunks_len",
            "        assert scl is not None, \"forgot to call save_chunks_state?\"",
            "        tail_chunks = self.chunks[scl:]",
            "        del self.chunks[scl:]",
            "        self.saved_chunks_len = None",
            "        return tail_chunks",
            "",
            "",
            "class CacheChunkBuffer(ChunkBuffer):",
            "    def __init__(self, cache, key, stats, chunker_params=ITEMS_CHUNKER_PARAMS):",
            "        super().__init__(key, chunker_params)",
            "        self.cache = cache",
            "        self.stats = stats",
            "",
            "    def write_chunk(self, chunk):",
            "        id_, _ = self.cache.add_chunk(self.key.id_hash(chunk), {}, chunk, stats=self.stats, wait=False)",
            "        logger.debug(f\"writing item metadata stream chunk {bin_to_hex(id_)}\")",
            "        self.cache.repository.async_response(wait=False)",
            "        return id_",
            "",
            "",
            "def get_item_uid_gid(item, *, numeric, uid_forced=None, gid_forced=None, uid_default=0, gid_default=0):",
            "    if uid_forced is not None:",
            "        uid = uid_forced",
            "    else:",
            "        uid = None if numeric else user2uid(item.get(\"user\"))",
            "        uid = item.get(\"uid\") if uid is None else uid",
            "        if uid is None or uid < 0:",
            "            uid = uid_default",
            "    if gid_forced is not None:",
            "        gid = gid_forced",
            "    else:",
            "        gid = None if numeric else group2gid(item.get(\"group\"))",
            "        gid = item.get(\"gid\") if gid is None else gid",
            "        if gid is None or gid < 0:",
            "            gid = gid_default",
            "    return uid, gid",
            "",
            "",
            "def archive_get_items(metadata, *, repo_objs, repository):",
            "    if \"item_ptrs\" in metadata:  # looks like a v2+ archive",
            "        assert \"items\" not in metadata",
            "        items = []",
            "        for id, cdata in zip(metadata.item_ptrs, repository.get_many(metadata.item_ptrs)):",
            "            _, data = repo_objs.parse(id, cdata)",
            "            ids = msgpack.unpackb(data)",
            "            items.extend(ids)",
            "        return items",
            "",
            "    if \"items\" in metadata:  # legacy, v1 archive",
            "        assert \"item_ptrs\" not in metadata",
            "        return metadata.items",
            "",
            "",
            "def archive_put_items(chunk_ids, *, repo_objs, cache=None, stats=None, add_reference=None):",
            "    \"\"\"gets a (potentially large) list of archive metadata stream chunk ids and writes them to repo objects\"\"\"",
            "    item_ptrs = []",
            "    for i in range(0, len(chunk_ids), IDS_PER_CHUNK):",
            "        data = msgpack.packb(chunk_ids[i : i + IDS_PER_CHUNK])",
            "        id = repo_objs.id_hash(data)",
            "        logger.debug(f\"writing item_ptrs chunk {bin_to_hex(id)}\")",
            "        if cache is not None and stats is not None:",
            "            cache.add_chunk(id, {}, data, stats=stats)",
            "        elif add_reference is not None:",
            "            cdata = repo_objs.format(id, {}, data)",
            "            add_reference(id, len(data), cdata)",
            "        else:",
            "            raise NotImplementedError",
            "        item_ptrs.append(id)",
            "    return item_ptrs",
            "",
            "",
            "class Archive:",
            "    class DoesNotExist(Error):",
            "        \"\"\"Archive {} does not exist\"\"\"",
            "",
            "    class AlreadyExists(Error):",
            "        \"\"\"Archive {} already exists\"\"\"",
            "",
            "    class IncompatibleFilesystemEncodingError(Error):",
            "        \"\"\"Failed to encode filename \"{}\" into file system encoding \"{}\". Consider configuring the LANG environment variable.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        manifest,",
            "        name,",
            "        cache=None,",
            "        create=False,",
            "        numeric_ids=False,",
            "        noatime=False,",
            "        noctime=False,",
            "        noflags=False,",
            "        noacls=False,",
            "        noxattrs=False,",
            "        progress=False,",
            "        chunker_params=CHUNKER_PARAMS,",
            "        start=None,",
            "        start_monotonic=None,",
            "        end=None,",
            "        log_json=False,",
            "        iec=False,",
            "    ):",
            "        self.cwd = os.getcwd()",
            "        assert isinstance(manifest, Manifest)",
            "        self.manifest = manifest",
            "        self.key = manifest.repo_objs.key",
            "        self.repo_objs = manifest.repo_objs",
            "        self.repository = manifest.repository",
            "        self.cache = cache",
            "        self.stats = Statistics(output_json=log_json, iec=iec)",
            "        self.iec = iec",
            "        self.show_progress = progress",
            "        self.name = name  # overwritten later with name from archive metadata",
            "        self.name_in_manifest = name  # can differ from .name later (if borg check fixed duplicate archive names)",
            "        self.comment = None",
            "        self.tam_verified = False",
            "        self.numeric_ids = numeric_ids",
            "        self.noatime = noatime",
            "        self.noctime = noctime",
            "        self.noflags = noflags",
            "        self.noacls = noacls",
            "        self.noxattrs = noxattrs",
            "        assert (start is None) == (",
            "            start_monotonic is None",
            "        ), \"Logic error: if start is given, start_monotonic must be given as well and vice versa.\"",
            "        if start is None:",
            "            start = archive_ts_now()",
            "            start_monotonic = time.monotonic()",
            "        self.chunker_params = chunker_params",
            "        self.start = start",
            "        self.start_monotonic = start_monotonic",
            "        if end is None:",
            "            end = archive_ts_now()",
            "        self.end = end",
            "        self.pipeline = DownloadPipeline(self.repository, self.repo_objs)",
            "        self.create = create",
            "        if self.create:",
            "            self.items_buffer = CacheChunkBuffer(self.cache, self.key, self.stats)",
            "            if name in manifest.archives:",
            "                raise self.AlreadyExists(name)",
            "            i = 0",
            "            while True:",
            "                self.checkpoint_name = \"{}.checkpoint{}\".format(name, i and (\".%d\" % i) or \"\")",
            "                if self.checkpoint_name not in manifest.archives:",
            "                    break",
            "                i += 1",
            "        else:",
            "            info = self.manifest.archives.get(name)",
            "            if info is None:",
            "                raise self.DoesNotExist(name)",
            "            self.load(info.id)",
            "",
            "    def _load_meta(self, id):",
            "        cdata = self.repository.get(id)",
            "        _, data = self.repo_objs.parse(id, cdata)",
            "        # we do not require TAM for archives, otherwise we can not even borg list a repo with old archives.",
            "        archive, self.tam_verified, _ = self.key.unpack_and_verify_archive(data, force_tam_not_required=True)",
            "        metadata = ArchiveItem(internal_dict=archive)",
            "        if metadata.version not in (1, 2):  # legacy: still need to read v1 archives",
            "            raise Exception(\"Unknown archive metadata version\")",
            "        # note: metadata.items must not get written to disk!",
            "        metadata.items = archive_get_items(metadata, repo_objs=self.repo_objs, repository=self.repository)",
            "        return metadata",
            "",
            "    def load(self, id):",
            "        self.id = id",
            "        self.metadata = self._load_meta(self.id)",
            "        self.name = self.metadata.name",
            "        self.comment = self.metadata.get(\"comment\", \"\")",
            "",
            "    @property",
            "    def ts(self):",
            "        \"\"\"Timestamp of archive creation (start) in UTC\"\"\"",
            "        ts = self.metadata.time",
            "        return parse_timestamp(ts)",
            "",
            "    @property",
            "    def ts_end(self):",
            "        \"\"\"Timestamp of archive creation (end) in UTC\"\"\"",
            "        # fall back to time if there is no time_end present in metadata",
            "        ts = self.metadata.get(\"time_end\") or self.metadata.time",
            "        return parse_timestamp(ts)",
            "",
            "    @property",
            "    def fpr(self):",
            "        return bin_to_hex(self.id)",
            "",
            "    @property",
            "    def duration(self):",
            "        return format_timedelta(self.end - self.start)",
            "",
            "    @property",
            "    def duration_from_meta(self):",
            "        return format_timedelta(self.ts_end - self.ts)",
            "",
            "    def info(self):",
            "        if self.create:",
            "            stats = self.stats",
            "            start = self.start",
            "            end = self.end",
            "        else:",
            "            stats = self.calc_stats(self.cache)",
            "            start = self.ts",
            "            end = self.ts_end",
            "        info = {",
            "            \"name\": self.name,",
            "            \"id\": self.fpr,",
            "            \"start\": OutputTimestamp(start),",
            "            \"end\": OutputTimestamp(end),",
            "            \"duration\": (end - start).total_seconds(),",
            "            \"stats\": stats.as_dict(),",
            "        }",
            "        if self.create:",
            "            info[\"command_line\"] = join_cmd(sys.argv)",
            "        else:",
            "            info.update(",
            "                {",
            "                    \"command_line\": self.metadata.command_line,",
            "                    \"hostname\": self.metadata.hostname,",
            "                    \"username\": self.metadata.username,",
            "                    \"comment\": self.metadata.get(\"comment\", \"\"),",
            "                    \"chunker_params\": self.metadata.get(\"chunker_params\", \"\"),",
            "                }",
            "            )",
            "        return info",
            "",
            "    def __str__(self):",
            "        return \"\"\"\\",
            "Repository: {location}",
            "Archive name: {0.name}",
            "Archive fingerprint: {0.fpr}",
            "Time (start): {start}",
            "Time (end):   {end}",
            "Duration: {0.duration}",
            "\"\"\".format(",
            "            self,",
            "            start=OutputTimestamp(self.start),",
            "            end=OutputTimestamp(self.end),",
            "            location=self.repository._location.canonical_path(),",
            "        )",
            "",
            "    def __repr__(self):",
            "        return \"Archive(%r)\" % self.name",
            "",
            "    def item_filter(self, item, filter=None):",
            "        return filter(item) if filter else True",
            "",
            "    def iter_items(self, filter=None, preload=False):",
            "        # note: when calling this with preload=True, later fetch_many() must be called with",
            "        # is_preloaded=True or the RemoteRepository code will leak memory!",
            "        yield from self.pipeline.unpack_many(",
            "            self.metadata.items, preload=preload, filter=lambda item: self.item_filter(item, filter)",
            "        )",
            "",
            "    def add_item(self, item, show_progress=True, stats=None):",
            "        if show_progress and self.show_progress:",
            "            if stats is None:",
            "                stats = self.stats",
            "            stats.show_progress(item=item, dt=0.2)",
            "        self.items_buffer.add(item)",
            "",
            "    def prepare_checkpoint(self):",
            "        # we need to flush the archive metadata stream to repo chunks, so that",
            "        # we have the metadata stream chunks WITHOUT the part file item we add later.",
            "        # The part file item will then get into its own metadata stream chunk, which we",
            "        # can easily NOT include into the next checkpoint or the final archive.",
            "        self.items_buffer.flush(flush=True)",
            "        # remember the current state of self.chunks, which corresponds to the flushed chunks",
            "        self.items_buffer.save_chunks_state()",
            "",
            "    def write_checkpoint(self):",
            "        metadata = self.save(self.checkpoint_name)",
            "        # that .save() has committed the repo.",
            "        # at next commit, we won't need this checkpoint archive any more because we will then",
            "        # have either a newer checkpoint archive or the final archive.",
            "        # so we can already remove it here, the next .save() will then commit this cleanup.",
            "        # remove its manifest entry, remove its ArchiveItem chunk, remove its item_ptrs chunks:",
            "        del self.manifest.archives[self.checkpoint_name]",
            "        self.cache.chunk_decref(self.id, self.stats)",
            "        for id in metadata.item_ptrs:",
            "            self.cache.chunk_decref(id, self.stats)",
            "        # also get rid of that part item, we do not want to have it in next checkpoint or final archive",
            "        tail_chunks = self.items_buffer.restore_chunks_state()",
            "        # tail_chunks contain the tail of the archive items metadata stream, not needed for next commit.",
            "        for id in tail_chunks:",
            "            self.cache.chunk_decref(id, self.stats)",
            "",
            "    def save(self, name=None, comment=None, timestamp=None, stats=None, additional_metadata=None):",
            "        name = name or self.name",
            "        if name in self.manifest.archives:",
            "            raise self.AlreadyExists(name)",
            "        self.items_buffer.flush(flush=True)",
            "        item_ptrs = archive_put_items(",
            "            self.items_buffer.chunks, repo_objs=self.repo_objs, cache=self.cache, stats=self.stats",
            "        )",
            "        duration = timedelta(seconds=time.monotonic() - self.start_monotonic)",
            "        if timestamp is None:",
            "            end = archive_ts_now()",
            "            start = end - duration",
            "        else:",
            "            start = timestamp",
            "            end = start + duration",
            "        self.start = start",
            "        self.end = end",
            "        metadata = {",
            "            \"version\": 2,",
            "            \"name\": name,",
            "            \"comment\": comment or \"\",",
            "            \"item_ptrs\": item_ptrs,  # see #1473",
            "            \"command_line\": join_cmd(sys.argv),",
            "            \"hostname\": hostname,",
            "            \"username\": getuser(),",
            "            \"time\": start.isoformat(timespec=\"microseconds\"),",
            "            \"time_end\": end.isoformat(timespec=\"microseconds\"),",
            "            \"chunker_params\": self.chunker_params,",
            "        }",
            "        # we always want to create archives with the addtl. metadata (nfiles, etc.),",
            "        # because borg info relies on them. so, either use the given stats (from args)",
            "        # or fall back to self.stats if it was not given.",
            "        stats = stats or self.stats",
            "        metadata.update({\"size\": stats.osize, \"nfiles\": stats.nfiles})",
            "        metadata.update(additional_metadata or {})",
            "        metadata = ArchiveItem(metadata)",
            "        data = self.key.pack_and_authenticate_metadata(metadata.as_dict(), context=b\"archive\")",
            "        self.id = self.repo_objs.id_hash(data)",
            "        try:",
            "            self.cache.add_chunk(self.id, {}, data, stats=self.stats)",
            "        except IntegrityError as err:",
            "            err_msg = str(err)",
            "            # hack to avoid changing the RPC protocol by introducing new (more specific) exception class",
            "            if \"More than allowed put data\" in err_msg:",
            "                raise Error(\"%s - archive too big (issue #1473)!\" % err_msg)",
            "            else:",
            "                raise",
            "        while self.repository.async_response(wait=True) is not None:",
            "            pass",
            "        self.manifest.archives[name] = (self.id, metadata.time)",
            "        self.manifest.write()",
            "        self.repository.commit(compact=False)",
            "        self.cache.commit()",
            "        return metadata",
            "",
            "    def calc_stats(self, cache, want_unique=True):",
            "        if not want_unique:",
            "            unique_size = 0",
            "        else:",
            "",
            "            def add(id):",
            "                entry = cache.chunks[id]",
            "                archive_index.add(id, 1, entry.size)",
            "",
            "            archive_index = ChunkIndex()",
            "            sync = CacheSynchronizer(archive_index)",
            "            add(self.id)",
            "            # we must escape any % char in the archive name, because we use it in a format string, see #6500",
            "            arch_name_escd = self.name.replace(\"%\", \"%%\")",
            "            pi = ProgressIndicatorPercent(",
            "                total=len(self.metadata.items),",
            "                msg=\"Calculating statistics for archive %s ... %%3.0f%%%%\" % arch_name_escd,",
            "                msgid=\"archive.calc_stats\",",
            "            )",
            "            for id, chunk in zip(self.metadata.items, self.repository.get_many(self.metadata.items)):",
            "                pi.show(increase=1)",
            "                add(id)",
            "                _, data = self.repo_objs.parse(id, chunk)",
            "                sync.feed(data)",
            "            unique_size = archive_index.stats_against(cache.chunks)[1]",
            "            pi.finish()",
            "",
            "        stats = Statistics(iec=self.iec)",
            "        stats.usize = unique_size",
            "        stats.nfiles = self.metadata.nfiles",
            "        stats.osize = self.metadata.size",
            "        return stats",
            "",
            "    @contextmanager",
            "    def extract_helper(self, item, path, hlm, *, dry_run=False):",
            "        hardlink_set = False",
            "        # Hard link?",
            "        if \"hlid\" in item:",
            "            link_target = hlm.retrieve(id=item.hlid)",
            "            if link_target is not None and has_link:",
            "                if not dry_run:",
            "                    # another hardlink to same inode (same hlid) was extracted previously, just link to it",
            "                    with backup_io(\"link\"):",
            "                        os.link(link_target, path, follow_symlinks=False)",
            "                hardlink_set = True",
            "        yield hardlink_set",
            "        if not hardlink_set:",
            "            if \"hlid\" in item and has_link:",
            "                # Update entry with extracted item path, so that following hardlinks don't extract twice.",
            "                # We have hardlinking support, so we will hardlink not extract.",
            "                hlm.remember(id=item.hlid, info=path)",
            "            else:",
            "                # Broken platform with no hardlinking support.",
            "                # In this case, we *want* to extract twice, because there is no other way.",
            "                pass",
            "",
            "    def extract_item(",
            "        self,",
            "        item,",
            "        *,",
            "        restore_attrs=True,",
            "        dry_run=False,",
            "        stdout=False,",
            "        sparse=False,",
            "        hlm=None,",
            "        pi=None,",
            "        continue_extraction=False,",
            "    ):",
            "        \"\"\"",
            "        Extract archive item.",
            "",
            "        :param item: the item to extract",
            "        :param restore_attrs: restore file attributes",
            "        :param dry_run: do not write any data",
            "        :param stdout: write extracted data to stdout",
            "        :param sparse: write sparse files (chunk-granularity, independent of the original being sparse)",
            "        :param hlm: maps hlid to link_target for extracting subtrees with hardlinks correctly",
            "        :param pi: ProgressIndicatorPercent (or similar) for file extraction progress (in bytes)",
            "        :param continue_extraction: continue a previously interrupted extraction of same archive",
            "        \"\"\"",
            "",
            "        def same_item(item, st):",
            "            \"\"\"is the archived item the same as the fs item at same path with stat st?\"\"\"",
            "            if not stat.S_ISREG(st.st_mode):",
            "                # we only \"optimize\" for regular files.",
            "                # other file types are less frequent and have no content extraction we could \"optimize away\".",
            "                return False",
            "            if item.mode != st.st_mode or item.size != st.st_size:",
            "                # the size check catches incomplete previous file extraction",
            "                return False",
            "            if item.get(\"mtime\") != st.st_mtime_ns:",
            "                # note: mtime is \"extracted\" late, after xattrs and ACLs, but before flags.",
            "                return False",
            "            # this is good enough for the intended use case:",
            "            # continuing an extraction of same archive that initially started in an empty directory.",
            "            # there is a very small risk that \"bsdflags\" of one file are wrong:",
            "            # if a previous extraction was interrupted between setting the mtime and setting non-default flags.",
            "            return True",
            "",
            "        has_damaged_chunks = \"chunks_healthy\" in item",
            "        if dry_run or stdout:",
            "            with self.extract_helper(item, \"\", hlm, dry_run=dry_run or stdout) as hardlink_set:",
            "                if not hardlink_set:",
            "                    # it does not really set hardlinks due to dry_run, but we need to behave same",
            "                    # as non-dry_run concerning fetching preloaded chunks from the pipeline or",
            "                    # it would get stuck.",
            "                    if \"chunks\" in item:",
            "                        item_chunks_size = 0",
            "                        for data in self.pipeline.fetch_many([c.id for c in item.chunks], is_preloaded=True):",
            "                            if pi:",
            "                                pi.show(increase=len(data), info=[remove_surrogates(item.path)])",
            "                            if stdout:",
            "                                sys.stdout.buffer.write(data)",
            "                            item_chunks_size += len(data)",
            "                        if stdout:",
            "                            sys.stdout.buffer.flush()",
            "                        if \"size\" in item:",
            "                            item_size = item.size",
            "                            if item_size != item_chunks_size:",
            "                                raise BackupError(",
            "                                    \"Size inconsistency detected: size {}, chunks size {}\".format(",
            "                                        item_size, item_chunks_size",
            "                                    )",
            "                                )",
            "            if has_damaged_chunks:",
            "                raise BackupError(\"File has damaged (all-zero) chunks. Try running borg check --repair.\")",
            "            return",
            "",
            "        dest = self.cwd",
            "        path = os.path.join(dest, item.path)",
            "        # Attempt to remove existing files, ignore errors on failure",
            "        try:",
            "            st = os.stat(path, follow_symlinks=False)",
            "            if continue_extraction and same_item(item, st):",
            "                return  # done! we already have fully extracted this file in a previous run.",
            "            elif stat.S_ISDIR(st.st_mode):",
            "                os.rmdir(path)",
            "            else:",
            "                os.unlink(path)",
            "        except UnicodeEncodeError:",
            "            raise self.IncompatibleFilesystemEncodingError(path, sys.getfilesystemencoding()) from None",
            "        except OSError:",
            "            pass",
            "",
            "        def make_parent(path):",
            "            parent_dir = os.path.dirname(path)",
            "            if not os.path.exists(parent_dir):",
            "                os.makedirs(parent_dir)",
            "",
            "        mode = item.mode",
            "        if stat.S_ISREG(mode):",
            "            with backup_io(\"makedirs\"):",
            "                make_parent(path)",
            "            with self.extract_helper(item, path, hlm) as hardlink_set:",
            "                if hardlink_set:",
            "                    return",
            "                with backup_io(\"open\"):",
            "                    fd = open(path, \"wb\")",
            "                with fd:",
            "                    ids = [c.id for c in item.chunks]",
            "                    for data in self.pipeline.fetch_many(ids, is_preloaded=True):",
            "                        if pi:",
            "                            pi.show(increase=len(data), info=[remove_surrogates(item.path)])",
            "                        with backup_io(\"write\"):",
            "                            if sparse and zeros.startswith(data):",
            "                                # all-zero chunk: create a hole in a sparse file",
            "                                fd.seek(len(data), 1)",
            "                            else:",
            "                                fd.write(data)",
            "                    with backup_io(\"truncate_and_attrs\"):",
            "                        pos = item_chunks_size = fd.tell()",
            "                        fd.truncate(pos)",
            "                        fd.flush()",
            "                        self.restore_attrs(path, item, fd=fd.fileno())",
            "                if \"size\" in item:",
            "                    item_size = item.size",
            "                    if item_size != item_chunks_size:",
            "                        raise BackupError(",
            "                            f\"Size inconsistency detected: size {item_size}, chunks size {item_chunks_size}\"",
            "                        )",
            "                if has_damaged_chunks:",
            "                    raise BackupError(\"File has damaged (all-zero) chunks. Try running borg check --repair.\")",
            "            return",
            "        with backup_io:",
            "            # No repository access beyond this point.",
            "            if stat.S_ISDIR(mode):",
            "                make_parent(path)",
            "                if not os.path.exists(path):",
            "                    os.mkdir(path)",
            "                if restore_attrs:",
            "                    self.restore_attrs(path, item)",
            "            elif stat.S_ISLNK(mode):",
            "                make_parent(path)",
            "                with self.extract_helper(item, path, hlm) as hardlink_set:",
            "                    if hardlink_set:",
            "                        # unusual, but possible: this is a hardlinked symlink.",
            "                        return",
            "                    target = item.target",
            "                    try:",
            "                        os.symlink(target, path)",
            "                    except UnicodeEncodeError:",
            "                        raise self.IncompatibleFilesystemEncodingError(target, sys.getfilesystemencoding()) from None",
            "                    self.restore_attrs(path, item, symlink=True)",
            "            elif stat.S_ISFIFO(mode):",
            "                make_parent(path)",
            "                with self.extract_helper(item, path, hlm) as hardlink_set:",
            "                    if hardlink_set:",
            "                        return",
            "                    os.mkfifo(path)",
            "                    self.restore_attrs(path, item)",
            "            elif stat.S_ISCHR(mode) or stat.S_ISBLK(mode):",
            "                make_parent(path)",
            "                with self.extract_helper(item, path, hlm) as hardlink_set:",
            "                    if hardlink_set:",
            "                        return",
            "                    os.mknod(path, item.mode, item.rdev)",
            "                    self.restore_attrs(path, item)",
            "            else:",
            "                raise Exception(\"Unknown archive item type %r\" % item.mode)",
            "",
            "    def restore_attrs(self, path, item, symlink=False, fd=None):",
            "        \"\"\"",
            "        Restore filesystem attributes on *path* (*fd*) from *item*.",
            "",
            "        Does not access the repository.",
            "        \"\"\"",
            "        backup_io.op = \"attrs\"",
            "        # This code is a bit of a mess due to OS specific differences.",
            "        if not is_win32:",
            "            # by using uid_default = -1 and gid_default = -1, they will not be restored if",
            "            # the archived item has no information about them.",
            "            uid, gid = get_item_uid_gid(item, numeric=self.numeric_ids, uid_default=-1, gid_default=-1)",
            "            # if uid and/or gid is -1, chown will keep it as is and not change it.",
            "            try:",
            "                if fd:",
            "                    os.fchown(fd, uid, gid)",
            "                else:",
            "                    os.chown(path, uid, gid, follow_symlinks=False)",
            "            except OSError:",
            "                pass",
            "            if fd:",
            "                os.fchmod(fd, item.mode)",
            "            else:",
            "                # To check whether a particular function in the os module accepts False for its",
            "                # follow_symlinks parameter, the in operator on supports_follow_symlinks should be",
            "                # used. However, os.chmod is special as some platforms without a working lchmod() do",
            "                # have fchmodat(), which has a flag that makes it behave like lchmod(). fchmodat()",
            "                # is ignored when deciding whether or not os.chmod should be set in",
            "                # os.supports_follow_symlinks. Work around this by using try/except.",
            "                try:",
            "                    os.chmod(path, item.mode, follow_symlinks=False)",
            "                except NotImplementedError:",
            "                    if not symlink:",
            "                        os.chmod(path, item.mode)",
            "            if not self.noacls:",
            "                acl_set(path, item, self.numeric_ids, fd=fd)",
            "            if not self.noxattrs and \"xattrs\" in item:",
            "                # chown removes Linux capabilities, so set the extended attributes at the end, after chown,",
            "                # since they include the Linux capabilities in the \"security.capability\" attribute.",
            "                warning = xattr.set_all(fd or path, item.xattrs, follow_symlinks=False)",
            "                if warning:",
            "                    set_ec(EXIT_WARNING)",
            "            # set timestamps rather late",
            "            mtime = item.mtime",
            "            atime = item.atime if \"atime\" in item else mtime",
            "            if \"birthtime\" in item:",
            "                birthtime = item.birthtime",
            "                try:",
            "                    # This should work on FreeBSD, NetBSD, and Darwin and be harmless on other platforms.",
            "                    # See utimes(2) on either of the BSDs for details.",
            "                    if fd:",
            "                        os.utime(fd, None, ns=(atime, birthtime))",
            "                    else:",
            "                        os.utime(path, None, ns=(atime, birthtime), follow_symlinks=False)",
            "                except OSError:",
            "                    # some systems don't support calling utime on a symlink",
            "                    pass",
            "            try:",
            "                if fd:",
            "                    os.utime(fd, None, ns=(atime, mtime))",
            "                else:",
            "                    os.utime(path, None, ns=(atime, mtime), follow_symlinks=False)",
            "            except OSError:",
            "                # some systems don't support calling utime on a symlink",
            "                pass",
            "            # bsdflags include the immutable flag and need to be set last:",
            "            if not self.noflags and \"bsdflags\" in item:",
            "                try:",
            "                    set_flags(path, item.bsdflags, fd=fd)",
            "                except OSError:",
            "                    pass",
            "        else:  # win32",
            "            # set timestamps rather late",
            "            mtime = item.mtime",
            "            atime = item.atime if \"atime\" in item else mtime",
            "            try:",
            "                # note: no fd support on win32",
            "                os.utime(path, None, ns=(atime, mtime))",
            "            except OSError:",
            "                # some systems don't support calling utime on a symlink",
            "                pass",
            "",
            "    def set_meta(self, key, value):",
            "        metadata = self._load_meta(self.id)",
            "        setattr(metadata, key, value)",
            "        if \"items\" in metadata:",
            "            del metadata.items",
            "        data = self.key.pack_and_authenticate_metadata(metadata.as_dict(), context=b\"archive\")",
            "        new_id = self.key.id_hash(data)",
            "        self.cache.add_chunk(new_id, {}, data, stats=self.stats)",
            "        self.manifest.archives[self.name] = (new_id, metadata.time)",
            "        self.cache.chunk_decref(self.id, self.stats)",
            "        self.id = new_id",
            "",
            "    def rename(self, name):",
            "        if name in self.manifest.archives:",
            "            raise self.AlreadyExists(name)",
            "        oldname = self.name",
            "        self.name = name",
            "        self.set_meta(\"name\", name)",
            "        del self.manifest.archives[oldname]",
            "",
            "    def delete(self, stats, progress=False, forced=False):",
            "        class ChunksIndexError(Error):",
            "            \"\"\"Chunk ID {} missing from chunks index, corrupted chunks index - aborting transaction.\"\"\"",
            "",
            "        exception_ignored = object()",
            "",
            "        def fetch_async_response(wait=True):",
            "            try:",
            "                return self.repository.async_response(wait=wait)",
            "            except Repository.ObjectNotFound:",
            "                nonlocal error",
            "                # object not in repo - strange, but we wanted to delete it anyway.",
            "                if forced == 0:",
            "                    raise",
            "                error = True",
            "                return exception_ignored  # must not return None here",
            "",
            "        def chunk_decref(id, stats):",
            "            try:",
            "                self.cache.chunk_decref(id, stats, wait=False)",
            "            except KeyError:",
            "                cid = bin_to_hex(id)",
            "                raise ChunksIndexError(cid)",
            "            else:",
            "                fetch_async_response(wait=False)",
            "",
            "        error = False",
            "        try:",
            "            unpacker = msgpack.Unpacker(use_list=False)",
            "            items_ids = self.metadata.items",
            "            pi = ProgressIndicatorPercent(",
            "                total=len(items_ids), msg=\"Decrementing references %3.0f%%\", msgid=\"archive.delete\"",
            "            )",
            "            for i, (items_id, data) in enumerate(zip(items_ids, self.repository.get_many(items_ids))):",
            "                if progress:",
            "                    pi.show(i)",
            "                _, data = self.repo_objs.parse(items_id, data)",
            "                unpacker.feed(data)",
            "                chunk_decref(items_id, stats)",
            "                try:",
            "                    for item in unpacker:",
            "                        item = Item(internal_dict=item)",
            "                        if \"chunks\" in item:",
            "                            for chunk_id, size in item.chunks:",
            "                                chunk_decref(chunk_id, stats)",
            "                except (TypeError, ValueError):",
            "                    # if items metadata spans multiple chunks and one chunk got dropped somehow,",
            "                    # it could be that unpacker yields bad types",
            "                    if forced == 0:",
            "                        raise",
            "                    error = True",
            "            if progress:",
            "                pi.finish()",
            "        except (msgpack.UnpackException, Repository.ObjectNotFound):",
            "            # items metadata corrupted",
            "            if forced == 0:",
            "                raise",
            "            error = True",
            "",
            "        # delete the blocks that store all the references that end up being loaded into metadata.items:",
            "        for id in self.metadata.item_ptrs:",
            "            chunk_decref(id, stats)",
            "",
            "        # in forced delete mode, we try hard to delete at least the manifest entry,",
            "        # if possible also the archive superblock, even if processing the items raises",
            "        # some harmless exception.",
            "        chunk_decref(self.id, stats)",
            "        del self.manifest.archives[self.name]",
            "        while fetch_async_response(wait=True) is not None:",
            "            # we did async deletes, process outstanding results (== exceptions),",
            "            # so there is nothing pending when we return and our caller wants to commit.",
            "            pass",
            "        if error:",
            "            logger.warning(\"forced deletion succeeded, but the deleted archive was corrupted.\")",
            "            logger.warning(\"borg check --repair is required to free all space.\")",
            "",
            "    @staticmethod",
            "    def compare_archives_iter(",
            "        archive1: \"Archive\", archive2: \"Archive\", matcher=None, can_compare_chunk_ids=False",
            "    ) -> Iterator[ItemDiff]:",
            "        \"\"\"",
            "        Yields an ItemDiff instance describing changes/indicating equality.",
            "",
            "        :param matcher: PatternMatcher class to restrict results to only matching paths.",
            "        :param can_compare_chunk_ids: Whether --chunker-params are the same for both archives.",
            "        \"\"\"",
            "",
            "        def compare_items(path: str, item1: Item, item2: Item):",
            "            return ItemDiff(",
            "                path,",
            "                item1,",
            "                item2,",
            "                archive1.pipeline.fetch_many([c.id for c in item1.get(\"chunks\", [])]),",
            "                archive2.pipeline.fetch_many([c.id for c in item2.get(\"chunks\", [])]),",
            "                can_compare_chunk_ids=can_compare_chunk_ids,",
            "            )",
            "",
            "        orphans_archive1: OrderedDict[str, Item] = OrderedDict()",
            "        orphans_archive2: OrderedDict[str, Item] = OrderedDict()",
            "",
            "        assert matcher is not None, \"matcher must be set\"",
            "",
            "        for item1, item2 in zip_longest(",
            "            archive1.iter_items(lambda item: matcher.match(item.path)),",
            "            archive2.iter_items(lambda item: matcher.match(item.path)),",
            "        ):",
            "            if item1 and item2 and item1.path == item2.path:",
            "                yield compare_items(item1.path, item1, item2)",
            "                continue",
            "            if item1:",
            "                matching_orphan = orphans_archive2.pop(item1.path, None)",
            "                if matching_orphan:",
            "                    yield compare_items(item1.path, item1, matching_orphan)",
            "                else:",
            "                    orphans_archive1[item1.path] = item1",
            "            if item2:",
            "                matching_orphan = orphans_archive1.pop(item2.path, None)",
            "                if matching_orphan:",
            "                    yield compare_items(matching_orphan.path, matching_orphan, item2)",
            "                else:",
            "                    orphans_archive2[item2.path] = item2",
            "        # At this point orphans_* contain items that had no matching partner in the other archive",
            "        for added in orphans_archive2.values():",
            "            path = added.path",
            "            deleted_item = Item.create_deleted(path)",
            "            yield compare_items(path, deleted_item, added)",
            "        for deleted in orphans_archive1.values():",
            "            path = deleted.path",
            "            deleted_item = Item.create_deleted(path)",
            "            yield compare_items(path, deleted, deleted_item)",
            "",
            "",
            "class MetadataCollector:",
            "    def __init__(self, *, noatime, noctime, nobirthtime, numeric_ids, noflags, noacls, noxattrs):",
            "        self.noatime = noatime",
            "        self.noctime = noctime",
            "        self.numeric_ids = numeric_ids",
            "        self.noflags = noflags",
            "        self.noacls = noacls",
            "        self.noxattrs = noxattrs",
            "        self.nobirthtime = nobirthtime",
            "",
            "    def stat_simple_attrs(self, st):",
            "        attrs = {}",
            "        attrs[\"mode\"] = st.st_mode",
            "        # borg can work with archives only having mtime (very old borg archives do not have",
            "        # atime/ctime). it can be useful to omit atime/ctime, if they change without the",
            "        # file content changing - e.g. to get better metadata deduplication.",
            "        attrs[\"mtime\"] = safe_ns(st.st_mtime_ns)",
            "        if not self.noatime:",
            "            attrs[\"atime\"] = safe_ns(st.st_atime_ns)",
            "        if not self.noctime:",
            "            attrs[\"ctime\"] = safe_ns(st.st_ctime_ns)",
            "        if not self.nobirthtime and hasattr(st, \"st_birthtime\"):",
            "            # sadly, there's no stat_result.st_birthtime_ns",
            "            attrs[\"birthtime\"] = safe_ns(int(st.st_birthtime * 10**9))",
            "        attrs[\"uid\"] = st.st_uid",
            "        attrs[\"gid\"] = st.st_gid",
            "        if not self.numeric_ids:",
            "            user = uid2user(st.st_uid)",
            "            if user is not None:",
            "                attrs[\"user\"] = user",
            "            group = gid2group(st.st_gid)",
            "            if group is not None:",
            "                attrs[\"group\"] = group",
            "        return attrs",
            "",
            "    def stat_ext_attrs(self, st, path, fd=None):",
            "        attrs = {}",
            "        if not self.noflags:",
            "            with backup_io(\"extended stat (flags)\"):",
            "                flags = get_flags(path, st, fd=fd)",
            "            attrs[\"bsdflags\"] = flags",
            "        if not self.noxattrs:",
            "            with backup_io(\"extended stat (xattrs)\"):",
            "                xattrs = xattr.get_all(fd or path, follow_symlinks=False)",
            "            attrs[\"xattrs\"] = StableDict(xattrs)",
            "        if not self.noacls:",
            "            with backup_io(\"extended stat (ACLs)\"):",
            "                acl_get(path, attrs, st, self.numeric_ids, fd=fd)",
            "        return attrs",
            "",
            "    def stat_attrs(self, st, path, fd=None):",
            "        attrs = self.stat_simple_attrs(st)",
            "        attrs.update(self.stat_ext_attrs(st, path, fd=fd))",
            "        return attrs",
            "",
            "",
            "# remember a few recently used all-zero chunk hashes in this mapping.",
            "# (hash_func, chunk_length) -> chunk_hash",
            "# we play safe and have the hash_func in the mapping key, in case we",
            "# have different hash_funcs within the same borg run.",
            "zero_chunk_ids = LRUCache(10)  # type: ignore[var-annotated]",
            "",
            "",
            "def cached_hash(chunk, id_hash):",
            "    allocation = chunk.meta[\"allocation\"]",
            "    if allocation == CH_DATA:",
            "        data = chunk.data",
            "        chunk_id = id_hash(data)",
            "    elif allocation in (CH_HOLE, CH_ALLOC):",
            "        size = chunk.meta[\"size\"]",
            "        assert size <= len(zeros)",
            "        data = memoryview(zeros)[:size]",
            "        try:",
            "            chunk_id = zero_chunk_ids[(id_hash, size)]",
            "        except KeyError:",
            "            chunk_id = id_hash(data)",
            "            zero_chunk_ids[(id_hash, size)] = chunk_id",
            "    else:",
            "        raise ValueError(\"unexpected allocation type\")",
            "    return chunk_id, data",
            "",
            "",
            "class ChunksProcessor:",
            "    # Processes an iterator of chunks for an Item",
            "",
            "    def __init__(",
            "        self,",
            "        *,",
            "        key,",
            "        cache,",
            "        add_item,",
            "        prepare_checkpoint,",
            "        write_checkpoint,",
            "        checkpoint_interval,",
            "        checkpoint_volume,",
            "        rechunkify,",
            "    ):",
            "        self.key = key",
            "        self.cache = cache",
            "        self.add_item = add_item",
            "        self.prepare_checkpoint = prepare_checkpoint",
            "        self.write_checkpoint = write_checkpoint",
            "        self.rechunkify = rechunkify",
            "        # time interval based checkpointing",
            "        self.checkpoint_interval = checkpoint_interval",
            "        self.last_checkpoint = time.monotonic()",
            "        # file content volume based checkpointing",
            "        self.checkpoint_volume = checkpoint_volume",
            "        self.current_volume = 0",
            "        self.last_volume_checkpoint = 0",
            "",
            "    def write_part_file(self, item):",
            "        self.prepare_checkpoint()",
            "        item = Item(internal_dict=item.as_dict())",
            "        # for borg recreate, we already have a size member in the source item (giving the total file size),",
            "        # but we consider only a part of the file here, thus we must recompute the size from the chunks:",
            "        item.get_size(memorize=True, from_chunks=True)",
            "        item.path += \".borg_part\"",
            "        self.add_item(item, show_progress=False)",
            "        self.write_checkpoint()",
            "",
            "    def maybe_checkpoint(self, item):",
            "        checkpoint_done = False",
            "        sig_int_triggered = sig_int and sig_int.action_triggered()",
            "        if (",
            "            sig_int_triggered",
            "            or (self.checkpoint_interval and time.monotonic() - self.last_checkpoint > self.checkpoint_interval)",
            "            or (self.checkpoint_volume and self.current_volume - self.last_volume_checkpoint >= self.checkpoint_volume)",
            "        ):",
            "            if sig_int_triggered:",
            "                logger.info(\"checkpoint requested: starting checkpoint creation...\")",
            "            self.write_part_file(item)",
            "            checkpoint_done = True",
            "            self.last_checkpoint = time.monotonic()",
            "            self.last_volume_checkpoint = self.current_volume",
            "            if sig_int_triggered:",
            "                sig_int.action_completed()",
            "                logger.info(\"checkpoint requested: finished checkpoint creation!\")",
            "        return checkpoint_done  # whether a checkpoint archive was created",
            "",
            "    def process_file_chunks(self, item, cache, stats, show_progress, chunk_iter, chunk_processor=None):",
            "        if not chunk_processor:",
            "",
            "            def chunk_processor(chunk):",
            "                started_hashing = time.monotonic()",
            "                chunk_id, data = cached_hash(chunk, self.key.id_hash)",
            "                stats.hashing_time += time.monotonic() - started_hashing",
            "                chunk_entry = cache.add_chunk(chunk_id, {}, data, stats=stats, wait=False)",
            "                self.cache.repository.async_response(wait=False)",
            "                return chunk_entry",
            "",
            "        item.chunks = []",
            "        # if we rechunkify, we'll get a fundamentally different chunks list, thus we need",
            "        # to get rid of .chunks_healthy, as it might not correspond to .chunks any more.",
            "        if self.rechunkify and \"chunks_healthy\" in item:",
            "            del item.chunks_healthy",
            "        for chunk in chunk_iter:",
            "            chunk_entry = chunk_processor(chunk)",
            "            item.chunks.append(chunk_entry)",
            "            self.current_volume += chunk_entry[1]",
            "            if show_progress:",
            "                stats.show_progress(item=item, dt=0.2)",
            "            self.maybe_checkpoint(item)",
            "",
            "",
            "class FilesystemObjectProcessors:",
            "    # When ported to threading, then this doesn't need chunker, cache, key any more.",
            "    # write_checkpoint should then be in the item buffer,",
            "    # and process_file becomes a callback passed to __init__.",
            "",
            "    def __init__(",
            "        self,",
            "        *,",
            "        metadata_collector,",
            "        cache,",
            "        key,",
            "        add_item,",
            "        process_file_chunks,",
            "        chunker_params,",
            "        show_progress,",
            "        sparse,",
            "        log_json,",
            "        iec,",
            "        file_status_printer=None,",
            "    ):",
            "        self.metadata_collector = metadata_collector",
            "        self.cache = cache",
            "        self.key = key",
            "        self.add_item = add_item",
            "        self.process_file_chunks = process_file_chunks",
            "        self.show_progress = show_progress",
            "        self.print_file_status = file_status_printer or (lambda *args: None)",
            "",
            "        self.hlm = HardLinkManager(id_type=tuple, info_type=(list, type(None)))  # (dev, ino) -> chunks or None",
            "        self.stats = Statistics(output_json=log_json, iec=iec)  # threading: done by cache (including progress)",
            "        self.cwd = os.getcwd()",
            "        self.chunker = get_chunker(*chunker_params, seed=key.chunk_seed, sparse=sparse)",
            "",
            "    @contextmanager",
            "    def create_helper(self, path, st, status=None, hardlinkable=True):",
            "        sanitized_path = remove_dotdot_prefixes(path)",
            "        item = Item(path=sanitized_path)",
            "        hardlinked = hardlinkable and st.st_nlink > 1",
            "        hl_chunks = None",
            "        update_map = False",
            "        if hardlinked:",
            "            status = \"h\"  # hardlink",
            "            nothing = object()",
            "            chunks = self.hlm.retrieve(id=(st.st_ino, st.st_dev), default=nothing)",
            "            if chunks is nothing:",
            "                update_map = True",
            "            elif chunks is not None:",
            "                hl_chunks = chunks",
            "            item.hlid = self.hlm.hardlink_id_from_inode(ino=st.st_ino, dev=st.st_dev)",
            "        yield item, status, hardlinked, hl_chunks",
            "        self.add_item(item, stats=self.stats)",
            "        if update_map:",
            "            # remember the hlid of this fs object and if the item has chunks,",
            "            # also remember them, so we do not have to re-chunk a hardlink.",
            "            chunks = item.chunks if \"chunks\" in item else None",
            "            self.hlm.remember(id=(st.st_ino, st.st_dev), info=chunks)",
            "",
            "    def process_dir_with_fd(self, *, path, fd, st):",
            "        with self.create_helper(path, st, \"d\", hardlinkable=False) as (item, status, hardlinked, hl_chunks):",
            "            item.update(self.metadata_collector.stat_attrs(st, path, fd=fd))",
            "            return status",
            "",
            "    def process_dir(self, *, path, parent_fd, name, st):",
            "        with self.create_helper(path, st, \"d\", hardlinkable=False) as (item, status, hardlinked, hl_chunks):",
            "            with OsOpen(path=path, parent_fd=parent_fd, name=name, flags=flags_dir, noatime=True, op=\"dir_open\") as fd:",
            "                # fd is None for directories on windows, in that case a race condition check is not possible.",
            "                if fd is not None:",
            "                    with backup_io(\"fstat\"):",
            "                        st = stat_update_check(st, os.fstat(fd))",
            "                item.update(self.metadata_collector.stat_attrs(st, path, fd=fd))",
            "                return status",
            "",
            "    def process_fifo(self, *, path, parent_fd, name, st):",
            "        with self.create_helper(path, st, \"f\") as (item, status, hardlinked, hl_chunks):  # fifo",
            "            with OsOpen(path=path, parent_fd=parent_fd, name=name, flags=flags_normal, noatime=True) as fd:",
            "                with backup_io(\"fstat\"):",
            "                    st = stat_update_check(st, os.fstat(fd))",
            "                item.update(self.metadata_collector.stat_attrs(st, path, fd=fd))",
            "                return status",
            "",
            "    def process_dev(self, *, path, parent_fd, name, st, dev_type):",
            "        with self.create_helper(path, st, dev_type) as (item, status, hardlinked, hl_chunks):  # char/block device",
            "            # looks like we can not work fd-based here without causing issues when trying to open/close the device",
            "            with backup_io(\"stat\"):",
            "                st = stat_update_check(st, os_stat(path=path, parent_fd=parent_fd, name=name, follow_symlinks=False))",
            "            item.rdev = st.st_rdev",
            "            item.update(self.metadata_collector.stat_attrs(st, path))",
            "            return status",
            "",
            "    def process_symlink(self, *, path, parent_fd, name, st):",
            "        with self.create_helper(path, st, \"s\", hardlinkable=True) as (item, status, hardlinked, hl_chunks):",
            "            fname = name if name is not None and parent_fd is not None else path",
            "            with backup_io(\"readlink\"):",
            "                target = os.readlink(fname, dir_fd=parent_fd)",
            "            item.target = target",
            "            item.update(self.metadata_collector.stat_attrs(st, path))  # can't use FD here?",
            "            return status",
            "",
            "    def process_pipe(self, *, path, cache, fd, mode, user=None, group=None):",
            "        status = \"i\"  # stdin (or other pipe)",
            "        self.print_file_status(status, path)",
            "        status = None  # we already printed the status",
            "        if user is not None:",
            "            uid = user2uid(user)",
            "            if uid is None:",
            "                raise Error(\"no such user: %s\" % user)",
            "        else:",
            "            uid = None",
            "        if group is not None:",
            "            gid = group2gid(group)",
            "            if gid is None:",
            "                raise Error(\"no such group: %s\" % group)",
            "        else:",
            "            gid = None",
            "        t = int(time.time()) * 1000000000",
            "        item = Item(path=path, mode=mode & 0o107777 | 0o100000, mtime=t, atime=t, ctime=t)  # forcing regular file mode",
            "        if user is not None:",
            "            item.user = user",
            "        if group is not None:",
            "            item.group = group",
            "        if uid is not None:",
            "            item.uid = uid",
            "        if gid is not None:",
            "            item.gid = gid",
            "        try:",
            "            self.process_file_chunks(",
            "                item, cache, self.stats, self.show_progress, backup_io_iter(self.chunker.chunkify(fd))",
            "            )",
            "        except BackupOSError:",
            "            # see comments in process_file's exception handler, same issue here.",
            "            for chunk in item.get(\"chunks\", []):",
            "                cache.chunk_decref(chunk.id, self.stats, wait=False)",
            "            raise",
            "        else:",
            "            item.get_size(memorize=True)",
            "            self.stats.nfiles += 1",
            "            self.add_item(item, stats=self.stats)",
            "            return status",
            "",
            "    def process_file(self, *, path, parent_fd, name, st, cache, flags=flags_normal, last_try=False):",
            "        with self.create_helper(path, st, None) as (item, status, hardlinked, hl_chunks):  # no status yet",
            "            with OsOpen(path=path, parent_fd=parent_fd, name=name, flags=flags, noatime=True) as fd:",
            "                with backup_io(\"fstat\"):",
            "                    st = stat_update_check(st, os.fstat(fd))",
            "                item.update(self.metadata_collector.stat_simple_attrs(st))",
            "                is_special_file = is_special(st.st_mode)",
            "                if is_special_file:",
            "                    # we process a special file like a regular file. reflect that in mode,",
            "                    # so it can be extracted / accessed in FUSE mount like a regular file.",
            "                    # this needs to be done early, so that part files also get the patched mode.",
            "                    item.mode = stat.S_IFREG | stat.S_IMODE(item.mode)",
            "                # we begin processing chunks now (writing or incref'ing them to the repository),",
            "                # which might require cleanup (see except-branch):",
            "                try:",
            "                    if hl_chunks is not None:  # create_helper gave us chunks from a previous hardlink",
            "                        item.chunks = []",
            "                        for chunk_id, chunk_size in hl_chunks:",
            "                            # process one-by-one, so we will know in item.chunks how far we got",
            "                            chunk_entry = cache.chunk_incref(chunk_id, self.stats)",
            "                            item.chunks.append(chunk_entry)",
            "                    else:  # normal case, no \"2nd+\" hardlink",
            "                        if not is_special_file:",
            "                            hashed_path = safe_encode(os.path.join(self.cwd, path))",
            "                            started_hashing = time.monotonic()",
            "                            path_hash = self.key.id_hash(hashed_path)",
            "                            self.stats.hashing_time += time.monotonic() - started_hashing",
            "                            known, ids = cache.file_known_and_unchanged(hashed_path, path_hash, st)",
            "                        else:",
            "                            # in --read-special mode, we may be called for special files.",
            "                            # there should be no information in the cache about special files processed in",
            "                            # read-special mode, but we better play safe as this was wrong in the past:",
            "                            hashed_path = path_hash = None",
            "                            known, ids = False, None",
            "                        if ids is not None:",
            "                            # Make sure all ids are available",
            "                            for id_ in ids:",
            "                                if not cache.seen_chunk(id_):",
            "                                    # cache said it is unmodified, but we lost a chunk: process file like modified",
            "                                    status = \"M\"",
            "                                    break",
            "                            else:",
            "                                item.chunks = []",
            "                                for chunk_id in ids:",
            "                                    # process one-by-one, so we will know in item.chunks how far we got",
            "                                    chunk_entry = cache.chunk_incref(chunk_id, self.stats)",
            "                                    item.chunks.append(chunk_entry)",
            "                                status = \"U\"  # regular file, unchanged",
            "                        else:",
            "                            status = \"M\" if known else \"A\"  # regular file, modified or added",
            "                        self.print_file_status(status, path)",
            "                        # Only chunkify the file if needed",
            "                        changed_while_backup = False",
            "                        if \"chunks\" not in item:",
            "                            with backup_io(\"read\"):",
            "                                self.process_file_chunks(",
            "                                    item,",
            "                                    cache,",
            "                                    self.stats,",
            "                                    self.show_progress,",
            "                                    backup_io_iter(self.chunker.chunkify(None, fd)),",
            "                                )",
            "                                self.stats.chunking_time = self.chunker.chunking_time",
            "                            if not is_win32:  # TODO for win32",
            "                                with backup_io(\"fstat2\"):",
            "                                    st2 = os.fstat(fd)",
            "                                # special files:",
            "                                # - fifos change naturally, because they are fed from the other side. no problem.",
            "                                # - blk/chr devices don't change ctime anyway.",
            "                                changed_while_backup = not is_special_file and st.st_ctime_ns != st2.st_ctime_ns",
            "                            if changed_while_backup:",
            "                                # regular file changed while we backed it up, might be inconsistent/corrupt!",
            "                                if last_try:",
            "                                    status = \"C\"  # crap! retries did not help.",
            "                                else:",
            "                                    raise BackupError(\"file changed while we read it!\")",
            "                            if not is_special_file and not changed_while_backup:",
            "                                # we must not memorize special files, because the contents of e.g. a",
            "                                # block or char device will change without its mtime/size/inode changing.",
            "                                # also, we must not memorize a potentially inconsistent/corrupt file that",
            "                                # changed while we backed it up.",
            "                                cache.memorize_file(hashed_path, path_hash, st, [c.id for c in item.chunks])",
            "                        self.stats.files_stats[status] += 1  # must be done late",
            "                        if not changed_while_backup:",
            "                            status = None  # we already called print_file_status",
            "                    self.stats.nfiles += 1",
            "                    item.update(self.metadata_collector.stat_ext_attrs(st, path, fd=fd))",
            "                    item.get_size(memorize=True)",
            "                    return status",
            "                except BackupOSError:",
            "                    # Something went wrong and we might need to clean up a bit.",
            "                    # Maybe we have already incref'ed some file content chunks in the repo -",
            "                    # but we will not add an item (see add_item in create_helper) and thus",
            "                    # they would be orphaned chunks in case that we commit the transaction.",
            "                    for chunk in item.get(\"chunks\", []):",
            "                        cache.chunk_decref(chunk.id, self.stats, wait=False)",
            "                    # Now that we have cleaned up the chunk references, we can re-raise the exception.",
            "                    # This will skip processing of this file, but might retry or continue with the next one.",
            "                    raise",
            "",
            "",
            "class TarfileObjectProcessors:",
            "    def __init__(",
            "        self,",
            "        *,",
            "        cache,",
            "        key,",
            "        add_item,",
            "        process_file_chunks,",
            "        chunker_params,",
            "        show_progress,",
            "        log_json,",
            "        iec,",
            "        file_status_printer=None,",
            "    ):",
            "        self.cache = cache",
            "        self.key = key",
            "        self.add_item = add_item",
            "        self.process_file_chunks = process_file_chunks",
            "        self.show_progress = show_progress",
            "        self.print_file_status = file_status_printer or (lambda *args: None)",
            "",
            "        self.stats = Statistics(output_json=log_json, iec=iec)  # threading: done by cache (including progress)",
            "        self.chunker = get_chunker(*chunker_params, seed=key.chunk_seed, sparse=False)",
            "        self.hlm = HardLinkManager(id_type=str, info_type=list)  # path -> chunks",
            "",
            "    @contextmanager",
            "    def create_helper(self, tarinfo, status=None, type=None):",
            "        ph = tarinfo.pax_headers",
            "        if ph and \"BORG.item.version\" in ph:",
            "            assert ph[\"BORG.item.version\"] == \"1\"",
            "            meta_bin = base64.b64decode(ph[\"BORG.item.meta\"])",
            "            meta_dict = msgpack.unpackb(meta_bin, object_hook=StableDict)",
            "            item = Item(internal_dict=meta_dict)",
            "        else:",
            "",
            "            def s_to_ns(s):",
            "                return safe_ns(int(float(s) * 1e9))",
            "",
            "            item = Item(",
            "                path=make_path_safe(tarinfo.name),",
            "                mode=tarinfo.mode | type,",
            "                uid=tarinfo.uid,",
            "                gid=tarinfo.gid,",
            "                mtime=s_to_ns(tarinfo.mtime),",
            "            )",
            "            if tarinfo.uname:",
            "                item.user = tarinfo.uname",
            "            if tarinfo.gname:",
            "                item.group = tarinfo.gname",
            "            if ph:",
            "                # note: for mtime this is a bit redundant as it is already done by tarfile module,",
            "                #       but we just do it in our way to be consistent for sure.",
            "                for name in \"atime\", \"ctime\", \"mtime\":",
            "                    if name in ph:",
            "                        ns = s_to_ns(ph[name])",
            "                        setattr(item, name, ns)",
            "        yield item, status",
            "        # if we get here, \"with\"-block worked ok without error/exception, the item was processed ok...",
            "        self.add_item(item, stats=self.stats)",
            "",
            "    def process_dir(self, *, tarinfo, status, type):",
            "        with self.create_helper(tarinfo, status, type) as (item, status):",
            "            return status",
            "",
            "    def process_fifo(self, *, tarinfo, status, type):",
            "        with self.create_helper(tarinfo, status, type) as (item, status):",
            "            return status",
            "",
            "    def process_dev(self, *, tarinfo, status, type):",
            "        with self.create_helper(tarinfo, status, type) as (item, status):",
            "            item.rdev = os.makedev(tarinfo.devmajor, tarinfo.devminor)",
            "            return status",
            "",
            "    def process_symlink(self, *, tarinfo, status, type):",
            "        with self.create_helper(tarinfo, status, type) as (item, status):",
            "            item.target = tarinfo.linkname",
            "            return status",
            "",
            "    def process_hardlink(self, *, tarinfo, status, type):",
            "        with self.create_helper(tarinfo, status, type) as (item, status):",
            "            # create a not hardlinked borg item, reusing the chunks, see HardLinkManager.__doc__",
            "            chunks = self.hlm.retrieve(tarinfo.linkname)",
            "            if chunks is not None:",
            "                item.chunks = chunks",
            "            item.get_size(memorize=True, from_chunks=True)",
            "            self.stats.nfiles += 1",
            "            return status",
            "",
            "    def process_file(self, *, tarinfo, status, type, tar):",
            "        with self.create_helper(tarinfo, status, type) as (item, status):",
            "            self.print_file_status(status, tarinfo.name)",
            "            status = None  # we already printed the status",
            "            try:",
            "                fd = tar.extractfile(tarinfo)",
            "                self.process_file_chunks(",
            "                    item, self.cache, self.stats, self.show_progress, backup_io_iter(self.chunker.chunkify(fd))",
            "                )",
            "                item.get_size(memorize=True, from_chunks=True)",
            "                self.stats.nfiles += 1",
            "                # we need to remember ALL files, see HardLinkManager.__doc__",
            "                self.hlm.remember(id=tarinfo.name, info=item.chunks)",
            "                return status",
            "            except BackupOSError:",
            "                # see comment in FilesystemObjectProcessors.process_file, same issue here.",
            "                for chunk in item.get(\"chunks\", []):",
            "                    self.cache.chunk_decref(chunk.id, self.stats, wait=False)",
            "                raise",
            "",
            "",
            "def valid_msgpacked_dict(d, keys_serialized):",
            "    \"\"\"check if the data <d> looks like a msgpacked dict\"\"\"",
            "    d_len = len(d)",
            "    if d_len == 0:",
            "        return False",
            "    if d[0] & 0xF0 == 0x80:  # object is a fixmap (up to 15 elements)",
            "        offs = 1",
            "    elif d[0] == 0xDE:  # object is a map16 (up to 2^16-1 elements)",
            "        offs = 3",
            "    else:",
            "        # object is not a map (dict)",
            "        # note: we must not have dicts with > 2^16-1 elements",
            "        return False",
            "    if d_len <= offs:",
            "        return False",
            "    # is the first dict key a bytestring?",
            "    if d[offs] & 0xE0 == 0xA0:  # key is a small bytestring (up to 31 chars)",
            "        pass",
            "    elif d[offs] in (0xD9, 0xDA, 0xDB):  # key is a str8, str16 or str32",
            "        pass",
            "    else:",
            "        # key is not a bytestring",
            "        return False",
            "    # is the bytestring any of the expected key names?",
            "    key_serialized = d[offs:]",
            "    return any(key_serialized.startswith(pattern) for pattern in keys_serialized)",
            "",
            "",
            "class RobustUnpacker:",
            "    \"\"\"A restartable/robust version of the streaming msgpack unpacker\"\"\"",
            "",
            "    def __init__(self, validator, item_keys):",
            "        super().__init__()",
            "        self.item_keys = [msgpack.packb(name) for name in item_keys]",
            "        self.validator = validator",
            "        self._buffered_data = []",
            "        self._resync = False",
            "        self._unpacker = msgpack.Unpacker(object_hook=StableDict)",
            "",
            "    def resync(self):",
            "        self._buffered_data = []",
            "        self._resync = True",
            "",
            "    def feed(self, data):",
            "        if self._resync:",
            "            self._buffered_data.append(data)",
            "        else:",
            "            self._unpacker.feed(data)",
            "",
            "    def __iter__(self):",
            "        return self",
            "",
            "    def __next__(self):",
            "        if self._resync:",
            "            data = b\"\".join(self._buffered_data)",
            "            while self._resync:",
            "                if not data:",
            "                    raise StopIteration",
            "                # Abort early if the data does not look like a serialized item dict",
            "                if not valid_msgpacked_dict(data, self.item_keys):",
            "                    data = data[1:]",
            "                    continue",
            "                self._unpacker = msgpack.Unpacker(object_hook=StableDict)",
            "                self._unpacker.feed(data)",
            "                try:",
            "                    item = next(self._unpacker)",
            "                except (msgpack.UnpackException, StopIteration):",
            "                    # as long as we are resyncing, we also ignore StopIteration",
            "                    pass",
            "                else:",
            "                    if self.validator(item):",
            "                        self._resync = False",
            "                        return item",
            "                data = data[1:]",
            "        else:",
            "            return next(self._unpacker)",
            "",
            "",
            "class ArchiveChecker:",
            "    def __init__(self):",
            "        self.error_found = False",
            "        self.possibly_superseded = set()",
            "",
            "    def check(",
            "        self,",
            "        repository,",
            "        *,",
            "        verify_data=False,",
            "        repair=False,",
            "        match=None,",
            "        sort_by=\"\",",
            "        first=0,",
            "        last=0,",
            "        older=None,",
            "        newer=None,",
            "        oldest=None,",
            "        newest=None,",
            "    ):",
            "        \"\"\"Perform a set of checks on 'repository'",
            "",
            "        :param repair: enable repair mode, write updated or corrected data into repository",
            "        :param first/last/sort_by: only check this number of first/last archives ordered by sort_by",
            "        :param match: only check archives matching this pattern",
            "        :param older/newer: only check archives older/newer than timedelta from now",
            "        :param oldest/newest: only check archives older/newer than timedelta from oldest/newest archive timestamp",
            "        :param verify_data: integrity verification of data referenced by archives",
            "        \"\"\"",
            "        logger.info(\"Starting archive consistency check...\")",
            "        self.check_all = not any((first, last, match, older, newer, oldest, newest))",
            "        self.repair = repair",
            "        self.repository = repository",
            "        self.init_chunks()",
            "        if not self.chunks:",
            "            logger.error(\"Repository contains no apparent data at all, cannot continue check/repair.\")",
            "            return False",
            "        self.key = self.make_key(repository)",
            "        self.repo_objs = RepoObj(self.key)",
            "        if verify_data:",
            "            self.verify_data()",
            "        if Manifest.MANIFEST_ID not in self.chunks:",
            "            logger.error(\"Repository manifest not found!\")",
            "            self.error_found = True",
            "            self.manifest = self.rebuild_manifest()",
            "        else:",
            "            try:",
            "                self.manifest = Manifest.load(repository, (Manifest.Operation.CHECK,), key=self.key)",
            "            except IntegrityErrorBase as exc:",
            "                logger.error(\"Repository manifest is corrupted: %s\", exc)",
            "                self.error_found = True",
            "                del self.chunks[Manifest.MANIFEST_ID]",
            "                self.manifest = self.rebuild_manifest()",
            "        self.rebuild_refcounts(",
            "            match=match, first=first, last=last, sort_by=sort_by, older=older, oldest=oldest, newer=newer, newest=newest",
            "        )",
            "        self.orphan_chunks_check()",
            "        self.finish()",
            "        if self.error_found:",
            "            logger.error(\"Archive consistency check complete, problems found.\")",
            "        else:",
            "            logger.info(\"Archive consistency check complete, no problems found.\")",
            "        return self.repair or not self.error_found",
            "",
            "    def init_chunks(self):",
            "        \"\"\"Fetch a list of all object keys from repository\"\"\"",
            "        # Explicitly set the initial usable hash table capacity to avoid performance issues",
            "        # due to hash table \"resonance\".",
            "        # Since reconstruction of archive items can add some new chunks, add 10 % headroom.",
            "        self.chunks = ChunkIndex(usable=len(self.repository) * 1.1)",
            "        marker = None",
            "        while True:",
            "            result = self.repository.list(limit=LIST_SCAN_LIMIT, marker=marker)",
            "            if not result:",
            "                break",
            "            marker = result[-1]",
            "            init_entry = ChunkIndexEntry(refcount=0, size=0)",
            "            for id_ in result:",
            "                self.chunks[id_] = init_entry",
            "",
            "    def make_key(self, repository):",
            "        attempt = 0",
            "        for chunkid, _ in self.chunks.iteritems():",
            "            attempt += 1",
            "            if attempt > 999:",
            "                # we did a lot of attempts, but could not create the key via key_factory, give up.",
            "                break",
            "            cdata = repository.get(chunkid)",
            "            try:",
            "                return key_factory(repository, cdata)",
            "            except UnsupportedPayloadError:",
            "                # we get here, if the cdata we got has a corrupted key type byte",
            "                pass  # ignore it, just try the next chunk",
            "        if attempt == 0:",
            "            msg = \"make_key: repository has no chunks at all!\"",
            "        else:",
            "            msg = \"make_key: failed to create the key (tried %d chunks)\" % attempt",
            "        raise IntegrityError(msg)",
            "",
            "    def verify_data(self):",
            "        logger.info(\"Starting cryptographic data integrity verification...\")",
            "        chunks_count_index = len(self.chunks)",
            "        chunks_count_segments = 0",
            "        errors = 0",
            "        defect_chunks = []",
            "        pi = ProgressIndicatorPercent(",
            "            total=chunks_count_index, msg=\"Verifying data %6.2f%%\", step=0.01, msgid=\"check.verify_data\"",
            "        )",
            "        state = None",
            "        while True:",
            "            chunk_ids, state = self.repository.scan(limit=100, state=state)",
            "            if not chunk_ids:",
            "                break",
            "            chunks_count_segments += len(chunk_ids)",
            "            chunk_data_iter = self.repository.get_many(chunk_ids)",
            "            chunk_ids_revd = list(reversed(chunk_ids))",
            "            while chunk_ids_revd:",
            "                pi.show()",
            "                chunk_id = chunk_ids_revd.pop(-1)  # better efficiency",
            "                try:",
            "                    encrypted_data = next(chunk_data_iter)",
            "                except (Repository.ObjectNotFound, IntegrityErrorBase) as err:",
            "                    self.error_found = True",
            "                    errors += 1",
            "                    logger.error(\"chunk %s: %s\", bin_to_hex(chunk_id), err)",
            "                    if isinstance(err, IntegrityErrorBase):",
            "                        defect_chunks.append(chunk_id)",
            "                    # as the exception killed our generator, make a new one for remaining chunks:",
            "                    if chunk_ids_revd:",
            "                        chunk_ids = list(reversed(chunk_ids_revd))",
            "                        chunk_data_iter = self.repository.get_many(chunk_ids)",
            "                else:",
            "                    try:",
            "                        # we must decompress, so it'll call assert_id() in there:",
            "                        self.repo_objs.parse(chunk_id, encrypted_data, decompress=True)",
            "                    except IntegrityErrorBase as integrity_error:",
            "                        self.error_found = True",
            "                        errors += 1",
            "                        logger.error(\"chunk %s, integrity error: %s\", bin_to_hex(chunk_id), integrity_error)",
            "                        defect_chunks.append(chunk_id)",
            "        pi.finish()",
            "        if chunks_count_index != chunks_count_segments:",
            "            logger.error(\"Repo/Chunks index object count vs. segment files object count mismatch.\")",
            "            logger.error(",
            "                \"Repo/Chunks index: %d objects != segment files: %d objects\", chunks_count_index, chunks_count_segments",
            "            )",
            "        if defect_chunks:",
            "            if self.repair:",
            "                # if we kill the defect chunk here, subsequent actions within this \"borg check\"",
            "                # run will find missing chunks and replace them with all-zero replacement",
            "                # chunks and flag the files as \"repaired\".",
            "                # if another backup is done later and the missing chunks get backed up again,",
            "                # a \"borg check\" afterwards can heal all files where this chunk was missing.",
            "                logger.warning(",
            "                    \"Found defect chunks. They will be deleted now, so affected files can \"",
            "                    \"get repaired now and maybe healed later.\"",
            "                )",
            "                for defect_chunk in defect_chunks:",
            "                    # remote repo (ssh): retry might help for strange network / NIC / RAM errors",
            "                    # as the chunk will be retransmitted from remote server.",
            "                    # local repo (fs): as chunks.iteritems loop usually pumps a lot of data through,",
            "                    # a defect chunk is likely not in the fs cache any more and really gets re-read",
            "                    # from the underlying media.",
            "                    try:",
            "                        encrypted_data = self.repository.get(defect_chunk)",
            "                        # we must decompress, so it'll call assert_id() in there:",
            "                        self.repo_objs.parse(defect_chunk, encrypted_data, decompress=True)",
            "                    except IntegrityErrorBase:",
            "                        # failed twice -> get rid of this chunk",
            "                        del self.chunks[defect_chunk]",
            "                        self.repository.delete(defect_chunk)",
            "                        logger.debug(\"chunk %s deleted.\", bin_to_hex(defect_chunk))",
            "                    else:",
            "                        logger.warning(\"chunk %s not deleted, did not consistently fail.\", bin_to_hex(defect_chunk))",
            "            else:",
            "                logger.warning(",
            "                    \"Found defect chunks. With --repair, they would get deleted, so affected \"",
            "                    \"files could get repaired then and maybe healed later.\"",
            "                )",
            "                for defect_chunk in defect_chunks:",
            "                    logger.debug(\"chunk %s is defect.\", bin_to_hex(defect_chunk))",
            "        log = logger.error if errors else logger.info",
            "        log(",
            "            \"Finished cryptographic data integrity verification, verified %d chunks with %d integrity errors.\",",
            "            chunks_count_segments,",
            "            errors,",
            "        )",
            "",
            "    def rebuild_manifest(self):",
            "        \"\"\"Rebuild the manifest object if it is missing",
            "",
            "        Iterates through all objects in the repository looking for archive metadata blocks.",
            "        \"\"\"",
            "",
            "        def valid_archive(obj):",
            "            if not isinstance(obj, dict):",
            "                return False",
            "            return REQUIRED_ARCHIVE_KEYS.issubset(obj)",
            "",
            "        logger.info(\"Rebuilding missing manifest, this might take some time...\")",
            "        # as we have lost the manifest, we do not know any more what valid item keys we had.",
            "        # collecting any key we encounter in a damaged repo seems unwise, thus we just use",
            "        # the hardcoded list from the source code. thus, it is not recommended to rebuild a",
            "        # lost manifest on a older borg version than the most recent one that was ever used",
            "        # within this repository (assuming that newer borg versions support more item keys).",
            "        manifest = Manifest(self.key, self.repository)",
            "        archive_keys_serialized = [msgpack.packb(name) for name in ARCHIVE_KEYS]",
            "        pi = ProgressIndicatorPercent(",
            "            total=len(self.chunks), msg=\"Rebuilding manifest %6.2f%%\", step=0.01, msgid=\"check.rebuild_manifest\"",
            "        )",
            "        for chunk_id, _ in self.chunks.iteritems():",
            "            pi.show()",
            "            cdata = self.repository.get(chunk_id)",
            "            try:",
            "                _, data = self.repo_objs.parse(chunk_id, cdata)",
            "            except IntegrityErrorBase as exc:",
            "                logger.error(\"Skipping corrupted chunk: %s\", exc)",
            "                self.error_found = True",
            "                continue",
            "            if not valid_msgpacked_dict(data, archive_keys_serialized):",
            "                continue",
            "            if b\"command_line\" not in data or b\"\\xa7version\\x02\" not in data:",
            "                continue",
            "            try:",
            "                archive = msgpack.unpackb(data)",
            "            # Ignore exceptions that might be raised when feeding msgpack with invalid data",
            "            except msgpack.UnpackException:",
            "                continue",
            "            if valid_archive(archive):",
            "                # **after** doing the low-level checks and having a strong indication that we",
            "                # are likely looking at an archive item here, also check the TAM authentication:",
            "                try:",
            "                    archive, verified, _ = self.key.unpack_and_verify_archive(data, force_tam_not_required=False)",
            "                except IntegrityError:",
            "                    # TAM issues - do not accept this archive!",
            "                    # either somebody is trying to attack us with a fake archive data or",
            "                    # we have an ancient archive made before TAM was a thing (borg < 1.0.9) **and** this repo",
            "                    # was not correctly upgraded to borg 1.2.5 (see advisory at top of the changelog).",
            "                    # borg can't tell the difference, so it has to assume this archive might be an attack",
            "                    # and drops this archive.",
            "                    continue",
            "                # note: if we get here and verified is False, a TAM is not required.",
            "                archive = ArchiveItem(internal_dict=archive)",
            "                name = archive.name",
            "                logger.info(\"Found archive %s\", name)",
            "                if name in manifest.archives:",
            "                    i = 1",
            "                    while True:",
            "                        new_name = \"%s.%d\" % (name, i)",
            "                        if new_name not in manifest.archives:",
            "                            break",
            "                        i += 1",
            "                    logger.warning(\"Duplicate archive name %s, storing as %s\", name, new_name)",
            "                    name = new_name",
            "                manifest.archives[name] = (chunk_id, archive.time)",
            "        pi.finish()",
            "        logger.info(\"Manifest rebuild complete.\")",
            "        return manifest",
            "",
            "    def rebuild_refcounts(",
            "        self, first=0, last=0, sort_by=\"\", match=None, older=None, newer=None, oldest=None, newest=None",
            "    ):",
            "        \"\"\"Rebuild object reference counts by walking the metadata",
            "",
            "        Missing and/or incorrect data is repaired when detected",
            "        \"\"\"",
            "        # Exclude the manifest from chunks (manifest entry might be already deleted from self.chunks)",
            "        self.chunks.pop(Manifest.MANIFEST_ID, None)",
            "",
            "        def mark_as_possibly_superseded(id_):",
            "            if self.chunks.get(id_, ChunkIndexEntry(0, 0)).refcount == 0:",
            "                self.possibly_superseded.add(id_)",
            "",
            "        def add_callback(chunk):",
            "            id_ = self.key.id_hash(chunk)",
            "            cdata = self.repo_objs.format(id_, {}, chunk)",
            "            add_reference(id_, len(chunk), cdata)",
            "            return id_",
            "",
            "        def add_reference(id_, size, cdata=None):",
            "            try:",
            "                self.chunks.incref(id_)",
            "            except KeyError:",
            "                assert cdata is not None",
            "                self.chunks[id_] = ChunkIndexEntry(refcount=1, size=size)",
            "                if self.repair:",
            "                    self.repository.put(id_, cdata)",
            "",
            "        def verify_file_chunks(archive_name, item):",
            "            \"\"\"Verifies that all file chunks are present.",
            "",
            "            Missing file chunks will be replaced with new chunks of the same length containing all zeros.",
            "            If a previously missing file chunk re-appears, the replacement chunk is replaced by the correct one.",
            "            \"\"\"",
            "",
            "            def replacement_chunk(size):",
            "                chunk = Chunk(None, allocation=CH_ALLOC, size=size)",
            "                chunk_id, data = cached_hash(chunk, self.key.id_hash)",
            "                cdata = self.repo_objs.format(chunk_id, {}, data)",
            "                return chunk_id, size, cdata",
            "",
            "            offset = 0",
            "            chunk_list = []",
            "            chunks_replaced = False",
            "            has_chunks_healthy = \"chunks_healthy\" in item",
            "            chunks_current = item.chunks",
            "            chunks_healthy = item.chunks_healthy if has_chunks_healthy else chunks_current",
            "            if has_chunks_healthy and len(chunks_current) != len(chunks_healthy):",
            "                # should never happen, but there was issue #3218.",
            "                logger.warning(f\"{archive_name}: {item.path}: Invalid chunks_healthy metadata removed!\")",
            "                del item.chunks_healthy",
            "                has_chunks_healthy = False",
            "                chunks_healthy = chunks_current",
            "            for chunk_current, chunk_healthy in zip(chunks_current, chunks_healthy):",
            "                chunk_id, size = chunk_healthy",
            "                if chunk_id not in self.chunks:",
            "                    # a chunk of the healthy list is missing",
            "                    if chunk_current == chunk_healthy:",
            "                        logger.error(",
            "                            \"{}: {}: New missing file chunk detected (Byte {}-{}, Chunk {}). \"",
            "                            \"Replacing with all-zero chunk.\".format(",
            "                                archive_name, item.path, offset, offset + size, bin_to_hex(chunk_id)",
            "                            )",
            "                        )",
            "                        self.error_found = chunks_replaced = True",
            "                        chunk_id, size, cdata = replacement_chunk(size)",
            "                        add_reference(chunk_id, size, cdata)",
            "                    else:",
            "                        logger.info(",
            "                            \"{}: {}: Previously missing file chunk is still missing (Byte {}-{}, Chunk {}). \"",
            "                            \"It has an all-zero replacement chunk already.\".format(",
            "                                archive_name, item.path, offset, offset + size, bin_to_hex(chunk_id)",
            "                            )",
            "                        )",
            "                        chunk_id, size = chunk_current",
            "                        if chunk_id in self.chunks:",
            "                            add_reference(chunk_id, size)",
            "                        else:",
            "                            logger.warning(",
            "                                \"{}: {}: Missing all-zero replacement chunk detected (Byte {}-{}, Chunk {}). \"",
            "                                \"Generating new replacement chunk.\".format(",
            "                                    archive_name, item.path, offset, offset + size, bin_to_hex(chunk_id)",
            "                                )",
            "                            )",
            "                            self.error_found = chunks_replaced = True",
            "                            chunk_id, size, cdata = replacement_chunk(size)",
            "                            add_reference(chunk_id, size, cdata)",
            "                else:",
            "                    if chunk_current == chunk_healthy:",
            "                        # normal case, all fine.",
            "                        add_reference(chunk_id, size)",
            "                    else:",
            "                        logger.info(",
            "                            \"{}: {}: Healed previously missing file chunk! (Byte {}-{}, Chunk {}).\".format(",
            "                                archive_name, item.path, offset, offset + size, bin_to_hex(chunk_id)",
            "                            )",
            "                        )",
            "                        add_reference(chunk_id, size)",
            "                        mark_as_possibly_superseded(chunk_current[0])  # maybe orphaned the all-zero replacement chunk",
            "                chunk_list.append([chunk_id, size])  # list-typed element as chunks_healthy is list-of-lists",
            "                offset += size",
            "            if chunks_replaced and not has_chunks_healthy:",
            "                # if this is first repair, remember the correct chunk IDs, so we can maybe heal the file later",
            "                item.chunks_healthy = item.chunks",
            "            if has_chunks_healthy and chunk_list == chunks_healthy:",
            "                logger.info(f\"{archive_name}: {item.path}: Completely healed previously damaged file!\")",
            "                del item.chunks_healthy",
            "            item.chunks = chunk_list",
            "            if \"size\" in item:",
            "                item_size = item.size",
            "                item_chunks_size = item.get_size(from_chunks=True)",
            "                if item_size != item_chunks_size:",
            "                    # just warn, but keep the inconsistency, so that borg extract can warn about it.",
            "                    logger.warning(",
            "                        \"{}: {}: size inconsistency detected: size {}, chunks size {}\".format(",
            "                            archive_name, item.path, item_size, item_chunks_size",
            "                        )",
            "                    )",
            "",
            "        def robust_iterator(archive):",
            "            \"\"\"Iterates through all archive items",
            "",
            "            Missing item chunks will be skipped and the msgpack stream will be restarted",
            "            \"\"\"",
            "            item_keys = self.manifest.item_keys",
            "            required_item_keys = REQUIRED_ITEM_KEYS",
            "            unpacker = RobustUnpacker(",
            "                lambda item: isinstance(item, StableDict) and \"path\" in item, self.manifest.item_keys",
            "            )",
            "            _state = 0",
            "",
            "            def missing_chunk_detector(chunk_id):",
            "                nonlocal _state",
            "                if _state % 2 != int(chunk_id not in self.chunks):",
            "                    _state += 1",
            "                return _state",
            "",
            "            def report(msg, chunk_id, chunk_no):",
            "                cid = bin_to_hex(chunk_id)",
            "                msg += \" [chunk: %06d_%s]\" % (chunk_no, cid)  # see \"debug dump-archive-items\"",
            "                self.error_found = True",
            "                logger.error(msg)",
            "",
            "            def list_keys_safe(keys):",
            "                return \", \".join(k.decode(errors=\"replace\") if isinstance(k, bytes) else str(k) for k in keys)",
            "",
            "            def valid_item(obj):",
            "                if not isinstance(obj, StableDict):",
            "                    return False, \"not a dictionary\"",
            "                keys = set(obj)",
            "                if not required_item_keys.issubset(keys):",
            "                    return False, \"missing required keys: \" + list_keys_safe(required_item_keys - keys)",
            "                if not keys.issubset(item_keys):",
            "                    return False, \"invalid keys: \" + list_keys_safe(keys - item_keys)",
            "                return True, \"\"",
            "",
            "            i = 0",
            "            archive_items = archive_get_items(archive, repo_objs=self.repo_objs, repository=repository)",
            "            for state, items in groupby(archive_items, missing_chunk_detector):",
            "                items = list(items)",
            "                if state % 2:",
            "                    for chunk_id in items:",
            "                        report(\"item metadata chunk missing\", chunk_id, i)",
            "                        i += 1",
            "                    continue",
            "                if state > 0:",
            "                    unpacker.resync()",
            "                for chunk_id, cdata in zip(items, repository.get_many(items)):",
            "                    try:",
            "                        _, data = self.repo_objs.parse(chunk_id, cdata)",
            "                        unpacker.feed(data)",
            "                        for item in unpacker:",
            "                            valid, reason = valid_item(item)",
            "                            if valid:",
            "                                yield Item(internal_dict=item)",
            "                            else:",
            "                                report(",
            "                                    \"Did not get expected metadata dict when unpacking item metadata (%s)\" % reason,",
            "                                    chunk_id,",
            "                                    i,",
            "                                )",
            "                    except IntegrityError as integrity_error:",
            "                        # repo_objs.parse() detected integrity issues.",
            "                        # maybe the repo gave us a valid cdata, but not for the chunk_id we wanted.",
            "                        # or the authentication of cdata failed, meaning the encrypted data was corrupted.",
            "                        report(str(integrity_error), chunk_id, i)",
            "                    except msgpack.UnpackException:",
            "                        report(\"Unpacker crashed while unpacking item metadata, trying to resync...\", chunk_id, i)",
            "                        unpacker.resync()",
            "                    except Exception:",
            "                        report(\"Exception while decrypting or unpacking item metadata\", chunk_id, i)",
            "                        raise",
            "                    i += 1",
            "",
            "        sort_by = sort_by.split(\",\")",
            "        if any((first, last, match, older, newer, newest, oldest)):",
            "            archive_infos = self.manifest.archives.list(",
            "                sort_by=sort_by,",
            "                match=match,",
            "                first=first,",
            "                last=last,",
            "                oldest=oldest,",
            "                newest=newest,",
            "                older=older,",
            "                newer=newer,",
            "            )",
            "            if match and not archive_infos:",
            "                logger.warning(\"--match-archives %s does not match any archives\", match)",
            "            if first and len(archive_infos) < first:",
            "                logger.warning(\"--first %d archives: only found %d archives\", first, len(archive_infos))",
            "            if last and len(archive_infos) < last:",
            "                logger.warning(\"--last %d archives: only found %d archives\", last, len(archive_infos))",
            "        else:",
            "            archive_infos = self.manifest.archives.list(sort_by=sort_by, consider_checkpoints=True)",
            "        num_archives = len(archive_infos)",
            "",
            "        pi = ProgressIndicatorPercent(",
            "            total=num_archives, msg=\"Checking archives %3.1f%%\", step=0.1, msgid=\"check.rebuild_refcounts\"",
            "        )",
            "        with cache_if_remote(self.repository) as repository:",
            "            for i, info in enumerate(archive_infos):",
            "                pi.show(i)",
            "                logger.info(f\"Analyzing archive {info.name} ({i + 1}/{num_archives})\")",
            "                archive_id = info.id",
            "                if archive_id not in self.chunks:",
            "                    logger.error(\"Archive metadata block %s is missing!\", bin_to_hex(archive_id))",
            "                    self.error_found = True",
            "                    del self.manifest.archives[info.name]",
            "                    continue",
            "                mark_as_possibly_superseded(archive_id)",
            "                cdata = self.repository.get(archive_id)",
            "                try:",
            "                    _, data = self.repo_objs.parse(archive_id, cdata)",
            "                except IntegrityError as integrity_error:",
            "                    logger.error(\"Archive metadata block %s is corrupted: %s\", bin_to_hex(archive_id), integrity_error)",
            "                    self.error_found = True",
            "                    del self.manifest.archives[info.name]",
            "                    continue",
            "                try:",
            "                    archive, verified, salt = self.key.unpack_and_verify_archive(data, force_tam_not_required=False)",
            "                except IntegrityError as integrity_error:",
            "                    # looks like there is a TAM issue with this archive, this might be an attack!",
            "                    # when upgrading to borg 1.2.5, users are expected to TAM-authenticate all archives they",
            "                    # trust, so there shouldn't be any without TAM.",
            "                    logger.error(\"Archive TAM authentication issue for archive %s: %s\", info.name, integrity_error)",
            "                    self.error_found = True",
            "                    del self.manifest.archives[info.name]",
            "                    continue",
            "                archive = ArchiveItem(internal_dict=archive)",
            "                if archive.version != 2:",
            "                    raise Exception(\"Unknown archive metadata version\")",
            "                items_buffer = ChunkBuffer(self.key)",
            "                items_buffer.write_chunk = add_callback",
            "                for item in robust_iterator(archive):",
            "                    if \"chunks\" in item:",
            "                        verify_file_chunks(info.name, item)",
            "                    items_buffer.add(item)",
            "                items_buffer.flush(flush=True)",
            "                for previous_item_id in archive_get_items(",
            "                    archive, repo_objs=self.repo_objs, repository=self.repository",
            "                ):",
            "                    mark_as_possibly_superseded(previous_item_id)",
            "                for previous_item_ptr in archive.item_ptrs:",
            "                    mark_as_possibly_superseded(previous_item_ptr)",
            "                archive.item_ptrs = archive_put_items(",
            "                    items_buffer.chunks, repo_objs=self.repo_objs, add_reference=add_reference",
            "                )",
            "                data = self.key.pack_and_authenticate_metadata(archive.as_dict(), context=b\"archive\", salt=salt)",
            "                new_archive_id = self.key.id_hash(data)",
            "                cdata = self.repo_objs.format(new_archive_id, {}, data)",
            "                add_reference(new_archive_id, len(data), cdata)",
            "                self.manifest.archives[info.name] = (new_archive_id, info.ts)",
            "            pi.finish()",
            "",
            "    def orphan_chunks_check(self):",
            "        if self.check_all:",
            "            unused = {id_ for id_, entry in self.chunks.iteritems() if entry.refcount == 0}",
            "            orphaned = unused - self.possibly_superseded",
            "            if orphaned:",
            "                logger.error(f\"{len(orphaned)} orphaned objects found!\")",
            "                for chunk_id in orphaned:",
            "                    logger.debug(f\"chunk {bin_to_hex(chunk_id)} is orphaned.\")",
            "                self.error_found = True",
            "            if self.repair and unused:",
            "                logger.info(",
            "                    \"Deleting %d orphaned and %d superseded objects...\" % (len(orphaned), len(self.possibly_superseded))",
            "                )",
            "                for id_ in unused:",
            "                    self.repository.delete(id_)",
            "                logger.info(\"Finished deleting orphaned/superseded objects.\")",
            "        else:",
            "            logger.info(\"Orphaned objects check skipped (needs all archives checked).\")",
            "",
            "    def finish(self):",
            "        if self.repair:",
            "            logger.info(\"Writing Manifest.\")",
            "            self.manifest.write()",
            "            logger.info(\"Committing repo.\")",
            "            self.repository.commit(compact=False)",
            "",
            "",
            "class ArchiveRecreater:",
            "    class Interrupted(Exception):",
            "        def __init__(self, metadata=None):",
            "            self.metadata = metadata or {}",
            "",
            "    @staticmethod",
            "    def is_temporary_archive(archive_name):",
            "        return archive_name.endswith(\".recreate\")",
            "",
            "    def __init__(",
            "        self,",
            "        manifest,",
            "        cache,",
            "        matcher,",
            "        exclude_caches=False,",
            "        exclude_if_present=None,",
            "        keep_exclude_tags=False,",
            "        chunker_params=None,",
            "        compression=None,",
            "        recompress=False,",
            "        always_recompress=False,",
            "        dry_run=False,",
            "        stats=False,",
            "        progress=False,",
            "        file_status_printer=None,",
            "        timestamp=None,",
            "        checkpoint_interval=1800,",
            "        checkpoint_volume=0,",
            "    ):",
            "        self.manifest = manifest",
            "        self.repository = manifest.repository",
            "        self.key = manifest.key",
            "        self.repo_objs = manifest.repo_objs",
            "        self.cache = cache",
            "",
            "        self.matcher = matcher",
            "        self.exclude_caches = exclude_caches",
            "        self.exclude_if_present = exclude_if_present or []",
            "        self.keep_exclude_tags = keep_exclude_tags",
            "",
            "        self.rechunkify = chunker_params is not None",
            "        if self.rechunkify:",
            "            logger.debug(\"Rechunking archives to %s\", chunker_params)",
            "        self.chunker_params = chunker_params or CHUNKER_PARAMS",
            "        self.recompress = recompress",
            "        self.always_recompress = always_recompress",
            "        self.compression = compression or CompressionSpec(\"none\")",
            "        self.seen_chunks = set()",
            "",
            "        self.timestamp = timestamp",
            "        self.dry_run = dry_run",
            "        self.stats = stats",
            "        self.progress = progress",
            "        self.print_file_status = file_status_printer or (lambda *args: None)",
            "        self.checkpoint_interval = None if dry_run else checkpoint_interval",
            "        self.checkpoint_volume = None if dry_run else checkpoint_volume",
            "",
            "    def recreate(self, archive_name, comment=None, target_name=None):",
            "        assert not self.is_temporary_archive(archive_name)",
            "        archive = self.open_archive(archive_name)",
            "        target = self.create_target(archive, target_name)",
            "        if self.exclude_if_present or self.exclude_caches:",
            "            self.matcher_add_tagged_dirs(archive)",
            "        if (",
            "            self.matcher.empty()",
            "            and not self.recompress",
            "            and not target.recreate_rechunkify",
            "            and comment is None",
            "            and target_name is None",
            "        ):",
            "            # nothing to do",
            "            return False",
            "        self.process_items(archive, target)",
            "        replace_original = target_name is None",
            "        self.save(archive, target, comment, replace_original=replace_original)",
            "        return True",
            "",
            "    def process_items(self, archive, target):",
            "        matcher = self.matcher",
            "",
            "        for item in archive.iter_items():",
            "            if not matcher.match(item.path):",
            "                self.print_file_status(\"-\", item.path)  # excluded (either by \"-\" or by \"!\")",
            "                continue",
            "            if self.dry_run:",
            "                self.print_file_status(\"+\", item.path)  # included",
            "            else:",
            "                self.process_item(archive, target, item)",
            "        if self.progress:",
            "            target.stats.show_progress(final=True)",
            "",
            "    def process_item(self, archive, target, item):",
            "        status = file_status(item.mode)",
            "        if \"chunks\" in item:",
            "            self.print_file_status(status, item.path)",
            "            status = None",
            "            self.process_chunks(archive, target, item)",
            "            target.stats.nfiles += 1",
            "        target.add_item(item, stats=target.stats)",
            "        self.print_file_status(status, item.path)",
            "",
            "    def process_chunks(self, archive, target, item):",
            "        if not self.recompress and not target.recreate_rechunkify:",
            "            for chunk_id, size in item.chunks:",
            "                self.cache.chunk_incref(chunk_id, target.stats)",
            "            return item.chunks",
            "        chunk_iterator = self.iter_chunks(archive, target, list(item.chunks))",
            "        chunk_processor = partial(self.chunk_processor, target)",
            "        target.process_file_chunks(item, self.cache, target.stats, self.progress, chunk_iterator, chunk_processor)",
            "",
            "    def chunk_processor(self, target, chunk):",
            "        chunk_id, data = cached_hash(chunk, self.key.id_hash)",
            "        if chunk_id in self.seen_chunks:",
            "            return self.cache.chunk_incref(chunk_id, target.stats)",
            "        overwrite = self.recompress",
            "        if self.recompress and not self.always_recompress and chunk_id in self.cache.chunks:",
            "            # Check if this chunk is already compressed the way we want it",
            "            old_meta = self.repo_objs.parse_meta(chunk_id, self.repository.get(chunk_id, read_data=False))",
            "            compr_hdr = bytes((old_meta[\"ctype\"], old_meta[\"clevel\"]))",
            "            compressor_cls, level = Compressor.detect(compr_hdr)",
            "            if (",
            "                compressor_cls.name == self.repo_objs.compressor.decide({}, data).name",
            "                and level == self.repo_objs.compressor.level",
            "            ):",
            "                # Stored chunk has the same compression method and level as we wanted",
            "                overwrite = False",
            "        chunk_entry = self.cache.add_chunk(chunk_id, {}, data, stats=target.stats, overwrite=overwrite, wait=False)",
            "        self.cache.repository.async_response(wait=False)",
            "        self.seen_chunks.add(chunk_entry.id)",
            "        return chunk_entry",
            "",
            "    def iter_chunks(self, archive, target, chunks):",
            "        chunk_iterator = archive.pipeline.fetch_many([chunk_id for chunk_id, _ in chunks])",
            "        if target.recreate_rechunkify:",
            "            # The target.chunker will read the file contents through ChunkIteratorFileWrapper chunk-by-chunk",
            "            # (does not load the entire file into memory)",
            "            file = ChunkIteratorFileWrapper(chunk_iterator)",
            "            yield from target.chunker.chunkify(file)",
            "        else:",
            "            for chunk in chunk_iterator:",
            "                yield Chunk(chunk, size=len(chunk), allocation=CH_DATA)",
            "",
            "    def save(self, archive, target, comment=None, replace_original=True):",
            "        if self.dry_run:",
            "            return",
            "        if comment is None:",
            "            comment = archive.metadata.get(\"comment\", \"\")",
            "",
            "        # Keep for the statistics if necessary",
            "        if self.stats:",
            "            _start = target.start",
            "",
            "        if self.timestamp is None:",
            "            additional_metadata = {",
            "                \"time\": archive.metadata.time,",
            "                \"time_end\": archive.metadata.get(\"time_end\") or archive.metadata.time,",
            "                \"command_line\": archive.metadata.command_line,",
            "                # but also remember recreate metadata:",
            "                \"recreate_command_line\": join_cmd(sys.argv),",
            "            }",
            "        else:",
            "            additional_metadata = {",
            "                \"command_line\": archive.metadata.command_line,",
            "                # but also remember recreate metadata:",
            "                \"recreate_command_line\": join_cmd(sys.argv),",
            "            }",
            "",
            "        target.save(comment=comment, timestamp=self.timestamp, additional_metadata=additional_metadata)",
            "        if replace_original:",
            "            archive.delete(Statistics(), progress=self.progress)",
            "            target.rename(archive.name)",
            "        if self.stats:",
            "            target.start = _start",
            "            target.end = archive_ts_now()",
            "            log_multi(str(target), str(target.stats))",
            "",
            "    def matcher_add_tagged_dirs(self, archive):",
            "        \"\"\"Add excludes to the matcher created by exclude_cache and exclude_if_present.\"\"\"",
            "",
            "        def exclude(dir, tag_item):",
            "            if self.keep_exclude_tags:",
            "                tag_files.append(PathPrefixPattern(tag_item.path, recurse_dir=False))",
            "                tagged_dirs.append(FnmatchPattern(dir + \"/\", recurse_dir=False))",
            "            else:",
            "                tagged_dirs.append(PathPrefixPattern(dir, recurse_dir=False))",
            "",
            "        matcher = self.matcher",
            "        tag_files = []",
            "        tagged_dirs = []",
            "",
            "        for item in archive.iter_items(",
            "            filter=lambda item: os.path.basename(item.path) == CACHE_TAG_NAME or matcher.match(item.path)",
            "        ):",
            "            dir, tag_file = os.path.split(item.path)",
            "            if tag_file in self.exclude_if_present:",
            "                exclude(dir, item)",
            "            elif self.exclude_caches and tag_file == CACHE_TAG_NAME and stat.S_ISREG(item.mode):",
            "                file = open_item(archive, item)",
            "                if file.read(len(CACHE_TAG_CONTENTS)) == CACHE_TAG_CONTENTS:",
            "                    exclude(dir, item)",
            "        matcher.add(tag_files, IECommand.Include)",
            "        matcher.add(tagged_dirs, IECommand.ExcludeNoRecurse)",
            "",
            "    def create_target(self, archive, target_name=None):",
            "        \"\"\"Create target archive.\"\"\"",
            "        target_name = target_name or archive.name + \".recreate\"",
            "        target = self.create_target_archive(target_name)",
            "        # If the archives use the same chunker params, then don't rechunkify",
            "        source_chunker_params = tuple(archive.metadata.get(\"chunker_params\", []))",
            "        if len(source_chunker_params) == 4 and isinstance(source_chunker_params[0], int):",
            "            # this is a borg < 1.2 chunker_params tuple, no chunker algo specified, but we only had buzhash:",
            "            source_chunker_params = (CH_BUZHASH,) + source_chunker_params",
            "        target.recreate_rechunkify = self.rechunkify and source_chunker_params != target.chunker_params",
            "        if target.recreate_rechunkify:",
            "            logger.debug(",
            "                \"Rechunking archive from %s to %s\", source_chunker_params or \"(unknown)\", target.chunker_params",
            "            )",
            "        target.process_file_chunks = ChunksProcessor(",
            "            cache=self.cache,",
            "            key=self.key,",
            "            add_item=target.add_item,",
            "            prepare_checkpoint=target.prepare_checkpoint,",
            "            write_checkpoint=target.write_checkpoint,",
            "            checkpoint_interval=self.checkpoint_interval,",
            "            checkpoint_volume=self.checkpoint_volume,",
            "            rechunkify=target.recreate_rechunkify,",
            "        ).process_file_chunks",
            "        target.chunker = get_chunker(*target.chunker_params, seed=self.key.chunk_seed, sparse=False)",
            "        return target",
            "",
            "    def create_target_archive(self, name):",
            "        target = Archive(",
            "            self.manifest,",
            "            name,",
            "            create=True,",
            "            progress=self.progress,",
            "            chunker_params=self.chunker_params,",
            "            cache=self.cache,",
            "        )",
            "        return target",
            "",
            "    def open_archive(self, name, **kwargs):",
            "        return Archive(self.manifest, name, cache=self.cache, **kwargs)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "535": [
                "Archive",
                "_load_meta"
            ],
            "1027": [
                "Archive",
                "set_meta"
            ],
            "2251": [
                "ArchiveChecker",
                "rebuild_refcounts"
            ],
            "2270": [
                "ArchiveChecker",
                "rebuild_refcounts"
            ]
        },
        "addLocation": [
            "src.borg.archive.Archive"
        ]
    },
    "src/borg/cache.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 755,
                "afterPatchRowNumber": 755,
                "PatchRowcode": "             nonlocal processed_item_metadata_chunks"
            },
            "1": {
                "beforePatchRowNumber": 756,
                "afterPatchRowNumber": 756,
                "PatchRowcode": "             csize, data = decrypted_repository.get(archive_id)"
            },
            "2": {
                "beforePatchRowNumber": 757,
                "afterPatchRowNumber": 757,
                "PatchRowcode": "             chunk_idx.add(archive_id, 1, len(data))"
            },
            "3": {
                "beforePatchRowNumber": 758,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            archive = ArchiveItem(internal_dict=msgpack.unpackb(data))"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 758,
                "PatchRowcode": "+            archive, verified, _ = self.key.unpack_and_verify_archive(data, force_tam_not_required=True)"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 759,
                "PatchRowcode": "+            archive = ArchiveItem(internal_dict=archive)"
            },
            "6": {
                "beforePatchRowNumber": 759,
                "afterPatchRowNumber": 760,
                "PatchRowcode": "             if archive.version not in (1, 2):  # legacy"
            },
            "7": {
                "beforePatchRowNumber": 760,
                "afterPatchRowNumber": 761,
                "PatchRowcode": "                 raise Exception(\"Unknown archive metadata version\")"
            },
            "8": {
                "beforePatchRowNumber": 761,
                "afterPatchRowNumber": 762,
                "PatchRowcode": "             if archive.version == 1:"
            }
        },
        "frontPatchFile": [
            "import configparser",
            "import os",
            "import shutil",
            "import stat",
            "from binascii import unhexlify",
            "from collections import namedtuple",
            "from time import perf_counter",
            "",
            "from .logger import create_logger",
            "",
            "logger = create_logger()",
            "",
            "files_cache_logger = create_logger(\"borg.debug.files_cache\")",
            "",
            "from .constants import CACHE_README, FILES_CACHE_MODE_DISABLED",
            "from .hashindex import ChunkIndex, ChunkIndexEntry, CacheSynchronizer",
            "from .helpers import Location",
            "from .helpers import Error",
            "from .helpers import get_cache_dir, get_security_dir",
            "from .helpers import bin_to_hex, parse_stringified_list",
            "from .helpers import format_file_size",
            "from .helpers import safe_ns",
            "from .helpers import yes",
            "from .helpers import remove_surrogates",
            "from .helpers import ProgressIndicatorPercent, ProgressIndicatorMessage",
            "from .helpers import set_ec, EXIT_WARNING",
            "from .helpers import safe_unlink",
            "from .helpers import msgpack",
            "from .helpers.msgpack import int_to_timestamp, timestamp_to_int",
            "from .item import ArchiveItem, ChunkListEntry",
            "from .crypto.key import PlaintextKey",
            "from .crypto.file_integrity import IntegrityCheckedFile, DetachedIntegrityCheckedFile, FileIntegrityError",
            "from .locking import Lock",
            "from .manifest import Manifest",
            "from .platform import SaveFile",
            "from .remote import cache_if_remote",
            "from .repository import LIST_SCAN_LIMIT",
            "",
            "# note: cmtime might me either a ctime or a mtime timestamp",
            "FileCacheEntry = namedtuple(\"FileCacheEntry\", \"age inode size cmtime chunk_ids\")",
            "",
            "",
            "class SecurityManager:",
            "    \"\"\"",
            "    Tracks repositories. Ensures that nothing bad happens (repository swaps,",
            "    replay attacks, unknown repositories etc.).",
            "",
            "    This is complicated by the Cache being initially used for this, while",
            "    only some commands actually use the Cache, which meant that other commands",
            "    did not perform these checks.",
            "",
            "    Further complications were created by the Cache being a cache, so it",
            "    could be legitimately deleted, which is annoying because Borg didn't",
            "    recognize repositories after that.",
            "",
            "    Therefore a second location, the security database (see get_security_dir),",
            "    was introduced which stores this information. However, this means that",
            "    the code has to deal with a cache existing but no security DB entry,",
            "    or inconsistencies between the security DB and the cache which have to",
            "    be reconciled, and also with no cache existing but a security DB entry.",
            "    \"\"\"",
            "",
            "    def __init__(self, repository):",
            "        self.repository = repository",
            "        self.dir = get_security_dir(repository.id_str, legacy=(repository.version == 1))",
            "        self.cache_dir = cache_dir(repository)",
            "        self.key_type_file = os.path.join(self.dir, \"key-type\")",
            "        self.location_file = os.path.join(self.dir, \"location\")",
            "        self.manifest_ts_file = os.path.join(self.dir, \"manifest-timestamp\")",
            "",
            "    @staticmethod",
            "    def destroy(repository, path=None):",
            "        \"\"\"destroy the security dir for ``repository`` or at ``path``\"\"\"",
            "        path = path or get_security_dir(repository.id_str, legacy=(repository.version == 1))",
            "        if os.path.exists(path):",
            "            shutil.rmtree(path)",
            "",
            "    def known(self):",
            "        return all(os.path.exists(f) for f in (self.key_type_file, self.location_file, self.manifest_ts_file))",
            "",
            "    def key_matches(self, key):",
            "        if not self.known():",
            "            return False",
            "        try:",
            "            with open(self.key_type_file) as fd:",
            "                type = fd.read()",
            "                return type == str(key.TYPE)",
            "        except OSError as exc:",
            "            logger.warning(\"Could not read/parse key type file: %s\", exc)",
            "",
            "    def save(self, manifest, key):",
            "        logger.debug(\"security: saving state for %s to %s\", self.repository.id_str, self.dir)",
            "        current_location = self.repository._location.canonical_path()",
            "        logger.debug(\"security: current location   %s\", current_location)",
            "        logger.debug(\"security: key type           %s\", str(key.TYPE))",
            "        logger.debug(\"security: manifest timestamp %s\", manifest.timestamp)",
            "        with SaveFile(self.location_file) as fd:",
            "            fd.write(current_location)",
            "        with SaveFile(self.key_type_file) as fd:",
            "            fd.write(str(key.TYPE))",
            "        with SaveFile(self.manifest_ts_file) as fd:",
            "            fd.write(manifest.timestamp)",
            "",
            "    def assert_location_matches(self, cache_config=None):",
            "        # Warn user before sending data to a relocated repository",
            "        try:",
            "            with open(self.location_file) as fd:",
            "                previous_location = fd.read()",
            "            logger.debug(\"security: read previous location %r\", previous_location)",
            "        except FileNotFoundError:",
            "            logger.debug(\"security: previous location file %s not found\", self.location_file)",
            "            previous_location = None",
            "        except OSError as exc:",
            "            logger.warning(\"Could not read previous location file: %s\", exc)",
            "            previous_location = None",
            "        if cache_config and cache_config.previous_location and previous_location != cache_config.previous_location:",
            "            # Reconcile cache and security dir; we take the cache location.",
            "            previous_location = cache_config.previous_location",
            "            logger.debug(\"security: using previous_location of cache: %r\", previous_location)",
            "",
            "        repository_location = self.repository._location.canonical_path()",
            "        if previous_location and previous_location != repository_location:",
            "            msg = (",
            "                \"Warning: The repository at location {} was previously located at {}\\n\".format(",
            "                    repository_location, previous_location",
            "                )",
            "                + \"Do you want to continue? [yN] \"",
            "            )",
            "            if not yes(",
            "                msg,",
            "                false_msg=\"Aborting.\",",
            "                invalid_msg=\"Invalid answer, aborting.\",",
            "                retry=False,",
            "                env_var_override=\"BORG_RELOCATED_REPO_ACCESS_IS_OK\",",
            "            ):",
            "                raise Cache.RepositoryAccessAborted()",
            "            # adapt on-disk config immediately if the new location was accepted",
            "            logger.debug(\"security: updating location stored in cache and security dir\")",
            "            with SaveFile(self.location_file) as fd:",
            "                fd.write(repository_location)",
            "            if cache_config:",
            "                cache_config.save()",
            "",
            "    def assert_no_manifest_replay(self, manifest, key, cache_config=None):",
            "        try:",
            "            with open(self.manifest_ts_file) as fd:",
            "                timestamp = fd.read()",
            "            logger.debug(\"security: read manifest timestamp %r\", timestamp)",
            "        except FileNotFoundError:",
            "            logger.debug(\"security: manifest timestamp file %s not found\", self.manifest_ts_file)",
            "            timestamp = \"\"",
            "        except OSError as exc:",
            "            logger.warning(\"Could not read previous location file: %s\", exc)",
            "            timestamp = \"\"",
            "        if cache_config:",
            "            timestamp = max(timestamp, cache_config.timestamp or \"\")",
            "        logger.debug(\"security: determined newest manifest timestamp as %s\", timestamp)",
            "        # If repository is older than the cache or security dir something fishy is going on",
            "        if timestamp and timestamp > manifest.timestamp:",
            "            if isinstance(key, PlaintextKey):",
            "                raise Cache.RepositoryIDNotUnique()",
            "            else:",
            "                raise Cache.RepositoryReplay()",
            "",
            "    def assert_key_type(self, key, cache_config=None):",
            "        # Make sure an encrypted repository has not been swapped for an unencrypted repository",
            "        if cache_config and cache_config.key_type is not None and cache_config.key_type != str(key.TYPE):",
            "            raise Cache.EncryptionMethodMismatch()",
            "        if self.known() and not self.key_matches(key):",
            "            raise Cache.EncryptionMethodMismatch()",
            "",
            "    def assert_secure(self, manifest, key, *, cache_config=None, warn_if_unencrypted=True, lock_wait=None):",
            "        # warn_if_unencrypted=False is only used for initializing a new repository.",
            "        # Thus, avoiding asking about a repository that's currently initializing.",
            "        self.assert_access_unknown(warn_if_unencrypted, manifest, key)",
            "        if cache_config:",
            "            self._assert_secure(manifest, key, cache_config)",
            "        else:",
            "            cache_config = CacheConfig(self.repository, lock_wait=lock_wait)",
            "            if cache_config.exists():",
            "                with cache_config:",
            "                    self._assert_secure(manifest, key, cache_config)",
            "            else:",
            "                self._assert_secure(manifest, key)",
            "        logger.debug(\"security: repository checks ok, allowing access\")",
            "",
            "    def _assert_secure(self, manifest, key, cache_config=None):",
            "        self.assert_location_matches(cache_config)",
            "        self.assert_key_type(key, cache_config)",
            "        self.assert_no_manifest_replay(manifest, key, cache_config)",
            "        if not self.known():",
            "            logger.debug(\"security: remembering previously unknown repository\")",
            "            self.save(manifest, key)",
            "",
            "    def assert_access_unknown(self, warn_if_unencrypted, manifest, key):",
            "        # warn_if_unencrypted=False is only used for initializing a new repository.",
            "        # Thus, avoiding asking about a repository that's currently initializing.",
            "        if not key.logically_encrypted and not self.known():",
            "            msg = (",
            "                \"Warning: Attempting to access a previously unknown unencrypted repository!\\n\"",
            "                + \"Do you want to continue? [yN] \"",
            "            )",
            "            allow_access = not warn_if_unencrypted or yes(",
            "                msg,",
            "                false_msg=\"Aborting.\",",
            "                invalid_msg=\"Invalid answer, aborting.\",",
            "                retry=False,",
            "                env_var_override=\"BORG_UNKNOWN_UNENCRYPTED_REPO_ACCESS_IS_OK\",",
            "            )",
            "            if allow_access:",
            "                if warn_if_unencrypted:",
            "                    logger.debug(\"security: remembering unknown unencrypted repository (explicitly allowed)\")",
            "                else:",
            "                    logger.debug(\"security: initializing unencrypted repository\")",
            "                self.save(manifest, key)",
            "            else:",
            "                raise Cache.CacheInitAbortedError()",
            "",
            "",
            "def assert_secure(repository, manifest, lock_wait):",
            "    sm = SecurityManager(repository)",
            "    sm.assert_secure(manifest, manifest.key, lock_wait=lock_wait)",
            "",
            "",
            "def recanonicalize_relative_location(cache_location, repository):",
            "    # borg < 1.0.8rc1 had different canonicalization for the repo location (see #1655 and #1741).",
            "    repo_location = repository._location.canonical_path()",
            "    rl = Location(repo_location)",
            "    cl = Location(cache_location)",
            "    if (",
            "        cl.proto == rl.proto",
            "        and cl.user == rl.user",
            "        and cl.host == rl.host",
            "        and cl.port == rl.port",
            "        and cl.path",
            "        and rl.path",
            "        and cl.path.startswith(\"/~/\")",
            "        and rl.path.startswith(\"/./\")",
            "        and cl.path[3:] == rl.path[3:]",
            "    ):",
            "        # everything is same except the expected change in relative path canonicalization,",
            "        # update previous_location to avoid warning / user query about changed location:",
            "        return repo_location",
            "    else:",
            "        return cache_location",
            "",
            "",
            "def cache_dir(repository, path=None):",
            "    return path or os.path.join(get_cache_dir(), repository.id_str)",
            "",
            "",
            "def files_cache_name():",
            "    suffix = os.environ.get(\"BORG_FILES_CACHE_SUFFIX\", \"\")",
            "    return \"files.\" + suffix if suffix else \"files\"",
            "",
            "",
            "def discover_files_cache_name(path):",
            "    return [fn for fn in os.listdir(path) if fn == \"files\" or fn.startswith(\"files.\")][0]",
            "",
            "",
            "class CacheConfig:",
            "    def __init__(self, repository, path=None, lock_wait=None):",
            "        self.repository = repository",
            "        self.path = cache_dir(repository, path)",
            "        logger.debug(\"Using %s as cache\", self.path)",
            "        self.config_path = os.path.join(self.path, \"config\")",
            "        self.lock = None",
            "        self.lock_wait = lock_wait",
            "",
            "    def __enter__(self):",
            "        self.open()",
            "        return self",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        self.close()",
            "",
            "    def exists(self):",
            "        return os.path.exists(self.config_path)",
            "",
            "    def create(self):",
            "        assert not self.exists()",
            "        config = configparser.ConfigParser(interpolation=None)",
            "        config.add_section(\"cache\")",
            "        config.set(\"cache\", \"version\", \"1\")",
            "        config.set(\"cache\", \"repository\", self.repository.id_str)",
            "        config.set(\"cache\", \"manifest\", \"\")",
            "        config.add_section(\"integrity\")",
            "        config.set(\"integrity\", \"manifest\", \"\")",
            "        with SaveFile(self.config_path) as fd:",
            "            config.write(fd)",
            "",
            "    def open(self):",
            "        self.lock = Lock(os.path.join(self.path, \"lock\"), exclusive=True, timeout=self.lock_wait).acquire()",
            "        self.load()",
            "",
            "    def load(self):",
            "        self._config = configparser.ConfigParser(interpolation=None)",
            "        with open(self.config_path) as fd:",
            "            self._config.read_file(fd)",
            "        self._check_upgrade(self.config_path)",
            "        self.id = self._config.get(\"cache\", \"repository\")",
            "        self.manifest_id = unhexlify(self._config.get(\"cache\", \"manifest\"))",
            "        self.timestamp = self._config.get(\"cache\", \"timestamp\", fallback=None)",
            "        self.key_type = self._config.get(\"cache\", \"key_type\", fallback=None)",
            "        self.ignored_features = set(parse_stringified_list(self._config.get(\"cache\", \"ignored_features\", fallback=\"\")))",
            "        self.mandatory_features = set(",
            "            parse_stringified_list(self._config.get(\"cache\", \"mandatory_features\", fallback=\"\"))",
            "        )",
            "        try:",
            "            self.integrity = dict(self._config.items(\"integrity\"))",
            "            if self._config.get(\"cache\", \"manifest\") != self.integrity.pop(\"manifest\"):",
            "                # The cache config file is updated (parsed with ConfigParser, the state of the ConfigParser",
            "                # is modified and then written out.), not re-created.",
            "                # Thus, older versions will leave our [integrity] section alone, making the section's data invalid.",
            "                # Therefore, we also add the manifest ID to this section and",
            "                # can discern whether an older version interfered by comparing the manifest IDs of this section",
            "                # and the main [cache] section.",
            "                self.integrity = {}",
            "                logger.warning(\"Cache integrity data not available: old Borg version modified the cache.\")",
            "        except configparser.NoSectionError:",
            "            logger.debug(\"Cache integrity: No integrity data found (files, chunks). Cache is from old version.\")",
            "            self.integrity = {}",
            "        previous_location = self._config.get(\"cache\", \"previous_location\", fallback=None)",
            "        if previous_location:",
            "            self.previous_location = recanonicalize_relative_location(previous_location, self.repository)",
            "        else:",
            "            self.previous_location = None",
            "        self._config.set(\"cache\", \"previous_location\", self.repository._location.canonical_path())",
            "",
            "    def save(self, manifest=None, key=None):",
            "        if manifest:",
            "            self._config.set(\"cache\", \"manifest\", manifest.id_str)",
            "            self._config.set(\"cache\", \"timestamp\", manifest.timestamp)",
            "            self._config.set(\"cache\", \"ignored_features\", \",\".join(self.ignored_features))",
            "            self._config.set(\"cache\", \"mandatory_features\", \",\".join(self.mandatory_features))",
            "            if not self._config.has_section(\"integrity\"):",
            "                self._config.add_section(\"integrity\")",
            "            for file, integrity_data in self.integrity.items():",
            "                self._config.set(\"integrity\", file, integrity_data)",
            "            self._config.set(\"integrity\", \"manifest\", manifest.id_str)",
            "        if key:",
            "            self._config.set(\"cache\", \"key_type\", str(key.TYPE))",
            "        with SaveFile(self.config_path) as fd:",
            "            self._config.write(fd)",
            "",
            "    def close(self):",
            "        if self.lock is not None:",
            "            self.lock.release()",
            "            self.lock = None",
            "",
            "    def _check_upgrade(self, config_path):",
            "        try:",
            "            cache_version = self._config.getint(\"cache\", \"version\")",
            "            wanted_version = 1",
            "            if cache_version != wanted_version:",
            "                self.close()",
            "                raise Exception(",
            "                    \"%s has unexpected cache version %d (wanted: %d).\" % (config_path, cache_version, wanted_version)",
            "                )",
            "        except configparser.NoSectionError:",
            "            self.close()",
            "            raise Exception(\"%s does not look like a Borg cache.\" % config_path) from None",
            "",
            "",
            "class Cache:",
            "    \"\"\"Client Side cache\"\"\"",
            "",
            "    class RepositoryIDNotUnique(Error):",
            "        \"\"\"Cache is newer than repository - do you have multiple, independently updated repos with same ID?\"\"\"",
            "",
            "    class RepositoryReplay(Error):",
            "        \"\"\"Cache, or information obtained from the security directory is newer than repository - this is either an attack or unsafe (multiple repos with same ID)\"\"\"",
            "",
            "    class CacheInitAbortedError(Error):",
            "        \"\"\"Cache initialization aborted\"\"\"",
            "",
            "    class RepositoryAccessAborted(Error):",
            "        \"\"\"Repository access aborted\"\"\"",
            "",
            "    class EncryptionMethodMismatch(Error):",
            "        \"\"\"Repository encryption method changed since last access, refusing to continue\"\"\"",
            "",
            "    @staticmethod",
            "    def break_lock(repository, path=None):",
            "        path = cache_dir(repository, path)",
            "        Lock(os.path.join(path, \"lock\"), exclusive=True).break_lock()",
            "",
            "    @staticmethod",
            "    def destroy(repository, path=None):",
            "        \"\"\"destroy the cache for ``repository`` or at ``path``\"\"\"",
            "        path = path or os.path.join(get_cache_dir(), repository.id_str)",
            "        config = os.path.join(path, \"config\")",
            "        if os.path.exists(config):",
            "            os.remove(config)  # kill config first",
            "            shutil.rmtree(path)",
            "",
            "    def __new__(",
            "        cls,",
            "        repository,",
            "        manifest,",
            "        path=None,",
            "        sync=True,",
            "        warn_if_unencrypted=True,",
            "        progress=False,",
            "        lock_wait=None,",
            "        permit_adhoc_cache=False,",
            "        cache_mode=FILES_CACHE_MODE_DISABLED,",
            "        iec=False,",
            "    ):",
            "        def local():",
            "            return LocalCache(",
            "                manifest=manifest,",
            "                path=path,",
            "                sync=sync,",
            "                warn_if_unencrypted=warn_if_unencrypted,",
            "                progress=progress,",
            "                iec=iec,",
            "                lock_wait=lock_wait,",
            "                cache_mode=cache_mode,",
            "            )",
            "",
            "        def adhoc():",
            "            return AdHocCache(manifest=manifest, lock_wait=lock_wait, iec=iec)",
            "",
            "        if not permit_adhoc_cache:",
            "            return local()",
            "",
            "        # ad-hoc cache may be permitted, but if the local cache is in sync it'd be stupid to invalidate",
            "        # it by needlessly using the ad-hoc cache.",
            "        # Check if the local cache exists and is in sync.",
            "",
            "        cache_config = CacheConfig(repository, path, lock_wait)",
            "        if cache_config.exists():",
            "            with cache_config:",
            "                cache_in_sync = cache_config.manifest_id == manifest.id",
            "            # Don't nest cache locks",
            "            if cache_in_sync:",
            "                # Local cache is in sync, use it",
            "                logger.debug(\"Cache: choosing local cache (in sync)\")",
            "                return local()",
            "        logger.debug(\"Cache: choosing ad-hoc cache (local cache does not exist or is not in sync)\")",
            "        return adhoc()",
            "",
            "",
            "class CacheStatsMixin:",
            "    str_format = \"\"\"\\",
            "Original size: {0.total_size}",
            "Deduplicated size: {0.unique_size}",
            "Unique chunks: {0.total_unique_chunks}",
            "Total chunks: {0.total_chunks}",
            "\"\"\"",
            "",
            "    def __init__(self, iec=False):",
            "        self.iec = iec",
            "",
            "    def __str__(self):",
            "        return self.str_format.format(self.format_tuple())",
            "",
            "    Summary = namedtuple(\"Summary\", [\"total_size\", \"unique_size\", \"total_unique_chunks\", \"total_chunks\"])",
            "",
            "    def stats(self):",
            "        from .archive import Archive",
            "",
            "        # XXX: this should really be moved down to `hashindex.pyx`",
            "        total_size, unique_size, total_unique_chunks, total_chunks = self.chunks.summarize()",
            "        # since borg 1.2 we have new archive metadata telling the total size per archive,",
            "        # so we can just sum up all archives to get the \"all archives\" stats:",
            "        total_size = 0",
            "        for archive_name in self.manifest.archives:",
            "            archive = Archive(self.manifest, archive_name)",
            "            stats = archive.calc_stats(self, want_unique=False)",
            "            total_size += stats.osize",
            "        stats = self.Summary(total_size, unique_size, total_unique_chunks, total_chunks)._asdict()",
            "        return stats",
            "",
            "    def format_tuple(self):",
            "        stats = self.stats()",
            "        for field in [\"total_size\", \"unique_size\"]:",
            "            stats[field] = format_file_size(stats[field], iec=self.iec)",
            "        return self.Summary(**stats)",
            "",
            "",
            "class LocalCache(CacheStatsMixin):",
            "    \"\"\"",
            "    Persistent, local (client-side) cache.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        manifest,",
            "        path=None,",
            "        sync=True,",
            "        warn_if_unencrypted=True,",
            "        progress=False,",
            "        lock_wait=None,",
            "        cache_mode=FILES_CACHE_MODE_DISABLED,",
            "        iec=False,",
            "    ):",
            "        \"\"\"",
            "        :param warn_if_unencrypted: print warning if accessing unknown unencrypted repository",
            "        :param lock_wait: timeout for lock acquisition (int [s] or None [wait forever])",
            "        :param sync: do :meth:`.sync`",
            "        :param cache_mode: what shall be compared in the file stat infos vs. cached stat infos comparison",
            "        \"\"\"",
            "        CacheStatsMixin.__init__(self, iec=iec)",
            "        assert isinstance(manifest, Manifest)",
            "        self.manifest = manifest",
            "        self.repository = manifest.repository",
            "        self.key = manifest.key",
            "        self.repo_objs = manifest.repo_objs",
            "        self.progress = progress",
            "        self.cache_mode = cache_mode",
            "        self.timestamp = None",
            "        self.txn_active = False",
            "",
            "        self.path = cache_dir(self.repository, path)",
            "        self.security_manager = SecurityManager(self.repository)",
            "        self.cache_config = CacheConfig(self.repository, self.path, lock_wait)",
            "",
            "        # Warn user before sending data to a never seen before unencrypted repository",
            "        if not os.path.exists(self.path):",
            "            self.security_manager.assert_access_unknown(warn_if_unencrypted, manifest, self.key)",
            "            self.create()",
            "",
            "        self.open()",
            "        try:",
            "            self.security_manager.assert_secure(manifest, self.key, cache_config=self.cache_config)",
            "",
            "            if not self.check_cache_compatibility():",
            "                self.wipe_cache()",
            "",
            "            self.update_compatibility()",
            "",
            "            if sync and self.manifest.id != self.cache_config.manifest_id:",
            "                self.sync()",
            "                self.commit()",
            "        except:  # noqa",
            "            self.close()",
            "            raise",
            "",
            "    def __enter__(self):",
            "        return self",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        self.close()",
            "",
            "    def create(self):",
            "        \"\"\"Create a new empty cache at `self.path`\"\"\"",
            "        os.makedirs(self.path)",
            "        with open(os.path.join(self.path, \"README\"), \"w\") as fd:",
            "            fd.write(CACHE_README)",
            "        self.cache_config.create()",
            "        ChunkIndex().write(os.path.join(self.path, \"chunks\"))",
            "        os.makedirs(os.path.join(self.path, \"chunks.archive.d\"))",
            "        with SaveFile(os.path.join(self.path, files_cache_name()), binary=True):",
            "            pass  # empty file",
            "",
            "    def _do_open(self):",
            "        self.cache_config.load()",
            "        with IntegrityCheckedFile(",
            "            path=os.path.join(self.path, \"chunks\"),",
            "            write=False,",
            "            integrity_data=self.cache_config.integrity.get(\"chunks\"),",
            "        ) as fd:",
            "            self.chunks = ChunkIndex.read(fd)",
            "        if \"d\" in self.cache_mode:  # d(isabled)",
            "            self.files = None",
            "        else:",
            "            self._read_files()",
            "",
            "    def open(self):",
            "        if not os.path.isdir(self.path):",
            "            raise Exception(\"%s Does not look like a Borg cache\" % self.path)",
            "        self.cache_config.open()",
            "        self.rollback()",
            "",
            "    def close(self):",
            "        if self.cache_config is not None:",
            "            self.cache_config.close()",
            "            self.cache_config = None",
            "",
            "    def _read_files(self):",
            "        self.files = {}",
            "        self._newest_cmtime = None",
            "        logger.debug(\"Reading files cache ...\")",
            "        files_cache_logger.debug(\"FILES-CACHE-LOAD: starting...\")",
            "        msg = None",
            "        try:",
            "            with IntegrityCheckedFile(",
            "                path=os.path.join(self.path, files_cache_name()),",
            "                write=False,",
            "                integrity_data=self.cache_config.integrity.get(files_cache_name()),",
            "            ) as fd:",
            "                u = msgpack.Unpacker(use_list=True)",
            "                while True:",
            "                    data = fd.read(64 * 1024)",
            "                    if not data:",
            "                        break",
            "                    u.feed(data)",
            "                    try:",
            "                        for path_hash, item in u:",
            "                            entry = FileCacheEntry(*item)",
            "                            # in the end, this takes about 240 Bytes per file",
            "                            self.files[path_hash] = msgpack.packb(entry._replace(age=entry.age + 1))",
            "                    except (TypeError, ValueError) as exc:",
            "                        msg = \"The files cache seems invalid. [%s]\" % str(exc)",
            "                        break",
            "        except OSError as exc:",
            "            msg = \"The files cache can't be read. [%s]\" % str(exc)",
            "        except FileIntegrityError as fie:",
            "            msg = \"The files cache is corrupted. [%s]\" % str(fie)",
            "        if msg is not None:",
            "            logger.warning(msg)",
            "            logger.warning(\"Continuing without files cache - expect lower performance.\")",
            "            self.files = {}",
            "        files_cache_logger.debug(\"FILES-CACHE-LOAD: finished, %d entries loaded.\", len(self.files))",
            "",
            "    def begin_txn(self):",
            "        # Initialize transaction snapshot",
            "        pi = ProgressIndicatorMessage(msgid=\"cache.begin_transaction\")",
            "        txn_dir = os.path.join(self.path, \"txn.tmp\")",
            "        os.mkdir(txn_dir)",
            "        pi.output(\"Initializing cache transaction: Reading config\")",
            "        shutil.copy(os.path.join(self.path, \"config\"), txn_dir)",
            "        pi.output(\"Initializing cache transaction: Reading chunks\")",
            "        shutil.copy(os.path.join(self.path, \"chunks\"), txn_dir)",
            "        pi.output(\"Initializing cache transaction: Reading files\")",
            "        try:",
            "            shutil.copy(os.path.join(self.path, files_cache_name()), txn_dir)",
            "        except FileNotFoundError:",
            "            with SaveFile(os.path.join(txn_dir, files_cache_name()), binary=True):",
            "                pass  # empty file",
            "        os.replace(txn_dir, os.path.join(self.path, \"txn.active\"))",
            "        self.txn_active = True",
            "        pi.finish()",
            "",
            "    def commit(self):",
            "        \"\"\"Commit transaction\"\"\"",
            "        if not self.txn_active:",
            "            return",
            "        self.security_manager.save(self.manifest, self.key)",
            "        pi = ProgressIndicatorMessage(msgid=\"cache.commit\")",
            "        if self.files is not None:",
            "            if self._newest_cmtime is None:",
            "                # was never set because no files were modified/added",
            "                self._newest_cmtime = 2**63 - 1  # nanoseconds, good until y2262",
            "            ttl = int(os.environ.get(\"BORG_FILES_CACHE_TTL\", 20))",
            "            pi.output(\"Saving files cache\")",
            "            files_cache_logger.debug(\"FILES-CACHE-SAVE: starting...\")",
            "            with IntegrityCheckedFile(path=os.path.join(self.path, files_cache_name()), write=True) as fd:",
            "                entry_count = 0",
            "                for path_hash, item in self.files.items():",
            "                    # Only keep files seen in this backup that are older than newest cmtime seen in this backup -",
            "                    # this is to avoid issues with filesystem snapshots and cmtime granularity.",
            "                    # Also keep files from older backups that have not reached BORG_FILES_CACHE_TTL yet.",
            "                    entry = FileCacheEntry(*msgpack.unpackb(item))",
            "                    if (",
            "                        entry.age == 0",
            "                        and timestamp_to_int(entry.cmtime) < self._newest_cmtime",
            "                        or entry.age > 0",
            "                        and entry.age < ttl",
            "                    ):",
            "                        msgpack.pack((path_hash, entry), fd)",
            "                        entry_count += 1",
            "            files_cache_logger.debug(\"FILES-CACHE-KILL: removed all old entries with age >= TTL [%d]\", ttl)",
            "            files_cache_logger.debug(",
            "                \"FILES-CACHE-KILL: removed all current entries with newest cmtime %d\", self._newest_cmtime",
            "            )",
            "            files_cache_logger.debug(\"FILES-CACHE-SAVE: finished, %d remaining entries saved.\", entry_count)",
            "            self.cache_config.integrity[files_cache_name()] = fd.integrity_data",
            "        pi.output(\"Saving chunks cache\")",
            "        with IntegrityCheckedFile(path=os.path.join(self.path, \"chunks\"), write=True) as fd:",
            "            self.chunks.write(fd)",
            "        self.cache_config.integrity[\"chunks\"] = fd.integrity_data",
            "        pi.output(\"Saving cache config\")",
            "        self.cache_config.save(self.manifest, self.key)",
            "        os.replace(os.path.join(self.path, \"txn.active\"), os.path.join(self.path, \"txn.tmp\"))",
            "        shutil.rmtree(os.path.join(self.path, \"txn.tmp\"))",
            "        self.txn_active = False",
            "        pi.finish()",
            "",
            "    def rollback(self):",
            "        \"\"\"Roll back partial and aborted transactions\"\"\"",
            "        # Remove partial transaction",
            "        if os.path.exists(os.path.join(self.path, \"txn.tmp\")):",
            "            shutil.rmtree(os.path.join(self.path, \"txn.tmp\"))",
            "        # Roll back active transaction",
            "        txn_dir = os.path.join(self.path, \"txn.active\")",
            "        if os.path.exists(txn_dir):",
            "            shutil.copy(os.path.join(txn_dir, \"config\"), self.path)",
            "            shutil.copy(os.path.join(txn_dir, \"chunks\"), self.path)",
            "            shutil.copy(os.path.join(txn_dir, discover_files_cache_name(txn_dir)), self.path)",
            "            txn_tmp = os.path.join(self.path, \"txn.tmp\")",
            "            os.replace(txn_dir, txn_tmp)",
            "            if os.path.exists(txn_tmp):",
            "                shutil.rmtree(txn_tmp)",
            "        self.txn_active = False",
            "        self._do_open()",
            "",
            "    def sync(self):",
            "        \"\"\"Re-synchronize chunks cache with repository.",
            "",
            "        Maintains a directory with known backup archive indexes, so it only",
            "        needs to fetch infos from repo and build a chunk index once per backup",
            "        archive.",
            "        If out of sync, missing archive indexes get added, outdated indexes",
            "        get removed and a new master chunks index is built by merging all",
            "        archive indexes.",
            "        \"\"\"",
            "        archive_path = os.path.join(self.path, \"chunks.archive.d\")",
            "        # Instrumentation",
            "        processed_item_metadata_bytes = 0",
            "        processed_item_metadata_chunks = 0",
            "        compact_chunks_archive_saved_space = 0",
            "",
            "        def mkpath(id, suffix=\"\"):",
            "            id_hex = bin_to_hex(id)",
            "            path = os.path.join(archive_path, id_hex + suffix)",
            "            return path",
            "",
            "        def cached_archives():",
            "            if self.do_cache:",
            "                fns = os.listdir(archive_path)",
            "                # filenames with 64 hex digits == 256bit,",
            "                # or compact indices which are 64 hex digits + \".compact\"",
            "                return {unhexlify(fn) for fn in fns if len(fn) == 64} | {",
            "                    unhexlify(fn[:64]) for fn in fns if len(fn) == 72 and fn.endswith(\".compact\")",
            "                }",
            "            else:",
            "                return set()",
            "",
            "        def repo_archives():",
            "            return {info.id for info in self.manifest.archives.list()}",
            "",
            "        def cleanup_outdated(ids):",
            "            for id in ids:",
            "                cleanup_cached_archive(id)",
            "",
            "        def cleanup_cached_archive(id, cleanup_compact=True):",
            "            try:",
            "                os.unlink(mkpath(id))",
            "                os.unlink(mkpath(id) + \".integrity\")",
            "            except FileNotFoundError:",
            "                pass",
            "            if not cleanup_compact:",
            "                return",
            "            try:",
            "                os.unlink(mkpath(id, suffix=\".compact\"))",
            "                os.unlink(mkpath(id, suffix=\".compact\") + \".integrity\")",
            "            except FileNotFoundError:",
            "                pass",
            "",
            "        def fetch_and_build_idx(archive_id, decrypted_repository, chunk_idx):",
            "            nonlocal processed_item_metadata_bytes",
            "            nonlocal processed_item_metadata_chunks",
            "            csize, data = decrypted_repository.get(archive_id)",
            "            chunk_idx.add(archive_id, 1, len(data))",
            "            archive = ArchiveItem(internal_dict=msgpack.unpackb(data))",
            "            if archive.version not in (1, 2):  # legacy",
            "                raise Exception(\"Unknown archive metadata version\")",
            "            if archive.version == 1:",
            "                items = archive.items",
            "            elif archive.version == 2:",
            "                items = []",
            "                for chunk_id, (csize, data) in zip(archive.item_ptrs, decrypted_repository.get_many(archive.item_ptrs)):",
            "                    chunk_idx.add(chunk_id, 1, len(data))",
            "                    ids = msgpack.unpackb(data)",
            "                    items.extend(ids)",
            "            sync = CacheSynchronizer(chunk_idx)",
            "            for item_id, (csize, data) in zip(items, decrypted_repository.get_many(items)):",
            "                chunk_idx.add(item_id, 1, len(data))",
            "                processed_item_metadata_bytes += len(data)",
            "                processed_item_metadata_chunks += 1",
            "                sync.feed(data)",
            "            if self.do_cache:",
            "                write_archive_index(archive_id, chunk_idx)",
            "",
            "        def write_archive_index(archive_id, chunk_idx):",
            "            nonlocal compact_chunks_archive_saved_space",
            "            compact_chunks_archive_saved_space += chunk_idx.compact()",
            "            fn = mkpath(archive_id, suffix=\".compact\")",
            "            fn_tmp = mkpath(archive_id, suffix=\".tmp\")",
            "            try:",
            "                with DetachedIntegrityCheckedFile(",
            "                    path=fn_tmp, write=True, filename=bin_to_hex(archive_id) + \".compact\"",
            "                ) as fd:",
            "                    chunk_idx.write(fd)",
            "            except Exception:",
            "                safe_unlink(fn_tmp)",
            "            else:",
            "                os.replace(fn_tmp, fn)",
            "",
            "        def read_archive_index(archive_id, archive_name):",
            "            archive_chunk_idx_path = mkpath(archive_id)",
            "            logger.info(\"Reading cached archive chunk index for %s\", archive_name)",
            "            try:",
            "                try:",
            "                    # Attempt to load compact index first",
            "                    with DetachedIntegrityCheckedFile(path=archive_chunk_idx_path + \".compact\", write=False) as fd:",
            "                        archive_chunk_idx = ChunkIndex.read(fd, permit_compact=True)",
            "                    # In case a non-compact index exists, delete it.",
            "                    cleanup_cached_archive(archive_id, cleanup_compact=False)",
            "                    # Compact index read - return index, no conversion necessary (below).",
            "                    return archive_chunk_idx",
            "                except FileNotFoundError:",
            "                    # No compact index found, load non-compact index, and convert below.",
            "                    with DetachedIntegrityCheckedFile(path=archive_chunk_idx_path, write=False) as fd:",
            "                        archive_chunk_idx = ChunkIndex.read(fd)",
            "            except FileIntegrityError as fie:",
            "                logger.error(\"Cached archive chunk index of %s is corrupted: %s\", archive_name, fie)",
            "                # Delete corrupted index, set warning. A new index must be build.",
            "                cleanup_cached_archive(archive_id)",
            "                set_ec(EXIT_WARNING)",
            "                return None",
            "",
            "            # Convert to compact index. Delete the existing index first.",
            "            logger.debug(\"Found non-compact index for %s, converting to compact.\", archive_name)",
            "            cleanup_cached_archive(archive_id)",
            "            write_archive_index(archive_id, archive_chunk_idx)",
            "            return archive_chunk_idx",
            "",
            "        def get_archive_ids_to_names(archive_ids):",
            "            # Pass once over all archives and build a mapping from ids to names.",
            "            # The easier approach, doing a similar loop for each archive, has",
            "            # square complexity and does about a dozen million functions calls",
            "            # with 1100 archives (which takes 30s CPU seconds _alone_).",
            "            archive_names = {}",
            "            for info in self.manifest.archives.list():",
            "                if info.id in archive_ids:",
            "                    archive_names[info.id] = info.name",
            "            assert len(archive_names) == len(archive_ids)",
            "            return archive_names",
            "",
            "        def create_master_idx(chunk_idx):",
            "            logger.debug(\"Synchronizing chunks index...\")",
            "            cached_ids = cached_archives()",
            "            archive_ids = repo_archives()",
            "            logger.info(",
            "                \"Cached archive chunk indexes: %d fresh, %d stale, %d need fetching.\",",
            "                len(archive_ids & cached_ids),",
            "                len(cached_ids - archive_ids),",
            "                len(archive_ids - cached_ids),",
            "            )",
            "            # deallocates old hashindex, creates empty hashindex:",
            "            chunk_idx.clear()",
            "            cleanup_outdated(cached_ids - archive_ids)",
            "            # Explicitly set the usable initial hash table capacity to avoid performance issues",
            "            # due to hash table \"resonance\".",
            "            master_index_capacity = len(self.repository)",
            "            if archive_ids:",
            "                chunk_idx = None if not self.do_cache else ChunkIndex(usable=master_index_capacity)",
            "                pi = ProgressIndicatorPercent(",
            "                    total=len(archive_ids),",
            "                    step=0.1,",
            "                    msg=\"%3.0f%% Syncing chunks index. Processing archive %s.\",",
            "                    msgid=\"cache.sync\",",
            "                )",
            "                archive_ids_to_names = get_archive_ids_to_names(archive_ids)",
            "                for archive_id, archive_name in archive_ids_to_names.items():",
            "                    pi.show(info=[remove_surrogates(archive_name)])  # legacy. borg2 always has pure unicode arch names.",
            "                    if self.do_cache:",
            "                        if archive_id in cached_ids:",
            "                            archive_chunk_idx = read_archive_index(archive_id, archive_name)",
            "                            if archive_chunk_idx is None:",
            "                                cached_ids.remove(archive_id)",
            "                        if archive_id not in cached_ids:",
            "                            # Do not make this an else branch; the FileIntegrityError exception handler",
            "                            # above can remove *archive_id* from *cached_ids*.",
            "                            logger.info(\"Fetching and building archive index for %s.\", archive_name)",
            "                            archive_chunk_idx = ChunkIndex()",
            "                            fetch_and_build_idx(archive_id, decrypted_repository, archive_chunk_idx)",
            "                        logger.debug(\"Merging into master chunks index.\")",
            "                        chunk_idx.merge(archive_chunk_idx)",
            "                    else:",
            "                        chunk_idx = chunk_idx or ChunkIndex(usable=master_index_capacity)",
            "                        logger.info(\"Fetching archive index for %s.\", archive_name)",
            "                        fetch_and_build_idx(archive_id, decrypted_repository, chunk_idx)",
            "                pi.finish()",
            "                logger.debug(",
            "                    \"Chunks index sync: processed %s (%d chunks) of metadata.\",",
            "                    format_file_size(processed_item_metadata_bytes),",
            "                    processed_item_metadata_chunks,",
            "                )",
            "                logger.debug(",
            "                    \"Chunks index sync: compact chunks.archive.d storage saved %s bytes.\",",
            "                    format_file_size(compact_chunks_archive_saved_space),",
            "                )",
            "            logger.debug(\"Chunks index sync done.\")",
            "            return chunk_idx",
            "",
            "        # The cache can be used by a command that e.g. only checks against Manifest.Operation.WRITE,",
            "        # which does not have to include all flags from Manifest.Operation.READ.",
            "        # Since the sync will attempt to read archives, check compatibility with Manifest.Operation.READ.",
            "        self.manifest.check_repository_compatibility((Manifest.Operation.READ,))",
            "",
            "        self.begin_txn()",
            "        with cache_if_remote(self.repository, decrypted_cache=self.repo_objs) as decrypted_repository:",
            "            # TEMPORARY HACK:",
            "            # to avoid archive index caching, create a FILE named ~/.cache/borg/REPOID/chunks.archive.d -",
            "            # this is only recommended if you have a fast, low latency connection to your repo (e.g. if repo is local).",
            "            self.do_cache = os.path.isdir(archive_path)",
            "            self.chunks = create_master_idx(self.chunks)",
            "",
            "    def check_cache_compatibility(self):",
            "        my_features = Manifest.SUPPORTED_REPO_FEATURES",
            "        if self.cache_config.ignored_features & my_features:",
            "            # The cache might not contain references of chunks that need a feature that is mandatory for some operation",
            "            # and which this version supports. To avoid corruption while executing that operation force rebuild.",
            "            return False",
            "        if not self.cache_config.mandatory_features <= my_features:",
            "            # The cache was build with consideration to at least one feature that this version does not understand.",
            "            # This client might misinterpret the cache. Thus force a rebuild.",
            "            return False",
            "        return True",
            "",
            "    def wipe_cache(self):",
            "        logger.warning(\"Discarding incompatible cache and forcing a cache rebuild\")",
            "        archive_path = os.path.join(self.path, \"chunks.archive.d\")",
            "        if os.path.isdir(archive_path):",
            "            shutil.rmtree(os.path.join(self.path, \"chunks.archive.d\"))",
            "            os.makedirs(os.path.join(self.path, \"chunks.archive.d\"))",
            "        self.chunks = ChunkIndex()",
            "        with SaveFile(os.path.join(self.path, files_cache_name()), binary=True):",
            "            pass  # empty file",
            "        self.cache_config.manifest_id = \"\"",
            "        self.cache_config._config.set(\"cache\", \"manifest\", \"\")",
            "",
            "        self.cache_config.ignored_features = set()",
            "        self.cache_config.mandatory_features = set()",
            "",
            "    def update_compatibility(self):",
            "        operation_to_features_map = self.manifest.get_all_mandatory_features()",
            "        my_features = Manifest.SUPPORTED_REPO_FEATURES",
            "        repo_features = set()",
            "        for operation, features in operation_to_features_map.items():",
            "            repo_features.update(features)",
            "",
            "        self.cache_config.ignored_features.update(repo_features - my_features)",
            "        self.cache_config.mandatory_features.update(repo_features & my_features)",
            "",
            "    def add_chunk(",
            "        self, id, meta, data, *, stats, overwrite=False, wait=True, compress=True, size=None, ctype=None, clevel=None",
            "    ):",
            "        if not self.txn_active:",
            "            self.begin_txn()",
            "        if size is None and compress:",
            "            size = len(data)  # data is still uncompressed",
            "        refcount = self.seen_chunk(id, size)",
            "        if refcount and not overwrite:",
            "            return self.chunk_incref(id, stats)",
            "        if size is None:",
            "            raise ValueError(\"when giving compressed data for a new chunk, the uncompressed size must be given also\")",
            "        cdata = self.repo_objs.format(id, meta, data, compress=compress, size=size, ctype=ctype, clevel=clevel)",
            "        self.repository.put(id, cdata, wait=wait)",
            "        self.chunks.add(id, 1, size)",
            "        stats.update(size, not refcount)",
            "        return ChunkListEntry(id, size)",
            "",
            "    def seen_chunk(self, id, size=None):",
            "        refcount, stored_size = self.chunks.get(id, ChunkIndexEntry(0, None))",
            "        if size is not None and stored_size is not None and size != stored_size:",
            "            # we already have a chunk with that id, but different size.",
            "            # this is either a hash collision (unlikely) or corruption or a bug.",
            "            raise Exception(",
            "                \"chunk has same id [%r], but different size (stored: %d new: %d)!\" % (id, stored_size, size)",
            "            )",
            "        return refcount",
            "",
            "    def chunk_incref(self, id, stats, size=None):",
            "        if not self.txn_active:",
            "            self.begin_txn()",
            "        count, _size = self.chunks.incref(id)",
            "        stats.update(_size, False)",
            "        return ChunkListEntry(id, _size)",
            "",
            "    def chunk_decref(self, id, stats, wait=True):",
            "        if not self.txn_active:",
            "            self.begin_txn()",
            "        count, size = self.chunks.decref(id)",
            "        if count == 0:",
            "            del self.chunks[id]",
            "            self.repository.delete(id, wait=wait)",
            "            stats.update(-size, True)",
            "        else:",
            "            stats.update(-size, False)",
            "",
            "    def file_known_and_unchanged(self, hashed_path, path_hash, st):",
            "        \"\"\"",
            "        Check if we know the file that has this path_hash (know == it is in our files cache) and",
            "        whether it is unchanged (the size/inode number/cmtime is same for stuff we check in this cache_mode).",
            "",
            "        :param hashed_path: the file's path as we gave it to hash(hashed_path)",
            "        :param path_hash: hash(hashed_path), to save some memory in the files cache",
            "        :param st: the file's stat() result",
            "        :return: known, ids (known is True if we have infos about this file in the cache,",
            "                             ids is the list of chunk ids IF the file has not changed, otherwise None).",
            "        \"\"\"",
            "        if not stat.S_ISREG(st.st_mode):",
            "            return False, None",
            "        cache_mode = self.cache_mode",
            "        if \"d\" in cache_mode:  # d(isabled)",
            "            files_cache_logger.debug(\"UNKNOWN: files cache disabled\")",
            "            return False, None",
            "        # note: r(echunk) does not need the files cache in this method, but the files cache will",
            "        # be updated and saved to disk to memorize the files. To preserve previous generations in",
            "        # the cache, this means that it also needs to get loaded from disk first.",
            "        if \"r\" in cache_mode:  # r(echunk)",
            "            files_cache_logger.debug(\"UNKNOWN: rechunking enforced\")",
            "            return False, None",
            "        entry = self.files.get(path_hash)",
            "        if not entry:",
            "            files_cache_logger.debug(\"UNKNOWN: no file metadata in cache for: %r\", hashed_path)",
            "            return False, None",
            "        # we know the file!",
            "        entry = FileCacheEntry(*msgpack.unpackb(entry))",
            "        if \"s\" in cache_mode and entry.size != st.st_size:",
            "            files_cache_logger.debug(\"KNOWN-CHANGED: file size has changed: %r\", hashed_path)",
            "            return True, None",
            "        if \"i\" in cache_mode and entry.inode != st.st_ino:",
            "            files_cache_logger.debug(\"KNOWN-CHANGED: file inode number has changed: %r\", hashed_path)",
            "            return True, None",
            "        if \"c\" in cache_mode and timestamp_to_int(entry.cmtime) != st.st_ctime_ns:",
            "            files_cache_logger.debug(\"KNOWN-CHANGED: file ctime has changed: %r\", hashed_path)",
            "            return True, None",
            "        elif \"m\" in cache_mode and timestamp_to_int(entry.cmtime) != st.st_mtime_ns:",
            "            files_cache_logger.debug(\"KNOWN-CHANGED: file mtime has changed: %r\", hashed_path)",
            "            return True, None",
            "        # we ignored the inode number in the comparison above or it is still same.",
            "        # if it is still the same, replacing it in the tuple doesn't change it.",
            "        # if we ignored it, a reason for doing that is that files were moved to a new",
            "        # disk / new fs (so a one-time change of inode number is expected) and we wanted",
            "        # to avoid everything getting chunked again. to be able to re-enable the inode",
            "        # number comparison in a future backup run (and avoid chunking everything",
            "        # again at that time), we need to update the inode number in the cache with what",
            "        # we see in the filesystem.",
            "        self.files[path_hash] = msgpack.packb(entry._replace(inode=st.st_ino, age=0))",
            "        return True, entry.chunk_ids",
            "",
            "    def memorize_file(self, hashed_path, path_hash, st, ids):",
            "        if not stat.S_ISREG(st.st_mode):",
            "            return",
            "        cache_mode = self.cache_mode",
            "        # note: r(echunk) modes will update the files cache, d(isabled) mode won't",
            "        if \"d\" in cache_mode:",
            "            files_cache_logger.debug(\"FILES-CACHE-NOUPDATE: files cache disabled\")",
            "            return",
            "        if \"c\" in cache_mode:",
            "            cmtime_type = \"ctime\"",
            "            cmtime_ns = safe_ns(st.st_ctime_ns)",
            "        elif \"m\" in cache_mode:",
            "            cmtime_type = \"mtime\"",
            "            cmtime_ns = safe_ns(st.st_mtime_ns)",
            "        else:  # neither 'c' nor 'm' in cache_mode, avoid UnboundLocalError",
            "            cmtime_type = \"ctime\"",
            "            cmtime_ns = safe_ns(st.st_ctime_ns)",
            "        entry = FileCacheEntry(",
            "            age=0, inode=st.st_ino, size=st.st_size, cmtime=int_to_timestamp(cmtime_ns), chunk_ids=ids",
            "        )",
            "        self.files[path_hash] = msgpack.packb(entry)",
            "        self._newest_cmtime = max(self._newest_cmtime or 0, cmtime_ns)",
            "        files_cache_logger.debug(",
            "            \"FILES-CACHE-UPDATE: put %r [has %s] <- %r\",",
            "            entry._replace(chunk_ids=\"[%d entries]\" % len(entry.chunk_ids)),",
            "            cmtime_type,",
            "            hashed_path,",
            "        )",
            "",
            "",
            "class AdHocCache(CacheStatsMixin):",
            "    \"\"\"",
            "    Ad-hoc, non-persistent cache.",
            "",
            "    Compared to the standard LocalCache the AdHocCache does not maintain accurate reference count,",
            "    nor does it provide a files cache (which would require persistence). Chunks that were not added",
            "    during the current AdHocCache lifetime won't have correct size set (0 bytes) and will",
            "    have an infinite reference count (MAX_VALUE).",
            "    \"\"\"",
            "",
            "    str_format = \"\"\"\\",
            "All archives:                unknown              unknown              unknown",
            "",
            "                       Unique chunks         Total chunks",
            "Chunk index:    {0.total_unique_chunks:20d}             unknown\"\"\"",
            "",
            "    def __init__(self, manifest, warn_if_unencrypted=True, lock_wait=None, iec=False):",
            "        CacheStatsMixin.__init__(self, iec=iec)",
            "        assert isinstance(manifest, Manifest)",
            "        self.manifest = manifest",
            "        self.repository = manifest.repository",
            "        self.key = manifest.key",
            "        self.repo_objs = manifest.repo_objs",
            "        self._txn_active = False",
            "",
            "        self.security_manager = SecurityManager(self.repository)",
            "        self.security_manager.assert_secure(manifest, self.key, lock_wait=lock_wait)",
            "",
            "        logger.warning(\"Note: --no-cache-sync is an experimental feature.\")",
            "",
            "    # Public API",
            "",
            "    def __enter__(self):",
            "        return self",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        pass",
            "",
            "    files = None  # type: ignore",
            "    cache_mode = \"d\"",
            "",
            "    def file_known_and_unchanged(self, hashed_path, path_hash, st):",
            "        files_cache_logger.debug(\"UNKNOWN: files cache not implemented\")",
            "        return False, None",
            "",
            "    def memorize_file(self, hashed_path, path_hash, st, ids):",
            "        pass",
            "",
            "    def add_chunk(self, id, meta, data, *, stats, overwrite=False, wait=True, compress=True, size=None):",
            "        assert not overwrite, \"AdHocCache does not permit overwrites \u2014 trying to use it for recreate?\"",
            "        if not self._txn_active:",
            "            self.begin_txn()",
            "        if size is None and compress:",
            "            size = len(data)  # data is still uncompressed",
            "        if size is None:",
            "            raise ValueError(\"when giving compressed data for a chunk, the uncompressed size must be given also\")",
            "        refcount = self.seen_chunk(id, size)",
            "        if refcount:",
            "            return self.chunk_incref(id, stats, size=size)",
            "        cdata = self.repo_objs.format(id, meta, data, compress=compress)",
            "        self.repository.put(id, cdata, wait=wait)",
            "        self.chunks.add(id, 1, size)",
            "        stats.update(size, not refcount)",
            "        return ChunkListEntry(id, size)",
            "",
            "    def seen_chunk(self, id, size=None):",
            "        if not self._txn_active:",
            "            self.begin_txn()",
            "        entry = self.chunks.get(id, ChunkIndexEntry(0, None))",
            "        if entry.refcount and size and not entry.size:",
            "            # The LocalCache has existing size information and uses *size* to make an effort at detecting collisions.",
            "            # This is of course not possible for the AdHocCache.",
            "            # Here *size* is used to update the chunk's size information, which will be zero for existing chunks.",
            "            self.chunks[id] = entry._replace(size=size)",
            "        return entry.refcount",
            "",
            "    def chunk_incref(self, id, stats, size=None):",
            "        if not self._txn_active:",
            "            self.begin_txn()",
            "        count, _size = self.chunks.incref(id)",
            "        # When _size is 0 and size is not given, then this chunk has not been locally visited yet (seen_chunk with",
            "        # size or add_chunk); we can't add references to those (size=0 is invalid) and generally don't try to.",
            "        size = _size or size",
            "        assert size",
            "        stats.update(size, False)",
            "        return ChunkListEntry(id, size)",
            "",
            "    def chunk_decref(self, id, stats, wait=True):",
            "        if not self._txn_active:",
            "            self.begin_txn()",
            "        count, size = self.chunks.decref(id)",
            "        if count == 0:",
            "            del self.chunks[id]",
            "            self.repository.delete(id, wait=wait)",
            "            stats.update(-size, True)",
            "        else:",
            "            stats.update(-size, False)",
            "",
            "    def commit(self):",
            "        if not self._txn_active:",
            "            return",
            "        self.security_manager.save(self.manifest, self.key)",
            "        self._txn_active = False",
            "",
            "    def rollback(self):",
            "        self._txn_active = False",
            "        del self.chunks",
            "",
            "    def begin_txn(self):",
            "        self._txn_active = True",
            "        # Explicitly set the initial usable hash table capacity to avoid performance issues",
            "        # due to hash table \"resonance\".",
            "        # Since we're creating an archive, add 10 % from the start.",
            "        num_chunks = len(self.repository)",
            "        self.chunks = ChunkIndex(usable=num_chunks * 1.1)",
            "        pi = ProgressIndicatorPercent(",
            "            total=num_chunks, msg=\"Downloading chunk list... %3.0f%%\", msgid=\"cache.download_chunks\"",
            "        )",
            "        t0 = perf_counter()",
            "        num_requests = 0",
            "        marker = None",
            "        while True:",
            "            result = self.repository.list(limit=LIST_SCAN_LIMIT, marker=marker)",
            "            num_requests += 1",
            "            if not result:",
            "                break",
            "            pi.show(increase=len(result))",
            "            marker = result[-1]",
            "            # All chunks from the repository have a refcount of MAX_VALUE, which is sticky,",
            "            # therefore we can't/won't delete them. Chunks we added ourselves in this transaction",
            "            # (e.g. checkpoint archives) are tracked correctly.",
            "            init_entry = ChunkIndexEntry(refcount=ChunkIndex.MAX_VALUE, size=0)",
            "            for id_ in result:",
            "                self.chunks[id_] = init_entry",
            "        assert len(self.chunks) == num_chunks",
            "        # LocalCache does not contain the manifest, either.",
            "        del self.chunks[self.manifest.MANIFEST_ID]",
            "        duration = perf_counter() - t0 or 0.01",
            "        pi.finish()",
            "        logger.debug(",
            "            \"AdHocCache: downloaded %d chunk IDs in %.2f s (%d requests), ~%s/s\",",
            "            num_chunks,",
            "            duration,",
            "            num_requests,",
            "            format_file_size(num_chunks * 34 / duration),",
            "        )",
            "        # Chunk IDs in a list are encoded in 34 bytes: 1 byte msgpack header, 1 byte length, 32 ID bytes.",
            "        # Protocol overhead is neglected in this calculation."
        ],
        "afterPatchFile": [
            "import configparser",
            "import os",
            "import shutil",
            "import stat",
            "from binascii import unhexlify",
            "from collections import namedtuple",
            "from time import perf_counter",
            "",
            "from .logger import create_logger",
            "",
            "logger = create_logger()",
            "",
            "files_cache_logger = create_logger(\"borg.debug.files_cache\")",
            "",
            "from .constants import CACHE_README, FILES_CACHE_MODE_DISABLED",
            "from .hashindex import ChunkIndex, ChunkIndexEntry, CacheSynchronizer",
            "from .helpers import Location",
            "from .helpers import Error",
            "from .helpers import get_cache_dir, get_security_dir",
            "from .helpers import bin_to_hex, parse_stringified_list",
            "from .helpers import format_file_size",
            "from .helpers import safe_ns",
            "from .helpers import yes",
            "from .helpers import remove_surrogates",
            "from .helpers import ProgressIndicatorPercent, ProgressIndicatorMessage",
            "from .helpers import set_ec, EXIT_WARNING",
            "from .helpers import safe_unlink",
            "from .helpers import msgpack",
            "from .helpers.msgpack import int_to_timestamp, timestamp_to_int",
            "from .item import ArchiveItem, ChunkListEntry",
            "from .crypto.key import PlaintextKey",
            "from .crypto.file_integrity import IntegrityCheckedFile, DetachedIntegrityCheckedFile, FileIntegrityError",
            "from .locking import Lock",
            "from .manifest import Manifest",
            "from .platform import SaveFile",
            "from .remote import cache_if_remote",
            "from .repository import LIST_SCAN_LIMIT",
            "",
            "# note: cmtime might me either a ctime or a mtime timestamp",
            "FileCacheEntry = namedtuple(\"FileCacheEntry\", \"age inode size cmtime chunk_ids\")",
            "",
            "",
            "class SecurityManager:",
            "    \"\"\"",
            "    Tracks repositories. Ensures that nothing bad happens (repository swaps,",
            "    replay attacks, unknown repositories etc.).",
            "",
            "    This is complicated by the Cache being initially used for this, while",
            "    only some commands actually use the Cache, which meant that other commands",
            "    did not perform these checks.",
            "",
            "    Further complications were created by the Cache being a cache, so it",
            "    could be legitimately deleted, which is annoying because Borg didn't",
            "    recognize repositories after that.",
            "",
            "    Therefore a second location, the security database (see get_security_dir),",
            "    was introduced which stores this information. However, this means that",
            "    the code has to deal with a cache existing but no security DB entry,",
            "    or inconsistencies between the security DB and the cache which have to",
            "    be reconciled, and also with no cache existing but a security DB entry.",
            "    \"\"\"",
            "",
            "    def __init__(self, repository):",
            "        self.repository = repository",
            "        self.dir = get_security_dir(repository.id_str, legacy=(repository.version == 1))",
            "        self.cache_dir = cache_dir(repository)",
            "        self.key_type_file = os.path.join(self.dir, \"key-type\")",
            "        self.location_file = os.path.join(self.dir, \"location\")",
            "        self.manifest_ts_file = os.path.join(self.dir, \"manifest-timestamp\")",
            "",
            "    @staticmethod",
            "    def destroy(repository, path=None):",
            "        \"\"\"destroy the security dir for ``repository`` or at ``path``\"\"\"",
            "        path = path or get_security_dir(repository.id_str, legacy=(repository.version == 1))",
            "        if os.path.exists(path):",
            "            shutil.rmtree(path)",
            "",
            "    def known(self):",
            "        return all(os.path.exists(f) for f in (self.key_type_file, self.location_file, self.manifest_ts_file))",
            "",
            "    def key_matches(self, key):",
            "        if not self.known():",
            "            return False",
            "        try:",
            "            with open(self.key_type_file) as fd:",
            "                type = fd.read()",
            "                return type == str(key.TYPE)",
            "        except OSError as exc:",
            "            logger.warning(\"Could not read/parse key type file: %s\", exc)",
            "",
            "    def save(self, manifest, key):",
            "        logger.debug(\"security: saving state for %s to %s\", self.repository.id_str, self.dir)",
            "        current_location = self.repository._location.canonical_path()",
            "        logger.debug(\"security: current location   %s\", current_location)",
            "        logger.debug(\"security: key type           %s\", str(key.TYPE))",
            "        logger.debug(\"security: manifest timestamp %s\", manifest.timestamp)",
            "        with SaveFile(self.location_file) as fd:",
            "            fd.write(current_location)",
            "        with SaveFile(self.key_type_file) as fd:",
            "            fd.write(str(key.TYPE))",
            "        with SaveFile(self.manifest_ts_file) as fd:",
            "            fd.write(manifest.timestamp)",
            "",
            "    def assert_location_matches(self, cache_config=None):",
            "        # Warn user before sending data to a relocated repository",
            "        try:",
            "            with open(self.location_file) as fd:",
            "                previous_location = fd.read()",
            "            logger.debug(\"security: read previous location %r\", previous_location)",
            "        except FileNotFoundError:",
            "            logger.debug(\"security: previous location file %s not found\", self.location_file)",
            "            previous_location = None",
            "        except OSError as exc:",
            "            logger.warning(\"Could not read previous location file: %s\", exc)",
            "            previous_location = None",
            "        if cache_config and cache_config.previous_location and previous_location != cache_config.previous_location:",
            "            # Reconcile cache and security dir; we take the cache location.",
            "            previous_location = cache_config.previous_location",
            "            logger.debug(\"security: using previous_location of cache: %r\", previous_location)",
            "",
            "        repository_location = self.repository._location.canonical_path()",
            "        if previous_location and previous_location != repository_location:",
            "            msg = (",
            "                \"Warning: The repository at location {} was previously located at {}\\n\".format(",
            "                    repository_location, previous_location",
            "                )",
            "                + \"Do you want to continue? [yN] \"",
            "            )",
            "            if not yes(",
            "                msg,",
            "                false_msg=\"Aborting.\",",
            "                invalid_msg=\"Invalid answer, aborting.\",",
            "                retry=False,",
            "                env_var_override=\"BORG_RELOCATED_REPO_ACCESS_IS_OK\",",
            "            ):",
            "                raise Cache.RepositoryAccessAborted()",
            "            # adapt on-disk config immediately if the new location was accepted",
            "            logger.debug(\"security: updating location stored in cache and security dir\")",
            "            with SaveFile(self.location_file) as fd:",
            "                fd.write(repository_location)",
            "            if cache_config:",
            "                cache_config.save()",
            "",
            "    def assert_no_manifest_replay(self, manifest, key, cache_config=None):",
            "        try:",
            "            with open(self.manifest_ts_file) as fd:",
            "                timestamp = fd.read()",
            "            logger.debug(\"security: read manifest timestamp %r\", timestamp)",
            "        except FileNotFoundError:",
            "            logger.debug(\"security: manifest timestamp file %s not found\", self.manifest_ts_file)",
            "            timestamp = \"\"",
            "        except OSError as exc:",
            "            logger.warning(\"Could not read previous location file: %s\", exc)",
            "            timestamp = \"\"",
            "        if cache_config:",
            "            timestamp = max(timestamp, cache_config.timestamp or \"\")",
            "        logger.debug(\"security: determined newest manifest timestamp as %s\", timestamp)",
            "        # If repository is older than the cache or security dir something fishy is going on",
            "        if timestamp and timestamp > manifest.timestamp:",
            "            if isinstance(key, PlaintextKey):",
            "                raise Cache.RepositoryIDNotUnique()",
            "            else:",
            "                raise Cache.RepositoryReplay()",
            "",
            "    def assert_key_type(self, key, cache_config=None):",
            "        # Make sure an encrypted repository has not been swapped for an unencrypted repository",
            "        if cache_config and cache_config.key_type is not None and cache_config.key_type != str(key.TYPE):",
            "            raise Cache.EncryptionMethodMismatch()",
            "        if self.known() and not self.key_matches(key):",
            "            raise Cache.EncryptionMethodMismatch()",
            "",
            "    def assert_secure(self, manifest, key, *, cache_config=None, warn_if_unencrypted=True, lock_wait=None):",
            "        # warn_if_unencrypted=False is only used for initializing a new repository.",
            "        # Thus, avoiding asking about a repository that's currently initializing.",
            "        self.assert_access_unknown(warn_if_unencrypted, manifest, key)",
            "        if cache_config:",
            "            self._assert_secure(manifest, key, cache_config)",
            "        else:",
            "            cache_config = CacheConfig(self.repository, lock_wait=lock_wait)",
            "            if cache_config.exists():",
            "                with cache_config:",
            "                    self._assert_secure(manifest, key, cache_config)",
            "            else:",
            "                self._assert_secure(manifest, key)",
            "        logger.debug(\"security: repository checks ok, allowing access\")",
            "",
            "    def _assert_secure(self, manifest, key, cache_config=None):",
            "        self.assert_location_matches(cache_config)",
            "        self.assert_key_type(key, cache_config)",
            "        self.assert_no_manifest_replay(manifest, key, cache_config)",
            "        if not self.known():",
            "            logger.debug(\"security: remembering previously unknown repository\")",
            "            self.save(manifest, key)",
            "",
            "    def assert_access_unknown(self, warn_if_unencrypted, manifest, key):",
            "        # warn_if_unencrypted=False is only used for initializing a new repository.",
            "        # Thus, avoiding asking about a repository that's currently initializing.",
            "        if not key.logically_encrypted and not self.known():",
            "            msg = (",
            "                \"Warning: Attempting to access a previously unknown unencrypted repository!\\n\"",
            "                + \"Do you want to continue? [yN] \"",
            "            )",
            "            allow_access = not warn_if_unencrypted or yes(",
            "                msg,",
            "                false_msg=\"Aborting.\",",
            "                invalid_msg=\"Invalid answer, aborting.\",",
            "                retry=False,",
            "                env_var_override=\"BORG_UNKNOWN_UNENCRYPTED_REPO_ACCESS_IS_OK\",",
            "            )",
            "            if allow_access:",
            "                if warn_if_unencrypted:",
            "                    logger.debug(\"security: remembering unknown unencrypted repository (explicitly allowed)\")",
            "                else:",
            "                    logger.debug(\"security: initializing unencrypted repository\")",
            "                self.save(manifest, key)",
            "            else:",
            "                raise Cache.CacheInitAbortedError()",
            "",
            "",
            "def assert_secure(repository, manifest, lock_wait):",
            "    sm = SecurityManager(repository)",
            "    sm.assert_secure(manifest, manifest.key, lock_wait=lock_wait)",
            "",
            "",
            "def recanonicalize_relative_location(cache_location, repository):",
            "    # borg < 1.0.8rc1 had different canonicalization for the repo location (see #1655 and #1741).",
            "    repo_location = repository._location.canonical_path()",
            "    rl = Location(repo_location)",
            "    cl = Location(cache_location)",
            "    if (",
            "        cl.proto == rl.proto",
            "        and cl.user == rl.user",
            "        and cl.host == rl.host",
            "        and cl.port == rl.port",
            "        and cl.path",
            "        and rl.path",
            "        and cl.path.startswith(\"/~/\")",
            "        and rl.path.startswith(\"/./\")",
            "        and cl.path[3:] == rl.path[3:]",
            "    ):",
            "        # everything is same except the expected change in relative path canonicalization,",
            "        # update previous_location to avoid warning / user query about changed location:",
            "        return repo_location",
            "    else:",
            "        return cache_location",
            "",
            "",
            "def cache_dir(repository, path=None):",
            "    return path or os.path.join(get_cache_dir(), repository.id_str)",
            "",
            "",
            "def files_cache_name():",
            "    suffix = os.environ.get(\"BORG_FILES_CACHE_SUFFIX\", \"\")",
            "    return \"files.\" + suffix if suffix else \"files\"",
            "",
            "",
            "def discover_files_cache_name(path):",
            "    return [fn for fn in os.listdir(path) if fn == \"files\" or fn.startswith(\"files.\")][0]",
            "",
            "",
            "class CacheConfig:",
            "    def __init__(self, repository, path=None, lock_wait=None):",
            "        self.repository = repository",
            "        self.path = cache_dir(repository, path)",
            "        logger.debug(\"Using %s as cache\", self.path)",
            "        self.config_path = os.path.join(self.path, \"config\")",
            "        self.lock = None",
            "        self.lock_wait = lock_wait",
            "",
            "    def __enter__(self):",
            "        self.open()",
            "        return self",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        self.close()",
            "",
            "    def exists(self):",
            "        return os.path.exists(self.config_path)",
            "",
            "    def create(self):",
            "        assert not self.exists()",
            "        config = configparser.ConfigParser(interpolation=None)",
            "        config.add_section(\"cache\")",
            "        config.set(\"cache\", \"version\", \"1\")",
            "        config.set(\"cache\", \"repository\", self.repository.id_str)",
            "        config.set(\"cache\", \"manifest\", \"\")",
            "        config.add_section(\"integrity\")",
            "        config.set(\"integrity\", \"manifest\", \"\")",
            "        with SaveFile(self.config_path) as fd:",
            "            config.write(fd)",
            "",
            "    def open(self):",
            "        self.lock = Lock(os.path.join(self.path, \"lock\"), exclusive=True, timeout=self.lock_wait).acquire()",
            "        self.load()",
            "",
            "    def load(self):",
            "        self._config = configparser.ConfigParser(interpolation=None)",
            "        with open(self.config_path) as fd:",
            "            self._config.read_file(fd)",
            "        self._check_upgrade(self.config_path)",
            "        self.id = self._config.get(\"cache\", \"repository\")",
            "        self.manifest_id = unhexlify(self._config.get(\"cache\", \"manifest\"))",
            "        self.timestamp = self._config.get(\"cache\", \"timestamp\", fallback=None)",
            "        self.key_type = self._config.get(\"cache\", \"key_type\", fallback=None)",
            "        self.ignored_features = set(parse_stringified_list(self._config.get(\"cache\", \"ignored_features\", fallback=\"\")))",
            "        self.mandatory_features = set(",
            "            parse_stringified_list(self._config.get(\"cache\", \"mandatory_features\", fallback=\"\"))",
            "        )",
            "        try:",
            "            self.integrity = dict(self._config.items(\"integrity\"))",
            "            if self._config.get(\"cache\", \"manifest\") != self.integrity.pop(\"manifest\"):",
            "                # The cache config file is updated (parsed with ConfigParser, the state of the ConfigParser",
            "                # is modified and then written out.), not re-created.",
            "                # Thus, older versions will leave our [integrity] section alone, making the section's data invalid.",
            "                # Therefore, we also add the manifest ID to this section and",
            "                # can discern whether an older version interfered by comparing the manifest IDs of this section",
            "                # and the main [cache] section.",
            "                self.integrity = {}",
            "                logger.warning(\"Cache integrity data not available: old Borg version modified the cache.\")",
            "        except configparser.NoSectionError:",
            "            logger.debug(\"Cache integrity: No integrity data found (files, chunks). Cache is from old version.\")",
            "            self.integrity = {}",
            "        previous_location = self._config.get(\"cache\", \"previous_location\", fallback=None)",
            "        if previous_location:",
            "            self.previous_location = recanonicalize_relative_location(previous_location, self.repository)",
            "        else:",
            "            self.previous_location = None",
            "        self._config.set(\"cache\", \"previous_location\", self.repository._location.canonical_path())",
            "",
            "    def save(self, manifest=None, key=None):",
            "        if manifest:",
            "            self._config.set(\"cache\", \"manifest\", manifest.id_str)",
            "            self._config.set(\"cache\", \"timestamp\", manifest.timestamp)",
            "            self._config.set(\"cache\", \"ignored_features\", \",\".join(self.ignored_features))",
            "            self._config.set(\"cache\", \"mandatory_features\", \",\".join(self.mandatory_features))",
            "            if not self._config.has_section(\"integrity\"):",
            "                self._config.add_section(\"integrity\")",
            "            for file, integrity_data in self.integrity.items():",
            "                self._config.set(\"integrity\", file, integrity_data)",
            "            self._config.set(\"integrity\", \"manifest\", manifest.id_str)",
            "        if key:",
            "            self._config.set(\"cache\", \"key_type\", str(key.TYPE))",
            "        with SaveFile(self.config_path) as fd:",
            "            self._config.write(fd)",
            "",
            "    def close(self):",
            "        if self.lock is not None:",
            "            self.lock.release()",
            "            self.lock = None",
            "",
            "    def _check_upgrade(self, config_path):",
            "        try:",
            "            cache_version = self._config.getint(\"cache\", \"version\")",
            "            wanted_version = 1",
            "            if cache_version != wanted_version:",
            "                self.close()",
            "                raise Exception(",
            "                    \"%s has unexpected cache version %d (wanted: %d).\" % (config_path, cache_version, wanted_version)",
            "                )",
            "        except configparser.NoSectionError:",
            "            self.close()",
            "            raise Exception(\"%s does not look like a Borg cache.\" % config_path) from None",
            "",
            "",
            "class Cache:",
            "    \"\"\"Client Side cache\"\"\"",
            "",
            "    class RepositoryIDNotUnique(Error):",
            "        \"\"\"Cache is newer than repository - do you have multiple, independently updated repos with same ID?\"\"\"",
            "",
            "    class RepositoryReplay(Error):",
            "        \"\"\"Cache, or information obtained from the security directory is newer than repository - this is either an attack or unsafe (multiple repos with same ID)\"\"\"",
            "",
            "    class CacheInitAbortedError(Error):",
            "        \"\"\"Cache initialization aborted\"\"\"",
            "",
            "    class RepositoryAccessAborted(Error):",
            "        \"\"\"Repository access aborted\"\"\"",
            "",
            "    class EncryptionMethodMismatch(Error):",
            "        \"\"\"Repository encryption method changed since last access, refusing to continue\"\"\"",
            "",
            "    @staticmethod",
            "    def break_lock(repository, path=None):",
            "        path = cache_dir(repository, path)",
            "        Lock(os.path.join(path, \"lock\"), exclusive=True).break_lock()",
            "",
            "    @staticmethod",
            "    def destroy(repository, path=None):",
            "        \"\"\"destroy the cache for ``repository`` or at ``path``\"\"\"",
            "        path = path or os.path.join(get_cache_dir(), repository.id_str)",
            "        config = os.path.join(path, \"config\")",
            "        if os.path.exists(config):",
            "            os.remove(config)  # kill config first",
            "            shutil.rmtree(path)",
            "",
            "    def __new__(",
            "        cls,",
            "        repository,",
            "        manifest,",
            "        path=None,",
            "        sync=True,",
            "        warn_if_unencrypted=True,",
            "        progress=False,",
            "        lock_wait=None,",
            "        permit_adhoc_cache=False,",
            "        cache_mode=FILES_CACHE_MODE_DISABLED,",
            "        iec=False,",
            "    ):",
            "        def local():",
            "            return LocalCache(",
            "                manifest=manifest,",
            "                path=path,",
            "                sync=sync,",
            "                warn_if_unencrypted=warn_if_unencrypted,",
            "                progress=progress,",
            "                iec=iec,",
            "                lock_wait=lock_wait,",
            "                cache_mode=cache_mode,",
            "            )",
            "",
            "        def adhoc():",
            "            return AdHocCache(manifest=manifest, lock_wait=lock_wait, iec=iec)",
            "",
            "        if not permit_adhoc_cache:",
            "            return local()",
            "",
            "        # ad-hoc cache may be permitted, but if the local cache is in sync it'd be stupid to invalidate",
            "        # it by needlessly using the ad-hoc cache.",
            "        # Check if the local cache exists and is in sync.",
            "",
            "        cache_config = CacheConfig(repository, path, lock_wait)",
            "        if cache_config.exists():",
            "            with cache_config:",
            "                cache_in_sync = cache_config.manifest_id == manifest.id",
            "            # Don't nest cache locks",
            "            if cache_in_sync:",
            "                # Local cache is in sync, use it",
            "                logger.debug(\"Cache: choosing local cache (in sync)\")",
            "                return local()",
            "        logger.debug(\"Cache: choosing ad-hoc cache (local cache does not exist or is not in sync)\")",
            "        return adhoc()",
            "",
            "",
            "class CacheStatsMixin:",
            "    str_format = \"\"\"\\",
            "Original size: {0.total_size}",
            "Deduplicated size: {0.unique_size}",
            "Unique chunks: {0.total_unique_chunks}",
            "Total chunks: {0.total_chunks}",
            "\"\"\"",
            "",
            "    def __init__(self, iec=False):",
            "        self.iec = iec",
            "",
            "    def __str__(self):",
            "        return self.str_format.format(self.format_tuple())",
            "",
            "    Summary = namedtuple(\"Summary\", [\"total_size\", \"unique_size\", \"total_unique_chunks\", \"total_chunks\"])",
            "",
            "    def stats(self):",
            "        from .archive import Archive",
            "",
            "        # XXX: this should really be moved down to `hashindex.pyx`",
            "        total_size, unique_size, total_unique_chunks, total_chunks = self.chunks.summarize()",
            "        # since borg 1.2 we have new archive metadata telling the total size per archive,",
            "        # so we can just sum up all archives to get the \"all archives\" stats:",
            "        total_size = 0",
            "        for archive_name in self.manifest.archives:",
            "            archive = Archive(self.manifest, archive_name)",
            "            stats = archive.calc_stats(self, want_unique=False)",
            "            total_size += stats.osize",
            "        stats = self.Summary(total_size, unique_size, total_unique_chunks, total_chunks)._asdict()",
            "        return stats",
            "",
            "    def format_tuple(self):",
            "        stats = self.stats()",
            "        for field in [\"total_size\", \"unique_size\"]:",
            "            stats[field] = format_file_size(stats[field], iec=self.iec)",
            "        return self.Summary(**stats)",
            "",
            "",
            "class LocalCache(CacheStatsMixin):",
            "    \"\"\"",
            "    Persistent, local (client-side) cache.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        manifest,",
            "        path=None,",
            "        sync=True,",
            "        warn_if_unencrypted=True,",
            "        progress=False,",
            "        lock_wait=None,",
            "        cache_mode=FILES_CACHE_MODE_DISABLED,",
            "        iec=False,",
            "    ):",
            "        \"\"\"",
            "        :param warn_if_unencrypted: print warning if accessing unknown unencrypted repository",
            "        :param lock_wait: timeout for lock acquisition (int [s] or None [wait forever])",
            "        :param sync: do :meth:`.sync`",
            "        :param cache_mode: what shall be compared in the file stat infos vs. cached stat infos comparison",
            "        \"\"\"",
            "        CacheStatsMixin.__init__(self, iec=iec)",
            "        assert isinstance(manifest, Manifest)",
            "        self.manifest = manifest",
            "        self.repository = manifest.repository",
            "        self.key = manifest.key",
            "        self.repo_objs = manifest.repo_objs",
            "        self.progress = progress",
            "        self.cache_mode = cache_mode",
            "        self.timestamp = None",
            "        self.txn_active = False",
            "",
            "        self.path = cache_dir(self.repository, path)",
            "        self.security_manager = SecurityManager(self.repository)",
            "        self.cache_config = CacheConfig(self.repository, self.path, lock_wait)",
            "",
            "        # Warn user before sending data to a never seen before unencrypted repository",
            "        if not os.path.exists(self.path):",
            "            self.security_manager.assert_access_unknown(warn_if_unencrypted, manifest, self.key)",
            "            self.create()",
            "",
            "        self.open()",
            "        try:",
            "            self.security_manager.assert_secure(manifest, self.key, cache_config=self.cache_config)",
            "",
            "            if not self.check_cache_compatibility():",
            "                self.wipe_cache()",
            "",
            "            self.update_compatibility()",
            "",
            "            if sync and self.manifest.id != self.cache_config.manifest_id:",
            "                self.sync()",
            "                self.commit()",
            "        except:  # noqa",
            "            self.close()",
            "            raise",
            "",
            "    def __enter__(self):",
            "        return self",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        self.close()",
            "",
            "    def create(self):",
            "        \"\"\"Create a new empty cache at `self.path`\"\"\"",
            "        os.makedirs(self.path)",
            "        with open(os.path.join(self.path, \"README\"), \"w\") as fd:",
            "            fd.write(CACHE_README)",
            "        self.cache_config.create()",
            "        ChunkIndex().write(os.path.join(self.path, \"chunks\"))",
            "        os.makedirs(os.path.join(self.path, \"chunks.archive.d\"))",
            "        with SaveFile(os.path.join(self.path, files_cache_name()), binary=True):",
            "            pass  # empty file",
            "",
            "    def _do_open(self):",
            "        self.cache_config.load()",
            "        with IntegrityCheckedFile(",
            "            path=os.path.join(self.path, \"chunks\"),",
            "            write=False,",
            "            integrity_data=self.cache_config.integrity.get(\"chunks\"),",
            "        ) as fd:",
            "            self.chunks = ChunkIndex.read(fd)",
            "        if \"d\" in self.cache_mode:  # d(isabled)",
            "            self.files = None",
            "        else:",
            "            self._read_files()",
            "",
            "    def open(self):",
            "        if not os.path.isdir(self.path):",
            "            raise Exception(\"%s Does not look like a Borg cache\" % self.path)",
            "        self.cache_config.open()",
            "        self.rollback()",
            "",
            "    def close(self):",
            "        if self.cache_config is not None:",
            "            self.cache_config.close()",
            "            self.cache_config = None",
            "",
            "    def _read_files(self):",
            "        self.files = {}",
            "        self._newest_cmtime = None",
            "        logger.debug(\"Reading files cache ...\")",
            "        files_cache_logger.debug(\"FILES-CACHE-LOAD: starting...\")",
            "        msg = None",
            "        try:",
            "            with IntegrityCheckedFile(",
            "                path=os.path.join(self.path, files_cache_name()),",
            "                write=False,",
            "                integrity_data=self.cache_config.integrity.get(files_cache_name()),",
            "            ) as fd:",
            "                u = msgpack.Unpacker(use_list=True)",
            "                while True:",
            "                    data = fd.read(64 * 1024)",
            "                    if not data:",
            "                        break",
            "                    u.feed(data)",
            "                    try:",
            "                        for path_hash, item in u:",
            "                            entry = FileCacheEntry(*item)",
            "                            # in the end, this takes about 240 Bytes per file",
            "                            self.files[path_hash] = msgpack.packb(entry._replace(age=entry.age + 1))",
            "                    except (TypeError, ValueError) as exc:",
            "                        msg = \"The files cache seems invalid. [%s]\" % str(exc)",
            "                        break",
            "        except OSError as exc:",
            "            msg = \"The files cache can't be read. [%s]\" % str(exc)",
            "        except FileIntegrityError as fie:",
            "            msg = \"The files cache is corrupted. [%s]\" % str(fie)",
            "        if msg is not None:",
            "            logger.warning(msg)",
            "            logger.warning(\"Continuing without files cache - expect lower performance.\")",
            "            self.files = {}",
            "        files_cache_logger.debug(\"FILES-CACHE-LOAD: finished, %d entries loaded.\", len(self.files))",
            "",
            "    def begin_txn(self):",
            "        # Initialize transaction snapshot",
            "        pi = ProgressIndicatorMessage(msgid=\"cache.begin_transaction\")",
            "        txn_dir = os.path.join(self.path, \"txn.tmp\")",
            "        os.mkdir(txn_dir)",
            "        pi.output(\"Initializing cache transaction: Reading config\")",
            "        shutil.copy(os.path.join(self.path, \"config\"), txn_dir)",
            "        pi.output(\"Initializing cache transaction: Reading chunks\")",
            "        shutil.copy(os.path.join(self.path, \"chunks\"), txn_dir)",
            "        pi.output(\"Initializing cache transaction: Reading files\")",
            "        try:",
            "            shutil.copy(os.path.join(self.path, files_cache_name()), txn_dir)",
            "        except FileNotFoundError:",
            "            with SaveFile(os.path.join(txn_dir, files_cache_name()), binary=True):",
            "                pass  # empty file",
            "        os.replace(txn_dir, os.path.join(self.path, \"txn.active\"))",
            "        self.txn_active = True",
            "        pi.finish()",
            "",
            "    def commit(self):",
            "        \"\"\"Commit transaction\"\"\"",
            "        if not self.txn_active:",
            "            return",
            "        self.security_manager.save(self.manifest, self.key)",
            "        pi = ProgressIndicatorMessage(msgid=\"cache.commit\")",
            "        if self.files is not None:",
            "            if self._newest_cmtime is None:",
            "                # was never set because no files were modified/added",
            "                self._newest_cmtime = 2**63 - 1  # nanoseconds, good until y2262",
            "            ttl = int(os.environ.get(\"BORG_FILES_CACHE_TTL\", 20))",
            "            pi.output(\"Saving files cache\")",
            "            files_cache_logger.debug(\"FILES-CACHE-SAVE: starting...\")",
            "            with IntegrityCheckedFile(path=os.path.join(self.path, files_cache_name()), write=True) as fd:",
            "                entry_count = 0",
            "                for path_hash, item in self.files.items():",
            "                    # Only keep files seen in this backup that are older than newest cmtime seen in this backup -",
            "                    # this is to avoid issues with filesystem snapshots and cmtime granularity.",
            "                    # Also keep files from older backups that have not reached BORG_FILES_CACHE_TTL yet.",
            "                    entry = FileCacheEntry(*msgpack.unpackb(item))",
            "                    if (",
            "                        entry.age == 0",
            "                        and timestamp_to_int(entry.cmtime) < self._newest_cmtime",
            "                        or entry.age > 0",
            "                        and entry.age < ttl",
            "                    ):",
            "                        msgpack.pack((path_hash, entry), fd)",
            "                        entry_count += 1",
            "            files_cache_logger.debug(\"FILES-CACHE-KILL: removed all old entries with age >= TTL [%d]\", ttl)",
            "            files_cache_logger.debug(",
            "                \"FILES-CACHE-KILL: removed all current entries with newest cmtime %d\", self._newest_cmtime",
            "            )",
            "            files_cache_logger.debug(\"FILES-CACHE-SAVE: finished, %d remaining entries saved.\", entry_count)",
            "            self.cache_config.integrity[files_cache_name()] = fd.integrity_data",
            "        pi.output(\"Saving chunks cache\")",
            "        with IntegrityCheckedFile(path=os.path.join(self.path, \"chunks\"), write=True) as fd:",
            "            self.chunks.write(fd)",
            "        self.cache_config.integrity[\"chunks\"] = fd.integrity_data",
            "        pi.output(\"Saving cache config\")",
            "        self.cache_config.save(self.manifest, self.key)",
            "        os.replace(os.path.join(self.path, \"txn.active\"), os.path.join(self.path, \"txn.tmp\"))",
            "        shutil.rmtree(os.path.join(self.path, \"txn.tmp\"))",
            "        self.txn_active = False",
            "        pi.finish()",
            "",
            "    def rollback(self):",
            "        \"\"\"Roll back partial and aborted transactions\"\"\"",
            "        # Remove partial transaction",
            "        if os.path.exists(os.path.join(self.path, \"txn.tmp\")):",
            "            shutil.rmtree(os.path.join(self.path, \"txn.tmp\"))",
            "        # Roll back active transaction",
            "        txn_dir = os.path.join(self.path, \"txn.active\")",
            "        if os.path.exists(txn_dir):",
            "            shutil.copy(os.path.join(txn_dir, \"config\"), self.path)",
            "            shutil.copy(os.path.join(txn_dir, \"chunks\"), self.path)",
            "            shutil.copy(os.path.join(txn_dir, discover_files_cache_name(txn_dir)), self.path)",
            "            txn_tmp = os.path.join(self.path, \"txn.tmp\")",
            "            os.replace(txn_dir, txn_tmp)",
            "            if os.path.exists(txn_tmp):",
            "                shutil.rmtree(txn_tmp)",
            "        self.txn_active = False",
            "        self._do_open()",
            "",
            "    def sync(self):",
            "        \"\"\"Re-synchronize chunks cache with repository.",
            "",
            "        Maintains a directory with known backup archive indexes, so it only",
            "        needs to fetch infos from repo and build a chunk index once per backup",
            "        archive.",
            "        If out of sync, missing archive indexes get added, outdated indexes",
            "        get removed and a new master chunks index is built by merging all",
            "        archive indexes.",
            "        \"\"\"",
            "        archive_path = os.path.join(self.path, \"chunks.archive.d\")",
            "        # Instrumentation",
            "        processed_item_metadata_bytes = 0",
            "        processed_item_metadata_chunks = 0",
            "        compact_chunks_archive_saved_space = 0",
            "",
            "        def mkpath(id, suffix=\"\"):",
            "            id_hex = bin_to_hex(id)",
            "            path = os.path.join(archive_path, id_hex + suffix)",
            "            return path",
            "",
            "        def cached_archives():",
            "            if self.do_cache:",
            "                fns = os.listdir(archive_path)",
            "                # filenames with 64 hex digits == 256bit,",
            "                # or compact indices which are 64 hex digits + \".compact\"",
            "                return {unhexlify(fn) for fn in fns if len(fn) == 64} | {",
            "                    unhexlify(fn[:64]) for fn in fns if len(fn) == 72 and fn.endswith(\".compact\")",
            "                }",
            "            else:",
            "                return set()",
            "",
            "        def repo_archives():",
            "            return {info.id for info in self.manifest.archives.list()}",
            "",
            "        def cleanup_outdated(ids):",
            "            for id in ids:",
            "                cleanup_cached_archive(id)",
            "",
            "        def cleanup_cached_archive(id, cleanup_compact=True):",
            "            try:",
            "                os.unlink(mkpath(id))",
            "                os.unlink(mkpath(id) + \".integrity\")",
            "            except FileNotFoundError:",
            "                pass",
            "            if not cleanup_compact:",
            "                return",
            "            try:",
            "                os.unlink(mkpath(id, suffix=\".compact\"))",
            "                os.unlink(mkpath(id, suffix=\".compact\") + \".integrity\")",
            "            except FileNotFoundError:",
            "                pass",
            "",
            "        def fetch_and_build_idx(archive_id, decrypted_repository, chunk_idx):",
            "            nonlocal processed_item_metadata_bytes",
            "            nonlocal processed_item_metadata_chunks",
            "            csize, data = decrypted_repository.get(archive_id)",
            "            chunk_idx.add(archive_id, 1, len(data))",
            "            archive, verified, _ = self.key.unpack_and_verify_archive(data, force_tam_not_required=True)",
            "            archive = ArchiveItem(internal_dict=archive)",
            "            if archive.version not in (1, 2):  # legacy",
            "                raise Exception(\"Unknown archive metadata version\")",
            "            if archive.version == 1:",
            "                items = archive.items",
            "            elif archive.version == 2:",
            "                items = []",
            "                for chunk_id, (csize, data) in zip(archive.item_ptrs, decrypted_repository.get_many(archive.item_ptrs)):",
            "                    chunk_idx.add(chunk_id, 1, len(data))",
            "                    ids = msgpack.unpackb(data)",
            "                    items.extend(ids)",
            "            sync = CacheSynchronizer(chunk_idx)",
            "            for item_id, (csize, data) in zip(items, decrypted_repository.get_many(items)):",
            "                chunk_idx.add(item_id, 1, len(data))",
            "                processed_item_metadata_bytes += len(data)",
            "                processed_item_metadata_chunks += 1",
            "                sync.feed(data)",
            "            if self.do_cache:",
            "                write_archive_index(archive_id, chunk_idx)",
            "",
            "        def write_archive_index(archive_id, chunk_idx):",
            "            nonlocal compact_chunks_archive_saved_space",
            "            compact_chunks_archive_saved_space += chunk_idx.compact()",
            "            fn = mkpath(archive_id, suffix=\".compact\")",
            "            fn_tmp = mkpath(archive_id, suffix=\".tmp\")",
            "            try:",
            "                with DetachedIntegrityCheckedFile(",
            "                    path=fn_tmp, write=True, filename=bin_to_hex(archive_id) + \".compact\"",
            "                ) as fd:",
            "                    chunk_idx.write(fd)",
            "            except Exception:",
            "                safe_unlink(fn_tmp)",
            "            else:",
            "                os.replace(fn_tmp, fn)",
            "",
            "        def read_archive_index(archive_id, archive_name):",
            "            archive_chunk_idx_path = mkpath(archive_id)",
            "            logger.info(\"Reading cached archive chunk index for %s\", archive_name)",
            "            try:",
            "                try:",
            "                    # Attempt to load compact index first",
            "                    with DetachedIntegrityCheckedFile(path=archive_chunk_idx_path + \".compact\", write=False) as fd:",
            "                        archive_chunk_idx = ChunkIndex.read(fd, permit_compact=True)",
            "                    # In case a non-compact index exists, delete it.",
            "                    cleanup_cached_archive(archive_id, cleanup_compact=False)",
            "                    # Compact index read - return index, no conversion necessary (below).",
            "                    return archive_chunk_idx",
            "                except FileNotFoundError:",
            "                    # No compact index found, load non-compact index, and convert below.",
            "                    with DetachedIntegrityCheckedFile(path=archive_chunk_idx_path, write=False) as fd:",
            "                        archive_chunk_idx = ChunkIndex.read(fd)",
            "            except FileIntegrityError as fie:",
            "                logger.error(\"Cached archive chunk index of %s is corrupted: %s\", archive_name, fie)",
            "                # Delete corrupted index, set warning. A new index must be build.",
            "                cleanup_cached_archive(archive_id)",
            "                set_ec(EXIT_WARNING)",
            "                return None",
            "",
            "            # Convert to compact index. Delete the existing index first.",
            "            logger.debug(\"Found non-compact index for %s, converting to compact.\", archive_name)",
            "            cleanup_cached_archive(archive_id)",
            "            write_archive_index(archive_id, archive_chunk_idx)",
            "            return archive_chunk_idx",
            "",
            "        def get_archive_ids_to_names(archive_ids):",
            "            # Pass once over all archives and build a mapping from ids to names.",
            "            # The easier approach, doing a similar loop for each archive, has",
            "            # square complexity and does about a dozen million functions calls",
            "            # with 1100 archives (which takes 30s CPU seconds _alone_).",
            "            archive_names = {}",
            "            for info in self.manifest.archives.list():",
            "                if info.id in archive_ids:",
            "                    archive_names[info.id] = info.name",
            "            assert len(archive_names) == len(archive_ids)",
            "            return archive_names",
            "",
            "        def create_master_idx(chunk_idx):",
            "            logger.debug(\"Synchronizing chunks index...\")",
            "            cached_ids = cached_archives()",
            "            archive_ids = repo_archives()",
            "            logger.info(",
            "                \"Cached archive chunk indexes: %d fresh, %d stale, %d need fetching.\",",
            "                len(archive_ids & cached_ids),",
            "                len(cached_ids - archive_ids),",
            "                len(archive_ids - cached_ids),",
            "            )",
            "            # deallocates old hashindex, creates empty hashindex:",
            "            chunk_idx.clear()",
            "            cleanup_outdated(cached_ids - archive_ids)",
            "            # Explicitly set the usable initial hash table capacity to avoid performance issues",
            "            # due to hash table \"resonance\".",
            "            master_index_capacity = len(self.repository)",
            "            if archive_ids:",
            "                chunk_idx = None if not self.do_cache else ChunkIndex(usable=master_index_capacity)",
            "                pi = ProgressIndicatorPercent(",
            "                    total=len(archive_ids),",
            "                    step=0.1,",
            "                    msg=\"%3.0f%% Syncing chunks index. Processing archive %s.\",",
            "                    msgid=\"cache.sync\",",
            "                )",
            "                archive_ids_to_names = get_archive_ids_to_names(archive_ids)",
            "                for archive_id, archive_name in archive_ids_to_names.items():",
            "                    pi.show(info=[remove_surrogates(archive_name)])  # legacy. borg2 always has pure unicode arch names.",
            "                    if self.do_cache:",
            "                        if archive_id in cached_ids:",
            "                            archive_chunk_idx = read_archive_index(archive_id, archive_name)",
            "                            if archive_chunk_idx is None:",
            "                                cached_ids.remove(archive_id)",
            "                        if archive_id not in cached_ids:",
            "                            # Do not make this an else branch; the FileIntegrityError exception handler",
            "                            # above can remove *archive_id* from *cached_ids*.",
            "                            logger.info(\"Fetching and building archive index for %s.\", archive_name)",
            "                            archive_chunk_idx = ChunkIndex()",
            "                            fetch_and_build_idx(archive_id, decrypted_repository, archive_chunk_idx)",
            "                        logger.debug(\"Merging into master chunks index.\")",
            "                        chunk_idx.merge(archive_chunk_idx)",
            "                    else:",
            "                        chunk_idx = chunk_idx or ChunkIndex(usable=master_index_capacity)",
            "                        logger.info(\"Fetching archive index for %s.\", archive_name)",
            "                        fetch_and_build_idx(archive_id, decrypted_repository, chunk_idx)",
            "                pi.finish()",
            "                logger.debug(",
            "                    \"Chunks index sync: processed %s (%d chunks) of metadata.\",",
            "                    format_file_size(processed_item_metadata_bytes),",
            "                    processed_item_metadata_chunks,",
            "                )",
            "                logger.debug(",
            "                    \"Chunks index sync: compact chunks.archive.d storage saved %s bytes.\",",
            "                    format_file_size(compact_chunks_archive_saved_space),",
            "                )",
            "            logger.debug(\"Chunks index sync done.\")",
            "            return chunk_idx",
            "",
            "        # The cache can be used by a command that e.g. only checks against Manifest.Operation.WRITE,",
            "        # which does not have to include all flags from Manifest.Operation.READ.",
            "        # Since the sync will attempt to read archives, check compatibility with Manifest.Operation.READ.",
            "        self.manifest.check_repository_compatibility((Manifest.Operation.READ,))",
            "",
            "        self.begin_txn()",
            "        with cache_if_remote(self.repository, decrypted_cache=self.repo_objs) as decrypted_repository:",
            "            # TEMPORARY HACK:",
            "            # to avoid archive index caching, create a FILE named ~/.cache/borg/REPOID/chunks.archive.d -",
            "            # this is only recommended if you have a fast, low latency connection to your repo (e.g. if repo is local).",
            "            self.do_cache = os.path.isdir(archive_path)",
            "            self.chunks = create_master_idx(self.chunks)",
            "",
            "    def check_cache_compatibility(self):",
            "        my_features = Manifest.SUPPORTED_REPO_FEATURES",
            "        if self.cache_config.ignored_features & my_features:",
            "            # The cache might not contain references of chunks that need a feature that is mandatory for some operation",
            "            # and which this version supports. To avoid corruption while executing that operation force rebuild.",
            "            return False",
            "        if not self.cache_config.mandatory_features <= my_features:",
            "            # The cache was build with consideration to at least one feature that this version does not understand.",
            "            # This client might misinterpret the cache. Thus force a rebuild.",
            "            return False",
            "        return True",
            "",
            "    def wipe_cache(self):",
            "        logger.warning(\"Discarding incompatible cache and forcing a cache rebuild\")",
            "        archive_path = os.path.join(self.path, \"chunks.archive.d\")",
            "        if os.path.isdir(archive_path):",
            "            shutil.rmtree(os.path.join(self.path, \"chunks.archive.d\"))",
            "            os.makedirs(os.path.join(self.path, \"chunks.archive.d\"))",
            "        self.chunks = ChunkIndex()",
            "        with SaveFile(os.path.join(self.path, files_cache_name()), binary=True):",
            "            pass  # empty file",
            "        self.cache_config.manifest_id = \"\"",
            "        self.cache_config._config.set(\"cache\", \"manifest\", \"\")",
            "",
            "        self.cache_config.ignored_features = set()",
            "        self.cache_config.mandatory_features = set()",
            "",
            "    def update_compatibility(self):",
            "        operation_to_features_map = self.manifest.get_all_mandatory_features()",
            "        my_features = Manifest.SUPPORTED_REPO_FEATURES",
            "        repo_features = set()",
            "        for operation, features in operation_to_features_map.items():",
            "            repo_features.update(features)",
            "",
            "        self.cache_config.ignored_features.update(repo_features - my_features)",
            "        self.cache_config.mandatory_features.update(repo_features & my_features)",
            "",
            "    def add_chunk(",
            "        self, id, meta, data, *, stats, overwrite=False, wait=True, compress=True, size=None, ctype=None, clevel=None",
            "    ):",
            "        if not self.txn_active:",
            "            self.begin_txn()",
            "        if size is None and compress:",
            "            size = len(data)  # data is still uncompressed",
            "        refcount = self.seen_chunk(id, size)",
            "        if refcount and not overwrite:",
            "            return self.chunk_incref(id, stats)",
            "        if size is None:",
            "            raise ValueError(\"when giving compressed data for a new chunk, the uncompressed size must be given also\")",
            "        cdata = self.repo_objs.format(id, meta, data, compress=compress, size=size, ctype=ctype, clevel=clevel)",
            "        self.repository.put(id, cdata, wait=wait)",
            "        self.chunks.add(id, 1, size)",
            "        stats.update(size, not refcount)",
            "        return ChunkListEntry(id, size)",
            "",
            "    def seen_chunk(self, id, size=None):",
            "        refcount, stored_size = self.chunks.get(id, ChunkIndexEntry(0, None))",
            "        if size is not None and stored_size is not None and size != stored_size:",
            "            # we already have a chunk with that id, but different size.",
            "            # this is either a hash collision (unlikely) or corruption or a bug.",
            "            raise Exception(",
            "                \"chunk has same id [%r], but different size (stored: %d new: %d)!\" % (id, stored_size, size)",
            "            )",
            "        return refcount",
            "",
            "    def chunk_incref(self, id, stats, size=None):",
            "        if not self.txn_active:",
            "            self.begin_txn()",
            "        count, _size = self.chunks.incref(id)",
            "        stats.update(_size, False)",
            "        return ChunkListEntry(id, _size)",
            "",
            "    def chunk_decref(self, id, stats, wait=True):",
            "        if not self.txn_active:",
            "            self.begin_txn()",
            "        count, size = self.chunks.decref(id)",
            "        if count == 0:",
            "            del self.chunks[id]",
            "            self.repository.delete(id, wait=wait)",
            "            stats.update(-size, True)",
            "        else:",
            "            stats.update(-size, False)",
            "",
            "    def file_known_and_unchanged(self, hashed_path, path_hash, st):",
            "        \"\"\"",
            "        Check if we know the file that has this path_hash (know == it is in our files cache) and",
            "        whether it is unchanged (the size/inode number/cmtime is same for stuff we check in this cache_mode).",
            "",
            "        :param hashed_path: the file's path as we gave it to hash(hashed_path)",
            "        :param path_hash: hash(hashed_path), to save some memory in the files cache",
            "        :param st: the file's stat() result",
            "        :return: known, ids (known is True if we have infos about this file in the cache,",
            "                             ids is the list of chunk ids IF the file has not changed, otherwise None).",
            "        \"\"\"",
            "        if not stat.S_ISREG(st.st_mode):",
            "            return False, None",
            "        cache_mode = self.cache_mode",
            "        if \"d\" in cache_mode:  # d(isabled)",
            "            files_cache_logger.debug(\"UNKNOWN: files cache disabled\")",
            "            return False, None",
            "        # note: r(echunk) does not need the files cache in this method, but the files cache will",
            "        # be updated and saved to disk to memorize the files. To preserve previous generations in",
            "        # the cache, this means that it also needs to get loaded from disk first.",
            "        if \"r\" in cache_mode:  # r(echunk)",
            "            files_cache_logger.debug(\"UNKNOWN: rechunking enforced\")",
            "            return False, None",
            "        entry = self.files.get(path_hash)",
            "        if not entry:",
            "            files_cache_logger.debug(\"UNKNOWN: no file metadata in cache for: %r\", hashed_path)",
            "            return False, None",
            "        # we know the file!",
            "        entry = FileCacheEntry(*msgpack.unpackb(entry))",
            "        if \"s\" in cache_mode and entry.size != st.st_size:",
            "            files_cache_logger.debug(\"KNOWN-CHANGED: file size has changed: %r\", hashed_path)",
            "            return True, None",
            "        if \"i\" in cache_mode and entry.inode != st.st_ino:",
            "            files_cache_logger.debug(\"KNOWN-CHANGED: file inode number has changed: %r\", hashed_path)",
            "            return True, None",
            "        if \"c\" in cache_mode and timestamp_to_int(entry.cmtime) != st.st_ctime_ns:",
            "            files_cache_logger.debug(\"KNOWN-CHANGED: file ctime has changed: %r\", hashed_path)",
            "            return True, None",
            "        elif \"m\" in cache_mode and timestamp_to_int(entry.cmtime) != st.st_mtime_ns:",
            "            files_cache_logger.debug(\"KNOWN-CHANGED: file mtime has changed: %r\", hashed_path)",
            "            return True, None",
            "        # we ignored the inode number in the comparison above or it is still same.",
            "        # if it is still the same, replacing it in the tuple doesn't change it.",
            "        # if we ignored it, a reason for doing that is that files were moved to a new",
            "        # disk / new fs (so a one-time change of inode number is expected) and we wanted",
            "        # to avoid everything getting chunked again. to be able to re-enable the inode",
            "        # number comparison in a future backup run (and avoid chunking everything",
            "        # again at that time), we need to update the inode number in the cache with what",
            "        # we see in the filesystem.",
            "        self.files[path_hash] = msgpack.packb(entry._replace(inode=st.st_ino, age=0))",
            "        return True, entry.chunk_ids",
            "",
            "    def memorize_file(self, hashed_path, path_hash, st, ids):",
            "        if not stat.S_ISREG(st.st_mode):",
            "            return",
            "        cache_mode = self.cache_mode",
            "        # note: r(echunk) modes will update the files cache, d(isabled) mode won't",
            "        if \"d\" in cache_mode:",
            "            files_cache_logger.debug(\"FILES-CACHE-NOUPDATE: files cache disabled\")",
            "            return",
            "        if \"c\" in cache_mode:",
            "            cmtime_type = \"ctime\"",
            "            cmtime_ns = safe_ns(st.st_ctime_ns)",
            "        elif \"m\" in cache_mode:",
            "            cmtime_type = \"mtime\"",
            "            cmtime_ns = safe_ns(st.st_mtime_ns)",
            "        else:  # neither 'c' nor 'm' in cache_mode, avoid UnboundLocalError",
            "            cmtime_type = \"ctime\"",
            "            cmtime_ns = safe_ns(st.st_ctime_ns)",
            "        entry = FileCacheEntry(",
            "            age=0, inode=st.st_ino, size=st.st_size, cmtime=int_to_timestamp(cmtime_ns), chunk_ids=ids",
            "        )",
            "        self.files[path_hash] = msgpack.packb(entry)",
            "        self._newest_cmtime = max(self._newest_cmtime or 0, cmtime_ns)",
            "        files_cache_logger.debug(",
            "            \"FILES-CACHE-UPDATE: put %r [has %s] <- %r\",",
            "            entry._replace(chunk_ids=\"[%d entries]\" % len(entry.chunk_ids)),",
            "            cmtime_type,",
            "            hashed_path,",
            "        )",
            "",
            "",
            "class AdHocCache(CacheStatsMixin):",
            "    \"\"\"",
            "    Ad-hoc, non-persistent cache.",
            "",
            "    Compared to the standard LocalCache the AdHocCache does not maintain accurate reference count,",
            "    nor does it provide a files cache (which would require persistence). Chunks that were not added",
            "    during the current AdHocCache lifetime won't have correct size set (0 bytes) and will",
            "    have an infinite reference count (MAX_VALUE).",
            "    \"\"\"",
            "",
            "    str_format = \"\"\"\\",
            "All archives:                unknown              unknown              unknown",
            "",
            "                       Unique chunks         Total chunks",
            "Chunk index:    {0.total_unique_chunks:20d}             unknown\"\"\"",
            "",
            "    def __init__(self, manifest, warn_if_unencrypted=True, lock_wait=None, iec=False):",
            "        CacheStatsMixin.__init__(self, iec=iec)",
            "        assert isinstance(manifest, Manifest)",
            "        self.manifest = manifest",
            "        self.repository = manifest.repository",
            "        self.key = manifest.key",
            "        self.repo_objs = manifest.repo_objs",
            "        self._txn_active = False",
            "",
            "        self.security_manager = SecurityManager(self.repository)",
            "        self.security_manager.assert_secure(manifest, self.key, lock_wait=lock_wait)",
            "",
            "        logger.warning(\"Note: --no-cache-sync is an experimental feature.\")",
            "",
            "    # Public API",
            "",
            "    def __enter__(self):",
            "        return self",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        pass",
            "",
            "    files = None  # type: ignore",
            "    cache_mode = \"d\"",
            "",
            "    def file_known_and_unchanged(self, hashed_path, path_hash, st):",
            "        files_cache_logger.debug(\"UNKNOWN: files cache not implemented\")",
            "        return False, None",
            "",
            "    def memorize_file(self, hashed_path, path_hash, st, ids):",
            "        pass",
            "",
            "    def add_chunk(self, id, meta, data, *, stats, overwrite=False, wait=True, compress=True, size=None):",
            "        assert not overwrite, \"AdHocCache does not permit overwrites \u2014 trying to use it for recreate?\"",
            "        if not self._txn_active:",
            "            self.begin_txn()",
            "        if size is None and compress:",
            "            size = len(data)  # data is still uncompressed",
            "        if size is None:",
            "            raise ValueError(\"when giving compressed data for a chunk, the uncompressed size must be given also\")",
            "        refcount = self.seen_chunk(id, size)",
            "        if refcount:",
            "            return self.chunk_incref(id, stats, size=size)",
            "        cdata = self.repo_objs.format(id, meta, data, compress=compress)",
            "        self.repository.put(id, cdata, wait=wait)",
            "        self.chunks.add(id, 1, size)",
            "        stats.update(size, not refcount)",
            "        return ChunkListEntry(id, size)",
            "",
            "    def seen_chunk(self, id, size=None):",
            "        if not self._txn_active:",
            "            self.begin_txn()",
            "        entry = self.chunks.get(id, ChunkIndexEntry(0, None))",
            "        if entry.refcount and size and not entry.size:",
            "            # The LocalCache has existing size information and uses *size* to make an effort at detecting collisions.",
            "            # This is of course not possible for the AdHocCache.",
            "            # Here *size* is used to update the chunk's size information, which will be zero for existing chunks.",
            "            self.chunks[id] = entry._replace(size=size)",
            "        return entry.refcount",
            "",
            "    def chunk_incref(self, id, stats, size=None):",
            "        if not self._txn_active:",
            "            self.begin_txn()",
            "        count, _size = self.chunks.incref(id)",
            "        # When _size is 0 and size is not given, then this chunk has not been locally visited yet (seen_chunk with",
            "        # size or add_chunk); we can't add references to those (size=0 is invalid) and generally don't try to.",
            "        size = _size or size",
            "        assert size",
            "        stats.update(size, False)",
            "        return ChunkListEntry(id, size)",
            "",
            "    def chunk_decref(self, id, stats, wait=True):",
            "        if not self._txn_active:",
            "            self.begin_txn()",
            "        count, size = self.chunks.decref(id)",
            "        if count == 0:",
            "            del self.chunks[id]",
            "            self.repository.delete(id, wait=wait)",
            "            stats.update(-size, True)",
            "        else:",
            "            stats.update(-size, False)",
            "",
            "    def commit(self):",
            "        if not self._txn_active:",
            "            return",
            "        self.security_manager.save(self.manifest, self.key)",
            "        self._txn_active = False",
            "",
            "    def rollback(self):",
            "        self._txn_active = False",
            "        del self.chunks",
            "",
            "    def begin_txn(self):",
            "        self._txn_active = True",
            "        # Explicitly set the initial usable hash table capacity to avoid performance issues",
            "        # due to hash table \"resonance\".",
            "        # Since we're creating an archive, add 10 % from the start.",
            "        num_chunks = len(self.repository)",
            "        self.chunks = ChunkIndex(usable=num_chunks * 1.1)",
            "        pi = ProgressIndicatorPercent(",
            "            total=num_chunks, msg=\"Downloading chunk list... %3.0f%%\", msgid=\"cache.download_chunks\"",
            "        )",
            "        t0 = perf_counter()",
            "        num_requests = 0",
            "        marker = None",
            "        while True:",
            "            result = self.repository.list(limit=LIST_SCAN_LIMIT, marker=marker)",
            "            num_requests += 1",
            "            if not result:",
            "                break",
            "            pi.show(increase=len(result))",
            "            marker = result[-1]",
            "            # All chunks from the repository have a refcount of MAX_VALUE, which is sticky,",
            "            # therefore we can't/won't delete them. Chunks we added ourselves in this transaction",
            "            # (e.g. checkpoint archives) are tracked correctly.",
            "            init_entry = ChunkIndexEntry(refcount=ChunkIndex.MAX_VALUE, size=0)",
            "            for id_ in result:",
            "                self.chunks[id_] = init_entry",
            "        assert len(self.chunks) == num_chunks",
            "        # LocalCache does not contain the manifest, either.",
            "        del self.chunks[self.manifest.MANIFEST_ID]",
            "        duration = perf_counter() - t0 or 0.01",
            "        pi.finish()",
            "        logger.debug(",
            "            \"AdHocCache: downloaded %d chunk IDs in %.2f s (%d requests), ~%s/s\",",
            "            num_chunks,",
            "            duration,",
            "            num_requests,",
            "            format_file_size(num_chunks * 34 / duration),",
            "        )",
            "        # Chunk IDs in a list are encoded in 34 bytes: 1 byte msgpack header, 1 byte length, 32 ID bytes.",
            "        # Protocol overhead is neglected in this calculation."
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "758": [
                "LocalCache",
                "sync",
                "fetch_and_build_idx"
            ]
        },
        "addLocation": []
    },
    "src/borg/crypto/key.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "     traceback = False"
            },
            "1": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 73,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 74,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+class ArchiveTAMRequiredError(TAMRequiredError):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+    __doc__ = textwrap.dedent("
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+        \"\"\""
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+    Archive '{}' is unauthenticated, but it is required for this repository."
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+    \"\"\""
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 80,
                "PatchRowcode": "+    ).strip()"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+    traceback = False"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 82,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 83,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 84,
                "PatchRowcode": " class TAMInvalid(IntegrityError):"
            },
            "13": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 85,
                "PatchRowcode": "     __doc__ = IntegrityError.__doc__"
            },
            "14": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 86,
                "PatchRowcode": "     traceback = False"
            },
            "15": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 90,
                "PatchRowcode": "         super().__init__(\"Manifest authentication did not verify\")"
            },
            "16": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 91,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 92,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+class ArchiveTAMInvalid(IntegrityError):"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+    __doc__ = IntegrityError.__doc__"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+    traceback = False"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+    def __init__(self):"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+        # Error message becomes: \"Data integrity error: Archive authentication did not verify\""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+        super().__init__(\"Archive authentication did not verify\")"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+"
            },
            "27": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 102,
                "PatchRowcode": " class TAMUnsupportedSuiteError(IntegrityError):"
            },
            "28": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 103,
                "PatchRowcode": "     \"\"\"Could not verify manifest: Unsupported suite {!r}; a newer version is needed.\"\"\""
            },
            "29": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 104,
                "PatchRowcode": " "
            },
            "30": {
                "beforePatchRowNumber": 225,
                "afterPatchRowNumber": 243,
                "PatchRowcode": "             output_length=64,"
            },
            "31": {
                "beforePatchRowNumber": 226,
                "afterPatchRowNumber": 244,
                "PatchRowcode": "         )"
            },
            "32": {
                "beforePatchRowNumber": 227,
                "afterPatchRowNumber": 245,
                "PatchRowcode": " "
            },
            "33": {
                "beforePatchRowNumber": 228,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def pack_and_authenticate_metadata(self, metadata_dict, context=b\"manifest\"):"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 246,
                "PatchRowcode": "+    def pack_and_authenticate_metadata(self, metadata_dict, context=b\"manifest\", salt=None):"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 247,
                "PatchRowcode": "+        if salt is None:"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 248,
                "PatchRowcode": "+            salt = os.urandom(64)"
            },
            "37": {
                "beforePatchRowNumber": 229,
                "afterPatchRowNumber": 249,
                "PatchRowcode": "         metadata_dict = StableDict(metadata_dict)"
            },
            "38": {
                "beforePatchRowNumber": 230,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        tam = metadata_dict[\"tam\"] = StableDict({\"type\": \"HKDF_HMAC_SHA512\", \"hmac\": bytes(64), \"salt\": os.urandom(64)})"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 250,
                "PatchRowcode": "+        tam = metadata_dict[\"tam\"] = StableDict({\"type\": \"HKDF_HMAC_SHA512\", \"hmac\": bytes(64), \"salt\": salt})"
            },
            "40": {
                "beforePatchRowNumber": 231,
                "afterPatchRowNumber": 251,
                "PatchRowcode": "         packed = msgpack.packb(metadata_dict)"
            },
            "41": {
                "beforePatchRowNumber": 232,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        tam_key = self._tam_key(tam[\"salt\"], context)"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 252,
                "PatchRowcode": "+        tam_key = self._tam_key(salt, context)"
            },
            "43": {
                "beforePatchRowNumber": 233,
                "afterPatchRowNumber": 253,
                "PatchRowcode": "         tam[\"hmac\"] = hmac.digest(tam_key, packed, \"sha512\")"
            },
            "44": {
                "beforePatchRowNumber": 234,
                "afterPatchRowNumber": 254,
                "PatchRowcode": "         return msgpack.packb(metadata_dict)"
            },
            "45": {
                "beforePatchRowNumber": 235,
                "afterPatchRowNumber": 255,
                "PatchRowcode": " "
            },
            "46": {
                "beforePatchRowNumber": 252,
                "afterPatchRowNumber": 272,
                "PatchRowcode": "             if tam_required:"
            },
            "47": {
                "beforePatchRowNumber": 253,
                "afterPatchRowNumber": 273,
                "PatchRowcode": "                 raise TAMRequiredError(self.repository._location.canonical_path())"
            },
            "48": {
                "beforePatchRowNumber": 254,
                "afterPatchRowNumber": 274,
                "PatchRowcode": "             else:"
            },
            "49": {
                "beforePatchRowNumber": 255,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                logger.debug(\"TAM not found and not required\")"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 275,
                "PatchRowcode": "+                logger.debug(\"Manifest TAM not found and not required\")"
            },
            "51": {
                "beforePatchRowNumber": 256,
                "afterPatchRowNumber": 276,
                "PatchRowcode": "                 return unpacked, False"
            },
            "52": {
                "beforePatchRowNumber": 257,
                "afterPatchRowNumber": 277,
                "PatchRowcode": "         tam = unpacked.pop(\"tam\", None)"
            },
            "53": {
                "beforePatchRowNumber": 258,
                "afterPatchRowNumber": 278,
                "PatchRowcode": "         if not isinstance(tam, dict):"
            },
            "54": {
                "beforePatchRowNumber": 262,
                "afterPatchRowNumber": 282,
                "PatchRowcode": "             if tam_required:"
            },
            "55": {
                "beforePatchRowNumber": 263,
                "afterPatchRowNumber": 283,
                "PatchRowcode": "                 raise TAMUnsupportedSuiteError(repr(tam_type))"
            },
            "56": {
                "beforePatchRowNumber": 264,
                "afterPatchRowNumber": 284,
                "PatchRowcode": "             else:"
            },
            "57": {
                "beforePatchRowNumber": 265,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                logger.debug(\"Ignoring TAM made with unsupported suite, since TAM is not required: %r\", tam_type)"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 285,
                "PatchRowcode": "+                logger.debug("
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 286,
                "PatchRowcode": "+                    \"Ignoring manifest TAM made with unsupported suite, since TAM is not required: %r\", tam_type"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 287,
                "PatchRowcode": "+                )"
            },
            "61": {
                "beforePatchRowNumber": 266,
                "afterPatchRowNumber": 288,
                "PatchRowcode": "                 return unpacked, False"
            },
            "62": {
                "beforePatchRowNumber": 267,
                "afterPatchRowNumber": 289,
                "PatchRowcode": "         tam_hmac = tam.get(\"hmac\")"
            },
            "63": {
                "beforePatchRowNumber": 268,
                "afterPatchRowNumber": 290,
                "PatchRowcode": "         tam_salt = tam.get(\"salt\")"
            },
            "64": {
                "beforePatchRowNumber": 279,
                "afterPatchRowNumber": 301,
                "PatchRowcode": "         logger.debug(\"TAM-verified manifest\")"
            },
            "65": {
                "beforePatchRowNumber": 280,
                "afterPatchRowNumber": 302,
                "PatchRowcode": "         return unpacked, True"
            },
            "66": {
                "beforePatchRowNumber": 281,
                "afterPatchRowNumber": 303,
                "PatchRowcode": " "
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 304,
                "PatchRowcode": "+    def unpack_and_verify_archive(self, data, force_tam_not_required=False):"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 305,
                "PatchRowcode": "+        \"\"\"Unpack msgpacked *data* and return (object, did_verify).\"\"\""
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 306,
                "PatchRowcode": "+        tam_required = self.tam_required"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 307,
                "PatchRowcode": "+        if force_tam_not_required and tam_required:"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 308,
                "PatchRowcode": "+            # for a long time, borg only checked manifest for \"tam_required\" and"
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 309,
                "PatchRowcode": "+            # people might have archives without TAM, so don't be too annoyingly loud here:"
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 310,
                "PatchRowcode": "+            logger.debug(\"Archive authentication DISABLED.\")"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 311,
                "PatchRowcode": "+            tam_required = False"
            },
            "75": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 312,
                "PatchRowcode": "+        data = bytearray(data)"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 313,
                "PatchRowcode": "+        unpacker = get_limited_unpacker(\"archive\")"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 314,
                "PatchRowcode": "+        unpacker.feed(data)"
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 315,
                "PatchRowcode": "+        unpacked = unpacker.unpack()"
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 316,
                "PatchRowcode": "+        if \"tam\" not in unpacked:"
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 317,
                "PatchRowcode": "+            if tam_required:"
            },
            "81": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 318,
                "PatchRowcode": "+                archive_name = unpacked.get(\"name\", \"<unknown>\")"
            },
            "82": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 319,
                "PatchRowcode": "+                raise ArchiveTAMRequiredError(archive_name)"
            },
            "83": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 320,
                "PatchRowcode": "+            else:"
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 321,
                "PatchRowcode": "+                logger.debug(\"Archive TAM not found and not required\")"
            },
            "85": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 322,
                "PatchRowcode": "+                return unpacked, False, None"
            },
            "86": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 323,
                "PatchRowcode": "+        tam = unpacked.pop(\"tam\", None)"
            },
            "87": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 324,
                "PatchRowcode": "+        if not isinstance(tam, dict):"
            },
            "88": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 325,
                "PatchRowcode": "+            raise ArchiveTAMInvalid()"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 326,
                "PatchRowcode": "+        tam_type = tam.get(\"type\", \"<none>\")"
            },
            "90": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 327,
                "PatchRowcode": "+        if tam_type != \"HKDF_HMAC_SHA512\":"
            },
            "91": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 328,
                "PatchRowcode": "+            if tam_required:"
            },
            "92": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 329,
                "PatchRowcode": "+                raise TAMUnsupportedSuiteError(repr(tam_type))"
            },
            "93": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 330,
                "PatchRowcode": "+            else:"
            },
            "94": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 331,
                "PatchRowcode": "+                logger.debug("
            },
            "95": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 332,
                "PatchRowcode": "+                    \"Ignoring archive TAM made with unsupported suite, since TAM is not required: %r\", tam_type"
            },
            "96": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 333,
                "PatchRowcode": "+                )"
            },
            "97": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 334,
                "PatchRowcode": "+                return unpacked, False, None"
            },
            "98": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 335,
                "PatchRowcode": "+        tam_hmac = tam.get(\"hmac\")"
            },
            "99": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 336,
                "PatchRowcode": "+        tam_salt = tam.get(\"salt\")"
            },
            "100": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 337,
                "PatchRowcode": "+        if not isinstance(tam_salt, (bytes, str)) or not isinstance(tam_hmac, (bytes, str)):"
            },
            "101": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 338,
                "PatchRowcode": "+            raise ArchiveTAMInvalid()"
            },
            "102": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 339,
                "PatchRowcode": "+        tam_hmac = want_bytes(tam_hmac)  # legacy"
            },
            "103": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 340,
                "PatchRowcode": "+        tam_salt = want_bytes(tam_salt)  # legacy"
            },
            "104": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 341,
                "PatchRowcode": "+        offset = data.index(tam_hmac)"
            },
            "105": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 342,
                "PatchRowcode": "+        data[offset : offset + 64] = bytes(64)"
            },
            "106": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 343,
                "PatchRowcode": "+        tam_key = self._tam_key(tam_salt, context=b\"archive\")"
            },
            "107": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 344,
                "PatchRowcode": "+        calculated_hmac = hmac.digest(tam_key, data, \"sha512\")"
            },
            "108": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 345,
                "PatchRowcode": "+        if not hmac.compare_digest(calculated_hmac, tam_hmac):"
            },
            "109": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 346,
                "PatchRowcode": "+            raise ArchiveTAMInvalid()"
            },
            "110": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 347,
                "PatchRowcode": "+        logger.debug(\"TAM-verified archive\")"
            },
            "111": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 348,
                "PatchRowcode": "+        return unpacked, True, tam_salt"
            },
            "112": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 349,
                "PatchRowcode": "+"
            },
            "113": {
                "beforePatchRowNumber": 282,
                "afterPatchRowNumber": 350,
                "PatchRowcode": " "
            },
            "114": {
                "beforePatchRowNumber": 283,
                "afterPatchRowNumber": 351,
                "PatchRowcode": " class PlaintextKey(KeyBase):"
            },
            "115": {
                "beforePatchRowNumber": 284,
                "afterPatchRowNumber": 352,
                "PatchRowcode": "     TYPE = KeyType.PLAINTEXT"
            }
        },
        "frontPatchFile": [
            "import binascii",
            "import hmac",
            "import os",
            "import textwrap",
            "from binascii import a2b_base64, b2a_base64, hexlify",
            "from hashlib import sha256, pbkdf2_hmac",
            "from typing import Literal, Callable, ClassVar",
            "",
            "from ..logger import create_logger",
            "",
            "logger = create_logger()",
            "",
            "import argon2.low_level",
            "",
            "from ..constants import *  # NOQA",
            "from ..helpers import StableDict",
            "from ..helpers import Error, IntegrityError",
            "from ..helpers import get_keys_dir, get_security_dir",
            "from ..helpers import get_limited_unpacker",
            "from ..helpers import bin_to_hex",
            "from ..helpers.passphrase import Passphrase, PasswordRetriesExceeded, PassphraseWrong",
            "from ..helpers import msgpack",
            "from ..helpers import workarounds",
            "from ..item import Key, EncryptedKey, want_bytes",
            "from ..manifest import Manifest",
            "from ..platform import SaveFile",
            "from ..repoobj import RepoObj",
            "",
            "",
            "from .low_level import AES, bytes_to_int, num_cipher_blocks, hmac_sha256, blake2b_256, hkdf_hmac_sha512",
            "from .low_level import AES256_CTR_HMAC_SHA256, AES256_CTR_BLAKE2b, AES256_OCB, CHACHA20_POLY1305",
            "from . import low_level",
            "",
            "# workaround for lost passphrase or key in \"authenticated\" or \"authenticated-blake2\" mode",
            "AUTHENTICATED_NO_KEY = \"authenticated_no_key\" in workarounds",
            "",
            "",
            "class UnsupportedPayloadError(Error):",
            "    \"\"\"Unsupported payload type {}. A newer version is required to access this repository.\"\"\"",
            "",
            "",
            "class UnsupportedManifestError(Error):",
            "    \"\"\"Unsupported manifest envelope. A newer version is required to access this repository.\"\"\"",
            "",
            "",
            "class KeyfileNotFoundError(Error):",
            "    \"\"\"No key file for repository {} found in {}.\"\"\"",
            "",
            "",
            "class KeyfileInvalidError(Error):",
            "    \"\"\"Invalid key file for repository {} found in {}.\"\"\"",
            "",
            "",
            "class KeyfileMismatchError(Error):",
            "    \"\"\"Mismatch between repository {} and key file {}.\"\"\"",
            "",
            "",
            "class RepoKeyNotFoundError(Error):",
            "    \"\"\"No key entry found in the config of repository {}.\"\"\"",
            "",
            "",
            "class UnsupportedKeyFormatError(Error):",
            "    \"\"\"Your borg key is stored in an unsupported format. Try using a newer version of borg.\"\"\"",
            "",
            "",
            "class TAMRequiredError(IntegrityError):",
            "    __doc__ = textwrap.dedent(",
            "        \"\"\"",
            "    Manifest is unauthenticated, but it is required for this repository. Is somebody attacking you?",
            "    \"\"\"",
            "    ).strip()",
            "    traceback = False",
            "",
            "",
            "class TAMInvalid(IntegrityError):",
            "    __doc__ = IntegrityError.__doc__",
            "    traceback = False",
            "",
            "    def __init__(self):",
            "        # Error message becomes: \"Data integrity error: Manifest authentication did not verify\"",
            "        super().__init__(\"Manifest authentication did not verify\")",
            "",
            "",
            "class TAMUnsupportedSuiteError(IntegrityError):",
            "    \"\"\"Could not verify manifest: Unsupported suite {!r}; a newer version is needed.\"\"\"",
            "",
            "    traceback = False",
            "",
            "",
            "def key_creator(repository, args, *, other_key=None):",
            "    for key in AVAILABLE_KEY_TYPES:",
            "        if key.ARG_NAME == args.encryption:",
            "            assert key.ARG_NAME is not None",
            "            return key.create(repository, args, other_key=other_key)",
            "    else:",
            "        raise ValueError('Invalid encryption mode \"%s\"' % args.encryption)",
            "",
            "",
            "def key_argument_names():",
            "    return [key.ARG_NAME for key in AVAILABLE_KEY_TYPES if key.ARG_NAME]",
            "",
            "",
            "def identify_key(manifest_data):",
            "    key_type = manifest_data[0]",
            "    if key_type == KeyType.PASSPHRASE:  # legacy, see comment in KeyType class.",
            "        return RepoKey",
            "",
            "    for key in LEGACY_KEY_TYPES + AVAILABLE_KEY_TYPES:",
            "        if key.TYPE == key_type:",
            "            return key",
            "    else:",
            "        raise UnsupportedPayloadError(key_type)",
            "",
            "",
            "def key_factory(repository, manifest_chunk, *, ro_cls=RepoObj):",
            "    manifest_data = ro_cls.extract_crypted_data(manifest_chunk)",
            "    assert manifest_data, \"manifest data must not be zero bytes long\"",
            "    return identify_key(manifest_data).detect(repository, manifest_data)",
            "",
            "",
            "def tam_required_file(repository):",
            "    security_dir = get_security_dir(bin_to_hex(repository.id), legacy=(repository.version == 1))",
            "    return os.path.join(security_dir, \"tam_required\")",
            "",
            "",
            "def tam_required(repository):",
            "    file = tam_required_file(repository)",
            "    return os.path.isfile(file)",
            "",
            "",
            "def uses_same_chunker_secret(other_key, key):",
            "    \"\"\"is the chunker secret the same?\"\"\"",
            "    # avoid breaking the deduplication by a different chunker secret",
            "    same_chunker_secret = other_key.chunk_seed == key.chunk_seed",
            "    return same_chunker_secret",
            "",
            "",
            "def uses_same_id_hash(other_key, key):",
            "    \"\"\"other_key -> key upgrade: is the id hash the same?\"\"\"",
            "    # avoid breaking the deduplication by changing the id hash",
            "    old_sha256_ids = (PlaintextKey,)",
            "    new_sha256_ids = (PlaintextKey,)",
            "    old_hmac_sha256_ids = (RepoKey, KeyfileKey, AuthenticatedKey)",
            "    new_hmac_sha256_ids = (AESOCBRepoKey, AESOCBKeyfileKey, CHPORepoKey, CHPOKeyfileKey, AuthenticatedKey)",
            "    old_blake2_ids = (Blake2RepoKey, Blake2KeyfileKey, Blake2AuthenticatedKey)",
            "    new_blake2_ids = (",
            "        Blake2AESOCBRepoKey,",
            "        Blake2AESOCBKeyfileKey,",
            "        Blake2CHPORepoKey,",
            "        Blake2CHPOKeyfileKey,",
            "        Blake2AuthenticatedKey,",
            "    )",
            "    same_ids = (",
            "        isinstance(other_key, old_hmac_sha256_ids + new_hmac_sha256_ids)",
            "        and isinstance(key, new_hmac_sha256_ids)",
            "        or isinstance(other_key, old_blake2_ids + new_blake2_ids)",
            "        and isinstance(key, new_blake2_ids)",
            "        or isinstance(other_key, old_sha256_ids + new_sha256_ids)",
            "        and isinstance(key, new_sha256_ids)",
            "    )",
            "    return same_ids",
            "",
            "",
            "class KeyBase:",
            "    # Numeric key type ID, must fit in one byte.",
            "    TYPE: int = None  # override in subclasses",
            "    # set of key type IDs the class can handle as input",
            "    TYPES_ACCEPTABLE: set[int] = None  # override in subclasses",
            "",
            "    # Human-readable name",
            "    NAME = \"UNDEFINED\"",
            "",
            "    # Name used in command line / API (e.g. borg init --encryption=...)",
            "    ARG_NAME = \"UNDEFINED\"",
            "",
            "    # Storage type (no key blob storage / keyfile / repo)",
            "    STORAGE: ClassVar[str] = KeyBlobStorage.NO_STORAGE",
            "",
            "    # Seed for the buzhash chunker (borg.algorithms.chunker.Chunker)",
            "    # type is int",
            "    chunk_seed: int = None",
            "",
            "    # Whether this *particular instance* is encrypted from a practical point of view,",
            "    # i.e. when it's using encryption with a empty passphrase, then",
            "    # that may be *technically* called encryption, but for all intents and purposes",
            "    # that's as good as not encrypting in the first place, and this member should be False.",
            "    #",
            "    # The empty passphrase is also special because Borg tries it first when no passphrase",
            "    # was supplied, and if an empty passphrase works, then Borg won't ask for one.",
            "    logically_encrypted = False",
            "",
            "    def __init__(self, repository):",
            "        self.TYPE_STR = bytes([self.TYPE])",
            "        self.repository = repository",
            "        self.target = None  # key location file path / repo obj",
            "        self.tam_required = True",
            "        self.copy_crypt_key = False",
            "",
            "    def id_hash(self, data):",
            "        \"\"\"Return HMAC hash using the \"id\" HMAC key\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def encrypt(self, id, data):",
            "        pass",
            "",
            "    def decrypt(self, id, data):",
            "        pass",
            "",
            "    def assert_id(self, id, data):",
            "        if id and id != Manifest.MANIFEST_ID:",
            "            id_computed = self.id_hash(data)",
            "            if not hmac.compare_digest(id_computed, id):",
            "                raise IntegrityError(\"Chunk %s: id verification failed\" % bin_to_hex(id))",
            "",
            "    def assert_type(self, type_byte, id=None):",
            "        if type_byte not in self.TYPES_ACCEPTABLE:",
            "            id_str = bin_to_hex(id) if id is not None else \"(unknown)\"",
            "            raise IntegrityError(f\"Chunk {id_str}: Invalid encryption envelope\")",
            "",
            "    def _tam_key(self, salt, context):",
            "        return hkdf_hmac_sha512(",
            "            ikm=self.id_key + self.crypt_key,",
            "            salt=salt,",
            "            info=b\"borg-metadata-authentication-\" + context,",
            "            output_length=64,",
            "        )",
            "",
            "    def pack_and_authenticate_metadata(self, metadata_dict, context=b\"manifest\"):",
            "        metadata_dict = StableDict(metadata_dict)",
            "        tam = metadata_dict[\"tam\"] = StableDict({\"type\": \"HKDF_HMAC_SHA512\", \"hmac\": bytes(64), \"salt\": os.urandom(64)})",
            "        packed = msgpack.packb(metadata_dict)",
            "        tam_key = self._tam_key(tam[\"salt\"], context)",
            "        tam[\"hmac\"] = hmac.digest(tam_key, packed, \"sha512\")",
            "        return msgpack.packb(metadata_dict)",
            "",
            "    def unpack_and_verify_manifest(self, data, force_tam_not_required=False):",
            "        \"\"\"Unpack msgpacked *data* and return (object, did_verify).\"\"\"",
            "        if data.startswith(b\"\\xc1\" * 4):",
            "            # This is a manifest from the future, we can't read it.",
            "            raise UnsupportedManifestError()",
            "        tam_required = self.tam_required",
            "        if force_tam_not_required and tam_required:",
            "            logger.warning(\"Manifest authentication DISABLED.\")",
            "            tam_required = False",
            "        data = bytearray(data)",
            "        unpacker = get_limited_unpacker(\"manifest\")",
            "        unpacker.feed(data)",
            "        unpacked = unpacker.unpack()",
            "        if AUTHENTICATED_NO_KEY:",
            "            return unpacked, True  # True is a lie.",
            "        if \"tam\" not in unpacked:",
            "            if tam_required:",
            "                raise TAMRequiredError(self.repository._location.canonical_path())",
            "            else:",
            "                logger.debug(\"TAM not found and not required\")",
            "                return unpacked, False",
            "        tam = unpacked.pop(\"tam\", None)",
            "        if not isinstance(tam, dict):",
            "            raise TAMInvalid()",
            "        tam_type = tam.get(\"type\", \"<none>\")",
            "        if tam_type != \"HKDF_HMAC_SHA512\":",
            "            if tam_required:",
            "                raise TAMUnsupportedSuiteError(repr(tam_type))",
            "            else:",
            "                logger.debug(\"Ignoring TAM made with unsupported suite, since TAM is not required: %r\", tam_type)",
            "                return unpacked, False",
            "        tam_hmac = tam.get(\"hmac\")",
            "        tam_salt = tam.get(\"salt\")",
            "        if not isinstance(tam_salt, (bytes, str)) or not isinstance(tam_hmac, (bytes, str)):",
            "            raise TAMInvalid()",
            "        tam_hmac = want_bytes(tam_hmac)  # legacy",
            "        tam_salt = want_bytes(tam_salt)  # legacy",
            "        offset = data.index(tam_hmac)",
            "        data[offset : offset + 64] = bytes(64)",
            "        tam_key = self._tam_key(tam_salt, context=b\"manifest\")",
            "        calculated_hmac = hmac.digest(tam_key, data, \"sha512\")",
            "        if not hmac.compare_digest(calculated_hmac, tam_hmac):",
            "            raise TAMInvalid()",
            "        logger.debug(\"TAM-verified manifest\")",
            "        return unpacked, True",
            "",
            "",
            "class PlaintextKey(KeyBase):",
            "    TYPE = KeyType.PLAINTEXT",
            "    TYPES_ACCEPTABLE = {TYPE}",
            "    NAME = \"plaintext\"",
            "    ARG_NAME = \"none\"",
            "",
            "    chunk_seed = 0",
            "    logically_encrypted = False",
            "",
            "    def __init__(self, repository):",
            "        super().__init__(repository)",
            "        self.tam_required = False",
            "",
            "    @classmethod",
            "    def create(cls, repository, args, **kw):",
            "        logger.info('Encryption NOT enabled.\\nUse the \"--encryption=repokey|keyfile\" to enable encryption.')",
            "        return cls(repository)",
            "",
            "    @classmethod",
            "    def detect(cls, repository, manifest_data):",
            "        return cls(repository)",
            "",
            "    def id_hash(self, data):",
            "        return sha256(data).digest()",
            "",
            "    def encrypt(self, id, data):",
            "        return b\"\".join([self.TYPE_STR, data])",
            "",
            "    def decrypt(self, id, data):",
            "        self.assert_type(data[0], id)",
            "        return memoryview(data)[1:]",
            "",
            "    def _tam_key(self, salt, context):",
            "        return salt + context",
            "",
            "",
            "def random_blake2b_256_key():",
            "    # This might look a bit curious, but is the same construction used in the keyed mode of BLAKE2b.",
            "    # Why limit the key to 64 bytes and pad it with 64 nulls nonetheless? The answer is that BLAKE2b",
            "    # has a 128 byte block size, but only 64 bytes of internal state (this is also referred to as a",
            "    # \"local wide pipe\" design, because the compression function transforms (block, state) => state,",
            "    # and len(block) >= len(state), hence wide.)",
            "    # In other words, a key longer than 64 bytes would have simply no advantage, since the function",
            "    # has no way of propagating more than 64 bytes of entropy internally.",
            "    # It's padded to a full block so that the key is never buffered internally by blake2b_update, ie.",
            "    # it remains in a single memory location that can be tracked and could be erased securely, if we",
            "    # wanted to.",
            "    return os.urandom(64) + bytes(64)",
            "",
            "",
            "class ID_BLAKE2b_256:",
            "    \"\"\"",
            "    Key mix-in class for using BLAKE2b-256 for the id key.",
            "",
            "    The id_key length must be 32 bytes.",
            "    \"\"\"",
            "",
            "    def id_hash(self, data):",
            "        return blake2b_256(self.id_key, data)",
            "",
            "    def init_from_random_data(self):",
            "        super().init_from_random_data()",
            "        enc_key = os.urandom(32)",
            "        enc_hmac_key = random_blake2b_256_key()",
            "        self.crypt_key = enc_key + enc_hmac_key",
            "        self.id_key = random_blake2b_256_key()",
            "",
            "",
            "class ID_HMAC_SHA_256:",
            "    \"\"\"",
            "    Key mix-in class for using HMAC-SHA-256 for the id key.",
            "",
            "    The id_key length must be 32 bytes.",
            "    \"\"\"",
            "",
            "    def id_hash(self, data):",
            "        return hmac_sha256(self.id_key, data)",
            "",
            "",
            "class AESKeyBase(KeyBase):",
            "    \"\"\"",
            "    Chunks are encrypted using 256bit AES in Counter Mode (CTR)",
            "",
            "    Payload layout: TYPE(1) + HMAC(32) + NONCE(8) + CIPHERTEXT",
            "",
            "    To reduce payload size only 8 bytes of the 16 bytes nonce is saved",
            "    in the payload, the first 8 bytes are always zeros. This does not",
            "    affect security but limits the maximum repository capacity to",
            "    only 295 exabytes!",
            "    \"\"\"",
            "",
            "    PAYLOAD_OVERHEAD = 1 + 32 + 8  # TYPE + HMAC + NONCE",
            "",
            "    CIPHERSUITE: Callable = None  # override in derived class",
            "",
            "    logically_encrypted = True",
            "",
            "    def encrypt(self, id, data):",
            "        # legacy, this is only used by the tests.",
            "        next_iv = self.cipher.next_iv()",
            "        return self.cipher.encrypt(data, header=self.TYPE_STR, iv=next_iv)",
            "",
            "    def decrypt(self, id, data):",
            "        self.assert_type(data[0], id)",
            "        try:",
            "            return self.cipher.decrypt(data)",
            "        except IntegrityError as e:",
            "            raise IntegrityError(f\"Chunk {bin_to_hex(id)}: Could not decrypt [{str(e)}]\")",
            "",
            "    def init_from_given_data(self, *, crypt_key, id_key, chunk_seed):",
            "        assert len(crypt_key) in (32 + 32, 32 + 128)",
            "        assert len(id_key) in (32, 128)",
            "        assert isinstance(chunk_seed, int)",
            "        self.crypt_key = crypt_key",
            "        self.id_key = id_key",
            "        self.chunk_seed = chunk_seed",
            "",
            "    def init_from_random_data(self):",
            "        data = os.urandom(100)",
            "        chunk_seed = bytes_to_int(data[96:100])",
            "        # Convert to signed int32",
            "        if chunk_seed & 0x80000000:",
            "            chunk_seed = chunk_seed - 0xFFFFFFFF - 1",
            "        self.init_from_given_data(crypt_key=data[0:64], id_key=data[64:96], chunk_seed=chunk_seed)",
            "",
            "    def init_ciphers(self, manifest_data=None):",
            "        enc_key, enc_hmac_key = self.crypt_key[0:32], self.crypt_key[32:]",
            "        self.cipher = self.CIPHERSUITE(mac_key=enc_hmac_key, enc_key=enc_key, header_len=1, aad_offset=1)",
            "        if manifest_data is None:",
            "            nonce = 0",
            "        else:",
            "            self.assert_type(manifest_data[0])",
            "            # manifest_blocks is a safe upper bound on the amount of cipher blocks needed",
            "            # to encrypt the manifest. depending on the ciphersuite and overhead, it might",
            "            # be a bit too high, but that does not matter.",
            "            manifest_blocks = num_cipher_blocks(len(manifest_data))",
            "            nonce = self.cipher.extract_iv(manifest_data) + manifest_blocks",
            "        self.cipher.set_iv(nonce)",
            "",
            "",
            "class FlexiKey:",
            "    FILE_ID = \"BORG_KEY\"",
            "    STORAGE: ClassVar[str] = KeyBlobStorage.NO_STORAGE  # override in subclass",
            "",
            "    @classmethod",
            "    def detect(cls, repository, manifest_data):",
            "        key = cls(repository)",
            "        target = key.find_key()",
            "        prompt = \"Enter passphrase for key %s: \" % target",
            "        passphrase = Passphrase.env_passphrase()",
            "        if passphrase is None:",
            "            passphrase = Passphrase()",
            "            if not key.load(target, passphrase):",
            "                for retry in range(0, 3):",
            "                    passphrase = Passphrase.getpass(prompt)",
            "                    if key.load(target, passphrase):",
            "                        break",
            "                else:",
            "                    raise PasswordRetriesExceeded",
            "        else:",
            "            if not key.load(target, passphrase):",
            "                raise PassphraseWrong",
            "        key.init_ciphers(manifest_data)",
            "        key._passphrase = passphrase",
            "        return key",
            "",
            "    def _load(self, key_data, passphrase):",
            "        cdata = a2b_base64(key_data)",
            "        data = self.decrypt_key_file(cdata, passphrase)",
            "        if data:",
            "            data = msgpack.unpackb(data)",
            "            key = Key(internal_dict=data)",
            "            if key.version not in (1, 2):  # legacy: item.Key can still process v1 keys",
            "                raise UnsupportedKeyFormatError()",
            "            self.repository_id = key.repository_id",
            "            self.crypt_key = key.crypt_key",
            "            self.id_key = key.id_key",
            "            self.chunk_seed = key.chunk_seed",
            "            self.tam_required = key.get(\"tam_required\", tam_required(self.repository))",
            "            return True",
            "        return False",
            "",
            "    def decrypt_key_file(self, data, passphrase):",
            "        unpacker = get_limited_unpacker(\"key\")",
            "        unpacker.feed(data)",
            "        data = unpacker.unpack()",
            "        encrypted_key = EncryptedKey(internal_dict=data)",
            "        if encrypted_key.version != 1:",
            "            raise UnsupportedKeyFormatError()",
            "        else:",
            "            self._encrypted_key_algorithm = encrypted_key.algorithm",
            "            if encrypted_key.algorithm == \"sha256\":",
            "                return self.decrypt_key_file_pbkdf2(encrypted_key, passphrase)",
            "            elif encrypted_key.algorithm == \"argon2 chacha20-poly1305\":",
            "                return self.decrypt_key_file_argon2(encrypted_key, passphrase)",
            "            else:",
            "                raise UnsupportedKeyFormatError()",
            "",
            "    @staticmethod",
            "    def pbkdf2(passphrase, salt, iterations, output_len_in_bytes):",
            "        if os.environ.get(\"BORG_TESTONLY_WEAKEN_KDF\") == \"1\":",
            "            iterations = 1",
            "        return pbkdf2_hmac(\"sha256\", passphrase.encode(\"utf-8\"), salt, iterations, output_len_in_bytes)",
            "",
            "    @staticmethod",
            "    def argon2(",
            "        passphrase: str,",
            "        output_len_in_bytes: int,",
            "        salt: bytes,",
            "        time_cost: int,",
            "        memory_cost: int,",
            "        parallelism: int,",
            "        type: Literal[\"i\", \"d\", \"id\"],",
            "    ) -> bytes:",
            "        if os.environ.get(\"BORG_TESTONLY_WEAKEN_KDF\") == \"1\":",
            "            time_cost = 1",
            "            parallelism = 1",
            "            # 8 is the smallest value that avoids the \"Memory cost is too small\" exception",
            "            memory_cost = 8",
            "        type_map = {\"i\": argon2.low_level.Type.I, \"d\": argon2.low_level.Type.D, \"id\": argon2.low_level.Type.ID}",
            "        key = argon2.low_level.hash_secret_raw(",
            "            secret=passphrase.encode(\"utf-8\"),",
            "            hash_len=output_len_in_bytes,",
            "            salt=salt,",
            "            time_cost=time_cost,",
            "            memory_cost=memory_cost,",
            "            parallelism=parallelism,",
            "            type=type_map[type],",
            "        )",
            "        return key",
            "",
            "    def decrypt_key_file_pbkdf2(self, encrypted_key, passphrase):",
            "        key = self.pbkdf2(passphrase, encrypted_key.salt, encrypted_key.iterations, 32)",
            "        data = AES(key, b\"\\0\" * 16).decrypt(encrypted_key.data)",
            "        if hmac.compare_digest(hmac_sha256(key, data), encrypted_key.hash):",
            "            return data",
            "        return None",
            "",
            "    def decrypt_key_file_argon2(self, encrypted_key, passphrase):",
            "        key = self.argon2(",
            "            passphrase,",
            "            output_len_in_bytes=32,",
            "            salt=encrypted_key.salt,",
            "            time_cost=encrypted_key.argon2_time_cost,",
            "            memory_cost=encrypted_key.argon2_memory_cost,",
            "            parallelism=encrypted_key.argon2_parallelism,",
            "            type=encrypted_key.argon2_type,",
            "        )",
            "        ae_cipher = CHACHA20_POLY1305(key=key, iv=0, header_len=0, aad_offset=0)",
            "        try:",
            "            return ae_cipher.decrypt(encrypted_key.data)",
            "        except low_level.IntegrityError:",
            "            return None",
            "",
            "    def encrypt_key_file(self, data, passphrase, algorithm):",
            "        if algorithm == \"sha256\":",
            "            return self.encrypt_key_file_pbkdf2(data, passphrase)",
            "        elif algorithm == \"argon2 chacha20-poly1305\":",
            "            return self.encrypt_key_file_argon2(data, passphrase)",
            "        else:",
            "            raise ValueError(f\"Unexpected algorithm: {algorithm}\")",
            "",
            "    def encrypt_key_file_pbkdf2(self, data, passphrase):",
            "        salt = os.urandom(32)",
            "        iterations = PBKDF2_ITERATIONS",
            "        key = self.pbkdf2(passphrase, salt, iterations, 32)",
            "        hash = hmac_sha256(key, data)",
            "        cdata = AES(key, b\"\\0\" * 16).encrypt(data)",
            "        enc_key = EncryptedKey(version=1, salt=salt, iterations=iterations, algorithm=\"sha256\", hash=hash, data=cdata)",
            "        return msgpack.packb(enc_key.as_dict())",
            "",
            "    def encrypt_key_file_argon2(self, data, passphrase):",
            "        salt = os.urandom(ARGON2_SALT_BYTES)",
            "        key = self.argon2(passphrase, output_len_in_bytes=32, salt=salt, **ARGON2_ARGS)",
            "        ae_cipher = CHACHA20_POLY1305(key=key, iv=0, header_len=0, aad_offset=0)",
            "        encrypted_key = EncryptedKey(",
            "            version=1,",
            "            algorithm=\"argon2 chacha20-poly1305\",",
            "            salt=salt,",
            "            data=ae_cipher.encrypt(data),",
            "            **{\"argon2_\" + k: v for k, v in ARGON2_ARGS.items()},",
            "        )",
            "        return msgpack.packb(encrypted_key.as_dict())",
            "",
            "    def _save(self, passphrase, algorithm):",
            "        key = Key(",
            "            version=2,",
            "            repository_id=self.repository_id,",
            "            crypt_key=self.crypt_key,",
            "            id_key=self.id_key,",
            "            chunk_seed=self.chunk_seed,",
            "            tam_required=self.tam_required,",
            "        )",
            "        data = self.encrypt_key_file(msgpack.packb(key.as_dict()), passphrase, algorithm)",
            "        key_data = \"\\n\".join(textwrap.wrap(b2a_base64(data).decode(\"ascii\")))",
            "        return key_data",
            "",
            "    def change_passphrase(self, passphrase=None):",
            "        if passphrase is None:",
            "            passphrase = Passphrase.new(allow_empty=True)",
            "        self.save(self.target, passphrase, algorithm=self._encrypted_key_algorithm)",
            "",
            "    @classmethod",
            "    def create(cls, repository, args, *, other_key=None):",
            "        key = cls(repository)",
            "        key.repository_id = repository.id",
            "        if other_key is not None:",
            "            if isinstance(other_key, PlaintextKey):",
            "                raise Error(\"Copying key material from an unencrypted repository is not possible.\")",
            "            if isinstance(key, AESKeyBase):",
            "                # user must use an AEADKeyBase subclass (AEAD modes with session keys)",
            "                raise Error(\"Copying key material to an AES-CTR based mode is insecure and unsupported.\")",
            "            if not uses_same_id_hash(other_key, key):",
            "                raise Error(\"You must keep the same ID hash (HMAC-SHA256 or BLAKE2b) or deduplication will break.\")",
            "            if other_key.copy_crypt_key:",
            "                # give the user the option to use the same authenticated encryption (AE) key",
            "                crypt_key = other_key.crypt_key",
            "            else:",
            "                # borg transfer re-encrypts all data anyway, thus we can default to a new, random AE key",
            "                crypt_key = os.urandom(64)",
            "            key.init_from_given_data(crypt_key=crypt_key, id_key=other_key.id_key, chunk_seed=other_key.chunk_seed)",
            "            passphrase = other_key._passphrase",
            "        else:",
            "            key.init_from_random_data()",
            "            passphrase = Passphrase.new(allow_empty=True)",
            "        key.init_ciphers()",
            "        target = key.get_new_target(args)",
            "        key.save(target, passphrase, create=True, algorithm=KEY_ALGORITHMS[\"argon2\"])",
            "        logger.info('Key in \"%s\" created.' % target)",
            "        logger.info(\"Keep this key safe. Your data will be inaccessible without it.\")",
            "        return key",
            "",
            "    def sanity_check(self, filename, id):",
            "        file_id = self.FILE_ID.encode() + b\" \"",
            "        repo_id = hexlify(id)",
            "        with open(filename, \"rb\") as fd:",
            "            # we do the magic / id check in binary mode to avoid stumbling over",
            "            # decoding errors if somebody has binary files in the keys dir for some reason.",
            "            if fd.read(len(file_id)) != file_id:",
            "                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)",
            "            if fd.read(len(repo_id)) != repo_id:",
            "                raise KeyfileMismatchError(self.repository._location.canonical_path(), filename)",
            "        # we get here if it really looks like a borg key for this repo,",
            "        # do some more checks that are close to how borg reads/parses the key.",
            "        with open(filename, \"r\") as fd:",
            "            lines = fd.readlines()",
            "            if len(lines) < 2:",
            "                logger.warning(f\"borg key sanity check: expected 2+ lines total. [{filename}]\")",
            "                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)",
            "            if len(lines[0].rstrip()) > len(file_id) + len(repo_id):",
            "                logger.warning(f\"borg key sanity check: key line 1 seems too long. [{filename}]\")",
            "                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)",
            "            key_b64 = \"\".join(lines[1:])",
            "            try:",
            "                key = a2b_base64(key_b64)",
            "            except binascii.Error:",
            "                logger.warning(f\"borg key sanity check: key line 2+ does not look like base64. [{filename}]\")",
            "                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)",
            "            if len(key) < 20:",
            "                # this is in no way a precise check, usually we have about 400b key data.",
            "                logger.warning(",
            "                    f\"borg key sanity check: binary encrypted key data from key line 2+ suspiciously short.\"",
            "                    f\" [{filename}]\"",
            "                )",
            "                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)",
            "        # looks good!",
            "        return filename",
            "",
            "    def find_key(self):",
            "        if self.STORAGE == KeyBlobStorage.KEYFILE:",
            "            keyfile = self._find_key_file_from_environment()",
            "            if keyfile is not None:",
            "                return self.sanity_check(keyfile, self.repository.id)",
            "            keyfile = self._find_key_in_keys_dir()",
            "            if keyfile is not None:",
            "                return keyfile",
            "            raise KeyfileNotFoundError(self.repository._location.canonical_path(), get_keys_dir())",
            "        elif self.STORAGE == KeyBlobStorage.REPO:",
            "            loc = self.repository._location.canonical_path()",
            "            key = self.repository.load_key()",
            "            if not key:",
            "                # if we got an empty key, it means there is no key.",
            "                raise RepoKeyNotFoundError(loc) from None",
            "            return loc",
            "        else:",
            "            raise TypeError(\"Unsupported borg key storage type\")",
            "",
            "    def get_existing_or_new_target(self, args):",
            "        keyfile = self._find_key_file_from_environment()",
            "        if keyfile is not None:",
            "            return keyfile",
            "        keyfile = self._find_key_in_keys_dir()",
            "        if keyfile is not None:",
            "            return keyfile",
            "        return self._get_new_target_in_keys_dir(args)",
            "",
            "    def _find_key_in_keys_dir(self):",
            "        id = self.repository.id",
            "        keys_dir = get_keys_dir()",
            "        for name in os.listdir(keys_dir):",
            "            filename = os.path.join(keys_dir, name)",
            "            try:",
            "                return self.sanity_check(filename, id)",
            "            except (KeyfileInvalidError, KeyfileMismatchError):",
            "                pass",
            "",
            "    def get_new_target(self, args):",
            "        if self.STORAGE == KeyBlobStorage.KEYFILE:",
            "            keyfile = self._find_key_file_from_environment()",
            "            if keyfile is not None:",
            "                return keyfile",
            "            return self._get_new_target_in_keys_dir(args)",
            "        elif self.STORAGE == KeyBlobStorage.REPO:",
            "            return self.repository",
            "        else:",
            "            raise TypeError(\"Unsupported borg key storage type\")",
            "",
            "    def _find_key_file_from_environment(self):",
            "        keyfile = os.environ.get(\"BORG_KEY_FILE\")",
            "        if keyfile:",
            "            return os.path.abspath(keyfile)",
            "",
            "    def _get_new_target_in_keys_dir(self, args):",
            "        filename = args.location.to_key_filename()",
            "        path = filename",
            "        i = 1",
            "        while os.path.exists(path):",
            "            i += 1",
            "            path = filename + \".%d\" % i",
            "        return path",
            "",
            "    def load(self, target, passphrase):",
            "        if self.STORAGE == KeyBlobStorage.KEYFILE:",
            "            with open(target) as fd:",
            "                key_data = \"\".join(fd.readlines()[1:])",
            "        elif self.STORAGE == KeyBlobStorage.REPO:",
            "            # While the repository is encrypted, we consider a repokey repository with a blank",
            "            # passphrase an unencrypted repository.",
            "            self.logically_encrypted = passphrase != \"\"",
            "",
            "            # what we get in target is just a repo location, but we already have the repo obj:",
            "            target = self.repository",
            "            key_data = target.load_key()",
            "            if not key_data:",
            "                # if we got an empty key, it means there is no key.",
            "                loc = target._location.canonical_path()",
            "                raise RepoKeyNotFoundError(loc) from None",
            "            key_data = key_data.decode(\"utf-8\")  # remote repo: msgpack issue #99, getting bytes",
            "        else:",
            "            raise TypeError(\"Unsupported borg key storage type\")",
            "        success = self._load(key_data, passphrase)",
            "        if success:",
            "            self.target = target",
            "        return success",
            "",
            "    def save(self, target, passphrase, algorithm, create=False):",
            "        key_data = self._save(passphrase, algorithm)",
            "        if self.STORAGE == KeyBlobStorage.KEYFILE:",
            "            if create and os.path.isfile(target):",
            "                # if a new keyfile key repository is created, ensure that an existing keyfile of another",
            "                # keyfile key repo is not accidentally overwritten by careless use of the BORG_KEY_FILE env var.",
            "                # see issue #6036",
            "                raise Error('Aborting because key in \"%s\" already exists.' % target)",
            "            with SaveFile(target) as fd:",
            "                fd.write(f\"{self.FILE_ID} {bin_to_hex(self.repository_id)}\\n\")",
            "                fd.write(key_data)",
            "                fd.write(\"\\n\")",
            "        elif self.STORAGE == KeyBlobStorage.REPO:",
            "            self.logically_encrypted = passphrase != \"\"",
            "            key_data = key_data.encode(\"utf-8\")  # remote repo: msgpack issue #99, giving bytes",
            "            target.save_key(key_data)",
            "        else:",
            "            raise TypeError(\"Unsupported borg key storage type\")",
            "        self.target = target",
            "",
            "    def remove(self, target):",
            "        if self.STORAGE == KeyBlobStorage.KEYFILE:",
            "            os.remove(target)",
            "        elif self.STORAGE == KeyBlobStorage.REPO:",
            "            target.save_key(b\"\")  # save empty key (no new api at remote repo necessary)",
            "        else:",
            "            raise TypeError(\"Unsupported borg key storage type\")",
            "",
            "",
            "class KeyfileKey(ID_HMAC_SHA_256, AESKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.KEYFILE, KeyType.REPO, KeyType.PASSPHRASE}",
            "    TYPE = KeyType.KEYFILE",
            "    NAME = \"key file\"",
            "    ARG_NAME = \"keyfile\"",
            "    STORAGE = KeyBlobStorage.KEYFILE",
            "    CIPHERSUITE = AES256_CTR_HMAC_SHA256",
            "",
            "",
            "class RepoKey(ID_HMAC_SHA_256, AESKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.KEYFILE, KeyType.REPO, KeyType.PASSPHRASE}",
            "    TYPE = KeyType.REPO",
            "    NAME = \"repokey\"",
            "    ARG_NAME = \"repokey\"",
            "    STORAGE = KeyBlobStorage.REPO",
            "    CIPHERSUITE = AES256_CTR_HMAC_SHA256",
            "",
            "",
            "class Blake2KeyfileKey(ID_BLAKE2b_256, AESKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.BLAKE2KEYFILE, KeyType.BLAKE2REPO}",
            "    TYPE = KeyType.BLAKE2KEYFILE",
            "    NAME = \"key file BLAKE2b\"",
            "    ARG_NAME = \"keyfile-blake2\"",
            "    STORAGE = KeyBlobStorage.KEYFILE",
            "    CIPHERSUITE = AES256_CTR_BLAKE2b",
            "",
            "",
            "class Blake2RepoKey(ID_BLAKE2b_256, AESKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.BLAKE2KEYFILE, KeyType.BLAKE2REPO}",
            "    TYPE = KeyType.BLAKE2REPO",
            "    NAME = \"repokey BLAKE2b\"",
            "    ARG_NAME = \"repokey-blake2\"",
            "    STORAGE = KeyBlobStorage.REPO",
            "    CIPHERSUITE = AES256_CTR_BLAKE2b",
            "",
            "",
            "class AuthenticatedKeyBase(AESKeyBase, FlexiKey):",
            "    STORAGE = KeyBlobStorage.REPO",
            "",
            "    # It's only authenticated, not encrypted.",
            "    logically_encrypted = False",
            "",
            "    def _load(self, key_data, passphrase):",
            "        if AUTHENTICATED_NO_KEY:",
            "            # fake _load if we have no key or passphrase",
            "            NOPE = bytes(32)  # 256 bit all-zero",
            "            self.repository_id = NOPE",
            "            self.enc_key = NOPE",
            "            self.enc_hmac_key = NOPE",
            "            self.id_key = NOPE",
            "            self.chunk_seed = 0",
            "            self.tam_required = False",
            "            return True",
            "        return super()._load(key_data, passphrase)",
            "",
            "    def load(self, target, passphrase):",
            "        success = super().load(target, passphrase)",
            "        self.logically_encrypted = False",
            "        return success",
            "",
            "    def save(self, target, passphrase, algorithm, create=False):",
            "        super().save(target, passphrase, algorithm, create=create)",
            "        self.logically_encrypted = False",
            "",
            "    def init_ciphers(self, manifest_data=None):",
            "        if manifest_data is not None:",
            "            self.assert_type(manifest_data[0])",
            "",
            "    def encrypt(self, id, data):",
            "        return b\"\".join([self.TYPE_STR, data])",
            "",
            "    def decrypt(self, id, data):",
            "        self.assert_type(data[0], id)",
            "        return memoryview(data)[1:]",
            "",
            "",
            "class AuthenticatedKey(ID_HMAC_SHA_256, AuthenticatedKeyBase):",
            "    TYPE = KeyType.AUTHENTICATED",
            "    TYPES_ACCEPTABLE = {TYPE}",
            "    NAME = \"authenticated\"",
            "    ARG_NAME = \"authenticated\"",
            "",
            "",
            "class Blake2AuthenticatedKey(ID_BLAKE2b_256, AuthenticatedKeyBase):",
            "    TYPE = KeyType.BLAKE2AUTHENTICATED",
            "    TYPES_ACCEPTABLE = {TYPE}",
            "    NAME = \"authenticated BLAKE2b\"",
            "    ARG_NAME = \"authenticated-blake2\"",
            "",
            "",
            "# ------------ new crypto ------------",
            "",
            "",
            "class AEADKeyBase(KeyBase):",
            "    \"\"\"",
            "    Chunks are encrypted and authenticated using some AEAD ciphersuite",
            "",
            "    Layout: suite:4 keytype:4 reserved:8 messageIV:48 sessionID:192 auth_tag:128 payload:... [bits]",
            "            ^-------------------- AAD ----------------------------^",
            "    Offsets:0                 1          2            8             32           48 [bytes]",
            "",
            "    suite: 1010b for new AEAD crypto, 0000b is old crypto",
            "    keytype: see constants.KeyType (suite+keytype)",
            "    reserved: all-zero, for future use",
            "    messageIV: a counter starting from 0 for all new encrypted messages of one session",
            "    sessionID: 192bit random, computed once per session (the session key is derived from this)",
            "    auth_tag: authentication tag output of the AEAD cipher (computed over payload and AAD)",
            "    payload: encrypted chunk data",
            "    \"\"\"",
            "",
            "    PAYLOAD_OVERHEAD = 1 + 1 + 6 + 24 + 16  # [bytes], see Layout",
            "",
            "    CIPHERSUITE: Callable = None  # override in subclass",
            "",
            "    logically_encrypted = True",
            "",
            "    MAX_IV = 2**48 - 1",
            "",
            "    def assert_id(self, id, data):",
            "        # Comparing the id hash here would not be needed any more for the new AEAD crypto **IF** we",
            "        # could be sure that chunks were created by normal (not tampered, not evil) borg code:",
            "        # We put the id into AAD when storing the chunk, so it gets into the authentication tag computation.",
            "        # when decrypting, we provide the id we **want** as AAD for the auth tag verification, so",
            "        # decrypting only succeeds if we got the ciphertext we wrote **for that chunk id**.",
            "        # So, basically the **repository** can not cheat on us by giving us a different chunk.",
            "        #",
            "        # **BUT**, if chunks are created by tampered, evil borg code, the borg client code could put",
            "        # a wrong chunkid into AAD and then AEAD-encrypt-and-auth this and store it into the",
            "        # repository using this bad chunkid as key (violating the usual chunkid == id_hash(data)).",
            "        # Later, when reading such a bad chunk, AEAD-auth-and-decrypt would not notice any",
            "        # issue and decrypt successfully.",
            "        # Thus, to notice such evil borg activity, we must check for such violations here:",
            "        if id and id != Manifest.MANIFEST_ID:",
            "            id_computed = self.id_hash(data)",
            "            if not hmac.compare_digest(id_computed, id):",
            "                raise IntegrityError(\"Chunk %s: id verification failed\" % bin_to_hex(id))",
            "",
            "    def encrypt(self, id, data):",
            "        # to encrypt new data in this session we use always self.cipher and self.sessionid",
            "        reserved = b\"\\0\"",
            "        iv = self.cipher.next_iv()",
            "        if iv > self.MAX_IV:  # see the data-structures docs about why the IV range is enough",
            "            raise IntegrityError(\"IV overflow, should never happen.\")",
            "        iv_48bit = iv.to_bytes(6, \"big\")",
            "        header = self.TYPE_STR + reserved + iv_48bit + self.sessionid",
            "        return self.cipher.encrypt(data, header=header, iv=iv, aad=id)",
            "",
            "    def decrypt(self, id, data):",
            "        # to decrypt existing data, we need to get a cipher configured for the sessionid and iv from header",
            "        self.assert_type(data[0], id)",
            "        iv_48bit = data[2:8]",
            "        sessionid = data[8:32]",
            "        iv = int.from_bytes(iv_48bit, \"big\")",
            "        cipher = self._get_cipher(sessionid, iv)",
            "        try:",
            "            return cipher.decrypt(data, aad=id)",
            "        except IntegrityError as e:",
            "            raise IntegrityError(f\"Chunk {bin_to_hex(id)}: Could not decrypt [{str(e)}]\")",
            "",
            "    def init_from_given_data(self, *, crypt_key, id_key, chunk_seed):",
            "        assert len(crypt_key) in (32 + 32, 32 + 128)",
            "        assert len(id_key) in (32, 128)",
            "        assert isinstance(chunk_seed, int)",
            "        self.crypt_key = crypt_key",
            "        self.id_key = id_key",
            "        self.chunk_seed = chunk_seed",
            "",
            "    def init_from_random_data(self):",
            "        data = os.urandom(100)",
            "        chunk_seed = bytes_to_int(data[96:100])",
            "        # Convert to signed int32",
            "        if chunk_seed & 0x80000000:",
            "            chunk_seed = chunk_seed - 0xFFFFFFFF - 1",
            "        self.init_from_given_data(crypt_key=data[0:64], id_key=data[64:96], chunk_seed=chunk_seed)",
            "",
            "    def _get_session_key(self, sessionid):",
            "        assert len(sessionid) == 24  # 192bit",
            "        key = hkdf_hmac_sha512(",
            "            ikm=self.crypt_key,",
            "            salt=sessionid,",
            "            info=b\"borg-session-key-\" + self.CIPHERSUITE.__name__.encode(),",
            "            output_length=32,",
            "        )",
            "        return key",
            "",
            "    def _get_cipher(self, sessionid, iv):",
            "        assert isinstance(iv, int)",
            "        key = self._get_session_key(sessionid)",
            "        cipher = self.CIPHERSUITE(key=key, iv=iv, header_len=1 + 1 + 6 + 24, aad_offset=0)",
            "        return cipher",
            "",
            "    def init_ciphers(self, manifest_data=None, iv=0):",
            "        # in every new session we start with a fresh sessionid and at iv == 0, manifest_data and iv params are ignored",
            "        self.sessionid = os.urandom(24)",
            "        self.cipher = self._get_cipher(self.sessionid, iv=0)",
            "",
            "",
            "class AESOCBKeyfileKey(ID_HMAC_SHA_256, AEADKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.AESOCBKEYFILE, KeyType.AESOCBREPO}",
            "    TYPE = KeyType.AESOCBKEYFILE",
            "    NAME = \"key file AES-OCB\"",
            "    ARG_NAME = \"keyfile-aes-ocb\"",
            "    STORAGE = KeyBlobStorage.KEYFILE",
            "    CIPHERSUITE = AES256_OCB",
            "",
            "",
            "class AESOCBRepoKey(ID_HMAC_SHA_256, AEADKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.AESOCBKEYFILE, KeyType.AESOCBREPO}",
            "    TYPE = KeyType.AESOCBREPO",
            "    NAME = \"repokey AES-OCB\"",
            "    ARG_NAME = \"repokey-aes-ocb\"",
            "    STORAGE = KeyBlobStorage.REPO",
            "    CIPHERSUITE = AES256_OCB",
            "",
            "",
            "class CHPOKeyfileKey(ID_HMAC_SHA_256, AEADKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.CHPOKEYFILE, KeyType.CHPOREPO}",
            "    TYPE = KeyType.CHPOKEYFILE",
            "    NAME = \"key file ChaCha20-Poly1305\"",
            "    ARG_NAME = \"keyfile-chacha20-poly1305\"",
            "    STORAGE = KeyBlobStorage.KEYFILE",
            "    CIPHERSUITE = CHACHA20_POLY1305",
            "",
            "",
            "class CHPORepoKey(ID_HMAC_SHA_256, AEADKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.CHPOKEYFILE, KeyType.CHPOREPO}",
            "    TYPE = KeyType.CHPOREPO",
            "    NAME = \"repokey ChaCha20-Poly1305\"",
            "    ARG_NAME = \"repokey-chacha20-poly1305\"",
            "    STORAGE = KeyBlobStorage.REPO",
            "    CIPHERSUITE = CHACHA20_POLY1305",
            "",
            "",
            "class Blake2AESOCBKeyfileKey(ID_BLAKE2b_256, AEADKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.BLAKE2AESOCBKEYFILE, KeyType.BLAKE2AESOCBREPO}",
            "    TYPE = KeyType.BLAKE2AESOCBKEYFILE",
            "    NAME = \"key file BLAKE2b AES-OCB\"",
            "    ARG_NAME = \"keyfile-blake2-aes-ocb\"",
            "    STORAGE = KeyBlobStorage.KEYFILE",
            "    CIPHERSUITE = AES256_OCB",
            "",
            "",
            "class Blake2AESOCBRepoKey(ID_BLAKE2b_256, AEADKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.BLAKE2AESOCBKEYFILE, KeyType.BLAKE2AESOCBREPO}",
            "    TYPE = KeyType.BLAKE2AESOCBREPO",
            "    NAME = \"repokey BLAKE2b AES-OCB\"",
            "    ARG_NAME = \"repokey-blake2-aes-ocb\"",
            "    STORAGE = KeyBlobStorage.REPO",
            "    CIPHERSUITE = AES256_OCB",
            "",
            "",
            "class Blake2CHPOKeyfileKey(ID_BLAKE2b_256, AEADKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.BLAKE2CHPOKEYFILE, KeyType.BLAKE2CHPOREPO}",
            "    TYPE = KeyType.BLAKE2CHPOKEYFILE",
            "    NAME = \"key file BLAKE2b ChaCha20-Poly1305\"",
            "    ARG_NAME = \"keyfile-blake2-chacha20-poly1305\"",
            "    STORAGE = KeyBlobStorage.KEYFILE",
            "    CIPHERSUITE = CHACHA20_POLY1305",
            "",
            "",
            "class Blake2CHPORepoKey(ID_BLAKE2b_256, AEADKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.BLAKE2CHPOKEYFILE, KeyType.BLAKE2CHPOREPO}",
            "    TYPE = KeyType.BLAKE2CHPOREPO",
            "    NAME = \"repokey BLAKE2b ChaCha20-Poly1305\"",
            "    ARG_NAME = \"repokey-blake2-chacha20-poly1305\"",
            "    STORAGE = KeyBlobStorage.REPO",
            "    CIPHERSUITE = CHACHA20_POLY1305",
            "",
            "",
            "LEGACY_KEY_TYPES = (",
            "    # legacy (AES-CTR based) crypto",
            "    KeyfileKey,",
            "    RepoKey,",
            "    Blake2KeyfileKey,",
            "    Blake2RepoKey,",
            ")",
            "",
            "AVAILABLE_KEY_TYPES = (",
            "    # these are available encryption modes for new repositories",
            "    # not encrypted modes",
            "    PlaintextKey,",
            "    AuthenticatedKey,",
            "    Blake2AuthenticatedKey,",
            "    # new crypto",
            "    AESOCBKeyfileKey,",
            "    AESOCBRepoKey,",
            "    CHPOKeyfileKey,",
            "    CHPORepoKey,",
            "    Blake2AESOCBKeyfileKey,",
            "    Blake2AESOCBRepoKey,",
            "    Blake2CHPOKeyfileKey,",
            "    Blake2CHPORepoKey,",
            ")"
        ],
        "afterPatchFile": [
            "import binascii",
            "import hmac",
            "import os",
            "import textwrap",
            "from binascii import a2b_base64, b2a_base64, hexlify",
            "from hashlib import sha256, pbkdf2_hmac",
            "from typing import Literal, Callable, ClassVar",
            "",
            "from ..logger import create_logger",
            "",
            "logger = create_logger()",
            "",
            "import argon2.low_level",
            "",
            "from ..constants import *  # NOQA",
            "from ..helpers import StableDict",
            "from ..helpers import Error, IntegrityError",
            "from ..helpers import get_keys_dir, get_security_dir",
            "from ..helpers import get_limited_unpacker",
            "from ..helpers import bin_to_hex",
            "from ..helpers.passphrase import Passphrase, PasswordRetriesExceeded, PassphraseWrong",
            "from ..helpers import msgpack",
            "from ..helpers import workarounds",
            "from ..item import Key, EncryptedKey, want_bytes",
            "from ..manifest import Manifest",
            "from ..platform import SaveFile",
            "from ..repoobj import RepoObj",
            "",
            "",
            "from .low_level import AES, bytes_to_int, num_cipher_blocks, hmac_sha256, blake2b_256, hkdf_hmac_sha512",
            "from .low_level import AES256_CTR_HMAC_SHA256, AES256_CTR_BLAKE2b, AES256_OCB, CHACHA20_POLY1305",
            "from . import low_level",
            "",
            "# workaround for lost passphrase or key in \"authenticated\" or \"authenticated-blake2\" mode",
            "AUTHENTICATED_NO_KEY = \"authenticated_no_key\" in workarounds",
            "",
            "",
            "class UnsupportedPayloadError(Error):",
            "    \"\"\"Unsupported payload type {}. A newer version is required to access this repository.\"\"\"",
            "",
            "",
            "class UnsupportedManifestError(Error):",
            "    \"\"\"Unsupported manifest envelope. A newer version is required to access this repository.\"\"\"",
            "",
            "",
            "class KeyfileNotFoundError(Error):",
            "    \"\"\"No key file for repository {} found in {}.\"\"\"",
            "",
            "",
            "class KeyfileInvalidError(Error):",
            "    \"\"\"Invalid key file for repository {} found in {}.\"\"\"",
            "",
            "",
            "class KeyfileMismatchError(Error):",
            "    \"\"\"Mismatch between repository {} and key file {}.\"\"\"",
            "",
            "",
            "class RepoKeyNotFoundError(Error):",
            "    \"\"\"No key entry found in the config of repository {}.\"\"\"",
            "",
            "",
            "class UnsupportedKeyFormatError(Error):",
            "    \"\"\"Your borg key is stored in an unsupported format. Try using a newer version of borg.\"\"\"",
            "",
            "",
            "class TAMRequiredError(IntegrityError):",
            "    __doc__ = textwrap.dedent(",
            "        \"\"\"",
            "    Manifest is unauthenticated, but it is required for this repository. Is somebody attacking you?",
            "    \"\"\"",
            "    ).strip()",
            "    traceback = False",
            "",
            "",
            "class ArchiveTAMRequiredError(TAMRequiredError):",
            "    __doc__ = textwrap.dedent(",
            "        \"\"\"",
            "    Archive '{}' is unauthenticated, but it is required for this repository.",
            "    \"\"\"",
            "    ).strip()",
            "    traceback = False",
            "",
            "",
            "class TAMInvalid(IntegrityError):",
            "    __doc__ = IntegrityError.__doc__",
            "    traceback = False",
            "",
            "    def __init__(self):",
            "        # Error message becomes: \"Data integrity error: Manifest authentication did not verify\"",
            "        super().__init__(\"Manifest authentication did not verify\")",
            "",
            "",
            "class ArchiveTAMInvalid(IntegrityError):",
            "    __doc__ = IntegrityError.__doc__",
            "    traceback = False",
            "",
            "    def __init__(self):",
            "        # Error message becomes: \"Data integrity error: Archive authentication did not verify\"",
            "        super().__init__(\"Archive authentication did not verify\")",
            "",
            "",
            "class TAMUnsupportedSuiteError(IntegrityError):",
            "    \"\"\"Could not verify manifest: Unsupported suite {!r}; a newer version is needed.\"\"\"",
            "",
            "    traceback = False",
            "",
            "",
            "def key_creator(repository, args, *, other_key=None):",
            "    for key in AVAILABLE_KEY_TYPES:",
            "        if key.ARG_NAME == args.encryption:",
            "            assert key.ARG_NAME is not None",
            "            return key.create(repository, args, other_key=other_key)",
            "    else:",
            "        raise ValueError('Invalid encryption mode \"%s\"' % args.encryption)",
            "",
            "",
            "def key_argument_names():",
            "    return [key.ARG_NAME for key in AVAILABLE_KEY_TYPES if key.ARG_NAME]",
            "",
            "",
            "def identify_key(manifest_data):",
            "    key_type = manifest_data[0]",
            "    if key_type == KeyType.PASSPHRASE:  # legacy, see comment in KeyType class.",
            "        return RepoKey",
            "",
            "    for key in LEGACY_KEY_TYPES + AVAILABLE_KEY_TYPES:",
            "        if key.TYPE == key_type:",
            "            return key",
            "    else:",
            "        raise UnsupportedPayloadError(key_type)",
            "",
            "",
            "def key_factory(repository, manifest_chunk, *, ro_cls=RepoObj):",
            "    manifest_data = ro_cls.extract_crypted_data(manifest_chunk)",
            "    assert manifest_data, \"manifest data must not be zero bytes long\"",
            "    return identify_key(manifest_data).detect(repository, manifest_data)",
            "",
            "",
            "def tam_required_file(repository):",
            "    security_dir = get_security_dir(bin_to_hex(repository.id), legacy=(repository.version == 1))",
            "    return os.path.join(security_dir, \"tam_required\")",
            "",
            "",
            "def tam_required(repository):",
            "    file = tam_required_file(repository)",
            "    return os.path.isfile(file)",
            "",
            "",
            "def uses_same_chunker_secret(other_key, key):",
            "    \"\"\"is the chunker secret the same?\"\"\"",
            "    # avoid breaking the deduplication by a different chunker secret",
            "    same_chunker_secret = other_key.chunk_seed == key.chunk_seed",
            "    return same_chunker_secret",
            "",
            "",
            "def uses_same_id_hash(other_key, key):",
            "    \"\"\"other_key -> key upgrade: is the id hash the same?\"\"\"",
            "    # avoid breaking the deduplication by changing the id hash",
            "    old_sha256_ids = (PlaintextKey,)",
            "    new_sha256_ids = (PlaintextKey,)",
            "    old_hmac_sha256_ids = (RepoKey, KeyfileKey, AuthenticatedKey)",
            "    new_hmac_sha256_ids = (AESOCBRepoKey, AESOCBKeyfileKey, CHPORepoKey, CHPOKeyfileKey, AuthenticatedKey)",
            "    old_blake2_ids = (Blake2RepoKey, Blake2KeyfileKey, Blake2AuthenticatedKey)",
            "    new_blake2_ids = (",
            "        Blake2AESOCBRepoKey,",
            "        Blake2AESOCBKeyfileKey,",
            "        Blake2CHPORepoKey,",
            "        Blake2CHPOKeyfileKey,",
            "        Blake2AuthenticatedKey,",
            "    )",
            "    same_ids = (",
            "        isinstance(other_key, old_hmac_sha256_ids + new_hmac_sha256_ids)",
            "        and isinstance(key, new_hmac_sha256_ids)",
            "        or isinstance(other_key, old_blake2_ids + new_blake2_ids)",
            "        and isinstance(key, new_blake2_ids)",
            "        or isinstance(other_key, old_sha256_ids + new_sha256_ids)",
            "        and isinstance(key, new_sha256_ids)",
            "    )",
            "    return same_ids",
            "",
            "",
            "class KeyBase:",
            "    # Numeric key type ID, must fit in one byte.",
            "    TYPE: int = None  # override in subclasses",
            "    # set of key type IDs the class can handle as input",
            "    TYPES_ACCEPTABLE: set[int] = None  # override in subclasses",
            "",
            "    # Human-readable name",
            "    NAME = \"UNDEFINED\"",
            "",
            "    # Name used in command line / API (e.g. borg init --encryption=...)",
            "    ARG_NAME = \"UNDEFINED\"",
            "",
            "    # Storage type (no key blob storage / keyfile / repo)",
            "    STORAGE: ClassVar[str] = KeyBlobStorage.NO_STORAGE",
            "",
            "    # Seed for the buzhash chunker (borg.algorithms.chunker.Chunker)",
            "    # type is int",
            "    chunk_seed: int = None",
            "",
            "    # Whether this *particular instance* is encrypted from a practical point of view,",
            "    # i.e. when it's using encryption with a empty passphrase, then",
            "    # that may be *technically* called encryption, but for all intents and purposes",
            "    # that's as good as not encrypting in the first place, and this member should be False.",
            "    #",
            "    # The empty passphrase is also special because Borg tries it first when no passphrase",
            "    # was supplied, and if an empty passphrase works, then Borg won't ask for one.",
            "    logically_encrypted = False",
            "",
            "    def __init__(self, repository):",
            "        self.TYPE_STR = bytes([self.TYPE])",
            "        self.repository = repository",
            "        self.target = None  # key location file path / repo obj",
            "        self.tam_required = True",
            "        self.copy_crypt_key = False",
            "",
            "    def id_hash(self, data):",
            "        \"\"\"Return HMAC hash using the \"id\" HMAC key\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def encrypt(self, id, data):",
            "        pass",
            "",
            "    def decrypt(self, id, data):",
            "        pass",
            "",
            "    def assert_id(self, id, data):",
            "        if id and id != Manifest.MANIFEST_ID:",
            "            id_computed = self.id_hash(data)",
            "            if not hmac.compare_digest(id_computed, id):",
            "                raise IntegrityError(\"Chunk %s: id verification failed\" % bin_to_hex(id))",
            "",
            "    def assert_type(self, type_byte, id=None):",
            "        if type_byte not in self.TYPES_ACCEPTABLE:",
            "            id_str = bin_to_hex(id) if id is not None else \"(unknown)\"",
            "            raise IntegrityError(f\"Chunk {id_str}: Invalid encryption envelope\")",
            "",
            "    def _tam_key(self, salt, context):",
            "        return hkdf_hmac_sha512(",
            "            ikm=self.id_key + self.crypt_key,",
            "            salt=salt,",
            "            info=b\"borg-metadata-authentication-\" + context,",
            "            output_length=64,",
            "        )",
            "",
            "    def pack_and_authenticate_metadata(self, metadata_dict, context=b\"manifest\", salt=None):",
            "        if salt is None:",
            "            salt = os.urandom(64)",
            "        metadata_dict = StableDict(metadata_dict)",
            "        tam = metadata_dict[\"tam\"] = StableDict({\"type\": \"HKDF_HMAC_SHA512\", \"hmac\": bytes(64), \"salt\": salt})",
            "        packed = msgpack.packb(metadata_dict)",
            "        tam_key = self._tam_key(salt, context)",
            "        tam[\"hmac\"] = hmac.digest(tam_key, packed, \"sha512\")",
            "        return msgpack.packb(metadata_dict)",
            "",
            "    def unpack_and_verify_manifest(self, data, force_tam_not_required=False):",
            "        \"\"\"Unpack msgpacked *data* and return (object, did_verify).\"\"\"",
            "        if data.startswith(b\"\\xc1\" * 4):",
            "            # This is a manifest from the future, we can't read it.",
            "            raise UnsupportedManifestError()",
            "        tam_required = self.tam_required",
            "        if force_tam_not_required and tam_required:",
            "            logger.warning(\"Manifest authentication DISABLED.\")",
            "            tam_required = False",
            "        data = bytearray(data)",
            "        unpacker = get_limited_unpacker(\"manifest\")",
            "        unpacker.feed(data)",
            "        unpacked = unpacker.unpack()",
            "        if AUTHENTICATED_NO_KEY:",
            "            return unpacked, True  # True is a lie.",
            "        if \"tam\" not in unpacked:",
            "            if tam_required:",
            "                raise TAMRequiredError(self.repository._location.canonical_path())",
            "            else:",
            "                logger.debug(\"Manifest TAM not found and not required\")",
            "                return unpacked, False",
            "        tam = unpacked.pop(\"tam\", None)",
            "        if not isinstance(tam, dict):",
            "            raise TAMInvalid()",
            "        tam_type = tam.get(\"type\", \"<none>\")",
            "        if tam_type != \"HKDF_HMAC_SHA512\":",
            "            if tam_required:",
            "                raise TAMUnsupportedSuiteError(repr(tam_type))",
            "            else:",
            "                logger.debug(",
            "                    \"Ignoring manifest TAM made with unsupported suite, since TAM is not required: %r\", tam_type",
            "                )",
            "                return unpacked, False",
            "        tam_hmac = tam.get(\"hmac\")",
            "        tam_salt = tam.get(\"salt\")",
            "        if not isinstance(tam_salt, (bytes, str)) or not isinstance(tam_hmac, (bytes, str)):",
            "            raise TAMInvalid()",
            "        tam_hmac = want_bytes(tam_hmac)  # legacy",
            "        tam_salt = want_bytes(tam_salt)  # legacy",
            "        offset = data.index(tam_hmac)",
            "        data[offset : offset + 64] = bytes(64)",
            "        tam_key = self._tam_key(tam_salt, context=b\"manifest\")",
            "        calculated_hmac = hmac.digest(tam_key, data, \"sha512\")",
            "        if not hmac.compare_digest(calculated_hmac, tam_hmac):",
            "            raise TAMInvalid()",
            "        logger.debug(\"TAM-verified manifest\")",
            "        return unpacked, True",
            "",
            "    def unpack_and_verify_archive(self, data, force_tam_not_required=False):",
            "        \"\"\"Unpack msgpacked *data* and return (object, did_verify).\"\"\"",
            "        tam_required = self.tam_required",
            "        if force_tam_not_required and tam_required:",
            "            # for a long time, borg only checked manifest for \"tam_required\" and",
            "            # people might have archives without TAM, so don't be too annoyingly loud here:",
            "            logger.debug(\"Archive authentication DISABLED.\")",
            "            tam_required = False",
            "        data = bytearray(data)",
            "        unpacker = get_limited_unpacker(\"archive\")",
            "        unpacker.feed(data)",
            "        unpacked = unpacker.unpack()",
            "        if \"tam\" not in unpacked:",
            "            if tam_required:",
            "                archive_name = unpacked.get(\"name\", \"<unknown>\")",
            "                raise ArchiveTAMRequiredError(archive_name)",
            "            else:",
            "                logger.debug(\"Archive TAM not found and not required\")",
            "                return unpacked, False, None",
            "        tam = unpacked.pop(\"tam\", None)",
            "        if not isinstance(tam, dict):",
            "            raise ArchiveTAMInvalid()",
            "        tam_type = tam.get(\"type\", \"<none>\")",
            "        if tam_type != \"HKDF_HMAC_SHA512\":",
            "            if tam_required:",
            "                raise TAMUnsupportedSuiteError(repr(tam_type))",
            "            else:",
            "                logger.debug(",
            "                    \"Ignoring archive TAM made with unsupported suite, since TAM is not required: %r\", tam_type",
            "                )",
            "                return unpacked, False, None",
            "        tam_hmac = tam.get(\"hmac\")",
            "        tam_salt = tam.get(\"salt\")",
            "        if not isinstance(tam_salt, (bytes, str)) or not isinstance(tam_hmac, (bytes, str)):",
            "            raise ArchiveTAMInvalid()",
            "        tam_hmac = want_bytes(tam_hmac)  # legacy",
            "        tam_salt = want_bytes(tam_salt)  # legacy",
            "        offset = data.index(tam_hmac)",
            "        data[offset : offset + 64] = bytes(64)",
            "        tam_key = self._tam_key(tam_salt, context=b\"archive\")",
            "        calculated_hmac = hmac.digest(tam_key, data, \"sha512\")",
            "        if not hmac.compare_digest(calculated_hmac, tam_hmac):",
            "            raise ArchiveTAMInvalid()",
            "        logger.debug(\"TAM-verified archive\")",
            "        return unpacked, True, tam_salt",
            "",
            "",
            "class PlaintextKey(KeyBase):",
            "    TYPE = KeyType.PLAINTEXT",
            "    TYPES_ACCEPTABLE = {TYPE}",
            "    NAME = \"plaintext\"",
            "    ARG_NAME = \"none\"",
            "",
            "    chunk_seed = 0",
            "    logically_encrypted = False",
            "",
            "    def __init__(self, repository):",
            "        super().__init__(repository)",
            "        self.tam_required = False",
            "",
            "    @classmethod",
            "    def create(cls, repository, args, **kw):",
            "        logger.info('Encryption NOT enabled.\\nUse the \"--encryption=repokey|keyfile\" to enable encryption.')",
            "        return cls(repository)",
            "",
            "    @classmethod",
            "    def detect(cls, repository, manifest_data):",
            "        return cls(repository)",
            "",
            "    def id_hash(self, data):",
            "        return sha256(data).digest()",
            "",
            "    def encrypt(self, id, data):",
            "        return b\"\".join([self.TYPE_STR, data])",
            "",
            "    def decrypt(self, id, data):",
            "        self.assert_type(data[0], id)",
            "        return memoryview(data)[1:]",
            "",
            "    def _tam_key(self, salt, context):",
            "        return salt + context",
            "",
            "",
            "def random_blake2b_256_key():",
            "    # This might look a bit curious, but is the same construction used in the keyed mode of BLAKE2b.",
            "    # Why limit the key to 64 bytes and pad it with 64 nulls nonetheless? The answer is that BLAKE2b",
            "    # has a 128 byte block size, but only 64 bytes of internal state (this is also referred to as a",
            "    # \"local wide pipe\" design, because the compression function transforms (block, state) => state,",
            "    # and len(block) >= len(state), hence wide.)",
            "    # In other words, a key longer than 64 bytes would have simply no advantage, since the function",
            "    # has no way of propagating more than 64 bytes of entropy internally.",
            "    # It's padded to a full block so that the key is never buffered internally by blake2b_update, ie.",
            "    # it remains in a single memory location that can be tracked and could be erased securely, if we",
            "    # wanted to.",
            "    return os.urandom(64) + bytes(64)",
            "",
            "",
            "class ID_BLAKE2b_256:",
            "    \"\"\"",
            "    Key mix-in class for using BLAKE2b-256 for the id key.",
            "",
            "    The id_key length must be 32 bytes.",
            "    \"\"\"",
            "",
            "    def id_hash(self, data):",
            "        return blake2b_256(self.id_key, data)",
            "",
            "    def init_from_random_data(self):",
            "        super().init_from_random_data()",
            "        enc_key = os.urandom(32)",
            "        enc_hmac_key = random_blake2b_256_key()",
            "        self.crypt_key = enc_key + enc_hmac_key",
            "        self.id_key = random_blake2b_256_key()",
            "",
            "",
            "class ID_HMAC_SHA_256:",
            "    \"\"\"",
            "    Key mix-in class for using HMAC-SHA-256 for the id key.",
            "",
            "    The id_key length must be 32 bytes.",
            "    \"\"\"",
            "",
            "    def id_hash(self, data):",
            "        return hmac_sha256(self.id_key, data)",
            "",
            "",
            "class AESKeyBase(KeyBase):",
            "    \"\"\"",
            "    Chunks are encrypted using 256bit AES in Counter Mode (CTR)",
            "",
            "    Payload layout: TYPE(1) + HMAC(32) + NONCE(8) + CIPHERTEXT",
            "",
            "    To reduce payload size only 8 bytes of the 16 bytes nonce is saved",
            "    in the payload, the first 8 bytes are always zeros. This does not",
            "    affect security but limits the maximum repository capacity to",
            "    only 295 exabytes!",
            "    \"\"\"",
            "",
            "    PAYLOAD_OVERHEAD = 1 + 32 + 8  # TYPE + HMAC + NONCE",
            "",
            "    CIPHERSUITE: Callable = None  # override in derived class",
            "",
            "    logically_encrypted = True",
            "",
            "    def encrypt(self, id, data):",
            "        # legacy, this is only used by the tests.",
            "        next_iv = self.cipher.next_iv()",
            "        return self.cipher.encrypt(data, header=self.TYPE_STR, iv=next_iv)",
            "",
            "    def decrypt(self, id, data):",
            "        self.assert_type(data[0], id)",
            "        try:",
            "            return self.cipher.decrypt(data)",
            "        except IntegrityError as e:",
            "            raise IntegrityError(f\"Chunk {bin_to_hex(id)}: Could not decrypt [{str(e)}]\")",
            "",
            "    def init_from_given_data(self, *, crypt_key, id_key, chunk_seed):",
            "        assert len(crypt_key) in (32 + 32, 32 + 128)",
            "        assert len(id_key) in (32, 128)",
            "        assert isinstance(chunk_seed, int)",
            "        self.crypt_key = crypt_key",
            "        self.id_key = id_key",
            "        self.chunk_seed = chunk_seed",
            "",
            "    def init_from_random_data(self):",
            "        data = os.urandom(100)",
            "        chunk_seed = bytes_to_int(data[96:100])",
            "        # Convert to signed int32",
            "        if chunk_seed & 0x80000000:",
            "            chunk_seed = chunk_seed - 0xFFFFFFFF - 1",
            "        self.init_from_given_data(crypt_key=data[0:64], id_key=data[64:96], chunk_seed=chunk_seed)",
            "",
            "    def init_ciphers(self, manifest_data=None):",
            "        enc_key, enc_hmac_key = self.crypt_key[0:32], self.crypt_key[32:]",
            "        self.cipher = self.CIPHERSUITE(mac_key=enc_hmac_key, enc_key=enc_key, header_len=1, aad_offset=1)",
            "        if manifest_data is None:",
            "            nonce = 0",
            "        else:",
            "            self.assert_type(manifest_data[0])",
            "            # manifest_blocks is a safe upper bound on the amount of cipher blocks needed",
            "            # to encrypt the manifest. depending on the ciphersuite and overhead, it might",
            "            # be a bit too high, but that does not matter.",
            "            manifest_blocks = num_cipher_blocks(len(manifest_data))",
            "            nonce = self.cipher.extract_iv(manifest_data) + manifest_blocks",
            "        self.cipher.set_iv(nonce)",
            "",
            "",
            "class FlexiKey:",
            "    FILE_ID = \"BORG_KEY\"",
            "    STORAGE: ClassVar[str] = KeyBlobStorage.NO_STORAGE  # override in subclass",
            "",
            "    @classmethod",
            "    def detect(cls, repository, manifest_data):",
            "        key = cls(repository)",
            "        target = key.find_key()",
            "        prompt = \"Enter passphrase for key %s: \" % target",
            "        passphrase = Passphrase.env_passphrase()",
            "        if passphrase is None:",
            "            passphrase = Passphrase()",
            "            if not key.load(target, passphrase):",
            "                for retry in range(0, 3):",
            "                    passphrase = Passphrase.getpass(prompt)",
            "                    if key.load(target, passphrase):",
            "                        break",
            "                else:",
            "                    raise PasswordRetriesExceeded",
            "        else:",
            "            if not key.load(target, passphrase):",
            "                raise PassphraseWrong",
            "        key.init_ciphers(manifest_data)",
            "        key._passphrase = passphrase",
            "        return key",
            "",
            "    def _load(self, key_data, passphrase):",
            "        cdata = a2b_base64(key_data)",
            "        data = self.decrypt_key_file(cdata, passphrase)",
            "        if data:",
            "            data = msgpack.unpackb(data)",
            "            key = Key(internal_dict=data)",
            "            if key.version not in (1, 2):  # legacy: item.Key can still process v1 keys",
            "                raise UnsupportedKeyFormatError()",
            "            self.repository_id = key.repository_id",
            "            self.crypt_key = key.crypt_key",
            "            self.id_key = key.id_key",
            "            self.chunk_seed = key.chunk_seed",
            "            self.tam_required = key.get(\"tam_required\", tam_required(self.repository))",
            "            return True",
            "        return False",
            "",
            "    def decrypt_key_file(self, data, passphrase):",
            "        unpacker = get_limited_unpacker(\"key\")",
            "        unpacker.feed(data)",
            "        data = unpacker.unpack()",
            "        encrypted_key = EncryptedKey(internal_dict=data)",
            "        if encrypted_key.version != 1:",
            "            raise UnsupportedKeyFormatError()",
            "        else:",
            "            self._encrypted_key_algorithm = encrypted_key.algorithm",
            "            if encrypted_key.algorithm == \"sha256\":",
            "                return self.decrypt_key_file_pbkdf2(encrypted_key, passphrase)",
            "            elif encrypted_key.algorithm == \"argon2 chacha20-poly1305\":",
            "                return self.decrypt_key_file_argon2(encrypted_key, passphrase)",
            "            else:",
            "                raise UnsupportedKeyFormatError()",
            "",
            "    @staticmethod",
            "    def pbkdf2(passphrase, salt, iterations, output_len_in_bytes):",
            "        if os.environ.get(\"BORG_TESTONLY_WEAKEN_KDF\") == \"1\":",
            "            iterations = 1",
            "        return pbkdf2_hmac(\"sha256\", passphrase.encode(\"utf-8\"), salt, iterations, output_len_in_bytes)",
            "",
            "    @staticmethod",
            "    def argon2(",
            "        passphrase: str,",
            "        output_len_in_bytes: int,",
            "        salt: bytes,",
            "        time_cost: int,",
            "        memory_cost: int,",
            "        parallelism: int,",
            "        type: Literal[\"i\", \"d\", \"id\"],",
            "    ) -> bytes:",
            "        if os.environ.get(\"BORG_TESTONLY_WEAKEN_KDF\") == \"1\":",
            "            time_cost = 1",
            "            parallelism = 1",
            "            # 8 is the smallest value that avoids the \"Memory cost is too small\" exception",
            "            memory_cost = 8",
            "        type_map = {\"i\": argon2.low_level.Type.I, \"d\": argon2.low_level.Type.D, \"id\": argon2.low_level.Type.ID}",
            "        key = argon2.low_level.hash_secret_raw(",
            "            secret=passphrase.encode(\"utf-8\"),",
            "            hash_len=output_len_in_bytes,",
            "            salt=salt,",
            "            time_cost=time_cost,",
            "            memory_cost=memory_cost,",
            "            parallelism=parallelism,",
            "            type=type_map[type],",
            "        )",
            "        return key",
            "",
            "    def decrypt_key_file_pbkdf2(self, encrypted_key, passphrase):",
            "        key = self.pbkdf2(passphrase, encrypted_key.salt, encrypted_key.iterations, 32)",
            "        data = AES(key, b\"\\0\" * 16).decrypt(encrypted_key.data)",
            "        if hmac.compare_digest(hmac_sha256(key, data), encrypted_key.hash):",
            "            return data",
            "        return None",
            "",
            "    def decrypt_key_file_argon2(self, encrypted_key, passphrase):",
            "        key = self.argon2(",
            "            passphrase,",
            "            output_len_in_bytes=32,",
            "            salt=encrypted_key.salt,",
            "            time_cost=encrypted_key.argon2_time_cost,",
            "            memory_cost=encrypted_key.argon2_memory_cost,",
            "            parallelism=encrypted_key.argon2_parallelism,",
            "            type=encrypted_key.argon2_type,",
            "        )",
            "        ae_cipher = CHACHA20_POLY1305(key=key, iv=0, header_len=0, aad_offset=0)",
            "        try:",
            "            return ae_cipher.decrypt(encrypted_key.data)",
            "        except low_level.IntegrityError:",
            "            return None",
            "",
            "    def encrypt_key_file(self, data, passphrase, algorithm):",
            "        if algorithm == \"sha256\":",
            "            return self.encrypt_key_file_pbkdf2(data, passphrase)",
            "        elif algorithm == \"argon2 chacha20-poly1305\":",
            "            return self.encrypt_key_file_argon2(data, passphrase)",
            "        else:",
            "            raise ValueError(f\"Unexpected algorithm: {algorithm}\")",
            "",
            "    def encrypt_key_file_pbkdf2(self, data, passphrase):",
            "        salt = os.urandom(32)",
            "        iterations = PBKDF2_ITERATIONS",
            "        key = self.pbkdf2(passphrase, salt, iterations, 32)",
            "        hash = hmac_sha256(key, data)",
            "        cdata = AES(key, b\"\\0\" * 16).encrypt(data)",
            "        enc_key = EncryptedKey(version=1, salt=salt, iterations=iterations, algorithm=\"sha256\", hash=hash, data=cdata)",
            "        return msgpack.packb(enc_key.as_dict())",
            "",
            "    def encrypt_key_file_argon2(self, data, passphrase):",
            "        salt = os.urandom(ARGON2_SALT_BYTES)",
            "        key = self.argon2(passphrase, output_len_in_bytes=32, salt=salt, **ARGON2_ARGS)",
            "        ae_cipher = CHACHA20_POLY1305(key=key, iv=0, header_len=0, aad_offset=0)",
            "        encrypted_key = EncryptedKey(",
            "            version=1,",
            "            algorithm=\"argon2 chacha20-poly1305\",",
            "            salt=salt,",
            "            data=ae_cipher.encrypt(data),",
            "            **{\"argon2_\" + k: v for k, v in ARGON2_ARGS.items()},",
            "        )",
            "        return msgpack.packb(encrypted_key.as_dict())",
            "",
            "    def _save(self, passphrase, algorithm):",
            "        key = Key(",
            "            version=2,",
            "            repository_id=self.repository_id,",
            "            crypt_key=self.crypt_key,",
            "            id_key=self.id_key,",
            "            chunk_seed=self.chunk_seed,",
            "            tam_required=self.tam_required,",
            "        )",
            "        data = self.encrypt_key_file(msgpack.packb(key.as_dict()), passphrase, algorithm)",
            "        key_data = \"\\n\".join(textwrap.wrap(b2a_base64(data).decode(\"ascii\")))",
            "        return key_data",
            "",
            "    def change_passphrase(self, passphrase=None):",
            "        if passphrase is None:",
            "            passphrase = Passphrase.new(allow_empty=True)",
            "        self.save(self.target, passphrase, algorithm=self._encrypted_key_algorithm)",
            "",
            "    @classmethod",
            "    def create(cls, repository, args, *, other_key=None):",
            "        key = cls(repository)",
            "        key.repository_id = repository.id",
            "        if other_key is not None:",
            "            if isinstance(other_key, PlaintextKey):",
            "                raise Error(\"Copying key material from an unencrypted repository is not possible.\")",
            "            if isinstance(key, AESKeyBase):",
            "                # user must use an AEADKeyBase subclass (AEAD modes with session keys)",
            "                raise Error(\"Copying key material to an AES-CTR based mode is insecure and unsupported.\")",
            "            if not uses_same_id_hash(other_key, key):",
            "                raise Error(\"You must keep the same ID hash (HMAC-SHA256 or BLAKE2b) or deduplication will break.\")",
            "            if other_key.copy_crypt_key:",
            "                # give the user the option to use the same authenticated encryption (AE) key",
            "                crypt_key = other_key.crypt_key",
            "            else:",
            "                # borg transfer re-encrypts all data anyway, thus we can default to a new, random AE key",
            "                crypt_key = os.urandom(64)",
            "            key.init_from_given_data(crypt_key=crypt_key, id_key=other_key.id_key, chunk_seed=other_key.chunk_seed)",
            "            passphrase = other_key._passphrase",
            "        else:",
            "            key.init_from_random_data()",
            "            passphrase = Passphrase.new(allow_empty=True)",
            "        key.init_ciphers()",
            "        target = key.get_new_target(args)",
            "        key.save(target, passphrase, create=True, algorithm=KEY_ALGORITHMS[\"argon2\"])",
            "        logger.info('Key in \"%s\" created.' % target)",
            "        logger.info(\"Keep this key safe. Your data will be inaccessible without it.\")",
            "        return key",
            "",
            "    def sanity_check(self, filename, id):",
            "        file_id = self.FILE_ID.encode() + b\" \"",
            "        repo_id = hexlify(id)",
            "        with open(filename, \"rb\") as fd:",
            "            # we do the magic / id check in binary mode to avoid stumbling over",
            "            # decoding errors if somebody has binary files in the keys dir for some reason.",
            "            if fd.read(len(file_id)) != file_id:",
            "                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)",
            "            if fd.read(len(repo_id)) != repo_id:",
            "                raise KeyfileMismatchError(self.repository._location.canonical_path(), filename)",
            "        # we get here if it really looks like a borg key for this repo,",
            "        # do some more checks that are close to how borg reads/parses the key.",
            "        with open(filename, \"r\") as fd:",
            "            lines = fd.readlines()",
            "            if len(lines) < 2:",
            "                logger.warning(f\"borg key sanity check: expected 2+ lines total. [{filename}]\")",
            "                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)",
            "            if len(lines[0].rstrip()) > len(file_id) + len(repo_id):",
            "                logger.warning(f\"borg key sanity check: key line 1 seems too long. [{filename}]\")",
            "                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)",
            "            key_b64 = \"\".join(lines[1:])",
            "            try:",
            "                key = a2b_base64(key_b64)",
            "            except binascii.Error:",
            "                logger.warning(f\"borg key sanity check: key line 2+ does not look like base64. [{filename}]\")",
            "                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)",
            "            if len(key) < 20:",
            "                # this is in no way a precise check, usually we have about 400b key data.",
            "                logger.warning(",
            "                    f\"borg key sanity check: binary encrypted key data from key line 2+ suspiciously short.\"",
            "                    f\" [{filename}]\"",
            "                )",
            "                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)",
            "        # looks good!",
            "        return filename",
            "",
            "    def find_key(self):",
            "        if self.STORAGE == KeyBlobStorage.KEYFILE:",
            "            keyfile = self._find_key_file_from_environment()",
            "            if keyfile is not None:",
            "                return self.sanity_check(keyfile, self.repository.id)",
            "            keyfile = self._find_key_in_keys_dir()",
            "            if keyfile is not None:",
            "                return keyfile",
            "            raise KeyfileNotFoundError(self.repository._location.canonical_path(), get_keys_dir())",
            "        elif self.STORAGE == KeyBlobStorage.REPO:",
            "            loc = self.repository._location.canonical_path()",
            "            key = self.repository.load_key()",
            "            if not key:",
            "                # if we got an empty key, it means there is no key.",
            "                raise RepoKeyNotFoundError(loc) from None",
            "            return loc",
            "        else:",
            "            raise TypeError(\"Unsupported borg key storage type\")",
            "",
            "    def get_existing_or_new_target(self, args):",
            "        keyfile = self._find_key_file_from_environment()",
            "        if keyfile is not None:",
            "            return keyfile",
            "        keyfile = self._find_key_in_keys_dir()",
            "        if keyfile is not None:",
            "            return keyfile",
            "        return self._get_new_target_in_keys_dir(args)",
            "",
            "    def _find_key_in_keys_dir(self):",
            "        id = self.repository.id",
            "        keys_dir = get_keys_dir()",
            "        for name in os.listdir(keys_dir):",
            "            filename = os.path.join(keys_dir, name)",
            "            try:",
            "                return self.sanity_check(filename, id)",
            "            except (KeyfileInvalidError, KeyfileMismatchError):",
            "                pass",
            "",
            "    def get_new_target(self, args):",
            "        if self.STORAGE == KeyBlobStorage.KEYFILE:",
            "            keyfile = self._find_key_file_from_environment()",
            "            if keyfile is not None:",
            "                return keyfile",
            "            return self._get_new_target_in_keys_dir(args)",
            "        elif self.STORAGE == KeyBlobStorage.REPO:",
            "            return self.repository",
            "        else:",
            "            raise TypeError(\"Unsupported borg key storage type\")",
            "",
            "    def _find_key_file_from_environment(self):",
            "        keyfile = os.environ.get(\"BORG_KEY_FILE\")",
            "        if keyfile:",
            "            return os.path.abspath(keyfile)",
            "",
            "    def _get_new_target_in_keys_dir(self, args):",
            "        filename = args.location.to_key_filename()",
            "        path = filename",
            "        i = 1",
            "        while os.path.exists(path):",
            "            i += 1",
            "            path = filename + \".%d\" % i",
            "        return path",
            "",
            "    def load(self, target, passphrase):",
            "        if self.STORAGE == KeyBlobStorage.KEYFILE:",
            "            with open(target) as fd:",
            "                key_data = \"\".join(fd.readlines()[1:])",
            "        elif self.STORAGE == KeyBlobStorage.REPO:",
            "            # While the repository is encrypted, we consider a repokey repository with a blank",
            "            # passphrase an unencrypted repository.",
            "            self.logically_encrypted = passphrase != \"\"",
            "",
            "            # what we get in target is just a repo location, but we already have the repo obj:",
            "            target = self.repository",
            "            key_data = target.load_key()",
            "            if not key_data:",
            "                # if we got an empty key, it means there is no key.",
            "                loc = target._location.canonical_path()",
            "                raise RepoKeyNotFoundError(loc) from None",
            "            key_data = key_data.decode(\"utf-8\")  # remote repo: msgpack issue #99, getting bytes",
            "        else:",
            "            raise TypeError(\"Unsupported borg key storage type\")",
            "        success = self._load(key_data, passphrase)",
            "        if success:",
            "            self.target = target",
            "        return success",
            "",
            "    def save(self, target, passphrase, algorithm, create=False):",
            "        key_data = self._save(passphrase, algorithm)",
            "        if self.STORAGE == KeyBlobStorage.KEYFILE:",
            "            if create and os.path.isfile(target):",
            "                # if a new keyfile key repository is created, ensure that an existing keyfile of another",
            "                # keyfile key repo is not accidentally overwritten by careless use of the BORG_KEY_FILE env var.",
            "                # see issue #6036",
            "                raise Error('Aborting because key in \"%s\" already exists.' % target)",
            "            with SaveFile(target) as fd:",
            "                fd.write(f\"{self.FILE_ID} {bin_to_hex(self.repository_id)}\\n\")",
            "                fd.write(key_data)",
            "                fd.write(\"\\n\")",
            "        elif self.STORAGE == KeyBlobStorage.REPO:",
            "            self.logically_encrypted = passphrase != \"\"",
            "            key_data = key_data.encode(\"utf-8\")  # remote repo: msgpack issue #99, giving bytes",
            "            target.save_key(key_data)",
            "        else:",
            "            raise TypeError(\"Unsupported borg key storage type\")",
            "        self.target = target",
            "",
            "    def remove(self, target):",
            "        if self.STORAGE == KeyBlobStorage.KEYFILE:",
            "            os.remove(target)",
            "        elif self.STORAGE == KeyBlobStorage.REPO:",
            "            target.save_key(b\"\")  # save empty key (no new api at remote repo necessary)",
            "        else:",
            "            raise TypeError(\"Unsupported borg key storage type\")",
            "",
            "",
            "class KeyfileKey(ID_HMAC_SHA_256, AESKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.KEYFILE, KeyType.REPO, KeyType.PASSPHRASE}",
            "    TYPE = KeyType.KEYFILE",
            "    NAME = \"key file\"",
            "    ARG_NAME = \"keyfile\"",
            "    STORAGE = KeyBlobStorage.KEYFILE",
            "    CIPHERSUITE = AES256_CTR_HMAC_SHA256",
            "",
            "",
            "class RepoKey(ID_HMAC_SHA_256, AESKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.KEYFILE, KeyType.REPO, KeyType.PASSPHRASE}",
            "    TYPE = KeyType.REPO",
            "    NAME = \"repokey\"",
            "    ARG_NAME = \"repokey\"",
            "    STORAGE = KeyBlobStorage.REPO",
            "    CIPHERSUITE = AES256_CTR_HMAC_SHA256",
            "",
            "",
            "class Blake2KeyfileKey(ID_BLAKE2b_256, AESKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.BLAKE2KEYFILE, KeyType.BLAKE2REPO}",
            "    TYPE = KeyType.BLAKE2KEYFILE",
            "    NAME = \"key file BLAKE2b\"",
            "    ARG_NAME = \"keyfile-blake2\"",
            "    STORAGE = KeyBlobStorage.KEYFILE",
            "    CIPHERSUITE = AES256_CTR_BLAKE2b",
            "",
            "",
            "class Blake2RepoKey(ID_BLAKE2b_256, AESKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.BLAKE2KEYFILE, KeyType.BLAKE2REPO}",
            "    TYPE = KeyType.BLAKE2REPO",
            "    NAME = \"repokey BLAKE2b\"",
            "    ARG_NAME = \"repokey-blake2\"",
            "    STORAGE = KeyBlobStorage.REPO",
            "    CIPHERSUITE = AES256_CTR_BLAKE2b",
            "",
            "",
            "class AuthenticatedKeyBase(AESKeyBase, FlexiKey):",
            "    STORAGE = KeyBlobStorage.REPO",
            "",
            "    # It's only authenticated, not encrypted.",
            "    logically_encrypted = False",
            "",
            "    def _load(self, key_data, passphrase):",
            "        if AUTHENTICATED_NO_KEY:",
            "            # fake _load if we have no key or passphrase",
            "            NOPE = bytes(32)  # 256 bit all-zero",
            "            self.repository_id = NOPE",
            "            self.enc_key = NOPE",
            "            self.enc_hmac_key = NOPE",
            "            self.id_key = NOPE",
            "            self.chunk_seed = 0",
            "            self.tam_required = False",
            "            return True",
            "        return super()._load(key_data, passphrase)",
            "",
            "    def load(self, target, passphrase):",
            "        success = super().load(target, passphrase)",
            "        self.logically_encrypted = False",
            "        return success",
            "",
            "    def save(self, target, passphrase, algorithm, create=False):",
            "        super().save(target, passphrase, algorithm, create=create)",
            "        self.logically_encrypted = False",
            "",
            "    def init_ciphers(self, manifest_data=None):",
            "        if manifest_data is not None:",
            "            self.assert_type(manifest_data[0])",
            "",
            "    def encrypt(self, id, data):",
            "        return b\"\".join([self.TYPE_STR, data])",
            "",
            "    def decrypt(self, id, data):",
            "        self.assert_type(data[0], id)",
            "        return memoryview(data)[1:]",
            "",
            "",
            "class AuthenticatedKey(ID_HMAC_SHA_256, AuthenticatedKeyBase):",
            "    TYPE = KeyType.AUTHENTICATED",
            "    TYPES_ACCEPTABLE = {TYPE}",
            "    NAME = \"authenticated\"",
            "    ARG_NAME = \"authenticated\"",
            "",
            "",
            "class Blake2AuthenticatedKey(ID_BLAKE2b_256, AuthenticatedKeyBase):",
            "    TYPE = KeyType.BLAKE2AUTHENTICATED",
            "    TYPES_ACCEPTABLE = {TYPE}",
            "    NAME = \"authenticated BLAKE2b\"",
            "    ARG_NAME = \"authenticated-blake2\"",
            "",
            "",
            "# ------------ new crypto ------------",
            "",
            "",
            "class AEADKeyBase(KeyBase):",
            "    \"\"\"",
            "    Chunks are encrypted and authenticated using some AEAD ciphersuite",
            "",
            "    Layout: suite:4 keytype:4 reserved:8 messageIV:48 sessionID:192 auth_tag:128 payload:... [bits]",
            "            ^-------------------- AAD ----------------------------^",
            "    Offsets:0                 1          2            8             32           48 [bytes]",
            "",
            "    suite: 1010b for new AEAD crypto, 0000b is old crypto",
            "    keytype: see constants.KeyType (suite+keytype)",
            "    reserved: all-zero, for future use",
            "    messageIV: a counter starting from 0 for all new encrypted messages of one session",
            "    sessionID: 192bit random, computed once per session (the session key is derived from this)",
            "    auth_tag: authentication tag output of the AEAD cipher (computed over payload and AAD)",
            "    payload: encrypted chunk data",
            "    \"\"\"",
            "",
            "    PAYLOAD_OVERHEAD = 1 + 1 + 6 + 24 + 16  # [bytes], see Layout",
            "",
            "    CIPHERSUITE: Callable = None  # override in subclass",
            "",
            "    logically_encrypted = True",
            "",
            "    MAX_IV = 2**48 - 1",
            "",
            "    def assert_id(self, id, data):",
            "        # Comparing the id hash here would not be needed any more for the new AEAD crypto **IF** we",
            "        # could be sure that chunks were created by normal (not tampered, not evil) borg code:",
            "        # We put the id into AAD when storing the chunk, so it gets into the authentication tag computation.",
            "        # when decrypting, we provide the id we **want** as AAD for the auth tag verification, so",
            "        # decrypting only succeeds if we got the ciphertext we wrote **for that chunk id**.",
            "        # So, basically the **repository** can not cheat on us by giving us a different chunk.",
            "        #",
            "        # **BUT**, if chunks are created by tampered, evil borg code, the borg client code could put",
            "        # a wrong chunkid into AAD and then AEAD-encrypt-and-auth this and store it into the",
            "        # repository using this bad chunkid as key (violating the usual chunkid == id_hash(data)).",
            "        # Later, when reading such a bad chunk, AEAD-auth-and-decrypt would not notice any",
            "        # issue and decrypt successfully.",
            "        # Thus, to notice such evil borg activity, we must check for such violations here:",
            "        if id and id != Manifest.MANIFEST_ID:",
            "            id_computed = self.id_hash(data)",
            "            if not hmac.compare_digest(id_computed, id):",
            "                raise IntegrityError(\"Chunk %s: id verification failed\" % bin_to_hex(id))",
            "",
            "    def encrypt(self, id, data):",
            "        # to encrypt new data in this session we use always self.cipher and self.sessionid",
            "        reserved = b\"\\0\"",
            "        iv = self.cipher.next_iv()",
            "        if iv > self.MAX_IV:  # see the data-structures docs about why the IV range is enough",
            "            raise IntegrityError(\"IV overflow, should never happen.\")",
            "        iv_48bit = iv.to_bytes(6, \"big\")",
            "        header = self.TYPE_STR + reserved + iv_48bit + self.sessionid",
            "        return self.cipher.encrypt(data, header=header, iv=iv, aad=id)",
            "",
            "    def decrypt(self, id, data):",
            "        # to decrypt existing data, we need to get a cipher configured for the sessionid and iv from header",
            "        self.assert_type(data[0], id)",
            "        iv_48bit = data[2:8]",
            "        sessionid = data[8:32]",
            "        iv = int.from_bytes(iv_48bit, \"big\")",
            "        cipher = self._get_cipher(sessionid, iv)",
            "        try:",
            "            return cipher.decrypt(data, aad=id)",
            "        except IntegrityError as e:",
            "            raise IntegrityError(f\"Chunk {bin_to_hex(id)}: Could not decrypt [{str(e)}]\")",
            "",
            "    def init_from_given_data(self, *, crypt_key, id_key, chunk_seed):",
            "        assert len(crypt_key) in (32 + 32, 32 + 128)",
            "        assert len(id_key) in (32, 128)",
            "        assert isinstance(chunk_seed, int)",
            "        self.crypt_key = crypt_key",
            "        self.id_key = id_key",
            "        self.chunk_seed = chunk_seed",
            "",
            "    def init_from_random_data(self):",
            "        data = os.urandom(100)",
            "        chunk_seed = bytes_to_int(data[96:100])",
            "        # Convert to signed int32",
            "        if chunk_seed & 0x80000000:",
            "            chunk_seed = chunk_seed - 0xFFFFFFFF - 1",
            "        self.init_from_given_data(crypt_key=data[0:64], id_key=data[64:96], chunk_seed=chunk_seed)",
            "",
            "    def _get_session_key(self, sessionid):",
            "        assert len(sessionid) == 24  # 192bit",
            "        key = hkdf_hmac_sha512(",
            "            ikm=self.crypt_key,",
            "            salt=sessionid,",
            "            info=b\"borg-session-key-\" + self.CIPHERSUITE.__name__.encode(),",
            "            output_length=32,",
            "        )",
            "        return key",
            "",
            "    def _get_cipher(self, sessionid, iv):",
            "        assert isinstance(iv, int)",
            "        key = self._get_session_key(sessionid)",
            "        cipher = self.CIPHERSUITE(key=key, iv=iv, header_len=1 + 1 + 6 + 24, aad_offset=0)",
            "        return cipher",
            "",
            "    def init_ciphers(self, manifest_data=None, iv=0):",
            "        # in every new session we start with a fresh sessionid and at iv == 0, manifest_data and iv params are ignored",
            "        self.sessionid = os.urandom(24)",
            "        self.cipher = self._get_cipher(self.sessionid, iv=0)",
            "",
            "",
            "class AESOCBKeyfileKey(ID_HMAC_SHA_256, AEADKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.AESOCBKEYFILE, KeyType.AESOCBREPO}",
            "    TYPE = KeyType.AESOCBKEYFILE",
            "    NAME = \"key file AES-OCB\"",
            "    ARG_NAME = \"keyfile-aes-ocb\"",
            "    STORAGE = KeyBlobStorage.KEYFILE",
            "    CIPHERSUITE = AES256_OCB",
            "",
            "",
            "class AESOCBRepoKey(ID_HMAC_SHA_256, AEADKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.AESOCBKEYFILE, KeyType.AESOCBREPO}",
            "    TYPE = KeyType.AESOCBREPO",
            "    NAME = \"repokey AES-OCB\"",
            "    ARG_NAME = \"repokey-aes-ocb\"",
            "    STORAGE = KeyBlobStorage.REPO",
            "    CIPHERSUITE = AES256_OCB",
            "",
            "",
            "class CHPOKeyfileKey(ID_HMAC_SHA_256, AEADKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.CHPOKEYFILE, KeyType.CHPOREPO}",
            "    TYPE = KeyType.CHPOKEYFILE",
            "    NAME = \"key file ChaCha20-Poly1305\"",
            "    ARG_NAME = \"keyfile-chacha20-poly1305\"",
            "    STORAGE = KeyBlobStorage.KEYFILE",
            "    CIPHERSUITE = CHACHA20_POLY1305",
            "",
            "",
            "class CHPORepoKey(ID_HMAC_SHA_256, AEADKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.CHPOKEYFILE, KeyType.CHPOREPO}",
            "    TYPE = KeyType.CHPOREPO",
            "    NAME = \"repokey ChaCha20-Poly1305\"",
            "    ARG_NAME = \"repokey-chacha20-poly1305\"",
            "    STORAGE = KeyBlobStorage.REPO",
            "    CIPHERSUITE = CHACHA20_POLY1305",
            "",
            "",
            "class Blake2AESOCBKeyfileKey(ID_BLAKE2b_256, AEADKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.BLAKE2AESOCBKEYFILE, KeyType.BLAKE2AESOCBREPO}",
            "    TYPE = KeyType.BLAKE2AESOCBKEYFILE",
            "    NAME = \"key file BLAKE2b AES-OCB\"",
            "    ARG_NAME = \"keyfile-blake2-aes-ocb\"",
            "    STORAGE = KeyBlobStorage.KEYFILE",
            "    CIPHERSUITE = AES256_OCB",
            "",
            "",
            "class Blake2AESOCBRepoKey(ID_BLAKE2b_256, AEADKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.BLAKE2AESOCBKEYFILE, KeyType.BLAKE2AESOCBREPO}",
            "    TYPE = KeyType.BLAKE2AESOCBREPO",
            "    NAME = \"repokey BLAKE2b AES-OCB\"",
            "    ARG_NAME = \"repokey-blake2-aes-ocb\"",
            "    STORAGE = KeyBlobStorage.REPO",
            "    CIPHERSUITE = AES256_OCB",
            "",
            "",
            "class Blake2CHPOKeyfileKey(ID_BLAKE2b_256, AEADKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.BLAKE2CHPOKEYFILE, KeyType.BLAKE2CHPOREPO}",
            "    TYPE = KeyType.BLAKE2CHPOKEYFILE",
            "    NAME = \"key file BLAKE2b ChaCha20-Poly1305\"",
            "    ARG_NAME = \"keyfile-blake2-chacha20-poly1305\"",
            "    STORAGE = KeyBlobStorage.KEYFILE",
            "    CIPHERSUITE = CHACHA20_POLY1305",
            "",
            "",
            "class Blake2CHPORepoKey(ID_BLAKE2b_256, AEADKeyBase, FlexiKey):",
            "    TYPES_ACCEPTABLE = {KeyType.BLAKE2CHPOKEYFILE, KeyType.BLAKE2CHPOREPO}",
            "    TYPE = KeyType.BLAKE2CHPOREPO",
            "    NAME = \"repokey BLAKE2b ChaCha20-Poly1305\"",
            "    ARG_NAME = \"repokey-blake2-chacha20-poly1305\"",
            "    STORAGE = KeyBlobStorage.REPO",
            "    CIPHERSUITE = CHACHA20_POLY1305",
            "",
            "",
            "LEGACY_KEY_TYPES = (",
            "    # legacy (AES-CTR based) crypto",
            "    KeyfileKey,",
            "    RepoKey,",
            "    Blake2KeyfileKey,",
            "    Blake2RepoKey,",
            ")",
            "",
            "AVAILABLE_KEY_TYPES = (",
            "    # these are available encryption modes for new repositories",
            "    # not encrypted modes",
            "    PlaintextKey,",
            "    AuthenticatedKey,",
            "    Blake2AuthenticatedKey,",
            "    # new crypto",
            "    AESOCBKeyfileKey,",
            "    AESOCBRepoKey,",
            "    CHPOKeyfileKey,",
            "    CHPORepoKey,",
            "    Blake2AESOCBKeyfileKey,",
            "    Blake2AESOCBRepoKey,",
            "    Blake2CHPOKeyfileKey,",
            "    Blake2CHPORepoKey,",
            ")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "228": [
                "KeyBase",
                "pack_and_authenticate_metadata"
            ],
            "230": [
                "KeyBase",
                "pack_and_authenticate_metadata"
            ],
            "232": [
                "KeyBase",
                "pack_and_authenticate_metadata"
            ],
            "255": [
                "KeyBase",
                "unpack_and_verify_manifest"
            ],
            "265": [
                "KeyBase",
                "unpack_and_verify_manifest"
            ]
        },
        "addLocation": []
    },
    "src/borg/helpers/msgpack.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 219,
                "afterPatchRowNumber": 219,
                "PatchRowcode": "     args = dict(use_list=False, max_buffer_size=3 * max(BUFSIZE, MAX_OBJECT_SIZE))  # return tuples, not lists"
            },
            "1": {
                "beforePatchRowNumber": 220,
                "afterPatchRowNumber": 220,
                "PatchRowcode": "     if kind in (\"server\", \"client\"):"
            },
            "2": {
                "beforePatchRowNumber": 221,
                "afterPatchRowNumber": 221,
                "PatchRowcode": "         pass  # nothing special"
            },
            "3": {
                "beforePatchRowNumber": 222,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    elif kind in (\"manifest\", \"key\"):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 222,
                "PatchRowcode": "+    elif kind in (\"manifest\", \"archive\", \"key\"):"
            },
            "5": {
                "beforePatchRowNumber": 223,
                "afterPatchRowNumber": 223,
                "PatchRowcode": "         args.update(dict(use_list=True, object_hook=StableDict))  # default value"
            },
            "6": {
                "beforePatchRowNumber": 224,
                "afterPatchRowNumber": 224,
                "PatchRowcode": "     else:"
            },
            "7": {
                "beforePatchRowNumber": 225,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        raise ValueError('kind must be \"server\", \"client\", \"manifest\" or \"key\"')"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 225,
                "PatchRowcode": "+        raise ValueError('kind must be \"server\", \"client\", \"manifest\", \"archive\" or \"key\"')"
            },
            "9": {
                "beforePatchRowNumber": 226,
                "afterPatchRowNumber": 226,
                "PatchRowcode": "     return Unpacker(**args)"
            },
            "10": {
                "beforePatchRowNumber": 227,
                "afterPatchRowNumber": 227,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 228,
                "afterPatchRowNumber": 228,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "wrapping msgpack",
            "================",
            "",
            "We wrap msgpack here the way we need it - to avoid having lots of clutter in the calling code.",
            "",
            "Packing",
            "-------",
            "- use_bin_type = True (used by borg since borg 2.0)",
            "  This is used to generate output according to new msgpack 2.0 spec.",
            "  This cleanly keeps bytes and str types apart.",
            "",
            "- use_bin_type = False (used by borg < 1.3)",
            "  This creates output according to the older msgpack spec.",
            "  BAD: str and bytes were packed into same \"raw\" representation.",
            "",
            "- unicode_errors = 'surrogateescape'",
            "  Guess backup applications are one of the rare cases when this needs to be used.",
            "  It is needed because borg also needs to deal with data that does not cleanly encode/decode using utf-8.",
            "  There's a lot of crap out there, e.g. in filenames and as a backup tool, we must keep them as good as possible.",
            "",
            "Unpacking",
            "---------",
            "- raw = False (used by borg since borg 2.0)",
            "  We already can use this with borg 2.0 due to the type conversion to the desired type in item.py update_internal",
            "  methods. This type conversion code can be removed in future, when we do not have to deal with data any more",
            "  that was packed the old way.",
            "  It will then unpack according to the msgpack 2.0 spec format and directly output bytes or str.",
            "",
            "- raw = True (the old way, used by borg < 1.3)",
            "",
            "- unicode_errors = 'surrogateescape' -> see description above (will be used when raw is False).",
            "",
            "As of borg 2.0, we have fixed most of the msgpack str/bytes mess, #968.",
            "Borg now still needs to **read** old repos, archives, keys, ... so we can not yet fix it completely.",
            "But from now on, borg only **writes** new data according to the new msgpack 2.0 spec,",
            "thus we can remove some legacy support in a later borg release (some places are marked with \"legacy\").",
            "",
            "current way in msgpack terms",
            "----------------------------",
            "",
            "- pack with use_bin_type=True (according to msgpack 2.0 spec)",
            "- packs str -> raw and bytes -> bin",
            "- unpack with raw=False (according to msgpack 2.0 spec, using unicode_errors='surrogateescape')",
            "- unpacks bin to bytes and raw to str (thus we need to convert to desired type if we want bytes from \"raw\")",
            "\"\"\"",
            "",
            "from .datastruct import StableDict",
            "from ..constants import *  # NOQA",
            "",
            "from msgpack import Packer as mp_Packer",
            "from msgpack import packb as mp_packb",
            "from msgpack import pack as mp_pack",
            "from msgpack import Unpacker as mp_Unpacker",
            "from msgpack import unpackb as mp_unpackb",
            "from msgpack import unpack as mp_unpack",
            "from msgpack import version as mp_version",
            "",
            "from msgpack import ExtType, Timestamp",
            "from msgpack import OutOfData",
            "",
            "",
            "version = mp_version",
            "",
            "USE_BIN_TYPE = True",
            "RAW = False",
            "UNICODE_ERRORS = \"surrogateescape\"",
            "",
            "",
            "class PackException(Exception):",
            "    \"\"\"Exception while msgpack packing\"\"\"",
            "",
            "",
            "class UnpackException(Exception):",
            "    \"\"\"Exception while msgpack unpacking\"\"\"",
            "",
            "",
            "class Packer(mp_Packer):",
            "    def __init__(",
            "        self,",
            "        *,",
            "        default=None,",
            "        unicode_errors=UNICODE_ERRORS,",
            "        use_single_float=False,",
            "        autoreset=True,",
            "        use_bin_type=USE_BIN_TYPE,",
            "        strict_types=False",
            "    ):",
            "        assert unicode_errors == UNICODE_ERRORS",
            "        super().__init__(",
            "            default=default,",
            "            unicode_errors=unicode_errors,",
            "            use_single_float=use_single_float,",
            "            autoreset=autoreset,",
            "            use_bin_type=use_bin_type,",
            "            strict_types=strict_types,",
            "        )",
            "",
            "    def pack(self, obj):",
            "        try:",
            "            return super().pack(obj)",
            "        except Exception as e:",
            "            raise PackException(e)",
            "",
            "",
            "def packb(o, *, use_bin_type=USE_BIN_TYPE, unicode_errors=UNICODE_ERRORS, **kwargs):",
            "    assert unicode_errors == UNICODE_ERRORS",
            "    try:",
            "        return mp_packb(o, use_bin_type=use_bin_type, unicode_errors=unicode_errors, **kwargs)",
            "    except Exception as e:",
            "        raise PackException(e)",
            "",
            "",
            "def pack(o, stream, *, use_bin_type=USE_BIN_TYPE, unicode_errors=UNICODE_ERRORS, **kwargs):",
            "    assert unicode_errors == UNICODE_ERRORS",
            "    try:",
            "        return mp_pack(o, stream, use_bin_type=use_bin_type, unicode_errors=unicode_errors, **kwargs)",
            "    except Exception as e:",
            "        raise PackException(e)",
            "",
            "",
            "class Unpacker(mp_Unpacker):",
            "    def __init__(",
            "        self,",
            "        file_like=None,",
            "        *,",
            "        read_size=0,",
            "        use_list=True,",
            "        raw=RAW,",
            "        object_hook=None,",
            "        object_pairs_hook=None,",
            "        list_hook=None,",
            "        unicode_errors=UNICODE_ERRORS,",
            "        max_buffer_size=0,",
            "        ext_hook=ExtType,",
            "        strict_map_key=False",
            "    ):",
            "        assert raw == RAW",
            "        assert unicode_errors == UNICODE_ERRORS",
            "        kw = dict(",
            "            file_like=file_like,",
            "            read_size=read_size,",
            "            use_list=use_list,",
            "            raw=raw,",
            "            object_hook=object_hook,",
            "            object_pairs_hook=object_pairs_hook,",
            "            list_hook=list_hook,",
            "            unicode_errors=unicode_errors,",
            "            max_buffer_size=max_buffer_size,",
            "            ext_hook=ext_hook,",
            "            strict_map_key=strict_map_key,",
            "        )",
            "        super().__init__(**kw)",
            "",
            "    def unpack(self):",
            "        try:",
            "            return super().unpack()",
            "        except OutOfData:",
            "            raise",
            "        except Exception as e:",
            "            raise UnpackException(e)",
            "",
            "    def __next__(self):",
            "        try:",
            "            return super().__next__()",
            "        except StopIteration:",
            "            raise",
            "        except Exception as e:",
            "            raise UnpackException(e)",
            "",
            "    next = __next__",
            "",
            "",
            "def unpackb(packed, *, raw=RAW, unicode_errors=UNICODE_ERRORS, strict_map_key=False, **kwargs):",
            "    assert raw == RAW",
            "    assert unicode_errors == UNICODE_ERRORS",
            "    try:",
            "        kw = dict(raw=raw, unicode_errors=unicode_errors, strict_map_key=strict_map_key)",
            "        kw.update(kwargs)",
            "        return mp_unpackb(packed, **kw)",
            "    except Exception as e:",
            "        raise UnpackException(e)",
            "",
            "",
            "def unpack(stream, *, raw=RAW, unicode_errors=UNICODE_ERRORS, strict_map_key=False, **kwargs):",
            "    assert raw == RAW",
            "    assert unicode_errors == UNICODE_ERRORS",
            "    try:",
            "        kw = dict(raw=raw, unicode_errors=unicode_errors, strict_map_key=strict_map_key)",
            "        kw.update(kwargs)",
            "        return mp_unpack(stream, **kw)",
            "    except Exception as e:",
            "        raise UnpackException(e)",
            "",
            "",
            "# msgpacking related utilities -----------------------------------------------",
            "",
            "",
            "def is_slow_msgpack():",
            "    import msgpack",
            "    import msgpack.fallback",
            "",
            "    return msgpack.Packer is msgpack.fallback.Packer",
            "",
            "",
            "def is_supported_msgpack():",
            "    # DO NOT CHANGE OR REMOVE! See also requirements and comments in setup.cfg.",
            "    import msgpack",
            "",
            "    if msgpack.version in []:  # < add bad releases here to deny list",
            "        return False",
            "    return (1, 0, 3) <= msgpack.version <= (1, 0, 5)",
            "",
            "",
            "def get_limited_unpacker(kind):",
            "    \"\"\"return a limited Unpacker because we should not trust msgpack data received from remote\"\"\"",
            "    # Note: msgpack >= 0.6.1 auto-computes DoS-safe max values from len(data) for",
            "    #       unpack(data) or from max_buffer_size for Unpacker(max_buffer_size=N).",
            "    args = dict(use_list=False, max_buffer_size=3 * max(BUFSIZE, MAX_OBJECT_SIZE))  # return tuples, not lists",
            "    if kind in (\"server\", \"client\"):",
            "        pass  # nothing special",
            "    elif kind in (\"manifest\", \"key\"):",
            "        args.update(dict(use_list=True, object_hook=StableDict))  # default value",
            "    else:",
            "        raise ValueError('kind must be \"server\", \"client\", \"manifest\" or \"key\"')",
            "    return Unpacker(**args)",
            "",
            "",
            "def int_to_timestamp(ns):",
            "    assert isinstance(ns, int)",
            "    return Timestamp.from_unix_nano(ns)",
            "",
            "",
            "def timestamp_to_int(ts):",
            "    assert isinstance(ts, Timestamp)",
            "    return ts.to_unix_nano()"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "wrapping msgpack",
            "================",
            "",
            "We wrap msgpack here the way we need it - to avoid having lots of clutter in the calling code.",
            "",
            "Packing",
            "-------",
            "- use_bin_type = True (used by borg since borg 2.0)",
            "  This is used to generate output according to new msgpack 2.0 spec.",
            "  This cleanly keeps bytes and str types apart.",
            "",
            "- use_bin_type = False (used by borg < 1.3)",
            "  This creates output according to the older msgpack spec.",
            "  BAD: str and bytes were packed into same \"raw\" representation.",
            "",
            "- unicode_errors = 'surrogateescape'",
            "  Guess backup applications are one of the rare cases when this needs to be used.",
            "  It is needed because borg also needs to deal with data that does not cleanly encode/decode using utf-8.",
            "  There's a lot of crap out there, e.g. in filenames and as a backup tool, we must keep them as good as possible.",
            "",
            "Unpacking",
            "---------",
            "- raw = False (used by borg since borg 2.0)",
            "  We already can use this with borg 2.0 due to the type conversion to the desired type in item.py update_internal",
            "  methods. This type conversion code can be removed in future, when we do not have to deal with data any more",
            "  that was packed the old way.",
            "  It will then unpack according to the msgpack 2.0 spec format and directly output bytes or str.",
            "",
            "- raw = True (the old way, used by borg < 1.3)",
            "",
            "- unicode_errors = 'surrogateescape' -> see description above (will be used when raw is False).",
            "",
            "As of borg 2.0, we have fixed most of the msgpack str/bytes mess, #968.",
            "Borg now still needs to **read** old repos, archives, keys, ... so we can not yet fix it completely.",
            "But from now on, borg only **writes** new data according to the new msgpack 2.0 spec,",
            "thus we can remove some legacy support in a later borg release (some places are marked with \"legacy\").",
            "",
            "current way in msgpack terms",
            "----------------------------",
            "",
            "- pack with use_bin_type=True (according to msgpack 2.0 spec)",
            "- packs str -> raw and bytes -> bin",
            "- unpack with raw=False (according to msgpack 2.0 spec, using unicode_errors='surrogateescape')",
            "- unpacks bin to bytes and raw to str (thus we need to convert to desired type if we want bytes from \"raw\")",
            "\"\"\"",
            "",
            "from .datastruct import StableDict",
            "from ..constants import *  # NOQA",
            "",
            "from msgpack import Packer as mp_Packer",
            "from msgpack import packb as mp_packb",
            "from msgpack import pack as mp_pack",
            "from msgpack import Unpacker as mp_Unpacker",
            "from msgpack import unpackb as mp_unpackb",
            "from msgpack import unpack as mp_unpack",
            "from msgpack import version as mp_version",
            "",
            "from msgpack import ExtType, Timestamp",
            "from msgpack import OutOfData",
            "",
            "",
            "version = mp_version",
            "",
            "USE_BIN_TYPE = True",
            "RAW = False",
            "UNICODE_ERRORS = \"surrogateescape\"",
            "",
            "",
            "class PackException(Exception):",
            "    \"\"\"Exception while msgpack packing\"\"\"",
            "",
            "",
            "class UnpackException(Exception):",
            "    \"\"\"Exception while msgpack unpacking\"\"\"",
            "",
            "",
            "class Packer(mp_Packer):",
            "    def __init__(",
            "        self,",
            "        *,",
            "        default=None,",
            "        unicode_errors=UNICODE_ERRORS,",
            "        use_single_float=False,",
            "        autoreset=True,",
            "        use_bin_type=USE_BIN_TYPE,",
            "        strict_types=False",
            "    ):",
            "        assert unicode_errors == UNICODE_ERRORS",
            "        super().__init__(",
            "            default=default,",
            "            unicode_errors=unicode_errors,",
            "            use_single_float=use_single_float,",
            "            autoreset=autoreset,",
            "            use_bin_type=use_bin_type,",
            "            strict_types=strict_types,",
            "        )",
            "",
            "    def pack(self, obj):",
            "        try:",
            "            return super().pack(obj)",
            "        except Exception as e:",
            "            raise PackException(e)",
            "",
            "",
            "def packb(o, *, use_bin_type=USE_BIN_TYPE, unicode_errors=UNICODE_ERRORS, **kwargs):",
            "    assert unicode_errors == UNICODE_ERRORS",
            "    try:",
            "        return mp_packb(o, use_bin_type=use_bin_type, unicode_errors=unicode_errors, **kwargs)",
            "    except Exception as e:",
            "        raise PackException(e)",
            "",
            "",
            "def pack(o, stream, *, use_bin_type=USE_BIN_TYPE, unicode_errors=UNICODE_ERRORS, **kwargs):",
            "    assert unicode_errors == UNICODE_ERRORS",
            "    try:",
            "        return mp_pack(o, stream, use_bin_type=use_bin_type, unicode_errors=unicode_errors, **kwargs)",
            "    except Exception as e:",
            "        raise PackException(e)",
            "",
            "",
            "class Unpacker(mp_Unpacker):",
            "    def __init__(",
            "        self,",
            "        file_like=None,",
            "        *,",
            "        read_size=0,",
            "        use_list=True,",
            "        raw=RAW,",
            "        object_hook=None,",
            "        object_pairs_hook=None,",
            "        list_hook=None,",
            "        unicode_errors=UNICODE_ERRORS,",
            "        max_buffer_size=0,",
            "        ext_hook=ExtType,",
            "        strict_map_key=False",
            "    ):",
            "        assert raw == RAW",
            "        assert unicode_errors == UNICODE_ERRORS",
            "        kw = dict(",
            "            file_like=file_like,",
            "            read_size=read_size,",
            "            use_list=use_list,",
            "            raw=raw,",
            "            object_hook=object_hook,",
            "            object_pairs_hook=object_pairs_hook,",
            "            list_hook=list_hook,",
            "            unicode_errors=unicode_errors,",
            "            max_buffer_size=max_buffer_size,",
            "            ext_hook=ext_hook,",
            "            strict_map_key=strict_map_key,",
            "        )",
            "        super().__init__(**kw)",
            "",
            "    def unpack(self):",
            "        try:",
            "            return super().unpack()",
            "        except OutOfData:",
            "            raise",
            "        except Exception as e:",
            "            raise UnpackException(e)",
            "",
            "    def __next__(self):",
            "        try:",
            "            return super().__next__()",
            "        except StopIteration:",
            "            raise",
            "        except Exception as e:",
            "            raise UnpackException(e)",
            "",
            "    next = __next__",
            "",
            "",
            "def unpackb(packed, *, raw=RAW, unicode_errors=UNICODE_ERRORS, strict_map_key=False, **kwargs):",
            "    assert raw == RAW",
            "    assert unicode_errors == UNICODE_ERRORS",
            "    try:",
            "        kw = dict(raw=raw, unicode_errors=unicode_errors, strict_map_key=strict_map_key)",
            "        kw.update(kwargs)",
            "        return mp_unpackb(packed, **kw)",
            "    except Exception as e:",
            "        raise UnpackException(e)",
            "",
            "",
            "def unpack(stream, *, raw=RAW, unicode_errors=UNICODE_ERRORS, strict_map_key=False, **kwargs):",
            "    assert raw == RAW",
            "    assert unicode_errors == UNICODE_ERRORS",
            "    try:",
            "        kw = dict(raw=raw, unicode_errors=unicode_errors, strict_map_key=strict_map_key)",
            "        kw.update(kwargs)",
            "        return mp_unpack(stream, **kw)",
            "    except Exception as e:",
            "        raise UnpackException(e)",
            "",
            "",
            "# msgpacking related utilities -----------------------------------------------",
            "",
            "",
            "def is_slow_msgpack():",
            "    import msgpack",
            "    import msgpack.fallback",
            "",
            "    return msgpack.Packer is msgpack.fallback.Packer",
            "",
            "",
            "def is_supported_msgpack():",
            "    # DO NOT CHANGE OR REMOVE! See also requirements and comments in setup.cfg.",
            "    import msgpack",
            "",
            "    if msgpack.version in []:  # < add bad releases here to deny list",
            "        return False",
            "    return (1, 0, 3) <= msgpack.version <= (1, 0, 5)",
            "",
            "",
            "def get_limited_unpacker(kind):",
            "    \"\"\"return a limited Unpacker because we should not trust msgpack data received from remote\"\"\"",
            "    # Note: msgpack >= 0.6.1 auto-computes DoS-safe max values from len(data) for",
            "    #       unpack(data) or from max_buffer_size for Unpacker(max_buffer_size=N).",
            "    args = dict(use_list=False, max_buffer_size=3 * max(BUFSIZE, MAX_OBJECT_SIZE))  # return tuples, not lists",
            "    if kind in (\"server\", \"client\"):",
            "        pass  # nothing special",
            "    elif kind in (\"manifest\", \"archive\", \"key\"):",
            "        args.update(dict(use_list=True, object_hook=StableDict))  # default value",
            "    else:",
            "        raise ValueError('kind must be \"server\", \"client\", \"manifest\", \"archive\" or \"key\"')",
            "    return Unpacker(**args)",
            "",
            "",
            "def int_to_timestamp(ns):",
            "    assert isinstance(ns, int)",
            "    return Timestamp.from_unix_nano(ns)",
            "",
            "",
            "def timestamp_to_int(ts):",
            "    assert isinstance(ts, Timestamp)",
            "    return ts.to_unix_nano()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "222": [
                "get_limited_unpacker"
            ],
            "225": [
                "get_limited_unpacker"
            ]
        },
        "addLocation": []
    },
    "src/borg/helpers/parseformat.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 723,
                "afterPatchRowNumber": 723,
                "PatchRowcode": "         \"id\": \"internal ID of the archive\","
            },
            "1": {
                "beforePatchRowNumber": 724,
                "afterPatchRowNumber": 724,
                "PatchRowcode": "         \"hostname\": \"hostname of host on which this archive was created\","
            },
            "2": {
                "beforePatchRowNumber": 725,
                "afterPatchRowNumber": 725,
                "PatchRowcode": "         \"username\": \"username of user who created this archive\","
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 726,
                "PatchRowcode": "+        \"tam\": \"TAM authentication state of this archive\","
            },
            "4": {
                "beforePatchRowNumber": 726,
                "afterPatchRowNumber": 727,
                "PatchRowcode": "         \"size\": \"size of this archive (data plus metadata, not considering compression and deduplication)\","
            },
            "5": {
                "beforePatchRowNumber": 727,
                "afterPatchRowNumber": 728,
                "PatchRowcode": "         \"nfiles\": \"count of files in this archive\","
            },
            "6": {
                "beforePatchRowNumber": 728,
                "afterPatchRowNumber": 729,
                "PatchRowcode": "     }"
            },
            "7": {
                "beforePatchRowNumber": 729,
                "afterPatchRowNumber": 730,
                "PatchRowcode": "     KEY_GROUPS = ("
            },
            "8": {
                "beforePatchRowNumber": 730,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        (\"archive\", \"name\", \"comment\", \"id\"),"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 731,
                "PatchRowcode": "+        (\"archive\", \"name\", \"comment\", \"id\", \"tam\"),"
            },
            "10": {
                "beforePatchRowNumber": 731,
                "afterPatchRowNumber": 732,
                "PatchRowcode": "         (\"start\", \"time\", \"end\", \"command_line\"),"
            },
            "11": {
                "beforePatchRowNumber": 732,
                "afterPatchRowNumber": 733,
                "PatchRowcode": "         (\"hostname\", \"username\"),"
            },
            "12": {
                "beforePatchRowNumber": 733,
                "afterPatchRowNumber": 734,
                "PatchRowcode": "         (\"size\", \"nfiles\"),"
            },
            "13": {
                "beforePatchRowNumber": 750,
                "afterPatchRowNumber": 751,
                "PatchRowcode": "             \"username\": partial(self.get_meta, \"username\", \"\"),"
            },
            "14": {
                "beforePatchRowNumber": 751,
                "afterPatchRowNumber": 752,
                "PatchRowcode": "             \"comment\": partial(self.get_meta, \"comment\", \"\"),"
            },
            "15": {
                "beforePatchRowNumber": 752,
                "afterPatchRowNumber": 753,
                "PatchRowcode": "             \"command_line\": partial(self.get_meta, \"command_line\", \"\"),"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 754,
                "PatchRowcode": "+            \"tam\": self.get_tam,"
            },
            "17": {
                "beforePatchRowNumber": 753,
                "afterPatchRowNumber": 755,
                "PatchRowcode": "             \"size\": partial(self.get_meta, \"size\", 0),"
            },
            "18": {
                "beforePatchRowNumber": 754,
                "afterPatchRowNumber": 756,
                "PatchRowcode": "             \"nfiles\": partial(self.get_meta, \"nfiles\", 0),"
            },
            "19": {
                "beforePatchRowNumber": 755,
                "afterPatchRowNumber": 757,
                "PatchRowcode": "             \"end\": self.get_ts_end,"
            },
            "20": {
                "beforePatchRowNumber": 795,
                "afterPatchRowNumber": 797,
                "PatchRowcode": "     def get_ts_end(self):"
            },
            "21": {
                "beforePatchRowNumber": 796,
                "afterPatchRowNumber": 798,
                "PatchRowcode": "         return self.format_time(self.archive.ts_end)"
            },
            "22": {
                "beforePatchRowNumber": 797,
                "afterPatchRowNumber": 799,
                "PatchRowcode": " "
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 800,
                "PatchRowcode": "+    def get_tam(self):"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 801,
                "PatchRowcode": "+        return \"verified\" if self.archive.tam_verified else \"none\""
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 802,
                "PatchRowcode": "+"
            },
            "26": {
                "beforePatchRowNumber": 798,
                "afterPatchRowNumber": 803,
                "PatchRowcode": "     def format_time(self, ts):"
            },
            "27": {
                "beforePatchRowNumber": 799,
                "afterPatchRowNumber": 804,
                "PatchRowcode": "         return OutputTimestamp(ts)"
            },
            "28": {
                "beforePatchRowNumber": 800,
                "afterPatchRowNumber": 805,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "import abc",
            "import argparse",
            "import base64",
            "import hashlib",
            "import json",
            "import os",
            "import os.path",
            "import re",
            "import shlex",
            "import stat",
            "import uuid",
            "from typing import Dict, Set, Tuple, ClassVar, Any, TYPE_CHECKING, Literal",
            "from binascii import hexlify",
            "from collections import Counter, OrderedDict",
            "from datetime import datetime, timezone",
            "from functools import partial",
            "from string import Formatter",
            "",
            "from ..logger import create_logger",
            "",
            "logger = create_logger()",
            "",
            "from .errors import Error",
            "from .fs import get_keys_dir, make_path_safe",
            "from .msgpack import Timestamp",
            "from .time import OutputTimestamp, format_time, safe_timestamp",
            "from .. import __version__ as borg_version",
            "from .. import __version_tuple__ as borg_version_tuple",
            "from ..constants import *  # NOQA",
            "",
            "if TYPE_CHECKING:",
            "    from ..item import ItemDiff",
            "",
            "",
            "def bin_to_hex(binary):",
            "    return hexlify(binary).decode(\"ascii\")",
            "",
            "",
            "def safe_decode(s, coding=\"utf-8\", errors=\"surrogateescape\"):",
            "    \"\"\"decode bytes to str, with round-tripping \"invalid\" bytes\"\"\"",
            "    if s is None:",
            "        return None",
            "    return s.decode(coding, errors)",
            "",
            "",
            "def safe_encode(s, coding=\"utf-8\", errors=\"surrogateescape\"):",
            "    \"\"\"encode str to bytes, with round-tripping \"invalid\" bytes\"\"\"",
            "    if s is None:",
            "        return None",
            "    return s.encode(coding, errors)",
            "",
            "",
            "def remove_surrogates(s, errors=\"replace\"):",
            "    \"\"\"Replace surrogates generated by fsdecode with '?'\"\"\"",
            "    return s.encode(\"utf-8\", errors).decode(\"utf-8\")",
            "",
            "",
            "def binary_to_json(key, value):",
            "    assert isinstance(key, str)",
            "    assert isinstance(value, bytes)",
            "    return {key + \"_b64\": base64.b64encode(value).decode(\"ascii\")}",
            "",
            "",
            "def text_to_json(key, value):",
            "    \"\"\"",
            "    Return a dict made from key/value that can be fed safely into a JSON encoder.",
            "",
            "    JSON can only contain pure, valid unicode (but not: unicode with surrogate escapes).",
            "",
            "    But sometimes we have to deal with such values and we do it like this:",
            "    - <key>: value as pure unicode text (surrogate escapes, if any, replaced by ?)",
            "    - <key>_b64: value as base64 encoded binary representation (only set if value has surrogate-escapes)",
            "    \"\"\"",
            "    coding = \"utf-8\"",
            "    assert isinstance(key, str)",
            "    assert isinstance(value, str)  # str might contain surrogate escapes",
            "    data = {}",
            "    try:",
            "        value.encode(coding, errors=\"strict\")  # check if pure unicode",
            "    except UnicodeEncodeError:",
            "        # value has surrogate escape sequences",
            "        data[key] = remove_surrogates(value)",
            "        value_bytes = value.encode(coding, errors=\"surrogateescape\")",
            "        data.update(binary_to_json(key, value_bytes))",
            "    else:",
            "        # value is pure unicode",
            "        data[key] = value",
            "        # we do not give the b64 representation, not needed",
            "    return data",
            "",
            "",
            "def join_cmd(argv, rs=False):",
            "    cmd = shlex.join(argv)",
            "    return remove_surrogates(cmd) if rs else cmd",
            "",
            "",
            "def eval_escapes(s):",
            "    \"\"\"Evaluate literal escape sequences in a string (eg `\\\\n` -> `\\n`).\"\"\"",
            "    return s.encode(\"ascii\", \"backslashreplace\").decode(\"unicode-escape\")",
            "",
            "",
            "def decode_dict(d, keys, encoding=\"utf-8\", errors=\"surrogateescape\"):",
            "    for key in keys:",
            "        if isinstance(d.get(key), bytes):",
            "            d[key] = d[key].decode(encoding, errors)",
            "    return d",
            "",
            "",
            "def positive_int_validator(value):",
            "    \"\"\"argparse type for positive integers\"\"\"",
            "    int_value = int(value)",
            "    if int_value <= 0:",
            "        raise argparse.ArgumentTypeError(\"A positive integer is required: %s\" % value)",
            "    return int_value",
            "",
            "",
            "def interval(s):",
            "    \"\"\"Convert a string representing a valid interval to a number of hours.\"\"\"",
            "    multiplier = {\"H\": 1, \"d\": 24, \"w\": 24 * 7, \"m\": 24 * 31, \"y\": 24 * 365}",
            "",
            "    if s.endswith(tuple(multiplier.keys())):",
            "        number = s[:-1]",
            "        suffix = s[-1]",
            "    else:",
            "        # range suffixes in ascending multiplier order",
            "        ranges = [k for k, v in sorted(multiplier.items(), key=lambda t: t[1])]",
            "        raise argparse.ArgumentTypeError(f'Unexpected interval time unit \"{s[-1]}\": expected one of {ranges!r}')",
            "",
            "    try:",
            "        hours = int(number) * multiplier[suffix]",
            "    except ValueError:",
            "        hours = -1",
            "",
            "    if hours <= 0:",
            "        raise argparse.ArgumentTypeError('Unexpected interval number \"%s\": expected an integer greater than 0' % number)",
            "",
            "    return hours",
            "",
            "",
            "def ChunkerParams(s):",
            "    params = s.strip().split(\",\")",
            "    count = len(params)",
            "    if count == 0:",
            "        raise argparse.ArgumentTypeError(\"no chunker params given\")",
            "    algo = params[0].lower()",
            "    if algo == CH_FAIL and count == 3:",
            "        block_size = int(params[1])",
            "        fail_map = str(params[2])",
            "        return algo, block_size, fail_map",
            "    if algo == CH_FIXED and 2 <= count <= 3:  # fixed, block_size[, header_size]",
            "        block_size = int(params[1])",
            "        header_size = int(params[2]) if count == 3 else 0",
            "        if block_size < 64:",
            "            # we are only disallowing the most extreme cases of abuse here - this does NOT imply",
            "            # that cutting chunks of the minimum allowed size is efficient concerning storage",
            "            # or in-memory chunk management.",
            "            # choose the block (chunk) size wisely: if you have a lot of data and you cut",
            "            # it into very small chunks, you are asking for trouble!",
            "            raise argparse.ArgumentTypeError(\"block_size must not be less than 64 Bytes\")",
            "        if block_size > MAX_DATA_SIZE or header_size > MAX_DATA_SIZE:",
            "            raise argparse.ArgumentTypeError(",
            "                \"block_size and header_size must not exceed MAX_DATA_SIZE [%d]\" % MAX_DATA_SIZE",
            "            )",
            "        return algo, block_size, header_size",
            "    if algo == \"default\" and count == 1:  # default",
            "        return CHUNKER_PARAMS",
            "    # this must stay last as it deals with old-style compat mode (no algorithm, 4 params, buzhash):",
            "    if algo == CH_BUZHASH and count == 5 or count == 4:  # [buzhash, ]chunk_min, chunk_max, chunk_mask, window_size",
            "        chunk_min, chunk_max, chunk_mask, window_size = (int(p) for p in params[count - 4 :])",
            "        if not (chunk_min <= chunk_mask <= chunk_max):",
            "            raise argparse.ArgumentTypeError(\"required: chunk_min <= chunk_mask <= chunk_max\")",
            "        if chunk_min < 6:",
            "            # see comment in 'fixed' algo check",
            "            raise argparse.ArgumentTypeError(",
            "                \"min. chunk size exponent must not be less than 6 (2^6 = 64B min. chunk size)\"",
            "            )",
            "        if chunk_max > 23:",
            "            raise argparse.ArgumentTypeError(",
            "                \"max. chunk size exponent must not be more than 23 (2^23 = 8MiB max. chunk size)\"",
            "            )",
            "        return CH_BUZHASH, chunk_min, chunk_max, chunk_mask, window_size",
            "    raise argparse.ArgumentTypeError(\"invalid chunker params\")",
            "",
            "",
            "def FilesCacheMode(s):",
            "    ENTRIES_MAP = dict(ctime=\"c\", mtime=\"m\", size=\"s\", inode=\"i\", rechunk=\"r\", disabled=\"d\")",
            "    VALID_MODES = (\"cis\", \"ims\", \"cs\", \"ms\", \"cr\", \"mr\", \"d\", \"s\")  # letters in alpha order",
            "    entries = set(s.strip().split(\",\"))",
            "    if not entries <= set(ENTRIES_MAP):",
            "        raise argparse.ArgumentTypeError(",
            "            \"cache mode must be a comma-separated list of: %s\" % \",\".join(sorted(ENTRIES_MAP))",
            "        )",
            "    short_entries = {ENTRIES_MAP[entry] for entry in entries}",
            "    mode = \"\".join(sorted(short_entries))",
            "    if mode not in VALID_MODES:",
            "        raise argparse.ArgumentTypeError(\"cache mode short must be one of: %s\" % \",\".join(VALID_MODES))",
            "    return mode",
            "",
            "",
            "def partial_format(format, mapping):",
            "    \"\"\"",
            "    Apply format.format_map(mapping) while preserving unknown keys",
            "",
            "    Does not support attribute access, indexing and ![rsa] conversions",
            "    \"\"\"",
            "    for key, value in mapping.items():",
            "        key = re.escape(key)",
            "        format = re.sub(",
            "            rf\"(?<!\\{{)((\\{{{key}\\}})|(\\{{{key}:[^\\}}]*\\}}))\", lambda match: match.group(1).format_map(mapping), format",
            "        )",
            "    return format",
            "",
            "",
            "class DatetimeWrapper:",
            "    def __init__(self, dt):",
            "        self.dt = dt",
            "",
            "    def __format__(self, format_spec):",
            "        if format_spec == \"\":",
            "            format_spec = ISO_FORMAT_NO_USECS",
            "        return self.dt.__format__(format_spec)",
            "",
            "",
            "class PlaceholderError(Error):",
            "    \"\"\"Formatting Error: \"{}\".format({}): {}({})\"\"\"",
            "",
            "",
            "class InvalidPlaceholder(PlaceholderError):",
            "    \"\"\"Invalid placeholder \"{}\" in string: {}\"\"\"",
            "",
            "",
            "def format_line(format, data):",
            "    for _, key, _, conversion in Formatter().parse(format):",
            "        if not key:",
            "            continue",
            "        if conversion or key not in data:",
            "            raise InvalidPlaceholder(key, format)",
            "    try:",
            "        return format.format_map(data)",
            "    except Exception as e:",
            "        raise PlaceholderError(format, data, e.__class__.__name__, str(e))",
            "",
            "",
            "def _replace_placeholders(text, overrides={}):",
            "    \"\"\"Replace placeholders in text with their values.\"\"\"",
            "    from ..platform import fqdn, hostname, getosusername",
            "",
            "    current_time = datetime.now(timezone.utc)",
            "    data = {",
            "        \"pid\": os.getpid(),",
            "        \"fqdn\": fqdn,",
            "        \"reverse-fqdn\": \".\".join(reversed(fqdn.split(\".\"))),",
            "        \"hostname\": hostname,",
            "        \"now\": DatetimeWrapper(current_time.astimezone()),",
            "        \"utcnow\": DatetimeWrapper(current_time),",
            "        \"user\": getosusername(),",
            "        \"uuid4\": str(uuid.uuid4()),",
            "        \"borgversion\": borg_version,",
            "        \"borgmajor\": \"%d\" % borg_version_tuple[:1],",
            "        \"borgminor\": \"%d.%d\" % borg_version_tuple[:2],",
            "        \"borgpatch\": \"%d.%d.%d\" % borg_version_tuple[:3],",
            "        **overrides,",
            "    }",
            "    return format_line(text, data)",
            "",
            "",
            "class PlaceholderReplacer:",
            "    def __init__(self):",
            "        self.reset()",
            "",
            "    def override(self, key, value):",
            "        self.overrides[key] = value",
            "",
            "    def reset(self):",
            "        self.overrides = {}",
            "",
            "    def __call__(self, text, overrides=None):",
            "        ovr = {}",
            "        ovr.update(self.overrides)",
            "        ovr.update(overrides or {})",
            "        return _replace_placeholders(text, overrides=ovr)",
            "",
            "",
            "replace_placeholders = PlaceholderReplacer()",
            "",
            "",
            "def SortBySpec(text):",
            "    from ..manifest import AI_HUMAN_SORT_KEYS",
            "",
            "    for token in text.split(\",\"):",
            "        if token not in AI_HUMAN_SORT_KEYS:",
            "            raise argparse.ArgumentTypeError(\"Invalid sort key: %s\" % token)",
            "    return text.replace(\"timestamp\", \"ts\")",
            "",
            "",
            "def format_file_size(v, precision=2, sign=False, iec=False):",
            "    \"\"\"Format file size into a human friendly format\"\"\"",
            "    fn = sizeof_fmt_iec if iec else sizeof_fmt_decimal",
            "    return fn(v, suffix=\"B\", sep=\" \", precision=precision, sign=sign)",
            "",
            "",
            "class FileSize(int):",
            "    def __new__(cls, value, iec=False):",
            "        obj = int.__new__(cls, value)",
            "        obj.iec = iec",
            "        return obj",
            "",
            "    def __format__(self, format_spec):",
            "        return format_file_size(int(self), iec=self.iec).__format__(format_spec)",
            "",
            "",
            "def parse_file_size(s):",
            "    \"\"\"Return int from file size (1234, 55G, 1.7T).\"\"\"",
            "    if not s:",
            "        return int(s)  # will raise",
            "    suffix = s[-1]",
            "    power = 1000",
            "    try:",
            "        factor = {\"K\": power, \"M\": power**2, \"G\": power**3, \"T\": power**4, \"P\": power**5}[suffix]",
            "        s = s[:-1]",
            "    except KeyError:",
            "        factor = 1",
            "    return int(float(s) * factor)",
            "",
            "",
            "def parse_storage_quota(storage_quota):",
            "    parsed = parse_file_size(storage_quota)",
            "    if parsed < parse_file_size(\"10M\"):",
            "        raise argparse.ArgumentTypeError(\"quota is too small (%s). At least 10M are required.\" % storage_quota)",
            "    return parsed",
            "",
            "",
            "def sizeof_fmt(num, suffix=\"B\", units=None, power=None, sep=\"\", precision=2, sign=False):",
            "    sign = \"+\" if sign and num > 0 else \"\"",
            "    fmt = \"{0:{1}.{2}f}{3}{4}{5}\"",
            "    prec = 0",
            "    for unit in units[:-1]:",
            "        if abs(round(num, precision)) < power:",
            "            break",
            "        num /= float(power)",
            "        prec = precision",
            "    else:",
            "        unit = units[-1]",
            "    return fmt.format(num, sign, prec, sep, unit, suffix)",
            "",
            "",
            "def sizeof_fmt_iec(num, suffix=\"B\", sep=\"\", precision=2, sign=False):",
            "    return sizeof_fmt(",
            "        num,",
            "        suffix=suffix,",
            "        sep=sep,",
            "        precision=precision,",
            "        sign=sign,",
            "        units=[\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\", \"Yi\"],",
            "        power=1024,",
            "    )",
            "",
            "",
            "def sizeof_fmt_decimal(num, suffix=\"B\", sep=\"\", precision=2, sign=False):",
            "    return sizeof_fmt(",
            "        num,",
            "        suffix=suffix,",
            "        sep=sep,",
            "        precision=precision,",
            "        sign=sign,",
            "        units=[\"\", \"k\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\", \"Y\"],",
            "        power=1000,",
            "    )",
            "",
            "",
            "def format_archive(archive):",
            "    return \"%-36s %s [%s]\" % (archive.name, format_time(archive.ts), bin_to_hex(archive.id))",
            "",
            "",
            "def parse_stringified_list(s):",
            "    items = re.split(\" *, *\", s)",
            "    return [item for item in items if item != \"\"]",
            "",
            "",
            "class Location:",
            "    \"\"\"Object representing a repository location\"\"\"",
            "",
            "    # user must not contain \"@\", \":\" or \"/\".",
            "    # Quoting adduser error message:",
            "    # \"To avoid problems, the username should consist only of letters, digits,",
            "    # underscores, periods, at signs and dashes, and not start with a dash",
            "    # (as defined by IEEE Std 1003.1-2001).\"",
            "    # We use \"@\" as separator between username and hostname, so we must",
            "    # disallow it within the pure username part.",
            "    optional_user_re = r\"\"\"",
            "        (?:(?P<user>[^@:/]+)@)?",
            "    \"\"\"",
            "",
            "    # path must not contain :: (it ends at :: or string end), but may contain single colons.",
            "    # to avoid ambiguities with other regexes, it must also not start with \":\" nor with \"//\" nor with \"ssh://\".",
            "    local_path_re = r\"\"\"",
            "        (?!(:|//|ssh://|socket://))                         # not starting with \":\" or // or ssh:// or socket://",
            "        (?P<path>([^:]|(:(?!:)))+)                          # any chars, but no \"::\"",
            "        \"\"\"",
            "",
            "    # file_path must not contain :: (it ends at :: or string end), but may contain single colons.",
            "    # it must start with a / and that slash is part of the path.",
            "    file_path_re = r\"\"\"",
            "        (?P<path>(([^/]*)/([^:]|(:(?!:)))+))                # start opt. servername, then /, then any chars, but no \"::\"",
            "        \"\"\"",
            "",
            "    # abs_path must not contain :: (it ends at :: or string end), but may contain single colons.",
            "    # it must start with a / and that slash is part of the path.",
            "    abs_path_re = r\"\"\"",
            "        (?P<path>(/([^:]|(:(?!:)))+))                       # start with /, then any chars, but no \"::\"",
            "        \"\"\"",
            "",
            "    # host NAME, or host IP ADDRESS (v4 or v6, v6 must be in square brackets)",
            "    host_re = r\"\"\"",
            "        (?P<host>(",
            "            (?!\\[)[^:/]+(?<!\\])     # hostname or v4 addr, not containing : or / (does not match v6 addr: no brackets!)",
            "            |",
            "            \\[[0-9a-fA-F:.]+\\])     # ipv6 address in brackets",
            "        )",
            "    \"\"\"",
            "",
            "    # regexes for misc. kinds of supported location specifiers:",
            "    ssh_re = re.compile(",
            "        r\"\"\"",
            "        (?P<proto>ssh)://                                       # ssh://",
            "        \"\"\"",
            "        + optional_user_re",
            "        + host_re",
            "        + r\"\"\"                 # user@  (optional), host name or address",
            "        (?::(?P<port>\\d+))?                                     # :port (optional)",
            "        \"\"\"",
            "        + abs_path_re,",
            "        re.VERBOSE,",
            "    )  # path",
            "",
            "    socket_re = re.compile(",
            "        r\"\"\"",
            "        (?P<proto>socket)://                                    # socket://",
            "        \"\"\"",
            "        + abs_path_re,",
            "        re.VERBOSE,",
            "    )  # path",
            "",
            "    file_re = re.compile(",
            "        r\"\"\"",
            "        (?P<proto>file)://                                      # file://",
            "        \"\"\"",
            "        + file_path_re,",
            "        re.VERBOSE,",
            "    )  # servername/path or path",
            "",
            "    local_re = re.compile(local_path_re, re.VERBOSE)  # local path",
            "",
            "    win_file_re = re.compile(",
            "        r\"\"\"",
            "        (?:file://)?                                        # optional file protocol",
            "        (?P<path>",
            "            (?:[a-zA-Z]:)?                                  # Drive letter followed by a colon (optional)",
            "            (?:[^:]+)                                       # Anything which does not contain a :, at least one char",
            "        )",
            "        \"\"\",",
            "        re.VERBOSE,",
            "    )",
            "",
            "    def __init__(self, text=\"\", overrides={}, other=False):",
            "        self.repo_env_var = \"BORG_OTHER_REPO\" if other else \"BORG_REPO\"",
            "        self.valid = False",
            "        self.proto = None",
            "        self.user = None",
            "        self._host = None",
            "        self.port = None",
            "        self.path = None",
            "        self.raw = None",
            "        self.processed = None",
            "        self.parse(text, overrides)",
            "",
            "    def parse(self, text, overrides={}):",
            "        if not text:",
            "            # we did not get a text to parse, so we try to fetch from the environment",
            "            text = os.environ.get(self.repo_env_var)",
            "            if text is None:",
            "                return",
            "",
            "        self.raw = text  # as given by user, might contain placeholders",
            "        self.processed = replace_placeholders(self.raw, overrides)  # after placeholder replacement",
            "        valid = self._parse(self.processed)",
            "        if valid:",
            "            self.valid = True",
            "        else:",
            "            raise ValueError('Invalid location format: \"%s\"' % self.processed)",
            "",
            "    def _parse(self, text):",
            "        def normpath_special(p):",
            "            # avoid that normpath strips away our relative path hack and even makes p absolute",
            "            relative = p.startswith(\"/./\")",
            "            p = os.path.normpath(p)",
            "            return (\"/.\" + p) if relative else p",
            "",
            "        m = self.ssh_re.match(text)",
            "        if m:",
            "            self.proto = m.group(\"proto\")",
            "            self.user = m.group(\"user\")",
            "            self._host = m.group(\"host\")",
            "            self.port = m.group(\"port\") and int(m.group(\"port\")) or None",
            "            self.path = normpath_special(m.group(\"path\"))",
            "            return True",
            "        m = self.file_re.match(text)",
            "        if m:",
            "            self.proto = m.group(\"proto\")",
            "            self.path = normpath_special(m.group(\"path\"))",
            "            return True",
            "        m = self.socket_re.match(text)",
            "        if m:",
            "            self.proto = m.group(\"proto\")",
            "            self.path = normpath_special(m.group(\"path\"))",
            "            return True",
            "        m = self.local_re.match(text)",
            "        if m:",
            "            self.proto = \"file\"",
            "            self.path = normpath_special(m.group(\"path\"))",
            "            return True",
            "        return False",
            "",
            "    def __str__(self):",
            "        items = [",
            "            \"proto=%r\" % self.proto,",
            "            \"user=%r\" % self.user,",
            "            \"host=%r\" % self.host,",
            "            \"port=%r\" % self.port,",
            "            \"path=%r\" % self.path,",
            "        ]",
            "        return \", \".join(items)",
            "",
            "    def to_key_filename(self):",
            "        name = re.sub(r\"[^\\w]\", \"_\", self.path).strip(\"_\")",
            "        if self.proto not in (\"file\", \"socket\"):",
            "            name = re.sub(r\"[^\\w]\", \"_\", self.host) + \"__\" + name",
            "        if len(name) > 100:",
            "            # Limit file names to some reasonable length. Most file systems",
            "            # limit them to 255 [unit of choice]; due to variations in unicode",
            "            # handling we truncate to 100 *characters*.",
            "            name = name[:100]",
            "        return os.path.join(get_keys_dir(), name)",
            "",
            "    def __repr__(self):",
            "        return \"Location(%s)\" % self",
            "",
            "    @property",
            "    def host(self):",
            "        # strip square brackets used for IPv6 addrs",
            "        if self._host is not None:",
            "            return self._host.lstrip(\"[\").rstrip(\"]\")",
            "",
            "    def canonical_path(self):",
            "        if self.proto in (\"file\", \"socket\"):",
            "            return self.path",
            "        else:",
            "            if self.path and self.path.startswith(\"~\"):",
            "                path = \"/\" + self.path  # /~/x = path x relative to home dir",
            "            elif self.path and not self.path.startswith(\"/\"):",
            "                path = \"/./\" + self.path  # /./x = path x relative to cwd",
            "            else:",
            "                path = self.path",
            "            return \"ssh://{}{}{}{}\".format(",
            "                f\"{self.user}@\" if self.user else \"\",",
            "                self._host,  # needed for ipv6 addrs",
            "                f\":{self.port}\" if self.port else \"\",",
            "                path,",
            "            )",
            "",
            "    def with_timestamp(self, timestamp):",
            "        # note: this only affects the repository URL/path, not the archive name!",
            "        return Location(",
            "            self.raw,",
            "            overrides={",
            "                \"now\": DatetimeWrapper(timestamp),",
            "                \"utcnow\": DatetimeWrapper(timestamp.astimezone(timezone.utc)),",
            "            },",
            "        )",
            "",
            "",
            "def location_validator(proto=None, other=False):",
            "    def validator(text):",
            "        try:",
            "            loc = Location(text, other=other)",
            "        except ValueError as err:",
            "            raise argparse.ArgumentTypeError(str(err)) from None",
            "        if proto is not None and loc.proto != proto:",
            "            if proto == \"file\":",
            "                raise argparse.ArgumentTypeError('\"%s\": Repository must be local' % text)",
            "            else:",
            "                raise argparse.ArgumentTypeError('\"%s\": Repository must be remote' % text)",
            "        return loc",
            "",
            "    return validator",
            "",
            "",
            "def relative_time_marker_validator(text: str):",
            "    time_marker_regex = r\"^\\d+[md]$\"",
            "    match = re.compile(time_marker_regex).search(text)",
            "    if not match:",
            "        raise argparse.ArgumentTypeError(f\"Invalid relative time marker used: {text}\")",
            "    else:",
            "        return text",
            "",
            "",
            "def text_validator(*, name, max_length, min_length=0, invalid_ctrl_chars=\"\\0\", invalid_chars=\"\", no_blanks=False):",
            "    def validator(text):",
            "        assert isinstance(text, str)",
            "        if len(text) < min_length:",
            "            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [length < {min_length}]')",
            "        if len(text) > max_length:",
            "            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [length > {max_length}]')",
            "        if invalid_ctrl_chars and re.search(f\"[{re.escape(invalid_ctrl_chars)}]\", text):",
            "            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [invalid control chars detected]')",
            "        if invalid_chars and re.search(f\"[{re.escape(invalid_chars)}]\", text):",
            "            raise argparse.ArgumentTypeError(",
            "                f'Invalid {name}: \"{text}\" [invalid chars detected matching \"{invalid_chars}\"]'",
            "            )",
            "        if no_blanks and (text.startswith(\" \") or text.endswith(\" \")):",
            "            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [leading or trailing blanks detected]')",
            "        try:",
            "            text.encode(\"utf-8\", errors=\"strict\")",
            "        except UnicodeEncodeError:",
            "            # looks like text contains surrogate-escapes",
            "            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [contains non-unicode characters]')",
            "        return text",
            "",
            "    return validator",
            "",
            "",
            "comment_validator = text_validator(name=\"comment\", max_length=10000)",
            "",
            "",
            "def archivename_validator(text):",
            "    # we make sure that the archive name can be used as directory name (for borg mount)",
            "    MAX_PATH = 260  # Windows default. Since Win10, there is a registry setting LongPathsEnabled to get more.",
            "    MAX_DIRNAME = MAX_PATH - len(\"12345678.123\")",
            "    SAFETY_MARGIN = 48  # borgfs path: mountpoint / archivename / dir / dir / ... / file",
            "    MAX_ARCHIVENAME = MAX_DIRNAME - SAFETY_MARGIN",
            "    invalid_ctrl_chars = \"\".join(chr(i) for i in range(32))",
            "    # note: \":\" is also an invalid path char on windows, but we can not blacklist it,",
            "    # because e.g. our {now} placeholder creates ISO-8601 like output like 2022-12-10T20:47:42 .",
            "    invalid_chars = r\"/\" + r\"\\\"<|>?*\"  # posix + windows",
            "    validate_text = text_validator(",
            "        name=\"archive name\",",
            "        min_length=1,",
            "        max_length=MAX_ARCHIVENAME,",
            "        invalid_ctrl_chars=invalid_ctrl_chars,",
            "        invalid_chars=invalid_chars,",
            "        no_blanks=True,",
            "    )",
            "    return validate_text(text)",
            "",
            "",
            "class BaseFormatter(metaclass=abc.ABCMeta):",
            "    format: str",
            "    static_data: Dict[str, Any]",
            "    FIXED_KEYS: ClassVar[Dict[str, str]] = {",
            "        # Formatting aids",
            "        \"LF\": \"\\n\",",
            "        \"SPACE\": \" \",",
            "        \"TAB\": \"\\t\",",
            "        \"CR\": \"\\r\",",
            "        \"NUL\": \"\\0\",",
            "        \"NEWLINE\": \"\\n\",",
            "        \"NL\": \"\\n\",  # \\n is automatically converted to os.linesep on write",
            "    }",
            "    KEY_DESCRIPTIONS: ClassVar[Dict[str, str]] = {",
            "        \"NEWLINE\": \"OS dependent line separator\",",
            "        \"NL\": \"alias of NEWLINE\",",
            "        \"NUL\": \"NUL character for creating print0 / xargs -0 like output\",",
            "        \"SPACE\": \"space character\",",
            "        \"TAB\": \"tab character\",",
            "        \"CR\": \"carriage return character\",",
            "        \"LF\": \"line feed character\",",
            "    }",
            "    KEY_GROUPS: ClassVar[Tuple[Tuple[str, ...], ...]] = ((\"NEWLINE\", \"NL\", \"NUL\", \"SPACE\", \"TAB\", \"CR\", \"LF\"),)",
            "",
            "    def __init__(self, format: str, static: Dict[str, Any]) -> None:",
            "        self.format = partial_format(format, static)",
            "        self.static_data = static",
            "",
            "    @abc.abstractmethod",
            "    def get_item_data(self, item, jsonline=False) -> dict:",
            "        raise NotImplementedError",
            "",
            "    def format_item(self, item, jsonline=False, sort=False):",
            "        data = self.get_item_data(item, jsonline)",
            "        return (",
            "            f\"{json.dumps(data, cls=BorgJsonEncoder, sort_keys=sort)}\\n\" if jsonline else self.format.format_map(data)",
            "        )",
            "",
            "    @classmethod",
            "    def keys_help(cls):",
            "        help = []",
            "        keys: Set[str] = set()",
            "        keys.update(cls.KEY_DESCRIPTIONS.keys())",
            "        keys.update(key for group in cls.KEY_GROUPS for key in group)",
            "",
            "        for group in cls.KEY_GROUPS:",
            "            for key in group:",
            "                keys.remove(key)",
            "                text = \"- \" + key",
            "                if key in cls.KEY_DESCRIPTIONS:",
            "                    text += \": \" + cls.KEY_DESCRIPTIONS[key]",
            "                help.append(text)",
            "            help.append(\"\")",
            "        assert not keys, str(keys)",
            "        return \"\\n\".join(help)",
            "",
            "",
            "class ArchiveFormatter(BaseFormatter):",
            "    KEY_DESCRIPTIONS = {",
            "        \"archive\": \"archive name\",",
            "        \"name\": 'alias of \"archive\"',",
            "        \"comment\": \"archive comment\",",
            "        # *start* is the key used by borg-info for this timestamp, this makes the formats more compatible",
            "        \"start\": \"time (start) of creation of the archive\",",
            "        \"time\": 'alias of \"start\"',",
            "        \"end\": \"time (end) of creation of the archive\",",
            "        \"command_line\": \"command line which was used to create the archive\",",
            "        \"id\": \"internal ID of the archive\",",
            "        \"hostname\": \"hostname of host on which this archive was created\",",
            "        \"username\": \"username of user who created this archive\",",
            "        \"size\": \"size of this archive (data plus metadata, not considering compression and deduplication)\",",
            "        \"nfiles\": \"count of files in this archive\",",
            "    }",
            "    KEY_GROUPS = (",
            "        (\"archive\", \"name\", \"comment\", \"id\"),",
            "        (\"start\", \"time\", \"end\", \"command_line\"),",
            "        (\"hostname\", \"username\"),",
            "        (\"size\", \"nfiles\"),",
            "    )",
            "",
            "    def __init__(self, format, repository, manifest, key, *, iec=False):",
            "        static_data = {}  # here could be stuff on repo level, above archive level",
            "        static_data.update(self.FIXED_KEYS)",
            "        super().__init__(format, static_data)",
            "        self.repository = repository",
            "        self.manifest = manifest",
            "        self.key = key",
            "        self.name = None",
            "        self.id = None",
            "        self._archive = None",
            "        self.iec = iec",
            "        self.format_keys = {f[1] for f in Formatter().parse(format)}",
            "        self.call_keys = {",
            "            \"hostname\": partial(self.get_meta, \"hostname\", \"\"),",
            "            \"username\": partial(self.get_meta, \"username\", \"\"),",
            "            \"comment\": partial(self.get_meta, \"comment\", \"\"),",
            "            \"command_line\": partial(self.get_meta, \"command_line\", \"\"),",
            "            \"size\": partial(self.get_meta, \"size\", 0),",
            "            \"nfiles\": partial(self.get_meta, \"nfiles\", 0),",
            "            \"end\": self.get_ts_end,",
            "        }",
            "        self.used_call_keys = set(self.call_keys) & self.format_keys",
            "",
            "    def get_item_data(self, archive_info, jsonline=False):",
            "        self.name = archive_info.name",
            "        self.id = archive_info.id",
            "        item_data = {}",
            "        item_data.update({} if jsonline else self.static_data)",
            "        item_data.update(",
            "            {",
            "                \"name\": archive_info.name,",
            "                \"archive\": archive_info.name,",
            "                \"id\": bin_to_hex(archive_info.id),",
            "                \"time\": self.format_time(archive_info.ts),",
            "                \"start\": self.format_time(archive_info.ts),",
            "            }",
            "        )",
            "        for key in self.used_call_keys:",
            "            item_data[key] = self.call_keys[key]()",
            "",
            "        # Note: name and comment are validated, should never contain surrogate escapes.",
            "        # But unsure whether hostname, username, command_line could contain surrogate escapes, play safe:",
            "        for key in \"hostname\", \"username\", \"command_line\":",
            "            if key in item_data:",
            "                item_data.update(text_to_json(key, item_data[key]))",
            "        return item_data",
            "",
            "    @property",
            "    def archive(self):",
            "        \"\"\"lazy load / update loaded archive\"\"\"",
            "        if self._archive is None or self._archive.id != self.id:",
            "            from ..archive import Archive",
            "",
            "            self._archive = Archive(self.manifest, self.name, iec=self.iec)",
            "        return self._archive",
            "",
            "    def get_meta(self, key, default=None):",
            "        return self.archive.metadata.get(key, default)",
            "",
            "    def get_ts_end(self):",
            "        return self.format_time(self.archive.ts_end)",
            "",
            "    def format_time(self, ts):",
            "        return OutputTimestamp(ts)",
            "",
            "",
            "class ItemFormatter(BaseFormatter):",
            "    # we provide the hash algos from python stdlib (except shake_*) and additionally xxh64.",
            "    # shake_* is not provided because it uses an incompatible .digest() method to support variable length.",
            "    hash_algorithms = set(hashlib.algorithms_guaranteed).union({\"xxh64\"}).difference({\"shake_128\", \"shake_256\"})",
            "    KEY_DESCRIPTIONS = {",
            "        \"type\": \"file type (file, dir, symlink, ...)\",",
            "        \"mode\": \"file mode (as in stat)\",",
            "        \"uid\": \"user id of file owner\",",
            "        \"gid\": \"group id of file owner\",",
            "        \"user\": \"user name of file owner\",",
            "        \"group\": \"group name of file owner\",",
            "        \"path\": \"file path\",",
            "        \"target\": \"link target for symlinks\",",
            "        \"hlid\": \"hard link identity (same if hardlinking same fs object)\",",
            "        \"flags\": \"file flags\",",
            "        \"extra\": 'prepends {target} with \" -> \" for soft links and \" link to \" for hard links',",
            "        \"size\": \"file size\",",
            "        \"dsize\": \"deduplicated size\",",
            "        \"num_chunks\": \"number of chunks in this file\",",
            "        \"unique_chunks\": \"number of unique chunks in this file\",",
            "        \"mtime\": \"file modification time\",",
            "        \"ctime\": \"file change time\",",
            "        \"atime\": \"file access time\",",
            "        \"isomtime\": \"file modification time (ISO 8601 format)\",",
            "        \"isoctime\": \"file change time (ISO 8601 format)\",",
            "        \"isoatime\": \"file access time (ISO 8601 format)\",",
            "        \"xxh64\": \"XXH64 checksum of this file (note: this is NOT a cryptographic hash!)\",",
            "        \"health\": 'either \"healthy\" (file ok) or \"broken\" (if file has all-zero replacement chunks)',",
            "        \"archiveid\": \"internal ID of the archive\",",
            "        \"archivename\": \"name of the archive\",",
            "    }",
            "    KEY_GROUPS = (",
            "        (\"type\", \"mode\", \"uid\", \"gid\", \"user\", \"group\", \"path\", \"target\", \"hlid\", \"flags\"),",
            "        (\"size\", \"dsize\", \"num_chunks\", \"unique_chunks\"),",
            "        (\"mtime\", \"ctime\", \"atime\", \"isomtime\", \"isoctime\", \"isoatime\"),",
            "        tuple(sorted(hash_algorithms)),",
            "        (\"archiveid\", \"archivename\", \"extra\"),",
            "        (\"health\",),",
            "    )",
            "",
            "    KEYS_REQUIRING_CACHE = (\"dsize\", \"unique_chunks\")",
            "",
            "    @classmethod",
            "    def format_needs_cache(cls, format):",
            "        format_keys = {f[1] for f in Formatter().parse(format)}",
            "        return any(key in cls.KEYS_REQUIRING_CACHE for key in format_keys)",
            "",
            "    def __init__(self, archive, format):",
            "        from ..checksums import StreamingXXH64",
            "",
            "        static_data = {\"archivename\": archive.name, \"archiveid\": archive.fpr}",
            "        static_data.update(self.FIXED_KEYS)",
            "        super().__init__(format, static_data)",
            "        self.xxh64 = StreamingXXH64",
            "        self.archive = archive",
            "        self.format_keys = {f[1] for f in Formatter().parse(format)}",
            "        self.call_keys = {",
            "            \"size\": self.calculate_size,",
            "            \"dsize\": partial(self.sum_unique_chunks_metadata, lambda chunk: chunk.size),",
            "            \"num_chunks\": self.calculate_num_chunks,",
            "            \"unique_chunks\": partial(self.sum_unique_chunks_metadata, lambda chunk: 1),",
            "            \"isomtime\": partial(self.format_iso_time, \"mtime\"),",
            "            \"isoctime\": partial(self.format_iso_time, \"ctime\"),",
            "            \"isoatime\": partial(self.format_iso_time, \"atime\"),",
            "            \"mtime\": partial(self.format_time, \"mtime\"),",
            "            \"ctime\": partial(self.format_time, \"ctime\"),",
            "            \"atime\": partial(self.format_time, \"atime\"),",
            "        }",
            "        for hash_function in self.hash_algorithms:",
            "            self.call_keys[hash_function] = partial(self.hash_item, hash_function)",
            "        self.used_call_keys = set(self.call_keys) & self.format_keys",
            "",
            "    def get_item_data(self, item, jsonline=False):",
            "        item_data = {}",
            "        item_data.update({} if jsonline else self.static_data)",
            "",
            "        item_data.update(text_to_json(\"path\", item.path))",
            "        target = item.get(\"target\", \"\")",
            "        item_data.update(text_to_json(\"target\", target))",
            "        if not jsonline:",
            "            item_data[\"extra\"] = \"\" if not target else f\" -> {item_data['target']}\"",
            "",
            "        hlid = item.get(\"hlid\")",
            "        hlid = bin_to_hex(hlid) if hlid else \"\"",
            "        item_data[\"hlid\"] = hlid",
            "",
            "        mode = stat.filemode(item.mode)",
            "        item_type = mode[0]",
            "        item_data[\"type\"] = item_type",
            "        item_data[\"mode\"] = mode",
            "",
            "        item_data[\"uid\"] = item.get(\"uid\")  # int or None",
            "        item_data[\"gid\"] = item.get(\"gid\")  # int or None",
            "        item_data.update(text_to_json(\"user\", item.get(\"user\", str(item_data[\"uid\"]))))",
            "        item_data.update(text_to_json(\"group\", item.get(\"group\", str(item_data[\"gid\"]))))",
            "",
            "        if jsonline:",
            "            item_data[\"healthy\"] = \"chunks_healthy\" not in item",
            "        else:",
            "            item_data[\"health\"] = \"broken\" if \"chunks_healthy\" in item else \"healthy\"",
            "        item_data[\"flags\"] = item.get(\"bsdflags\")  # int if flags known, else (if flags unknown) None",
            "        for key in self.used_call_keys:",
            "            item_data[key] = self.call_keys[key](item)",
            "        return item_data",
            "",
            "    def sum_unique_chunks_metadata(self, metadata_func, item):",
            "        \"\"\"",
            "        sum unique chunks metadata, a unique chunk is a chunk which is referenced globally as often as it is in the",
            "        item",
            "",
            "        item: The item to sum its unique chunks' metadata",
            "        metadata_func: A function that takes a parameter of type ChunkIndexEntry and returns a number, used to return",
            "        the metadata needed from the chunk",
            "        \"\"\"",
            "        chunk_index = self.archive.cache.chunks",
            "        chunks = item.get(\"chunks\", [])",
            "        chunks_counter = Counter(c.id for c in chunks)",
            "        return sum(metadata_func(c) for c in chunks if chunk_index[c.id].refcount == chunks_counter[c.id])",
            "",
            "    def calculate_num_chunks(self, item):",
            "        return len(item.get(\"chunks\", []))",
            "",
            "    def calculate_size(self, item):",
            "        # note: does not support hardlink slaves, they will be size 0",
            "        return item.get_size()",
            "",
            "    def hash_item(self, hash_function, item):",
            "        if \"chunks\" not in item:",
            "            return \"\"",
            "        if hash_function == \"xxh64\":",
            "            hash = self.xxh64()",
            "        elif hash_function in self.hash_algorithms:",
            "            hash = hashlib.new(hash_function)",
            "        for data in self.archive.pipeline.fetch_many([c.id for c in item.chunks]):",
            "            hash.update(data)",
            "        return hash.hexdigest()",
            "",
            "    def format_time(self, key, item):",
            "        return OutputTimestamp(safe_timestamp(item.get(key) or item.mtime))",
            "",
            "    def format_iso_time(self, key, item):",
            "        return self.format_time(key, item).isoformat()",
            "",
            "",
            "class DiffFormatter(BaseFormatter):",
            "    KEY_DESCRIPTIONS = {",
            "        \"path\": \"archived file path\",",
            "        \"change\": \"all available changes\",",
            "        \"content\": \"file content change\",",
            "        \"mode\": \"file mode change\",",
            "        \"type\": \"file type change\",",
            "        \"owner\": \"file owner (user/group) change\",",
            "        \"user\": \"file user change\",",
            "        \"group\": \"file group change\",",
            "        \"link\": \"file link change\",",
            "        \"directory\": \"file directory change\",",
            "        \"blkdev\": \"file block device change\",",
            "        \"chrdev\": \"file character device change\",",
            "        \"fifo\": \"file fifo change\",",
            "        \"mtime\": \"file modification time change\",",
            "        \"ctime\": \"file change time change\",",
            "        \"isomtime\": \"file modification time change (ISO 8601)\",",
            "        \"isoctime\": \"file creation time change (ISO 8601)\",",
            "    }",
            "    KEY_GROUPS = (",
            "        (\"path\", \"change\"),",
            "        (\"content\", \"mode\", \"type\", \"owner\", \"group\", \"user\"),",
            "        (\"link\", \"directory\", \"blkdev\", \"chrdev\", \"fifo\"),",
            "        (\"mtime\", \"ctime\", \"isomtime\", \"isoctime\"),",
            "    )",
            "    METADATA = (\"mode\", \"type\", \"owner\", \"group\", \"user\", \"mtime\", \"ctime\")",
            "",
            "    def __init__(self, format, content_only=False):",
            "        static_data = {}",
            "        static_data.update(self.FIXED_KEYS)",
            "        super().__init__(format or \"{content}{link}{directory}{blkdev}{chrdev}{fifo} {path}{NL}\", static_data)",
            "        self.content_only = content_only",
            "        self.format_keys = {f[1] for f in Formatter().parse(format)}",
            "        self.call_keys = {",
            "            \"content\": self.format_content,",
            "            \"mode\": self.format_mode,",
            "            \"type\": partial(self.format_mode, filetype=True),",
            "            \"owner\": partial(self.format_owner),",
            "            \"group\": partial(self.format_owner, spec=\"group\"),",
            "            \"user\": partial(self.format_owner, spec=\"user\"),",
            "            \"link\": partial(self.format_other, \"link\"),",
            "            \"directory\": partial(self.format_other, \"directory\"),",
            "            \"blkdev\": partial(self.format_other, \"blkdev\"),",
            "            \"chrdev\": partial(self.format_other, \"chrdev\"),",
            "            \"fifo\": partial(self.format_other, \"fifo\"),",
            "            \"mtime\": partial(self.format_time, \"mtime\"),",
            "            \"ctime\": partial(self.format_time, \"ctime\"),",
            "            \"isomtime\": partial(self.format_iso_time, \"mtime\"),",
            "            \"isoctime\": partial(self.format_iso_time, \"ctime\"),",
            "        }",
            "        self.used_call_keys = set(self.call_keys) & self.format_keys",
            "        if self.content_only:",
            "            self.used_call_keys -= set(self.METADATA)",
            "",
            "    def get_item_data(self, item: \"ItemDiff\", jsonline=False) -> dict:",
            "        diff_data = {}",
            "        for key in self.used_call_keys:",
            "            diff_data[key] = self.call_keys[key](item)",
            "",
            "        change = []",
            "        for key in self.call_keys:",
            "            if key in (\"isomtime\", \"isoctime\"):",
            "                continue",
            "            if self.content_only and key in self.METADATA:",
            "                continue",
            "            change.append(self.call_keys[key](item))",
            "        diff_data[\"change\"] = \" \".join([v for v in change if v])",
            "        diff_data[\"path\"] = item.path",
            "        diff_data.update({} if jsonline else self.static_data)",
            "        return diff_data",
            "",
            "    def format_other(self, key, diff: \"ItemDiff\"):",
            "        change = diff.changes().get(key)",
            "        return f\"{change.diff_type}\".ljust(27) if change else \"\"  # 27 is the length of the content change",
            "",
            "    def format_mode(self, diff: \"ItemDiff\", filetype=False):",
            "        change = diff.type() if filetype else diff.mode()",
            "        return f\"[{change.diff_data['item1']} -> {change.diff_data['item2']}]\" if change else \"\"",
            "",
            "    def format_owner(self, diff: \"ItemDiff\", spec: Literal[\"owner\", \"user\", \"group\"] = \"owner\"):",
            "        if spec == \"user\":",
            "            change = diff.user()",
            "            return f\"[{change.diff_data['item1']} -> {change.diff_data['item2']}]\" if change else \"\"",
            "        if spec == \"group\":",
            "            change = diff.group()",
            "            return f\"[{change.diff_data['item1']} -> {change.diff_data['item2']}]\" if change else \"\"",
            "        if spec != \"owner\":",
            "            raise ValueError(f\"Invalid owner spec: {spec}\")",
            "        change = diff.owner()",
            "        if change:",
            "            return \"[{}:{} -> {}:{}]\".format(",
            "                change.diff_data[\"item1\"][0],",
            "                change.diff_data[\"item1\"][1],",
            "                change.diff_data[\"item2\"][0],",
            "                change.diff_data[\"item2\"][1],",
            "            )",
            "        return \"\"",
            "",
            "    def format_content(self, diff: \"ItemDiff\"):",
            "        change = diff.content()",
            "        if change:",
            "            if change.diff_type == \"added\":",
            "                return \"{}: {:>20}\".format(change.diff_type, format_file_size(change.diff_data[\"added\"]))",
            "            if change.diff_type == \"removed\":",
            "                return \"{}: {:>18}\".format(change.diff_type, format_file_size(change.diff_data[\"removed\"]))",
            "            if \"added\" not in change.diff_data and \"removed\" not in change.diff_data:",
            "                return \"modified:  (can't get size)\"",
            "            return \"{}: {:>8} {:>8}\".format(",
            "                change.diff_type,",
            "                format_file_size(change.diff_data[\"added\"], precision=1, sign=True),",
            "                format_file_size(-change.diff_data[\"removed\"], precision=1, sign=True),",
            "            )",
            "        return \"\"",
            "",
            "    def format_time(self, key, diff: \"ItemDiff\"):",
            "        change = diff.changes().get(key)",
            "        return f\"[{key}: {change.diff_data['item1']} -> {change.diff_data['item2']}]\" if change else \"\"",
            "",
            "    def format_iso_time(self, key, diff: \"ItemDiff\"):",
            "        change = diff.changes().get(key)",
            "        return (",
            "            f\"[{key}: {change.diff_data['item1'].isoformat()} -> {change.diff_data['item2'].isoformat()}]\"",
            "            if change",
            "            else \"\"",
            "        )",
            "",
            "",
            "def file_status(mode):",
            "    if stat.S_ISREG(mode):",
            "        return \"A\"",
            "    elif stat.S_ISDIR(mode):",
            "        return \"d\"",
            "    elif stat.S_ISBLK(mode):",
            "        return \"b\"",
            "    elif stat.S_ISCHR(mode):",
            "        return \"c\"",
            "    elif stat.S_ISLNK(mode):",
            "        return \"s\"",
            "    elif stat.S_ISFIFO(mode):",
            "        return \"f\"",
            "    return \"?\"",
            "",
            "",
            "def clean_lines(lines, lstrip=None, rstrip=None, remove_empty=True, remove_comments=True):",
            "    \"\"\"",
            "    clean lines (usually read from a config file):",
            "",
            "    1. strip whitespace (left and right), 2. remove empty lines, 3. remove comments.",
            "",
            "    note: only \"pure comment lines\" are supported, no support for \"trailing comments\".",
            "",
            "    :param lines: input line iterator (e.g. list or open text file) that gives unclean input lines",
            "    :param lstrip: lstrip call arguments or False, if lstripping is not desired",
            "    :param rstrip: rstrip call arguments or False, if rstripping is not desired",
            "    :param remove_comments: remove comment lines (lines starting with \"#\")",
            "    :param remove_empty: remove empty lines",
            "    :return: yields processed lines",
            "    \"\"\"",
            "    for line in lines:",
            "        if lstrip is not False:",
            "            line = line.lstrip(lstrip)",
            "        if rstrip is not False:",
            "            line = line.rstrip(rstrip)",
            "        if remove_empty and not line:",
            "            continue",
            "        if remove_comments and line.startswith(\"#\"):",
            "            continue",
            "        yield line",
            "",
            "",
            "def swidth_slice(string, max_width):",
            "    \"\"\"",
            "    Return a slice of *max_width* cells from *string*.",
            "",
            "    Negative *max_width* means from the end of string.",
            "",
            "    *max_width* is in units of character cells (or \"columns\").",
            "    Latin characters are usually one cell wide, many CJK characters are two cells wide.",
            "    \"\"\"",
            "    from ..platform import swidth",
            "",
            "    reverse = max_width < 0",
            "    max_width = abs(max_width)",
            "    if reverse:",
            "        string = reversed(string)",
            "    current_swidth = 0",
            "    result = []",
            "    for character in string:",
            "        current_swidth += swidth(character)",
            "        if current_swidth > max_width:",
            "            break",
            "        result.append(character)",
            "    if reverse:",
            "        result.reverse()",
            "    return \"\".join(result)",
            "",
            "",
            "def ellipsis_truncate(msg, space):",
            "    \"\"\"",
            "    shorten a long string by adding ellipsis between it and return it, example:",
            "    this_is_a_very_long_string -------> this_is..._string",
            "    \"\"\"",
            "    from ..platform import swidth",
            "",
            "    ellipsis_width = swidth(\"...\")",
            "    msg_width = swidth(msg)",
            "    if space < 8:",
            "        # if there is very little space, just show ...",
            "        return \"...\" + \" \" * (space - ellipsis_width)",
            "    if space < ellipsis_width + msg_width:",
            "        return f\"{swidth_slice(msg, space // 2 - ellipsis_width)}...{swidth_slice(msg, -space // 2)}\"",
            "    return msg + \" \" * (space - msg_width)",
            "",
            "",
            "class BorgJsonEncoder(json.JSONEncoder):",
            "    def default(self, o):",
            "        from ..repository import Repository",
            "        from ..remote import RemoteRepository",
            "        from ..archive import Archive",
            "        from ..cache import LocalCache, AdHocCache",
            "",
            "        if isinstance(o, Repository) or isinstance(o, RemoteRepository):",
            "            return {\"id\": bin_to_hex(o.id), \"location\": o._location.canonical_path()}",
            "        if isinstance(o, Archive):",
            "            return o.info()",
            "        if isinstance(o, LocalCache):",
            "            return {\"path\": o.path, \"stats\": o.stats()}",
            "        if isinstance(o, AdHocCache):",
            "            return {\"stats\": o.stats()}",
            "        if callable(getattr(o, \"to_json\", None)):",
            "            return o.to_json()",
            "        return super().default(o)",
            "",
            "",
            "def basic_json_data(manifest, *, cache=None, extra=None):",
            "    key = manifest.key",
            "    data = extra or {}",
            "    data.update({\"repository\": BorgJsonEncoder().default(manifest.repository), \"encryption\": {\"mode\": key.ARG_NAME}})",
            "    data[\"repository\"][\"last_modified\"] = OutputTimestamp(manifest.last_timestamp)",
            "    if key.NAME.startswith(\"key file\"):",
            "        data[\"encryption\"][\"keyfile\"] = key.find_key()",
            "    if cache:",
            "        data[\"cache\"] = cache",
            "    return data",
            "",
            "",
            "def json_dump(obj):",
            "    \"\"\"Dump using BorgJSONEncoder.\"\"\"",
            "    return json.dumps(obj, sort_keys=True, indent=4, cls=BorgJsonEncoder)",
            "",
            "",
            "def json_print(obj):",
            "    print(json_dump(obj))",
            "",
            "",
            "def prepare_dump_dict(d):",
            "    def decode_bytes(value):",
            "        # this should somehow be reversible later, but usual strings should",
            "        # look nice and chunk ids should mostly show in hex. Use a special",
            "        # inband signaling character (ASCII DEL) to distinguish between",
            "        # decoded and hex mode.",
            "        if not value.startswith(b\"\\x7f\"):",
            "            try:",
            "                value = value.decode()",
            "                return value",
            "            except UnicodeDecodeError:",
            "                pass",
            "        return \"\\u007f\" + bin_to_hex(value)",
            "",
            "    def decode_tuple(t):",
            "        res = []",
            "        for value in t:",
            "            if isinstance(value, dict):",
            "                value = decode(value)",
            "            elif isinstance(value, tuple) or isinstance(value, list):",
            "                value = decode_tuple(value)",
            "            elif isinstance(value, bytes):",
            "                value = decode_bytes(value)",
            "            res.append(value)",
            "        return res",
            "",
            "    def decode(d):",
            "        res = OrderedDict()",
            "        for key, value in d.items():",
            "            if isinstance(value, dict):",
            "                value = decode(value)",
            "            elif isinstance(value, (tuple, list)):",
            "                value = decode_tuple(value)",
            "            elif isinstance(value, bytes):",
            "                value = decode_bytes(value)",
            "            elif isinstance(value, Timestamp):",
            "                value = value.to_unix_nano()",
            "            if isinstance(key, bytes):",
            "                key = key.decode()",
            "            res[key] = value",
            "        return res",
            "",
            "    return decode(d)",
            "",
            "",
            "class Highlander(argparse.Action):",
            "    \"\"\"make sure some option is only given once\"\"\"",
            "",
            "    def __init__(self, *args, **kwargs):",
            "        self.__called = False",
            "        super().__init__(*args, **kwargs)",
            "",
            "    def __call__(self, parser, namespace, values, option_string=None):",
            "        if self.__called:",
            "            raise argparse.ArgumentError(self, \"There can be only one.\")",
            "        self.__called = True",
            "        setattr(namespace, self.dest, values)",
            "",
            "",
            "class MakePathSafeAction(Highlander):",
            "    def __call__(self, parser, namespace, path, option_string=None):",
            "        try:",
            "            sanitized_path = make_path_safe(path)",
            "        except ValueError as e:",
            "            raise argparse.ArgumentError(self, e)",
            "        if sanitized_path == \".\":",
            "            raise argparse.ArgumentError(self, f\"{path!r} is not a valid file name\")",
            "        setattr(namespace, self.dest, sanitized_path)"
        ],
        "afterPatchFile": [
            "import abc",
            "import argparse",
            "import base64",
            "import hashlib",
            "import json",
            "import os",
            "import os.path",
            "import re",
            "import shlex",
            "import stat",
            "import uuid",
            "from typing import Dict, Set, Tuple, ClassVar, Any, TYPE_CHECKING, Literal",
            "from binascii import hexlify",
            "from collections import Counter, OrderedDict",
            "from datetime import datetime, timezone",
            "from functools import partial",
            "from string import Formatter",
            "",
            "from ..logger import create_logger",
            "",
            "logger = create_logger()",
            "",
            "from .errors import Error",
            "from .fs import get_keys_dir, make_path_safe",
            "from .msgpack import Timestamp",
            "from .time import OutputTimestamp, format_time, safe_timestamp",
            "from .. import __version__ as borg_version",
            "from .. import __version_tuple__ as borg_version_tuple",
            "from ..constants import *  # NOQA",
            "",
            "if TYPE_CHECKING:",
            "    from ..item import ItemDiff",
            "",
            "",
            "def bin_to_hex(binary):",
            "    return hexlify(binary).decode(\"ascii\")",
            "",
            "",
            "def safe_decode(s, coding=\"utf-8\", errors=\"surrogateescape\"):",
            "    \"\"\"decode bytes to str, with round-tripping \"invalid\" bytes\"\"\"",
            "    if s is None:",
            "        return None",
            "    return s.decode(coding, errors)",
            "",
            "",
            "def safe_encode(s, coding=\"utf-8\", errors=\"surrogateescape\"):",
            "    \"\"\"encode str to bytes, with round-tripping \"invalid\" bytes\"\"\"",
            "    if s is None:",
            "        return None",
            "    return s.encode(coding, errors)",
            "",
            "",
            "def remove_surrogates(s, errors=\"replace\"):",
            "    \"\"\"Replace surrogates generated by fsdecode with '?'\"\"\"",
            "    return s.encode(\"utf-8\", errors).decode(\"utf-8\")",
            "",
            "",
            "def binary_to_json(key, value):",
            "    assert isinstance(key, str)",
            "    assert isinstance(value, bytes)",
            "    return {key + \"_b64\": base64.b64encode(value).decode(\"ascii\")}",
            "",
            "",
            "def text_to_json(key, value):",
            "    \"\"\"",
            "    Return a dict made from key/value that can be fed safely into a JSON encoder.",
            "",
            "    JSON can only contain pure, valid unicode (but not: unicode with surrogate escapes).",
            "",
            "    But sometimes we have to deal with such values and we do it like this:",
            "    - <key>: value as pure unicode text (surrogate escapes, if any, replaced by ?)",
            "    - <key>_b64: value as base64 encoded binary representation (only set if value has surrogate-escapes)",
            "    \"\"\"",
            "    coding = \"utf-8\"",
            "    assert isinstance(key, str)",
            "    assert isinstance(value, str)  # str might contain surrogate escapes",
            "    data = {}",
            "    try:",
            "        value.encode(coding, errors=\"strict\")  # check if pure unicode",
            "    except UnicodeEncodeError:",
            "        # value has surrogate escape sequences",
            "        data[key] = remove_surrogates(value)",
            "        value_bytes = value.encode(coding, errors=\"surrogateescape\")",
            "        data.update(binary_to_json(key, value_bytes))",
            "    else:",
            "        # value is pure unicode",
            "        data[key] = value",
            "        # we do not give the b64 representation, not needed",
            "    return data",
            "",
            "",
            "def join_cmd(argv, rs=False):",
            "    cmd = shlex.join(argv)",
            "    return remove_surrogates(cmd) if rs else cmd",
            "",
            "",
            "def eval_escapes(s):",
            "    \"\"\"Evaluate literal escape sequences in a string (eg `\\\\n` -> `\\n`).\"\"\"",
            "    return s.encode(\"ascii\", \"backslashreplace\").decode(\"unicode-escape\")",
            "",
            "",
            "def decode_dict(d, keys, encoding=\"utf-8\", errors=\"surrogateescape\"):",
            "    for key in keys:",
            "        if isinstance(d.get(key), bytes):",
            "            d[key] = d[key].decode(encoding, errors)",
            "    return d",
            "",
            "",
            "def positive_int_validator(value):",
            "    \"\"\"argparse type for positive integers\"\"\"",
            "    int_value = int(value)",
            "    if int_value <= 0:",
            "        raise argparse.ArgumentTypeError(\"A positive integer is required: %s\" % value)",
            "    return int_value",
            "",
            "",
            "def interval(s):",
            "    \"\"\"Convert a string representing a valid interval to a number of hours.\"\"\"",
            "    multiplier = {\"H\": 1, \"d\": 24, \"w\": 24 * 7, \"m\": 24 * 31, \"y\": 24 * 365}",
            "",
            "    if s.endswith(tuple(multiplier.keys())):",
            "        number = s[:-1]",
            "        suffix = s[-1]",
            "    else:",
            "        # range suffixes in ascending multiplier order",
            "        ranges = [k for k, v in sorted(multiplier.items(), key=lambda t: t[1])]",
            "        raise argparse.ArgumentTypeError(f'Unexpected interval time unit \"{s[-1]}\": expected one of {ranges!r}')",
            "",
            "    try:",
            "        hours = int(number) * multiplier[suffix]",
            "    except ValueError:",
            "        hours = -1",
            "",
            "    if hours <= 0:",
            "        raise argparse.ArgumentTypeError('Unexpected interval number \"%s\": expected an integer greater than 0' % number)",
            "",
            "    return hours",
            "",
            "",
            "def ChunkerParams(s):",
            "    params = s.strip().split(\",\")",
            "    count = len(params)",
            "    if count == 0:",
            "        raise argparse.ArgumentTypeError(\"no chunker params given\")",
            "    algo = params[0].lower()",
            "    if algo == CH_FAIL and count == 3:",
            "        block_size = int(params[1])",
            "        fail_map = str(params[2])",
            "        return algo, block_size, fail_map",
            "    if algo == CH_FIXED and 2 <= count <= 3:  # fixed, block_size[, header_size]",
            "        block_size = int(params[1])",
            "        header_size = int(params[2]) if count == 3 else 0",
            "        if block_size < 64:",
            "            # we are only disallowing the most extreme cases of abuse here - this does NOT imply",
            "            # that cutting chunks of the minimum allowed size is efficient concerning storage",
            "            # or in-memory chunk management.",
            "            # choose the block (chunk) size wisely: if you have a lot of data and you cut",
            "            # it into very small chunks, you are asking for trouble!",
            "            raise argparse.ArgumentTypeError(\"block_size must not be less than 64 Bytes\")",
            "        if block_size > MAX_DATA_SIZE or header_size > MAX_DATA_SIZE:",
            "            raise argparse.ArgumentTypeError(",
            "                \"block_size and header_size must not exceed MAX_DATA_SIZE [%d]\" % MAX_DATA_SIZE",
            "            )",
            "        return algo, block_size, header_size",
            "    if algo == \"default\" and count == 1:  # default",
            "        return CHUNKER_PARAMS",
            "    # this must stay last as it deals with old-style compat mode (no algorithm, 4 params, buzhash):",
            "    if algo == CH_BUZHASH and count == 5 or count == 4:  # [buzhash, ]chunk_min, chunk_max, chunk_mask, window_size",
            "        chunk_min, chunk_max, chunk_mask, window_size = (int(p) for p in params[count - 4 :])",
            "        if not (chunk_min <= chunk_mask <= chunk_max):",
            "            raise argparse.ArgumentTypeError(\"required: chunk_min <= chunk_mask <= chunk_max\")",
            "        if chunk_min < 6:",
            "            # see comment in 'fixed' algo check",
            "            raise argparse.ArgumentTypeError(",
            "                \"min. chunk size exponent must not be less than 6 (2^6 = 64B min. chunk size)\"",
            "            )",
            "        if chunk_max > 23:",
            "            raise argparse.ArgumentTypeError(",
            "                \"max. chunk size exponent must not be more than 23 (2^23 = 8MiB max. chunk size)\"",
            "            )",
            "        return CH_BUZHASH, chunk_min, chunk_max, chunk_mask, window_size",
            "    raise argparse.ArgumentTypeError(\"invalid chunker params\")",
            "",
            "",
            "def FilesCacheMode(s):",
            "    ENTRIES_MAP = dict(ctime=\"c\", mtime=\"m\", size=\"s\", inode=\"i\", rechunk=\"r\", disabled=\"d\")",
            "    VALID_MODES = (\"cis\", \"ims\", \"cs\", \"ms\", \"cr\", \"mr\", \"d\", \"s\")  # letters in alpha order",
            "    entries = set(s.strip().split(\",\"))",
            "    if not entries <= set(ENTRIES_MAP):",
            "        raise argparse.ArgumentTypeError(",
            "            \"cache mode must be a comma-separated list of: %s\" % \",\".join(sorted(ENTRIES_MAP))",
            "        )",
            "    short_entries = {ENTRIES_MAP[entry] for entry in entries}",
            "    mode = \"\".join(sorted(short_entries))",
            "    if mode not in VALID_MODES:",
            "        raise argparse.ArgumentTypeError(\"cache mode short must be one of: %s\" % \",\".join(VALID_MODES))",
            "    return mode",
            "",
            "",
            "def partial_format(format, mapping):",
            "    \"\"\"",
            "    Apply format.format_map(mapping) while preserving unknown keys",
            "",
            "    Does not support attribute access, indexing and ![rsa] conversions",
            "    \"\"\"",
            "    for key, value in mapping.items():",
            "        key = re.escape(key)",
            "        format = re.sub(",
            "            rf\"(?<!\\{{)((\\{{{key}\\}})|(\\{{{key}:[^\\}}]*\\}}))\", lambda match: match.group(1).format_map(mapping), format",
            "        )",
            "    return format",
            "",
            "",
            "class DatetimeWrapper:",
            "    def __init__(self, dt):",
            "        self.dt = dt",
            "",
            "    def __format__(self, format_spec):",
            "        if format_spec == \"\":",
            "            format_spec = ISO_FORMAT_NO_USECS",
            "        return self.dt.__format__(format_spec)",
            "",
            "",
            "class PlaceholderError(Error):",
            "    \"\"\"Formatting Error: \"{}\".format({}): {}({})\"\"\"",
            "",
            "",
            "class InvalidPlaceholder(PlaceholderError):",
            "    \"\"\"Invalid placeholder \"{}\" in string: {}\"\"\"",
            "",
            "",
            "def format_line(format, data):",
            "    for _, key, _, conversion in Formatter().parse(format):",
            "        if not key:",
            "            continue",
            "        if conversion or key not in data:",
            "            raise InvalidPlaceholder(key, format)",
            "    try:",
            "        return format.format_map(data)",
            "    except Exception as e:",
            "        raise PlaceholderError(format, data, e.__class__.__name__, str(e))",
            "",
            "",
            "def _replace_placeholders(text, overrides={}):",
            "    \"\"\"Replace placeholders in text with their values.\"\"\"",
            "    from ..platform import fqdn, hostname, getosusername",
            "",
            "    current_time = datetime.now(timezone.utc)",
            "    data = {",
            "        \"pid\": os.getpid(),",
            "        \"fqdn\": fqdn,",
            "        \"reverse-fqdn\": \".\".join(reversed(fqdn.split(\".\"))),",
            "        \"hostname\": hostname,",
            "        \"now\": DatetimeWrapper(current_time.astimezone()),",
            "        \"utcnow\": DatetimeWrapper(current_time),",
            "        \"user\": getosusername(),",
            "        \"uuid4\": str(uuid.uuid4()),",
            "        \"borgversion\": borg_version,",
            "        \"borgmajor\": \"%d\" % borg_version_tuple[:1],",
            "        \"borgminor\": \"%d.%d\" % borg_version_tuple[:2],",
            "        \"borgpatch\": \"%d.%d.%d\" % borg_version_tuple[:3],",
            "        **overrides,",
            "    }",
            "    return format_line(text, data)",
            "",
            "",
            "class PlaceholderReplacer:",
            "    def __init__(self):",
            "        self.reset()",
            "",
            "    def override(self, key, value):",
            "        self.overrides[key] = value",
            "",
            "    def reset(self):",
            "        self.overrides = {}",
            "",
            "    def __call__(self, text, overrides=None):",
            "        ovr = {}",
            "        ovr.update(self.overrides)",
            "        ovr.update(overrides or {})",
            "        return _replace_placeholders(text, overrides=ovr)",
            "",
            "",
            "replace_placeholders = PlaceholderReplacer()",
            "",
            "",
            "def SortBySpec(text):",
            "    from ..manifest import AI_HUMAN_SORT_KEYS",
            "",
            "    for token in text.split(\",\"):",
            "        if token not in AI_HUMAN_SORT_KEYS:",
            "            raise argparse.ArgumentTypeError(\"Invalid sort key: %s\" % token)",
            "    return text.replace(\"timestamp\", \"ts\")",
            "",
            "",
            "def format_file_size(v, precision=2, sign=False, iec=False):",
            "    \"\"\"Format file size into a human friendly format\"\"\"",
            "    fn = sizeof_fmt_iec if iec else sizeof_fmt_decimal",
            "    return fn(v, suffix=\"B\", sep=\" \", precision=precision, sign=sign)",
            "",
            "",
            "class FileSize(int):",
            "    def __new__(cls, value, iec=False):",
            "        obj = int.__new__(cls, value)",
            "        obj.iec = iec",
            "        return obj",
            "",
            "    def __format__(self, format_spec):",
            "        return format_file_size(int(self), iec=self.iec).__format__(format_spec)",
            "",
            "",
            "def parse_file_size(s):",
            "    \"\"\"Return int from file size (1234, 55G, 1.7T).\"\"\"",
            "    if not s:",
            "        return int(s)  # will raise",
            "    suffix = s[-1]",
            "    power = 1000",
            "    try:",
            "        factor = {\"K\": power, \"M\": power**2, \"G\": power**3, \"T\": power**4, \"P\": power**5}[suffix]",
            "        s = s[:-1]",
            "    except KeyError:",
            "        factor = 1",
            "    return int(float(s) * factor)",
            "",
            "",
            "def parse_storage_quota(storage_quota):",
            "    parsed = parse_file_size(storage_quota)",
            "    if parsed < parse_file_size(\"10M\"):",
            "        raise argparse.ArgumentTypeError(\"quota is too small (%s). At least 10M are required.\" % storage_quota)",
            "    return parsed",
            "",
            "",
            "def sizeof_fmt(num, suffix=\"B\", units=None, power=None, sep=\"\", precision=2, sign=False):",
            "    sign = \"+\" if sign and num > 0 else \"\"",
            "    fmt = \"{0:{1}.{2}f}{3}{4}{5}\"",
            "    prec = 0",
            "    for unit in units[:-1]:",
            "        if abs(round(num, precision)) < power:",
            "            break",
            "        num /= float(power)",
            "        prec = precision",
            "    else:",
            "        unit = units[-1]",
            "    return fmt.format(num, sign, prec, sep, unit, suffix)",
            "",
            "",
            "def sizeof_fmt_iec(num, suffix=\"B\", sep=\"\", precision=2, sign=False):",
            "    return sizeof_fmt(",
            "        num,",
            "        suffix=suffix,",
            "        sep=sep,",
            "        precision=precision,",
            "        sign=sign,",
            "        units=[\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\", \"Yi\"],",
            "        power=1024,",
            "    )",
            "",
            "",
            "def sizeof_fmt_decimal(num, suffix=\"B\", sep=\"\", precision=2, sign=False):",
            "    return sizeof_fmt(",
            "        num,",
            "        suffix=suffix,",
            "        sep=sep,",
            "        precision=precision,",
            "        sign=sign,",
            "        units=[\"\", \"k\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\", \"Y\"],",
            "        power=1000,",
            "    )",
            "",
            "",
            "def format_archive(archive):",
            "    return \"%-36s %s [%s]\" % (archive.name, format_time(archive.ts), bin_to_hex(archive.id))",
            "",
            "",
            "def parse_stringified_list(s):",
            "    items = re.split(\" *, *\", s)",
            "    return [item for item in items if item != \"\"]",
            "",
            "",
            "class Location:",
            "    \"\"\"Object representing a repository location\"\"\"",
            "",
            "    # user must not contain \"@\", \":\" or \"/\".",
            "    # Quoting adduser error message:",
            "    # \"To avoid problems, the username should consist only of letters, digits,",
            "    # underscores, periods, at signs and dashes, and not start with a dash",
            "    # (as defined by IEEE Std 1003.1-2001).\"",
            "    # We use \"@\" as separator between username and hostname, so we must",
            "    # disallow it within the pure username part.",
            "    optional_user_re = r\"\"\"",
            "        (?:(?P<user>[^@:/]+)@)?",
            "    \"\"\"",
            "",
            "    # path must not contain :: (it ends at :: or string end), but may contain single colons.",
            "    # to avoid ambiguities with other regexes, it must also not start with \":\" nor with \"//\" nor with \"ssh://\".",
            "    local_path_re = r\"\"\"",
            "        (?!(:|//|ssh://|socket://))                         # not starting with \":\" or // or ssh:// or socket://",
            "        (?P<path>([^:]|(:(?!:)))+)                          # any chars, but no \"::\"",
            "        \"\"\"",
            "",
            "    # file_path must not contain :: (it ends at :: or string end), but may contain single colons.",
            "    # it must start with a / and that slash is part of the path.",
            "    file_path_re = r\"\"\"",
            "        (?P<path>(([^/]*)/([^:]|(:(?!:)))+))                # start opt. servername, then /, then any chars, but no \"::\"",
            "        \"\"\"",
            "",
            "    # abs_path must not contain :: (it ends at :: or string end), but may contain single colons.",
            "    # it must start with a / and that slash is part of the path.",
            "    abs_path_re = r\"\"\"",
            "        (?P<path>(/([^:]|(:(?!:)))+))                       # start with /, then any chars, but no \"::\"",
            "        \"\"\"",
            "",
            "    # host NAME, or host IP ADDRESS (v4 or v6, v6 must be in square brackets)",
            "    host_re = r\"\"\"",
            "        (?P<host>(",
            "            (?!\\[)[^:/]+(?<!\\])     # hostname or v4 addr, not containing : or / (does not match v6 addr: no brackets!)",
            "            |",
            "            \\[[0-9a-fA-F:.]+\\])     # ipv6 address in brackets",
            "        )",
            "    \"\"\"",
            "",
            "    # regexes for misc. kinds of supported location specifiers:",
            "    ssh_re = re.compile(",
            "        r\"\"\"",
            "        (?P<proto>ssh)://                                       # ssh://",
            "        \"\"\"",
            "        + optional_user_re",
            "        + host_re",
            "        + r\"\"\"                 # user@  (optional), host name or address",
            "        (?::(?P<port>\\d+))?                                     # :port (optional)",
            "        \"\"\"",
            "        + abs_path_re,",
            "        re.VERBOSE,",
            "    )  # path",
            "",
            "    socket_re = re.compile(",
            "        r\"\"\"",
            "        (?P<proto>socket)://                                    # socket://",
            "        \"\"\"",
            "        + abs_path_re,",
            "        re.VERBOSE,",
            "    )  # path",
            "",
            "    file_re = re.compile(",
            "        r\"\"\"",
            "        (?P<proto>file)://                                      # file://",
            "        \"\"\"",
            "        + file_path_re,",
            "        re.VERBOSE,",
            "    )  # servername/path or path",
            "",
            "    local_re = re.compile(local_path_re, re.VERBOSE)  # local path",
            "",
            "    win_file_re = re.compile(",
            "        r\"\"\"",
            "        (?:file://)?                                        # optional file protocol",
            "        (?P<path>",
            "            (?:[a-zA-Z]:)?                                  # Drive letter followed by a colon (optional)",
            "            (?:[^:]+)                                       # Anything which does not contain a :, at least one char",
            "        )",
            "        \"\"\",",
            "        re.VERBOSE,",
            "    )",
            "",
            "    def __init__(self, text=\"\", overrides={}, other=False):",
            "        self.repo_env_var = \"BORG_OTHER_REPO\" if other else \"BORG_REPO\"",
            "        self.valid = False",
            "        self.proto = None",
            "        self.user = None",
            "        self._host = None",
            "        self.port = None",
            "        self.path = None",
            "        self.raw = None",
            "        self.processed = None",
            "        self.parse(text, overrides)",
            "",
            "    def parse(self, text, overrides={}):",
            "        if not text:",
            "            # we did not get a text to parse, so we try to fetch from the environment",
            "            text = os.environ.get(self.repo_env_var)",
            "            if text is None:",
            "                return",
            "",
            "        self.raw = text  # as given by user, might contain placeholders",
            "        self.processed = replace_placeholders(self.raw, overrides)  # after placeholder replacement",
            "        valid = self._parse(self.processed)",
            "        if valid:",
            "            self.valid = True",
            "        else:",
            "            raise ValueError('Invalid location format: \"%s\"' % self.processed)",
            "",
            "    def _parse(self, text):",
            "        def normpath_special(p):",
            "            # avoid that normpath strips away our relative path hack and even makes p absolute",
            "            relative = p.startswith(\"/./\")",
            "            p = os.path.normpath(p)",
            "            return (\"/.\" + p) if relative else p",
            "",
            "        m = self.ssh_re.match(text)",
            "        if m:",
            "            self.proto = m.group(\"proto\")",
            "            self.user = m.group(\"user\")",
            "            self._host = m.group(\"host\")",
            "            self.port = m.group(\"port\") and int(m.group(\"port\")) or None",
            "            self.path = normpath_special(m.group(\"path\"))",
            "            return True",
            "        m = self.file_re.match(text)",
            "        if m:",
            "            self.proto = m.group(\"proto\")",
            "            self.path = normpath_special(m.group(\"path\"))",
            "            return True",
            "        m = self.socket_re.match(text)",
            "        if m:",
            "            self.proto = m.group(\"proto\")",
            "            self.path = normpath_special(m.group(\"path\"))",
            "            return True",
            "        m = self.local_re.match(text)",
            "        if m:",
            "            self.proto = \"file\"",
            "            self.path = normpath_special(m.group(\"path\"))",
            "            return True",
            "        return False",
            "",
            "    def __str__(self):",
            "        items = [",
            "            \"proto=%r\" % self.proto,",
            "            \"user=%r\" % self.user,",
            "            \"host=%r\" % self.host,",
            "            \"port=%r\" % self.port,",
            "            \"path=%r\" % self.path,",
            "        ]",
            "        return \", \".join(items)",
            "",
            "    def to_key_filename(self):",
            "        name = re.sub(r\"[^\\w]\", \"_\", self.path).strip(\"_\")",
            "        if self.proto not in (\"file\", \"socket\"):",
            "            name = re.sub(r\"[^\\w]\", \"_\", self.host) + \"__\" + name",
            "        if len(name) > 100:",
            "            # Limit file names to some reasonable length. Most file systems",
            "            # limit them to 255 [unit of choice]; due to variations in unicode",
            "            # handling we truncate to 100 *characters*.",
            "            name = name[:100]",
            "        return os.path.join(get_keys_dir(), name)",
            "",
            "    def __repr__(self):",
            "        return \"Location(%s)\" % self",
            "",
            "    @property",
            "    def host(self):",
            "        # strip square brackets used for IPv6 addrs",
            "        if self._host is not None:",
            "            return self._host.lstrip(\"[\").rstrip(\"]\")",
            "",
            "    def canonical_path(self):",
            "        if self.proto in (\"file\", \"socket\"):",
            "            return self.path",
            "        else:",
            "            if self.path and self.path.startswith(\"~\"):",
            "                path = \"/\" + self.path  # /~/x = path x relative to home dir",
            "            elif self.path and not self.path.startswith(\"/\"):",
            "                path = \"/./\" + self.path  # /./x = path x relative to cwd",
            "            else:",
            "                path = self.path",
            "            return \"ssh://{}{}{}{}\".format(",
            "                f\"{self.user}@\" if self.user else \"\",",
            "                self._host,  # needed for ipv6 addrs",
            "                f\":{self.port}\" if self.port else \"\",",
            "                path,",
            "            )",
            "",
            "    def with_timestamp(self, timestamp):",
            "        # note: this only affects the repository URL/path, not the archive name!",
            "        return Location(",
            "            self.raw,",
            "            overrides={",
            "                \"now\": DatetimeWrapper(timestamp),",
            "                \"utcnow\": DatetimeWrapper(timestamp.astimezone(timezone.utc)),",
            "            },",
            "        )",
            "",
            "",
            "def location_validator(proto=None, other=False):",
            "    def validator(text):",
            "        try:",
            "            loc = Location(text, other=other)",
            "        except ValueError as err:",
            "            raise argparse.ArgumentTypeError(str(err)) from None",
            "        if proto is not None and loc.proto != proto:",
            "            if proto == \"file\":",
            "                raise argparse.ArgumentTypeError('\"%s\": Repository must be local' % text)",
            "            else:",
            "                raise argparse.ArgumentTypeError('\"%s\": Repository must be remote' % text)",
            "        return loc",
            "",
            "    return validator",
            "",
            "",
            "def relative_time_marker_validator(text: str):",
            "    time_marker_regex = r\"^\\d+[md]$\"",
            "    match = re.compile(time_marker_regex).search(text)",
            "    if not match:",
            "        raise argparse.ArgumentTypeError(f\"Invalid relative time marker used: {text}\")",
            "    else:",
            "        return text",
            "",
            "",
            "def text_validator(*, name, max_length, min_length=0, invalid_ctrl_chars=\"\\0\", invalid_chars=\"\", no_blanks=False):",
            "    def validator(text):",
            "        assert isinstance(text, str)",
            "        if len(text) < min_length:",
            "            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [length < {min_length}]')",
            "        if len(text) > max_length:",
            "            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [length > {max_length}]')",
            "        if invalid_ctrl_chars and re.search(f\"[{re.escape(invalid_ctrl_chars)}]\", text):",
            "            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [invalid control chars detected]')",
            "        if invalid_chars and re.search(f\"[{re.escape(invalid_chars)}]\", text):",
            "            raise argparse.ArgumentTypeError(",
            "                f'Invalid {name}: \"{text}\" [invalid chars detected matching \"{invalid_chars}\"]'",
            "            )",
            "        if no_blanks and (text.startswith(\" \") or text.endswith(\" \")):",
            "            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [leading or trailing blanks detected]')",
            "        try:",
            "            text.encode(\"utf-8\", errors=\"strict\")",
            "        except UnicodeEncodeError:",
            "            # looks like text contains surrogate-escapes",
            "            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [contains non-unicode characters]')",
            "        return text",
            "",
            "    return validator",
            "",
            "",
            "comment_validator = text_validator(name=\"comment\", max_length=10000)",
            "",
            "",
            "def archivename_validator(text):",
            "    # we make sure that the archive name can be used as directory name (for borg mount)",
            "    MAX_PATH = 260  # Windows default. Since Win10, there is a registry setting LongPathsEnabled to get more.",
            "    MAX_DIRNAME = MAX_PATH - len(\"12345678.123\")",
            "    SAFETY_MARGIN = 48  # borgfs path: mountpoint / archivename / dir / dir / ... / file",
            "    MAX_ARCHIVENAME = MAX_DIRNAME - SAFETY_MARGIN",
            "    invalid_ctrl_chars = \"\".join(chr(i) for i in range(32))",
            "    # note: \":\" is also an invalid path char on windows, but we can not blacklist it,",
            "    # because e.g. our {now} placeholder creates ISO-8601 like output like 2022-12-10T20:47:42 .",
            "    invalid_chars = r\"/\" + r\"\\\"<|>?*\"  # posix + windows",
            "    validate_text = text_validator(",
            "        name=\"archive name\",",
            "        min_length=1,",
            "        max_length=MAX_ARCHIVENAME,",
            "        invalid_ctrl_chars=invalid_ctrl_chars,",
            "        invalid_chars=invalid_chars,",
            "        no_blanks=True,",
            "    )",
            "    return validate_text(text)",
            "",
            "",
            "class BaseFormatter(metaclass=abc.ABCMeta):",
            "    format: str",
            "    static_data: Dict[str, Any]",
            "    FIXED_KEYS: ClassVar[Dict[str, str]] = {",
            "        # Formatting aids",
            "        \"LF\": \"\\n\",",
            "        \"SPACE\": \" \",",
            "        \"TAB\": \"\\t\",",
            "        \"CR\": \"\\r\",",
            "        \"NUL\": \"\\0\",",
            "        \"NEWLINE\": \"\\n\",",
            "        \"NL\": \"\\n\",  # \\n is automatically converted to os.linesep on write",
            "    }",
            "    KEY_DESCRIPTIONS: ClassVar[Dict[str, str]] = {",
            "        \"NEWLINE\": \"OS dependent line separator\",",
            "        \"NL\": \"alias of NEWLINE\",",
            "        \"NUL\": \"NUL character for creating print0 / xargs -0 like output\",",
            "        \"SPACE\": \"space character\",",
            "        \"TAB\": \"tab character\",",
            "        \"CR\": \"carriage return character\",",
            "        \"LF\": \"line feed character\",",
            "    }",
            "    KEY_GROUPS: ClassVar[Tuple[Tuple[str, ...], ...]] = ((\"NEWLINE\", \"NL\", \"NUL\", \"SPACE\", \"TAB\", \"CR\", \"LF\"),)",
            "",
            "    def __init__(self, format: str, static: Dict[str, Any]) -> None:",
            "        self.format = partial_format(format, static)",
            "        self.static_data = static",
            "",
            "    @abc.abstractmethod",
            "    def get_item_data(self, item, jsonline=False) -> dict:",
            "        raise NotImplementedError",
            "",
            "    def format_item(self, item, jsonline=False, sort=False):",
            "        data = self.get_item_data(item, jsonline)",
            "        return (",
            "            f\"{json.dumps(data, cls=BorgJsonEncoder, sort_keys=sort)}\\n\" if jsonline else self.format.format_map(data)",
            "        )",
            "",
            "    @classmethod",
            "    def keys_help(cls):",
            "        help = []",
            "        keys: Set[str] = set()",
            "        keys.update(cls.KEY_DESCRIPTIONS.keys())",
            "        keys.update(key for group in cls.KEY_GROUPS for key in group)",
            "",
            "        for group in cls.KEY_GROUPS:",
            "            for key in group:",
            "                keys.remove(key)",
            "                text = \"- \" + key",
            "                if key in cls.KEY_DESCRIPTIONS:",
            "                    text += \": \" + cls.KEY_DESCRIPTIONS[key]",
            "                help.append(text)",
            "            help.append(\"\")",
            "        assert not keys, str(keys)",
            "        return \"\\n\".join(help)",
            "",
            "",
            "class ArchiveFormatter(BaseFormatter):",
            "    KEY_DESCRIPTIONS = {",
            "        \"archive\": \"archive name\",",
            "        \"name\": 'alias of \"archive\"',",
            "        \"comment\": \"archive comment\",",
            "        # *start* is the key used by borg-info for this timestamp, this makes the formats more compatible",
            "        \"start\": \"time (start) of creation of the archive\",",
            "        \"time\": 'alias of \"start\"',",
            "        \"end\": \"time (end) of creation of the archive\",",
            "        \"command_line\": \"command line which was used to create the archive\",",
            "        \"id\": \"internal ID of the archive\",",
            "        \"hostname\": \"hostname of host on which this archive was created\",",
            "        \"username\": \"username of user who created this archive\",",
            "        \"tam\": \"TAM authentication state of this archive\",",
            "        \"size\": \"size of this archive (data plus metadata, not considering compression and deduplication)\",",
            "        \"nfiles\": \"count of files in this archive\",",
            "    }",
            "    KEY_GROUPS = (",
            "        (\"archive\", \"name\", \"comment\", \"id\", \"tam\"),",
            "        (\"start\", \"time\", \"end\", \"command_line\"),",
            "        (\"hostname\", \"username\"),",
            "        (\"size\", \"nfiles\"),",
            "    )",
            "",
            "    def __init__(self, format, repository, manifest, key, *, iec=False):",
            "        static_data = {}  # here could be stuff on repo level, above archive level",
            "        static_data.update(self.FIXED_KEYS)",
            "        super().__init__(format, static_data)",
            "        self.repository = repository",
            "        self.manifest = manifest",
            "        self.key = key",
            "        self.name = None",
            "        self.id = None",
            "        self._archive = None",
            "        self.iec = iec",
            "        self.format_keys = {f[1] for f in Formatter().parse(format)}",
            "        self.call_keys = {",
            "            \"hostname\": partial(self.get_meta, \"hostname\", \"\"),",
            "            \"username\": partial(self.get_meta, \"username\", \"\"),",
            "            \"comment\": partial(self.get_meta, \"comment\", \"\"),",
            "            \"command_line\": partial(self.get_meta, \"command_line\", \"\"),",
            "            \"tam\": self.get_tam,",
            "            \"size\": partial(self.get_meta, \"size\", 0),",
            "            \"nfiles\": partial(self.get_meta, \"nfiles\", 0),",
            "            \"end\": self.get_ts_end,",
            "        }",
            "        self.used_call_keys = set(self.call_keys) & self.format_keys",
            "",
            "    def get_item_data(self, archive_info, jsonline=False):",
            "        self.name = archive_info.name",
            "        self.id = archive_info.id",
            "        item_data = {}",
            "        item_data.update({} if jsonline else self.static_data)",
            "        item_data.update(",
            "            {",
            "                \"name\": archive_info.name,",
            "                \"archive\": archive_info.name,",
            "                \"id\": bin_to_hex(archive_info.id),",
            "                \"time\": self.format_time(archive_info.ts),",
            "                \"start\": self.format_time(archive_info.ts),",
            "            }",
            "        )",
            "        for key in self.used_call_keys:",
            "            item_data[key] = self.call_keys[key]()",
            "",
            "        # Note: name and comment are validated, should never contain surrogate escapes.",
            "        # But unsure whether hostname, username, command_line could contain surrogate escapes, play safe:",
            "        for key in \"hostname\", \"username\", \"command_line\":",
            "            if key in item_data:",
            "                item_data.update(text_to_json(key, item_data[key]))",
            "        return item_data",
            "",
            "    @property",
            "    def archive(self):",
            "        \"\"\"lazy load / update loaded archive\"\"\"",
            "        if self._archive is None or self._archive.id != self.id:",
            "            from ..archive import Archive",
            "",
            "            self._archive = Archive(self.manifest, self.name, iec=self.iec)",
            "        return self._archive",
            "",
            "    def get_meta(self, key, default=None):",
            "        return self.archive.metadata.get(key, default)",
            "",
            "    def get_ts_end(self):",
            "        return self.format_time(self.archive.ts_end)",
            "",
            "    def get_tam(self):",
            "        return \"verified\" if self.archive.tam_verified else \"none\"",
            "",
            "    def format_time(self, ts):",
            "        return OutputTimestamp(ts)",
            "",
            "",
            "class ItemFormatter(BaseFormatter):",
            "    # we provide the hash algos from python stdlib (except shake_*) and additionally xxh64.",
            "    # shake_* is not provided because it uses an incompatible .digest() method to support variable length.",
            "    hash_algorithms = set(hashlib.algorithms_guaranteed).union({\"xxh64\"}).difference({\"shake_128\", \"shake_256\"})",
            "    KEY_DESCRIPTIONS = {",
            "        \"type\": \"file type (file, dir, symlink, ...)\",",
            "        \"mode\": \"file mode (as in stat)\",",
            "        \"uid\": \"user id of file owner\",",
            "        \"gid\": \"group id of file owner\",",
            "        \"user\": \"user name of file owner\",",
            "        \"group\": \"group name of file owner\",",
            "        \"path\": \"file path\",",
            "        \"target\": \"link target for symlinks\",",
            "        \"hlid\": \"hard link identity (same if hardlinking same fs object)\",",
            "        \"flags\": \"file flags\",",
            "        \"extra\": 'prepends {target} with \" -> \" for soft links and \" link to \" for hard links',",
            "        \"size\": \"file size\",",
            "        \"dsize\": \"deduplicated size\",",
            "        \"num_chunks\": \"number of chunks in this file\",",
            "        \"unique_chunks\": \"number of unique chunks in this file\",",
            "        \"mtime\": \"file modification time\",",
            "        \"ctime\": \"file change time\",",
            "        \"atime\": \"file access time\",",
            "        \"isomtime\": \"file modification time (ISO 8601 format)\",",
            "        \"isoctime\": \"file change time (ISO 8601 format)\",",
            "        \"isoatime\": \"file access time (ISO 8601 format)\",",
            "        \"xxh64\": \"XXH64 checksum of this file (note: this is NOT a cryptographic hash!)\",",
            "        \"health\": 'either \"healthy\" (file ok) or \"broken\" (if file has all-zero replacement chunks)',",
            "        \"archiveid\": \"internal ID of the archive\",",
            "        \"archivename\": \"name of the archive\",",
            "    }",
            "    KEY_GROUPS = (",
            "        (\"type\", \"mode\", \"uid\", \"gid\", \"user\", \"group\", \"path\", \"target\", \"hlid\", \"flags\"),",
            "        (\"size\", \"dsize\", \"num_chunks\", \"unique_chunks\"),",
            "        (\"mtime\", \"ctime\", \"atime\", \"isomtime\", \"isoctime\", \"isoatime\"),",
            "        tuple(sorted(hash_algorithms)),",
            "        (\"archiveid\", \"archivename\", \"extra\"),",
            "        (\"health\",),",
            "    )",
            "",
            "    KEYS_REQUIRING_CACHE = (\"dsize\", \"unique_chunks\")",
            "",
            "    @classmethod",
            "    def format_needs_cache(cls, format):",
            "        format_keys = {f[1] for f in Formatter().parse(format)}",
            "        return any(key in cls.KEYS_REQUIRING_CACHE for key in format_keys)",
            "",
            "    def __init__(self, archive, format):",
            "        from ..checksums import StreamingXXH64",
            "",
            "        static_data = {\"archivename\": archive.name, \"archiveid\": archive.fpr}",
            "        static_data.update(self.FIXED_KEYS)",
            "        super().__init__(format, static_data)",
            "        self.xxh64 = StreamingXXH64",
            "        self.archive = archive",
            "        self.format_keys = {f[1] for f in Formatter().parse(format)}",
            "        self.call_keys = {",
            "            \"size\": self.calculate_size,",
            "            \"dsize\": partial(self.sum_unique_chunks_metadata, lambda chunk: chunk.size),",
            "            \"num_chunks\": self.calculate_num_chunks,",
            "            \"unique_chunks\": partial(self.sum_unique_chunks_metadata, lambda chunk: 1),",
            "            \"isomtime\": partial(self.format_iso_time, \"mtime\"),",
            "            \"isoctime\": partial(self.format_iso_time, \"ctime\"),",
            "            \"isoatime\": partial(self.format_iso_time, \"atime\"),",
            "            \"mtime\": partial(self.format_time, \"mtime\"),",
            "            \"ctime\": partial(self.format_time, \"ctime\"),",
            "            \"atime\": partial(self.format_time, \"atime\"),",
            "        }",
            "        for hash_function in self.hash_algorithms:",
            "            self.call_keys[hash_function] = partial(self.hash_item, hash_function)",
            "        self.used_call_keys = set(self.call_keys) & self.format_keys",
            "",
            "    def get_item_data(self, item, jsonline=False):",
            "        item_data = {}",
            "        item_data.update({} if jsonline else self.static_data)",
            "",
            "        item_data.update(text_to_json(\"path\", item.path))",
            "        target = item.get(\"target\", \"\")",
            "        item_data.update(text_to_json(\"target\", target))",
            "        if not jsonline:",
            "            item_data[\"extra\"] = \"\" if not target else f\" -> {item_data['target']}\"",
            "",
            "        hlid = item.get(\"hlid\")",
            "        hlid = bin_to_hex(hlid) if hlid else \"\"",
            "        item_data[\"hlid\"] = hlid",
            "",
            "        mode = stat.filemode(item.mode)",
            "        item_type = mode[0]",
            "        item_data[\"type\"] = item_type",
            "        item_data[\"mode\"] = mode",
            "",
            "        item_data[\"uid\"] = item.get(\"uid\")  # int or None",
            "        item_data[\"gid\"] = item.get(\"gid\")  # int or None",
            "        item_data.update(text_to_json(\"user\", item.get(\"user\", str(item_data[\"uid\"]))))",
            "        item_data.update(text_to_json(\"group\", item.get(\"group\", str(item_data[\"gid\"]))))",
            "",
            "        if jsonline:",
            "            item_data[\"healthy\"] = \"chunks_healthy\" not in item",
            "        else:",
            "            item_data[\"health\"] = \"broken\" if \"chunks_healthy\" in item else \"healthy\"",
            "        item_data[\"flags\"] = item.get(\"bsdflags\")  # int if flags known, else (if flags unknown) None",
            "        for key in self.used_call_keys:",
            "            item_data[key] = self.call_keys[key](item)",
            "        return item_data",
            "",
            "    def sum_unique_chunks_metadata(self, metadata_func, item):",
            "        \"\"\"",
            "        sum unique chunks metadata, a unique chunk is a chunk which is referenced globally as often as it is in the",
            "        item",
            "",
            "        item: The item to sum its unique chunks' metadata",
            "        metadata_func: A function that takes a parameter of type ChunkIndexEntry and returns a number, used to return",
            "        the metadata needed from the chunk",
            "        \"\"\"",
            "        chunk_index = self.archive.cache.chunks",
            "        chunks = item.get(\"chunks\", [])",
            "        chunks_counter = Counter(c.id for c in chunks)",
            "        return sum(metadata_func(c) for c in chunks if chunk_index[c.id].refcount == chunks_counter[c.id])",
            "",
            "    def calculate_num_chunks(self, item):",
            "        return len(item.get(\"chunks\", []))",
            "",
            "    def calculate_size(self, item):",
            "        # note: does not support hardlink slaves, they will be size 0",
            "        return item.get_size()",
            "",
            "    def hash_item(self, hash_function, item):",
            "        if \"chunks\" not in item:",
            "            return \"\"",
            "        if hash_function == \"xxh64\":",
            "            hash = self.xxh64()",
            "        elif hash_function in self.hash_algorithms:",
            "            hash = hashlib.new(hash_function)",
            "        for data in self.archive.pipeline.fetch_many([c.id for c in item.chunks]):",
            "            hash.update(data)",
            "        return hash.hexdigest()",
            "",
            "    def format_time(self, key, item):",
            "        return OutputTimestamp(safe_timestamp(item.get(key) or item.mtime))",
            "",
            "    def format_iso_time(self, key, item):",
            "        return self.format_time(key, item).isoformat()",
            "",
            "",
            "class DiffFormatter(BaseFormatter):",
            "    KEY_DESCRIPTIONS = {",
            "        \"path\": \"archived file path\",",
            "        \"change\": \"all available changes\",",
            "        \"content\": \"file content change\",",
            "        \"mode\": \"file mode change\",",
            "        \"type\": \"file type change\",",
            "        \"owner\": \"file owner (user/group) change\",",
            "        \"user\": \"file user change\",",
            "        \"group\": \"file group change\",",
            "        \"link\": \"file link change\",",
            "        \"directory\": \"file directory change\",",
            "        \"blkdev\": \"file block device change\",",
            "        \"chrdev\": \"file character device change\",",
            "        \"fifo\": \"file fifo change\",",
            "        \"mtime\": \"file modification time change\",",
            "        \"ctime\": \"file change time change\",",
            "        \"isomtime\": \"file modification time change (ISO 8601)\",",
            "        \"isoctime\": \"file creation time change (ISO 8601)\",",
            "    }",
            "    KEY_GROUPS = (",
            "        (\"path\", \"change\"),",
            "        (\"content\", \"mode\", \"type\", \"owner\", \"group\", \"user\"),",
            "        (\"link\", \"directory\", \"blkdev\", \"chrdev\", \"fifo\"),",
            "        (\"mtime\", \"ctime\", \"isomtime\", \"isoctime\"),",
            "    )",
            "    METADATA = (\"mode\", \"type\", \"owner\", \"group\", \"user\", \"mtime\", \"ctime\")",
            "",
            "    def __init__(self, format, content_only=False):",
            "        static_data = {}",
            "        static_data.update(self.FIXED_KEYS)",
            "        super().__init__(format or \"{content}{link}{directory}{blkdev}{chrdev}{fifo} {path}{NL}\", static_data)",
            "        self.content_only = content_only",
            "        self.format_keys = {f[1] for f in Formatter().parse(format)}",
            "        self.call_keys = {",
            "            \"content\": self.format_content,",
            "            \"mode\": self.format_mode,",
            "            \"type\": partial(self.format_mode, filetype=True),",
            "            \"owner\": partial(self.format_owner),",
            "            \"group\": partial(self.format_owner, spec=\"group\"),",
            "            \"user\": partial(self.format_owner, spec=\"user\"),",
            "            \"link\": partial(self.format_other, \"link\"),",
            "            \"directory\": partial(self.format_other, \"directory\"),",
            "            \"blkdev\": partial(self.format_other, \"blkdev\"),",
            "            \"chrdev\": partial(self.format_other, \"chrdev\"),",
            "            \"fifo\": partial(self.format_other, \"fifo\"),",
            "            \"mtime\": partial(self.format_time, \"mtime\"),",
            "            \"ctime\": partial(self.format_time, \"ctime\"),",
            "            \"isomtime\": partial(self.format_iso_time, \"mtime\"),",
            "            \"isoctime\": partial(self.format_iso_time, \"ctime\"),",
            "        }",
            "        self.used_call_keys = set(self.call_keys) & self.format_keys",
            "        if self.content_only:",
            "            self.used_call_keys -= set(self.METADATA)",
            "",
            "    def get_item_data(self, item: \"ItemDiff\", jsonline=False) -> dict:",
            "        diff_data = {}",
            "        for key in self.used_call_keys:",
            "            diff_data[key] = self.call_keys[key](item)",
            "",
            "        change = []",
            "        for key in self.call_keys:",
            "            if key in (\"isomtime\", \"isoctime\"):",
            "                continue",
            "            if self.content_only and key in self.METADATA:",
            "                continue",
            "            change.append(self.call_keys[key](item))",
            "        diff_data[\"change\"] = \" \".join([v for v in change if v])",
            "        diff_data[\"path\"] = item.path",
            "        diff_data.update({} if jsonline else self.static_data)",
            "        return diff_data",
            "",
            "    def format_other(self, key, diff: \"ItemDiff\"):",
            "        change = diff.changes().get(key)",
            "        return f\"{change.diff_type}\".ljust(27) if change else \"\"  # 27 is the length of the content change",
            "",
            "    def format_mode(self, diff: \"ItemDiff\", filetype=False):",
            "        change = diff.type() if filetype else diff.mode()",
            "        return f\"[{change.diff_data['item1']} -> {change.diff_data['item2']}]\" if change else \"\"",
            "",
            "    def format_owner(self, diff: \"ItemDiff\", spec: Literal[\"owner\", \"user\", \"group\"] = \"owner\"):",
            "        if spec == \"user\":",
            "            change = diff.user()",
            "            return f\"[{change.diff_data['item1']} -> {change.diff_data['item2']}]\" if change else \"\"",
            "        if spec == \"group\":",
            "            change = diff.group()",
            "            return f\"[{change.diff_data['item1']} -> {change.diff_data['item2']}]\" if change else \"\"",
            "        if spec != \"owner\":",
            "            raise ValueError(f\"Invalid owner spec: {spec}\")",
            "        change = diff.owner()",
            "        if change:",
            "            return \"[{}:{} -> {}:{}]\".format(",
            "                change.diff_data[\"item1\"][0],",
            "                change.diff_data[\"item1\"][1],",
            "                change.diff_data[\"item2\"][0],",
            "                change.diff_data[\"item2\"][1],",
            "            )",
            "        return \"\"",
            "",
            "    def format_content(self, diff: \"ItemDiff\"):",
            "        change = diff.content()",
            "        if change:",
            "            if change.diff_type == \"added\":",
            "                return \"{}: {:>20}\".format(change.diff_type, format_file_size(change.diff_data[\"added\"]))",
            "            if change.diff_type == \"removed\":",
            "                return \"{}: {:>18}\".format(change.diff_type, format_file_size(change.diff_data[\"removed\"]))",
            "            if \"added\" not in change.diff_data and \"removed\" not in change.diff_data:",
            "                return \"modified:  (can't get size)\"",
            "            return \"{}: {:>8} {:>8}\".format(",
            "                change.diff_type,",
            "                format_file_size(change.diff_data[\"added\"], precision=1, sign=True),",
            "                format_file_size(-change.diff_data[\"removed\"], precision=1, sign=True),",
            "            )",
            "        return \"\"",
            "",
            "    def format_time(self, key, diff: \"ItemDiff\"):",
            "        change = diff.changes().get(key)",
            "        return f\"[{key}: {change.diff_data['item1']} -> {change.diff_data['item2']}]\" if change else \"\"",
            "",
            "    def format_iso_time(self, key, diff: \"ItemDiff\"):",
            "        change = diff.changes().get(key)",
            "        return (",
            "            f\"[{key}: {change.diff_data['item1'].isoformat()} -> {change.diff_data['item2'].isoformat()}]\"",
            "            if change",
            "            else \"\"",
            "        )",
            "",
            "",
            "def file_status(mode):",
            "    if stat.S_ISREG(mode):",
            "        return \"A\"",
            "    elif stat.S_ISDIR(mode):",
            "        return \"d\"",
            "    elif stat.S_ISBLK(mode):",
            "        return \"b\"",
            "    elif stat.S_ISCHR(mode):",
            "        return \"c\"",
            "    elif stat.S_ISLNK(mode):",
            "        return \"s\"",
            "    elif stat.S_ISFIFO(mode):",
            "        return \"f\"",
            "    return \"?\"",
            "",
            "",
            "def clean_lines(lines, lstrip=None, rstrip=None, remove_empty=True, remove_comments=True):",
            "    \"\"\"",
            "    clean lines (usually read from a config file):",
            "",
            "    1. strip whitespace (left and right), 2. remove empty lines, 3. remove comments.",
            "",
            "    note: only \"pure comment lines\" are supported, no support for \"trailing comments\".",
            "",
            "    :param lines: input line iterator (e.g. list or open text file) that gives unclean input lines",
            "    :param lstrip: lstrip call arguments or False, if lstripping is not desired",
            "    :param rstrip: rstrip call arguments or False, if rstripping is not desired",
            "    :param remove_comments: remove comment lines (lines starting with \"#\")",
            "    :param remove_empty: remove empty lines",
            "    :return: yields processed lines",
            "    \"\"\"",
            "    for line in lines:",
            "        if lstrip is not False:",
            "            line = line.lstrip(lstrip)",
            "        if rstrip is not False:",
            "            line = line.rstrip(rstrip)",
            "        if remove_empty and not line:",
            "            continue",
            "        if remove_comments and line.startswith(\"#\"):",
            "            continue",
            "        yield line",
            "",
            "",
            "def swidth_slice(string, max_width):",
            "    \"\"\"",
            "    Return a slice of *max_width* cells from *string*.",
            "",
            "    Negative *max_width* means from the end of string.",
            "",
            "    *max_width* is in units of character cells (or \"columns\").",
            "    Latin characters are usually one cell wide, many CJK characters are two cells wide.",
            "    \"\"\"",
            "    from ..platform import swidth",
            "",
            "    reverse = max_width < 0",
            "    max_width = abs(max_width)",
            "    if reverse:",
            "        string = reversed(string)",
            "    current_swidth = 0",
            "    result = []",
            "    for character in string:",
            "        current_swidth += swidth(character)",
            "        if current_swidth > max_width:",
            "            break",
            "        result.append(character)",
            "    if reverse:",
            "        result.reverse()",
            "    return \"\".join(result)",
            "",
            "",
            "def ellipsis_truncate(msg, space):",
            "    \"\"\"",
            "    shorten a long string by adding ellipsis between it and return it, example:",
            "    this_is_a_very_long_string -------> this_is..._string",
            "    \"\"\"",
            "    from ..platform import swidth",
            "",
            "    ellipsis_width = swidth(\"...\")",
            "    msg_width = swidth(msg)",
            "    if space < 8:",
            "        # if there is very little space, just show ...",
            "        return \"...\" + \" \" * (space - ellipsis_width)",
            "    if space < ellipsis_width + msg_width:",
            "        return f\"{swidth_slice(msg, space // 2 - ellipsis_width)}...{swidth_slice(msg, -space // 2)}\"",
            "    return msg + \" \" * (space - msg_width)",
            "",
            "",
            "class BorgJsonEncoder(json.JSONEncoder):",
            "    def default(self, o):",
            "        from ..repository import Repository",
            "        from ..remote import RemoteRepository",
            "        from ..archive import Archive",
            "        from ..cache import LocalCache, AdHocCache",
            "",
            "        if isinstance(o, Repository) or isinstance(o, RemoteRepository):",
            "            return {\"id\": bin_to_hex(o.id), \"location\": o._location.canonical_path()}",
            "        if isinstance(o, Archive):",
            "            return o.info()",
            "        if isinstance(o, LocalCache):",
            "            return {\"path\": o.path, \"stats\": o.stats()}",
            "        if isinstance(o, AdHocCache):",
            "            return {\"stats\": o.stats()}",
            "        if callable(getattr(o, \"to_json\", None)):",
            "            return o.to_json()",
            "        return super().default(o)",
            "",
            "",
            "def basic_json_data(manifest, *, cache=None, extra=None):",
            "    key = manifest.key",
            "    data = extra or {}",
            "    data.update({\"repository\": BorgJsonEncoder().default(manifest.repository), \"encryption\": {\"mode\": key.ARG_NAME}})",
            "    data[\"repository\"][\"last_modified\"] = OutputTimestamp(manifest.last_timestamp)",
            "    if key.NAME.startswith(\"key file\"):",
            "        data[\"encryption\"][\"keyfile\"] = key.find_key()",
            "    if cache:",
            "        data[\"cache\"] = cache",
            "    return data",
            "",
            "",
            "def json_dump(obj):",
            "    \"\"\"Dump using BorgJSONEncoder.\"\"\"",
            "    return json.dumps(obj, sort_keys=True, indent=4, cls=BorgJsonEncoder)",
            "",
            "",
            "def json_print(obj):",
            "    print(json_dump(obj))",
            "",
            "",
            "def prepare_dump_dict(d):",
            "    def decode_bytes(value):",
            "        # this should somehow be reversible later, but usual strings should",
            "        # look nice and chunk ids should mostly show in hex. Use a special",
            "        # inband signaling character (ASCII DEL) to distinguish between",
            "        # decoded and hex mode.",
            "        if not value.startswith(b\"\\x7f\"):",
            "            try:",
            "                value = value.decode()",
            "                return value",
            "            except UnicodeDecodeError:",
            "                pass",
            "        return \"\\u007f\" + bin_to_hex(value)",
            "",
            "    def decode_tuple(t):",
            "        res = []",
            "        for value in t:",
            "            if isinstance(value, dict):",
            "                value = decode(value)",
            "            elif isinstance(value, tuple) or isinstance(value, list):",
            "                value = decode_tuple(value)",
            "            elif isinstance(value, bytes):",
            "                value = decode_bytes(value)",
            "            res.append(value)",
            "        return res",
            "",
            "    def decode(d):",
            "        res = OrderedDict()",
            "        for key, value in d.items():",
            "            if isinstance(value, dict):",
            "                value = decode(value)",
            "            elif isinstance(value, (tuple, list)):",
            "                value = decode_tuple(value)",
            "            elif isinstance(value, bytes):",
            "                value = decode_bytes(value)",
            "            elif isinstance(value, Timestamp):",
            "                value = value.to_unix_nano()",
            "            if isinstance(key, bytes):",
            "                key = key.decode()",
            "            res[key] = value",
            "        return res",
            "",
            "    return decode(d)",
            "",
            "",
            "class Highlander(argparse.Action):",
            "    \"\"\"make sure some option is only given once\"\"\"",
            "",
            "    def __init__(self, *args, **kwargs):",
            "        self.__called = False",
            "        super().__init__(*args, **kwargs)",
            "",
            "    def __call__(self, parser, namespace, values, option_string=None):",
            "        if self.__called:",
            "            raise argparse.ArgumentError(self, \"There can be only one.\")",
            "        self.__called = True",
            "        setattr(namespace, self.dest, values)",
            "",
            "",
            "class MakePathSafeAction(Highlander):",
            "    def __call__(self, parser, namespace, path, option_string=None):",
            "        try:",
            "            sanitized_path = make_path_safe(path)",
            "        except ValueError as e:",
            "            raise argparse.ArgumentError(self, e)",
            "        if sanitized_path == \".\":",
            "            raise argparse.ArgumentError(self, f\"{path!r} is not a valid file name\")",
            "        setattr(namespace, self.dest, sanitized_path)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "730": [
                "ArchiveFormatter"
            ]
        },
        "addLocation": []
    },
    "src/borg/testsuite/archiver/check_cmd.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " from ...archive import ChunkBuffer"
            },
            "1": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " from ...constants import *  # NOQA"
            },
            "2": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " from ...helpers import bin_to_hex"
            },
            "3": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from ...helpers import msgpack"
            },
            "4": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " from ...manifest import Manifest"
            },
            "5": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " from ...repository import Repository"
            },
            "6": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " from . import cmd, src_file, create_src_archive, open_archive, generate_archiver_tests, RK_ENCRYPTION"
            },
            "7": {
                "beforePatchRowNumber": 233,
                "afterPatchRowNumber": 232,
                "PatchRowcode": "         manifest = repository.get(Manifest.MANIFEST_ID)"
            },
            "8": {
                "beforePatchRowNumber": 234,
                "afterPatchRowNumber": 233,
                "PatchRowcode": "         corrupted_manifest = manifest + b\"corrupted!\""
            },
            "9": {
                "beforePatchRowNumber": 235,
                "afterPatchRowNumber": 234,
                "PatchRowcode": "         repository.put(Manifest.MANIFEST_ID, corrupted_manifest)"
            },
            "10": {
                "beforePatchRowNumber": 236,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        archive = msgpack.packb("
            },
            "11": {
                "beforePatchRowNumber": 237,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            {"
            },
            "12": {
                "beforePatchRowNumber": 238,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"command_line\": \"\","
            },
            "13": {
                "beforePatchRowNumber": 239,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"item_ptrs\": [],"
            },
            "14": {
                "beforePatchRowNumber": 240,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"hostname\": \"foo\","
            },
            "15": {
                "beforePatchRowNumber": 241,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"username\": \"bar\","
            },
            "16": {
                "beforePatchRowNumber": 242,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"name\": \"archive1\","
            },
            "17": {
                "beforePatchRowNumber": 243,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"time\": \"2016-12-15T18:49:51.849711\","
            },
            "18": {
                "beforePatchRowNumber": 244,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"version\": 2,"
            },
            "19": {
                "beforePatchRowNumber": 245,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            }"
            },
            "20": {
                "beforePatchRowNumber": 246,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        )"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 235,
                "PatchRowcode": "+        archive_dict = {"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 236,
                "PatchRowcode": "+            \"command_line\": \"\","
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 237,
                "PatchRowcode": "+            \"item_ptrs\": [],"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 238,
                "PatchRowcode": "+            \"hostname\": \"foo\","
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 239,
                "PatchRowcode": "+            \"username\": \"bar\","
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 240,
                "PatchRowcode": "+            \"name\": \"archive1\","
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 241,
                "PatchRowcode": "+            \"time\": \"2016-12-15T18:49:51.849711\","
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 242,
                "PatchRowcode": "+            \"version\": 2,"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 243,
                "PatchRowcode": "+        }"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 244,
                "PatchRowcode": "+        archive = repo_objs.key.pack_and_authenticate_metadata(archive_dict, context=b\"archive\")"
            },
            "31": {
                "beforePatchRowNumber": 247,
                "afterPatchRowNumber": 245,
                "PatchRowcode": "         archive_id = repo_objs.id_hash(archive)"
            },
            "32": {
                "beforePatchRowNumber": 248,
                "afterPatchRowNumber": 246,
                "PatchRowcode": "         repository.put(archive_id, repo_objs.format(archive_id, {}, archive))"
            },
            "33": {
                "beforePatchRowNumber": 249,
                "afterPatchRowNumber": 247,
                "PatchRowcode": "         repository.commit(compact=False)"
            }
        },
        "frontPatchFile": [
            "import shutil",
            "from unittest.mock import patch",
            "",
            "import pytest",
            "",
            "from ...archive import ChunkBuffer",
            "from ...constants import *  # NOQA",
            "from ...helpers import bin_to_hex",
            "from ...helpers import msgpack",
            "from ...manifest import Manifest",
            "from ...repository import Repository",
            "from . import cmd, src_file, create_src_archive, open_archive, generate_archiver_tests, RK_ENCRYPTION",
            "",
            "pytest_generate_tests = lambda metafunc: generate_archiver_tests(metafunc, kinds=\"local,remote,binary\")  # NOQA",
            "",
            "",
            "def check_cmd_setup(archiver):",
            "    with patch.object(ChunkBuffer, \"BUFFER_SIZE\", 10):",
            "        cmd(archiver, \"rcreate\", RK_ENCRYPTION)",
            "        create_src_archive(archiver, \"archive1\")",
            "        create_src_archive(archiver, \"archive2\")",
            "",
            "",
            "def test_check_usage(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--progress\", exit_code=0)",
            "    assert \"Starting repository check\" in output",
            "    assert \"Starting archive consistency check\" in output",
            "    assert \"Checking segments\" in output",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--repository-only\", exit_code=0)",
            "    assert \"Starting repository check\" in output",
            "    assert \"Starting archive consistency check\" not in output",
            "    assert \"Checking segments\" not in output",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", exit_code=0)",
            "    assert \"Starting repository check\" not in output",
            "    assert \"Starting archive consistency check\" in output",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--match-archives=archive2\", exit_code=0)",
            "    assert \"archive1\" not in output",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--first=1\", exit_code=0)",
            "    assert \"archive1\" in output",
            "    assert \"archive2\" not in output",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--last=1\", exit_code=0)",
            "    assert \"archive1\" not in output",
            "    assert \"archive2\" in output",
            "",
            "",
            "def test_date_matching(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "",
            "    shutil.rmtree(archiver.repository_path)",
            "    cmd(archiver, \"rcreate\", RK_ENCRYPTION)",
            "    earliest_ts = \"2022-11-20T23:59:59\"",
            "    ts_in_between = \"2022-12-18T23:59:59\"",
            "    create_src_archive(archiver, \"archive1\", ts=earliest_ts)",
            "    create_src_archive(archiver, \"archive2\", ts=ts_in_between)",
            "    create_src_archive(archiver, \"archive3\")",
            "    cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--oldest=23e\", exit_code=2)",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--oldest=1m\", exit_code=0)",
            "    assert \"archive1\" in output",
            "    assert \"archive2\" in output",
            "    assert \"archive3\" not in output",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--newest=1m\", exit_code=0)",
            "    assert \"archive3\" in output",
            "    assert \"archive2\" not in output",
            "    assert \"archive1\" not in output",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--newer=1d\", exit_code=0)",
            "    assert \"archive3\" in output",
            "    assert \"archive1\" not in output",
            "    assert \"archive2\" not in output",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--older=1d\", exit_code=0)",
            "    assert \"archive1\" in output",
            "    assert \"archive2\" in output",
            "    assert \"archive3\" not in output",
            "",
            "    # check for output when timespan older than the earliest archive is given. Issue #1711",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--older=9999m\", exit_code=0)",
            "    for archive in (\"archive1\", \"archive2\", \"archive3\"):",
            "        assert archive not in output",
            "",
            "",
            "def test_missing_file_chunk(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "",
            "    archive, repository = open_archive(archiver.repository_path, \"archive1\")",
            "",
            "    with repository:",
            "        for item in archive.iter_items():",
            "            if item.path.endswith(src_file):",
            "                valid_chunks = item.chunks",
            "                killed_chunk = valid_chunks[-1]",
            "                repository.delete(killed_chunk.id)",
            "                break",
            "        else:",
            "            pytest.fail(\"should not happen\")  # convert 'fail'",
            "        repository.commit(compact=False)",
            "",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    output = cmd(archiver, \"check\", \"--repair\", exit_code=0)",
            "    assert \"New missing file chunk detected\" in output",
            "",
            "    cmd(archiver, \"check\", exit_code=0)",
            "    output = cmd(archiver, \"list\", \"archive1\", \"--format={health}#{path}{NL}\", exit_code=0)",
            "    assert \"broken#\" in output",
            "",
            "    # check that the file in the old archives has now a different chunk list without the killed chunk",
            "    for archive_name in (\"archive1\", \"archive2\"):",
            "        archive, repository = open_archive(archiver.repository_path, archive_name)",
            "        with repository:",
            "            for item in archive.iter_items():",
            "                if item.path.endswith(src_file):",
            "                    assert valid_chunks != item.chunks",
            "                    assert killed_chunk not in item.chunks",
            "                    break",
            "            else:",
            "                pytest.fail(\"should not happen\")  # convert 'fail'",
            "",
            "    # do a fresh backup (that will include the killed chunk)",
            "    with patch.object(ChunkBuffer, \"BUFFER_SIZE\", 10):",
            "        create_src_archive(archiver, \"archive3\")",
            "",
            "    # check should be able to heal the file now:",
            "    output = cmd(archiver, \"check\", \"-v\", \"--repair\", exit_code=0)",
            "    assert \"Healed previously missing file chunk\" in output",
            "    assert f\"{src_file}: Completely healed previously damaged file!\" in output",
            "",
            "    # check that the file in the old archives has the correct chunks again",
            "    for archive_name in (\"archive1\", \"archive2\"):",
            "        archive, repository = open_archive(archiver.repository_path, archive_name)",
            "        with repository:",
            "            for item in archive.iter_items():",
            "                if item.path.endswith(src_file):",
            "                    assert valid_chunks == item.chunks",
            "                    break",
            "            else:",
            "                pytest.fail(\"should not happen\")",
            "",
            "    # list is also all-healthy again",
            "    output = cmd(archiver, \"list\", \"archive1\", \"--format={health}#{path}{NL}\", exit_code=0)",
            "    assert \"broken#\" not in output",
            "",
            "",
            "def test_missing_archive_item_chunk(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "    archive, repository = open_archive(archiver.repository_path, \"archive1\")",
            "    with repository:",
            "        repository.delete(archive.metadata.items[0])",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    cmd(archiver, \"check\", \"--repair\", exit_code=0)",
            "    cmd(archiver, \"check\", exit_code=0)",
            "",
            "",
            "def test_missing_archive_metadata(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "    archive, repository = open_archive(archiver.repository_path, \"archive1\")",
            "    with repository:",
            "        repository.delete(archive.id)",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    cmd(archiver, \"check\", \"--repair\", exit_code=0)",
            "    cmd(archiver, \"check\", exit_code=0)",
            "",
            "",
            "def test_missing_manifest(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "    archive, repository = open_archive(archiver.repository_path, \"archive1\")",
            "    with repository:",
            "        repository.delete(Manifest.MANIFEST_ID)",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    output = cmd(archiver, \"check\", \"-v\", \"--repair\", exit_code=0)",
            "    assert \"archive1\" in output",
            "    assert \"archive2\" in output",
            "    cmd(archiver, \"check\", exit_code=0)",
            "",
            "",
            "def test_corrupted_manifest(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "    archive, repository = open_archive(archiver.repository_path, \"archive1\")",
            "    with repository:",
            "        manifest = repository.get(Manifest.MANIFEST_ID)",
            "        corrupted_manifest = manifest + b\"corrupted!\"",
            "        repository.put(Manifest.MANIFEST_ID, corrupted_manifest)",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    output = cmd(archiver, \"check\", \"-v\", \"--repair\", exit_code=0)",
            "    assert \"archive1\" in output",
            "    assert \"archive2\" in output",
            "    cmd(archiver, \"check\", exit_code=0)",
            "",
            "",
            "def test_manifest_rebuild_corrupted_chunk(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "    archive, repository = open_archive(archiver.repository_path, \"archive1\")",
            "    with repository:",
            "        manifest = repository.get(Manifest.MANIFEST_ID)",
            "        corrupted_manifest = manifest + b\"corrupted!\"",
            "        repository.put(Manifest.MANIFEST_ID, corrupted_manifest)",
            "        chunk = repository.get(archive.id)",
            "        corrupted_chunk = chunk + b\"corrupted!\"",
            "        repository.put(archive.id, corrupted_chunk)",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    output = cmd(archiver, \"check\", \"-v\", \"--repair\", exit_code=0)",
            "    assert \"archive2\" in output",
            "    cmd(archiver, \"check\", exit_code=0)",
            "",
            "",
            "def test_manifest_rebuild_duplicate_archive(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "    archive, repository = open_archive(archiver.repository_path, \"archive1\")",
            "    repo_objs = archive.repo_objs",
            "    with repository:",
            "        manifest = repository.get(Manifest.MANIFEST_ID)",
            "        corrupted_manifest = manifest + b\"corrupted!\"",
            "        repository.put(Manifest.MANIFEST_ID, corrupted_manifest)",
            "        archive = msgpack.packb(",
            "            {",
            "                \"command_line\": \"\",",
            "                \"item_ptrs\": [],",
            "                \"hostname\": \"foo\",",
            "                \"username\": \"bar\",",
            "                \"name\": \"archive1\",",
            "                \"time\": \"2016-12-15T18:49:51.849711\",",
            "                \"version\": 2,",
            "            }",
            "        )",
            "        archive_id = repo_objs.id_hash(archive)",
            "        repository.put(archive_id, repo_objs.format(archive_id, {}, archive))",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    cmd(archiver, \"check\", \"--repair\", exit_code=0)",
            "    output = cmd(archiver, \"rlist\")",
            "    assert \"archive1\" in output",
            "    assert \"archive1.1\" in output",
            "    assert \"archive2\" in output",
            "",
            "",
            "def test_extra_chunks(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    if archiver.get_kind() == \"remote\":",
            "        pytest.skip(\"only works locally\")",
            "    check_cmd_setup(archiver)",
            "    cmd(archiver, \"check\", exit_code=0)",
            "    with Repository(archiver.repository_location, exclusive=True) as repository:",
            "        repository.put(b\"01234567890123456789012345678901\", b\"xxxx\")",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    cmd(archiver, \"check\", \"--repair\", exit_code=0)",
            "    cmd(archiver, \"check\", exit_code=0)",
            "    cmd(archiver, \"extract\", \"archive1\", \"--dry-run\", exit_code=0)",
            "",
            "",
            "@pytest.mark.parametrize(\"init_args\", [[\"--encryption=repokey-aes-ocb\"], [\"--encryption\", \"none\"]])",
            "def test_verify_data(archivers, request, init_args):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "    shutil.rmtree(archiver.repository_path)",
            "    cmd(archiver, \"rcreate\", *init_args)",
            "    create_src_archive(archiver, \"archive1\")",
            "    archive, repository = open_archive(archiver.repository_path, \"archive1\")",
            "    with repository:",
            "        for item in archive.iter_items():",
            "            if item.path.endswith(src_file):",
            "                chunk = item.chunks[-1]",
            "                data = repository.get(chunk.id)",
            "                data = data[0:100] + b\"x\" + data[101:]",
            "                repository.put(chunk.id, data)",
            "                break",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=0)",
            "    output = cmd(archiver, \"check\", \"--verify-data\", exit_code=1)",
            "    assert bin_to_hex(chunk.id) + \", integrity error\" in output",
            "",
            "    # repair (heal is tested in another test)",
            "    output = cmd(archiver, \"check\", \"--repair\", \"--verify-data\", exit_code=0)",
            "    assert bin_to_hex(chunk.id) + \", integrity error\" in output",
            "    assert f\"{src_file}: New missing file chunk detected\" in output",
            "",
            "",
            "def test_empty_repository(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    if archiver.get_kind() == \"remote\":",
            "        pytest.skip(\"only works locally\")",
            "    check_cmd_setup(archiver)",
            "    with Repository(archiver.repository_location, exclusive=True) as repository:",
            "        for id_ in repository.list():",
            "            repository.delete(id_)",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=1)"
        ],
        "afterPatchFile": [
            "import shutil",
            "from unittest.mock import patch",
            "",
            "import pytest",
            "",
            "from ...archive import ChunkBuffer",
            "from ...constants import *  # NOQA",
            "from ...helpers import bin_to_hex",
            "from ...manifest import Manifest",
            "from ...repository import Repository",
            "from . import cmd, src_file, create_src_archive, open_archive, generate_archiver_tests, RK_ENCRYPTION",
            "",
            "pytest_generate_tests = lambda metafunc: generate_archiver_tests(metafunc, kinds=\"local,remote,binary\")  # NOQA",
            "",
            "",
            "def check_cmd_setup(archiver):",
            "    with patch.object(ChunkBuffer, \"BUFFER_SIZE\", 10):",
            "        cmd(archiver, \"rcreate\", RK_ENCRYPTION)",
            "        create_src_archive(archiver, \"archive1\")",
            "        create_src_archive(archiver, \"archive2\")",
            "",
            "",
            "def test_check_usage(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--progress\", exit_code=0)",
            "    assert \"Starting repository check\" in output",
            "    assert \"Starting archive consistency check\" in output",
            "    assert \"Checking segments\" in output",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--repository-only\", exit_code=0)",
            "    assert \"Starting repository check\" in output",
            "    assert \"Starting archive consistency check\" not in output",
            "    assert \"Checking segments\" not in output",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", exit_code=0)",
            "    assert \"Starting repository check\" not in output",
            "    assert \"Starting archive consistency check\" in output",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--match-archives=archive2\", exit_code=0)",
            "    assert \"archive1\" not in output",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--first=1\", exit_code=0)",
            "    assert \"archive1\" in output",
            "    assert \"archive2\" not in output",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--last=1\", exit_code=0)",
            "    assert \"archive1\" not in output",
            "    assert \"archive2\" in output",
            "",
            "",
            "def test_date_matching(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "",
            "    shutil.rmtree(archiver.repository_path)",
            "    cmd(archiver, \"rcreate\", RK_ENCRYPTION)",
            "    earliest_ts = \"2022-11-20T23:59:59\"",
            "    ts_in_between = \"2022-12-18T23:59:59\"",
            "    create_src_archive(archiver, \"archive1\", ts=earliest_ts)",
            "    create_src_archive(archiver, \"archive2\", ts=ts_in_between)",
            "    create_src_archive(archiver, \"archive3\")",
            "    cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--oldest=23e\", exit_code=2)",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--oldest=1m\", exit_code=0)",
            "    assert \"archive1\" in output",
            "    assert \"archive2\" in output",
            "    assert \"archive3\" not in output",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--newest=1m\", exit_code=0)",
            "    assert \"archive3\" in output",
            "    assert \"archive2\" not in output",
            "    assert \"archive1\" not in output",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--newer=1d\", exit_code=0)",
            "    assert \"archive3\" in output",
            "    assert \"archive1\" not in output",
            "    assert \"archive2\" not in output",
            "",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--older=1d\", exit_code=0)",
            "    assert \"archive1\" in output",
            "    assert \"archive2\" in output",
            "    assert \"archive3\" not in output",
            "",
            "    # check for output when timespan older than the earliest archive is given. Issue #1711",
            "    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--older=9999m\", exit_code=0)",
            "    for archive in (\"archive1\", \"archive2\", \"archive3\"):",
            "        assert archive not in output",
            "",
            "",
            "def test_missing_file_chunk(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "",
            "    archive, repository = open_archive(archiver.repository_path, \"archive1\")",
            "",
            "    with repository:",
            "        for item in archive.iter_items():",
            "            if item.path.endswith(src_file):",
            "                valid_chunks = item.chunks",
            "                killed_chunk = valid_chunks[-1]",
            "                repository.delete(killed_chunk.id)",
            "                break",
            "        else:",
            "            pytest.fail(\"should not happen\")  # convert 'fail'",
            "        repository.commit(compact=False)",
            "",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    output = cmd(archiver, \"check\", \"--repair\", exit_code=0)",
            "    assert \"New missing file chunk detected\" in output",
            "",
            "    cmd(archiver, \"check\", exit_code=0)",
            "    output = cmd(archiver, \"list\", \"archive1\", \"--format={health}#{path}{NL}\", exit_code=0)",
            "    assert \"broken#\" in output",
            "",
            "    # check that the file in the old archives has now a different chunk list without the killed chunk",
            "    for archive_name in (\"archive1\", \"archive2\"):",
            "        archive, repository = open_archive(archiver.repository_path, archive_name)",
            "        with repository:",
            "            for item in archive.iter_items():",
            "                if item.path.endswith(src_file):",
            "                    assert valid_chunks != item.chunks",
            "                    assert killed_chunk not in item.chunks",
            "                    break",
            "            else:",
            "                pytest.fail(\"should not happen\")  # convert 'fail'",
            "",
            "    # do a fresh backup (that will include the killed chunk)",
            "    with patch.object(ChunkBuffer, \"BUFFER_SIZE\", 10):",
            "        create_src_archive(archiver, \"archive3\")",
            "",
            "    # check should be able to heal the file now:",
            "    output = cmd(archiver, \"check\", \"-v\", \"--repair\", exit_code=0)",
            "    assert \"Healed previously missing file chunk\" in output",
            "    assert f\"{src_file}: Completely healed previously damaged file!\" in output",
            "",
            "    # check that the file in the old archives has the correct chunks again",
            "    for archive_name in (\"archive1\", \"archive2\"):",
            "        archive, repository = open_archive(archiver.repository_path, archive_name)",
            "        with repository:",
            "            for item in archive.iter_items():",
            "                if item.path.endswith(src_file):",
            "                    assert valid_chunks == item.chunks",
            "                    break",
            "            else:",
            "                pytest.fail(\"should not happen\")",
            "",
            "    # list is also all-healthy again",
            "    output = cmd(archiver, \"list\", \"archive1\", \"--format={health}#{path}{NL}\", exit_code=0)",
            "    assert \"broken#\" not in output",
            "",
            "",
            "def test_missing_archive_item_chunk(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "    archive, repository = open_archive(archiver.repository_path, \"archive1\")",
            "    with repository:",
            "        repository.delete(archive.metadata.items[0])",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    cmd(archiver, \"check\", \"--repair\", exit_code=0)",
            "    cmd(archiver, \"check\", exit_code=0)",
            "",
            "",
            "def test_missing_archive_metadata(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "    archive, repository = open_archive(archiver.repository_path, \"archive1\")",
            "    with repository:",
            "        repository.delete(archive.id)",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    cmd(archiver, \"check\", \"--repair\", exit_code=0)",
            "    cmd(archiver, \"check\", exit_code=0)",
            "",
            "",
            "def test_missing_manifest(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "    archive, repository = open_archive(archiver.repository_path, \"archive1\")",
            "    with repository:",
            "        repository.delete(Manifest.MANIFEST_ID)",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    output = cmd(archiver, \"check\", \"-v\", \"--repair\", exit_code=0)",
            "    assert \"archive1\" in output",
            "    assert \"archive2\" in output",
            "    cmd(archiver, \"check\", exit_code=0)",
            "",
            "",
            "def test_corrupted_manifest(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "    archive, repository = open_archive(archiver.repository_path, \"archive1\")",
            "    with repository:",
            "        manifest = repository.get(Manifest.MANIFEST_ID)",
            "        corrupted_manifest = manifest + b\"corrupted!\"",
            "        repository.put(Manifest.MANIFEST_ID, corrupted_manifest)",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    output = cmd(archiver, \"check\", \"-v\", \"--repair\", exit_code=0)",
            "    assert \"archive1\" in output",
            "    assert \"archive2\" in output",
            "    cmd(archiver, \"check\", exit_code=0)",
            "",
            "",
            "def test_manifest_rebuild_corrupted_chunk(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "    archive, repository = open_archive(archiver.repository_path, \"archive1\")",
            "    with repository:",
            "        manifest = repository.get(Manifest.MANIFEST_ID)",
            "        corrupted_manifest = manifest + b\"corrupted!\"",
            "        repository.put(Manifest.MANIFEST_ID, corrupted_manifest)",
            "        chunk = repository.get(archive.id)",
            "        corrupted_chunk = chunk + b\"corrupted!\"",
            "        repository.put(archive.id, corrupted_chunk)",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    output = cmd(archiver, \"check\", \"-v\", \"--repair\", exit_code=0)",
            "    assert \"archive2\" in output",
            "    cmd(archiver, \"check\", exit_code=0)",
            "",
            "",
            "def test_manifest_rebuild_duplicate_archive(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "    archive, repository = open_archive(archiver.repository_path, \"archive1\")",
            "    repo_objs = archive.repo_objs",
            "    with repository:",
            "        manifest = repository.get(Manifest.MANIFEST_ID)",
            "        corrupted_manifest = manifest + b\"corrupted!\"",
            "        repository.put(Manifest.MANIFEST_ID, corrupted_manifest)",
            "        archive_dict = {",
            "            \"command_line\": \"\",",
            "            \"item_ptrs\": [],",
            "            \"hostname\": \"foo\",",
            "            \"username\": \"bar\",",
            "            \"name\": \"archive1\",",
            "            \"time\": \"2016-12-15T18:49:51.849711\",",
            "            \"version\": 2,",
            "        }",
            "        archive = repo_objs.key.pack_and_authenticate_metadata(archive_dict, context=b\"archive\")",
            "        archive_id = repo_objs.id_hash(archive)",
            "        repository.put(archive_id, repo_objs.format(archive_id, {}, archive))",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    cmd(archiver, \"check\", \"--repair\", exit_code=0)",
            "    output = cmd(archiver, \"rlist\")",
            "    assert \"archive1\" in output",
            "    assert \"archive1.1\" in output",
            "    assert \"archive2\" in output",
            "",
            "",
            "def test_extra_chunks(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    if archiver.get_kind() == \"remote\":",
            "        pytest.skip(\"only works locally\")",
            "    check_cmd_setup(archiver)",
            "    cmd(archiver, \"check\", exit_code=0)",
            "    with Repository(archiver.repository_location, exclusive=True) as repository:",
            "        repository.put(b\"01234567890123456789012345678901\", b\"xxxx\")",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    cmd(archiver, \"check\", exit_code=1)",
            "    cmd(archiver, \"check\", \"--repair\", exit_code=0)",
            "    cmd(archiver, \"check\", exit_code=0)",
            "    cmd(archiver, \"extract\", \"archive1\", \"--dry-run\", exit_code=0)",
            "",
            "",
            "@pytest.mark.parametrize(\"init_args\", [[\"--encryption=repokey-aes-ocb\"], [\"--encryption\", \"none\"]])",
            "def test_verify_data(archivers, request, init_args):",
            "    archiver = request.getfixturevalue(archivers)",
            "    check_cmd_setup(archiver)",
            "    shutil.rmtree(archiver.repository_path)",
            "    cmd(archiver, \"rcreate\", *init_args)",
            "    create_src_archive(archiver, \"archive1\")",
            "    archive, repository = open_archive(archiver.repository_path, \"archive1\")",
            "    with repository:",
            "        for item in archive.iter_items():",
            "            if item.path.endswith(src_file):",
            "                chunk = item.chunks[-1]",
            "                data = repository.get(chunk.id)",
            "                data = data[0:100] + b\"x\" + data[101:]",
            "                repository.put(chunk.id, data)",
            "                break",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=0)",
            "    output = cmd(archiver, \"check\", \"--verify-data\", exit_code=1)",
            "    assert bin_to_hex(chunk.id) + \", integrity error\" in output",
            "",
            "    # repair (heal is tested in another test)",
            "    output = cmd(archiver, \"check\", \"--repair\", \"--verify-data\", exit_code=0)",
            "    assert bin_to_hex(chunk.id) + \", integrity error\" in output",
            "    assert f\"{src_file}: New missing file chunk detected\" in output",
            "",
            "",
            "def test_empty_repository(archivers, request):",
            "    archiver = request.getfixturevalue(archivers)",
            "    if archiver.get_kind() == \"remote\":",
            "        pytest.skip(\"only works locally\")",
            "    check_cmd_setup(archiver)",
            "    with Repository(archiver.repository_location, exclusive=True) as repository:",
            "        for id_ in repository.list():",
            "            repository.delete(id_)",
            "        repository.commit(compact=False)",
            "    cmd(archiver, \"check\", exit_code=1)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "9": [],
            "236": [
                "test_manifest_rebuild_duplicate_archive"
            ],
            "237": [
                "test_manifest_rebuild_duplicate_archive"
            ],
            "238": [
                "test_manifest_rebuild_duplicate_archive"
            ],
            "239": [
                "test_manifest_rebuild_duplicate_archive"
            ],
            "240": [
                "test_manifest_rebuild_duplicate_archive"
            ],
            "241": [
                "test_manifest_rebuild_duplicate_archive"
            ],
            "242": [
                "test_manifest_rebuild_duplicate_archive"
            ],
            "243": [
                "test_manifest_rebuild_duplicate_archive"
            ],
            "244": [
                "test_manifest_rebuild_duplicate_archive"
            ],
            "245": [
                "test_manifest_rebuild_duplicate_archive"
            ],
            "246": [
                "test_manifest_rebuild_duplicate_archive"
            ]
        },
        "addLocation": []
    }
}