{
    "tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 154,
                "afterPatchRowNumber": 154,
                "PatchRowcode": "               x=inputs, x_min=[[1.0], [2.0], [4.0]], x_max=1.0))"
            },
            "1": {
                "beforePatchRowNumber": 155,
                "afterPatchRowNumber": 155,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 156,
                "afterPatchRowNumber": 156,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 157,
                "PatchRowcode": "+class QuantizedAvgPoolingOpTest(test_util.TensorFlowTestCase):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 158,
                "PatchRowcode": "+"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 159,
                "PatchRowcode": "+  @test_util.run_in_graph_and_eager_modes"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 160,
                "PatchRowcode": "+  def test_invalid_inputs(self):"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 161,
                "PatchRowcode": "+    inputs = constant_op.constant("
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 162,
                "PatchRowcode": "+        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 163,
                "PatchRowcode": "+    ksize = [1, 1, 1, 1]"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 164,
                "PatchRowcode": "+    strides = [1, 1, 1, 1]"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 165,
                "PatchRowcode": "+    padding = \"SAME\""
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 166,
                "PatchRowcode": "+"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 167,
                "PatchRowcode": "+    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 168,
                "PatchRowcode": "+                                \"must be.* rank 0\"):"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 169,
                "PatchRowcode": "+      self.evaluate("
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 170,
                "PatchRowcode": "+          nn_ops.quantized_avg_pool("
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 171,
                "PatchRowcode": "+              input=inputs,"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 172,
                "PatchRowcode": "+              min_input=[],"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 173,
                "PatchRowcode": "+              max_input=1.0,"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 174,
                "PatchRowcode": "+              ksize=ksize,"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 175,
                "PatchRowcode": "+              strides=strides,"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 176,
                "PatchRowcode": "+              padding=padding))"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 177,
                "PatchRowcode": "+"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 178,
                "PatchRowcode": "+    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 179,
                "PatchRowcode": "+                                \"must be.* rank 0\"):"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 180,
                "PatchRowcode": "+      self.evaluate("
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 181,
                "PatchRowcode": "+          nn_ops.quantized_avg_pool("
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 182,
                "PatchRowcode": "+              input=inputs,"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 183,
                "PatchRowcode": "+              min_input=0.0,"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 184,
                "PatchRowcode": "+              max_input=[],"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 185,
                "PatchRowcode": "+              ksize=ksize,"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 186,
                "PatchRowcode": "+              strides=strides,"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 187,
                "PatchRowcode": "+              padding=padding))"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 188,
                "PatchRowcode": "+"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 189,
                "PatchRowcode": "+"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 190,
                "PatchRowcode": "+class QuantizedMaxPoolingOpTest(test_util.TensorFlowTestCase):"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 191,
                "PatchRowcode": "+"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 192,
                "PatchRowcode": "+  @test_util.run_in_graph_and_eager_modes"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 193,
                "PatchRowcode": "+  def test_invalid_inputs(self):"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 194,
                "PatchRowcode": "+    inputs = constant_op.constant("
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 195,
                "PatchRowcode": "+        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 196,
                "PatchRowcode": "+    ksize = [1, 1, 1, 1]"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 197,
                "PatchRowcode": "+    strides = [1, 1, 1, 1]"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 198,
                "PatchRowcode": "+    padding = \"SAME\""
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 199,
                "PatchRowcode": "+"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 200,
                "PatchRowcode": "+    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 201,
                "PatchRowcode": "+                                \"must be.* rank 0\"):"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 202,
                "PatchRowcode": "+      self.evaluate("
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 203,
                "PatchRowcode": "+          nn_ops.quantized_max_pool("
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 204,
                "PatchRowcode": "+              input=inputs,"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 205,
                "PatchRowcode": "+              min_input=[],"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 206,
                "PatchRowcode": "+              max_input=1.0,"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 207,
                "PatchRowcode": "+              ksize=ksize,"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 208,
                "PatchRowcode": "+              strides=strides,"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 209,
                "PatchRowcode": "+              padding=padding))"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 210,
                "PatchRowcode": "+"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 211,
                "PatchRowcode": "+    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 212,
                "PatchRowcode": "+                                \"must be.* rank 0\"):"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 213,
                "PatchRowcode": "+      self.evaluate("
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 214,
                "PatchRowcode": "+          nn_ops.quantized_max_pool("
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 215,
                "PatchRowcode": "+              input=inputs,"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 216,
                "PatchRowcode": "+              min_input=0.0,"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 217,
                "PatchRowcode": "+              max_input=[],"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 218,
                "PatchRowcode": "+              ksize=ksize,"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 219,
                "PatchRowcode": "+              strides=strides,"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 220,
                "PatchRowcode": "+              padding=padding))"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 221,
                "PatchRowcode": "+"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 222,
                "PatchRowcode": "+"
            },
            "69": {
                "beforePatchRowNumber": 157,
                "afterPatchRowNumber": 223,
                "PatchRowcode": " class RequantizeOpTest(test_util.TensorFlowTestCase):"
            },
            "70": {
                "beforePatchRowNumber": 158,
                "afterPatchRowNumber": 224,
                "PatchRowcode": " "
            },
            "71": {
                "beforePatchRowNumber": 159,
                "afterPatchRowNumber": 225,
                "PatchRowcode": "   @test_util.run_in_graph_and_eager_modes"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ==============================================================================",
            "\"\"\"Tests for tf.quantize ops.\"\"\"",
            "import numpy as np",
            "",
            "from tensorflow.python.framework import constant_op",
            "from tensorflow.python.framework import dtypes",
            "from tensorflow.python.framework import errors",
            "from tensorflow.python.framework import test_util",
            "from tensorflow.python.ops import array_ops",
            "from tensorflow.python.ops import math_ops",
            "from tensorflow.python.ops import nn_ops",
            "from tensorflow.python.platform import googletest",
            "",
            "",
            "class FakeQuantWithMinMaxVarsOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars(",
            "              inputs=inputs, min=0.0, max=[[1.0], [2.0], [4.0]]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars(",
            "              inputs=inputs, min=[[1.0], [2.0], [4.0]], max=1.0))",
            "",
            "",
            "class FakeQuantWithMinMaxVarsPerChannelOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 1\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel(",
            "              inputs=inputs, min=[[0.0]], max=[1.0]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"Dimensions must be equal|incorrect size\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel(",
            "              inputs=inputs, min=[0.0, 0.1], max=[1.0]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 1\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel(",
            "              inputs=inputs, min=[1.0], max=[[1.0]]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"Dimensions must be equal|incorrect size\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel(",
            "              inputs=inputs, min=[0.0], max=[1.0, 1.1]))",
            "",
            "",
            "class QuantizedBiasedAddTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.qint8)",
            "    bias = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.qint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_bias_add(",
            "              input=inputs,",
            "              bias=bias,",
            "              min_input=[],",
            "              max_input=1.0,",
            "              min_bias=0.0,",
            "              max_bias=1.0,",
            "              out_type=dtypes.qint32))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_bias_add(",
            "              input=inputs,",
            "              bias=bias,",
            "              min_input=0.0,",
            "              max_input=[],",
            "              min_bias=0.0,",
            "              max_bias=1.0,",
            "              out_type=dtypes.qint32))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_bias_add(",
            "              input=inputs,",
            "              bias=bias,",
            "              min_input=0.0,",
            "              max_input=1.0,",
            "              min_bias=[],",
            "              max_bias=1.0,",
            "              out_type=dtypes.qint32))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_bias_add(",
            "              input=inputs,",
            "              bias=bias,",
            "              min_input=0.0,",
            "              max_input=1.0,",
            "              min_bias=0.0,",
            "              max_bias=[],",
            "              out_type=dtypes.qint32))",
            "",
            "",
            "class QuantizedInstanceNormOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.quantized_instance_norm(",
            "              x=inputs, x_min=0.0, x_max=[[1.0], [2.0], [4.0]]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.quantized_instance_norm(",
            "              x=inputs, x_min=[[1.0], [2.0], [4.0]], x_max=1.0))",
            "",
            "",
            "class RequantizeOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.requantize(",
            "              input=inputs,",
            "              input_min=[],",
            "              input_max=1.0,",
            "              requested_output_min=0.0,",
            "              requested_output_max=1.0,",
            "              out_type=dtypes.qint8))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.requantize(",
            "              input=inputs,",
            "              input_min=0.0,",
            "              input_max=[],",
            "              requested_output_min=0.0,",
            "              requested_output_max=1.0,",
            "              out_type=dtypes.qint8))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.requantize(",
            "              input=inputs,",
            "              input_min=0.0,",
            "              input_max=1.0,",
            "              requested_output_min=[],",
            "              requested_output_max=1.0,",
            "              out_type=dtypes.qint8))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.requantize(",
            "              input=inputs,",
            "              input_min=0.0,",
            "              input_max=1.0,",
            "              requested_output_min=0.0,",
            "              requested_output_max=[],",
            "              out_type=dtypes.qint8))",
            "",
            "",
            "class QuantizedAddOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    x = constant_op.constant(",
            "        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "    y = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.quint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.quantized_add(",
            "              x=x,",
            "              y=y,",
            "              min_x=[],",
            "              max_x=1.0,",
            "              min_y=0.0,",
            "              max_y=1.0,",
            "              Toutput=dtypes.qint32))",
            "",
            "",
            "class QuantizedReluOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_relu(",
            "              features=inputs,",
            "              min_features=[],",
            "              max_features=127.0,",
            "              out_type=dtypes.quint8))",
            "",
            "",
            "class QuantizedRelu6OpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_relu6(",
            "              features=inputs,",
            "              min_features=[],",
            "              max_features=127.0,",
            "              out_type=dtypes.quint8))",
            "",
            "",
            "class QuantizeDownAndShrinkRangeOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.quantize_down_and_shrink_range(input=inputs,",
            "                                                  input_min=[],",
            "                                                  input_max=4.0,",
            "                                                  out_type=dtypes.quint8))",
            "",
            "",
            "if __name__ == \"__main__\":",
            "  googletest.main()"
        ],
        "afterPatchFile": [
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ==============================================================================",
            "\"\"\"Tests for tf.quantize ops.\"\"\"",
            "import numpy as np",
            "",
            "from tensorflow.python.framework import constant_op",
            "from tensorflow.python.framework import dtypes",
            "from tensorflow.python.framework import errors",
            "from tensorflow.python.framework import test_util",
            "from tensorflow.python.ops import array_ops",
            "from tensorflow.python.ops import math_ops",
            "from tensorflow.python.ops import nn_ops",
            "from tensorflow.python.platform import googletest",
            "",
            "",
            "class FakeQuantWithMinMaxVarsOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars(",
            "              inputs=inputs, min=0.0, max=[[1.0], [2.0], [4.0]]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars(",
            "              inputs=inputs, min=[[1.0], [2.0], [4.0]], max=1.0))",
            "",
            "",
            "class FakeQuantWithMinMaxVarsPerChannelOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 1\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel(",
            "              inputs=inputs, min=[[0.0]], max=[1.0]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"Dimensions must be equal|incorrect size\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel(",
            "              inputs=inputs, min=[0.0, 0.1], max=[1.0]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 1\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel(",
            "              inputs=inputs, min=[1.0], max=[[1.0]]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"Dimensions must be equal|incorrect size\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel(",
            "              inputs=inputs, min=[0.0], max=[1.0, 1.1]))",
            "",
            "",
            "class QuantizedBiasedAddTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.qint8)",
            "    bias = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.qint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_bias_add(",
            "              input=inputs,",
            "              bias=bias,",
            "              min_input=[],",
            "              max_input=1.0,",
            "              min_bias=0.0,",
            "              max_bias=1.0,",
            "              out_type=dtypes.qint32))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_bias_add(",
            "              input=inputs,",
            "              bias=bias,",
            "              min_input=0.0,",
            "              max_input=[],",
            "              min_bias=0.0,",
            "              max_bias=1.0,",
            "              out_type=dtypes.qint32))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_bias_add(",
            "              input=inputs,",
            "              bias=bias,",
            "              min_input=0.0,",
            "              max_input=1.0,",
            "              min_bias=[],",
            "              max_bias=1.0,",
            "              out_type=dtypes.qint32))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_bias_add(",
            "              input=inputs,",
            "              bias=bias,",
            "              min_input=0.0,",
            "              max_input=1.0,",
            "              min_bias=0.0,",
            "              max_bias=[],",
            "              out_type=dtypes.qint32))",
            "",
            "",
            "class QuantizedInstanceNormOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.quantized_instance_norm(",
            "              x=inputs, x_min=0.0, x_max=[[1.0], [2.0], [4.0]]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.quantized_instance_norm(",
            "              x=inputs, x_min=[[1.0], [2.0], [4.0]], x_max=1.0))",
            "",
            "",
            "class QuantizedAvgPoolingOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "    ksize = [1, 1, 1, 1]",
            "    strides = [1, 1, 1, 1]",
            "    padding = \"SAME\"",
            "",
            "    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),",
            "                                \"must be.* rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_avg_pool(",
            "              input=inputs,",
            "              min_input=[],",
            "              max_input=1.0,",
            "              ksize=ksize,",
            "              strides=strides,",
            "              padding=padding))",
            "",
            "    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),",
            "                                \"must be.* rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_avg_pool(",
            "              input=inputs,",
            "              min_input=0.0,",
            "              max_input=[],",
            "              ksize=ksize,",
            "              strides=strides,",
            "              padding=padding))",
            "",
            "",
            "class QuantizedMaxPoolingOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "    ksize = [1, 1, 1, 1]",
            "    strides = [1, 1, 1, 1]",
            "    padding = \"SAME\"",
            "",
            "    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),",
            "                                \"must be.* rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_max_pool(",
            "              input=inputs,",
            "              min_input=[],",
            "              max_input=1.0,",
            "              ksize=ksize,",
            "              strides=strides,",
            "              padding=padding))",
            "",
            "    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),",
            "                                \"must be.* rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_max_pool(",
            "              input=inputs,",
            "              min_input=0.0,",
            "              max_input=[],",
            "              ksize=ksize,",
            "              strides=strides,",
            "              padding=padding))",
            "",
            "",
            "class RequantizeOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.requantize(",
            "              input=inputs,",
            "              input_min=[],",
            "              input_max=1.0,",
            "              requested_output_min=0.0,",
            "              requested_output_max=1.0,",
            "              out_type=dtypes.qint8))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.requantize(",
            "              input=inputs,",
            "              input_min=0.0,",
            "              input_max=[],",
            "              requested_output_min=0.0,",
            "              requested_output_max=1.0,",
            "              out_type=dtypes.qint8))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.requantize(",
            "              input=inputs,",
            "              input_min=0.0,",
            "              input_max=1.0,",
            "              requested_output_min=[],",
            "              requested_output_max=1.0,",
            "              out_type=dtypes.qint8))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.requantize(",
            "              input=inputs,",
            "              input_min=0.0,",
            "              input_max=1.0,",
            "              requested_output_min=0.0,",
            "              requested_output_max=[],",
            "              out_type=dtypes.qint8))",
            "",
            "",
            "class QuantizedAddOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    x = constant_op.constant(",
            "        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "    y = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.quint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.quantized_add(",
            "              x=x,",
            "              y=y,",
            "              min_x=[],",
            "              max_x=1.0,",
            "              min_y=0.0,",
            "              max_y=1.0,",
            "              Toutput=dtypes.qint32))",
            "",
            "",
            "class QuantizedReluOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_relu(",
            "              features=inputs,",
            "              min_features=[],",
            "              max_features=127.0,",
            "              out_type=dtypes.quint8))",
            "",
            "",
            "class QuantizedRelu6OpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_relu6(",
            "              features=inputs,",
            "              min_features=[],",
            "              max_features=127.0,",
            "              out_type=dtypes.quint8))",
            "",
            "",
            "class QuantizeDownAndShrinkRangeOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.quantize_down_and_shrink_range(input=inputs,",
            "                                                  input_min=[],",
            "                                                  input_max=4.0,",
            "                                                  out_type=dtypes.quint8))",
            "",
            "",
            "if __name__ == \"__main__\":",
            "  googletest.main()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "notebook.notebookapp"
        ]
    }
}