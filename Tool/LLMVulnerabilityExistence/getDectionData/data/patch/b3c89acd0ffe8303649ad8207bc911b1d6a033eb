{
    "redis/asyncio/client.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1374,
                "afterPatchRowNumber": 1374,
                "PatchRowcode": "         conn = cast(Connection, conn)"
            },
            "1": {
                "beforePatchRowNumber": 1375,
                "afterPatchRowNumber": 1375,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 1376,
                "afterPatchRowNumber": 1376,
                "PatchRowcode": "         try:"
            },
            "3": {
                "beforePatchRowNumber": 1377,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return await conn.retry.call_with_retry("
            },
            "4": {
                "beforePatchRowNumber": 1378,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                lambda: execute(conn, stack, raise_on_error),"
            },
            "5": {
                "beforePatchRowNumber": 1379,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                lambda error: self._disconnect_raise_reset(conn, error),"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1377,
                "PatchRowcode": "+            return await asyncio.shield("
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1378,
                "PatchRowcode": "+                conn.retry.call_with_retry("
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1379,
                "PatchRowcode": "+                    lambda: execute(conn, stack, raise_on_error),"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1380,
                "PatchRowcode": "+                    lambda error: self._disconnect_raise_reset(conn, error),"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1381,
                "PatchRowcode": "+                )"
            },
            "11": {
                "beforePatchRowNumber": 1380,
                "afterPatchRowNumber": 1382,
                "PatchRowcode": "             )"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1383,
                "PatchRowcode": "+        except asyncio.CancelledError:"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1384,
                "PatchRowcode": "+            # not supposed to be possible, yet here we are"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1385,
                "PatchRowcode": "+            await conn.disconnect(nowait=True)"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1386,
                "PatchRowcode": "+            raise"
            },
            "16": {
                "beforePatchRowNumber": 1381,
                "afterPatchRowNumber": 1387,
                "PatchRowcode": "         finally:"
            },
            "17": {
                "beforePatchRowNumber": 1382,
                "afterPatchRowNumber": 1388,
                "PatchRowcode": "             await self.reset()"
            },
            "18": {
                "beforePatchRowNumber": 1383,
                "afterPatchRowNumber": 1389,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "import asyncio",
            "import copy",
            "import inspect",
            "import re",
            "import warnings",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    AsyncIterator,",
            "    Awaitable,",
            "    Callable,",
            "    Dict,",
            "    Iterable,",
            "    List,",
            "    Mapping,",
            "    MutableMapping,",
            "    NoReturn,",
            "    Optional,",
            "    Set,",
            "    Tuple,",
            "    Type,",
            "    TypeVar,",
            "    Union,",
            "    cast,",
            ")",
            "",
            "from redis.asyncio.connection import (",
            "    Connection,",
            "    ConnectionPool,",
            "    SSLConnection,",
            "    UnixDomainSocketConnection,",
            ")",
            "from redis.asyncio.lock import Lock",
            "from redis.asyncio.retry import Retry",
            "from redis.client import (",
            "    EMPTY_RESPONSE,",
            "    NEVER_DECODE,",
            "    AbstractRedis,",
            "    CaseInsensitiveDict,",
            "    bool_ok,",
            ")",
            "from redis.commands import (",
            "    AsyncCoreCommands,",
            "    AsyncRedisModuleCommands,",
            "    AsyncSentinelCommands,",
            "    list_or_args,",
            ")",
            "from redis.compat import Protocol, TypedDict",
            "from redis.credentials import CredentialProvider",
            "from redis.exceptions import (",
            "    ConnectionError,",
            "    ExecAbortError,",
            "    PubSubError,",
            "    RedisError,",
            "    ResponseError,",
            "    TimeoutError,",
            "    WatchError,",
            ")",
            "from redis.typing import ChannelT, EncodableT, KeyT",
            "from redis.utils import safe_str, str_if_bytes",
            "",
            "PubSubHandler = Callable[[Dict[str, str]], Awaitable[None]]",
            "_KeyT = TypeVar(\"_KeyT\", bound=KeyT)",
            "_ArgT = TypeVar(\"_ArgT\", KeyT, EncodableT)",
            "_RedisT = TypeVar(\"_RedisT\", bound=\"Redis\")",
            "_NormalizeKeysT = TypeVar(\"_NormalizeKeysT\", bound=Mapping[ChannelT, object])",
            "if TYPE_CHECKING:",
            "    from redis.commands.core import Script",
            "",
            "",
            "class ResponseCallbackProtocol(Protocol):",
            "    def __call__(self, response: Any, **kwargs):",
            "        ...",
            "",
            "",
            "class AsyncResponseCallbackProtocol(Protocol):",
            "    async def __call__(self, response: Any, **kwargs):",
            "        ...",
            "",
            "",
            "ResponseCallbackT = Union[ResponseCallbackProtocol, AsyncResponseCallbackProtocol]",
            "",
            "",
            "class Redis(",
            "    AbstractRedis, AsyncRedisModuleCommands, AsyncCoreCommands, AsyncSentinelCommands",
            "):",
            "    \"\"\"",
            "    Implementation of the Redis protocol.",
            "",
            "    This abstract class provides a Python interface to all Redis commands",
            "    and an implementation of the Redis protocol.",
            "",
            "    Pipelines derive from this, implementing how",
            "    the commands are sent and received to the Redis server. Based on",
            "    configuration, an instance will either use a ConnectionPool, or",
            "    Connection object to talk to redis.",
            "    \"\"\"",
            "",
            "    response_callbacks: MutableMapping[Union[str, bytes], ResponseCallbackT]",
            "",
            "    @classmethod",
            "    def from_url(cls, url: str, **kwargs):",
            "        \"\"\"",
            "        Return a Redis client object configured from the given URL",
            "",
            "        For example::",
            "",
            "            redis://[[username]:[password]]@localhost:6379/0",
            "            rediss://[[username]:[password]]@localhost:6379/0",
            "            unix://[username@]/path/to/socket.sock?db=0[&password=password]",
            "",
            "        Three URL schemes are supported:",
            "",
            "        - `redis://` creates a TCP socket connection. See more at:",
            "          <https://www.iana.org/assignments/uri-schemes/prov/redis>",
            "        - `rediss://` creates a SSL wrapped TCP socket connection. See more at:",
            "          <https://www.iana.org/assignments/uri-schemes/prov/rediss>",
            "        - ``unix://``: creates a Unix Domain Socket connection.",
            "",
            "        The username, password, hostname, path and all querystring values",
            "        are passed through urllib.parse.unquote in order to replace any",
            "        percent-encoded values with their corresponding characters.",
            "",
            "        There are several ways to specify a database number. The first value",
            "        found will be used:",
            "            1. A ``db`` querystring option, e.g. redis://localhost?db=0",
            "            2. If using the redis:// or rediss:// schemes, the path argument",
            "               of the url, e.g. redis://localhost/0",
            "            3. A ``db`` keyword argument to this function.",
            "",
            "        If none of these options are specified, the default db=0 is used.",
            "",
            "        All querystring options are cast to their appropriate Python types.",
            "        Boolean arguments can be specified with string values \"True\"/\"False\"",
            "        or \"Yes\"/\"No\". Values that cannot be properly cast cause a",
            "        ``ValueError`` to be raised. Once parsed, the querystring arguments",
            "        and keyword arguments are passed to the ``ConnectionPool``'s",
            "        class initializer. In the case of conflicting arguments, querystring",
            "        arguments always win.",
            "",
            "        \"\"\"",
            "        connection_pool = ConnectionPool.from_url(url, **kwargs)",
            "        return cls(connection_pool=connection_pool)",
            "",
            "    def __init__(",
            "        self,",
            "        *,",
            "        host: str = \"localhost\",",
            "        port: int = 6379,",
            "        db: Union[str, int] = 0,",
            "        password: Optional[str] = None,",
            "        socket_timeout: Optional[float] = None,",
            "        socket_connect_timeout: Optional[float] = None,",
            "        socket_keepalive: Optional[bool] = None,",
            "        socket_keepalive_options: Optional[Mapping[int, Union[int, bytes]]] = None,",
            "        connection_pool: Optional[ConnectionPool] = None,",
            "        unix_socket_path: Optional[str] = None,",
            "        encoding: str = \"utf-8\",",
            "        encoding_errors: str = \"strict\",",
            "        decode_responses: bool = False,",
            "        retry_on_timeout: bool = False,",
            "        retry_on_error: Optional[list] = None,",
            "        ssl: bool = False,",
            "        ssl_keyfile: Optional[str] = None,",
            "        ssl_certfile: Optional[str] = None,",
            "        ssl_cert_reqs: str = \"required\",",
            "        ssl_ca_certs: Optional[str] = None,",
            "        ssl_ca_data: Optional[str] = None,",
            "        ssl_check_hostname: bool = False,",
            "        max_connections: Optional[int] = None,",
            "        single_connection_client: bool = False,",
            "        health_check_interval: int = 0,",
            "        client_name: Optional[str] = None,",
            "        username: Optional[str] = None,",
            "        retry: Optional[Retry] = None,",
            "        auto_close_connection_pool: bool = True,",
            "        redis_connect_func=None,",
            "        credential_provider: Optional[CredentialProvider] = None,",
            "    ):",
            "        \"\"\"",
            "        Initialize a new Redis client.",
            "        To specify a retry policy for specific errors, first set",
            "        `retry_on_error` to a list of the error/s to retry on, then set",
            "        `retry` to a valid `Retry` object.",
            "        To retry on TimeoutError, `retry_on_timeout` can also be set to `True`.",
            "        \"\"\"",
            "        kwargs: Dict[str, Any]",
            "        # auto_close_connection_pool only has an effect if connection_pool is",
            "        # None. This is a similar feature to the missing __del__ to resolve #1103,",
            "        # but it accounts for whether a user wants to manually close the connection",
            "        # pool, as a similar feature to ConnectionPool's __del__.",
            "        self.auto_close_connection_pool = (",
            "            auto_close_connection_pool if connection_pool is None else False",
            "        )",
            "        if not connection_pool:",
            "            if not retry_on_error:",
            "                retry_on_error = []",
            "            if retry_on_timeout is True:",
            "                retry_on_error.append(TimeoutError)",
            "            kwargs = {",
            "                \"db\": db,",
            "                \"username\": username,",
            "                \"password\": password,",
            "                \"credential_provider\": credential_provider,",
            "                \"socket_timeout\": socket_timeout,",
            "                \"encoding\": encoding,",
            "                \"encoding_errors\": encoding_errors,",
            "                \"decode_responses\": decode_responses,",
            "                \"retry_on_timeout\": retry_on_timeout,",
            "                \"retry_on_error\": retry_on_error,",
            "                \"retry\": copy.deepcopy(retry),",
            "                \"max_connections\": max_connections,",
            "                \"health_check_interval\": health_check_interval,",
            "                \"client_name\": client_name,",
            "                \"redis_connect_func\": redis_connect_func,",
            "            }",
            "            # based on input, setup appropriate connection args",
            "            if unix_socket_path is not None:",
            "                kwargs.update(",
            "                    {",
            "                        \"path\": unix_socket_path,",
            "                        \"connection_class\": UnixDomainSocketConnection,",
            "                    }",
            "                )",
            "            else:",
            "                # TCP specific options",
            "                kwargs.update(",
            "                    {",
            "                        \"host\": host,",
            "                        \"port\": port,",
            "                        \"socket_connect_timeout\": socket_connect_timeout,",
            "                        \"socket_keepalive\": socket_keepalive,",
            "                        \"socket_keepalive_options\": socket_keepalive_options,",
            "                    }",
            "                )",
            "",
            "                if ssl:",
            "                    kwargs.update(",
            "                        {",
            "                            \"connection_class\": SSLConnection,",
            "                            \"ssl_keyfile\": ssl_keyfile,",
            "                            \"ssl_certfile\": ssl_certfile,",
            "                            \"ssl_cert_reqs\": ssl_cert_reqs,",
            "                            \"ssl_ca_certs\": ssl_ca_certs,",
            "                            \"ssl_ca_data\": ssl_ca_data,",
            "                            \"ssl_check_hostname\": ssl_check_hostname,",
            "                        }",
            "                    )",
            "            connection_pool = ConnectionPool(**kwargs)",
            "        self.connection_pool = connection_pool",
            "        self.single_connection_client = single_connection_client",
            "        self.connection: Optional[Connection] = None",
            "",
            "        self.response_callbacks = CaseInsensitiveDict(self.__class__.RESPONSE_CALLBACKS)",
            "",
            "    def __repr__(self):",
            "        return f\"{self.__class__.__name__}<{self.connection_pool!r}>\"",
            "",
            "    def __await__(self):",
            "        return self.initialize().__await__()",
            "",
            "    async def initialize(self: _RedisT) -> _RedisT:",
            "        if self.single_connection_client and self.connection is None:",
            "            self.connection = await self.connection_pool.get_connection(\"_\")",
            "        return self",
            "",
            "    def set_response_callback(self, command: str, callback: ResponseCallbackT):",
            "        \"\"\"Set a custom Response Callback\"\"\"",
            "        self.response_callbacks[command] = callback",
            "",
            "    def get_encoder(self):",
            "        \"\"\"Get the connection pool's encoder\"\"\"",
            "        return self.connection_pool.get_encoder()",
            "",
            "    def get_connection_kwargs(self):",
            "        \"\"\"Get the connection's key-word arguments\"\"\"",
            "        return self.connection_pool.connection_kwargs",
            "",
            "    def get_retry(self) -> Optional[\"Retry\"]:",
            "        return self.get_connection_kwargs().get(\"retry\")",
            "",
            "    def set_retry(self, retry: \"Retry\") -> None:",
            "        self.get_connection_kwargs().update({\"retry\": retry})",
            "        self.connection_pool.set_retry(retry)",
            "",
            "    def load_external_module(self, funcname, func):",
            "        \"\"\"",
            "        This function can be used to add externally defined redis modules,",
            "        and their namespaces to the redis client.",
            "",
            "        funcname - A string containing the name of the function to create",
            "        func - The function, being added to this class.",
            "",
            "        ex: Assume that one has a custom redis module named foomod that",
            "        creates command named 'foo.dothing' and 'foo.anotherthing' in redis.",
            "        To load function functions into this namespace:",
            "",
            "        from redis import Redis",
            "        from foomodule import F",
            "        r = Redis()",
            "        r.load_external_module(\"foo\", F)",
            "        r.foo().dothing('your', 'arguments')",
            "",
            "        For a concrete example see the reimport of the redisjson module in",
            "        tests/test_connection.py::test_loading_external_modules",
            "        \"\"\"",
            "        setattr(self, funcname, func)",
            "",
            "    def pipeline(",
            "        self, transaction: bool = True, shard_hint: Optional[str] = None",
            "    ) -> \"Pipeline\":",
            "        \"\"\"",
            "        Return a new pipeline object that can queue multiple commands for",
            "        later execution. ``transaction`` indicates whether all commands",
            "        should be executed atomically. Apart from making a group of operations",
            "        atomic, pipelines are useful for reducing the back-and-forth overhead",
            "        between the client and server.",
            "        \"\"\"",
            "        return Pipeline(",
            "            self.connection_pool, self.response_callbacks, transaction, shard_hint",
            "        )",
            "",
            "    async def transaction(",
            "        self,",
            "        func: Callable[[\"Pipeline\"], Union[Any, Awaitable[Any]]],",
            "        *watches: KeyT,",
            "        shard_hint: Optional[str] = None,",
            "        value_from_callable: bool = False,",
            "        watch_delay: Optional[float] = None,",
            "    ):",
            "        \"\"\"",
            "        Convenience method for executing the callable `func` as a transaction",
            "        while watching all keys specified in `watches`. The 'func' callable",
            "        should expect a single argument which is a Pipeline object.",
            "        \"\"\"",
            "        pipe: Pipeline",
            "        async with self.pipeline(True, shard_hint) as pipe:",
            "            while True:",
            "                try:",
            "                    if watches:",
            "                        await pipe.watch(*watches)",
            "                    func_value = func(pipe)",
            "                    if inspect.isawaitable(func_value):",
            "                        func_value = await func_value",
            "                    exec_value = await pipe.execute()",
            "                    return func_value if value_from_callable else exec_value",
            "                except WatchError:",
            "                    if watch_delay is not None and watch_delay > 0:",
            "                        await asyncio.sleep(watch_delay)",
            "                    continue",
            "",
            "    def lock(",
            "        self,",
            "        name: KeyT,",
            "        timeout: Optional[float] = None,",
            "        sleep: float = 0.1,",
            "        blocking: bool = True,",
            "        blocking_timeout: Optional[float] = None,",
            "        lock_class: Optional[Type[Lock]] = None,",
            "        thread_local: bool = True,",
            "    ) -> Lock:",
            "        \"\"\"",
            "        Return a new Lock object using key ``name`` that mimics",
            "        the behavior of threading.Lock.",
            "",
            "        If specified, ``timeout`` indicates a maximum life for the lock.",
            "        By default, it will remain locked until release() is called.",
            "",
            "        ``sleep`` indicates the amount of time to sleep per loop iteration",
            "        when the lock is in blocking mode and another client is currently",
            "        holding the lock.",
            "",
            "        ``blocking`` indicates whether calling ``acquire`` should block until",
            "        the lock has been acquired or to fail immediately, causing ``acquire``",
            "        to return False and the lock not being acquired. Defaults to True.",
            "        Note this value can be overridden by passing a ``blocking``",
            "        argument to ``acquire``.",
            "",
            "        ``blocking_timeout`` indicates the maximum amount of time in seconds to",
            "        spend trying to acquire the lock. A value of ``None`` indicates",
            "        continue trying forever. ``blocking_timeout`` can be specified as a",
            "        float or integer, both representing the number of seconds to wait.",
            "",
            "        ``lock_class`` forces the specified lock implementation. Note that as",
            "        of redis-py 3.0, the only lock class we implement is ``Lock`` (which is",
            "        a Lua-based lock). So, it's unlikely you'll need this parameter, unless",
            "        you have created your own custom lock class.",
            "",
            "        ``thread_local`` indicates whether the lock token is placed in",
            "        thread-local storage. By default, the token is placed in thread local",
            "        storage so that a thread only sees its token, not a token set by",
            "        another thread. Consider the following timeline:",
            "",
            "            time: 0, thread-1 acquires `my-lock`, with a timeout of 5 seconds.",
            "                     thread-1 sets the token to \"abc\"",
            "            time: 1, thread-2 blocks trying to acquire `my-lock` using the",
            "                     Lock instance.",
            "            time: 5, thread-1 has not yet completed. redis expires the lock",
            "                     key.",
            "            time: 5, thread-2 acquired `my-lock` now that it's available.",
            "                     thread-2 sets the token to \"xyz\"",
            "            time: 6, thread-1 finishes its work and calls release(). if the",
            "                     token is *not* stored in thread local storage, then",
            "                     thread-1 would see the token value as \"xyz\" and would be",
            "                     able to successfully release the thread-2's lock.",
            "",
            "        In some use cases it's necessary to disable thread local storage. For",
            "        example, if you have code where one thread acquires a lock and passes",
            "        that lock instance to a worker thread to release later. If thread",
            "        local storage isn't disabled in this case, the worker thread won't see",
            "        the token set by the thread that acquired the lock. Our assumption",
            "        is that these cases aren't common and as such default to using",
            "        thread local storage.\"\"\"",
            "        if lock_class is None:",
            "            lock_class = Lock",
            "        return lock_class(",
            "            self,",
            "            name,",
            "            timeout=timeout,",
            "            sleep=sleep,",
            "            blocking=blocking,",
            "            blocking_timeout=blocking_timeout,",
            "            thread_local=thread_local,",
            "        )",
            "",
            "    def pubsub(self, **kwargs) -> \"PubSub\":",
            "        \"\"\"",
            "        Return a Publish/Subscribe object. With this object, you can",
            "        subscribe to channels and listen for messages that get published to",
            "        them.",
            "        \"\"\"",
            "        return PubSub(self.connection_pool, **kwargs)",
            "",
            "    def monitor(self) -> \"Monitor\":",
            "        return Monitor(self.connection_pool)",
            "",
            "    def client(self) -> \"Redis\":",
            "        return self.__class__(",
            "            connection_pool=self.connection_pool, single_connection_client=True",
            "        )",
            "",
            "    async def __aenter__(self: _RedisT) -> _RedisT:",
            "        return await self.initialize()",
            "",
            "    async def __aexit__(self, exc_type, exc_value, traceback):",
            "        await self.close()",
            "",
            "    _DEL_MESSAGE = \"Unclosed Redis client\"",
            "",
            "    def __del__(self, _warnings: Any = warnings) -> None:",
            "        if self.connection is not None:",
            "            _warnings.warn(",
            "                f\"Unclosed client session {self!r}\", ResourceWarning, source=self",
            "            )",
            "            context = {\"client\": self, \"message\": self._DEL_MESSAGE}",
            "            asyncio.get_running_loop().call_exception_handler(context)",
            "",
            "    async def close(self, close_connection_pool: Optional[bool] = None) -> None:",
            "        \"\"\"",
            "        Closes Redis client connection",
            "",
            "        :param close_connection_pool: decides whether to close the connection pool used",
            "        by this Redis client, overriding Redis.auto_close_connection_pool. By default,",
            "        let Redis.auto_close_connection_pool decide whether to close the connection",
            "        pool.",
            "        \"\"\"",
            "        conn = self.connection",
            "        if conn:",
            "            self.connection = None",
            "            await self.connection_pool.release(conn)",
            "        if close_connection_pool or (",
            "            close_connection_pool is None and self.auto_close_connection_pool",
            "        ):",
            "            await self.connection_pool.disconnect()",
            "",
            "    async def _send_command_parse_response(self, conn, command_name, *args, **options):",
            "        \"\"\"",
            "        Send a command and parse the response",
            "        \"\"\"",
            "        await conn.send_command(*args)",
            "        return await self.parse_response(conn, command_name, **options)",
            "",
            "    async def _disconnect_raise(self, conn: Connection, error: Exception):",
            "        \"\"\"",
            "        Close the connection and raise an exception",
            "        if retry_on_error is not set or the error",
            "        is not one of the specified error types",
            "        \"\"\"",
            "        await conn.disconnect()",
            "        if (",
            "            conn.retry_on_error is None",
            "            or isinstance(error, tuple(conn.retry_on_error)) is False",
            "        ):",
            "            raise error",
            "",
            "    # COMMAND EXECUTION AND PROTOCOL PARSING",
            "    async def execute_command(self, *args, **options):",
            "        \"\"\"Execute a command and return a parsed response\"\"\"",
            "        await self.initialize()",
            "        pool = self.connection_pool",
            "        command_name = args[0]",
            "        conn = self.connection or await pool.get_connection(command_name, **options)",
            "",
            "        try:",
            "            return await conn.retry.call_with_retry(",
            "                lambda: self._send_command_parse_response(",
            "                    conn, command_name, *args, **options",
            "                ),",
            "                lambda error: self._disconnect_raise(conn, error),",
            "            )",
            "        finally:",
            "            if not self.connection:",
            "                await pool.release(conn)",
            "",
            "    async def parse_response(",
            "        self, connection: Connection, command_name: Union[str, bytes], **options",
            "    ):",
            "        \"\"\"Parses a response from the Redis server\"\"\"",
            "        try:",
            "            if NEVER_DECODE in options:",
            "                response = await connection.read_response(disable_decoding=True)",
            "                options.pop(NEVER_DECODE)",
            "            else:",
            "                response = await connection.read_response()",
            "        except ResponseError:",
            "            if EMPTY_RESPONSE in options:",
            "                return options[EMPTY_RESPONSE]",
            "            raise",
            "",
            "        if EMPTY_RESPONSE in options:",
            "            options.pop(EMPTY_RESPONSE)",
            "",
            "        if command_name in self.response_callbacks:",
            "            # Mypy bug: https://github.com/python/mypy/issues/10977",
            "            command_name = cast(str, command_name)",
            "            retval = self.response_callbacks[command_name](response, **options)",
            "            return await retval if inspect.isawaitable(retval) else retval",
            "        return response",
            "",
            "",
            "StrictRedis = Redis",
            "",
            "",
            "class MonitorCommandInfo(TypedDict):",
            "    time: float",
            "    db: int",
            "    client_address: str",
            "    client_port: str",
            "    client_type: str",
            "    command: str",
            "",
            "",
            "class Monitor:",
            "    \"\"\"",
            "    Monitor is useful for handling the MONITOR command to the redis server.",
            "    next_command() method returns one command from monitor",
            "    listen() method yields commands from monitor.",
            "    \"\"\"",
            "",
            "    monitor_re = re.compile(r\"\\[(\\d+) (.*)\\] (.*)\")",
            "    command_re = re.compile(r'\"(.*?)(?<!\\\\)\"')",
            "",
            "    def __init__(self, connection_pool: ConnectionPool):",
            "        self.connection_pool = connection_pool",
            "        self.connection: Optional[Connection] = None",
            "",
            "    async def connect(self):",
            "        if self.connection is None:",
            "            self.connection = await self.connection_pool.get_connection(\"MONITOR\")",
            "",
            "    async def __aenter__(self):",
            "        await self.connect()",
            "        await self.connection.send_command(\"MONITOR\")",
            "        # check that monitor returns 'OK', but don't return it to user",
            "        response = await self.connection.read_response()",
            "        if not bool_ok(response):",
            "            raise RedisError(f\"MONITOR failed: {response}\")",
            "        return self",
            "",
            "    async def __aexit__(self, *args):",
            "        await self.connection.disconnect()",
            "        await self.connection_pool.release(self.connection)",
            "",
            "    async def next_command(self) -> MonitorCommandInfo:",
            "        \"\"\"Parse the response from a monitor command\"\"\"",
            "        await self.connect()",
            "        response = await self.connection.read_response()",
            "        if isinstance(response, bytes):",
            "            response = self.connection.encoder.decode(response, force=True)",
            "        command_time, command_data = response.split(\" \", 1)",
            "        m = self.monitor_re.match(command_data)",
            "        db_id, client_info, command = m.groups()",
            "        command = \" \".join(self.command_re.findall(command))",
            "        # Redis escapes double quotes because each piece of the command",
            "        # string is surrounded by double quotes. We don't have that",
            "        # requirement so remove the escaping and leave the quote.",
            "        command = command.replace('\\\\\"', '\"')",
            "",
            "        if client_info == \"lua\":",
            "            client_address = \"lua\"",
            "            client_port = \"\"",
            "            client_type = \"lua\"",
            "        elif client_info.startswith(\"unix\"):",
            "            client_address = \"unix\"",
            "            client_port = client_info[5:]",
            "            client_type = \"unix\"",
            "        else:",
            "            # use rsplit as ipv6 addresses contain colons",
            "            client_address, client_port = client_info.rsplit(\":\", 1)",
            "            client_type = \"tcp\"",
            "        return {",
            "            \"time\": float(command_time),",
            "            \"db\": int(db_id),",
            "            \"client_address\": client_address,",
            "            \"client_port\": client_port,",
            "            \"client_type\": client_type,",
            "            \"command\": command,",
            "        }",
            "",
            "    async def listen(self) -> AsyncIterator[MonitorCommandInfo]:",
            "        \"\"\"Listen for commands coming to the server.\"\"\"",
            "        while True:",
            "            yield await self.next_command()",
            "",
            "",
            "class PubSub:",
            "    \"\"\"",
            "    PubSub provides publish, subscribe and listen support to Redis channels.",
            "",
            "    After subscribing to one or more channels, the listen() method will block",
            "    until a message arrives on one of the subscribed channels. That message",
            "    will be returned and it's safe to start listening again.",
            "    \"\"\"",
            "",
            "    PUBLISH_MESSAGE_TYPES = (\"message\", \"pmessage\")",
            "    UNSUBSCRIBE_MESSAGE_TYPES = (\"unsubscribe\", \"punsubscribe\")",
            "    HEALTH_CHECK_MESSAGE = \"redis-py-health-check\"",
            "",
            "    def __init__(",
            "        self,",
            "        connection_pool: ConnectionPool,",
            "        shard_hint: Optional[str] = None,",
            "        ignore_subscribe_messages: bool = False,",
            "        encoder=None,",
            "    ):",
            "        self.connection_pool = connection_pool",
            "        self.shard_hint = shard_hint",
            "        self.ignore_subscribe_messages = ignore_subscribe_messages",
            "        self.connection = None",
            "        # we need to know the encoding options for this connection in order",
            "        # to lookup channel and pattern names for callback handlers.",
            "        self.encoder = encoder",
            "        if self.encoder is None:",
            "            self.encoder = self.connection_pool.get_encoder()",
            "        if self.encoder.decode_responses:",
            "            self.health_check_response: Iterable[Union[str, bytes]] = [",
            "                \"pong\",",
            "                self.HEALTH_CHECK_MESSAGE,",
            "            ]",
            "        else:",
            "            self.health_check_response = [",
            "                b\"pong\",",
            "                self.encoder.encode(self.HEALTH_CHECK_MESSAGE),",
            "            ]",
            "        self.channels = {}",
            "        self.pending_unsubscribe_channels = set()",
            "        self.patterns = {}",
            "        self.pending_unsubscribe_patterns = set()",
            "        self._lock = asyncio.Lock()",
            "",
            "    async def __aenter__(self):",
            "        return self",
            "",
            "    async def __aexit__(self, exc_type, exc_value, traceback):",
            "        await self.reset()",
            "",
            "    def __del__(self):",
            "        if self.connection:",
            "            self.connection.clear_connect_callbacks()",
            "",
            "    async def reset(self):",
            "        async with self._lock:",
            "            if self.connection:",
            "                await self.connection.disconnect()",
            "                self.connection.clear_connect_callbacks()",
            "                await self.connection_pool.release(self.connection)",
            "                self.connection = None",
            "            self.channels = {}",
            "            self.pending_unsubscribe_channels = set()",
            "            self.patterns = {}",
            "            self.pending_unsubscribe_patterns = set()",
            "",
            "    def close(self) -> Awaitable[NoReturn]:",
            "        return self.reset()",
            "",
            "    async def on_connect(self, connection: Connection):",
            "        \"\"\"Re-subscribe to any channels and patterns previously subscribed to\"\"\"",
            "        # NOTE: for python3, we can't pass bytestrings as keyword arguments",
            "        # so we need to decode channel/pattern names back to unicode strings",
            "        # before passing them to [p]subscribe.",
            "        self.pending_unsubscribe_channels.clear()",
            "        self.pending_unsubscribe_patterns.clear()",
            "        if self.channels:",
            "            channels = {}",
            "            for k, v in self.channels.items():",
            "                channels[self.encoder.decode(k, force=True)] = v",
            "            await self.subscribe(**channels)",
            "        if self.patterns:",
            "            patterns = {}",
            "            for k, v in self.patterns.items():",
            "                patterns[self.encoder.decode(k, force=True)] = v",
            "            await self.psubscribe(**patterns)",
            "",
            "    @property",
            "    def subscribed(self):",
            "        \"\"\"Indicates if there are subscriptions to any channels or patterns\"\"\"",
            "        return bool(self.channels or self.patterns)",
            "",
            "    async def execute_command(self, *args: EncodableT):",
            "        \"\"\"Execute a publish/subscribe command\"\"\"",
            "",
            "        # NOTE: don't parse the response in this function -- it could pull a",
            "        # legitimate message off the stack if the connection is already",
            "        # subscribed to one or more channels",
            "",
            "        await self.connect()",
            "        connection = self.connection",
            "        kwargs = {\"check_health\": not self.subscribed}",
            "        await self._execute(connection, connection.send_command, *args, **kwargs)",
            "",
            "    async def connect(self):",
            "        \"\"\"",
            "        Ensure that the PubSub is connected",
            "        \"\"\"",
            "        if self.connection is None:",
            "            self.connection = await self.connection_pool.get_connection(",
            "                \"pubsub\", self.shard_hint",
            "            )",
            "            # register a callback that re-subscribes to any channels we",
            "            # were listening to when we were disconnected",
            "            self.connection.register_connect_callback(self.on_connect)",
            "        else:",
            "            await self.connection.connect()",
            "",
            "    async def _disconnect_raise_connect(self, conn, error):",
            "        \"\"\"",
            "        Close the connection and raise an exception",
            "        if retry_on_timeout is not set or the error",
            "        is not a TimeoutError. Otherwise, try to reconnect",
            "        \"\"\"",
            "        await conn.disconnect()",
            "        if not (conn.retry_on_timeout and isinstance(error, TimeoutError)):",
            "            raise error",
            "        await conn.connect()",
            "",
            "    async def _execute(self, conn, command, *args, **kwargs):",
            "        \"\"\"",
            "        Connect manually upon disconnection. If the Redis server is down,",
            "        this will fail and raise a ConnectionError as desired.",
            "        After reconnection, the ``on_connect`` callback should have been",
            "        called by the # connection to resubscribe us to any channels and",
            "        patterns we were previously listening to",
            "        \"\"\"",
            "        return await conn.retry.call_with_retry(",
            "            lambda: command(*args, **kwargs),",
            "            lambda error: self._disconnect_raise_connect(conn, error),",
            "        )",
            "",
            "    async def parse_response(self, block: bool = True, timeout: float = 0):",
            "        \"\"\"Parse the response from a publish/subscribe command\"\"\"",
            "        conn = self.connection",
            "        if conn is None:",
            "            raise RuntimeError(",
            "                \"pubsub connection not set: \"",
            "                \"did you forget to call subscribe() or psubscribe()?\"",
            "            )",
            "",
            "        await self.check_health()",
            "",
            "        if not conn.is_connected:",
            "            await conn.connect()",
            "",
            "        read_timeout = None if block else timeout",
            "        response = await self._execute(conn, conn.read_response, timeout=read_timeout)",
            "",
            "        if conn.health_check_interval and response == self.health_check_response:",
            "            # ignore the health check message as user might not expect it",
            "            return None",
            "        return response",
            "",
            "    async def check_health(self):",
            "        conn = self.connection",
            "        if conn is None:",
            "            raise RuntimeError(",
            "                \"pubsub connection not set: \"",
            "                \"did you forget to call subscribe() or psubscribe()?\"",
            "            )",
            "",
            "        if (",
            "            conn.health_check_interval",
            "            and asyncio.get_running_loop().time() > conn.next_health_check",
            "        ):",
            "            await conn.send_command(",
            "                \"PING\", self.HEALTH_CHECK_MESSAGE, check_health=False",
            "            )",
            "",
            "    def _normalize_keys(self, data: _NormalizeKeysT) -> _NormalizeKeysT:",
            "        \"\"\"",
            "        normalize channel/pattern names to be either bytes or strings",
            "        based on whether responses are automatically decoded. this saves us",
            "        from coercing the value for each message coming in.",
            "        \"\"\"",
            "        encode = self.encoder.encode",
            "        decode = self.encoder.decode",
            "        return {decode(encode(k)): v for k, v in data.items()}  # type: ignore[return-value]  # noqa: E501",
            "",
            "    async def psubscribe(self, *args: ChannelT, **kwargs: PubSubHandler):",
            "        \"\"\"",
            "        Subscribe to channel patterns. Patterns supplied as keyword arguments",
            "        expect a pattern name as the key and a callable as the value. A",
            "        pattern's callable will be invoked automatically when a message is",
            "        received on that pattern rather than producing a message via",
            "        ``listen()``.",
            "        \"\"\"",
            "        parsed_args = list_or_args((args[0],), args[1:]) if args else args",
            "        new_patterns: Dict[ChannelT, PubSubHandler] = dict.fromkeys(parsed_args)",
            "        # Mypy bug: https://github.com/python/mypy/issues/10970",
            "        new_patterns.update(kwargs)  # type: ignore[arg-type]",
            "        ret_val = await self.execute_command(\"PSUBSCRIBE\", *new_patterns.keys())",
            "        # update the patterns dict AFTER we send the command. we don't want to",
            "        # subscribe twice to these patterns, once for the command and again",
            "        # for the reconnection.",
            "        new_patterns = self._normalize_keys(new_patterns)",
            "        self.patterns.update(new_patterns)",
            "        self.pending_unsubscribe_patterns.difference_update(new_patterns)",
            "        return ret_val",
            "",
            "    def punsubscribe(self, *args: ChannelT) -> Awaitable:",
            "        \"\"\"",
            "        Unsubscribe from the supplied patterns. If empty, unsubscribe from",
            "        all patterns.",
            "        \"\"\"",
            "        patterns: Iterable[ChannelT]",
            "        if args:",
            "            parsed_args = list_or_args((args[0],), args[1:])",
            "            patterns = self._normalize_keys(dict.fromkeys(parsed_args)).keys()",
            "        else:",
            "            parsed_args = []",
            "            patterns = self.patterns",
            "        self.pending_unsubscribe_patterns.update(patterns)",
            "        return self.execute_command(\"PUNSUBSCRIBE\", *parsed_args)",
            "",
            "    async def subscribe(self, *args: ChannelT, **kwargs: Callable):",
            "        \"\"\"",
            "        Subscribe to channels. Channels supplied as keyword arguments expect",
            "        a channel name as the key and a callable as the value. A channel's",
            "        callable will be invoked automatically when a message is received on",
            "        that channel rather than producing a message via ``listen()`` or",
            "        ``get_message()``.",
            "        \"\"\"",
            "        parsed_args = list_or_args((args[0],), args[1:]) if args else ()",
            "        new_channels = dict.fromkeys(parsed_args)",
            "        # Mypy bug: https://github.com/python/mypy/issues/10970",
            "        new_channels.update(kwargs)  # type: ignore[arg-type]",
            "        ret_val = await self.execute_command(\"SUBSCRIBE\", *new_channels.keys())",
            "        # update the channels dict AFTER we send the command. we don't want to",
            "        # subscribe twice to these channels, once for the command and again",
            "        # for the reconnection.",
            "        new_channels = self._normalize_keys(new_channels)",
            "        self.channels.update(new_channels)",
            "        self.pending_unsubscribe_channels.difference_update(new_channels)",
            "        return ret_val",
            "",
            "    def unsubscribe(self, *args) -> Awaitable:",
            "        \"\"\"",
            "        Unsubscribe from the supplied channels. If empty, unsubscribe from",
            "        all channels",
            "        \"\"\"",
            "        if args:",
            "            parsed_args = list_or_args(args[0], args[1:])",
            "            channels = self._normalize_keys(dict.fromkeys(parsed_args))",
            "        else:",
            "            parsed_args = []",
            "            channels = self.channels",
            "        self.pending_unsubscribe_channels.update(channels)",
            "        return self.execute_command(\"UNSUBSCRIBE\", *parsed_args)",
            "",
            "    async def listen(self) -> AsyncIterator:",
            "        \"\"\"Listen for messages on channels this client has been subscribed to\"\"\"",
            "        while self.subscribed:",
            "            response = await self.handle_message(await self.parse_response(block=True))",
            "            if response is not None:",
            "                yield response",
            "",
            "    async def get_message(",
            "        self, ignore_subscribe_messages: bool = False, timeout: Optional[float] = 0.0",
            "    ):",
            "        \"\"\"",
            "        Get the next message if one is available, otherwise None.",
            "",
            "        If timeout is specified, the system will wait for `timeout` seconds",
            "        before returning. Timeout should be specified as a floating point",
            "        number or None to wait indefinitely.",
            "        \"\"\"",
            "        response = await self.parse_response(block=(timeout is None), timeout=timeout)",
            "        if response:",
            "            return await self.handle_message(response, ignore_subscribe_messages)",
            "        return None",
            "",
            "    def ping(self, message=None) -> Awaitable:",
            "        \"\"\"",
            "        Ping the Redis server",
            "        \"\"\"",
            "        message = \"\" if message is None else message",
            "        return self.execute_command(\"PING\", message)",
            "",
            "    async def handle_message(self, response, ignore_subscribe_messages=False):",
            "        \"\"\"",
            "        Parses a pub/sub message. If the channel or pattern was subscribed to",
            "        with a message handler, the handler is invoked instead of a parsed",
            "        message being returned.",
            "        \"\"\"",
            "        message_type = str_if_bytes(response[0])",
            "        if message_type == \"pmessage\":",
            "            message = {",
            "                \"type\": message_type,",
            "                \"pattern\": response[1],",
            "                \"channel\": response[2],",
            "                \"data\": response[3],",
            "            }",
            "        elif message_type == \"pong\":",
            "            message = {",
            "                \"type\": message_type,",
            "                \"pattern\": None,",
            "                \"channel\": None,",
            "                \"data\": response[1],",
            "            }",
            "        else:",
            "            message = {",
            "                \"type\": message_type,",
            "                \"pattern\": None,",
            "                \"channel\": response[1],",
            "                \"data\": response[2],",
            "            }",
            "",
            "        # if this is an unsubscribe message, remove it from memory",
            "        if message_type in self.UNSUBSCRIBE_MESSAGE_TYPES:",
            "            if message_type == \"punsubscribe\":",
            "                pattern = response[1]",
            "                if pattern in self.pending_unsubscribe_patterns:",
            "                    self.pending_unsubscribe_patterns.remove(pattern)",
            "                    self.patterns.pop(pattern, None)",
            "            else:",
            "                channel = response[1]",
            "                if channel in self.pending_unsubscribe_channels:",
            "                    self.pending_unsubscribe_channels.remove(channel)",
            "                    self.channels.pop(channel, None)",
            "",
            "        if message_type in self.PUBLISH_MESSAGE_TYPES:",
            "            # if there's a message handler, invoke it",
            "            if message_type == \"pmessage\":",
            "                handler = self.patterns.get(message[\"pattern\"], None)",
            "            else:",
            "                handler = self.channels.get(message[\"channel\"], None)",
            "            if handler:",
            "                if inspect.iscoroutinefunction(handler):",
            "                    await handler(message)",
            "                else:",
            "                    handler(message)",
            "                return None",
            "        elif message_type != \"pong\":",
            "            # this is a subscribe/unsubscribe message. ignore if we don't",
            "            # want them",
            "            if ignore_subscribe_messages or self.ignore_subscribe_messages:",
            "                return None",
            "",
            "        return message",
            "",
            "    async def run(",
            "        self,",
            "        *,",
            "        exception_handler: Optional[\"PSWorkerThreadExcHandlerT\"] = None,",
            "        poll_timeout: float = 1.0,",
            "    ) -> None:",
            "        \"\"\"Process pub/sub messages using registered callbacks.",
            "",
            "        This is the equivalent of :py:meth:`redis.PubSub.run_in_thread` in",
            "        redis-py, but it is a coroutine. To launch it as a separate task, use",
            "        ``asyncio.create_task``:",
            "",
            "            >>> task = asyncio.create_task(pubsub.run())",
            "",
            "        To shut it down, use asyncio cancellation:",
            "",
            "            >>> task.cancel()",
            "            >>> await task",
            "        \"\"\"",
            "        for channel, handler in self.channels.items():",
            "            if handler is None:",
            "                raise PubSubError(f\"Channel: '{channel}' has no handler registered\")",
            "        for pattern, handler in self.patterns.items():",
            "            if handler is None:",
            "                raise PubSubError(f\"Pattern: '{pattern}' has no handler registered\")",
            "",
            "        await self.connect()",
            "        while True:",
            "            try:",
            "                await self.get_message(",
            "                    ignore_subscribe_messages=True, timeout=poll_timeout",
            "                )",
            "            except asyncio.CancelledError:",
            "                raise",
            "            except BaseException as e:",
            "                if exception_handler is None:",
            "                    raise",
            "                res = exception_handler(e, self)",
            "                if inspect.isawaitable(res):",
            "                    await res",
            "            # Ensure that other tasks on the event loop get a chance to run",
            "            # if we didn't have to block for I/O anywhere.",
            "            await asyncio.sleep(0)",
            "",
            "",
            "class PubsubWorkerExceptionHandler(Protocol):",
            "    def __call__(self, e: BaseException, pubsub: PubSub):",
            "        ...",
            "",
            "",
            "class AsyncPubsubWorkerExceptionHandler(Protocol):",
            "    async def __call__(self, e: BaseException, pubsub: PubSub):",
            "        ...",
            "",
            "",
            "PSWorkerThreadExcHandlerT = Union[",
            "    PubsubWorkerExceptionHandler, AsyncPubsubWorkerExceptionHandler",
            "]",
            "",
            "",
            "CommandT = Tuple[Tuple[Union[str, bytes], ...], Mapping[str, Any]]",
            "CommandStackT = List[CommandT]",
            "",
            "",
            "class Pipeline(Redis):  # lgtm [py/init-calls-subclass]",
            "    \"\"\"",
            "    Pipelines provide a way to transmit multiple commands to the Redis server",
            "    in one transmission.  This is convenient for batch processing, such as",
            "    saving all the values in a list to Redis.",
            "",
            "    All commands executed within a pipeline are wrapped with MULTI and EXEC",
            "    calls. This guarantees all commands executed in the pipeline will be",
            "    executed atomically.",
            "",
            "    Any command raising an exception does *not* halt the execution of",
            "    subsequent commands in the pipeline. Instead, the exception is caught",
            "    and its instance is placed into the response list returned by execute().",
            "    Code iterating over the response list should be able to deal with an",
            "    instance of an exception as a potential value. In general, these will be",
            "    ResponseError exceptions, such as those raised when issuing a command",
            "    on a key of a different datatype.",
            "    \"\"\"",
            "",
            "    UNWATCH_COMMANDS = {\"DISCARD\", \"EXEC\", \"UNWATCH\"}",
            "",
            "    def __init__(",
            "        self,",
            "        connection_pool: ConnectionPool,",
            "        response_callbacks: MutableMapping[Union[str, bytes], ResponseCallbackT],",
            "        transaction: bool,",
            "        shard_hint: Optional[str],",
            "    ):",
            "        self.connection_pool = connection_pool",
            "        self.connection = None",
            "        self.response_callbacks = response_callbacks",
            "        self.is_transaction = transaction",
            "        self.shard_hint = shard_hint",
            "        self.watching = False",
            "        self.command_stack: CommandStackT = []",
            "        self.scripts: Set[\"Script\"] = set()",
            "        self.explicit_transaction = False",
            "",
            "    async def __aenter__(self: _RedisT) -> _RedisT:",
            "        return self",
            "",
            "    async def __aexit__(self, exc_type, exc_value, traceback):",
            "        await self.reset()",
            "",
            "    def __await__(self):",
            "        return self._async_self().__await__()",
            "",
            "    _DEL_MESSAGE = \"Unclosed Pipeline client\"",
            "",
            "    def __len__(self):",
            "        return len(self.command_stack)",
            "",
            "    def __bool__(self):",
            "        \"\"\"Pipeline instances should always evaluate to True\"\"\"",
            "        return True",
            "",
            "    async def _async_self(self):",
            "        return self",
            "",
            "    async def reset(self):",
            "        self.command_stack = []",
            "        self.scripts = set()",
            "        # make sure to reset the connection state in the event that we were",
            "        # watching something",
            "        if self.watching and self.connection:",
            "            try:",
            "                # call this manually since our unwatch or",
            "                # immediate_execute_command methods can call reset()",
            "                await self.connection.send_command(\"UNWATCH\")",
            "                await self.connection.read_response()",
            "            except ConnectionError:",
            "                # disconnect will also remove any previous WATCHes",
            "                if self.connection:",
            "                    await self.connection.disconnect()",
            "        # clean up the other instance attributes",
            "        self.watching = False",
            "        self.explicit_transaction = False",
            "        # we can safely return the connection to the pool here since we're",
            "        # sure we're no longer WATCHing anything",
            "        if self.connection:",
            "            await self.connection_pool.release(self.connection)",
            "            self.connection = None",
            "",
            "    def multi(self):",
            "        \"\"\"",
            "        Start a transactional block of the pipeline after WATCH commands",
            "        are issued. End the transactional block with `execute`.",
            "        \"\"\"",
            "        if self.explicit_transaction:",
            "            raise RedisError(\"Cannot issue nested calls to MULTI\")",
            "        if self.command_stack:",
            "            raise RedisError(",
            "                \"Commands without an initial WATCH have already been issued\"",
            "            )",
            "        self.explicit_transaction = True",
            "",
            "    def execute_command(",
            "        self, *args, **kwargs",
            "    ) -> Union[\"Pipeline\", Awaitable[\"Pipeline\"]]:",
            "        if (self.watching or args[0] == \"WATCH\") and not self.explicit_transaction:",
            "            return self.immediate_execute_command(*args, **kwargs)",
            "        return self.pipeline_execute_command(*args, **kwargs)",
            "",
            "    async def _disconnect_reset_raise(self, conn, error):",
            "        \"\"\"",
            "        Close the connection, reset watching state and",
            "        raise an exception if we were watching,",
            "        retry_on_timeout is not set,",
            "        or the error is not a TimeoutError",
            "        \"\"\"",
            "        await conn.disconnect()",
            "        # if we were already watching a variable, the watch is no longer",
            "        # valid since this connection has died. raise a WatchError, which",
            "        # indicates the user should retry this transaction.",
            "        if self.watching:",
            "            await self.reset()",
            "            raise WatchError(",
            "                \"A ConnectionError occurred on while watching one or more keys\"",
            "            )",
            "        # if retry_on_timeout is not set, or the error is not",
            "        # a TimeoutError, raise it",
            "        if not (conn.retry_on_timeout and isinstance(error, TimeoutError)):",
            "            await self.reset()",
            "            raise",
            "",
            "    async def immediate_execute_command(self, *args, **options):",
            "        \"\"\"",
            "        Execute a command immediately, but don't auto-retry on a",
            "        ConnectionError if we're already WATCHing a variable. Used when",
            "        issuing WATCH or subsequent commands retrieving their values but before",
            "        MULTI is called.",
            "        \"\"\"",
            "        command_name = args[0]",
            "        conn = self.connection",
            "        # if this is the first call, we need a connection",
            "        if not conn:",
            "            conn = await self.connection_pool.get_connection(",
            "                command_name, self.shard_hint",
            "            )",
            "            self.connection = conn",
            "",
            "        return await conn.retry.call_with_retry(",
            "            lambda: self._send_command_parse_response(",
            "                conn, command_name, *args, **options",
            "            ),",
            "            lambda error: self._disconnect_reset_raise(conn, error),",
            "        )",
            "",
            "    def pipeline_execute_command(self, *args, **options):",
            "        \"\"\"",
            "        Stage a command to be executed when execute() is next called",
            "",
            "        Returns the current Pipeline object back so commands can be",
            "        chained together, such as:",
            "",
            "        pipe = pipe.set('foo', 'bar').incr('baz').decr('bang')",
            "",
            "        At some other point, you can then run: pipe.execute(),",
            "        which will execute all commands queued in the pipe.",
            "        \"\"\"",
            "        self.command_stack.append((args, options))",
            "        return self",
            "",
            "    async def _execute_transaction(  # noqa: C901",
            "        self, connection: Connection, commands: CommandStackT, raise_on_error",
            "    ):",
            "        pre: CommandT = ((\"MULTI\",), {})",
            "        post: CommandT = ((\"EXEC\",), {})",
            "        cmds = (pre, *commands, post)",
            "        all_cmds = connection.pack_commands(",
            "            args for args, options in cmds if EMPTY_RESPONSE not in options",
            "        )",
            "        await connection.send_packed_command(all_cmds)",
            "        errors = []",
            "",
            "        # parse off the response for MULTI",
            "        # NOTE: we need to handle ResponseErrors here and continue",
            "        # so that we read all the additional command messages from",
            "        # the socket",
            "        try:",
            "            await self.parse_response(connection, \"_\")",
            "        except ResponseError as err:",
            "            errors.append((0, err))",
            "",
            "        # and all the other commands",
            "        for i, command in enumerate(commands):",
            "            if EMPTY_RESPONSE in command[1]:",
            "                errors.append((i, command[1][EMPTY_RESPONSE]))",
            "            else:",
            "                try:",
            "                    await self.parse_response(connection, \"_\")",
            "                except ResponseError as err:",
            "                    self.annotate_exception(err, i + 1, command[0])",
            "                    errors.append((i, err))",
            "",
            "        # parse the EXEC.",
            "        try:",
            "            response = await self.parse_response(connection, \"_\")",
            "        except ExecAbortError as err:",
            "            if errors:",
            "                raise errors[0][1] from err",
            "            raise",
            "",
            "        # EXEC clears any watched keys",
            "        self.watching = False",
            "",
            "        if response is None:",
            "            raise WatchError(\"Watched variable changed.\") from None",
            "",
            "        # put any parse errors into the response",
            "        for i, e in errors:",
            "            response.insert(i, e)",
            "",
            "        if len(response) != len(commands):",
            "            if self.connection:",
            "                await self.connection.disconnect()",
            "            raise ResponseError(",
            "                \"Wrong number of response items from pipeline execution\"",
            "            ) from None",
            "",
            "        # find any errors in the response and raise if necessary",
            "        if raise_on_error:",
            "            self.raise_first_error(commands, response)",
            "",
            "        # We have to run response callbacks manually",
            "        data = []",
            "        for r, cmd in zip(response, commands):",
            "            if not isinstance(r, Exception):",
            "                args, options = cmd",
            "                command_name = args[0]",
            "                if command_name in self.response_callbacks:",
            "                    r = self.response_callbacks[command_name](r, **options)",
            "                    if inspect.isawaitable(r):",
            "                        r = await r",
            "            data.append(r)",
            "        return data",
            "",
            "    async def _execute_pipeline(",
            "        self, connection: Connection, commands: CommandStackT, raise_on_error: bool",
            "    ):",
            "        # build up all commands into a single request to increase network perf",
            "        all_cmds = connection.pack_commands([args for args, _ in commands])",
            "        await connection.send_packed_command(all_cmds)",
            "",
            "        response = []",
            "        for args, options in commands:",
            "            try:",
            "                response.append(",
            "                    await self.parse_response(connection, args[0], **options)",
            "                )",
            "            except ResponseError as e:",
            "                response.append(e)",
            "",
            "        if raise_on_error:",
            "            self.raise_first_error(commands, response)",
            "        return response",
            "",
            "    def raise_first_error(self, commands: CommandStackT, response: Iterable[Any]):",
            "        for i, r in enumerate(response):",
            "            if isinstance(r, ResponseError):",
            "                self.annotate_exception(r, i + 1, commands[i][0])",
            "                raise r",
            "",
            "    def annotate_exception(",
            "        self, exception: Exception, number: int, command: Iterable[object]",
            "    ) -> None:",
            "        cmd = \" \".join(map(safe_str, command))",
            "        msg = f\"Command # {number} ({cmd}) of pipeline caused error: {exception.args}\"",
            "        exception.args = (msg,) + exception.args[1:]",
            "",
            "    async def parse_response(",
            "        self, connection: Connection, command_name: Union[str, bytes], **options",
            "    ):",
            "        result = await super().parse_response(connection, command_name, **options)",
            "        if command_name in self.UNWATCH_COMMANDS:",
            "            self.watching = False",
            "        elif command_name == \"WATCH\":",
            "            self.watching = True",
            "        return result",
            "",
            "    async def load_scripts(self):",
            "        # make sure all scripts that are about to be run on this pipeline exist",
            "        scripts = list(self.scripts)",
            "        immediate = self.immediate_execute_command",
            "        shas = [s.sha for s in scripts]",
            "        # we can't use the normal script_* methods because they would just",
            "        # get buffered in the pipeline.",
            "        exists = await immediate(\"SCRIPT EXISTS\", *shas)",
            "        if not all(exists):",
            "            for s, exist in zip(scripts, exists):",
            "                if not exist:",
            "                    s.sha = await immediate(\"SCRIPT LOAD\", s.script)",
            "",
            "    async def _disconnect_raise_reset(self, conn: Connection, error: Exception):",
            "        \"\"\"",
            "        Close the connection, raise an exception if we were watching,",
            "        and raise an exception if retry_on_timeout is not set,",
            "        or the error is not a TimeoutError",
            "        \"\"\"",
            "        await conn.disconnect()",
            "        # if we were watching a variable, the watch is no longer valid",
            "        # since this connection has died. raise a WatchError, which",
            "        # indicates the user should retry this transaction.",
            "        if self.watching:",
            "            raise WatchError(",
            "                \"A ConnectionError occurred on while watching one or more keys\"",
            "            )",
            "        # if retry_on_timeout is not set, or the error is not",
            "        # a TimeoutError, raise it",
            "        if not (conn.retry_on_timeout and isinstance(error, TimeoutError)):",
            "            await self.reset()",
            "            raise",
            "",
            "    async def execute(self, raise_on_error: bool = True):",
            "        \"\"\"Execute all the commands in the current pipeline\"\"\"",
            "        stack = self.command_stack",
            "        if not stack and not self.watching:",
            "            return []",
            "        if self.scripts:",
            "            await self.load_scripts()",
            "        if self.is_transaction or self.explicit_transaction:",
            "            execute = self._execute_transaction",
            "        else:",
            "            execute = self._execute_pipeline",
            "",
            "        conn = self.connection",
            "        if not conn:",
            "            conn = await self.connection_pool.get_connection(\"MULTI\", self.shard_hint)",
            "            # assign to self.connection so reset() releases the connection",
            "            # back to the pool after we're done",
            "            self.connection = conn",
            "        conn = cast(Connection, conn)",
            "",
            "        try:",
            "            return await conn.retry.call_with_retry(",
            "                lambda: execute(conn, stack, raise_on_error),",
            "                lambda error: self._disconnect_raise_reset(conn, error),",
            "            )",
            "        finally:",
            "            await self.reset()",
            "",
            "    async def discard(self):",
            "        \"\"\"Flushes all previously queued commands",
            "        See: https://redis.io/commands/DISCARD",
            "        \"\"\"",
            "        await self.execute_command(\"DISCARD\")",
            "",
            "    async def watch(self, *names: KeyT):",
            "        \"\"\"Watches the values at keys ``names``\"\"\"",
            "        if self.explicit_transaction:",
            "            raise RedisError(\"Cannot issue a WATCH after a MULTI\")",
            "        return await self.execute_command(\"WATCH\", *names)",
            "",
            "    async def unwatch(self):",
            "        \"\"\"Unwatches all previously specified keys\"\"\"",
            "        return self.watching and await self.execute_command(\"UNWATCH\") or True"
        ],
        "afterPatchFile": [
            "import asyncio",
            "import copy",
            "import inspect",
            "import re",
            "import warnings",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    AsyncIterator,",
            "    Awaitable,",
            "    Callable,",
            "    Dict,",
            "    Iterable,",
            "    List,",
            "    Mapping,",
            "    MutableMapping,",
            "    NoReturn,",
            "    Optional,",
            "    Set,",
            "    Tuple,",
            "    Type,",
            "    TypeVar,",
            "    Union,",
            "    cast,",
            ")",
            "",
            "from redis.asyncio.connection import (",
            "    Connection,",
            "    ConnectionPool,",
            "    SSLConnection,",
            "    UnixDomainSocketConnection,",
            ")",
            "from redis.asyncio.lock import Lock",
            "from redis.asyncio.retry import Retry",
            "from redis.client import (",
            "    EMPTY_RESPONSE,",
            "    NEVER_DECODE,",
            "    AbstractRedis,",
            "    CaseInsensitiveDict,",
            "    bool_ok,",
            ")",
            "from redis.commands import (",
            "    AsyncCoreCommands,",
            "    AsyncRedisModuleCommands,",
            "    AsyncSentinelCommands,",
            "    list_or_args,",
            ")",
            "from redis.compat import Protocol, TypedDict",
            "from redis.credentials import CredentialProvider",
            "from redis.exceptions import (",
            "    ConnectionError,",
            "    ExecAbortError,",
            "    PubSubError,",
            "    RedisError,",
            "    ResponseError,",
            "    TimeoutError,",
            "    WatchError,",
            ")",
            "from redis.typing import ChannelT, EncodableT, KeyT",
            "from redis.utils import safe_str, str_if_bytes",
            "",
            "PubSubHandler = Callable[[Dict[str, str]], Awaitable[None]]",
            "_KeyT = TypeVar(\"_KeyT\", bound=KeyT)",
            "_ArgT = TypeVar(\"_ArgT\", KeyT, EncodableT)",
            "_RedisT = TypeVar(\"_RedisT\", bound=\"Redis\")",
            "_NormalizeKeysT = TypeVar(\"_NormalizeKeysT\", bound=Mapping[ChannelT, object])",
            "if TYPE_CHECKING:",
            "    from redis.commands.core import Script",
            "",
            "",
            "class ResponseCallbackProtocol(Protocol):",
            "    def __call__(self, response: Any, **kwargs):",
            "        ...",
            "",
            "",
            "class AsyncResponseCallbackProtocol(Protocol):",
            "    async def __call__(self, response: Any, **kwargs):",
            "        ...",
            "",
            "",
            "ResponseCallbackT = Union[ResponseCallbackProtocol, AsyncResponseCallbackProtocol]",
            "",
            "",
            "class Redis(",
            "    AbstractRedis, AsyncRedisModuleCommands, AsyncCoreCommands, AsyncSentinelCommands",
            "):",
            "    \"\"\"",
            "    Implementation of the Redis protocol.",
            "",
            "    This abstract class provides a Python interface to all Redis commands",
            "    and an implementation of the Redis protocol.",
            "",
            "    Pipelines derive from this, implementing how",
            "    the commands are sent and received to the Redis server. Based on",
            "    configuration, an instance will either use a ConnectionPool, or",
            "    Connection object to talk to redis.",
            "    \"\"\"",
            "",
            "    response_callbacks: MutableMapping[Union[str, bytes], ResponseCallbackT]",
            "",
            "    @classmethod",
            "    def from_url(cls, url: str, **kwargs):",
            "        \"\"\"",
            "        Return a Redis client object configured from the given URL",
            "",
            "        For example::",
            "",
            "            redis://[[username]:[password]]@localhost:6379/0",
            "            rediss://[[username]:[password]]@localhost:6379/0",
            "            unix://[username@]/path/to/socket.sock?db=0[&password=password]",
            "",
            "        Three URL schemes are supported:",
            "",
            "        - `redis://` creates a TCP socket connection. See more at:",
            "          <https://www.iana.org/assignments/uri-schemes/prov/redis>",
            "        - `rediss://` creates a SSL wrapped TCP socket connection. See more at:",
            "          <https://www.iana.org/assignments/uri-schemes/prov/rediss>",
            "        - ``unix://``: creates a Unix Domain Socket connection.",
            "",
            "        The username, password, hostname, path and all querystring values",
            "        are passed through urllib.parse.unquote in order to replace any",
            "        percent-encoded values with their corresponding characters.",
            "",
            "        There are several ways to specify a database number. The first value",
            "        found will be used:",
            "            1. A ``db`` querystring option, e.g. redis://localhost?db=0",
            "            2. If using the redis:// or rediss:// schemes, the path argument",
            "               of the url, e.g. redis://localhost/0",
            "            3. A ``db`` keyword argument to this function.",
            "",
            "        If none of these options are specified, the default db=0 is used.",
            "",
            "        All querystring options are cast to their appropriate Python types.",
            "        Boolean arguments can be specified with string values \"True\"/\"False\"",
            "        or \"Yes\"/\"No\". Values that cannot be properly cast cause a",
            "        ``ValueError`` to be raised. Once parsed, the querystring arguments",
            "        and keyword arguments are passed to the ``ConnectionPool``'s",
            "        class initializer. In the case of conflicting arguments, querystring",
            "        arguments always win.",
            "",
            "        \"\"\"",
            "        connection_pool = ConnectionPool.from_url(url, **kwargs)",
            "        return cls(connection_pool=connection_pool)",
            "",
            "    def __init__(",
            "        self,",
            "        *,",
            "        host: str = \"localhost\",",
            "        port: int = 6379,",
            "        db: Union[str, int] = 0,",
            "        password: Optional[str] = None,",
            "        socket_timeout: Optional[float] = None,",
            "        socket_connect_timeout: Optional[float] = None,",
            "        socket_keepalive: Optional[bool] = None,",
            "        socket_keepalive_options: Optional[Mapping[int, Union[int, bytes]]] = None,",
            "        connection_pool: Optional[ConnectionPool] = None,",
            "        unix_socket_path: Optional[str] = None,",
            "        encoding: str = \"utf-8\",",
            "        encoding_errors: str = \"strict\",",
            "        decode_responses: bool = False,",
            "        retry_on_timeout: bool = False,",
            "        retry_on_error: Optional[list] = None,",
            "        ssl: bool = False,",
            "        ssl_keyfile: Optional[str] = None,",
            "        ssl_certfile: Optional[str] = None,",
            "        ssl_cert_reqs: str = \"required\",",
            "        ssl_ca_certs: Optional[str] = None,",
            "        ssl_ca_data: Optional[str] = None,",
            "        ssl_check_hostname: bool = False,",
            "        max_connections: Optional[int] = None,",
            "        single_connection_client: bool = False,",
            "        health_check_interval: int = 0,",
            "        client_name: Optional[str] = None,",
            "        username: Optional[str] = None,",
            "        retry: Optional[Retry] = None,",
            "        auto_close_connection_pool: bool = True,",
            "        redis_connect_func=None,",
            "        credential_provider: Optional[CredentialProvider] = None,",
            "    ):",
            "        \"\"\"",
            "        Initialize a new Redis client.",
            "        To specify a retry policy for specific errors, first set",
            "        `retry_on_error` to a list of the error/s to retry on, then set",
            "        `retry` to a valid `Retry` object.",
            "        To retry on TimeoutError, `retry_on_timeout` can also be set to `True`.",
            "        \"\"\"",
            "        kwargs: Dict[str, Any]",
            "        # auto_close_connection_pool only has an effect if connection_pool is",
            "        # None. This is a similar feature to the missing __del__ to resolve #1103,",
            "        # but it accounts for whether a user wants to manually close the connection",
            "        # pool, as a similar feature to ConnectionPool's __del__.",
            "        self.auto_close_connection_pool = (",
            "            auto_close_connection_pool if connection_pool is None else False",
            "        )",
            "        if not connection_pool:",
            "            if not retry_on_error:",
            "                retry_on_error = []",
            "            if retry_on_timeout is True:",
            "                retry_on_error.append(TimeoutError)",
            "            kwargs = {",
            "                \"db\": db,",
            "                \"username\": username,",
            "                \"password\": password,",
            "                \"credential_provider\": credential_provider,",
            "                \"socket_timeout\": socket_timeout,",
            "                \"encoding\": encoding,",
            "                \"encoding_errors\": encoding_errors,",
            "                \"decode_responses\": decode_responses,",
            "                \"retry_on_timeout\": retry_on_timeout,",
            "                \"retry_on_error\": retry_on_error,",
            "                \"retry\": copy.deepcopy(retry),",
            "                \"max_connections\": max_connections,",
            "                \"health_check_interval\": health_check_interval,",
            "                \"client_name\": client_name,",
            "                \"redis_connect_func\": redis_connect_func,",
            "            }",
            "            # based on input, setup appropriate connection args",
            "            if unix_socket_path is not None:",
            "                kwargs.update(",
            "                    {",
            "                        \"path\": unix_socket_path,",
            "                        \"connection_class\": UnixDomainSocketConnection,",
            "                    }",
            "                )",
            "            else:",
            "                # TCP specific options",
            "                kwargs.update(",
            "                    {",
            "                        \"host\": host,",
            "                        \"port\": port,",
            "                        \"socket_connect_timeout\": socket_connect_timeout,",
            "                        \"socket_keepalive\": socket_keepalive,",
            "                        \"socket_keepalive_options\": socket_keepalive_options,",
            "                    }",
            "                )",
            "",
            "                if ssl:",
            "                    kwargs.update(",
            "                        {",
            "                            \"connection_class\": SSLConnection,",
            "                            \"ssl_keyfile\": ssl_keyfile,",
            "                            \"ssl_certfile\": ssl_certfile,",
            "                            \"ssl_cert_reqs\": ssl_cert_reqs,",
            "                            \"ssl_ca_certs\": ssl_ca_certs,",
            "                            \"ssl_ca_data\": ssl_ca_data,",
            "                            \"ssl_check_hostname\": ssl_check_hostname,",
            "                        }",
            "                    )",
            "            connection_pool = ConnectionPool(**kwargs)",
            "        self.connection_pool = connection_pool",
            "        self.single_connection_client = single_connection_client",
            "        self.connection: Optional[Connection] = None",
            "",
            "        self.response_callbacks = CaseInsensitiveDict(self.__class__.RESPONSE_CALLBACKS)",
            "",
            "    def __repr__(self):",
            "        return f\"{self.__class__.__name__}<{self.connection_pool!r}>\"",
            "",
            "    def __await__(self):",
            "        return self.initialize().__await__()",
            "",
            "    async def initialize(self: _RedisT) -> _RedisT:",
            "        if self.single_connection_client and self.connection is None:",
            "            self.connection = await self.connection_pool.get_connection(\"_\")",
            "        return self",
            "",
            "    def set_response_callback(self, command: str, callback: ResponseCallbackT):",
            "        \"\"\"Set a custom Response Callback\"\"\"",
            "        self.response_callbacks[command] = callback",
            "",
            "    def get_encoder(self):",
            "        \"\"\"Get the connection pool's encoder\"\"\"",
            "        return self.connection_pool.get_encoder()",
            "",
            "    def get_connection_kwargs(self):",
            "        \"\"\"Get the connection's key-word arguments\"\"\"",
            "        return self.connection_pool.connection_kwargs",
            "",
            "    def get_retry(self) -> Optional[\"Retry\"]:",
            "        return self.get_connection_kwargs().get(\"retry\")",
            "",
            "    def set_retry(self, retry: \"Retry\") -> None:",
            "        self.get_connection_kwargs().update({\"retry\": retry})",
            "        self.connection_pool.set_retry(retry)",
            "",
            "    def load_external_module(self, funcname, func):",
            "        \"\"\"",
            "        This function can be used to add externally defined redis modules,",
            "        and their namespaces to the redis client.",
            "",
            "        funcname - A string containing the name of the function to create",
            "        func - The function, being added to this class.",
            "",
            "        ex: Assume that one has a custom redis module named foomod that",
            "        creates command named 'foo.dothing' and 'foo.anotherthing' in redis.",
            "        To load function functions into this namespace:",
            "",
            "        from redis import Redis",
            "        from foomodule import F",
            "        r = Redis()",
            "        r.load_external_module(\"foo\", F)",
            "        r.foo().dothing('your', 'arguments')",
            "",
            "        For a concrete example see the reimport of the redisjson module in",
            "        tests/test_connection.py::test_loading_external_modules",
            "        \"\"\"",
            "        setattr(self, funcname, func)",
            "",
            "    def pipeline(",
            "        self, transaction: bool = True, shard_hint: Optional[str] = None",
            "    ) -> \"Pipeline\":",
            "        \"\"\"",
            "        Return a new pipeline object that can queue multiple commands for",
            "        later execution. ``transaction`` indicates whether all commands",
            "        should be executed atomically. Apart from making a group of operations",
            "        atomic, pipelines are useful for reducing the back-and-forth overhead",
            "        between the client and server.",
            "        \"\"\"",
            "        return Pipeline(",
            "            self.connection_pool, self.response_callbacks, transaction, shard_hint",
            "        )",
            "",
            "    async def transaction(",
            "        self,",
            "        func: Callable[[\"Pipeline\"], Union[Any, Awaitable[Any]]],",
            "        *watches: KeyT,",
            "        shard_hint: Optional[str] = None,",
            "        value_from_callable: bool = False,",
            "        watch_delay: Optional[float] = None,",
            "    ):",
            "        \"\"\"",
            "        Convenience method for executing the callable `func` as a transaction",
            "        while watching all keys specified in `watches`. The 'func' callable",
            "        should expect a single argument which is a Pipeline object.",
            "        \"\"\"",
            "        pipe: Pipeline",
            "        async with self.pipeline(True, shard_hint) as pipe:",
            "            while True:",
            "                try:",
            "                    if watches:",
            "                        await pipe.watch(*watches)",
            "                    func_value = func(pipe)",
            "                    if inspect.isawaitable(func_value):",
            "                        func_value = await func_value",
            "                    exec_value = await pipe.execute()",
            "                    return func_value if value_from_callable else exec_value",
            "                except WatchError:",
            "                    if watch_delay is not None and watch_delay > 0:",
            "                        await asyncio.sleep(watch_delay)",
            "                    continue",
            "",
            "    def lock(",
            "        self,",
            "        name: KeyT,",
            "        timeout: Optional[float] = None,",
            "        sleep: float = 0.1,",
            "        blocking: bool = True,",
            "        blocking_timeout: Optional[float] = None,",
            "        lock_class: Optional[Type[Lock]] = None,",
            "        thread_local: bool = True,",
            "    ) -> Lock:",
            "        \"\"\"",
            "        Return a new Lock object using key ``name`` that mimics",
            "        the behavior of threading.Lock.",
            "",
            "        If specified, ``timeout`` indicates a maximum life for the lock.",
            "        By default, it will remain locked until release() is called.",
            "",
            "        ``sleep`` indicates the amount of time to sleep per loop iteration",
            "        when the lock is in blocking mode and another client is currently",
            "        holding the lock.",
            "",
            "        ``blocking`` indicates whether calling ``acquire`` should block until",
            "        the lock has been acquired or to fail immediately, causing ``acquire``",
            "        to return False and the lock not being acquired. Defaults to True.",
            "        Note this value can be overridden by passing a ``blocking``",
            "        argument to ``acquire``.",
            "",
            "        ``blocking_timeout`` indicates the maximum amount of time in seconds to",
            "        spend trying to acquire the lock. A value of ``None`` indicates",
            "        continue trying forever. ``blocking_timeout`` can be specified as a",
            "        float or integer, both representing the number of seconds to wait.",
            "",
            "        ``lock_class`` forces the specified lock implementation. Note that as",
            "        of redis-py 3.0, the only lock class we implement is ``Lock`` (which is",
            "        a Lua-based lock). So, it's unlikely you'll need this parameter, unless",
            "        you have created your own custom lock class.",
            "",
            "        ``thread_local`` indicates whether the lock token is placed in",
            "        thread-local storage. By default, the token is placed in thread local",
            "        storage so that a thread only sees its token, not a token set by",
            "        another thread. Consider the following timeline:",
            "",
            "            time: 0, thread-1 acquires `my-lock`, with a timeout of 5 seconds.",
            "                     thread-1 sets the token to \"abc\"",
            "            time: 1, thread-2 blocks trying to acquire `my-lock` using the",
            "                     Lock instance.",
            "            time: 5, thread-1 has not yet completed. redis expires the lock",
            "                     key.",
            "            time: 5, thread-2 acquired `my-lock` now that it's available.",
            "                     thread-2 sets the token to \"xyz\"",
            "            time: 6, thread-1 finishes its work and calls release(). if the",
            "                     token is *not* stored in thread local storage, then",
            "                     thread-1 would see the token value as \"xyz\" and would be",
            "                     able to successfully release the thread-2's lock.",
            "",
            "        In some use cases it's necessary to disable thread local storage. For",
            "        example, if you have code where one thread acquires a lock and passes",
            "        that lock instance to a worker thread to release later. If thread",
            "        local storage isn't disabled in this case, the worker thread won't see",
            "        the token set by the thread that acquired the lock. Our assumption",
            "        is that these cases aren't common and as such default to using",
            "        thread local storage.\"\"\"",
            "        if lock_class is None:",
            "            lock_class = Lock",
            "        return lock_class(",
            "            self,",
            "            name,",
            "            timeout=timeout,",
            "            sleep=sleep,",
            "            blocking=blocking,",
            "            blocking_timeout=blocking_timeout,",
            "            thread_local=thread_local,",
            "        )",
            "",
            "    def pubsub(self, **kwargs) -> \"PubSub\":",
            "        \"\"\"",
            "        Return a Publish/Subscribe object. With this object, you can",
            "        subscribe to channels and listen for messages that get published to",
            "        them.",
            "        \"\"\"",
            "        return PubSub(self.connection_pool, **kwargs)",
            "",
            "    def monitor(self) -> \"Monitor\":",
            "        return Monitor(self.connection_pool)",
            "",
            "    def client(self) -> \"Redis\":",
            "        return self.__class__(",
            "            connection_pool=self.connection_pool, single_connection_client=True",
            "        )",
            "",
            "    async def __aenter__(self: _RedisT) -> _RedisT:",
            "        return await self.initialize()",
            "",
            "    async def __aexit__(self, exc_type, exc_value, traceback):",
            "        await self.close()",
            "",
            "    _DEL_MESSAGE = \"Unclosed Redis client\"",
            "",
            "    def __del__(self, _warnings: Any = warnings) -> None:",
            "        if self.connection is not None:",
            "            _warnings.warn(",
            "                f\"Unclosed client session {self!r}\", ResourceWarning, source=self",
            "            )",
            "            context = {\"client\": self, \"message\": self._DEL_MESSAGE}",
            "            asyncio.get_running_loop().call_exception_handler(context)",
            "",
            "    async def close(self, close_connection_pool: Optional[bool] = None) -> None:",
            "        \"\"\"",
            "        Closes Redis client connection",
            "",
            "        :param close_connection_pool: decides whether to close the connection pool used",
            "        by this Redis client, overriding Redis.auto_close_connection_pool. By default,",
            "        let Redis.auto_close_connection_pool decide whether to close the connection",
            "        pool.",
            "        \"\"\"",
            "        conn = self.connection",
            "        if conn:",
            "            self.connection = None",
            "            await self.connection_pool.release(conn)",
            "        if close_connection_pool or (",
            "            close_connection_pool is None and self.auto_close_connection_pool",
            "        ):",
            "            await self.connection_pool.disconnect()",
            "",
            "    async def _send_command_parse_response(self, conn, command_name, *args, **options):",
            "        \"\"\"",
            "        Send a command and parse the response",
            "        \"\"\"",
            "        await conn.send_command(*args)",
            "        return await self.parse_response(conn, command_name, **options)",
            "",
            "    async def _disconnect_raise(self, conn: Connection, error: Exception):",
            "        \"\"\"",
            "        Close the connection and raise an exception",
            "        if retry_on_error is not set or the error",
            "        is not one of the specified error types",
            "        \"\"\"",
            "        await conn.disconnect()",
            "        if (",
            "            conn.retry_on_error is None",
            "            or isinstance(error, tuple(conn.retry_on_error)) is False",
            "        ):",
            "            raise error",
            "",
            "    # COMMAND EXECUTION AND PROTOCOL PARSING",
            "    async def execute_command(self, *args, **options):",
            "        \"\"\"Execute a command and return a parsed response\"\"\"",
            "        await self.initialize()",
            "        pool = self.connection_pool",
            "        command_name = args[0]",
            "        conn = self.connection or await pool.get_connection(command_name, **options)",
            "",
            "        try:",
            "            return await conn.retry.call_with_retry(",
            "                lambda: self._send_command_parse_response(",
            "                    conn, command_name, *args, **options",
            "                ),",
            "                lambda error: self._disconnect_raise(conn, error),",
            "            )",
            "        finally:",
            "            if not self.connection:",
            "                await pool.release(conn)",
            "",
            "    async def parse_response(",
            "        self, connection: Connection, command_name: Union[str, bytes], **options",
            "    ):",
            "        \"\"\"Parses a response from the Redis server\"\"\"",
            "        try:",
            "            if NEVER_DECODE in options:",
            "                response = await connection.read_response(disable_decoding=True)",
            "                options.pop(NEVER_DECODE)",
            "            else:",
            "                response = await connection.read_response()",
            "        except ResponseError:",
            "            if EMPTY_RESPONSE in options:",
            "                return options[EMPTY_RESPONSE]",
            "            raise",
            "",
            "        if EMPTY_RESPONSE in options:",
            "            options.pop(EMPTY_RESPONSE)",
            "",
            "        if command_name in self.response_callbacks:",
            "            # Mypy bug: https://github.com/python/mypy/issues/10977",
            "            command_name = cast(str, command_name)",
            "            retval = self.response_callbacks[command_name](response, **options)",
            "            return await retval if inspect.isawaitable(retval) else retval",
            "        return response",
            "",
            "",
            "StrictRedis = Redis",
            "",
            "",
            "class MonitorCommandInfo(TypedDict):",
            "    time: float",
            "    db: int",
            "    client_address: str",
            "    client_port: str",
            "    client_type: str",
            "    command: str",
            "",
            "",
            "class Monitor:",
            "    \"\"\"",
            "    Monitor is useful for handling the MONITOR command to the redis server.",
            "    next_command() method returns one command from monitor",
            "    listen() method yields commands from monitor.",
            "    \"\"\"",
            "",
            "    monitor_re = re.compile(r\"\\[(\\d+) (.*)\\] (.*)\")",
            "    command_re = re.compile(r'\"(.*?)(?<!\\\\)\"')",
            "",
            "    def __init__(self, connection_pool: ConnectionPool):",
            "        self.connection_pool = connection_pool",
            "        self.connection: Optional[Connection] = None",
            "",
            "    async def connect(self):",
            "        if self.connection is None:",
            "            self.connection = await self.connection_pool.get_connection(\"MONITOR\")",
            "",
            "    async def __aenter__(self):",
            "        await self.connect()",
            "        await self.connection.send_command(\"MONITOR\")",
            "        # check that monitor returns 'OK', but don't return it to user",
            "        response = await self.connection.read_response()",
            "        if not bool_ok(response):",
            "            raise RedisError(f\"MONITOR failed: {response}\")",
            "        return self",
            "",
            "    async def __aexit__(self, *args):",
            "        await self.connection.disconnect()",
            "        await self.connection_pool.release(self.connection)",
            "",
            "    async def next_command(self) -> MonitorCommandInfo:",
            "        \"\"\"Parse the response from a monitor command\"\"\"",
            "        await self.connect()",
            "        response = await self.connection.read_response()",
            "        if isinstance(response, bytes):",
            "            response = self.connection.encoder.decode(response, force=True)",
            "        command_time, command_data = response.split(\" \", 1)",
            "        m = self.monitor_re.match(command_data)",
            "        db_id, client_info, command = m.groups()",
            "        command = \" \".join(self.command_re.findall(command))",
            "        # Redis escapes double quotes because each piece of the command",
            "        # string is surrounded by double quotes. We don't have that",
            "        # requirement so remove the escaping and leave the quote.",
            "        command = command.replace('\\\\\"', '\"')",
            "",
            "        if client_info == \"lua\":",
            "            client_address = \"lua\"",
            "            client_port = \"\"",
            "            client_type = \"lua\"",
            "        elif client_info.startswith(\"unix\"):",
            "            client_address = \"unix\"",
            "            client_port = client_info[5:]",
            "            client_type = \"unix\"",
            "        else:",
            "            # use rsplit as ipv6 addresses contain colons",
            "            client_address, client_port = client_info.rsplit(\":\", 1)",
            "            client_type = \"tcp\"",
            "        return {",
            "            \"time\": float(command_time),",
            "            \"db\": int(db_id),",
            "            \"client_address\": client_address,",
            "            \"client_port\": client_port,",
            "            \"client_type\": client_type,",
            "            \"command\": command,",
            "        }",
            "",
            "    async def listen(self) -> AsyncIterator[MonitorCommandInfo]:",
            "        \"\"\"Listen for commands coming to the server.\"\"\"",
            "        while True:",
            "            yield await self.next_command()",
            "",
            "",
            "class PubSub:",
            "    \"\"\"",
            "    PubSub provides publish, subscribe and listen support to Redis channels.",
            "",
            "    After subscribing to one or more channels, the listen() method will block",
            "    until a message arrives on one of the subscribed channels. That message",
            "    will be returned and it's safe to start listening again.",
            "    \"\"\"",
            "",
            "    PUBLISH_MESSAGE_TYPES = (\"message\", \"pmessage\")",
            "    UNSUBSCRIBE_MESSAGE_TYPES = (\"unsubscribe\", \"punsubscribe\")",
            "    HEALTH_CHECK_MESSAGE = \"redis-py-health-check\"",
            "",
            "    def __init__(",
            "        self,",
            "        connection_pool: ConnectionPool,",
            "        shard_hint: Optional[str] = None,",
            "        ignore_subscribe_messages: bool = False,",
            "        encoder=None,",
            "    ):",
            "        self.connection_pool = connection_pool",
            "        self.shard_hint = shard_hint",
            "        self.ignore_subscribe_messages = ignore_subscribe_messages",
            "        self.connection = None",
            "        # we need to know the encoding options for this connection in order",
            "        # to lookup channel and pattern names for callback handlers.",
            "        self.encoder = encoder",
            "        if self.encoder is None:",
            "            self.encoder = self.connection_pool.get_encoder()",
            "        if self.encoder.decode_responses:",
            "            self.health_check_response: Iterable[Union[str, bytes]] = [",
            "                \"pong\",",
            "                self.HEALTH_CHECK_MESSAGE,",
            "            ]",
            "        else:",
            "            self.health_check_response = [",
            "                b\"pong\",",
            "                self.encoder.encode(self.HEALTH_CHECK_MESSAGE),",
            "            ]",
            "        self.channels = {}",
            "        self.pending_unsubscribe_channels = set()",
            "        self.patterns = {}",
            "        self.pending_unsubscribe_patterns = set()",
            "        self._lock = asyncio.Lock()",
            "",
            "    async def __aenter__(self):",
            "        return self",
            "",
            "    async def __aexit__(self, exc_type, exc_value, traceback):",
            "        await self.reset()",
            "",
            "    def __del__(self):",
            "        if self.connection:",
            "            self.connection.clear_connect_callbacks()",
            "",
            "    async def reset(self):",
            "        async with self._lock:",
            "            if self.connection:",
            "                await self.connection.disconnect()",
            "                self.connection.clear_connect_callbacks()",
            "                await self.connection_pool.release(self.connection)",
            "                self.connection = None",
            "            self.channels = {}",
            "            self.pending_unsubscribe_channels = set()",
            "            self.patterns = {}",
            "            self.pending_unsubscribe_patterns = set()",
            "",
            "    def close(self) -> Awaitable[NoReturn]:",
            "        return self.reset()",
            "",
            "    async def on_connect(self, connection: Connection):",
            "        \"\"\"Re-subscribe to any channels and patterns previously subscribed to\"\"\"",
            "        # NOTE: for python3, we can't pass bytestrings as keyword arguments",
            "        # so we need to decode channel/pattern names back to unicode strings",
            "        # before passing them to [p]subscribe.",
            "        self.pending_unsubscribe_channels.clear()",
            "        self.pending_unsubscribe_patterns.clear()",
            "        if self.channels:",
            "            channels = {}",
            "            for k, v in self.channels.items():",
            "                channels[self.encoder.decode(k, force=True)] = v",
            "            await self.subscribe(**channels)",
            "        if self.patterns:",
            "            patterns = {}",
            "            for k, v in self.patterns.items():",
            "                patterns[self.encoder.decode(k, force=True)] = v",
            "            await self.psubscribe(**patterns)",
            "",
            "    @property",
            "    def subscribed(self):",
            "        \"\"\"Indicates if there are subscriptions to any channels or patterns\"\"\"",
            "        return bool(self.channels or self.patterns)",
            "",
            "    async def execute_command(self, *args: EncodableT):",
            "        \"\"\"Execute a publish/subscribe command\"\"\"",
            "",
            "        # NOTE: don't parse the response in this function -- it could pull a",
            "        # legitimate message off the stack if the connection is already",
            "        # subscribed to one or more channels",
            "",
            "        await self.connect()",
            "        connection = self.connection",
            "        kwargs = {\"check_health\": not self.subscribed}",
            "        await self._execute(connection, connection.send_command, *args, **kwargs)",
            "",
            "    async def connect(self):",
            "        \"\"\"",
            "        Ensure that the PubSub is connected",
            "        \"\"\"",
            "        if self.connection is None:",
            "            self.connection = await self.connection_pool.get_connection(",
            "                \"pubsub\", self.shard_hint",
            "            )",
            "            # register a callback that re-subscribes to any channels we",
            "            # were listening to when we were disconnected",
            "            self.connection.register_connect_callback(self.on_connect)",
            "        else:",
            "            await self.connection.connect()",
            "",
            "    async def _disconnect_raise_connect(self, conn, error):",
            "        \"\"\"",
            "        Close the connection and raise an exception",
            "        if retry_on_timeout is not set or the error",
            "        is not a TimeoutError. Otherwise, try to reconnect",
            "        \"\"\"",
            "        await conn.disconnect()",
            "        if not (conn.retry_on_timeout and isinstance(error, TimeoutError)):",
            "            raise error",
            "        await conn.connect()",
            "",
            "    async def _execute(self, conn, command, *args, **kwargs):",
            "        \"\"\"",
            "        Connect manually upon disconnection. If the Redis server is down,",
            "        this will fail and raise a ConnectionError as desired.",
            "        After reconnection, the ``on_connect`` callback should have been",
            "        called by the # connection to resubscribe us to any channels and",
            "        patterns we were previously listening to",
            "        \"\"\"",
            "        return await conn.retry.call_with_retry(",
            "            lambda: command(*args, **kwargs),",
            "            lambda error: self._disconnect_raise_connect(conn, error),",
            "        )",
            "",
            "    async def parse_response(self, block: bool = True, timeout: float = 0):",
            "        \"\"\"Parse the response from a publish/subscribe command\"\"\"",
            "        conn = self.connection",
            "        if conn is None:",
            "            raise RuntimeError(",
            "                \"pubsub connection not set: \"",
            "                \"did you forget to call subscribe() or psubscribe()?\"",
            "            )",
            "",
            "        await self.check_health()",
            "",
            "        if not conn.is_connected:",
            "            await conn.connect()",
            "",
            "        read_timeout = None if block else timeout",
            "        response = await self._execute(conn, conn.read_response, timeout=read_timeout)",
            "",
            "        if conn.health_check_interval and response == self.health_check_response:",
            "            # ignore the health check message as user might not expect it",
            "            return None",
            "        return response",
            "",
            "    async def check_health(self):",
            "        conn = self.connection",
            "        if conn is None:",
            "            raise RuntimeError(",
            "                \"pubsub connection not set: \"",
            "                \"did you forget to call subscribe() or psubscribe()?\"",
            "            )",
            "",
            "        if (",
            "            conn.health_check_interval",
            "            and asyncio.get_running_loop().time() > conn.next_health_check",
            "        ):",
            "            await conn.send_command(",
            "                \"PING\", self.HEALTH_CHECK_MESSAGE, check_health=False",
            "            )",
            "",
            "    def _normalize_keys(self, data: _NormalizeKeysT) -> _NormalizeKeysT:",
            "        \"\"\"",
            "        normalize channel/pattern names to be either bytes or strings",
            "        based on whether responses are automatically decoded. this saves us",
            "        from coercing the value for each message coming in.",
            "        \"\"\"",
            "        encode = self.encoder.encode",
            "        decode = self.encoder.decode",
            "        return {decode(encode(k)): v for k, v in data.items()}  # type: ignore[return-value]  # noqa: E501",
            "",
            "    async def psubscribe(self, *args: ChannelT, **kwargs: PubSubHandler):",
            "        \"\"\"",
            "        Subscribe to channel patterns. Patterns supplied as keyword arguments",
            "        expect a pattern name as the key and a callable as the value. A",
            "        pattern's callable will be invoked automatically when a message is",
            "        received on that pattern rather than producing a message via",
            "        ``listen()``.",
            "        \"\"\"",
            "        parsed_args = list_or_args((args[0],), args[1:]) if args else args",
            "        new_patterns: Dict[ChannelT, PubSubHandler] = dict.fromkeys(parsed_args)",
            "        # Mypy bug: https://github.com/python/mypy/issues/10970",
            "        new_patterns.update(kwargs)  # type: ignore[arg-type]",
            "        ret_val = await self.execute_command(\"PSUBSCRIBE\", *new_patterns.keys())",
            "        # update the patterns dict AFTER we send the command. we don't want to",
            "        # subscribe twice to these patterns, once for the command and again",
            "        # for the reconnection.",
            "        new_patterns = self._normalize_keys(new_patterns)",
            "        self.patterns.update(new_patterns)",
            "        self.pending_unsubscribe_patterns.difference_update(new_patterns)",
            "        return ret_val",
            "",
            "    def punsubscribe(self, *args: ChannelT) -> Awaitable:",
            "        \"\"\"",
            "        Unsubscribe from the supplied patterns. If empty, unsubscribe from",
            "        all patterns.",
            "        \"\"\"",
            "        patterns: Iterable[ChannelT]",
            "        if args:",
            "            parsed_args = list_or_args((args[0],), args[1:])",
            "            patterns = self._normalize_keys(dict.fromkeys(parsed_args)).keys()",
            "        else:",
            "            parsed_args = []",
            "            patterns = self.patterns",
            "        self.pending_unsubscribe_patterns.update(patterns)",
            "        return self.execute_command(\"PUNSUBSCRIBE\", *parsed_args)",
            "",
            "    async def subscribe(self, *args: ChannelT, **kwargs: Callable):",
            "        \"\"\"",
            "        Subscribe to channels. Channels supplied as keyword arguments expect",
            "        a channel name as the key and a callable as the value. A channel's",
            "        callable will be invoked automatically when a message is received on",
            "        that channel rather than producing a message via ``listen()`` or",
            "        ``get_message()``.",
            "        \"\"\"",
            "        parsed_args = list_or_args((args[0],), args[1:]) if args else ()",
            "        new_channels = dict.fromkeys(parsed_args)",
            "        # Mypy bug: https://github.com/python/mypy/issues/10970",
            "        new_channels.update(kwargs)  # type: ignore[arg-type]",
            "        ret_val = await self.execute_command(\"SUBSCRIBE\", *new_channels.keys())",
            "        # update the channels dict AFTER we send the command. we don't want to",
            "        # subscribe twice to these channels, once for the command and again",
            "        # for the reconnection.",
            "        new_channels = self._normalize_keys(new_channels)",
            "        self.channels.update(new_channels)",
            "        self.pending_unsubscribe_channels.difference_update(new_channels)",
            "        return ret_val",
            "",
            "    def unsubscribe(self, *args) -> Awaitable:",
            "        \"\"\"",
            "        Unsubscribe from the supplied channels. If empty, unsubscribe from",
            "        all channels",
            "        \"\"\"",
            "        if args:",
            "            parsed_args = list_or_args(args[0], args[1:])",
            "            channels = self._normalize_keys(dict.fromkeys(parsed_args))",
            "        else:",
            "            parsed_args = []",
            "            channels = self.channels",
            "        self.pending_unsubscribe_channels.update(channels)",
            "        return self.execute_command(\"UNSUBSCRIBE\", *parsed_args)",
            "",
            "    async def listen(self) -> AsyncIterator:",
            "        \"\"\"Listen for messages on channels this client has been subscribed to\"\"\"",
            "        while self.subscribed:",
            "            response = await self.handle_message(await self.parse_response(block=True))",
            "            if response is not None:",
            "                yield response",
            "",
            "    async def get_message(",
            "        self, ignore_subscribe_messages: bool = False, timeout: Optional[float] = 0.0",
            "    ):",
            "        \"\"\"",
            "        Get the next message if one is available, otherwise None.",
            "",
            "        If timeout is specified, the system will wait for `timeout` seconds",
            "        before returning. Timeout should be specified as a floating point",
            "        number or None to wait indefinitely.",
            "        \"\"\"",
            "        response = await self.parse_response(block=(timeout is None), timeout=timeout)",
            "        if response:",
            "            return await self.handle_message(response, ignore_subscribe_messages)",
            "        return None",
            "",
            "    def ping(self, message=None) -> Awaitable:",
            "        \"\"\"",
            "        Ping the Redis server",
            "        \"\"\"",
            "        message = \"\" if message is None else message",
            "        return self.execute_command(\"PING\", message)",
            "",
            "    async def handle_message(self, response, ignore_subscribe_messages=False):",
            "        \"\"\"",
            "        Parses a pub/sub message. If the channel or pattern was subscribed to",
            "        with a message handler, the handler is invoked instead of a parsed",
            "        message being returned.",
            "        \"\"\"",
            "        message_type = str_if_bytes(response[0])",
            "        if message_type == \"pmessage\":",
            "            message = {",
            "                \"type\": message_type,",
            "                \"pattern\": response[1],",
            "                \"channel\": response[2],",
            "                \"data\": response[3],",
            "            }",
            "        elif message_type == \"pong\":",
            "            message = {",
            "                \"type\": message_type,",
            "                \"pattern\": None,",
            "                \"channel\": None,",
            "                \"data\": response[1],",
            "            }",
            "        else:",
            "            message = {",
            "                \"type\": message_type,",
            "                \"pattern\": None,",
            "                \"channel\": response[1],",
            "                \"data\": response[2],",
            "            }",
            "",
            "        # if this is an unsubscribe message, remove it from memory",
            "        if message_type in self.UNSUBSCRIBE_MESSAGE_TYPES:",
            "            if message_type == \"punsubscribe\":",
            "                pattern = response[1]",
            "                if pattern in self.pending_unsubscribe_patterns:",
            "                    self.pending_unsubscribe_patterns.remove(pattern)",
            "                    self.patterns.pop(pattern, None)",
            "            else:",
            "                channel = response[1]",
            "                if channel in self.pending_unsubscribe_channels:",
            "                    self.pending_unsubscribe_channels.remove(channel)",
            "                    self.channels.pop(channel, None)",
            "",
            "        if message_type in self.PUBLISH_MESSAGE_TYPES:",
            "            # if there's a message handler, invoke it",
            "            if message_type == \"pmessage\":",
            "                handler = self.patterns.get(message[\"pattern\"], None)",
            "            else:",
            "                handler = self.channels.get(message[\"channel\"], None)",
            "            if handler:",
            "                if inspect.iscoroutinefunction(handler):",
            "                    await handler(message)",
            "                else:",
            "                    handler(message)",
            "                return None",
            "        elif message_type != \"pong\":",
            "            # this is a subscribe/unsubscribe message. ignore if we don't",
            "            # want them",
            "            if ignore_subscribe_messages or self.ignore_subscribe_messages:",
            "                return None",
            "",
            "        return message",
            "",
            "    async def run(",
            "        self,",
            "        *,",
            "        exception_handler: Optional[\"PSWorkerThreadExcHandlerT\"] = None,",
            "        poll_timeout: float = 1.0,",
            "    ) -> None:",
            "        \"\"\"Process pub/sub messages using registered callbacks.",
            "",
            "        This is the equivalent of :py:meth:`redis.PubSub.run_in_thread` in",
            "        redis-py, but it is a coroutine. To launch it as a separate task, use",
            "        ``asyncio.create_task``:",
            "",
            "            >>> task = asyncio.create_task(pubsub.run())",
            "",
            "        To shut it down, use asyncio cancellation:",
            "",
            "            >>> task.cancel()",
            "            >>> await task",
            "        \"\"\"",
            "        for channel, handler in self.channels.items():",
            "            if handler is None:",
            "                raise PubSubError(f\"Channel: '{channel}' has no handler registered\")",
            "        for pattern, handler in self.patterns.items():",
            "            if handler is None:",
            "                raise PubSubError(f\"Pattern: '{pattern}' has no handler registered\")",
            "",
            "        await self.connect()",
            "        while True:",
            "            try:",
            "                await self.get_message(",
            "                    ignore_subscribe_messages=True, timeout=poll_timeout",
            "                )",
            "            except asyncio.CancelledError:",
            "                raise",
            "            except BaseException as e:",
            "                if exception_handler is None:",
            "                    raise",
            "                res = exception_handler(e, self)",
            "                if inspect.isawaitable(res):",
            "                    await res",
            "            # Ensure that other tasks on the event loop get a chance to run",
            "            # if we didn't have to block for I/O anywhere.",
            "            await asyncio.sleep(0)",
            "",
            "",
            "class PubsubWorkerExceptionHandler(Protocol):",
            "    def __call__(self, e: BaseException, pubsub: PubSub):",
            "        ...",
            "",
            "",
            "class AsyncPubsubWorkerExceptionHandler(Protocol):",
            "    async def __call__(self, e: BaseException, pubsub: PubSub):",
            "        ...",
            "",
            "",
            "PSWorkerThreadExcHandlerT = Union[",
            "    PubsubWorkerExceptionHandler, AsyncPubsubWorkerExceptionHandler",
            "]",
            "",
            "",
            "CommandT = Tuple[Tuple[Union[str, bytes], ...], Mapping[str, Any]]",
            "CommandStackT = List[CommandT]",
            "",
            "",
            "class Pipeline(Redis):  # lgtm [py/init-calls-subclass]",
            "    \"\"\"",
            "    Pipelines provide a way to transmit multiple commands to the Redis server",
            "    in one transmission.  This is convenient for batch processing, such as",
            "    saving all the values in a list to Redis.",
            "",
            "    All commands executed within a pipeline are wrapped with MULTI and EXEC",
            "    calls. This guarantees all commands executed in the pipeline will be",
            "    executed atomically.",
            "",
            "    Any command raising an exception does *not* halt the execution of",
            "    subsequent commands in the pipeline. Instead, the exception is caught",
            "    and its instance is placed into the response list returned by execute().",
            "    Code iterating over the response list should be able to deal with an",
            "    instance of an exception as a potential value. In general, these will be",
            "    ResponseError exceptions, such as those raised when issuing a command",
            "    on a key of a different datatype.",
            "    \"\"\"",
            "",
            "    UNWATCH_COMMANDS = {\"DISCARD\", \"EXEC\", \"UNWATCH\"}",
            "",
            "    def __init__(",
            "        self,",
            "        connection_pool: ConnectionPool,",
            "        response_callbacks: MutableMapping[Union[str, bytes], ResponseCallbackT],",
            "        transaction: bool,",
            "        shard_hint: Optional[str],",
            "    ):",
            "        self.connection_pool = connection_pool",
            "        self.connection = None",
            "        self.response_callbacks = response_callbacks",
            "        self.is_transaction = transaction",
            "        self.shard_hint = shard_hint",
            "        self.watching = False",
            "        self.command_stack: CommandStackT = []",
            "        self.scripts: Set[\"Script\"] = set()",
            "        self.explicit_transaction = False",
            "",
            "    async def __aenter__(self: _RedisT) -> _RedisT:",
            "        return self",
            "",
            "    async def __aexit__(self, exc_type, exc_value, traceback):",
            "        await self.reset()",
            "",
            "    def __await__(self):",
            "        return self._async_self().__await__()",
            "",
            "    _DEL_MESSAGE = \"Unclosed Pipeline client\"",
            "",
            "    def __len__(self):",
            "        return len(self.command_stack)",
            "",
            "    def __bool__(self):",
            "        \"\"\"Pipeline instances should always evaluate to True\"\"\"",
            "        return True",
            "",
            "    async def _async_self(self):",
            "        return self",
            "",
            "    async def reset(self):",
            "        self.command_stack = []",
            "        self.scripts = set()",
            "        # make sure to reset the connection state in the event that we were",
            "        # watching something",
            "        if self.watching and self.connection:",
            "            try:",
            "                # call this manually since our unwatch or",
            "                # immediate_execute_command methods can call reset()",
            "                await self.connection.send_command(\"UNWATCH\")",
            "                await self.connection.read_response()",
            "            except ConnectionError:",
            "                # disconnect will also remove any previous WATCHes",
            "                if self.connection:",
            "                    await self.connection.disconnect()",
            "        # clean up the other instance attributes",
            "        self.watching = False",
            "        self.explicit_transaction = False",
            "        # we can safely return the connection to the pool here since we're",
            "        # sure we're no longer WATCHing anything",
            "        if self.connection:",
            "            await self.connection_pool.release(self.connection)",
            "            self.connection = None",
            "",
            "    def multi(self):",
            "        \"\"\"",
            "        Start a transactional block of the pipeline after WATCH commands",
            "        are issued. End the transactional block with `execute`.",
            "        \"\"\"",
            "        if self.explicit_transaction:",
            "            raise RedisError(\"Cannot issue nested calls to MULTI\")",
            "        if self.command_stack:",
            "            raise RedisError(",
            "                \"Commands without an initial WATCH have already been issued\"",
            "            )",
            "        self.explicit_transaction = True",
            "",
            "    def execute_command(",
            "        self, *args, **kwargs",
            "    ) -> Union[\"Pipeline\", Awaitable[\"Pipeline\"]]:",
            "        if (self.watching or args[0] == \"WATCH\") and not self.explicit_transaction:",
            "            return self.immediate_execute_command(*args, **kwargs)",
            "        return self.pipeline_execute_command(*args, **kwargs)",
            "",
            "    async def _disconnect_reset_raise(self, conn, error):",
            "        \"\"\"",
            "        Close the connection, reset watching state and",
            "        raise an exception if we were watching,",
            "        retry_on_timeout is not set,",
            "        or the error is not a TimeoutError",
            "        \"\"\"",
            "        await conn.disconnect()",
            "        # if we were already watching a variable, the watch is no longer",
            "        # valid since this connection has died. raise a WatchError, which",
            "        # indicates the user should retry this transaction.",
            "        if self.watching:",
            "            await self.reset()",
            "            raise WatchError(",
            "                \"A ConnectionError occurred on while watching one or more keys\"",
            "            )",
            "        # if retry_on_timeout is not set, or the error is not",
            "        # a TimeoutError, raise it",
            "        if not (conn.retry_on_timeout and isinstance(error, TimeoutError)):",
            "            await self.reset()",
            "            raise",
            "",
            "    async def immediate_execute_command(self, *args, **options):",
            "        \"\"\"",
            "        Execute a command immediately, but don't auto-retry on a",
            "        ConnectionError if we're already WATCHing a variable. Used when",
            "        issuing WATCH or subsequent commands retrieving their values but before",
            "        MULTI is called.",
            "        \"\"\"",
            "        command_name = args[0]",
            "        conn = self.connection",
            "        # if this is the first call, we need a connection",
            "        if not conn:",
            "            conn = await self.connection_pool.get_connection(",
            "                command_name, self.shard_hint",
            "            )",
            "            self.connection = conn",
            "",
            "        return await conn.retry.call_with_retry(",
            "            lambda: self._send_command_parse_response(",
            "                conn, command_name, *args, **options",
            "            ),",
            "            lambda error: self._disconnect_reset_raise(conn, error),",
            "        )",
            "",
            "    def pipeline_execute_command(self, *args, **options):",
            "        \"\"\"",
            "        Stage a command to be executed when execute() is next called",
            "",
            "        Returns the current Pipeline object back so commands can be",
            "        chained together, such as:",
            "",
            "        pipe = pipe.set('foo', 'bar').incr('baz').decr('bang')",
            "",
            "        At some other point, you can then run: pipe.execute(),",
            "        which will execute all commands queued in the pipe.",
            "        \"\"\"",
            "        self.command_stack.append((args, options))",
            "        return self",
            "",
            "    async def _execute_transaction(  # noqa: C901",
            "        self, connection: Connection, commands: CommandStackT, raise_on_error",
            "    ):",
            "        pre: CommandT = ((\"MULTI\",), {})",
            "        post: CommandT = ((\"EXEC\",), {})",
            "        cmds = (pre, *commands, post)",
            "        all_cmds = connection.pack_commands(",
            "            args for args, options in cmds if EMPTY_RESPONSE not in options",
            "        )",
            "        await connection.send_packed_command(all_cmds)",
            "        errors = []",
            "",
            "        # parse off the response for MULTI",
            "        # NOTE: we need to handle ResponseErrors here and continue",
            "        # so that we read all the additional command messages from",
            "        # the socket",
            "        try:",
            "            await self.parse_response(connection, \"_\")",
            "        except ResponseError as err:",
            "            errors.append((0, err))",
            "",
            "        # and all the other commands",
            "        for i, command in enumerate(commands):",
            "            if EMPTY_RESPONSE in command[1]:",
            "                errors.append((i, command[1][EMPTY_RESPONSE]))",
            "            else:",
            "                try:",
            "                    await self.parse_response(connection, \"_\")",
            "                except ResponseError as err:",
            "                    self.annotate_exception(err, i + 1, command[0])",
            "                    errors.append((i, err))",
            "",
            "        # parse the EXEC.",
            "        try:",
            "            response = await self.parse_response(connection, \"_\")",
            "        except ExecAbortError as err:",
            "            if errors:",
            "                raise errors[0][1] from err",
            "            raise",
            "",
            "        # EXEC clears any watched keys",
            "        self.watching = False",
            "",
            "        if response is None:",
            "            raise WatchError(\"Watched variable changed.\") from None",
            "",
            "        # put any parse errors into the response",
            "        for i, e in errors:",
            "            response.insert(i, e)",
            "",
            "        if len(response) != len(commands):",
            "            if self.connection:",
            "                await self.connection.disconnect()",
            "            raise ResponseError(",
            "                \"Wrong number of response items from pipeline execution\"",
            "            ) from None",
            "",
            "        # find any errors in the response and raise if necessary",
            "        if raise_on_error:",
            "            self.raise_first_error(commands, response)",
            "",
            "        # We have to run response callbacks manually",
            "        data = []",
            "        for r, cmd in zip(response, commands):",
            "            if not isinstance(r, Exception):",
            "                args, options = cmd",
            "                command_name = args[0]",
            "                if command_name in self.response_callbacks:",
            "                    r = self.response_callbacks[command_name](r, **options)",
            "                    if inspect.isawaitable(r):",
            "                        r = await r",
            "            data.append(r)",
            "        return data",
            "",
            "    async def _execute_pipeline(",
            "        self, connection: Connection, commands: CommandStackT, raise_on_error: bool",
            "    ):",
            "        # build up all commands into a single request to increase network perf",
            "        all_cmds = connection.pack_commands([args for args, _ in commands])",
            "        await connection.send_packed_command(all_cmds)",
            "",
            "        response = []",
            "        for args, options in commands:",
            "            try:",
            "                response.append(",
            "                    await self.parse_response(connection, args[0], **options)",
            "                )",
            "            except ResponseError as e:",
            "                response.append(e)",
            "",
            "        if raise_on_error:",
            "            self.raise_first_error(commands, response)",
            "        return response",
            "",
            "    def raise_first_error(self, commands: CommandStackT, response: Iterable[Any]):",
            "        for i, r in enumerate(response):",
            "            if isinstance(r, ResponseError):",
            "                self.annotate_exception(r, i + 1, commands[i][0])",
            "                raise r",
            "",
            "    def annotate_exception(",
            "        self, exception: Exception, number: int, command: Iterable[object]",
            "    ) -> None:",
            "        cmd = \" \".join(map(safe_str, command))",
            "        msg = f\"Command # {number} ({cmd}) of pipeline caused error: {exception.args}\"",
            "        exception.args = (msg,) + exception.args[1:]",
            "",
            "    async def parse_response(",
            "        self, connection: Connection, command_name: Union[str, bytes], **options",
            "    ):",
            "        result = await super().parse_response(connection, command_name, **options)",
            "        if command_name in self.UNWATCH_COMMANDS:",
            "            self.watching = False",
            "        elif command_name == \"WATCH\":",
            "            self.watching = True",
            "        return result",
            "",
            "    async def load_scripts(self):",
            "        # make sure all scripts that are about to be run on this pipeline exist",
            "        scripts = list(self.scripts)",
            "        immediate = self.immediate_execute_command",
            "        shas = [s.sha for s in scripts]",
            "        # we can't use the normal script_* methods because they would just",
            "        # get buffered in the pipeline.",
            "        exists = await immediate(\"SCRIPT EXISTS\", *shas)",
            "        if not all(exists):",
            "            for s, exist in zip(scripts, exists):",
            "                if not exist:",
            "                    s.sha = await immediate(\"SCRIPT LOAD\", s.script)",
            "",
            "    async def _disconnect_raise_reset(self, conn: Connection, error: Exception):",
            "        \"\"\"",
            "        Close the connection, raise an exception if we were watching,",
            "        and raise an exception if retry_on_timeout is not set,",
            "        or the error is not a TimeoutError",
            "        \"\"\"",
            "        await conn.disconnect()",
            "        # if we were watching a variable, the watch is no longer valid",
            "        # since this connection has died. raise a WatchError, which",
            "        # indicates the user should retry this transaction.",
            "        if self.watching:",
            "            raise WatchError(",
            "                \"A ConnectionError occurred on while watching one or more keys\"",
            "            )",
            "        # if retry_on_timeout is not set, or the error is not",
            "        # a TimeoutError, raise it",
            "        if not (conn.retry_on_timeout and isinstance(error, TimeoutError)):",
            "            await self.reset()",
            "            raise",
            "",
            "    async def execute(self, raise_on_error: bool = True):",
            "        \"\"\"Execute all the commands in the current pipeline\"\"\"",
            "        stack = self.command_stack",
            "        if not stack and not self.watching:",
            "            return []",
            "        if self.scripts:",
            "            await self.load_scripts()",
            "        if self.is_transaction or self.explicit_transaction:",
            "            execute = self._execute_transaction",
            "        else:",
            "            execute = self._execute_pipeline",
            "",
            "        conn = self.connection",
            "        if not conn:",
            "            conn = await self.connection_pool.get_connection(\"MULTI\", self.shard_hint)",
            "            # assign to self.connection so reset() releases the connection",
            "            # back to the pool after we're done",
            "            self.connection = conn",
            "        conn = cast(Connection, conn)",
            "",
            "        try:",
            "            return await asyncio.shield(",
            "                conn.retry.call_with_retry(",
            "                    lambda: execute(conn, stack, raise_on_error),",
            "                    lambda error: self._disconnect_raise_reset(conn, error),",
            "                )",
            "            )",
            "        except asyncio.CancelledError:",
            "            # not supposed to be possible, yet here we are",
            "            await conn.disconnect(nowait=True)",
            "            raise",
            "        finally:",
            "            await self.reset()",
            "",
            "    async def discard(self):",
            "        \"\"\"Flushes all previously queued commands",
            "        See: https://redis.io/commands/DISCARD",
            "        \"\"\"",
            "        await self.execute_command(\"DISCARD\")",
            "",
            "    async def watch(self, *names: KeyT):",
            "        \"\"\"Watches the values at keys ``names``\"\"\"",
            "        if self.explicit_transaction:",
            "            raise RedisError(\"Cannot issue a WATCH after a MULTI\")",
            "        return await self.execute_command(\"WATCH\", *names)",
            "",
            "    async def unwatch(self):",
            "        \"\"\"Unwatches all previously specified keys\"\"\"",
            "        return self.watching and await self.execute_command(\"UNWATCH\") or True"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1377": [
                "Pipeline"
            ],
            "1378": [
                "Pipeline"
            ],
            "1379": [
                "Pipeline"
            ]
        },
        "addLocation": []
    },
    "redis/asyncio/cluster.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1002,
                "afterPatchRowNumber": 1002,
                "PatchRowcode": "         await connection.send_packed_command(connection.pack_command(*args), False)"
            },
            "1": {
                "beforePatchRowNumber": 1003,
                "afterPatchRowNumber": 1003,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 1004,
                "afterPatchRowNumber": 1004,
                "PatchRowcode": "         # Read response"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1005,
                "PatchRowcode": "+        return await asyncio.shield("
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1006,
                "PatchRowcode": "+            self._parse_and_release(connection, args[0], **kwargs)"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1007,
                "PatchRowcode": "+        )"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1008,
                "PatchRowcode": "+"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1009,
                "PatchRowcode": "+    async def _parse_and_release(self, connection, *args, **kwargs):"
            },
            "8": {
                "beforePatchRowNumber": 1005,
                "afterPatchRowNumber": 1010,
                "PatchRowcode": "         try:"
            },
            "9": {
                "beforePatchRowNumber": 1006,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return await self.parse_response(connection, args[0], **kwargs)"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1011,
                "PatchRowcode": "+            return await self.parse_response(connection, *args, **kwargs)"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1012,
                "PatchRowcode": "+        except asyncio.CancelledError:"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1013,
                "PatchRowcode": "+            # should not be possible"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1014,
                "PatchRowcode": "+            await connection.disconnect(nowait=True)"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1015,
                "PatchRowcode": "+            raise"
            },
            "15": {
                "beforePatchRowNumber": 1007,
                "afterPatchRowNumber": 1016,
                "PatchRowcode": "         finally:"
            },
            "16": {
                "beforePatchRowNumber": 1008,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # Release connection"
            },
            "17": {
                "beforePatchRowNumber": 1009,
                "afterPatchRowNumber": 1017,
                "PatchRowcode": "             self._free.append(connection)"
            },
            "18": {
                "beforePatchRowNumber": 1010,
                "afterPatchRowNumber": 1018,
                "PatchRowcode": " "
            },
            "19": {
                "beforePatchRowNumber": 1011,
                "afterPatchRowNumber": 1019,
                "PatchRowcode": "     async def execute_pipeline(self, commands: List[\"PipelineCommand\"]) -> bool:"
            }
        },
        "frontPatchFile": [
            "import asyncio",
            "import collections",
            "import random",
            "import socket",
            "import warnings",
            "from typing import (",
            "    Any,",
            "    Deque,",
            "    Dict,",
            "    Generator,",
            "    List,",
            "    Mapping,",
            "    Optional,",
            "    Type,",
            "    TypeVar,",
            "    Union,",
            ")",
            "",
            "from redis.asyncio.client import ResponseCallbackT",
            "from redis.asyncio.connection import (",
            "    Connection,",
            "    DefaultParser,",
            "    Encoder,",
            "    SSLConnection,",
            "    parse_url,",
            ")",
            "from redis.asyncio.lock import Lock",
            "from redis.asyncio.parser import CommandsParser",
            "from redis.asyncio.retry import Retry",
            "from redis.backoff import default_backoff",
            "from redis.client import EMPTY_RESPONSE, NEVER_DECODE, AbstractRedis",
            "from redis.cluster import (",
            "    PIPELINE_BLOCKED_COMMANDS,",
            "    PRIMARY,",
            "    REPLICA,",
            "    SLOT_ID,",
            "    AbstractRedisCluster,",
            "    LoadBalancer,",
            "    block_pipeline_command,",
            "    get_node_name,",
            "    parse_cluster_slots,",
            ")",
            "from redis.commands import READ_COMMANDS, AsyncRedisClusterCommands",
            "from redis.crc import REDIS_CLUSTER_HASH_SLOTS, key_slot",
            "from redis.credentials import CredentialProvider",
            "from redis.exceptions import (",
            "    AskError,",
            "    BusyLoadingError,",
            "    ClusterCrossSlotError,",
            "    ClusterDownError,",
            "    ClusterError,",
            "    ConnectionError,",
            "    DataError,",
            "    MasterDownError,",
            "    MaxConnectionsError,",
            "    MovedError,",
            "    RedisClusterException,",
            "    ResponseError,",
            "    SlotNotCoveredError,",
            "    TimeoutError,",
            "    TryAgainError,",
            ")",
            "from redis.typing import AnyKeyT, EncodableT, KeyT",
            "from redis.utils import dict_merge, safe_str, str_if_bytes",
            "",
            "TargetNodesT = TypeVar(",
            "    \"TargetNodesT\", str, \"ClusterNode\", List[\"ClusterNode\"], Dict[Any, \"ClusterNode\"]",
            ")",
            "",
            "",
            "class ClusterParser(DefaultParser):",
            "    EXCEPTION_CLASSES = dict_merge(",
            "        DefaultParser.EXCEPTION_CLASSES,",
            "        {",
            "            \"ASK\": AskError,",
            "            \"CLUSTERDOWN\": ClusterDownError,",
            "            \"CROSSSLOT\": ClusterCrossSlotError,",
            "            \"MASTERDOWN\": MasterDownError,",
            "            \"MOVED\": MovedError,",
            "            \"TRYAGAIN\": TryAgainError,",
            "        },",
            "    )",
            "",
            "",
            "class RedisCluster(AbstractRedis, AbstractRedisCluster, AsyncRedisClusterCommands):",
            "    \"\"\"",
            "    Create a new RedisCluster client.",
            "",
            "    Pass one of parameters:",
            "",
            "      - `host` & `port`",
            "      - `startup_nodes`",
            "",
            "    | Use ``await`` :meth:`initialize` to find cluster nodes & create connections.",
            "    | Use ``await`` :meth:`close` to disconnect connections & close client.",
            "",
            "    Many commands support the target_nodes kwarg. It can be one of the",
            "    :attr:`NODE_FLAGS`:",
            "",
            "      - :attr:`PRIMARIES`",
            "      - :attr:`REPLICAS`",
            "      - :attr:`ALL_NODES`",
            "      - :attr:`RANDOM`",
            "      - :attr:`DEFAULT_NODE`",
            "",
            "    Note: This client is not thread/process/fork safe.",
            "",
            "    :param host:",
            "        | Can be used to point to a startup node",
            "    :param port:",
            "        | Port used if **host** is provided",
            "    :param startup_nodes:",
            "        | :class:`~.ClusterNode` to used as a startup node",
            "    :param require_full_coverage:",
            "        | When set to ``False``: the client will not require a full coverage of",
            "          the slots. However, if not all slots are covered, and at least one node",
            "          has ``cluster-require-full-coverage`` set to ``yes``, the server will throw",
            "          a :class:`~.ClusterDownError` for some key-based commands.",
            "        | When set to ``True``: all slots must be covered to construct the cluster",
            "          client. If not all slots are covered, :class:`~.RedisClusterException` will be",
            "          thrown.",
            "        | See:",
            "          https://redis.io/docs/manual/scaling/#redis-cluster-configuration-parameters",
            "    :param read_from_replicas:",
            "        | Enable read from replicas in READONLY mode. You can read possibly stale data.",
            "          When set to true, read commands will be assigned between the primary and",
            "          its replications in a Round-Robin manner.",
            "    :param reinitialize_steps:",
            "        | Specifies the number of MOVED errors that need to occur before reinitializing",
            "          the whole cluster topology. If a MOVED error occurs and the cluster does not",
            "          need to be reinitialized on this current error handling, only the MOVED slot",
            "          will be patched with the redirected node.",
            "          To reinitialize the cluster on every MOVED error, set reinitialize_steps to 1.",
            "          To avoid reinitializing the cluster on moved errors, set reinitialize_steps to",
            "          0.",
            "    :param cluster_error_retry_attempts:",
            "        | Number of times to retry before raising an error when :class:`~.TimeoutError`",
            "          or :class:`~.ConnectionError` or :class:`~.ClusterDownError` are encountered",
            "    :param connection_error_retry_attempts:",
            "        | Number of times to retry before reinitializing when :class:`~.TimeoutError`",
            "          or :class:`~.ConnectionError` are encountered.",
            "          The default backoff strategy will be set if Retry object is not passed (see",
            "          default_backoff in backoff.py). To change it, pass a custom Retry object",
            "          using the \"retry\" keyword.",
            "    :param max_connections:",
            "        | Maximum number of connections per node. If there are no free connections & the",
            "          maximum number of connections are already created, a",
            "          :class:`~.MaxConnectionsError` is raised. This error may be retried as defined",
            "          by :attr:`connection_error_retry_attempts`",
            "",
            "    | Rest of the arguments will be passed to the",
            "      :class:`~redis.asyncio.connection.Connection` instances when created",
            "",
            "    :raises RedisClusterException:",
            "        if any arguments are invalid or unknown. Eg:",
            "",
            "        - `db` != 0 or None",
            "        - `path` argument for unix socket connection",
            "        - none of the `host`/`port` & `startup_nodes` were provided",
            "",
            "    \"\"\"",
            "",
            "    @classmethod",
            "    def from_url(cls, url: str, **kwargs: Any) -> \"RedisCluster\":",
            "        \"\"\"",
            "        Return a Redis client object configured from the given URL.",
            "",
            "        For example::",
            "",
            "            redis://[[username]:[password]]@localhost:6379/0",
            "            rediss://[[username]:[password]]@localhost:6379/0",
            "",
            "        Three URL schemes are supported:",
            "",
            "        - `redis://` creates a TCP socket connection. See more at:",
            "          <https://www.iana.org/assignments/uri-schemes/prov/redis>",
            "        - `rediss://` creates a SSL wrapped TCP socket connection. See more at:",
            "          <https://www.iana.org/assignments/uri-schemes/prov/rediss>",
            "",
            "        The username, password, hostname, path and all querystring values are passed",
            "        through ``urllib.parse.unquote`` in order to replace any percent-encoded values",
            "        with their corresponding characters.",
            "",
            "        All querystring options are cast to their appropriate Python types. Boolean",
            "        arguments can be specified with string values \"True\"/\"False\" or \"Yes\"/\"No\".",
            "        Values that cannot be properly cast cause a ``ValueError`` to be raised. Once",
            "        parsed, the querystring arguments and keyword arguments are passed to",
            "        :class:`~redis.asyncio.connection.Connection` when created.",
            "        In the case of conflicting arguments, querystring arguments are used.",
            "        \"\"\"",
            "        kwargs.update(parse_url(url))",
            "        if kwargs.pop(\"connection_class\", None) is SSLConnection:",
            "            kwargs[\"ssl\"] = True",
            "        return cls(**kwargs)",
            "",
            "    __slots__ = (",
            "        \"_initialize\",",
            "        \"_lock\",",
            "        \"cluster_error_retry_attempts\",",
            "        \"command_flags\",",
            "        \"commands_parser\",",
            "        \"connection_error_retry_attempts\",",
            "        \"connection_kwargs\",",
            "        \"encoder\",",
            "        \"node_flags\",",
            "        \"nodes_manager\",",
            "        \"read_from_replicas\",",
            "        \"reinitialize_counter\",",
            "        \"reinitialize_steps\",",
            "        \"response_callbacks\",",
            "        \"result_callbacks\",",
            "    )",
            "",
            "    def __init__(",
            "        self,",
            "        host: Optional[str] = None,",
            "        port: Union[str, int] = 6379,",
            "        # Cluster related kwargs",
            "        startup_nodes: Optional[List[\"ClusterNode\"]] = None,",
            "        require_full_coverage: bool = True,",
            "        read_from_replicas: bool = False,",
            "        reinitialize_steps: int = 5,",
            "        cluster_error_retry_attempts: int = 3,",
            "        connection_error_retry_attempts: int = 3,",
            "        max_connections: int = 2**31,",
            "        # Client related kwargs",
            "        db: Union[str, int] = 0,",
            "        path: Optional[str] = None,",
            "        credential_provider: Optional[CredentialProvider] = None,",
            "        username: Optional[str] = None,",
            "        password: Optional[str] = None,",
            "        client_name: Optional[str] = None,",
            "        # Encoding related kwargs",
            "        encoding: str = \"utf-8\",",
            "        encoding_errors: str = \"strict\",",
            "        decode_responses: bool = False,",
            "        # Connection related kwargs",
            "        health_check_interval: float = 0,",
            "        socket_connect_timeout: Optional[float] = None,",
            "        socket_keepalive: bool = False,",
            "        socket_keepalive_options: Optional[Mapping[int, Union[int, bytes]]] = None,",
            "        socket_timeout: Optional[float] = None,",
            "        retry: Optional[\"Retry\"] = None,",
            "        retry_on_error: Optional[List[Exception]] = None,",
            "        # SSL related kwargs",
            "        ssl: bool = False,",
            "        ssl_ca_certs: Optional[str] = None,",
            "        ssl_ca_data: Optional[str] = None,",
            "        ssl_cert_reqs: str = \"required\",",
            "        ssl_certfile: Optional[str] = None,",
            "        ssl_check_hostname: bool = False,",
            "        ssl_keyfile: Optional[str] = None,",
            "    ) -> None:",
            "        if db:",
            "            raise RedisClusterException(",
            "                \"Argument 'db' must be 0 or None in cluster mode\"",
            "            )",
            "",
            "        if path:",
            "            raise RedisClusterException(",
            "                \"Unix domain socket is not supported in cluster mode\"",
            "            )",
            "",
            "        if (not host or not port) and not startup_nodes:",
            "            raise RedisClusterException(",
            "                \"RedisCluster requires at least one node to discover the cluster.\\n\"",
            "                \"Please provide one of the following or use RedisCluster.from_url:\\n\"",
            "                '   - host and port: RedisCluster(host=\"localhost\", port=6379)\\n'",
            "                \"   - startup_nodes: RedisCluster(startup_nodes=[\"",
            "                'ClusterNode(\"localhost\", 6379), ClusterNode(\"localhost\", 6380)])'",
            "            )",
            "",
            "        kwargs: Dict[str, Any] = {",
            "            \"max_connections\": max_connections,",
            "            \"connection_class\": Connection,",
            "            \"parser_class\": ClusterParser,",
            "            # Client related kwargs",
            "            \"credential_provider\": credential_provider,",
            "            \"username\": username,",
            "            \"password\": password,",
            "            \"client_name\": client_name,",
            "            # Encoding related kwargs",
            "            \"encoding\": encoding,",
            "            \"encoding_errors\": encoding_errors,",
            "            \"decode_responses\": decode_responses,",
            "            # Connection related kwargs",
            "            \"health_check_interval\": health_check_interval,",
            "            \"socket_connect_timeout\": socket_connect_timeout,",
            "            \"socket_keepalive\": socket_keepalive,",
            "            \"socket_keepalive_options\": socket_keepalive_options,",
            "            \"socket_timeout\": socket_timeout,",
            "            \"retry\": retry,",
            "        }",
            "",
            "        if ssl:",
            "            # SSL related kwargs",
            "            kwargs.update(",
            "                {",
            "                    \"connection_class\": SSLConnection,",
            "                    \"ssl_ca_certs\": ssl_ca_certs,",
            "                    \"ssl_ca_data\": ssl_ca_data,",
            "                    \"ssl_cert_reqs\": ssl_cert_reqs,",
            "                    \"ssl_certfile\": ssl_certfile,",
            "                    \"ssl_check_hostname\": ssl_check_hostname,",
            "                    \"ssl_keyfile\": ssl_keyfile,",
            "                }",
            "            )",
            "",
            "        if read_from_replicas:",
            "            # Call our on_connect function to configure READONLY mode",
            "            kwargs[\"redis_connect_func\"] = self.on_connect",
            "",
            "        self.retry = retry",
            "        if retry or retry_on_error or connection_error_retry_attempts > 0:",
            "            # Set a retry object for all cluster nodes",
            "            self.retry = retry or Retry(",
            "                default_backoff(), connection_error_retry_attempts",
            "            )",
            "            if not retry_on_error:",
            "                # Default errors for retrying",
            "                retry_on_error = [ConnectionError, TimeoutError]",
            "            self.retry.update_supported_errors(retry_on_error)",
            "            kwargs.update({\"retry\": self.retry})",
            "",
            "        kwargs[\"response_callbacks\"] = self.__class__.RESPONSE_CALLBACKS.copy()",
            "        self.connection_kwargs = kwargs",
            "",
            "        if startup_nodes:",
            "            passed_nodes = []",
            "            for node in startup_nodes:",
            "                passed_nodes.append(",
            "                    ClusterNode(node.host, node.port, **self.connection_kwargs)",
            "                )",
            "            startup_nodes = passed_nodes",
            "        else:",
            "            startup_nodes = []",
            "        if host and port:",
            "            startup_nodes.append(ClusterNode(host, port, **self.connection_kwargs))",
            "",
            "        self.nodes_manager = NodesManager(startup_nodes, require_full_coverage, kwargs)",
            "        self.encoder = Encoder(encoding, encoding_errors, decode_responses)",
            "        self.read_from_replicas = read_from_replicas",
            "        self.reinitialize_steps = reinitialize_steps",
            "        self.cluster_error_retry_attempts = cluster_error_retry_attempts",
            "        self.connection_error_retry_attempts = connection_error_retry_attempts",
            "        self.reinitialize_counter = 0",
            "        self.commands_parser = CommandsParser()",
            "        self.node_flags = self.__class__.NODE_FLAGS.copy()",
            "        self.command_flags = self.__class__.COMMAND_FLAGS.copy()",
            "        self.response_callbacks = kwargs[\"response_callbacks\"]",
            "        self.result_callbacks = self.__class__.RESULT_CALLBACKS.copy()",
            "        self.result_callbacks[",
            "            \"CLUSTER SLOTS\"",
            "        ] = lambda cmd, res, **kwargs: parse_cluster_slots(",
            "            list(res.values())[0], **kwargs",
            "        )",
            "",
            "        self._initialize = True",
            "        self._lock: Optional[asyncio.Lock] = None",
            "",
            "    async def initialize(self) -> \"RedisCluster\":",
            "        \"\"\"Get all nodes from startup nodes & creates connections if not initialized.\"\"\"",
            "        if self._initialize:",
            "            if not self._lock:",
            "                self._lock = asyncio.Lock()",
            "            async with self._lock:",
            "                if self._initialize:",
            "                    try:",
            "                        await self.nodes_manager.initialize()",
            "                        await self.commands_parser.initialize(",
            "                            self.nodes_manager.default_node",
            "                        )",
            "                        self._initialize = False",
            "                    except BaseException:",
            "                        await self.nodes_manager.close()",
            "                        await self.nodes_manager.close(\"startup_nodes\")",
            "                        raise",
            "        return self",
            "",
            "    async def close(self) -> None:",
            "        \"\"\"Close all connections & client if initialized.\"\"\"",
            "        if not self._initialize:",
            "            if not self._lock:",
            "                self._lock = asyncio.Lock()",
            "            async with self._lock:",
            "                if not self._initialize:",
            "                    self._initialize = True",
            "                    await self.nodes_manager.close()",
            "                    await self.nodes_manager.close(\"startup_nodes\")",
            "",
            "    async def __aenter__(self) -> \"RedisCluster\":",
            "        return await self.initialize()",
            "",
            "    async def __aexit__(self, exc_type: None, exc_value: None, traceback: None) -> None:",
            "        await self.close()",
            "",
            "    def __await__(self) -> Generator[Any, None, \"RedisCluster\"]:",
            "        return self.initialize().__await__()",
            "",
            "    _DEL_MESSAGE = \"Unclosed RedisCluster client\"",
            "",
            "    def __del__(self) -> None:",
            "        if hasattr(self, \"_initialize\") and not self._initialize:",
            "            warnings.warn(f\"{self._DEL_MESSAGE} {self!r}\", ResourceWarning, source=self)",
            "            try:",
            "                context = {\"client\": self, \"message\": self._DEL_MESSAGE}",
            "                asyncio.get_running_loop().call_exception_handler(context)",
            "            except RuntimeError:",
            "                ...",
            "",
            "    async def on_connect(self, connection: Connection) -> None:",
            "        await connection.on_connect()",
            "",
            "        # Sending READONLY command to server to configure connection as",
            "        # readonly. Since each cluster node may change its server type due",
            "        # to a failover, we should establish a READONLY connection",
            "        # regardless of the server type. If this is a primary connection,",
            "        # READONLY would not affect executing write commands.",
            "        await connection.send_command(\"READONLY\")",
            "        if str_if_bytes(await connection.read_response()) != \"OK\":",
            "            raise ConnectionError(\"READONLY command failed\")",
            "",
            "    def get_nodes(self) -> List[\"ClusterNode\"]:",
            "        \"\"\"Get all nodes of the cluster.\"\"\"",
            "        return list(self.nodes_manager.nodes_cache.values())",
            "",
            "    def get_primaries(self) -> List[\"ClusterNode\"]:",
            "        \"\"\"Get the primary nodes of the cluster.\"\"\"",
            "        return self.nodes_manager.get_nodes_by_server_type(PRIMARY)",
            "",
            "    def get_replicas(self) -> List[\"ClusterNode\"]:",
            "        \"\"\"Get the replica nodes of the cluster.\"\"\"",
            "        return self.nodes_manager.get_nodes_by_server_type(REPLICA)",
            "",
            "    def get_random_node(self) -> \"ClusterNode\":",
            "        \"\"\"Get a random node of the cluster.\"\"\"",
            "        return random.choice(list(self.nodes_manager.nodes_cache.values()))",
            "",
            "    def get_default_node(self) -> \"ClusterNode\":",
            "        \"\"\"Get the default node of the client.\"\"\"",
            "        return self.nodes_manager.default_node",
            "",
            "    def set_default_node(self, node: \"ClusterNode\") -> None:",
            "        \"\"\"",
            "        Set the default node of the client.",
            "",
            "        :raises DataError: if None is passed or node does not exist in cluster.",
            "        \"\"\"",
            "        if not node or not self.get_node(node_name=node.name):",
            "            raise DataError(\"The requested node does not exist in the cluster.\")",
            "",
            "        self.nodes_manager.default_node = node",
            "",
            "    def get_node(",
            "        self,",
            "        host: Optional[str] = None,",
            "        port: Optional[int] = None,",
            "        node_name: Optional[str] = None,",
            "    ) -> Optional[\"ClusterNode\"]:",
            "        \"\"\"Get node by (host, port) or node_name.\"\"\"",
            "        return self.nodes_manager.get_node(host, port, node_name)",
            "",
            "    def get_node_from_key(",
            "        self, key: str, replica: bool = False",
            "    ) -> Optional[\"ClusterNode\"]:",
            "        \"\"\"",
            "        Get the cluster node corresponding to the provided key.",
            "",
            "        :param key:",
            "        :param replica:",
            "            | Indicates if a replica should be returned",
            "            |",
            "              None will returned if no replica holds this key",
            "",
            "        :raises SlotNotCoveredError: if the key is not covered by any slot.",
            "        \"\"\"",
            "        slot = self.keyslot(key)",
            "        slot_cache = self.nodes_manager.slots_cache.get(slot)",
            "        if not slot_cache:",
            "            raise SlotNotCoveredError(f'Slot \"{slot}\" is not covered by the cluster.')",
            "",
            "        if replica:",
            "            if len(self.nodes_manager.slots_cache[slot]) < 2:",
            "                return None",
            "            node_idx = 1",
            "        else:",
            "            node_idx = 0",
            "",
            "        return slot_cache[node_idx]",
            "",
            "    def keyslot(self, key: EncodableT) -> int:",
            "        \"\"\"",
            "        Find the keyslot for a given key.",
            "",
            "        See: https://redis.io/docs/manual/scaling/#redis-cluster-data-sharding",
            "        \"\"\"",
            "        return key_slot(self.encoder.encode(key))",
            "",
            "    def get_encoder(self) -> Encoder:",
            "        \"\"\"Get the encoder object of the client.\"\"\"",
            "        return self.encoder",
            "",
            "    def get_connection_kwargs(self) -> Dict[str, Optional[Any]]:",
            "        \"\"\"Get the kwargs passed to :class:`~redis.asyncio.connection.Connection`.\"\"\"",
            "        return self.connection_kwargs",
            "",
            "    def get_retry(self) -> Optional[\"Retry\"]:",
            "        return self.retry",
            "",
            "    def set_retry(self, retry: \"Retry\") -> None:",
            "        self.retry = retry",
            "        for node in self.get_nodes():",
            "            node.connection_kwargs.update({\"retry\": retry})",
            "            for conn in node._connections:",
            "                conn.retry = retry",
            "",
            "    def set_response_callback(self, command: str, callback: ResponseCallbackT) -> None:",
            "        \"\"\"Set a custom response callback.\"\"\"",
            "        self.response_callbacks[command] = callback",
            "",
            "    async def _determine_nodes(",
            "        self, command: str, *args: Any, node_flag: Optional[str] = None",
            "    ) -> List[\"ClusterNode\"]:",
            "        # Determine which nodes should be executed the command on.",
            "        # Returns a list of target nodes.",
            "        if not node_flag:",
            "            # get the nodes group for this command if it was predefined",
            "            node_flag = self.command_flags.get(command)",
            "",
            "        if node_flag in self.node_flags:",
            "            if node_flag == self.__class__.DEFAULT_NODE:",
            "                # return the cluster's default node",
            "                return [self.nodes_manager.default_node]",
            "            if node_flag == self.__class__.PRIMARIES:",
            "                # return all primaries",
            "                return self.nodes_manager.get_nodes_by_server_type(PRIMARY)",
            "            if node_flag == self.__class__.REPLICAS:",
            "                # return all replicas",
            "                return self.nodes_manager.get_nodes_by_server_type(REPLICA)",
            "            if node_flag == self.__class__.ALL_NODES:",
            "                # return all nodes",
            "                return list(self.nodes_manager.nodes_cache.values())",
            "            if node_flag == self.__class__.RANDOM:",
            "                # return a random node",
            "                return [random.choice(list(self.nodes_manager.nodes_cache.values()))]",
            "",
            "        # get the node that holds the key's slot",
            "        return [",
            "            self.nodes_manager.get_node_from_slot(",
            "                await self._determine_slot(command, *args),",
            "                self.read_from_replicas and command in READ_COMMANDS,",
            "            )",
            "        ]",
            "",
            "    async def _determine_slot(self, command: str, *args: Any) -> int:",
            "        if self.command_flags.get(command) == SLOT_ID:",
            "            # The command contains the slot ID",
            "            return int(args[0])",
            "",
            "        # Get the keys in the command",
            "",
            "        # EVAL and EVALSHA are common enough that it's wasteful to go to the",
            "        # redis server to parse the keys. Besides, there is a bug in redis<7.0",
            "        # where `self._get_command_keys()` fails anyway. So, we special case",
            "        # EVAL/EVALSHA.",
            "        # - issue: https://github.com/redis/redis/issues/9493",
            "        # - fix: https://github.com/redis/redis/pull/9733",
            "        if command in (\"EVAL\", \"EVALSHA\"):",
            "            # command syntax: EVAL \"script body\" num_keys ...",
            "            if len(args) < 2:",
            "                raise RedisClusterException(",
            "                    f\"Invalid args in command: {command, *args}\"",
            "                )",
            "            keys = args[2 : 2 + args[1]]",
            "            # if there are 0 keys, that means the script can be run on any node",
            "            # so we can just return a random slot",
            "            if not keys:",
            "                return random.randrange(0, REDIS_CLUSTER_HASH_SLOTS)",
            "        else:",
            "            keys = await self.commands_parser.get_keys(command, *args)",
            "            if not keys:",
            "                # FCALL can call a function with 0 keys, that means the function",
            "                #  can be run on any node so we can just return a random slot",
            "                if command in (\"FCALL\", \"FCALL_RO\"):",
            "                    return random.randrange(0, REDIS_CLUSTER_HASH_SLOTS)",
            "                raise RedisClusterException(",
            "                    \"No way to dispatch this command to Redis Cluster. \"",
            "                    \"Missing key.\\nYou can execute the command by specifying \"",
            "                    f\"target nodes.\\nCommand: {args}\"",
            "                )",
            "",
            "        # single key command",
            "        if len(keys) == 1:",
            "            return self.keyslot(keys[0])",
            "",
            "        # multi-key command; we need to make sure all keys are mapped to",
            "        # the same slot",
            "        slots = {self.keyslot(key) for key in keys}",
            "        if len(slots) != 1:",
            "            raise RedisClusterException(",
            "                f\"{command} - all keys must map to the same key slot\"",
            "            )",
            "",
            "        return slots.pop()",
            "",
            "    def _is_node_flag(self, target_nodes: Any) -> bool:",
            "        return isinstance(target_nodes, str) and target_nodes in self.node_flags",
            "",
            "    def _parse_target_nodes(self, target_nodes: Any) -> List[\"ClusterNode\"]:",
            "        if isinstance(target_nodes, list):",
            "            nodes = target_nodes",
            "        elif isinstance(target_nodes, ClusterNode):",
            "            # Supports passing a single ClusterNode as a variable",
            "            nodes = [target_nodes]",
            "        elif isinstance(target_nodes, dict):",
            "            # Supports dictionaries of the format {node_name: node}.",
            "            # It enables to execute commands with multi nodes as follows:",
            "            # rc.cluster_save_config(rc.get_primaries())",
            "            nodes = list(target_nodes.values())",
            "        else:",
            "            raise TypeError(",
            "                \"target_nodes type can be one of the following: \"",
            "                \"node_flag (PRIMARIES, REPLICAS, RANDOM, ALL_NODES),\"",
            "                \"ClusterNode, list<ClusterNode>, or dict<any, ClusterNode>. \"",
            "                f\"The passed type is {type(target_nodes)}\"",
            "            )",
            "        return nodes",
            "",
            "    async def execute_command(self, *args: EncodableT, **kwargs: Any) -> Any:",
            "        \"\"\"",
            "        Execute a raw command on the appropriate cluster node or target_nodes.",
            "",
            "        It will retry the command as specified by :attr:`cluster_error_retry_attempts` &",
            "        then raise an exception.",
            "",
            "        :param args:",
            "            | Raw command args",
            "        :param kwargs:",
            "",
            "            - target_nodes: :attr:`NODE_FLAGS` or :class:`~.ClusterNode`",
            "              or List[:class:`~.ClusterNode`] or Dict[Any, :class:`~.ClusterNode`]",
            "            - Rest of the kwargs are passed to the Redis connection",
            "",
            "        :raises RedisClusterException: if target_nodes is not provided & the command",
            "            can't be mapped to a slot",
            "        \"\"\"",
            "        command = args[0]",
            "        target_nodes = []",
            "        target_nodes_specified = False",
            "        retry_attempts = self.cluster_error_retry_attempts",
            "",
            "        passed_targets = kwargs.pop(\"target_nodes\", None)",
            "        if passed_targets and not self._is_node_flag(passed_targets):",
            "            target_nodes = self._parse_target_nodes(passed_targets)",
            "            target_nodes_specified = True",
            "            retry_attempts = 0",
            "",
            "        # Add one for the first execution",
            "        execute_attempts = 1 + retry_attempts",
            "        for _ in range(execute_attempts):",
            "            if self._initialize:",
            "                await self.initialize()",
            "                if (",
            "                    len(target_nodes) == 1",
            "                    and target_nodes[0] == self.get_default_node()",
            "                ):",
            "                    # Replace the default cluster node",
            "                    self.replace_default_node()",
            "            try:",
            "                if not target_nodes_specified:",
            "                    # Determine the nodes to execute the command on",
            "                    target_nodes = await self._determine_nodes(",
            "                        *args, node_flag=passed_targets",
            "                    )",
            "                    if not target_nodes:",
            "                        raise RedisClusterException(",
            "                            f\"No targets were found to execute {args} command on\"",
            "                        )",
            "",
            "                if len(target_nodes) == 1:",
            "                    # Return the processed result",
            "                    ret = await self._execute_command(target_nodes[0], *args, **kwargs)",
            "                    if command in self.result_callbacks:",
            "                        return self.result_callbacks[command](",
            "                            command, {target_nodes[0].name: ret}, **kwargs",
            "                        )",
            "                    return ret",
            "                else:",
            "                    keys = [node.name for node in target_nodes]",
            "                    values = await asyncio.gather(",
            "                        *(",
            "                            asyncio.create_task(",
            "                                self._execute_command(node, *args, **kwargs)",
            "                            )",
            "                            for node in target_nodes",
            "                        )",
            "                    )",
            "                    if command in self.result_callbacks:",
            "                        return self.result_callbacks[command](",
            "                            command, dict(zip(keys, values)), **kwargs",
            "                        )",
            "                    return dict(zip(keys, values))",
            "            except Exception as e:",
            "                if retry_attempts > 0 and type(e) in self.__class__.ERRORS_ALLOW_RETRY:",
            "                    # The nodes and slots cache were should be reinitialized.",
            "                    # Try again with the new cluster setup.",
            "                    retry_attempts -= 1",
            "                    continue",
            "                else:",
            "                    # raise the exception",
            "                    raise e",
            "",
            "    async def _execute_command(",
            "        self, target_node: \"ClusterNode\", *args: Union[KeyT, EncodableT], **kwargs: Any",
            "    ) -> Any:",
            "        asking = moved = False",
            "        redirect_addr = None",
            "        ttl = self.RedisClusterRequestTTL",
            "",
            "        while ttl > 0:",
            "            ttl -= 1",
            "            try:",
            "                if asking:",
            "                    target_node = self.get_node(node_name=redirect_addr)",
            "                    await target_node.execute_command(\"ASKING\")",
            "                    asking = False",
            "                elif moved:",
            "                    # MOVED occurred and the slots cache was updated,",
            "                    # refresh the target node",
            "                    slot = await self._determine_slot(*args)",
            "                    target_node = self.nodes_manager.get_node_from_slot(",
            "                        slot, self.read_from_replicas and args[0] in READ_COMMANDS",
            "                    )",
            "                    moved = False",
            "",
            "                return await target_node.execute_command(*args, **kwargs)",
            "            except (BusyLoadingError, MaxConnectionsError):",
            "                raise",
            "            except (ConnectionError, TimeoutError):",
            "                # Connection retries are being handled in the node's",
            "                # Retry object.",
            "                # Remove the failed node from the startup nodes before we try",
            "                # to reinitialize the cluster",
            "                self.nodes_manager.startup_nodes.pop(target_node.name, None)",
            "                # Hard force of reinitialize of the node/slots setup",
            "                # and try again with the new setup",
            "                await self.close()",
            "                raise",
            "            except ClusterDownError:",
            "                # ClusterDownError can occur during a failover and to get",
            "                # self-healed, we will try to reinitialize the cluster layout",
            "                # and retry executing the command",
            "                await self.close()",
            "                await asyncio.sleep(0.25)",
            "                raise",
            "            except MovedError as e:",
            "                # First, we will try to patch the slots/nodes cache with the",
            "                # redirected node output and try again. If MovedError exceeds",
            "                # 'reinitialize_steps' number of times, we will force",
            "                # reinitializing the tables, and then try again.",
            "                # 'reinitialize_steps' counter will increase faster when",
            "                # the same client object is shared between multiple threads. To",
            "                # reduce the frequency you can set this variable in the",
            "                # RedisCluster constructor.",
            "                self.reinitialize_counter += 1",
            "                if (",
            "                    self.reinitialize_steps",
            "                    and self.reinitialize_counter % self.reinitialize_steps == 0",
            "                ):",
            "                    await self.close()",
            "                    # Reset the counter",
            "                    self.reinitialize_counter = 0",
            "                else:",
            "                    self.nodes_manager._moved_exception = e",
            "                moved = True",
            "            except AskError as e:",
            "                redirect_addr = get_node_name(host=e.host, port=e.port)",
            "                asking = True",
            "            except TryAgainError:",
            "                if ttl < self.RedisClusterRequestTTL / 2:",
            "                    await asyncio.sleep(0.05)",
            "",
            "        raise ClusterError(\"TTL exhausted.\")",
            "",
            "    def pipeline(",
            "        self, transaction: Optional[Any] = None, shard_hint: Optional[Any] = None",
            "    ) -> \"ClusterPipeline\":",
            "        \"\"\"",
            "        Create & return a new :class:`~.ClusterPipeline` object.",
            "",
            "        Cluster implementation of pipeline does not support transaction or shard_hint.",
            "",
            "        :raises RedisClusterException: if transaction or shard_hint are truthy values",
            "        \"\"\"",
            "        if shard_hint:",
            "            raise RedisClusterException(\"shard_hint is deprecated in cluster mode\")",
            "",
            "        if transaction:",
            "            raise RedisClusterException(\"transaction is deprecated in cluster mode\")",
            "",
            "        return ClusterPipeline(self)",
            "",
            "    def lock(",
            "        self,",
            "        name: KeyT,",
            "        timeout: Optional[float] = None,",
            "        sleep: float = 0.1,",
            "        blocking: bool = True,",
            "        blocking_timeout: Optional[float] = None,",
            "        lock_class: Optional[Type[Lock]] = None,",
            "        thread_local: bool = True,",
            "    ) -> Lock:",
            "        \"\"\"",
            "        Return a new Lock object using key ``name`` that mimics",
            "        the behavior of threading.Lock.",
            "",
            "        If specified, ``timeout`` indicates a maximum life for the lock.",
            "        By default, it will remain locked until release() is called.",
            "",
            "        ``sleep`` indicates the amount of time to sleep per loop iteration",
            "        when the lock is in blocking mode and another client is currently",
            "        holding the lock.",
            "",
            "        ``blocking`` indicates whether calling ``acquire`` should block until",
            "        the lock has been acquired or to fail immediately, causing ``acquire``",
            "        to return False and the lock not being acquired. Defaults to True.",
            "        Note this value can be overridden by passing a ``blocking``",
            "        argument to ``acquire``.",
            "",
            "        ``blocking_timeout`` indicates the maximum amount of time in seconds to",
            "        spend trying to acquire the lock. A value of ``None`` indicates",
            "        continue trying forever. ``blocking_timeout`` can be specified as a",
            "        float or integer, both representing the number of seconds to wait.",
            "",
            "        ``lock_class`` forces the specified lock implementation. Note that as",
            "        of redis-py 3.0, the only lock class we implement is ``Lock`` (which is",
            "        a Lua-based lock). So, it's unlikely you'll need this parameter, unless",
            "        you have created your own custom lock class.",
            "",
            "        ``thread_local`` indicates whether the lock token is placed in",
            "        thread-local storage. By default, the token is placed in thread local",
            "        storage so that a thread only sees its token, not a token set by",
            "        another thread. Consider the following timeline:",
            "",
            "            time: 0, thread-1 acquires `my-lock`, with a timeout of 5 seconds.",
            "                     thread-1 sets the token to \"abc\"",
            "            time: 1, thread-2 blocks trying to acquire `my-lock` using the",
            "                     Lock instance.",
            "            time: 5, thread-1 has not yet completed. redis expires the lock",
            "                     key.",
            "            time: 5, thread-2 acquired `my-lock` now that it's available.",
            "                     thread-2 sets the token to \"xyz\"",
            "            time: 6, thread-1 finishes its work and calls release(). if the",
            "                     token is *not* stored in thread local storage, then",
            "                     thread-1 would see the token value as \"xyz\" and would be",
            "                     able to successfully release the thread-2's lock.",
            "",
            "        In some use cases it's necessary to disable thread local storage. For",
            "        example, if you have code where one thread acquires a lock and passes",
            "        that lock instance to a worker thread to release later. If thread",
            "        local storage isn't disabled in this case, the worker thread won't see",
            "        the token set by the thread that acquired the lock. Our assumption",
            "        is that these cases aren't common and as such default to using",
            "        thread local storage.\"\"\"",
            "        if lock_class is None:",
            "            lock_class = Lock",
            "        return lock_class(",
            "            self,",
            "            name,",
            "            timeout=timeout,",
            "            sleep=sleep,",
            "            blocking=blocking,",
            "            blocking_timeout=blocking_timeout,",
            "            thread_local=thread_local,",
            "        )",
            "",
            "",
            "class ClusterNode:",
            "    \"\"\"",
            "    Create a new ClusterNode.",
            "",
            "    Each ClusterNode manages multiple :class:`~redis.asyncio.connection.Connection`",
            "    objects for the (host, port).",
            "    \"\"\"",
            "",
            "    __slots__ = (",
            "        \"_connections\",",
            "        \"_free\",",
            "        \"connection_class\",",
            "        \"connection_kwargs\",",
            "        \"host\",",
            "        \"max_connections\",",
            "        \"name\",",
            "        \"port\",",
            "        \"response_callbacks\",",
            "        \"server_type\",",
            "    )",
            "",
            "    def __init__(",
            "        self,",
            "        host: str,",
            "        port: Union[str, int],",
            "        server_type: Optional[str] = None,",
            "        *,",
            "        max_connections: int = 2**31,",
            "        connection_class: Type[Connection] = Connection,",
            "        **connection_kwargs: Any,",
            "    ) -> None:",
            "        if host == \"localhost\":",
            "            host = socket.gethostbyname(host)",
            "",
            "        connection_kwargs[\"host\"] = host",
            "        connection_kwargs[\"port\"] = port",
            "        self.host = host",
            "        self.port = port",
            "        self.name = get_node_name(host, port)",
            "        self.server_type = server_type",
            "",
            "        self.max_connections = max_connections",
            "        self.connection_class = connection_class",
            "        self.connection_kwargs = connection_kwargs",
            "        self.response_callbacks = connection_kwargs.pop(\"response_callbacks\", {})",
            "",
            "        self._connections: List[Connection] = []",
            "        self._free: Deque[Connection] = collections.deque(maxlen=self.max_connections)",
            "",
            "    def __repr__(self) -> str:",
            "        return (",
            "            f\"[host={self.host}, port={self.port}, \"",
            "            f\"name={self.name}, server_type={self.server_type}]\"",
            "        )",
            "",
            "    def __eq__(self, obj: Any) -> bool:",
            "        return isinstance(obj, ClusterNode) and obj.name == self.name",
            "",
            "    _DEL_MESSAGE = \"Unclosed ClusterNode object\"",
            "",
            "    def __del__(self) -> None:",
            "        for connection in self._connections:",
            "            if connection.is_connected:",
            "                warnings.warn(",
            "                    f\"{self._DEL_MESSAGE} {self!r}\", ResourceWarning, source=self",
            "                )",
            "                try:",
            "                    context = {\"client\": self, \"message\": self._DEL_MESSAGE}",
            "                    asyncio.get_running_loop().call_exception_handler(context)",
            "                except RuntimeError:",
            "                    ...",
            "                break",
            "",
            "    async def disconnect(self) -> None:",
            "        ret = await asyncio.gather(",
            "            *(",
            "                asyncio.create_task(connection.disconnect())",
            "                for connection in self._connections",
            "            ),",
            "            return_exceptions=True,",
            "        )",
            "        exc = next((res for res in ret if isinstance(res, Exception)), None)",
            "        if exc:",
            "            raise exc",
            "",
            "    def acquire_connection(self) -> Connection:",
            "        try:",
            "            return self._free.popleft()",
            "        except IndexError:",
            "            if len(self._connections) < self.max_connections:",
            "                connection = self.connection_class(**self.connection_kwargs)",
            "                self._connections.append(connection)",
            "                return connection",
            "",
            "            raise MaxConnectionsError()",
            "",
            "    async def parse_response(",
            "        self, connection: Connection, command: str, **kwargs: Any",
            "    ) -> Any:",
            "        try:",
            "            if NEVER_DECODE in kwargs:",
            "                response = await connection.read_response(disable_decoding=True)",
            "                kwargs.pop(NEVER_DECODE)",
            "            else:",
            "                response = await connection.read_response()",
            "        except ResponseError:",
            "            if EMPTY_RESPONSE in kwargs:",
            "                return kwargs[EMPTY_RESPONSE]",
            "            raise",
            "",
            "        if EMPTY_RESPONSE in kwargs:",
            "            kwargs.pop(EMPTY_RESPONSE)",
            "",
            "        # Return response",
            "        if command in self.response_callbacks:",
            "            return self.response_callbacks[command](response, **kwargs)",
            "",
            "        return response",
            "",
            "    async def execute_command(self, *args: Any, **kwargs: Any) -> Any:",
            "        # Acquire connection",
            "        connection = self.acquire_connection()",
            "",
            "        # Execute command",
            "        await connection.send_packed_command(connection.pack_command(*args), False)",
            "",
            "        # Read response",
            "        try:",
            "            return await self.parse_response(connection, args[0], **kwargs)",
            "        finally:",
            "            # Release connection",
            "            self._free.append(connection)",
            "",
            "    async def execute_pipeline(self, commands: List[\"PipelineCommand\"]) -> bool:",
            "        # Acquire connection",
            "        connection = self.acquire_connection()",
            "",
            "        # Execute command",
            "        await connection.send_packed_command(",
            "            connection.pack_commands(cmd.args for cmd in commands), False",
            "        )",
            "",
            "        # Read responses",
            "        ret = False",
            "        for cmd in commands:",
            "            try:",
            "                cmd.result = await self.parse_response(",
            "                    connection, cmd.args[0], **cmd.kwargs",
            "                )",
            "            except Exception as e:",
            "                cmd.result = e",
            "                ret = True",
            "",
            "        # Release connection",
            "        self._free.append(connection)",
            "",
            "        return ret",
            "",
            "",
            "class NodesManager:",
            "    __slots__ = (",
            "        \"_moved_exception\",",
            "        \"connection_kwargs\",",
            "        \"default_node\",",
            "        \"nodes_cache\",",
            "        \"read_load_balancer\",",
            "        \"require_full_coverage\",",
            "        \"slots_cache\",",
            "        \"startup_nodes\",",
            "    )",
            "",
            "    def __init__(",
            "        self,",
            "        startup_nodes: List[\"ClusterNode\"],",
            "        require_full_coverage: bool,",
            "        connection_kwargs: Dict[str, Any],",
            "    ) -> None:",
            "        self.startup_nodes = {node.name: node for node in startup_nodes}",
            "        self.require_full_coverage = require_full_coverage",
            "        self.connection_kwargs = connection_kwargs",
            "",
            "        self.default_node: \"ClusterNode\" = None",
            "        self.nodes_cache: Dict[str, \"ClusterNode\"] = {}",
            "        self.slots_cache: Dict[int, List[\"ClusterNode\"]] = {}",
            "        self.read_load_balancer = LoadBalancer()",
            "        self._moved_exception: MovedError = None",
            "",
            "    def get_node(",
            "        self,",
            "        host: Optional[str] = None,",
            "        port: Optional[int] = None,",
            "        node_name: Optional[str] = None,",
            "    ) -> Optional[\"ClusterNode\"]:",
            "        if host and port:",
            "            # the user passed host and port",
            "            if host == \"localhost\":",
            "                host = socket.gethostbyname(host)",
            "            return self.nodes_cache.get(get_node_name(host=host, port=port))",
            "        elif node_name:",
            "            return self.nodes_cache.get(node_name)",
            "        else:",
            "            raise DataError(",
            "                \"get_node requires one of the following: \"",
            "                \"1. node name \"",
            "                \"2. host and port\"",
            "            )",
            "",
            "    def set_nodes(",
            "        self,",
            "        old: Dict[str, \"ClusterNode\"],",
            "        new: Dict[str, \"ClusterNode\"],",
            "        remove_old: bool = False,",
            "    ) -> None:",
            "        if remove_old:",
            "            for name in list(old.keys()):",
            "                if name not in new:",
            "                    asyncio.create_task(old.pop(name).disconnect())",
            "",
            "        for name, node in new.items():",
            "            if name in old:",
            "                if old[name] is node:",
            "                    continue",
            "                asyncio.create_task(old[name].disconnect())",
            "            old[name] = node",
            "",
            "    def _update_moved_slots(self) -> None:",
            "        e = self._moved_exception",
            "        redirected_node = self.get_node(host=e.host, port=e.port)",
            "        if redirected_node:",
            "            # The node already exists",
            "            if redirected_node.server_type != PRIMARY:",
            "                # Update the node's server type",
            "                redirected_node.server_type = PRIMARY",
            "        else:",
            "            # This is a new node, we will add it to the nodes cache",
            "            redirected_node = ClusterNode(",
            "                e.host, e.port, PRIMARY, **self.connection_kwargs",
            "            )",
            "            self.set_nodes(self.nodes_cache, {redirected_node.name: redirected_node})",
            "        if redirected_node in self.slots_cache[e.slot_id]:",
            "            # The MOVED error resulted from a failover, and the new slot owner",
            "            # had previously been a replica.",
            "            old_primary = self.slots_cache[e.slot_id][0]",
            "            # Update the old primary to be a replica and add it to the end of",
            "            # the slot's node list",
            "            old_primary.server_type = REPLICA",
            "            self.slots_cache[e.slot_id].append(old_primary)",
            "            # Remove the old replica, which is now a primary, from the slot's",
            "            # node list",
            "            self.slots_cache[e.slot_id].remove(redirected_node)",
            "            # Override the old primary with the new one",
            "            self.slots_cache[e.slot_id][0] = redirected_node",
            "            if self.default_node == old_primary:",
            "                # Update the default node with the new primary",
            "                self.default_node = redirected_node",
            "        else:",
            "            # The new slot owner is a new server, or a server from a different",
            "            # shard. We need to remove all current nodes from the slot's list",
            "            # (including replications) and add just the new node.",
            "            self.slots_cache[e.slot_id] = [redirected_node]",
            "        # Reset moved_exception",
            "        self._moved_exception = None",
            "",
            "    def get_node_from_slot(",
            "        self, slot: int, read_from_replicas: bool = False",
            "    ) -> \"ClusterNode\":",
            "        if self._moved_exception:",
            "            self._update_moved_slots()",
            "",
            "        try:",
            "            if read_from_replicas:",
            "                # get the server index in a Round-Robin manner",
            "                primary_name = self.slots_cache[slot][0].name",
            "                node_idx = self.read_load_balancer.get_server_index(",
            "                    primary_name, len(self.slots_cache[slot])",
            "                )",
            "                return self.slots_cache[slot][node_idx]",
            "            return self.slots_cache[slot][0]",
            "        except (IndexError, TypeError):",
            "            raise SlotNotCoveredError(",
            "                f'Slot \"{slot}\" not covered by the cluster. '",
            "                f'\"require_full_coverage={self.require_full_coverage}\"'",
            "            )",
            "",
            "    def get_nodes_by_server_type(self, server_type: str) -> List[\"ClusterNode\"]:",
            "        return [",
            "            node",
            "            for node in self.nodes_cache.values()",
            "            if node.server_type == server_type",
            "        ]",
            "",
            "    async def initialize(self) -> None:",
            "        self.read_load_balancer.reset()",
            "        tmp_nodes_cache: Dict[str, \"ClusterNode\"] = {}",
            "        tmp_slots: Dict[int, List[\"ClusterNode\"]] = {}",
            "        disagreements = []",
            "        startup_nodes_reachable = False",
            "        fully_covered = False",
            "        exception = None",
            "        for startup_node in self.startup_nodes.values():",
            "            try:",
            "                # Make sure cluster mode is enabled on this node",
            "                if not (await startup_node.execute_command(\"INFO\")).get(",
            "                    \"cluster_enabled\"",
            "                ):",
            "                    raise RedisClusterException(",
            "                        \"Cluster mode is not enabled on this node\"",
            "                    )",
            "                cluster_slots = await startup_node.execute_command(\"CLUSTER SLOTS\")",
            "                startup_nodes_reachable = True",
            "            except Exception as e:",
            "                # Try the next startup node.",
            "                # The exception is saved and raised only if we have no more nodes.",
            "                exception = e",
            "                continue",
            "",
            "            # CLUSTER SLOTS command results in the following output:",
            "            # [[slot_section[from_slot,to_slot,master,replica1,...,replicaN]]]",
            "            # where each node contains the following list: [IP, port, node_id]",
            "            # Therefore, cluster_slots[0][2][0] will be the IP address of the",
            "            # primary node of the first slot section.",
            "            # If there's only one server in the cluster, its ``host`` is ''",
            "            # Fix it to the host in startup_nodes",
            "            if (",
            "                len(cluster_slots) == 1",
            "                and not cluster_slots[0][2][0]",
            "                and len(self.startup_nodes) == 1",
            "            ):",
            "                cluster_slots[0][2][0] = startup_node.host",
            "",
            "            for slot in cluster_slots:",
            "                for i in range(2, len(slot)):",
            "                    slot[i] = [str_if_bytes(val) for val in slot[i]]",
            "                primary_node = slot[2]",
            "                host = primary_node[0]",
            "                if host == \"\":",
            "                    host = startup_node.host",
            "                port = int(primary_node[1])",
            "",
            "                target_node = tmp_nodes_cache.get(get_node_name(host, port))",
            "                if not target_node:",
            "                    target_node = ClusterNode(",
            "                        host, port, PRIMARY, **self.connection_kwargs",
            "                    )",
            "                # add this node to the nodes cache",
            "                tmp_nodes_cache[target_node.name] = target_node",
            "",
            "                for i in range(int(slot[0]), int(slot[1]) + 1):",
            "                    if i not in tmp_slots:",
            "                        tmp_slots[i] = []",
            "                        tmp_slots[i].append(target_node)",
            "                        replica_nodes = [slot[j] for j in range(3, len(slot))]",
            "",
            "                        for replica_node in replica_nodes:",
            "                            host = replica_node[0]",
            "                            port = replica_node[1]",
            "",
            "                            target_replica_node = tmp_nodes_cache.get(",
            "                                get_node_name(host, port)",
            "                            )",
            "                            if not target_replica_node:",
            "                                target_replica_node = ClusterNode(",
            "                                    host, port, REPLICA, **self.connection_kwargs",
            "                                )",
            "                            tmp_slots[i].append(target_replica_node)",
            "                            # add this node to the nodes cache",
            "                            tmp_nodes_cache[",
            "                                target_replica_node.name",
            "                            ] = target_replica_node",
            "                    else:",
            "                        # Validate that 2 nodes want to use the same slot cache",
            "                        # setup",
            "                        tmp_slot = tmp_slots[i][0]",
            "                        if tmp_slot.name != target_node.name:",
            "                            disagreements.append(",
            "                                f\"{tmp_slot.name} vs {target_node.name} on slot: {i}\"",
            "                            )",
            "",
            "                            if len(disagreements) > 5:",
            "                                raise RedisClusterException(",
            "                                    f\"startup_nodes could not agree on a valid \"",
            "                                    f'slots cache: {\", \".join(disagreements)}'",
            "                                )",
            "",
            "            # Validate if all slots are covered or if we should try next startup node",
            "            fully_covered = True",
            "            for i in range(REDIS_CLUSTER_HASH_SLOTS):",
            "                if i not in tmp_slots:",
            "                    fully_covered = False",
            "                    break",
            "            if fully_covered:",
            "                break",
            "",
            "        if not startup_nodes_reachable:",
            "            raise RedisClusterException(",
            "                f\"Redis Cluster cannot be connected. Please provide at least \"",
            "                f\"one reachable node: {str(exception)}\"",
            "            ) from exception",
            "",
            "        # Check if the slots are not fully covered",
            "        if not fully_covered and self.require_full_coverage:",
            "            # Despite the requirement that the slots be covered, there",
            "            # isn't a full coverage",
            "            raise RedisClusterException(",
            "                f\"All slots are not covered after query all startup_nodes. \"",
            "                f\"{len(tmp_slots)} of {REDIS_CLUSTER_HASH_SLOTS} \"",
            "                f\"covered...\"",
            "            )",
            "",
            "        # Set the tmp variables to the real variables",
            "        self.slots_cache = tmp_slots",
            "        self.set_nodes(self.nodes_cache, tmp_nodes_cache, remove_old=True)",
            "        # Populate the startup nodes with all discovered nodes",
            "        self.set_nodes(self.startup_nodes, self.nodes_cache, remove_old=True)",
            "",
            "        # Set the default node",
            "        self.default_node = self.get_nodes_by_server_type(PRIMARY)[0]",
            "        # If initialize was called after a MovedError, clear it",
            "        self._moved_exception = None",
            "",
            "    async def close(self, attr: str = \"nodes_cache\") -> None:",
            "        self.default_node = None",
            "        await asyncio.gather(",
            "            *(",
            "                asyncio.create_task(node.disconnect())",
            "                for node in getattr(self, attr).values()",
            "            )",
            "        )",
            "",
            "",
            "class ClusterPipeline(AbstractRedis, AbstractRedisCluster, AsyncRedisClusterCommands):",
            "    \"\"\"",
            "    Create a new ClusterPipeline object.",
            "",
            "    Usage::",
            "",
            "        result = await (",
            "            rc.pipeline()",
            "            .set(\"A\", 1)",
            "            .get(\"A\")",
            "            .hset(\"K\", \"F\", \"V\")",
            "            .hgetall(\"K\")",
            "            .mset_nonatomic({\"A\": 2, \"B\": 3})",
            "            .get(\"A\")",
            "            .get(\"B\")",
            "            .delete(\"A\", \"B\", \"K\")",
            "            .execute()",
            "        )",
            "        # result = [True, \"1\", 1, {\"F\": \"V\"}, True, True, \"2\", \"3\", 1, 1, 1]",
            "",
            "    Note: For commands `DELETE`, `EXISTS`, `TOUCH`, `UNLINK`, `mset_nonatomic`, which",
            "    are split across multiple nodes, you'll get multiple results for them in the array.",
            "",
            "    Retryable errors:",
            "        - :class:`~.ClusterDownError`",
            "        - :class:`~.ConnectionError`",
            "        - :class:`~.TimeoutError`",
            "",
            "    Redirection errors:",
            "        - :class:`~.TryAgainError`",
            "        - :class:`~.MovedError`",
            "        - :class:`~.AskError`",
            "",
            "    :param client:",
            "        | Existing :class:`~.RedisCluster` client",
            "    \"\"\"",
            "",
            "    __slots__ = (\"_command_stack\", \"_client\")",
            "",
            "    def __init__(self, client: RedisCluster) -> None:",
            "        self._client = client",
            "",
            "        self._command_stack: List[\"PipelineCommand\"] = []",
            "",
            "    async def initialize(self) -> \"ClusterPipeline\":",
            "        if self._client._initialize:",
            "            await self._client.initialize()",
            "        self._command_stack = []",
            "        return self",
            "",
            "    async def __aenter__(self) -> \"ClusterPipeline\":",
            "        return await self.initialize()",
            "",
            "    async def __aexit__(self, exc_type: None, exc_value: None, traceback: None) -> None:",
            "        self._command_stack = []",
            "",
            "    def __await__(self) -> Generator[Any, None, \"ClusterPipeline\"]:",
            "        return self.initialize().__await__()",
            "",
            "    def __enter__(self) -> \"ClusterPipeline\":",
            "        self._command_stack = []",
            "        return self",
            "",
            "    def __exit__(self, exc_type: None, exc_value: None, traceback: None) -> None:",
            "        self._command_stack = []",
            "",
            "    def __bool__(self) -> bool:",
            "        return bool(self._command_stack)",
            "",
            "    def __len__(self) -> int:",
            "        return len(self._command_stack)",
            "",
            "    def execute_command(",
            "        self, *args: Union[KeyT, EncodableT], **kwargs: Any",
            "    ) -> \"ClusterPipeline\":",
            "        \"\"\"",
            "        Append a raw command to the pipeline.",
            "",
            "        :param args:",
            "            | Raw command args",
            "        :param kwargs:",
            "",
            "            - target_nodes: :attr:`NODE_FLAGS` or :class:`~.ClusterNode`",
            "              or List[:class:`~.ClusterNode`] or Dict[Any, :class:`~.ClusterNode`]",
            "            - Rest of the kwargs are passed to the Redis connection",
            "        \"\"\"",
            "        self._command_stack.append(",
            "            PipelineCommand(len(self._command_stack), *args, **kwargs)",
            "        )",
            "        return self",
            "",
            "    async def execute(",
            "        self, raise_on_error: bool = True, allow_redirections: bool = True",
            "    ) -> List[Any]:",
            "        \"\"\"",
            "        Execute the pipeline.",
            "",
            "        It will retry the commands as specified by :attr:`cluster_error_retry_attempts`",
            "        & then raise an exception.",
            "",
            "        :param raise_on_error:",
            "            | Raise the first error if there are any errors",
            "        :param allow_redirections:",
            "            | Whether to retry each failed command individually in case of redirection",
            "              errors",
            "",
            "        :raises RedisClusterException: if target_nodes is not provided & the command",
            "            can't be mapped to a slot",
            "        \"\"\"",
            "        if not self._command_stack:",
            "            return []",
            "",
            "        try:",
            "            for _ in range(self._client.cluster_error_retry_attempts):",
            "                if self._client._initialize:",
            "                    await self._client.initialize()",
            "",
            "                try:",
            "                    return await self._execute(",
            "                        self._client,",
            "                        self._command_stack,",
            "                        raise_on_error=raise_on_error,",
            "                        allow_redirections=allow_redirections,",
            "                    )",
            "                except BaseException as e:",
            "                    if type(e) in self.__class__.ERRORS_ALLOW_RETRY:",
            "                        # Try again with the new cluster setup.",
            "                        exception = e",
            "                        await self._client.close()",
            "                        await asyncio.sleep(0.25)",
            "                    else:",
            "                        # All other errors should be raised.",
            "                        raise",
            "",
            "            # If it fails the configured number of times then raise an exception",
            "            raise exception",
            "        finally:",
            "            self._command_stack = []",
            "",
            "    async def _execute(",
            "        self,",
            "        client: \"RedisCluster\",",
            "        stack: List[\"PipelineCommand\"],",
            "        raise_on_error: bool = True,",
            "        allow_redirections: bool = True,",
            "    ) -> List[Any]:",
            "        todo = [",
            "            cmd for cmd in stack if not cmd.result or isinstance(cmd.result, Exception)",
            "        ]",
            "",
            "        nodes = {}",
            "        for cmd in todo:",
            "            passed_targets = cmd.kwargs.pop(\"target_nodes\", None)",
            "            if passed_targets and not client._is_node_flag(passed_targets):",
            "                target_nodes = client._parse_target_nodes(passed_targets)",
            "            else:",
            "                target_nodes = await client._determine_nodes(",
            "                    *cmd.args, node_flag=passed_targets",
            "                )",
            "                if not target_nodes:",
            "                    raise RedisClusterException(",
            "                        f\"No targets were found to execute {cmd.args} command on\"",
            "                    )",
            "            if len(target_nodes) > 1:",
            "                raise RedisClusterException(f\"Too many targets for command {cmd.args}\")",
            "            node = target_nodes[0]",
            "            if node.name not in nodes:",
            "                nodes[node.name] = (node, [])",
            "            nodes[node.name][1].append(cmd)",
            "",
            "        errors = await asyncio.gather(",
            "            *(",
            "                asyncio.create_task(node[0].execute_pipeline(node[1]))",
            "                for node in nodes.values()",
            "            )",
            "        )",
            "",
            "        if any(errors):",
            "            if allow_redirections:",
            "                # send each errored command individually",
            "                for cmd in todo:",
            "                    if isinstance(cmd.result, (TryAgainError, MovedError, AskError)):",
            "                        try:",
            "                            cmd.result = await client.execute_command(",
            "                                *cmd.args, **cmd.kwargs",
            "                            )",
            "                        except Exception as e:",
            "                            cmd.result = e",
            "",
            "            if raise_on_error:",
            "                for cmd in todo:",
            "                    result = cmd.result",
            "                    if isinstance(result, Exception):",
            "                        command = \" \".join(map(safe_str, cmd.args))",
            "                        msg = (",
            "                            f\"Command # {cmd.position + 1} ({command}) of pipeline \"",
            "                            f\"caused error: {result.args}\"",
            "                        )",
            "                        result.args = (msg,) + result.args[1:]",
            "                        raise result",
            "",
            "            default_node = nodes.get(client.get_default_node().name)",
            "            if default_node is not None:",
            "                # This pipeline execution used the default node, check if we need",
            "                # to replace it.",
            "                # Note: when the error is raised we'll reset the default node in the",
            "                # caller function.",
            "                for cmd in default_node[1]:",
            "                    # Check if it has a command that failed with a relevant",
            "                    # exception",
            "                    if type(cmd.result) in self.__class__.ERRORS_ALLOW_RETRY:",
            "                        client.replace_default_node()",
            "                        break",
            "",
            "        return [cmd.result for cmd in stack]",
            "",
            "    def _split_command_across_slots(",
            "        self, command: str, *keys: KeyT",
            "    ) -> \"ClusterPipeline\":",
            "        for slot_keys in self._client._partition_keys_by_slot(keys).values():",
            "            self.execute_command(command, *slot_keys)",
            "",
            "        return self",
            "",
            "    def mset_nonatomic(",
            "        self, mapping: Mapping[AnyKeyT, EncodableT]",
            "    ) -> \"ClusterPipeline\":",
            "        encoder = self._client.encoder",
            "",
            "        slots_pairs = {}",
            "        for pair in mapping.items():",
            "            slot = key_slot(encoder.encode(pair[0]))",
            "            slots_pairs.setdefault(slot, []).extend(pair)",
            "",
            "        for pairs in slots_pairs.values():",
            "            self.execute_command(\"MSET\", *pairs)",
            "",
            "        return self",
            "",
            "",
            "for command in PIPELINE_BLOCKED_COMMANDS:",
            "    command = command.replace(\" \", \"_\").lower()",
            "    if command == \"mset_nonatomic\":",
            "        continue",
            "",
            "    setattr(ClusterPipeline, command, block_pipeline_command(command))",
            "",
            "",
            "class PipelineCommand:",
            "    def __init__(self, position: int, *args: Any, **kwargs: Any) -> None:",
            "        self.args = args",
            "        self.kwargs = kwargs",
            "        self.position = position",
            "        self.result: Union[Any, Exception] = None",
            "",
            "    def __repr__(self) -> str:",
            "        return f\"[{self.position}] {self.args} ({self.kwargs})\""
        ],
        "afterPatchFile": [
            "import asyncio",
            "import collections",
            "import random",
            "import socket",
            "import warnings",
            "from typing import (",
            "    Any,",
            "    Deque,",
            "    Dict,",
            "    Generator,",
            "    List,",
            "    Mapping,",
            "    Optional,",
            "    Type,",
            "    TypeVar,",
            "    Union,",
            ")",
            "",
            "from redis.asyncio.client import ResponseCallbackT",
            "from redis.asyncio.connection import (",
            "    Connection,",
            "    DefaultParser,",
            "    Encoder,",
            "    SSLConnection,",
            "    parse_url,",
            ")",
            "from redis.asyncio.lock import Lock",
            "from redis.asyncio.parser import CommandsParser",
            "from redis.asyncio.retry import Retry",
            "from redis.backoff import default_backoff",
            "from redis.client import EMPTY_RESPONSE, NEVER_DECODE, AbstractRedis",
            "from redis.cluster import (",
            "    PIPELINE_BLOCKED_COMMANDS,",
            "    PRIMARY,",
            "    REPLICA,",
            "    SLOT_ID,",
            "    AbstractRedisCluster,",
            "    LoadBalancer,",
            "    block_pipeline_command,",
            "    get_node_name,",
            "    parse_cluster_slots,",
            ")",
            "from redis.commands import READ_COMMANDS, AsyncRedisClusterCommands",
            "from redis.crc import REDIS_CLUSTER_HASH_SLOTS, key_slot",
            "from redis.credentials import CredentialProvider",
            "from redis.exceptions import (",
            "    AskError,",
            "    BusyLoadingError,",
            "    ClusterCrossSlotError,",
            "    ClusterDownError,",
            "    ClusterError,",
            "    ConnectionError,",
            "    DataError,",
            "    MasterDownError,",
            "    MaxConnectionsError,",
            "    MovedError,",
            "    RedisClusterException,",
            "    ResponseError,",
            "    SlotNotCoveredError,",
            "    TimeoutError,",
            "    TryAgainError,",
            ")",
            "from redis.typing import AnyKeyT, EncodableT, KeyT",
            "from redis.utils import dict_merge, safe_str, str_if_bytes",
            "",
            "TargetNodesT = TypeVar(",
            "    \"TargetNodesT\", str, \"ClusterNode\", List[\"ClusterNode\"], Dict[Any, \"ClusterNode\"]",
            ")",
            "",
            "",
            "class ClusterParser(DefaultParser):",
            "    EXCEPTION_CLASSES = dict_merge(",
            "        DefaultParser.EXCEPTION_CLASSES,",
            "        {",
            "            \"ASK\": AskError,",
            "            \"CLUSTERDOWN\": ClusterDownError,",
            "            \"CROSSSLOT\": ClusterCrossSlotError,",
            "            \"MASTERDOWN\": MasterDownError,",
            "            \"MOVED\": MovedError,",
            "            \"TRYAGAIN\": TryAgainError,",
            "        },",
            "    )",
            "",
            "",
            "class RedisCluster(AbstractRedis, AbstractRedisCluster, AsyncRedisClusterCommands):",
            "    \"\"\"",
            "    Create a new RedisCluster client.",
            "",
            "    Pass one of parameters:",
            "",
            "      - `host` & `port`",
            "      - `startup_nodes`",
            "",
            "    | Use ``await`` :meth:`initialize` to find cluster nodes & create connections.",
            "    | Use ``await`` :meth:`close` to disconnect connections & close client.",
            "",
            "    Many commands support the target_nodes kwarg. It can be one of the",
            "    :attr:`NODE_FLAGS`:",
            "",
            "      - :attr:`PRIMARIES`",
            "      - :attr:`REPLICAS`",
            "      - :attr:`ALL_NODES`",
            "      - :attr:`RANDOM`",
            "      - :attr:`DEFAULT_NODE`",
            "",
            "    Note: This client is not thread/process/fork safe.",
            "",
            "    :param host:",
            "        | Can be used to point to a startup node",
            "    :param port:",
            "        | Port used if **host** is provided",
            "    :param startup_nodes:",
            "        | :class:`~.ClusterNode` to used as a startup node",
            "    :param require_full_coverage:",
            "        | When set to ``False``: the client will not require a full coverage of",
            "          the slots. However, if not all slots are covered, and at least one node",
            "          has ``cluster-require-full-coverage`` set to ``yes``, the server will throw",
            "          a :class:`~.ClusterDownError` for some key-based commands.",
            "        | When set to ``True``: all slots must be covered to construct the cluster",
            "          client. If not all slots are covered, :class:`~.RedisClusterException` will be",
            "          thrown.",
            "        | See:",
            "          https://redis.io/docs/manual/scaling/#redis-cluster-configuration-parameters",
            "    :param read_from_replicas:",
            "        | Enable read from replicas in READONLY mode. You can read possibly stale data.",
            "          When set to true, read commands will be assigned between the primary and",
            "          its replications in a Round-Robin manner.",
            "    :param reinitialize_steps:",
            "        | Specifies the number of MOVED errors that need to occur before reinitializing",
            "          the whole cluster topology. If a MOVED error occurs and the cluster does not",
            "          need to be reinitialized on this current error handling, only the MOVED slot",
            "          will be patched with the redirected node.",
            "          To reinitialize the cluster on every MOVED error, set reinitialize_steps to 1.",
            "          To avoid reinitializing the cluster on moved errors, set reinitialize_steps to",
            "          0.",
            "    :param cluster_error_retry_attempts:",
            "        | Number of times to retry before raising an error when :class:`~.TimeoutError`",
            "          or :class:`~.ConnectionError` or :class:`~.ClusterDownError` are encountered",
            "    :param connection_error_retry_attempts:",
            "        | Number of times to retry before reinitializing when :class:`~.TimeoutError`",
            "          or :class:`~.ConnectionError` are encountered.",
            "          The default backoff strategy will be set if Retry object is not passed (see",
            "          default_backoff in backoff.py). To change it, pass a custom Retry object",
            "          using the \"retry\" keyword.",
            "    :param max_connections:",
            "        | Maximum number of connections per node. If there are no free connections & the",
            "          maximum number of connections are already created, a",
            "          :class:`~.MaxConnectionsError` is raised. This error may be retried as defined",
            "          by :attr:`connection_error_retry_attempts`",
            "",
            "    | Rest of the arguments will be passed to the",
            "      :class:`~redis.asyncio.connection.Connection` instances when created",
            "",
            "    :raises RedisClusterException:",
            "        if any arguments are invalid or unknown. Eg:",
            "",
            "        - `db` != 0 or None",
            "        - `path` argument for unix socket connection",
            "        - none of the `host`/`port` & `startup_nodes` were provided",
            "",
            "    \"\"\"",
            "",
            "    @classmethod",
            "    def from_url(cls, url: str, **kwargs: Any) -> \"RedisCluster\":",
            "        \"\"\"",
            "        Return a Redis client object configured from the given URL.",
            "",
            "        For example::",
            "",
            "            redis://[[username]:[password]]@localhost:6379/0",
            "            rediss://[[username]:[password]]@localhost:6379/0",
            "",
            "        Three URL schemes are supported:",
            "",
            "        - `redis://` creates a TCP socket connection. See more at:",
            "          <https://www.iana.org/assignments/uri-schemes/prov/redis>",
            "        - `rediss://` creates a SSL wrapped TCP socket connection. See more at:",
            "          <https://www.iana.org/assignments/uri-schemes/prov/rediss>",
            "",
            "        The username, password, hostname, path and all querystring values are passed",
            "        through ``urllib.parse.unquote`` in order to replace any percent-encoded values",
            "        with their corresponding characters.",
            "",
            "        All querystring options are cast to their appropriate Python types. Boolean",
            "        arguments can be specified with string values \"True\"/\"False\" or \"Yes\"/\"No\".",
            "        Values that cannot be properly cast cause a ``ValueError`` to be raised. Once",
            "        parsed, the querystring arguments and keyword arguments are passed to",
            "        :class:`~redis.asyncio.connection.Connection` when created.",
            "        In the case of conflicting arguments, querystring arguments are used.",
            "        \"\"\"",
            "        kwargs.update(parse_url(url))",
            "        if kwargs.pop(\"connection_class\", None) is SSLConnection:",
            "            kwargs[\"ssl\"] = True",
            "        return cls(**kwargs)",
            "",
            "    __slots__ = (",
            "        \"_initialize\",",
            "        \"_lock\",",
            "        \"cluster_error_retry_attempts\",",
            "        \"command_flags\",",
            "        \"commands_parser\",",
            "        \"connection_error_retry_attempts\",",
            "        \"connection_kwargs\",",
            "        \"encoder\",",
            "        \"node_flags\",",
            "        \"nodes_manager\",",
            "        \"read_from_replicas\",",
            "        \"reinitialize_counter\",",
            "        \"reinitialize_steps\",",
            "        \"response_callbacks\",",
            "        \"result_callbacks\",",
            "    )",
            "",
            "    def __init__(",
            "        self,",
            "        host: Optional[str] = None,",
            "        port: Union[str, int] = 6379,",
            "        # Cluster related kwargs",
            "        startup_nodes: Optional[List[\"ClusterNode\"]] = None,",
            "        require_full_coverage: bool = True,",
            "        read_from_replicas: bool = False,",
            "        reinitialize_steps: int = 5,",
            "        cluster_error_retry_attempts: int = 3,",
            "        connection_error_retry_attempts: int = 3,",
            "        max_connections: int = 2**31,",
            "        # Client related kwargs",
            "        db: Union[str, int] = 0,",
            "        path: Optional[str] = None,",
            "        credential_provider: Optional[CredentialProvider] = None,",
            "        username: Optional[str] = None,",
            "        password: Optional[str] = None,",
            "        client_name: Optional[str] = None,",
            "        # Encoding related kwargs",
            "        encoding: str = \"utf-8\",",
            "        encoding_errors: str = \"strict\",",
            "        decode_responses: bool = False,",
            "        # Connection related kwargs",
            "        health_check_interval: float = 0,",
            "        socket_connect_timeout: Optional[float] = None,",
            "        socket_keepalive: bool = False,",
            "        socket_keepalive_options: Optional[Mapping[int, Union[int, bytes]]] = None,",
            "        socket_timeout: Optional[float] = None,",
            "        retry: Optional[\"Retry\"] = None,",
            "        retry_on_error: Optional[List[Exception]] = None,",
            "        # SSL related kwargs",
            "        ssl: bool = False,",
            "        ssl_ca_certs: Optional[str] = None,",
            "        ssl_ca_data: Optional[str] = None,",
            "        ssl_cert_reqs: str = \"required\",",
            "        ssl_certfile: Optional[str] = None,",
            "        ssl_check_hostname: bool = False,",
            "        ssl_keyfile: Optional[str] = None,",
            "    ) -> None:",
            "        if db:",
            "            raise RedisClusterException(",
            "                \"Argument 'db' must be 0 or None in cluster mode\"",
            "            )",
            "",
            "        if path:",
            "            raise RedisClusterException(",
            "                \"Unix domain socket is not supported in cluster mode\"",
            "            )",
            "",
            "        if (not host or not port) and not startup_nodes:",
            "            raise RedisClusterException(",
            "                \"RedisCluster requires at least one node to discover the cluster.\\n\"",
            "                \"Please provide one of the following or use RedisCluster.from_url:\\n\"",
            "                '   - host and port: RedisCluster(host=\"localhost\", port=6379)\\n'",
            "                \"   - startup_nodes: RedisCluster(startup_nodes=[\"",
            "                'ClusterNode(\"localhost\", 6379), ClusterNode(\"localhost\", 6380)])'",
            "            )",
            "",
            "        kwargs: Dict[str, Any] = {",
            "            \"max_connections\": max_connections,",
            "            \"connection_class\": Connection,",
            "            \"parser_class\": ClusterParser,",
            "            # Client related kwargs",
            "            \"credential_provider\": credential_provider,",
            "            \"username\": username,",
            "            \"password\": password,",
            "            \"client_name\": client_name,",
            "            # Encoding related kwargs",
            "            \"encoding\": encoding,",
            "            \"encoding_errors\": encoding_errors,",
            "            \"decode_responses\": decode_responses,",
            "            # Connection related kwargs",
            "            \"health_check_interval\": health_check_interval,",
            "            \"socket_connect_timeout\": socket_connect_timeout,",
            "            \"socket_keepalive\": socket_keepalive,",
            "            \"socket_keepalive_options\": socket_keepalive_options,",
            "            \"socket_timeout\": socket_timeout,",
            "            \"retry\": retry,",
            "        }",
            "",
            "        if ssl:",
            "            # SSL related kwargs",
            "            kwargs.update(",
            "                {",
            "                    \"connection_class\": SSLConnection,",
            "                    \"ssl_ca_certs\": ssl_ca_certs,",
            "                    \"ssl_ca_data\": ssl_ca_data,",
            "                    \"ssl_cert_reqs\": ssl_cert_reqs,",
            "                    \"ssl_certfile\": ssl_certfile,",
            "                    \"ssl_check_hostname\": ssl_check_hostname,",
            "                    \"ssl_keyfile\": ssl_keyfile,",
            "                }",
            "            )",
            "",
            "        if read_from_replicas:",
            "            # Call our on_connect function to configure READONLY mode",
            "            kwargs[\"redis_connect_func\"] = self.on_connect",
            "",
            "        self.retry = retry",
            "        if retry or retry_on_error or connection_error_retry_attempts > 0:",
            "            # Set a retry object for all cluster nodes",
            "            self.retry = retry or Retry(",
            "                default_backoff(), connection_error_retry_attempts",
            "            )",
            "            if not retry_on_error:",
            "                # Default errors for retrying",
            "                retry_on_error = [ConnectionError, TimeoutError]",
            "            self.retry.update_supported_errors(retry_on_error)",
            "            kwargs.update({\"retry\": self.retry})",
            "",
            "        kwargs[\"response_callbacks\"] = self.__class__.RESPONSE_CALLBACKS.copy()",
            "        self.connection_kwargs = kwargs",
            "",
            "        if startup_nodes:",
            "            passed_nodes = []",
            "            for node in startup_nodes:",
            "                passed_nodes.append(",
            "                    ClusterNode(node.host, node.port, **self.connection_kwargs)",
            "                )",
            "            startup_nodes = passed_nodes",
            "        else:",
            "            startup_nodes = []",
            "        if host and port:",
            "            startup_nodes.append(ClusterNode(host, port, **self.connection_kwargs))",
            "",
            "        self.nodes_manager = NodesManager(startup_nodes, require_full_coverage, kwargs)",
            "        self.encoder = Encoder(encoding, encoding_errors, decode_responses)",
            "        self.read_from_replicas = read_from_replicas",
            "        self.reinitialize_steps = reinitialize_steps",
            "        self.cluster_error_retry_attempts = cluster_error_retry_attempts",
            "        self.connection_error_retry_attempts = connection_error_retry_attempts",
            "        self.reinitialize_counter = 0",
            "        self.commands_parser = CommandsParser()",
            "        self.node_flags = self.__class__.NODE_FLAGS.copy()",
            "        self.command_flags = self.__class__.COMMAND_FLAGS.copy()",
            "        self.response_callbacks = kwargs[\"response_callbacks\"]",
            "        self.result_callbacks = self.__class__.RESULT_CALLBACKS.copy()",
            "        self.result_callbacks[",
            "            \"CLUSTER SLOTS\"",
            "        ] = lambda cmd, res, **kwargs: parse_cluster_slots(",
            "            list(res.values())[0], **kwargs",
            "        )",
            "",
            "        self._initialize = True",
            "        self._lock: Optional[asyncio.Lock] = None",
            "",
            "    async def initialize(self) -> \"RedisCluster\":",
            "        \"\"\"Get all nodes from startup nodes & creates connections if not initialized.\"\"\"",
            "        if self._initialize:",
            "            if not self._lock:",
            "                self._lock = asyncio.Lock()",
            "            async with self._lock:",
            "                if self._initialize:",
            "                    try:",
            "                        await self.nodes_manager.initialize()",
            "                        await self.commands_parser.initialize(",
            "                            self.nodes_manager.default_node",
            "                        )",
            "                        self._initialize = False",
            "                    except BaseException:",
            "                        await self.nodes_manager.close()",
            "                        await self.nodes_manager.close(\"startup_nodes\")",
            "                        raise",
            "        return self",
            "",
            "    async def close(self) -> None:",
            "        \"\"\"Close all connections & client if initialized.\"\"\"",
            "        if not self._initialize:",
            "            if not self._lock:",
            "                self._lock = asyncio.Lock()",
            "            async with self._lock:",
            "                if not self._initialize:",
            "                    self._initialize = True",
            "                    await self.nodes_manager.close()",
            "                    await self.nodes_manager.close(\"startup_nodes\")",
            "",
            "    async def __aenter__(self) -> \"RedisCluster\":",
            "        return await self.initialize()",
            "",
            "    async def __aexit__(self, exc_type: None, exc_value: None, traceback: None) -> None:",
            "        await self.close()",
            "",
            "    def __await__(self) -> Generator[Any, None, \"RedisCluster\"]:",
            "        return self.initialize().__await__()",
            "",
            "    _DEL_MESSAGE = \"Unclosed RedisCluster client\"",
            "",
            "    def __del__(self) -> None:",
            "        if hasattr(self, \"_initialize\") and not self._initialize:",
            "            warnings.warn(f\"{self._DEL_MESSAGE} {self!r}\", ResourceWarning, source=self)",
            "            try:",
            "                context = {\"client\": self, \"message\": self._DEL_MESSAGE}",
            "                asyncio.get_running_loop().call_exception_handler(context)",
            "            except RuntimeError:",
            "                ...",
            "",
            "    async def on_connect(self, connection: Connection) -> None:",
            "        await connection.on_connect()",
            "",
            "        # Sending READONLY command to server to configure connection as",
            "        # readonly. Since each cluster node may change its server type due",
            "        # to a failover, we should establish a READONLY connection",
            "        # regardless of the server type. If this is a primary connection,",
            "        # READONLY would not affect executing write commands.",
            "        await connection.send_command(\"READONLY\")",
            "        if str_if_bytes(await connection.read_response()) != \"OK\":",
            "            raise ConnectionError(\"READONLY command failed\")",
            "",
            "    def get_nodes(self) -> List[\"ClusterNode\"]:",
            "        \"\"\"Get all nodes of the cluster.\"\"\"",
            "        return list(self.nodes_manager.nodes_cache.values())",
            "",
            "    def get_primaries(self) -> List[\"ClusterNode\"]:",
            "        \"\"\"Get the primary nodes of the cluster.\"\"\"",
            "        return self.nodes_manager.get_nodes_by_server_type(PRIMARY)",
            "",
            "    def get_replicas(self) -> List[\"ClusterNode\"]:",
            "        \"\"\"Get the replica nodes of the cluster.\"\"\"",
            "        return self.nodes_manager.get_nodes_by_server_type(REPLICA)",
            "",
            "    def get_random_node(self) -> \"ClusterNode\":",
            "        \"\"\"Get a random node of the cluster.\"\"\"",
            "        return random.choice(list(self.nodes_manager.nodes_cache.values()))",
            "",
            "    def get_default_node(self) -> \"ClusterNode\":",
            "        \"\"\"Get the default node of the client.\"\"\"",
            "        return self.nodes_manager.default_node",
            "",
            "    def set_default_node(self, node: \"ClusterNode\") -> None:",
            "        \"\"\"",
            "        Set the default node of the client.",
            "",
            "        :raises DataError: if None is passed or node does not exist in cluster.",
            "        \"\"\"",
            "        if not node or not self.get_node(node_name=node.name):",
            "            raise DataError(\"The requested node does not exist in the cluster.\")",
            "",
            "        self.nodes_manager.default_node = node",
            "",
            "    def get_node(",
            "        self,",
            "        host: Optional[str] = None,",
            "        port: Optional[int] = None,",
            "        node_name: Optional[str] = None,",
            "    ) -> Optional[\"ClusterNode\"]:",
            "        \"\"\"Get node by (host, port) or node_name.\"\"\"",
            "        return self.nodes_manager.get_node(host, port, node_name)",
            "",
            "    def get_node_from_key(",
            "        self, key: str, replica: bool = False",
            "    ) -> Optional[\"ClusterNode\"]:",
            "        \"\"\"",
            "        Get the cluster node corresponding to the provided key.",
            "",
            "        :param key:",
            "        :param replica:",
            "            | Indicates if a replica should be returned",
            "            |",
            "              None will returned if no replica holds this key",
            "",
            "        :raises SlotNotCoveredError: if the key is not covered by any slot.",
            "        \"\"\"",
            "        slot = self.keyslot(key)",
            "        slot_cache = self.nodes_manager.slots_cache.get(slot)",
            "        if not slot_cache:",
            "            raise SlotNotCoveredError(f'Slot \"{slot}\" is not covered by the cluster.')",
            "",
            "        if replica:",
            "            if len(self.nodes_manager.slots_cache[slot]) < 2:",
            "                return None",
            "            node_idx = 1",
            "        else:",
            "            node_idx = 0",
            "",
            "        return slot_cache[node_idx]",
            "",
            "    def keyslot(self, key: EncodableT) -> int:",
            "        \"\"\"",
            "        Find the keyslot for a given key.",
            "",
            "        See: https://redis.io/docs/manual/scaling/#redis-cluster-data-sharding",
            "        \"\"\"",
            "        return key_slot(self.encoder.encode(key))",
            "",
            "    def get_encoder(self) -> Encoder:",
            "        \"\"\"Get the encoder object of the client.\"\"\"",
            "        return self.encoder",
            "",
            "    def get_connection_kwargs(self) -> Dict[str, Optional[Any]]:",
            "        \"\"\"Get the kwargs passed to :class:`~redis.asyncio.connection.Connection`.\"\"\"",
            "        return self.connection_kwargs",
            "",
            "    def get_retry(self) -> Optional[\"Retry\"]:",
            "        return self.retry",
            "",
            "    def set_retry(self, retry: \"Retry\") -> None:",
            "        self.retry = retry",
            "        for node in self.get_nodes():",
            "            node.connection_kwargs.update({\"retry\": retry})",
            "            for conn in node._connections:",
            "                conn.retry = retry",
            "",
            "    def set_response_callback(self, command: str, callback: ResponseCallbackT) -> None:",
            "        \"\"\"Set a custom response callback.\"\"\"",
            "        self.response_callbacks[command] = callback",
            "",
            "    async def _determine_nodes(",
            "        self, command: str, *args: Any, node_flag: Optional[str] = None",
            "    ) -> List[\"ClusterNode\"]:",
            "        # Determine which nodes should be executed the command on.",
            "        # Returns a list of target nodes.",
            "        if not node_flag:",
            "            # get the nodes group for this command if it was predefined",
            "            node_flag = self.command_flags.get(command)",
            "",
            "        if node_flag in self.node_flags:",
            "            if node_flag == self.__class__.DEFAULT_NODE:",
            "                # return the cluster's default node",
            "                return [self.nodes_manager.default_node]",
            "            if node_flag == self.__class__.PRIMARIES:",
            "                # return all primaries",
            "                return self.nodes_manager.get_nodes_by_server_type(PRIMARY)",
            "            if node_flag == self.__class__.REPLICAS:",
            "                # return all replicas",
            "                return self.nodes_manager.get_nodes_by_server_type(REPLICA)",
            "            if node_flag == self.__class__.ALL_NODES:",
            "                # return all nodes",
            "                return list(self.nodes_manager.nodes_cache.values())",
            "            if node_flag == self.__class__.RANDOM:",
            "                # return a random node",
            "                return [random.choice(list(self.nodes_manager.nodes_cache.values()))]",
            "",
            "        # get the node that holds the key's slot",
            "        return [",
            "            self.nodes_manager.get_node_from_slot(",
            "                await self._determine_slot(command, *args),",
            "                self.read_from_replicas and command in READ_COMMANDS,",
            "            )",
            "        ]",
            "",
            "    async def _determine_slot(self, command: str, *args: Any) -> int:",
            "        if self.command_flags.get(command) == SLOT_ID:",
            "            # The command contains the slot ID",
            "            return int(args[0])",
            "",
            "        # Get the keys in the command",
            "",
            "        # EVAL and EVALSHA are common enough that it's wasteful to go to the",
            "        # redis server to parse the keys. Besides, there is a bug in redis<7.0",
            "        # where `self._get_command_keys()` fails anyway. So, we special case",
            "        # EVAL/EVALSHA.",
            "        # - issue: https://github.com/redis/redis/issues/9493",
            "        # - fix: https://github.com/redis/redis/pull/9733",
            "        if command in (\"EVAL\", \"EVALSHA\"):",
            "            # command syntax: EVAL \"script body\" num_keys ...",
            "            if len(args) < 2:",
            "                raise RedisClusterException(",
            "                    f\"Invalid args in command: {command, *args}\"",
            "                )",
            "            keys = args[2 : 2 + args[1]]",
            "            # if there are 0 keys, that means the script can be run on any node",
            "            # so we can just return a random slot",
            "            if not keys:",
            "                return random.randrange(0, REDIS_CLUSTER_HASH_SLOTS)",
            "        else:",
            "            keys = await self.commands_parser.get_keys(command, *args)",
            "            if not keys:",
            "                # FCALL can call a function with 0 keys, that means the function",
            "                #  can be run on any node so we can just return a random slot",
            "                if command in (\"FCALL\", \"FCALL_RO\"):",
            "                    return random.randrange(0, REDIS_CLUSTER_HASH_SLOTS)",
            "                raise RedisClusterException(",
            "                    \"No way to dispatch this command to Redis Cluster. \"",
            "                    \"Missing key.\\nYou can execute the command by specifying \"",
            "                    f\"target nodes.\\nCommand: {args}\"",
            "                )",
            "",
            "        # single key command",
            "        if len(keys) == 1:",
            "            return self.keyslot(keys[0])",
            "",
            "        # multi-key command; we need to make sure all keys are mapped to",
            "        # the same slot",
            "        slots = {self.keyslot(key) for key in keys}",
            "        if len(slots) != 1:",
            "            raise RedisClusterException(",
            "                f\"{command} - all keys must map to the same key slot\"",
            "            )",
            "",
            "        return slots.pop()",
            "",
            "    def _is_node_flag(self, target_nodes: Any) -> bool:",
            "        return isinstance(target_nodes, str) and target_nodes in self.node_flags",
            "",
            "    def _parse_target_nodes(self, target_nodes: Any) -> List[\"ClusterNode\"]:",
            "        if isinstance(target_nodes, list):",
            "            nodes = target_nodes",
            "        elif isinstance(target_nodes, ClusterNode):",
            "            # Supports passing a single ClusterNode as a variable",
            "            nodes = [target_nodes]",
            "        elif isinstance(target_nodes, dict):",
            "            # Supports dictionaries of the format {node_name: node}.",
            "            # It enables to execute commands with multi nodes as follows:",
            "            # rc.cluster_save_config(rc.get_primaries())",
            "            nodes = list(target_nodes.values())",
            "        else:",
            "            raise TypeError(",
            "                \"target_nodes type can be one of the following: \"",
            "                \"node_flag (PRIMARIES, REPLICAS, RANDOM, ALL_NODES),\"",
            "                \"ClusterNode, list<ClusterNode>, or dict<any, ClusterNode>. \"",
            "                f\"The passed type is {type(target_nodes)}\"",
            "            )",
            "        return nodes",
            "",
            "    async def execute_command(self, *args: EncodableT, **kwargs: Any) -> Any:",
            "        \"\"\"",
            "        Execute a raw command on the appropriate cluster node or target_nodes.",
            "",
            "        It will retry the command as specified by :attr:`cluster_error_retry_attempts` &",
            "        then raise an exception.",
            "",
            "        :param args:",
            "            | Raw command args",
            "        :param kwargs:",
            "",
            "            - target_nodes: :attr:`NODE_FLAGS` or :class:`~.ClusterNode`",
            "              or List[:class:`~.ClusterNode`] or Dict[Any, :class:`~.ClusterNode`]",
            "            - Rest of the kwargs are passed to the Redis connection",
            "",
            "        :raises RedisClusterException: if target_nodes is not provided & the command",
            "            can't be mapped to a slot",
            "        \"\"\"",
            "        command = args[0]",
            "        target_nodes = []",
            "        target_nodes_specified = False",
            "        retry_attempts = self.cluster_error_retry_attempts",
            "",
            "        passed_targets = kwargs.pop(\"target_nodes\", None)",
            "        if passed_targets and not self._is_node_flag(passed_targets):",
            "            target_nodes = self._parse_target_nodes(passed_targets)",
            "            target_nodes_specified = True",
            "            retry_attempts = 0",
            "",
            "        # Add one for the first execution",
            "        execute_attempts = 1 + retry_attempts",
            "        for _ in range(execute_attempts):",
            "            if self._initialize:",
            "                await self.initialize()",
            "                if (",
            "                    len(target_nodes) == 1",
            "                    and target_nodes[0] == self.get_default_node()",
            "                ):",
            "                    # Replace the default cluster node",
            "                    self.replace_default_node()",
            "            try:",
            "                if not target_nodes_specified:",
            "                    # Determine the nodes to execute the command on",
            "                    target_nodes = await self._determine_nodes(",
            "                        *args, node_flag=passed_targets",
            "                    )",
            "                    if not target_nodes:",
            "                        raise RedisClusterException(",
            "                            f\"No targets were found to execute {args} command on\"",
            "                        )",
            "",
            "                if len(target_nodes) == 1:",
            "                    # Return the processed result",
            "                    ret = await self._execute_command(target_nodes[0], *args, **kwargs)",
            "                    if command in self.result_callbacks:",
            "                        return self.result_callbacks[command](",
            "                            command, {target_nodes[0].name: ret}, **kwargs",
            "                        )",
            "                    return ret",
            "                else:",
            "                    keys = [node.name for node in target_nodes]",
            "                    values = await asyncio.gather(",
            "                        *(",
            "                            asyncio.create_task(",
            "                                self._execute_command(node, *args, **kwargs)",
            "                            )",
            "                            for node in target_nodes",
            "                        )",
            "                    )",
            "                    if command in self.result_callbacks:",
            "                        return self.result_callbacks[command](",
            "                            command, dict(zip(keys, values)), **kwargs",
            "                        )",
            "                    return dict(zip(keys, values))",
            "            except Exception as e:",
            "                if retry_attempts > 0 and type(e) in self.__class__.ERRORS_ALLOW_RETRY:",
            "                    # The nodes and slots cache were should be reinitialized.",
            "                    # Try again with the new cluster setup.",
            "                    retry_attempts -= 1",
            "                    continue",
            "                else:",
            "                    # raise the exception",
            "                    raise e",
            "",
            "    async def _execute_command(",
            "        self, target_node: \"ClusterNode\", *args: Union[KeyT, EncodableT], **kwargs: Any",
            "    ) -> Any:",
            "        asking = moved = False",
            "        redirect_addr = None",
            "        ttl = self.RedisClusterRequestTTL",
            "",
            "        while ttl > 0:",
            "            ttl -= 1",
            "            try:",
            "                if asking:",
            "                    target_node = self.get_node(node_name=redirect_addr)",
            "                    await target_node.execute_command(\"ASKING\")",
            "                    asking = False",
            "                elif moved:",
            "                    # MOVED occurred and the slots cache was updated,",
            "                    # refresh the target node",
            "                    slot = await self._determine_slot(*args)",
            "                    target_node = self.nodes_manager.get_node_from_slot(",
            "                        slot, self.read_from_replicas and args[0] in READ_COMMANDS",
            "                    )",
            "                    moved = False",
            "",
            "                return await target_node.execute_command(*args, **kwargs)",
            "            except (BusyLoadingError, MaxConnectionsError):",
            "                raise",
            "            except (ConnectionError, TimeoutError):",
            "                # Connection retries are being handled in the node's",
            "                # Retry object.",
            "                # Remove the failed node from the startup nodes before we try",
            "                # to reinitialize the cluster",
            "                self.nodes_manager.startup_nodes.pop(target_node.name, None)",
            "                # Hard force of reinitialize of the node/slots setup",
            "                # and try again with the new setup",
            "                await self.close()",
            "                raise",
            "            except ClusterDownError:",
            "                # ClusterDownError can occur during a failover and to get",
            "                # self-healed, we will try to reinitialize the cluster layout",
            "                # and retry executing the command",
            "                await self.close()",
            "                await asyncio.sleep(0.25)",
            "                raise",
            "            except MovedError as e:",
            "                # First, we will try to patch the slots/nodes cache with the",
            "                # redirected node output and try again. If MovedError exceeds",
            "                # 'reinitialize_steps' number of times, we will force",
            "                # reinitializing the tables, and then try again.",
            "                # 'reinitialize_steps' counter will increase faster when",
            "                # the same client object is shared between multiple threads. To",
            "                # reduce the frequency you can set this variable in the",
            "                # RedisCluster constructor.",
            "                self.reinitialize_counter += 1",
            "                if (",
            "                    self.reinitialize_steps",
            "                    and self.reinitialize_counter % self.reinitialize_steps == 0",
            "                ):",
            "                    await self.close()",
            "                    # Reset the counter",
            "                    self.reinitialize_counter = 0",
            "                else:",
            "                    self.nodes_manager._moved_exception = e",
            "                moved = True",
            "            except AskError as e:",
            "                redirect_addr = get_node_name(host=e.host, port=e.port)",
            "                asking = True",
            "            except TryAgainError:",
            "                if ttl < self.RedisClusterRequestTTL / 2:",
            "                    await asyncio.sleep(0.05)",
            "",
            "        raise ClusterError(\"TTL exhausted.\")",
            "",
            "    def pipeline(",
            "        self, transaction: Optional[Any] = None, shard_hint: Optional[Any] = None",
            "    ) -> \"ClusterPipeline\":",
            "        \"\"\"",
            "        Create & return a new :class:`~.ClusterPipeline` object.",
            "",
            "        Cluster implementation of pipeline does not support transaction or shard_hint.",
            "",
            "        :raises RedisClusterException: if transaction or shard_hint are truthy values",
            "        \"\"\"",
            "        if shard_hint:",
            "            raise RedisClusterException(\"shard_hint is deprecated in cluster mode\")",
            "",
            "        if transaction:",
            "            raise RedisClusterException(\"transaction is deprecated in cluster mode\")",
            "",
            "        return ClusterPipeline(self)",
            "",
            "    def lock(",
            "        self,",
            "        name: KeyT,",
            "        timeout: Optional[float] = None,",
            "        sleep: float = 0.1,",
            "        blocking: bool = True,",
            "        blocking_timeout: Optional[float] = None,",
            "        lock_class: Optional[Type[Lock]] = None,",
            "        thread_local: bool = True,",
            "    ) -> Lock:",
            "        \"\"\"",
            "        Return a new Lock object using key ``name`` that mimics",
            "        the behavior of threading.Lock.",
            "",
            "        If specified, ``timeout`` indicates a maximum life for the lock.",
            "        By default, it will remain locked until release() is called.",
            "",
            "        ``sleep`` indicates the amount of time to sleep per loop iteration",
            "        when the lock is in blocking mode and another client is currently",
            "        holding the lock.",
            "",
            "        ``blocking`` indicates whether calling ``acquire`` should block until",
            "        the lock has been acquired or to fail immediately, causing ``acquire``",
            "        to return False and the lock not being acquired. Defaults to True.",
            "        Note this value can be overridden by passing a ``blocking``",
            "        argument to ``acquire``.",
            "",
            "        ``blocking_timeout`` indicates the maximum amount of time in seconds to",
            "        spend trying to acquire the lock. A value of ``None`` indicates",
            "        continue trying forever. ``blocking_timeout`` can be specified as a",
            "        float or integer, both representing the number of seconds to wait.",
            "",
            "        ``lock_class`` forces the specified lock implementation. Note that as",
            "        of redis-py 3.0, the only lock class we implement is ``Lock`` (which is",
            "        a Lua-based lock). So, it's unlikely you'll need this parameter, unless",
            "        you have created your own custom lock class.",
            "",
            "        ``thread_local`` indicates whether the lock token is placed in",
            "        thread-local storage. By default, the token is placed in thread local",
            "        storage so that a thread only sees its token, not a token set by",
            "        another thread. Consider the following timeline:",
            "",
            "            time: 0, thread-1 acquires `my-lock`, with a timeout of 5 seconds.",
            "                     thread-1 sets the token to \"abc\"",
            "            time: 1, thread-2 blocks trying to acquire `my-lock` using the",
            "                     Lock instance.",
            "            time: 5, thread-1 has not yet completed. redis expires the lock",
            "                     key.",
            "            time: 5, thread-2 acquired `my-lock` now that it's available.",
            "                     thread-2 sets the token to \"xyz\"",
            "            time: 6, thread-1 finishes its work and calls release(). if the",
            "                     token is *not* stored in thread local storage, then",
            "                     thread-1 would see the token value as \"xyz\" and would be",
            "                     able to successfully release the thread-2's lock.",
            "",
            "        In some use cases it's necessary to disable thread local storage. For",
            "        example, if you have code where one thread acquires a lock and passes",
            "        that lock instance to a worker thread to release later. If thread",
            "        local storage isn't disabled in this case, the worker thread won't see",
            "        the token set by the thread that acquired the lock. Our assumption",
            "        is that these cases aren't common and as such default to using",
            "        thread local storage.\"\"\"",
            "        if lock_class is None:",
            "            lock_class = Lock",
            "        return lock_class(",
            "            self,",
            "            name,",
            "            timeout=timeout,",
            "            sleep=sleep,",
            "            blocking=blocking,",
            "            blocking_timeout=blocking_timeout,",
            "            thread_local=thread_local,",
            "        )",
            "",
            "",
            "class ClusterNode:",
            "    \"\"\"",
            "    Create a new ClusterNode.",
            "",
            "    Each ClusterNode manages multiple :class:`~redis.asyncio.connection.Connection`",
            "    objects for the (host, port).",
            "    \"\"\"",
            "",
            "    __slots__ = (",
            "        \"_connections\",",
            "        \"_free\",",
            "        \"connection_class\",",
            "        \"connection_kwargs\",",
            "        \"host\",",
            "        \"max_connections\",",
            "        \"name\",",
            "        \"port\",",
            "        \"response_callbacks\",",
            "        \"server_type\",",
            "    )",
            "",
            "    def __init__(",
            "        self,",
            "        host: str,",
            "        port: Union[str, int],",
            "        server_type: Optional[str] = None,",
            "        *,",
            "        max_connections: int = 2**31,",
            "        connection_class: Type[Connection] = Connection,",
            "        **connection_kwargs: Any,",
            "    ) -> None:",
            "        if host == \"localhost\":",
            "            host = socket.gethostbyname(host)",
            "",
            "        connection_kwargs[\"host\"] = host",
            "        connection_kwargs[\"port\"] = port",
            "        self.host = host",
            "        self.port = port",
            "        self.name = get_node_name(host, port)",
            "        self.server_type = server_type",
            "",
            "        self.max_connections = max_connections",
            "        self.connection_class = connection_class",
            "        self.connection_kwargs = connection_kwargs",
            "        self.response_callbacks = connection_kwargs.pop(\"response_callbacks\", {})",
            "",
            "        self._connections: List[Connection] = []",
            "        self._free: Deque[Connection] = collections.deque(maxlen=self.max_connections)",
            "",
            "    def __repr__(self) -> str:",
            "        return (",
            "            f\"[host={self.host}, port={self.port}, \"",
            "            f\"name={self.name}, server_type={self.server_type}]\"",
            "        )",
            "",
            "    def __eq__(self, obj: Any) -> bool:",
            "        return isinstance(obj, ClusterNode) and obj.name == self.name",
            "",
            "    _DEL_MESSAGE = \"Unclosed ClusterNode object\"",
            "",
            "    def __del__(self) -> None:",
            "        for connection in self._connections:",
            "            if connection.is_connected:",
            "                warnings.warn(",
            "                    f\"{self._DEL_MESSAGE} {self!r}\", ResourceWarning, source=self",
            "                )",
            "                try:",
            "                    context = {\"client\": self, \"message\": self._DEL_MESSAGE}",
            "                    asyncio.get_running_loop().call_exception_handler(context)",
            "                except RuntimeError:",
            "                    ...",
            "                break",
            "",
            "    async def disconnect(self) -> None:",
            "        ret = await asyncio.gather(",
            "            *(",
            "                asyncio.create_task(connection.disconnect())",
            "                for connection in self._connections",
            "            ),",
            "            return_exceptions=True,",
            "        )",
            "        exc = next((res for res in ret if isinstance(res, Exception)), None)",
            "        if exc:",
            "            raise exc",
            "",
            "    def acquire_connection(self) -> Connection:",
            "        try:",
            "            return self._free.popleft()",
            "        except IndexError:",
            "            if len(self._connections) < self.max_connections:",
            "                connection = self.connection_class(**self.connection_kwargs)",
            "                self._connections.append(connection)",
            "                return connection",
            "",
            "            raise MaxConnectionsError()",
            "",
            "    async def parse_response(",
            "        self, connection: Connection, command: str, **kwargs: Any",
            "    ) -> Any:",
            "        try:",
            "            if NEVER_DECODE in kwargs:",
            "                response = await connection.read_response(disable_decoding=True)",
            "                kwargs.pop(NEVER_DECODE)",
            "            else:",
            "                response = await connection.read_response()",
            "        except ResponseError:",
            "            if EMPTY_RESPONSE in kwargs:",
            "                return kwargs[EMPTY_RESPONSE]",
            "            raise",
            "",
            "        if EMPTY_RESPONSE in kwargs:",
            "            kwargs.pop(EMPTY_RESPONSE)",
            "",
            "        # Return response",
            "        if command in self.response_callbacks:",
            "            return self.response_callbacks[command](response, **kwargs)",
            "",
            "        return response",
            "",
            "    async def execute_command(self, *args: Any, **kwargs: Any) -> Any:",
            "        # Acquire connection",
            "        connection = self.acquire_connection()",
            "",
            "        # Execute command",
            "        await connection.send_packed_command(connection.pack_command(*args), False)",
            "",
            "        # Read response",
            "        return await asyncio.shield(",
            "            self._parse_and_release(connection, args[0], **kwargs)",
            "        )",
            "",
            "    async def _parse_and_release(self, connection, *args, **kwargs):",
            "        try:",
            "            return await self.parse_response(connection, *args, **kwargs)",
            "        except asyncio.CancelledError:",
            "            # should not be possible",
            "            await connection.disconnect(nowait=True)",
            "            raise",
            "        finally:",
            "            self._free.append(connection)",
            "",
            "    async def execute_pipeline(self, commands: List[\"PipelineCommand\"]) -> bool:",
            "        # Acquire connection",
            "        connection = self.acquire_connection()",
            "",
            "        # Execute command",
            "        await connection.send_packed_command(",
            "            connection.pack_commands(cmd.args for cmd in commands), False",
            "        )",
            "",
            "        # Read responses",
            "        ret = False",
            "        for cmd in commands:",
            "            try:",
            "                cmd.result = await self.parse_response(",
            "                    connection, cmd.args[0], **cmd.kwargs",
            "                )",
            "            except Exception as e:",
            "                cmd.result = e",
            "                ret = True",
            "",
            "        # Release connection",
            "        self._free.append(connection)",
            "",
            "        return ret",
            "",
            "",
            "class NodesManager:",
            "    __slots__ = (",
            "        \"_moved_exception\",",
            "        \"connection_kwargs\",",
            "        \"default_node\",",
            "        \"nodes_cache\",",
            "        \"read_load_balancer\",",
            "        \"require_full_coverage\",",
            "        \"slots_cache\",",
            "        \"startup_nodes\",",
            "    )",
            "",
            "    def __init__(",
            "        self,",
            "        startup_nodes: List[\"ClusterNode\"],",
            "        require_full_coverage: bool,",
            "        connection_kwargs: Dict[str, Any],",
            "    ) -> None:",
            "        self.startup_nodes = {node.name: node for node in startup_nodes}",
            "        self.require_full_coverage = require_full_coverage",
            "        self.connection_kwargs = connection_kwargs",
            "",
            "        self.default_node: \"ClusterNode\" = None",
            "        self.nodes_cache: Dict[str, \"ClusterNode\"] = {}",
            "        self.slots_cache: Dict[int, List[\"ClusterNode\"]] = {}",
            "        self.read_load_balancer = LoadBalancer()",
            "        self._moved_exception: MovedError = None",
            "",
            "    def get_node(",
            "        self,",
            "        host: Optional[str] = None,",
            "        port: Optional[int] = None,",
            "        node_name: Optional[str] = None,",
            "    ) -> Optional[\"ClusterNode\"]:",
            "        if host and port:",
            "            # the user passed host and port",
            "            if host == \"localhost\":",
            "                host = socket.gethostbyname(host)",
            "            return self.nodes_cache.get(get_node_name(host=host, port=port))",
            "        elif node_name:",
            "            return self.nodes_cache.get(node_name)",
            "        else:",
            "            raise DataError(",
            "                \"get_node requires one of the following: \"",
            "                \"1. node name \"",
            "                \"2. host and port\"",
            "            )",
            "",
            "    def set_nodes(",
            "        self,",
            "        old: Dict[str, \"ClusterNode\"],",
            "        new: Dict[str, \"ClusterNode\"],",
            "        remove_old: bool = False,",
            "    ) -> None:",
            "        if remove_old:",
            "            for name in list(old.keys()):",
            "                if name not in new:",
            "                    asyncio.create_task(old.pop(name).disconnect())",
            "",
            "        for name, node in new.items():",
            "            if name in old:",
            "                if old[name] is node:",
            "                    continue",
            "                asyncio.create_task(old[name].disconnect())",
            "            old[name] = node",
            "",
            "    def _update_moved_slots(self) -> None:",
            "        e = self._moved_exception",
            "        redirected_node = self.get_node(host=e.host, port=e.port)",
            "        if redirected_node:",
            "            # The node already exists",
            "            if redirected_node.server_type != PRIMARY:",
            "                # Update the node's server type",
            "                redirected_node.server_type = PRIMARY",
            "        else:",
            "            # This is a new node, we will add it to the nodes cache",
            "            redirected_node = ClusterNode(",
            "                e.host, e.port, PRIMARY, **self.connection_kwargs",
            "            )",
            "            self.set_nodes(self.nodes_cache, {redirected_node.name: redirected_node})",
            "        if redirected_node in self.slots_cache[e.slot_id]:",
            "            # The MOVED error resulted from a failover, and the new slot owner",
            "            # had previously been a replica.",
            "            old_primary = self.slots_cache[e.slot_id][0]",
            "            # Update the old primary to be a replica and add it to the end of",
            "            # the slot's node list",
            "            old_primary.server_type = REPLICA",
            "            self.slots_cache[e.slot_id].append(old_primary)",
            "            # Remove the old replica, which is now a primary, from the slot's",
            "            # node list",
            "            self.slots_cache[e.slot_id].remove(redirected_node)",
            "            # Override the old primary with the new one",
            "            self.slots_cache[e.slot_id][0] = redirected_node",
            "            if self.default_node == old_primary:",
            "                # Update the default node with the new primary",
            "                self.default_node = redirected_node",
            "        else:",
            "            # The new slot owner is a new server, or a server from a different",
            "            # shard. We need to remove all current nodes from the slot's list",
            "            # (including replications) and add just the new node.",
            "            self.slots_cache[e.slot_id] = [redirected_node]",
            "        # Reset moved_exception",
            "        self._moved_exception = None",
            "",
            "    def get_node_from_slot(",
            "        self, slot: int, read_from_replicas: bool = False",
            "    ) -> \"ClusterNode\":",
            "        if self._moved_exception:",
            "            self._update_moved_slots()",
            "",
            "        try:",
            "            if read_from_replicas:",
            "                # get the server index in a Round-Robin manner",
            "                primary_name = self.slots_cache[slot][0].name",
            "                node_idx = self.read_load_balancer.get_server_index(",
            "                    primary_name, len(self.slots_cache[slot])",
            "                )",
            "                return self.slots_cache[slot][node_idx]",
            "            return self.slots_cache[slot][0]",
            "        except (IndexError, TypeError):",
            "            raise SlotNotCoveredError(",
            "                f'Slot \"{slot}\" not covered by the cluster. '",
            "                f'\"require_full_coverage={self.require_full_coverage}\"'",
            "            )",
            "",
            "    def get_nodes_by_server_type(self, server_type: str) -> List[\"ClusterNode\"]:",
            "        return [",
            "            node",
            "            for node in self.nodes_cache.values()",
            "            if node.server_type == server_type",
            "        ]",
            "",
            "    async def initialize(self) -> None:",
            "        self.read_load_balancer.reset()",
            "        tmp_nodes_cache: Dict[str, \"ClusterNode\"] = {}",
            "        tmp_slots: Dict[int, List[\"ClusterNode\"]] = {}",
            "        disagreements = []",
            "        startup_nodes_reachable = False",
            "        fully_covered = False",
            "        exception = None",
            "        for startup_node in self.startup_nodes.values():",
            "            try:",
            "                # Make sure cluster mode is enabled on this node",
            "                if not (await startup_node.execute_command(\"INFO\")).get(",
            "                    \"cluster_enabled\"",
            "                ):",
            "                    raise RedisClusterException(",
            "                        \"Cluster mode is not enabled on this node\"",
            "                    )",
            "                cluster_slots = await startup_node.execute_command(\"CLUSTER SLOTS\")",
            "                startup_nodes_reachable = True",
            "            except Exception as e:",
            "                # Try the next startup node.",
            "                # The exception is saved and raised only if we have no more nodes.",
            "                exception = e",
            "                continue",
            "",
            "            # CLUSTER SLOTS command results in the following output:",
            "            # [[slot_section[from_slot,to_slot,master,replica1,...,replicaN]]]",
            "            # where each node contains the following list: [IP, port, node_id]",
            "            # Therefore, cluster_slots[0][2][0] will be the IP address of the",
            "            # primary node of the first slot section.",
            "            # If there's only one server in the cluster, its ``host`` is ''",
            "            # Fix it to the host in startup_nodes",
            "            if (",
            "                len(cluster_slots) == 1",
            "                and not cluster_slots[0][2][0]",
            "                and len(self.startup_nodes) == 1",
            "            ):",
            "                cluster_slots[0][2][0] = startup_node.host",
            "",
            "            for slot in cluster_slots:",
            "                for i in range(2, len(slot)):",
            "                    slot[i] = [str_if_bytes(val) for val in slot[i]]",
            "                primary_node = slot[2]",
            "                host = primary_node[0]",
            "                if host == \"\":",
            "                    host = startup_node.host",
            "                port = int(primary_node[1])",
            "",
            "                target_node = tmp_nodes_cache.get(get_node_name(host, port))",
            "                if not target_node:",
            "                    target_node = ClusterNode(",
            "                        host, port, PRIMARY, **self.connection_kwargs",
            "                    )",
            "                # add this node to the nodes cache",
            "                tmp_nodes_cache[target_node.name] = target_node",
            "",
            "                for i in range(int(slot[0]), int(slot[1]) + 1):",
            "                    if i not in tmp_slots:",
            "                        tmp_slots[i] = []",
            "                        tmp_slots[i].append(target_node)",
            "                        replica_nodes = [slot[j] for j in range(3, len(slot))]",
            "",
            "                        for replica_node in replica_nodes:",
            "                            host = replica_node[0]",
            "                            port = replica_node[1]",
            "",
            "                            target_replica_node = tmp_nodes_cache.get(",
            "                                get_node_name(host, port)",
            "                            )",
            "                            if not target_replica_node:",
            "                                target_replica_node = ClusterNode(",
            "                                    host, port, REPLICA, **self.connection_kwargs",
            "                                )",
            "                            tmp_slots[i].append(target_replica_node)",
            "                            # add this node to the nodes cache",
            "                            tmp_nodes_cache[",
            "                                target_replica_node.name",
            "                            ] = target_replica_node",
            "                    else:",
            "                        # Validate that 2 nodes want to use the same slot cache",
            "                        # setup",
            "                        tmp_slot = tmp_slots[i][0]",
            "                        if tmp_slot.name != target_node.name:",
            "                            disagreements.append(",
            "                                f\"{tmp_slot.name} vs {target_node.name} on slot: {i}\"",
            "                            )",
            "",
            "                            if len(disagreements) > 5:",
            "                                raise RedisClusterException(",
            "                                    f\"startup_nodes could not agree on a valid \"",
            "                                    f'slots cache: {\", \".join(disagreements)}'",
            "                                )",
            "",
            "            # Validate if all slots are covered or if we should try next startup node",
            "            fully_covered = True",
            "            for i in range(REDIS_CLUSTER_HASH_SLOTS):",
            "                if i not in tmp_slots:",
            "                    fully_covered = False",
            "                    break",
            "            if fully_covered:",
            "                break",
            "",
            "        if not startup_nodes_reachable:",
            "            raise RedisClusterException(",
            "                f\"Redis Cluster cannot be connected. Please provide at least \"",
            "                f\"one reachable node: {str(exception)}\"",
            "            ) from exception",
            "",
            "        # Check if the slots are not fully covered",
            "        if not fully_covered and self.require_full_coverage:",
            "            # Despite the requirement that the slots be covered, there",
            "            # isn't a full coverage",
            "            raise RedisClusterException(",
            "                f\"All slots are not covered after query all startup_nodes. \"",
            "                f\"{len(tmp_slots)} of {REDIS_CLUSTER_HASH_SLOTS} \"",
            "                f\"covered...\"",
            "            )",
            "",
            "        # Set the tmp variables to the real variables",
            "        self.slots_cache = tmp_slots",
            "        self.set_nodes(self.nodes_cache, tmp_nodes_cache, remove_old=True)",
            "        # Populate the startup nodes with all discovered nodes",
            "        self.set_nodes(self.startup_nodes, self.nodes_cache, remove_old=True)",
            "",
            "        # Set the default node",
            "        self.default_node = self.get_nodes_by_server_type(PRIMARY)[0]",
            "        # If initialize was called after a MovedError, clear it",
            "        self._moved_exception = None",
            "",
            "    async def close(self, attr: str = \"nodes_cache\") -> None:",
            "        self.default_node = None",
            "        await asyncio.gather(",
            "            *(",
            "                asyncio.create_task(node.disconnect())",
            "                for node in getattr(self, attr).values()",
            "            )",
            "        )",
            "",
            "",
            "class ClusterPipeline(AbstractRedis, AbstractRedisCluster, AsyncRedisClusterCommands):",
            "    \"\"\"",
            "    Create a new ClusterPipeline object.",
            "",
            "    Usage::",
            "",
            "        result = await (",
            "            rc.pipeline()",
            "            .set(\"A\", 1)",
            "            .get(\"A\")",
            "            .hset(\"K\", \"F\", \"V\")",
            "            .hgetall(\"K\")",
            "            .mset_nonatomic({\"A\": 2, \"B\": 3})",
            "            .get(\"A\")",
            "            .get(\"B\")",
            "            .delete(\"A\", \"B\", \"K\")",
            "            .execute()",
            "        )",
            "        # result = [True, \"1\", 1, {\"F\": \"V\"}, True, True, \"2\", \"3\", 1, 1, 1]",
            "",
            "    Note: For commands `DELETE`, `EXISTS`, `TOUCH`, `UNLINK`, `mset_nonatomic`, which",
            "    are split across multiple nodes, you'll get multiple results for them in the array.",
            "",
            "    Retryable errors:",
            "        - :class:`~.ClusterDownError`",
            "        - :class:`~.ConnectionError`",
            "        - :class:`~.TimeoutError`",
            "",
            "    Redirection errors:",
            "        - :class:`~.TryAgainError`",
            "        - :class:`~.MovedError`",
            "        - :class:`~.AskError`",
            "",
            "    :param client:",
            "        | Existing :class:`~.RedisCluster` client",
            "    \"\"\"",
            "",
            "    __slots__ = (\"_command_stack\", \"_client\")",
            "",
            "    def __init__(self, client: RedisCluster) -> None:",
            "        self._client = client",
            "",
            "        self._command_stack: List[\"PipelineCommand\"] = []",
            "",
            "    async def initialize(self) -> \"ClusterPipeline\":",
            "        if self._client._initialize:",
            "            await self._client.initialize()",
            "        self._command_stack = []",
            "        return self",
            "",
            "    async def __aenter__(self) -> \"ClusterPipeline\":",
            "        return await self.initialize()",
            "",
            "    async def __aexit__(self, exc_type: None, exc_value: None, traceback: None) -> None:",
            "        self._command_stack = []",
            "",
            "    def __await__(self) -> Generator[Any, None, \"ClusterPipeline\"]:",
            "        return self.initialize().__await__()",
            "",
            "    def __enter__(self) -> \"ClusterPipeline\":",
            "        self._command_stack = []",
            "        return self",
            "",
            "    def __exit__(self, exc_type: None, exc_value: None, traceback: None) -> None:",
            "        self._command_stack = []",
            "",
            "    def __bool__(self) -> bool:",
            "        return bool(self._command_stack)",
            "",
            "    def __len__(self) -> int:",
            "        return len(self._command_stack)",
            "",
            "    def execute_command(",
            "        self, *args: Union[KeyT, EncodableT], **kwargs: Any",
            "    ) -> \"ClusterPipeline\":",
            "        \"\"\"",
            "        Append a raw command to the pipeline.",
            "",
            "        :param args:",
            "            | Raw command args",
            "        :param kwargs:",
            "",
            "            - target_nodes: :attr:`NODE_FLAGS` or :class:`~.ClusterNode`",
            "              or List[:class:`~.ClusterNode`] or Dict[Any, :class:`~.ClusterNode`]",
            "            - Rest of the kwargs are passed to the Redis connection",
            "        \"\"\"",
            "        self._command_stack.append(",
            "            PipelineCommand(len(self._command_stack), *args, **kwargs)",
            "        )",
            "        return self",
            "",
            "    async def execute(",
            "        self, raise_on_error: bool = True, allow_redirections: bool = True",
            "    ) -> List[Any]:",
            "        \"\"\"",
            "        Execute the pipeline.",
            "",
            "        It will retry the commands as specified by :attr:`cluster_error_retry_attempts`",
            "        & then raise an exception.",
            "",
            "        :param raise_on_error:",
            "            | Raise the first error if there are any errors",
            "        :param allow_redirections:",
            "            | Whether to retry each failed command individually in case of redirection",
            "              errors",
            "",
            "        :raises RedisClusterException: if target_nodes is not provided & the command",
            "            can't be mapped to a slot",
            "        \"\"\"",
            "        if not self._command_stack:",
            "            return []",
            "",
            "        try:",
            "            for _ in range(self._client.cluster_error_retry_attempts):",
            "                if self._client._initialize:",
            "                    await self._client.initialize()",
            "",
            "                try:",
            "                    return await self._execute(",
            "                        self._client,",
            "                        self._command_stack,",
            "                        raise_on_error=raise_on_error,",
            "                        allow_redirections=allow_redirections,",
            "                    )",
            "                except BaseException as e:",
            "                    if type(e) in self.__class__.ERRORS_ALLOW_RETRY:",
            "                        # Try again with the new cluster setup.",
            "                        exception = e",
            "                        await self._client.close()",
            "                        await asyncio.sleep(0.25)",
            "                    else:",
            "                        # All other errors should be raised.",
            "                        raise",
            "",
            "            # If it fails the configured number of times then raise an exception",
            "            raise exception",
            "        finally:",
            "            self._command_stack = []",
            "",
            "    async def _execute(",
            "        self,",
            "        client: \"RedisCluster\",",
            "        stack: List[\"PipelineCommand\"],",
            "        raise_on_error: bool = True,",
            "        allow_redirections: bool = True,",
            "    ) -> List[Any]:",
            "        todo = [",
            "            cmd for cmd in stack if not cmd.result or isinstance(cmd.result, Exception)",
            "        ]",
            "",
            "        nodes = {}",
            "        for cmd in todo:",
            "            passed_targets = cmd.kwargs.pop(\"target_nodes\", None)",
            "            if passed_targets and not client._is_node_flag(passed_targets):",
            "                target_nodes = client._parse_target_nodes(passed_targets)",
            "            else:",
            "                target_nodes = await client._determine_nodes(",
            "                    *cmd.args, node_flag=passed_targets",
            "                )",
            "                if not target_nodes:",
            "                    raise RedisClusterException(",
            "                        f\"No targets were found to execute {cmd.args} command on\"",
            "                    )",
            "            if len(target_nodes) > 1:",
            "                raise RedisClusterException(f\"Too many targets for command {cmd.args}\")",
            "            node = target_nodes[0]",
            "            if node.name not in nodes:",
            "                nodes[node.name] = (node, [])",
            "            nodes[node.name][1].append(cmd)",
            "",
            "        errors = await asyncio.gather(",
            "            *(",
            "                asyncio.create_task(node[0].execute_pipeline(node[1]))",
            "                for node in nodes.values()",
            "            )",
            "        )",
            "",
            "        if any(errors):",
            "            if allow_redirections:",
            "                # send each errored command individually",
            "                for cmd in todo:",
            "                    if isinstance(cmd.result, (TryAgainError, MovedError, AskError)):",
            "                        try:",
            "                            cmd.result = await client.execute_command(",
            "                                *cmd.args, **cmd.kwargs",
            "                            )",
            "                        except Exception as e:",
            "                            cmd.result = e",
            "",
            "            if raise_on_error:",
            "                for cmd in todo:",
            "                    result = cmd.result",
            "                    if isinstance(result, Exception):",
            "                        command = \" \".join(map(safe_str, cmd.args))",
            "                        msg = (",
            "                            f\"Command # {cmd.position + 1} ({command}) of pipeline \"",
            "                            f\"caused error: {result.args}\"",
            "                        )",
            "                        result.args = (msg,) + result.args[1:]",
            "                        raise result",
            "",
            "            default_node = nodes.get(client.get_default_node().name)",
            "            if default_node is not None:",
            "                # This pipeline execution used the default node, check if we need",
            "                # to replace it.",
            "                # Note: when the error is raised we'll reset the default node in the",
            "                # caller function.",
            "                for cmd in default_node[1]:",
            "                    # Check if it has a command that failed with a relevant",
            "                    # exception",
            "                    if type(cmd.result) in self.__class__.ERRORS_ALLOW_RETRY:",
            "                        client.replace_default_node()",
            "                        break",
            "",
            "        return [cmd.result for cmd in stack]",
            "",
            "    def _split_command_across_slots(",
            "        self, command: str, *keys: KeyT",
            "    ) -> \"ClusterPipeline\":",
            "        for slot_keys in self._client._partition_keys_by_slot(keys).values():",
            "            self.execute_command(command, *slot_keys)",
            "",
            "        return self",
            "",
            "    def mset_nonatomic(",
            "        self, mapping: Mapping[AnyKeyT, EncodableT]",
            "    ) -> \"ClusterPipeline\":",
            "        encoder = self._client.encoder",
            "",
            "        slots_pairs = {}",
            "        for pair in mapping.items():",
            "            slot = key_slot(encoder.encode(pair[0]))",
            "            slots_pairs.setdefault(slot, []).extend(pair)",
            "",
            "        for pairs in slots_pairs.values():",
            "            self.execute_command(\"MSET\", *pairs)",
            "",
            "        return self",
            "",
            "",
            "for command in PIPELINE_BLOCKED_COMMANDS:",
            "    command = command.replace(\" \", \"_\").lower()",
            "    if command == \"mset_nonatomic\":",
            "        continue",
            "",
            "    setattr(ClusterPipeline, command, block_pipeline_command(command))",
            "",
            "",
            "class PipelineCommand:",
            "    def __init__(self, position: int, *args: Any, **kwargs: Any) -> None:",
            "        self.args = args",
            "        self.kwargs = kwargs",
            "        self.position = position",
            "        self.result: Union[Any, Exception] = None",
            "",
            "    def __repr__(self) -> str:",
            "        return f\"[{self.position}] {self.args} ({self.kwargs})\""
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1006": [
                "ClusterNode"
            ],
            "1008": [
                "ClusterNode"
            ]
        },
        "addLocation": [
            "redis.asyncio.cluster.NodesManager.initialize.target_node",
            "redis.asyncio.cluster.NodesManager.default_node",
            "redis.asyncio.cluster.ClusterNode.self",
            "redis.asyncio.cluster.NodesManager.initialize",
            "redis.asyncio.cluster.NodesManager.initialize.target_replica_node",
            "redis.asyncio.cluster.NodesManager._update_moved_slots.redirected_node"
        ]
    },
    "setup.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 8,
                "PatchRowcode": "     long_description_content_type=\"text/markdown\","
            },
            "1": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 9,
                "PatchRowcode": "     keywords=[\"Redis\", \"key-value store\", \"database\"],"
            },
            "2": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 10,
                "PatchRowcode": "     license=\"MIT\","
            },
            "3": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    version=\"4.4.2\","
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 11,
                "PatchRowcode": "+    version=\"4.4.3\","
            },
            "5": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 12,
                "PatchRowcode": "     packages=find_packages("
            },
            "6": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": "         include=["
            },
            "7": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": "             \"redis\","
            }
        },
        "frontPatchFile": [
            "#!/usr/bin/env python",
            "from setuptools import find_packages, setup",
            "",
            "setup(",
            "    name=\"redis\",",
            "    description=\"Python client for Redis database and key-value store\",",
            "    long_description=open(\"README.md\").read().strip(),",
            "    long_description_content_type=\"text/markdown\",",
            "    keywords=[\"Redis\", \"key-value store\", \"database\"],",
            "    license=\"MIT\",",
            "    version=\"4.4.2\",",
            "    packages=find_packages(",
            "        include=[",
            "            \"redis\",",
            "            \"redis.asyncio\",",
            "            \"redis.commands\",",
            "            \"redis.commands.bf\",",
            "            \"redis.commands.json\",",
            "            \"redis.commands.search\",",
            "            \"redis.commands.timeseries\",",
            "            \"redis.commands.graph\",",
            "        ]",
            "    ),",
            "    url=\"https://github.com/redis/redis-py\",",
            "    project_urls={",
            "        \"Documentation\": \"https://redis.readthedocs.io/en/latest/\",",
            "        \"Changes\": \"https://github.com/redis/redis-py/releases\",",
            "        \"Code\": \"https://github.com/redis/redis-py\",",
            "        \"Issue tracker\": \"https://github.com/redis/redis-py/issues\",",
            "    },",
            "    author=\"Redis Inc.\",",
            "    author_email=\"oss@redis.com\",",
            "    python_requires=\">=3.7\",",
            "    install_requires=[",
            "        'importlib-metadata >= 1.0; python_version < \"3.8\"',",
            "        'typing-extensions; python_version<\"3.8\"',",
            "        \"async-timeout>=4.0.2\",",
            "    ],",
            "    classifiers=[",
            "        \"Development Status :: 5 - Production/Stable\",",
            "        \"Environment :: Console\",",
            "        \"Intended Audience :: Developers\",",
            "        \"License :: OSI Approved :: MIT License\",",
            "        \"Operating System :: OS Independent\",",
            "        \"Programming Language :: Python\",",
            "        \"Programming Language :: Python :: 3\",",
            "        \"Programming Language :: Python :: 3 :: Only\",",
            "        \"Programming Language :: Python :: 3.7\",",
            "        \"Programming Language :: Python :: 3.8\",",
            "        \"Programming Language :: Python :: 3.9\",",
            "        \"Programming Language :: Python :: 3.10\",",
            "        \"Programming Language :: Python :: 3.11\",",
            "        \"Programming Language :: Python :: Implementation :: CPython\",",
            "        \"Programming Language :: Python :: Implementation :: PyPy\",",
            "    ],",
            "    extras_require={",
            "        \"hiredis\": [\"hiredis>=1.0.0\"],",
            "        \"ocsp\": [\"cryptography>=36.0.1\", \"pyopenssl==20.0.1\", \"requests>=2.26.0\"],",
            "    },",
            ")"
        ],
        "afterPatchFile": [
            "#!/usr/bin/env python",
            "from setuptools import find_packages, setup",
            "",
            "setup(",
            "    name=\"redis\",",
            "    description=\"Python client for Redis database and key-value store\",",
            "    long_description=open(\"README.md\").read().strip(),",
            "    long_description_content_type=\"text/markdown\",",
            "    keywords=[\"Redis\", \"key-value store\", \"database\"],",
            "    license=\"MIT\",",
            "    version=\"4.4.3\",",
            "    packages=find_packages(",
            "        include=[",
            "            \"redis\",",
            "            \"redis.asyncio\",",
            "            \"redis.commands\",",
            "            \"redis.commands.bf\",",
            "            \"redis.commands.json\",",
            "            \"redis.commands.search\",",
            "            \"redis.commands.timeseries\",",
            "            \"redis.commands.graph\",",
            "        ]",
            "    ),",
            "    url=\"https://github.com/redis/redis-py\",",
            "    project_urls={",
            "        \"Documentation\": \"https://redis.readthedocs.io/en/latest/\",",
            "        \"Changes\": \"https://github.com/redis/redis-py/releases\",",
            "        \"Code\": \"https://github.com/redis/redis-py\",",
            "        \"Issue tracker\": \"https://github.com/redis/redis-py/issues\",",
            "    },",
            "    author=\"Redis Inc.\",",
            "    author_email=\"oss@redis.com\",",
            "    python_requires=\">=3.7\",",
            "    install_requires=[",
            "        'importlib-metadata >= 1.0; python_version < \"3.8\"',",
            "        'typing-extensions; python_version<\"3.8\"',",
            "        \"async-timeout>=4.0.2\",",
            "    ],",
            "    classifiers=[",
            "        \"Development Status :: 5 - Production/Stable\",",
            "        \"Environment :: Console\",",
            "        \"Intended Audience :: Developers\",",
            "        \"License :: OSI Approved :: MIT License\",",
            "        \"Operating System :: OS Independent\",",
            "        \"Programming Language :: Python\",",
            "        \"Programming Language :: Python :: 3\",",
            "        \"Programming Language :: Python :: 3 :: Only\",",
            "        \"Programming Language :: Python :: 3.7\",",
            "        \"Programming Language :: Python :: 3.8\",",
            "        \"Programming Language :: Python :: 3.9\",",
            "        \"Programming Language :: Python :: 3.10\",",
            "        \"Programming Language :: Python :: 3.11\",",
            "        \"Programming Language :: Python :: Implementation :: CPython\",",
            "        \"Programming Language :: Python :: Implementation :: PyPy\",",
            "    ],",
            "    extras_require={",
            "        \"hiredis\": [\"hiredis>=1.0.0\"],",
            "        \"ocsp\": [\"cryptography>=36.0.1\", \"pyopenssl==20.0.1\", \"requests>=2.26.0\"],",
            "    },",
            ")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "11": []
        },
        "addLocation": []
    }
}