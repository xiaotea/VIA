{
    "py7zr/helpers.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " import _hashlib  # type: ignore  # noqa"
            },
            "1": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " import py7zr.win32compat"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 36,
                "PatchRowcode": "+from py7zr import Bad7zFile"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+from py7zr.win32compat import is_windows_native_python, is_windows_unc_path"
            },
            "5": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 38,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 39,
                "PatchRowcode": " # String used at the beginning of relative paths"
            },
            "7": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 40,
                "PatchRowcode": " RELATIVE_PATH_MARKER = \"./\""
            },
            "8": {
                "beforePatchRowNumber": 265,
                "afterPatchRowNumber": 267,
                "PatchRowcode": " def islink(path):"
            },
            "9": {
                "beforePatchRowNumber": 266,
                "afterPatchRowNumber": 268,
                "PatchRowcode": "     \"\"\""
            },
            "10": {
                "beforePatchRowNumber": 267,
                "afterPatchRowNumber": 269,
                "PatchRowcode": "     Cross-platform islink implementation."
            },
            "11": {
                "beforePatchRowNumber": 268,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    Supports Windows NT symbolic links and reparse points."
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 270,
                "PatchRowcode": "+    Support Windows NT symbolic links and reparse points."
            },
            "13": {
                "beforePatchRowNumber": 269,
                "afterPatchRowNumber": 271,
                "PatchRowcode": "     \"\"\""
            },
            "14": {
                "beforePatchRowNumber": 270,
                "afterPatchRowNumber": 272,
                "PatchRowcode": "     is_symlink = os.path.islink(str(path))"
            },
            "15": {
                "beforePatchRowNumber": 271,
                "afterPatchRowNumber": 273,
                "PatchRowcode": "     if sys.version_info >= (3, 8) or sys.platform != \"win32\" or sys.getwindowsversion()[0] < 6:"
            },
            "16": {
                "beforePatchRowNumber": 280,
                "afterPatchRowNumber": 282,
                "PatchRowcode": " def readlink(path: Union[str, pathlib.Path], *, dir_fd=None) -> Union[str, pathlib.Path]:"
            },
            "17": {
                "beforePatchRowNumber": 281,
                "afterPatchRowNumber": 283,
                "PatchRowcode": "     \"\"\""
            },
            "18": {
                "beforePatchRowNumber": 282,
                "afterPatchRowNumber": 284,
                "PatchRowcode": "     Cross-platform compat implementation of os.readlink and Path.readlink()."
            },
            "19": {
                "beforePatchRowNumber": 283,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    Supports Windows NT symbolic links and reparse points."
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 285,
                "PatchRowcode": "+    Support Windows NT symbolic links and reparse points."
            },
            "21": {
                "beforePatchRowNumber": 284,
                "afterPatchRowNumber": 286,
                "PatchRowcode": "     When called with path argument as pathlike(str), return result as a pathlike(str)."
            },
            "22": {
                "beforePatchRowNumber": 285,
                "afterPatchRowNumber": 287,
                "PatchRowcode": "     When called with Path object, return also Path object."
            },
            "23": {
                "beforePatchRowNumber": 286,
                "afterPatchRowNumber": 288,
                "PatchRowcode": "     When called with path argument as bytes, return result as a bytes."
            },
            "24": {
                "beforePatchRowNumber": 431,
                "afterPatchRowNumber": 433,
                "PatchRowcode": "         processed_path = path[len(RELATIVE_PATH_MARKER) :]"
            },
            "25": {
                "beforePatchRowNumber": 432,
                "afterPatchRowNumber": 434,
                "PatchRowcode": " "
            },
            "26": {
                "beforePatchRowNumber": 433,
                "afterPatchRowNumber": 435,
                "PatchRowcode": "     return processed_path"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 436,
                "PatchRowcode": "+"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 437,
                "PatchRowcode": "+"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 438,
                "PatchRowcode": "+def get_sanitized_output_path(fname: str, path: Optional[pathlib.Path]) -> pathlib.Path:"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 439,
                "PatchRowcode": "+    \"\"\""
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 440,
                "PatchRowcode": "+    check f.filename has invalid directory traversals"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 441,
                "PatchRowcode": "+    do following but is_relative_to introduced in py 3.9,"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 442,
                "PatchRowcode": "+    so I replaced it with relative_to. when condition is not satisfied, raise ValueError"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 443,
                "PatchRowcode": "+    if not pathlib.Path(...).joinpath(remove_relative_path_marker(outname)).is_relative_to(...):"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 444,
                "PatchRowcode": "+        raise Bad7zFile"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 445,
                "PatchRowcode": "+    \"\"\""
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 446,
                "PatchRowcode": "+    if path is None:"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 447,
                "PatchRowcode": "+        try:"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 448,
                "PatchRowcode": "+            pathlib.Path(os.getcwd()).joinpath(fname).resolve().relative_to(os.getcwd())"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 449,
                "PatchRowcode": "+            outfile = pathlib.Path(remove_relative_path_marker(fname))"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 450,
                "PatchRowcode": "+        except ValueError:"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 451,
                "PatchRowcode": "+            raise Bad7zFile(f\"Specified path is bad: {fname}\")"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 452,
                "PatchRowcode": "+    else:"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 453,
                "PatchRowcode": "+        try:"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 454,
                "PatchRowcode": "+            outfile = path.joinpath(remove_relative_path_marker(fname))"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 455,
                "PatchRowcode": "+            outfile.resolve().relative_to(path)"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 456,
                "PatchRowcode": "+        except ValueError:"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 457,
                "PatchRowcode": "+            raise Bad7zFile(f\"Specified path is bad: {fname}\")"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 458,
                "PatchRowcode": "+    return outfile"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 459,
                "PatchRowcode": "+"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 460,
                "PatchRowcode": "+"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 461,
                "PatchRowcode": "+def check_archive_path(arcname: str) -> bool:"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 462,
                "PatchRowcode": "+    path = pathlib.Path(\"/foo/boo/fuga/hoge/a90sufoiasj09/dafj08sajfa/\")  # dummy path"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 463,
                "PatchRowcode": "+    return is_target_path_valid(path, path.joinpath(arcname))"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 464,
                "PatchRowcode": "+"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 465,
                "PatchRowcode": "+"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 466,
                "PatchRowcode": "+def is_target_path_valid(path: pathlib.Path, target: pathlib.Path) -> bool:"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 467,
                "PatchRowcode": "+    try:"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 468,
                "PatchRowcode": "+        if path.is_absolute():"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 469,
                "PatchRowcode": "+            target.resolve().relative_to(path)"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 470,
                "PatchRowcode": "+        else:"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 471,
                "PatchRowcode": "+            target.resolve().relative_to(pathlib.Path(os.getcwd()).joinpath(path))"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 472,
                "PatchRowcode": "+    except ValueError:"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 473,
                "PatchRowcode": "+        return False"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 474,
                "PatchRowcode": "+    return True"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 475,
                "PatchRowcode": "+"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 476,
                "PatchRowcode": "+"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 477,
                "PatchRowcode": "+def check_win32_file_namespace(pathname: pathlib.Path) -> pathlib.Path:"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 478,
                "PatchRowcode": "+    # When python on Windows and not python on Cygwin,"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 479,
                "PatchRowcode": "+    # Add win32 file namespace to exceed Microsoft Windows"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 480,
                "PatchRowcode": "+    # path length limitation to 260 bytes"
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 481,
                "PatchRowcode": "+    # ref."
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 482,
                "PatchRowcode": "+    # https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 483,
                "PatchRowcode": "+    # In editions of Windows before Windows 10 version 1607,"
            },
            "75": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 484,
                "PatchRowcode": "+    # the maximum length for a path is MAX_PATH, which is defined as"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 485,
                "PatchRowcode": "+    # 260 characters. In later versions of Windows, changing a registry key"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 486,
                "PatchRowcode": "+    # or select option when python installation is required to remove the limit."
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 487,
                "PatchRowcode": "+    if is_windows_native_python() and pathname.is_absolute() and not is_windows_unc_path(pathname):"
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 488,
                "PatchRowcode": "+        pathname = pathlib.WindowsPath(\"\\\\\\\\?\\\\\" + str(pathname))"
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 489,
                "PatchRowcode": "+    return pathname"
            }
        },
        "frontPatchFile": [
            "#!/usr/bin/python -u",
            "#",
            "# p7zr library",
            "#",
            "# Copyright (c) 2019-2021 Hiroshi Miura <miurahr@linux.com>",
            "# Copyright (c) 2004-2015 by Joachim Bauch, mail@joachim-bauch.de",
            "#",
            "# This library is free software; you can redistribute it and/or",
            "# modify it under the terms of the GNU Lesser General Public",
            "# License as published by the Free Software Foundation; either",
            "# version 2.1 of the License, or (at your option) any later version.",
            "#",
            "# This library is distributed in the hope that it will be useful,",
            "# but WITHOUT ANY WARRANTY; without even the implied warranty of",
            "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU",
            "# Lesser General Public License for more details.",
            "#",
            "# You should have received a copy of the GNU Lesser General Public",
            "# License along with this library; if not, write to the Free Software",
            "# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA",
            "#",
            "#",
            "import ctypes",
            "import os",
            "import pathlib",
            "import platform",
            "import sys",
            "import time as _time",
            "import zlib",
            "from datetime import datetime, timedelta, timezone, tzinfo",
            "from typing import BinaryIO, Optional, Union",
            "",
            "import _hashlib  # type: ignore  # noqa",
            "",
            "import py7zr.win32compat",
            "",
            "# String used at the beginning of relative paths",
            "RELATIVE_PATH_MARKER = \"./\"",
            "",
            "",
            "def calculate_crc32(data: bytes, value: int = 0, blocksize: int = 1024 * 1024) -> int:",
            "    \"\"\"Calculate CRC32 of strings with arbitrary lengths.\"\"\"",
            "    if len(data) <= blocksize:",
            "        value = zlib.crc32(data, value)",
            "    else:",
            "        length = len(data)",
            "        pos = blocksize",
            "        value = zlib.crc32(data[:pos], value)",
            "        while pos < length:",
            "            value = zlib.crc32(data[pos : pos + blocksize], value)",
            "            pos += blocksize",
            "    return value & 0xFFFFFFFF",
            "",
            "",
            "def _calculate_key1(password: bytes, cycles: int, salt: bytes, digest: str) -> bytes:",
            "    \"\"\"Calculate 7zip AES encryption key. Base implementation.\"\"\"",
            "    if digest not in (\"sha256\"):",
            "        raise ValueError(\"Unknown digest method for password protection.\")",
            "    assert cycles <= 0x3F",
            "    if cycles == 0x3F:",
            "        ba = bytearray(salt + password + bytes(32))",
            "        key: bytes = bytes(ba[:32])",
            "    else:",
            "        rounds = 1 << cycles",
            "        m = _hashlib.new(digest)",
            "        for round in range(rounds):",
            "            m.update(salt + password + round.to_bytes(8, byteorder=\"little\", signed=False))",
            "        key = m.digest()[:32]",
            "    return key",
            "",
            "",
            "def _calculate_key2(password: bytes, cycles: int, salt: bytes, digest: str):",
            "    \"\"\"Calculate 7zip AES encryption key.",
            "    It utilize ctypes and memoryview buffer and zero-copy technology on Python.\"\"\"",
            "    if digest not in (\"sha256\"):",
            "        raise ValueError(\"Unknown digest method for password protection.\")",
            "    assert cycles <= 0x3F",
            "    if cycles == 0x3F:",
            "        key: bytes = bytes(bytearray(salt + password + bytes(32))[:32])",
            "    else:",
            "        rounds = 1 << cycles",
            "        m = _hashlib.new(digest)",
            "        length = len(salt) + len(password)",
            "",
            "        class RoundBuf(ctypes.LittleEndianStructure):",
            "            _pack_ = 1",
            "            _fields_ = [",
            "                (\"saltpassword\", ctypes.c_ubyte * length),",
            "                (\"round\", ctypes.c_uint64),",
            "            ]",
            "",
            "        buf = RoundBuf()",
            "        for i, c in enumerate(salt + password):",
            "            buf.saltpassword[i] = c",
            "        buf.round = 0",
            "        mv = memoryview(buf)",
            "        while buf.round < rounds:",
            "            m.update(mv)",
            "            buf.round += 1",
            "        key = m.digest()[:32]",
            "    return key",
            "",
            "",
            "def _calculate_key3(password: bytes, cycles: int, salt: bytes, digest: str) -> bytes:",
            "    \"\"\"Calculate 7zip AES encryption key.",
            "    Concat values in order to reduce number of calls of Hash.update().\"\"\"",
            "    if digest not in (\"sha256\"):",
            "        raise ValueError(\"Unknown digest method for password protection.\")",
            "    assert cycles <= 0x3F",
            "    if cycles == 0x3F:",
            "        ba = bytearray(salt + password + bytes(32))",
            "        key: bytes = bytes(ba[:32])",
            "    else:",
            "        cat_cycle = 6",
            "        if cycles > cat_cycle:",
            "            rounds = 1 << cat_cycle",
            "            stages = 1 << (cycles - cat_cycle)",
            "        else:",
            "            rounds = 1 << cycles",
            "            stages = 1 << 0",
            "        m = _hashlib.new(digest)",
            "        saltpassword = salt + password",
            "        s = 0  # type: int  # (0..stages) * rounds",
            "        if platform.python_implementation() == \"PyPy\":",
            "            for _ in range(stages):",
            "                m.update(",
            "                    memoryview(",
            "                        b\"\".join(",
            "                            [saltpassword + (s + i).to_bytes(8, byteorder=\"little\", signed=False) for i in range(rounds)]",
            "                        )",
            "                    )",
            "                )",
            "                s += rounds",
            "        else:",
            "            for _ in range(stages):",
            "                m.update(",
            "                    b\"\".join([saltpassword + (s + i).to_bytes(8, byteorder=\"little\", signed=False) for i in range(rounds)])",
            "                )",
            "                s += rounds",
            "        key = m.digest()[:32]",
            "",
            "    return key",
            "",
            "",
            "if platform.python_implementation() == \"PyPy\" or sys.version_info > (3, 6):",
            "    calculate_key = _calculate_key3",
            "else:",
            "    calculate_key = _calculate_key2  # it is faster when CPython 3.6.x",
            "",
            "",
            "def filetime_to_dt(ft):",
            "    \"\"\"Convert Windows NTFS file time into python datetime object.\"\"\"",
            "    EPOCH_AS_FILETIME = 116444736000000000",
            "    us = (ft - EPOCH_AS_FILETIME) // 10",
            "    return datetime(1970, 1, 1, tzinfo=timezone.utc) + timedelta(microseconds=us)",
            "",
            "",
            "ZERO = timedelta(0)",
            "HOUR = timedelta(hours=1)",
            "SECOND = timedelta(seconds=1)",
            "",
            "# A class capturing the platform's idea of local time.",
            "# (May result in wrong values on historical times in",
            "#  timezones where UTC offset and/or the DST rules had",
            "#  changed in the past.)",
            "",
            "STDOFFSET = timedelta(seconds=-_time.timezone)",
            "if _time.daylight:",
            "    DSTOFFSET = timedelta(seconds=-_time.altzone)",
            "else:",
            "    DSTOFFSET = STDOFFSET",
            "",
            "DSTDIFF = DSTOFFSET - STDOFFSET",
            "",
            "",
            "class LocalTimezone(tzinfo):",
            "    def fromutc(self, dt):",
            "        assert dt.tzinfo is self",
            "        stamp = (dt - datetime(1970, 1, 1, tzinfo=self)) // SECOND",
            "        args = _time.localtime(stamp)[:6]",
            "        # dst_diff = DSTDIFF // SECOND",
            "        # Detect fold",
            "        # fold = args == _time.localtime(stamp - dst_diff)",
            "        return datetime(*args, microsecond=dt.microsecond, tzinfo=self)",
            "",
            "    def utcoffset(self, dt):",
            "        if self._isdst(dt):",
            "            return DSTOFFSET",
            "        else:",
            "            return STDOFFSET",
            "",
            "    def dst(self, dt):",
            "        if self._isdst(dt):",
            "            return DSTDIFF",
            "        else:",
            "            return ZERO",
            "",
            "    def tzname(self, dt):",
            "        return _time.tzname[self._isdst(dt)]",
            "",
            "    def _isdst(self, dt):",
            "        tt = (",
            "            dt.year,",
            "            dt.month,",
            "            dt.day,",
            "            dt.hour,",
            "            dt.minute,",
            "            dt.second,",
            "            dt.weekday(),",
            "            0,",
            "            0,",
            "        )",
            "        stamp = _time.mktime(tt)",
            "        tt = _time.localtime(stamp)",
            "        return tt.tm_isdst > 0",
            "",
            "",
            "Local = LocalTimezone()",
            "TIMESTAMP_ADJUST = -11644473600",
            "",
            "",
            "class UTC(tzinfo):",
            "    \"\"\"UTC\"\"\"",
            "",
            "    def utcoffset(self, dt):",
            "        return ZERO",
            "",
            "    def tzname(self, dt):",
            "        return \"UTC\"",
            "",
            "    def dst(self, dt):",
            "        return ZERO",
            "",
            "    def _call__(self):",
            "        return self",
            "",
            "",
            "class ArchiveTimestamp(int):",
            "    \"\"\"Windows FILETIME timestamp.\"\"\"",
            "",
            "    def __repr__(self):",
            "        return \"%s(%d)\" % (type(self).__name__, self)",
            "",
            "    def __index__(self):",
            "        return self.__int__()",
            "",
            "    def totimestamp(self) -> float:",
            "        \"\"\"Convert 7z FILETIME to Python timestamp.\"\"\"",
            "        # FILETIME is 100-nanosecond intervals since 1601/01/01 (UTC)",
            "        return (self / 10000000.0) + TIMESTAMP_ADJUST",
            "",
            "    def as_datetime(self):",
            "        \"\"\"Convert FILETIME to Python datetime object.\"\"\"",
            "        return datetime.fromtimestamp(self.totimestamp(), UTC())",
            "",
            "    @staticmethod",
            "    def from_datetime(val):",
            "        return ArchiveTimestamp((val - TIMESTAMP_ADJUST) * 10000000.0)",
            "",
            "    @staticmethod",
            "    def from_now():",
            "        return ArchiveTimestamp((_time.time() - TIMESTAMP_ADJUST) * 10000000.0)",
            "",
            "",
            "def islink(path):",
            "    \"\"\"",
            "    Cross-platform islink implementation.",
            "    Supports Windows NT symbolic links and reparse points.",
            "    \"\"\"",
            "    is_symlink = os.path.islink(str(path))",
            "    if sys.version_info >= (3, 8) or sys.platform != \"win32\" or sys.getwindowsversion()[0] < 6:",
            "        return is_symlink",
            "    # special check for directory junctions which py38 does.",
            "    if is_symlink:",
            "        if py7zr.win32compat.is_reparse_point(path):",
            "            is_symlink = False",
            "    return is_symlink",
            "",
            "",
            "def readlink(path: Union[str, pathlib.Path], *, dir_fd=None) -> Union[str, pathlib.Path]:",
            "    \"\"\"",
            "    Cross-platform compat implementation of os.readlink and Path.readlink().",
            "    Supports Windows NT symbolic links and reparse points.",
            "    When called with path argument as pathlike(str), return result as a pathlike(str).",
            "    When called with Path object, return also Path object.",
            "    When called with path argument as bytes, return result as a bytes.",
            "    \"\"\"",
            "    if sys.version_info >= (3, 9):",
            "        if isinstance(path, pathlib.Path) and dir_fd is None:",
            "            return path.readlink()",
            "        else:",
            "            return os.readlink(path, dir_fd=dir_fd)",
            "    elif sys.version_info >= (3, 8) or sys.platform != \"win32\":",
            "        res = os.readlink(path, dir_fd=dir_fd)",
            "        # Hack to handle a wrong type of results",
            "        if isinstance(res, bytes):",
            "            res = os.fsdecode(res)",
            "        if isinstance(path, pathlib.Path):",
            "            return pathlib.Path(res)",
            "        else:",
            "            return res",
            "    elif not os.path.exists(str(path)):",
            "        raise OSError(22, \"Invalid argument\", path)",
            "    return py7zr.win32compat.readlink(path)",
            "",
            "",
            "class MemIO:",
            "    \"\"\"pathlib.Path-like IO class to write memory(io.Bytes)\"\"\"",
            "",
            "    def __init__(self, buf: BinaryIO):",
            "        self._buf = buf",
            "",
            "    def write(self, data: bytes) -> int:",
            "        return self._buf.write(data)",
            "",
            "    def read(self, length: Optional[int] = None) -> bytes:",
            "        if length is not None:",
            "            return self._buf.read(length)",
            "        else:",
            "            return self._buf.read()",
            "",
            "    def close(self) -> None:",
            "        self._buf.seek(0)",
            "",
            "    def flush(self) -> None:",
            "        pass",
            "",
            "    def seek(self, position: int) -> None:",
            "        self._buf.seek(position)",
            "",
            "    def open(self, mode=None):",
            "        return self",
            "",
            "    @property",
            "    def parent(self):",
            "        return self",
            "",
            "    def mkdir(self, parents=None, exist_ok=False):",
            "        return None",
            "",
            "    def __enter__(self):",
            "        return self",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        pass",
            "",
            "",
            "class NullIO:",
            "    \"\"\"pathlib.Path-like IO class of /dev/null\"\"\"",
            "",
            "    def __init__(self):",
            "        pass",
            "",
            "    def write(self, data):",
            "        return len(data)",
            "",
            "    def read(self, length=None):",
            "        if length is not None:",
            "            return bytes(length)",
            "        else:",
            "            return b\"\"",
            "",
            "    def close(self):",
            "        pass",
            "",
            "    def flush(self):",
            "        pass",
            "",
            "    def open(self, mode=None):",
            "        return self",
            "",
            "    @property",
            "    def parent(self):",
            "        return self",
            "",
            "    def mkdir(self):",
            "        return None",
            "",
            "    def __enter__(self):",
            "        return self",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        pass",
            "",
            "",
            "class BufferOverflow(Exception):",
            "    pass",
            "",
            "",
            "class Buffer:",
            "    def __init__(self, size: int = 16):",
            "        self._buf = bytearray(size)",
            "        self._buflen = 0",
            "        self.view = memoryview(self._buf[0:0])",
            "",
            "    def add(self, data: Union[bytes, bytearray, memoryview]):",
            "        length = len(data)",
            "        self._buf[self._buflen :] = data",
            "        self._buflen += length",
            "        self.view = memoryview(self._buf[0 : self._buflen])",
            "",
            "    def reset(self) -> None:",
            "        self._buflen = 0",
            "        self.view = memoryview(self._buf[0:0])",
            "",
            "    def set(self, data: Union[bytes, bytearray, memoryview]) -> None:",
            "        length = len(data)",
            "        self._buf[0:] = data",
            "        self._buflen = length",
            "        self.view = memoryview(self._buf[0:length])",
            "",
            "    def get(self) -> bytearray:",
            "        val = self._buf[: self._buflen]",
            "        self.reset()",
            "        return val",
            "",
            "    def __len__(self) -> int:",
            "        return self._buflen",
            "",
            "    def __bytes__(self):",
            "        return bytes(self._buf[0 : self._buflen])",
            "",
            "",
            "def remove_relative_path_marker(path: str) -> str:",
            "    \"\"\"",
            "    Removes './' from the beginning of a path-like string",
            "    \"\"\"",
            "    processed_path = path",
            "",
            "    if path.startswith(RELATIVE_PATH_MARKER):",
            "        processed_path = path[len(RELATIVE_PATH_MARKER) :]",
            "",
            "    return processed_path"
        ],
        "afterPatchFile": [
            "#!/usr/bin/python -u",
            "#",
            "# p7zr library",
            "#",
            "# Copyright (c) 2019-2021 Hiroshi Miura <miurahr@linux.com>",
            "# Copyright (c) 2004-2015 by Joachim Bauch, mail@joachim-bauch.de",
            "#",
            "# This library is free software; you can redistribute it and/or",
            "# modify it under the terms of the GNU Lesser General Public",
            "# License as published by the Free Software Foundation; either",
            "# version 2.1 of the License, or (at your option) any later version.",
            "#",
            "# This library is distributed in the hope that it will be useful,",
            "# but WITHOUT ANY WARRANTY; without even the implied warranty of",
            "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU",
            "# Lesser General Public License for more details.",
            "#",
            "# You should have received a copy of the GNU Lesser General Public",
            "# License along with this library; if not, write to the Free Software",
            "# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA",
            "#",
            "#",
            "import ctypes",
            "import os",
            "import pathlib",
            "import platform",
            "import sys",
            "import time as _time",
            "import zlib",
            "from datetime import datetime, timedelta, timezone, tzinfo",
            "from typing import BinaryIO, Optional, Union",
            "",
            "import _hashlib  # type: ignore  # noqa",
            "",
            "import py7zr.win32compat",
            "from py7zr import Bad7zFile",
            "from py7zr.win32compat import is_windows_native_python, is_windows_unc_path",
            "",
            "# String used at the beginning of relative paths",
            "RELATIVE_PATH_MARKER = \"./\"",
            "",
            "",
            "def calculate_crc32(data: bytes, value: int = 0, blocksize: int = 1024 * 1024) -> int:",
            "    \"\"\"Calculate CRC32 of strings with arbitrary lengths.\"\"\"",
            "    if len(data) <= blocksize:",
            "        value = zlib.crc32(data, value)",
            "    else:",
            "        length = len(data)",
            "        pos = blocksize",
            "        value = zlib.crc32(data[:pos], value)",
            "        while pos < length:",
            "            value = zlib.crc32(data[pos : pos + blocksize], value)",
            "            pos += blocksize",
            "    return value & 0xFFFFFFFF",
            "",
            "",
            "def _calculate_key1(password: bytes, cycles: int, salt: bytes, digest: str) -> bytes:",
            "    \"\"\"Calculate 7zip AES encryption key. Base implementation.\"\"\"",
            "    if digest not in (\"sha256\"):",
            "        raise ValueError(\"Unknown digest method for password protection.\")",
            "    assert cycles <= 0x3F",
            "    if cycles == 0x3F:",
            "        ba = bytearray(salt + password + bytes(32))",
            "        key: bytes = bytes(ba[:32])",
            "    else:",
            "        rounds = 1 << cycles",
            "        m = _hashlib.new(digest)",
            "        for round in range(rounds):",
            "            m.update(salt + password + round.to_bytes(8, byteorder=\"little\", signed=False))",
            "        key = m.digest()[:32]",
            "    return key",
            "",
            "",
            "def _calculate_key2(password: bytes, cycles: int, salt: bytes, digest: str):",
            "    \"\"\"Calculate 7zip AES encryption key.",
            "    It utilize ctypes and memoryview buffer and zero-copy technology on Python.\"\"\"",
            "    if digest not in (\"sha256\"):",
            "        raise ValueError(\"Unknown digest method for password protection.\")",
            "    assert cycles <= 0x3F",
            "    if cycles == 0x3F:",
            "        key: bytes = bytes(bytearray(salt + password + bytes(32))[:32])",
            "    else:",
            "        rounds = 1 << cycles",
            "        m = _hashlib.new(digest)",
            "        length = len(salt) + len(password)",
            "",
            "        class RoundBuf(ctypes.LittleEndianStructure):",
            "            _pack_ = 1",
            "            _fields_ = [",
            "                (\"saltpassword\", ctypes.c_ubyte * length),",
            "                (\"round\", ctypes.c_uint64),",
            "            ]",
            "",
            "        buf = RoundBuf()",
            "        for i, c in enumerate(salt + password):",
            "            buf.saltpassword[i] = c",
            "        buf.round = 0",
            "        mv = memoryview(buf)",
            "        while buf.round < rounds:",
            "            m.update(mv)",
            "            buf.round += 1",
            "        key = m.digest()[:32]",
            "    return key",
            "",
            "",
            "def _calculate_key3(password: bytes, cycles: int, salt: bytes, digest: str) -> bytes:",
            "    \"\"\"Calculate 7zip AES encryption key.",
            "    Concat values in order to reduce number of calls of Hash.update().\"\"\"",
            "    if digest not in (\"sha256\"):",
            "        raise ValueError(\"Unknown digest method for password protection.\")",
            "    assert cycles <= 0x3F",
            "    if cycles == 0x3F:",
            "        ba = bytearray(salt + password + bytes(32))",
            "        key: bytes = bytes(ba[:32])",
            "    else:",
            "        cat_cycle = 6",
            "        if cycles > cat_cycle:",
            "            rounds = 1 << cat_cycle",
            "            stages = 1 << (cycles - cat_cycle)",
            "        else:",
            "            rounds = 1 << cycles",
            "            stages = 1 << 0",
            "        m = _hashlib.new(digest)",
            "        saltpassword = salt + password",
            "        s = 0  # type: int  # (0..stages) * rounds",
            "        if platform.python_implementation() == \"PyPy\":",
            "            for _ in range(stages):",
            "                m.update(",
            "                    memoryview(",
            "                        b\"\".join(",
            "                            [saltpassword + (s + i).to_bytes(8, byteorder=\"little\", signed=False) for i in range(rounds)]",
            "                        )",
            "                    )",
            "                )",
            "                s += rounds",
            "        else:",
            "            for _ in range(stages):",
            "                m.update(",
            "                    b\"\".join([saltpassword + (s + i).to_bytes(8, byteorder=\"little\", signed=False) for i in range(rounds)])",
            "                )",
            "                s += rounds",
            "        key = m.digest()[:32]",
            "",
            "    return key",
            "",
            "",
            "if platform.python_implementation() == \"PyPy\" or sys.version_info > (3, 6):",
            "    calculate_key = _calculate_key3",
            "else:",
            "    calculate_key = _calculate_key2  # it is faster when CPython 3.6.x",
            "",
            "",
            "def filetime_to_dt(ft):",
            "    \"\"\"Convert Windows NTFS file time into python datetime object.\"\"\"",
            "    EPOCH_AS_FILETIME = 116444736000000000",
            "    us = (ft - EPOCH_AS_FILETIME) // 10",
            "    return datetime(1970, 1, 1, tzinfo=timezone.utc) + timedelta(microseconds=us)",
            "",
            "",
            "ZERO = timedelta(0)",
            "HOUR = timedelta(hours=1)",
            "SECOND = timedelta(seconds=1)",
            "",
            "# A class capturing the platform's idea of local time.",
            "# (May result in wrong values on historical times in",
            "#  timezones where UTC offset and/or the DST rules had",
            "#  changed in the past.)",
            "",
            "STDOFFSET = timedelta(seconds=-_time.timezone)",
            "if _time.daylight:",
            "    DSTOFFSET = timedelta(seconds=-_time.altzone)",
            "else:",
            "    DSTOFFSET = STDOFFSET",
            "",
            "DSTDIFF = DSTOFFSET - STDOFFSET",
            "",
            "",
            "class LocalTimezone(tzinfo):",
            "    def fromutc(self, dt):",
            "        assert dt.tzinfo is self",
            "        stamp = (dt - datetime(1970, 1, 1, tzinfo=self)) // SECOND",
            "        args = _time.localtime(stamp)[:6]",
            "        # dst_diff = DSTDIFF // SECOND",
            "        # Detect fold",
            "        # fold = args == _time.localtime(stamp - dst_diff)",
            "        return datetime(*args, microsecond=dt.microsecond, tzinfo=self)",
            "",
            "    def utcoffset(self, dt):",
            "        if self._isdst(dt):",
            "            return DSTOFFSET",
            "        else:",
            "            return STDOFFSET",
            "",
            "    def dst(self, dt):",
            "        if self._isdst(dt):",
            "            return DSTDIFF",
            "        else:",
            "            return ZERO",
            "",
            "    def tzname(self, dt):",
            "        return _time.tzname[self._isdst(dt)]",
            "",
            "    def _isdst(self, dt):",
            "        tt = (",
            "            dt.year,",
            "            dt.month,",
            "            dt.day,",
            "            dt.hour,",
            "            dt.minute,",
            "            dt.second,",
            "            dt.weekday(),",
            "            0,",
            "            0,",
            "        )",
            "        stamp = _time.mktime(tt)",
            "        tt = _time.localtime(stamp)",
            "        return tt.tm_isdst > 0",
            "",
            "",
            "Local = LocalTimezone()",
            "TIMESTAMP_ADJUST = -11644473600",
            "",
            "",
            "class UTC(tzinfo):",
            "    \"\"\"UTC\"\"\"",
            "",
            "    def utcoffset(self, dt):",
            "        return ZERO",
            "",
            "    def tzname(self, dt):",
            "        return \"UTC\"",
            "",
            "    def dst(self, dt):",
            "        return ZERO",
            "",
            "    def _call__(self):",
            "        return self",
            "",
            "",
            "class ArchiveTimestamp(int):",
            "    \"\"\"Windows FILETIME timestamp.\"\"\"",
            "",
            "    def __repr__(self):",
            "        return \"%s(%d)\" % (type(self).__name__, self)",
            "",
            "    def __index__(self):",
            "        return self.__int__()",
            "",
            "    def totimestamp(self) -> float:",
            "        \"\"\"Convert 7z FILETIME to Python timestamp.\"\"\"",
            "        # FILETIME is 100-nanosecond intervals since 1601/01/01 (UTC)",
            "        return (self / 10000000.0) + TIMESTAMP_ADJUST",
            "",
            "    def as_datetime(self):",
            "        \"\"\"Convert FILETIME to Python datetime object.\"\"\"",
            "        return datetime.fromtimestamp(self.totimestamp(), UTC())",
            "",
            "    @staticmethod",
            "    def from_datetime(val):",
            "        return ArchiveTimestamp((val - TIMESTAMP_ADJUST) * 10000000.0)",
            "",
            "    @staticmethod",
            "    def from_now():",
            "        return ArchiveTimestamp((_time.time() - TIMESTAMP_ADJUST) * 10000000.0)",
            "",
            "",
            "def islink(path):",
            "    \"\"\"",
            "    Cross-platform islink implementation.",
            "    Support Windows NT symbolic links and reparse points.",
            "    \"\"\"",
            "    is_symlink = os.path.islink(str(path))",
            "    if sys.version_info >= (3, 8) or sys.platform != \"win32\" or sys.getwindowsversion()[0] < 6:",
            "        return is_symlink",
            "    # special check for directory junctions which py38 does.",
            "    if is_symlink:",
            "        if py7zr.win32compat.is_reparse_point(path):",
            "            is_symlink = False",
            "    return is_symlink",
            "",
            "",
            "def readlink(path: Union[str, pathlib.Path], *, dir_fd=None) -> Union[str, pathlib.Path]:",
            "    \"\"\"",
            "    Cross-platform compat implementation of os.readlink and Path.readlink().",
            "    Support Windows NT symbolic links and reparse points.",
            "    When called with path argument as pathlike(str), return result as a pathlike(str).",
            "    When called with Path object, return also Path object.",
            "    When called with path argument as bytes, return result as a bytes.",
            "    \"\"\"",
            "    if sys.version_info >= (3, 9):",
            "        if isinstance(path, pathlib.Path) and dir_fd is None:",
            "            return path.readlink()",
            "        else:",
            "            return os.readlink(path, dir_fd=dir_fd)",
            "    elif sys.version_info >= (3, 8) or sys.platform != \"win32\":",
            "        res = os.readlink(path, dir_fd=dir_fd)",
            "        # Hack to handle a wrong type of results",
            "        if isinstance(res, bytes):",
            "            res = os.fsdecode(res)",
            "        if isinstance(path, pathlib.Path):",
            "            return pathlib.Path(res)",
            "        else:",
            "            return res",
            "    elif not os.path.exists(str(path)):",
            "        raise OSError(22, \"Invalid argument\", path)",
            "    return py7zr.win32compat.readlink(path)",
            "",
            "",
            "class MemIO:",
            "    \"\"\"pathlib.Path-like IO class to write memory(io.Bytes)\"\"\"",
            "",
            "    def __init__(self, buf: BinaryIO):",
            "        self._buf = buf",
            "",
            "    def write(self, data: bytes) -> int:",
            "        return self._buf.write(data)",
            "",
            "    def read(self, length: Optional[int] = None) -> bytes:",
            "        if length is not None:",
            "            return self._buf.read(length)",
            "        else:",
            "            return self._buf.read()",
            "",
            "    def close(self) -> None:",
            "        self._buf.seek(0)",
            "",
            "    def flush(self) -> None:",
            "        pass",
            "",
            "    def seek(self, position: int) -> None:",
            "        self._buf.seek(position)",
            "",
            "    def open(self, mode=None):",
            "        return self",
            "",
            "    @property",
            "    def parent(self):",
            "        return self",
            "",
            "    def mkdir(self, parents=None, exist_ok=False):",
            "        return None",
            "",
            "    def __enter__(self):",
            "        return self",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        pass",
            "",
            "",
            "class NullIO:",
            "    \"\"\"pathlib.Path-like IO class of /dev/null\"\"\"",
            "",
            "    def __init__(self):",
            "        pass",
            "",
            "    def write(self, data):",
            "        return len(data)",
            "",
            "    def read(self, length=None):",
            "        if length is not None:",
            "            return bytes(length)",
            "        else:",
            "            return b\"\"",
            "",
            "    def close(self):",
            "        pass",
            "",
            "    def flush(self):",
            "        pass",
            "",
            "    def open(self, mode=None):",
            "        return self",
            "",
            "    @property",
            "    def parent(self):",
            "        return self",
            "",
            "    def mkdir(self):",
            "        return None",
            "",
            "    def __enter__(self):",
            "        return self",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        pass",
            "",
            "",
            "class BufferOverflow(Exception):",
            "    pass",
            "",
            "",
            "class Buffer:",
            "    def __init__(self, size: int = 16):",
            "        self._buf = bytearray(size)",
            "        self._buflen = 0",
            "        self.view = memoryview(self._buf[0:0])",
            "",
            "    def add(self, data: Union[bytes, bytearray, memoryview]):",
            "        length = len(data)",
            "        self._buf[self._buflen :] = data",
            "        self._buflen += length",
            "        self.view = memoryview(self._buf[0 : self._buflen])",
            "",
            "    def reset(self) -> None:",
            "        self._buflen = 0",
            "        self.view = memoryview(self._buf[0:0])",
            "",
            "    def set(self, data: Union[bytes, bytearray, memoryview]) -> None:",
            "        length = len(data)",
            "        self._buf[0:] = data",
            "        self._buflen = length",
            "        self.view = memoryview(self._buf[0:length])",
            "",
            "    def get(self) -> bytearray:",
            "        val = self._buf[: self._buflen]",
            "        self.reset()",
            "        return val",
            "",
            "    def __len__(self) -> int:",
            "        return self._buflen",
            "",
            "    def __bytes__(self):",
            "        return bytes(self._buf[0 : self._buflen])",
            "",
            "",
            "def remove_relative_path_marker(path: str) -> str:",
            "    \"\"\"",
            "    Removes './' from the beginning of a path-like string",
            "    \"\"\"",
            "    processed_path = path",
            "",
            "    if path.startswith(RELATIVE_PATH_MARKER):",
            "        processed_path = path[len(RELATIVE_PATH_MARKER) :]",
            "",
            "    return processed_path",
            "",
            "",
            "def get_sanitized_output_path(fname: str, path: Optional[pathlib.Path]) -> pathlib.Path:",
            "    \"\"\"",
            "    check f.filename has invalid directory traversals",
            "    do following but is_relative_to introduced in py 3.9,",
            "    so I replaced it with relative_to. when condition is not satisfied, raise ValueError",
            "    if not pathlib.Path(...).joinpath(remove_relative_path_marker(outname)).is_relative_to(...):",
            "        raise Bad7zFile",
            "    \"\"\"",
            "    if path is None:",
            "        try:",
            "            pathlib.Path(os.getcwd()).joinpath(fname).resolve().relative_to(os.getcwd())",
            "            outfile = pathlib.Path(remove_relative_path_marker(fname))",
            "        except ValueError:",
            "            raise Bad7zFile(f\"Specified path is bad: {fname}\")",
            "    else:",
            "        try:",
            "            outfile = path.joinpath(remove_relative_path_marker(fname))",
            "            outfile.resolve().relative_to(path)",
            "        except ValueError:",
            "            raise Bad7zFile(f\"Specified path is bad: {fname}\")",
            "    return outfile",
            "",
            "",
            "def check_archive_path(arcname: str) -> bool:",
            "    path = pathlib.Path(\"/foo/boo/fuga/hoge/a90sufoiasj09/dafj08sajfa/\")  # dummy path",
            "    return is_target_path_valid(path, path.joinpath(arcname))",
            "",
            "",
            "def is_target_path_valid(path: pathlib.Path, target: pathlib.Path) -> bool:",
            "    try:",
            "        if path.is_absolute():",
            "            target.resolve().relative_to(path)",
            "        else:",
            "            target.resolve().relative_to(pathlib.Path(os.getcwd()).joinpath(path))",
            "    except ValueError:",
            "        return False",
            "    return True",
            "",
            "",
            "def check_win32_file_namespace(pathname: pathlib.Path) -> pathlib.Path:",
            "    # When python on Windows and not python on Cygwin,",
            "    # Add win32 file namespace to exceed Microsoft Windows",
            "    # path length limitation to 260 bytes",
            "    # ref.",
            "    # https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file",
            "    # In editions of Windows before Windows 10 version 1607,",
            "    # the maximum length for a path is MAX_PATH, which is defined as",
            "    # 260 characters. In later versions of Windows, changing a registry key",
            "    # or select option when python installation is required to remove the limit.",
            "    if is_windows_native_python() and pathname.is_absolute() and not is_windows_unc_path(pathname):",
            "        pathname = pathlib.WindowsPath(\"\\\\\\\\?\\\\\" + str(pathname))",
            "    return pathname"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "268": [
                "islink"
            ],
            "283": [
                "readlink"
            ]
        },
        "addLocation": []
    },
    "py7zr/py7zr.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 50,
                "PatchRowcode": "     MemIO,"
            },
            "1": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 51,
                "PatchRowcode": "     NullIO,"
            },
            "2": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 52,
                "PatchRowcode": "     calculate_crc32,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+    check_archive_path,"
            },
            "4": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 54,
                "PatchRowcode": "     filetime_to_dt,"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+    get_sanitized_output_path,"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 56,
                "PatchRowcode": "+    is_target_path_valid,"
            },
            "7": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 57,
                "PatchRowcode": "     readlink,"
            },
            "8": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    remove_relative_path_marker,"
            },
            "9": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 58,
                "PatchRowcode": " )"
            },
            "10": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 59,
                "PatchRowcode": " from py7zr.properties import DEFAULT_FILTERS, FILTER_DEFLATE64, MAGIC_7Z, get_default_blocksize, get_memory_limit"
            },
            "11": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from py7zr.win32compat import is_windows_native_python, is_windows_unc_path"
            },
            "12": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 60,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 61,
                "PatchRowcode": " if sys.platform.startswith(\"win\"):"
            },
            "14": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 62,
                "PatchRowcode": "     import _winapi"
            },
            "15": {
                "beforePatchRowNumber": 567,
                "afterPatchRowNumber": 568,
                "PatchRowcode": "                         break"
            },
            "16": {
                "beforePatchRowNumber": 568,
                "afterPatchRowNumber": 569,
                "PatchRowcode": "                     i += 1"
            },
            "17": {
                "beforePatchRowNumber": 569,
                "afterPatchRowNumber": 570,
                "PatchRowcode": "             fnames.append(outname)"
            },
            "18": {
                "beforePatchRowNumber": 570,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # check f.filename has invalid directory traversals"
            },
            "19": {
                "beforePatchRowNumber": 571,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if path is None:"
            },
            "20": {
                "beforePatchRowNumber": 572,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                # do following but is_relative_to introduced in py 3.9"
            },
            "21": {
                "beforePatchRowNumber": 573,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                # so I replaced it with relative_to. when condition is not satisfied, raise ValueError"
            },
            "22": {
                "beforePatchRowNumber": 574,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                # if not pathlib.Path(...).joinpath(remove_relative_path_marker(outname)).is_relative_to(...):"
            },
            "23": {
                "beforePatchRowNumber": 575,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                #    raise Bad7zFile"
            },
            "24": {
                "beforePatchRowNumber": 576,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                try:"
            },
            "25": {
                "beforePatchRowNumber": 577,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    pathlib.Path(os.getcwd()).joinpath(remove_relative_path_marker(outname)).relative_to(os.getcwd())"
            },
            "26": {
                "beforePatchRowNumber": 578,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                except ValueError:"
            },
            "27": {
                "beforePatchRowNumber": 579,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    raise Bad7zFile"
            },
            "28": {
                "beforePatchRowNumber": 580,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                outfilename = pathlib.Path(remove_relative_path_marker(outname))"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 571,
                "PatchRowcode": "+            if path is None or path.is_absolute():"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 572,
                "PatchRowcode": "+                outfilename = get_sanitized_output_path(outname, path)"
            },
            "31": {
                "beforePatchRowNumber": 581,
                "afterPatchRowNumber": 573,
                "PatchRowcode": "             else:"
            },
            "32": {
                "beforePatchRowNumber": 582,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                outfilename = path.joinpath(remove_relative_path_marker(outname))"
            },
            "33": {
                "beforePatchRowNumber": 583,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                try:"
            },
            "34": {
                "beforePatchRowNumber": 584,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    outfilename.relative_to(path)"
            },
            "35": {
                "beforePatchRowNumber": 585,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                except ValueError:"
            },
            "36": {
                "beforePatchRowNumber": 586,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    raise Bad7zFile"
            },
            "37": {
                "beforePatchRowNumber": 587,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # When python on Windows and not python on Cygwin,"
            },
            "38": {
                "beforePatchRowNumber": 588,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # Add win32 file namespace to exceed microsoft windows"
            },
            "39": {
                "beforePatchRowNumber": 589,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # path length limitation to 260 bytes"
            },
            "40": {
                "beforePatchRowNumber": 590,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # ref."
            },
            "41": {
                "beforePatchRowNumber": 591,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file"
            },
            "42": {
                "beforePatchRowNumber": 592,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # In editions of Windows before Windows 10 version 1607,"
            },
            "43": {
                "beforePatchRowNumber": 593,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # the maximum length for a path is MAX_PATH, which is defined as"
            },
            "44": {
                "beforePatchRowNumber": 594,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # 260 characters. In later versions of Windows, changing a registry key"
            },
            "45": {
                "beforePatchRowNumber": 595,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # or select option when python installation is required to remove the limit."
            },
            "46": {
                "beforePatchRowNumber": 596,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if is_windows_native_python() and outfilename.is_absolute() and not is_windows_unc_path(outfilename):"
            },
            "47": {
                "beforePatchRowNumber": 597,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                outfilename = pathlib.WindowsPath(\"\\\\\\\\?\\\\\" + str(outfilename))"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 574,
                "PatchRowcode": "+                outfilename = get_sanitized_output_path(outname, pathlib.Path(os.getcwd()).joinpath(path))"
            },
            "49": {
                "beforePatchRowNumber": 598,
                "afterPatchRowNumber": 575,
                "PatchRowcode": "             if targets is not None and f.filename not in targets:"
            },
            "50": {
                "beforePatchRowNumber": 599,
                "afterPatchRowNumber": 576,
                "PatchRowcode": "                 self.worker.register_filelike(f.id, None)"
            },
            "51": {
                "beforePatchRowNumber": 600,
                "afterPatchRowNumber": 577,
                "PatchRowcode": "                 continue"
            },
            "52": {
                "beforePatchRowNumber": 634,
                "afterPatchRowNumber": 611,
                "PatchRowcode": "         if callback is not None:"
            },
            "53": {
                "beforePatchRowNumber": 635,
                "afterPatchRowNumber": 612,
                "PatchRowcode": "             self.worker.extract("
            },
            "54": {
                "beforePatchRowNumber": 636,
                "afterPatchRowNumber": 613,
                "PatchRowcode": "                 self.fp,"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 614,
                "PatchRowcode": "+                path,"
            },
            "56": {
                "beforePatchRowNumber": 637,
                "afterPatchRowNumber": 615,
                "PatchRowcode": "                 parallel=(not self.password_protected and not self._filePassed),"
            },
            "57": {
                "beforePatchRowNumber": 638,
                "afterPatchRowNumber": 616,
                "PatchRowcode": "                 q=self.q,"
            },
            "58": {
                "beforePatchRowNumber": 639,
                "afterPatchRowNumber": 617,
                "PatchRowcode": "             )"
            },
            "59": {
                "beforePatchRowNumber": 640,
                "afterPatchRowNumber": 618,
                "PatchRowcode": "         else:"
            },
            "60": {
                "beforePatchRowNumber": 641,
                "afterPatchRowNumber": 619,
                "PatchRowcode": "             self.worker.extract("
            },
            "61": {
                "beforePatchRowNumber": 642,
                "afterPatchRowNumber": 620,
                "PatchRowcode": "                 self.fp,"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 621,
                "PatchRowcode": "+                path,"
            },
            "63": {
                "beforePatchRowNumber": 643,
                "afterPatchRowNumber": 622,
                "PatchRowcode": "                 parallel=(not self.password_protected and not self._filePassed),"
            },
            "64": {
                "beforePatchRowNumber": 644,
                "afterPatchRowNumber": 623,
                "PatchRowcode": "             )"
            },
            "65": {
                "beforePatchRowNumber": 645,
                "afterPatchRowNumber": 624,
                "PatchRowcode": " "
            },
            "66": {
                "beforePatchRowNumber": 1040,
                "afterPatchRowNumber": 1019,
                "PatchRowcode": "             self.writef(input, target)"
            },
            "67": {
                "beforePatchRowNumber": 1041,
                "afterPatchRowNumber": 1020,
                "PatchRowcode": " "
            },
            "68": {
                "beforePatchRowNumber": 1042,
                "afterPatchRowNumber": 1021,
                "PatchRowcode": "     def writef(self, bio: IO[Any], arcname: str):"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1022,
                "PatchRowcode": "+        if not check_archive_path(arcname):"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1023,
                "PatchRowcode": "+            raise ValueError(f\"Specified path is bad: {arcname}\")"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1024,
                "PatchRowcode": "+        return self._writef(bio, arcname)"
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1025,
                "PatchRowcode": "+"
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1026,
                "PatchRowcode": "+    def _writef(self, bio: IO[Any], arcname: str):"
            },
            "74": {
                "beforePatchRowNumber": 1043,
                "afterPatchRowNumber": 1027,
                "PatchRowcode": "         if isinstance(bio, io.BytesIO):"
            },
            "75": {
                "beforePatchRowNumber": 1044,
                "afterPatchRowNumber": 1028,
                "PatchRowcode": "             size = bio.getbuffer().nbytes"
            },
            "76": {
                "beforePatchRowNumber": 1045,
                "afterPatchRowNumber": 1029,
                "PatchRowcode": "         elif isinstance(bio, io.TextIOBase):"
            },
            "77": {
                "beforePatchRowNumber": 1069,
                "afterPatchRowNumber": 1053,
                "PatchRowcode": "             self.files.append(file_info)"
            },
            "78": {
                "beforePatchRowNumber": 1070,
                "afterPatchRowNumber": 1054,
                "PatchRowcode": " "
            },
            "79": {
                "beforePatchRowNumber": 1071,
                "afterPatchRowNumber": 1055,
                "PatchRowcode": "     def writestr(self, data: Union[str, bytes, bytearray, memoryview], arcname: str):"
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1056,
                "PatchRowcode": "+        if not check_archive_path(arcname):"
            },
            "81": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1057,
                "PatchRowcode": "+            raise ValueError(f\"Specified path is bad: {arcname}\")"
            },
            "82": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1058,
                "PatchRowcode": "+        return self._writestr(data, arcname)"
            },
            "83": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1059,
                "PatchRowcode": "+"
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1060,
                "PatchRowcode": "+    def _writestr(self, data: Union[str, bytes, bytearray, memoryview], arcname: str):"
            },
            "85": {
                "beforePatchRowNumber": 1072,
                "afterPatchRowNumber": 1061,
                "PatchRowcode": "         if not isinstance(arcname, str):"
            },
            "86": {
                "beforePatchRowNumber": 1073,
                "afterPatchRowNumber": 1062,
                "PatchRowcode": "             raise ValueError(\"Unsupported arcname\")"
            },
            "87": {
                "beforePatchRowNumber": 1074,
                "afterPatchRowNumber": 1063,
                "PatchRowcode": "         if isinstance(data, str):"
            },
            "88": {
                "beforePatchRowNumber": 1075,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self.writef(io.BytesIO(data.encode(\"UTF-8\")), arcname)"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1064,
                "PatchRowcode": "+            self._writef(io.BytesIO(data.encode(\"UTF-8\")), arcname)"
            },
            "90": {
                "beforePatchRowNumber": 1076,
                "afterPatchRowNumber": 1065,
                "PatchRowcode": "         elif isinstance(data, bytes) or isinstance(data, bytearray) or isinstance(data, memoryview):"
            },
            "91": {
                "beforePatchRowNumber": 1077,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self.writef(io.BytesIO(data), arcname)"
            },
            "92": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1066,
                "PatchRowcode": "+            self._writef(io.BytesIO(bytes(data)), arcname)"
            },
            "93": {
                "beforePatchRowNumber": 1078,
                "afterPatchRowNumber": 1067,
                "PatchRowcode": "         else:"
            },
            "94": {
                "beforePatchRowNumber": 1079,
                "afterPatchRowNumber": 1068,
                "PatchRowcode": "             raise ValueError(\"Unsupported data type.\")"
            },
            "95": {
                "beforePatchRowNumber": 1080,
                "afterPatchRowNumber": 1069,
                "PatchRowcode": " "
            },
            "96": {
                "beforePatchRowNumber": 1131,
                "afterPatchRowNumber": 1120,
                "PatchRowcode": "         for f in self.files:"
            },
            "97": {
                "beforePatchRowNumber": 1132,
                "afterPatchRowNumber": 1121,
                "PatchRowcode": "             self.worker.register_filelike(f.id, None)"
            },
            "98": {
                "beforePatchRowNumber": 1133,
                "afterPatchRowNumber": 1122,
                "PatchRowcode": "         try:"
            },
            "99": {
                "beforePatchRowNumber": 1134,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self.worker.extract(self.fp, parallel=(not self.password_protected), skip_notarget=False)  # TODO: print progress"
            },
            "100": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1123,
                "PatchRowcode": "+            self.worker.extract("
            },
            "101": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1124,
                "PatchRowcode": "+                self.fp, None, parallel=(not self.password_protected), skip_notarget=False"
            },
            "102": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1125,
                "PatchRowcode": "+            )  # TODO: print progress"
            },
            "103": {
                "beforePatchRowNumber": 1135,
                "afterPatchRowNumber": 1126,
                "PatchRowcode": "         except CrcError as crce:"
            },
            "104": {
                "beforePatchRowNumber": 1136,
                "afterPatchRowNumber": 1127,
                "PatchRowcode": "             return crce.args[2]"
            },
            "105": {
                "beforePatchRowNumber": 1137,
                "afterPatchRowNumber": 1128,
                "PatchRowcode": "         else:"
            },
            "106": {
                "beforePatchRowNumber": 1200,
                "afterPatchRowNumber": 1191,
                "PatchRowcode": "         else:"
            },
            "107": {
                "beforePatchRowNumber": 1201,
                "afterPatchRowNumber": 1192,
                "PatchRowcode": "             self.concurrent = Thread"
            },
            "108": {
                "beforePatchRowNumber": 1202,
                "afterPatchRowNumber": 1193,
                "PatchRowcode": " "
            },
            "109": {
                "beforePatchRowNumber": 1203,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def extract(self, fp: BinaryIO, parallel: bool, skip_notarget=True, q=None) -> None:"
            },
            "110": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1194,
                "PatchRowcode": "+    def extract(self, fp: BinaryIO, path: Optional[pathlib.Path], parallel: bool, skip_notarget=True, q=None) -> None:"
            },
            "111": {
                "beforePatchRowNumber": 1204,
                "afterPatchRowNumber": 1195,
                "PatchRowcode": "         \"\"\"Extract worker method to handle 7zip folder and decompress each files.\"\"\""
            },
            "112": {
                "beforePatchRowNumber": 1205,
                "afterPatchRowNumber": 1196,
                "PatchRowcode": "         if hasattr(self.header, \"main_streams\") and self.header.main_streams is not None:"
            },
            "113": {
                "beforePatchRowNumber": 1206,
                "afterPatchRowNumber": 1197,
                "PatchRowcode": "             src_end = self.src_start + self.header.main_streams.packinfo.packpositions[-1]"
            },
            "114": {
                "beforePatchRowNumber": 1209,
                "afterPatchRowNumber": 1200,
                "PatchRowcode": "                 self.extract_single("
            },
            "115": {
                "beforePatchRowNumber": 1210,
                "afterPatchRowNumber": 1201,
                "PatchRowcode": "                     fp,"
            },
            "116": {
                "beforePatchRowNumber": 1211,
                "afterPatchRowNumber": 1202,
                "PatchRowcode": "                     self.files,"
            },
            "117": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1203,
                "PatchRowcode": "+                    path,"
            },
            "118": {
                "beforePatchRowNumber": 1212,
                "afterPatchRowNumber": 1204,
                "PatchRowcode": "                     self.src_start,"
            },
            "119": {
                "beforePatchRowNumber": 1213,
                "afterPatchRowNumber": 1205,
                "PatchRowcode": "                     src_end,"
            },
            "120": {
                "beforePatchRowNumber": 1214,
                "afterPatchRowNumber": 1206,
                "PatchRowcode": "                     q,"
            },
            "121": {
                "beforePatchRowNumber": 1219,
                "afterPatchRowNumber": 1211,
                "PatchRowcode": "                 positions = self.header.main_streams.packinfo.packpositions"
            },
            "122": {
                "beforePatchRowNumber": 1220,
                "afterPatchRowNumber": 1212,
                "PatchRowcode": "                 empty_files = [f for f in self.files if f.emptystream]"
            },
            "123": {
                "beforePatchRowNumber": 1221,
                "afterPatchRowNumber": 1213,
                "PatchRowcode": "                 if not parallel:"
            },
            "124": {
                "beforePatchRowNumber": 1222,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    self.extract_single(fp, empty_files, 0, 0, q)"
            },
            "125": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1214,
                "PatchRowcode": "+                    self.extract_single(fp, empty_files, path, 0, 0, q)"
            },
            "126": {
                "beforePatchRowNumber": 1223,
                "afterPatchRowNumber": 1215,
                "PatchRowcode": "                     for i in range(numfolders):"
            },
            "127": {
                "beforePatchRowNumber": 1224,
                "afterPatchRowNumber": 1216,
                "PatchRowcode": "                         if skip_notarget:"
            },
            "128": {
                "beforePatchRowNumber": 1225,
                "afterPatchRowNumber": 1217,
                "PatchRowcode": "                             if not any([self.target_filepath.get(f.id, None) for f in folders[i].files]):"
            },
            "129": {
                "beforePatchRowNumber": 1226,
                "afterPatchRowNumber": 1218,
                "PatchRowcode": "                                 continue"
            },
            "130": {
                "beforePatchRowNumber": 1227,
                "afterPatchRowNumber": 1219,
                "PatchRowcode": "                         self.extract_single("
            },
            "131": {
                "beforePatchRowNumber": 1228,
                "afterPatchRowNumber": 1220,
                "PatchRowcode": "                             fp,"
            },
            "132": {
                "beforePatchRowNumber": 1229,
                "afterPatchRowNumber": 1221,
                "PatchRowcode": "                             folders[i].files,"
            },
            "133": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1222,
                "PatchRowcode": "+                            path,"
            },
            "134": {
                "beforePatchRowNumber": 1230,
                "afterPatchRowNumber": 1223,
                "PatchRowcode": "                             self.src_start + positions[i],"
            },
            "135": {
                "beforePatchRowNumber": 1231,
                "afterPatchRowNumber": 1224,
                "PatchRowcode": "                             self.src_start + positions[i + 1],"
            },
            "136": {
                "beforePatchRowNumber": 1232,
                "afterPatchRowNumber": 1225,
                "PatchRowcode": "                             q,"
            },
            "137": {
                "beforePatchRowNumber": 1236,
                "afterPatchRowNumber": 1229,
                "PatchRowcode": "                     if getattr(fp, \"name\", None) is None:"
            },
            "138": {
                "beforePatchRowNumber": 1237,
                "afterPatchRowNumber": 1230,
                "PatchRowcode": "                         raise InternalError(\"Caught unknown variable status error\")"
            },
            "139": {
                "beforePatchRowNumber": 1238,
                "afterPatchRowNumber": 1231,
                "PatchRowcode": "                     filename: str = getattr(fp, \"name\", \"\")  # do not become \"\" but it is for type check."
            },
            "140": {
                "beforePatchRowNumber": 1239,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    self.extract_single(open(filename, \"rb\"), empty_files, 0, 0, q)"
            },
            "141": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1232,
                "PatchRowcode": "+                    self.extract_single(open(filename, \"rb\"), empty_files, path, 0, 0, q)"
            },
            "142": {
                "beforePatchRowNumber": 1240,
                "afterPatchRowNumber": 1233,
                "PatchRowcode": "                     concurrent_tasks = []"
            },
            "143": {
                "beforePatchRowNumber": 1241,
                "afterPatchRowNumber": 1234,
                "PatchRowcode": "                     exc_q: queue.Queue = queue.Queue()"
            },
            "144": {
                "beforePatchRowNumber": 1242,
                "afterPatchRowNumber": 1235,
                "PatchRowcode": "                     for i in range(numfolders):"
            },
            "145": {
                "beforePatchRowNumber": 1248,
                "afterPatchRowNumber": 1241,
                "PatchRowcode": "                             args=("
            },
            "146": {
                "beforePatchRowNumber": 1249,
                "afterPatchRowNumber": 1242,
                "PatchRowcode": "                                 filename,"
            },
            "147": {
                "beforePatchRowNumber": 1250,
                "afterPatchRowNumber": 1243,
                "PatchRowcode": "                                 folders[i].files,"
            },
            "148": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1244,
                "PatchRowcode": "+                                path,"
            },
            "149": {
                "beforePatchRowNumber": 1251,
                "afterPatchRowNumber": 1245,
                "PatchRowcode": "                                 self.src_start + positions[i],"
            },
            "150": {
                "beforePatchRowNumber": 1252,
                "afterPatchRowNumber": 1246,
                "PatchRowcode": "                                 self.src_start + positions[i + 1],"
            },
            "151": {
                "beforePatchRowNumber": 1253,
                "afterPatchRowNumber": 1247,
                "PatchRowcode": "                                 q,"
            },
            "152": {
                "beforePatchRowNumber": 1266,
                "afterPatchRowNumber": 1260,
                "PatchRowcode": "                         raise exc_info[1].with_traceback(exc_info[2])"
            },
            "153": {
                "beforePatchRowNumber": 1267,
                "afterPatchRowNumber": 1261,
                "PatchRowcode": "         else:"
            },
            "154": {
                "beforePatchRowNumber": 1268,
                "afterPatchRowNumber": 1262,
                "PatchRowcode": "             empty_files = [f for f in self.files if f.emptystream]"
            },
            "155": {
                "beforePatchRowNumber": 1269,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self.extract_single(fp, empty_files, 0, 0, q)"
            },
            "156": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1263,
                "PatchRowcode": "+            self.extract_single(fp, empty_files, path, 0, 0, q)"
            },
            "157": {
                "beforePatchRowNumber": 1270,
                "afterPatchRowNumber": 1264,
                "PatchRowcode": " "
            },
            "158": {
                "beforePatchRowNumber": 1271,
                "afterPatchRowNumber": 1265,
                "PatchRowcode": "     def extract_single("
            },
            "159": {
                "beforePatchRowNumber": 1272,
                "afterPatchRowNumber": 1266,
                "PatchRowcode": "         self,"
            },
            "160": {
                "beforePatchRowNumber": 1273,
                "afterPatchRowNumber": 1267,
                "PatchRowcode": "         fp: Union[BinaryIO, str],"
            },
            "161": {
                "beforePatchRowNumber": 1274,
                "afterPatchRowNumber": 1268,
                "PatchRowcode": "         files,"
            },
            "162": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1269,
                "PatchRowcode": "+        path,"
            },
            "163": {
                "beforePatchRowNumber": 1275,
                "afterPatchRowNumber": 1270,
                "PatchRowcode": "         src_start: int,"
            },
            "164": {
                "beforePatchRowNumber": 1276,
                "afterPatchRowNumber": 1271,
                "PatchRowcode": "         src_end: int,"
            },
            "165": {
                "beforePatchRowNumber": 1277,
                "afterPatchRowNumber": 1272,
                "PatchRowcode": "         q: Optional[queue.Queue],"
            },
            "166": {
                "beforePatchRowNumber": 1287,
                "afterPatchRowNumber": 1282,
                "PatchRowcode": "             if isinstance(fp, str):"
            },
            "167": {
                "beforePatchRowNumber": 1288,
                "afterPatchRowNumber": 1283,
                "PatchRowcode": "                 fp = open(fp, \"rb\")"
            },
            "168": {
                "beforePatchRowNumber": 1289,
                "afterPatchRowNumber": 1284,
                "PatchRowcode": "             fp.seek(src_start)"
            },
            "169": {
                "beforePatchRowNumber": 1290,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self._extract_single(fp, files, src_end, q, skip_notarget)"
            },
            "170": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1285,
                "PatchRowcode": "+            self._extract_single(fp, files, path, src_end, q, skip_notarget)"
            },
            "171": {
                "beforePatchRowNumber": 1291,
                "afterPatchRowNumber": 1286,
                "PatchRowcode": "         except Exception as e:"
            },
            "172": {
                "beforePatchRowNumber": 1292,
                "afterPatchRowNumber": 1287,
                "PatchRowcode": "             if exc_q is None:"
            },
            "173": {
                "beforePatchRowNumber": 1293,
                "afterPatchRowNumber": 1288,
                "PatchRowcode": "                 raise e"
            },
            "174": {
                "beforePatchRowNumber": 1299,
                "afterPatchRowNumber": 1294,
                "PatchRowcode": "         self,"
            },
            "175": {
                "beforePatchRowNumber": 1300,
                "afterPatchRowNumber": 1295,
                "PatchRowcode": "         fp: BinaryIO,"
            },
            "176": {
                "beforePatchRowNumber": 1301,
                "afterPatchRowNumber": 1296,
                "PatchRowcode": "         files,"
            },
            "177": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1297,
                "PatchRowcode": "+        path,"
            },
            "178": {
                "beforePatchRowNumber": 1302,
                "afterPatchRowNumber": 1298,
                "PatchRowcode": "         src_end: int,"
            },
            "179": {
                "beforePatchRowNumber": 1303,
                "afterPatchRowNumber": 1299,
                "PatchRowcode": "         q: Optional[queue.Queue],"
            },
            "180": {
                "beforePatchRowNumber": 1304,
                "afterPatchRowNumber": 1300,
                "PatchRowcode": "         skip_notarget=True,"
            },
            "181": {
                "beforePatchRowNumber": 1331,
                "afterPatchRowNumber": 1327,
                "PatchRowcode": "                         with io.BytesIO() as ofp:"
            },
            "182": {
                "beforePatchRowNumber": 1332,
                "afterPatchRowNumber": 1328,
                "PatchRowcode": "                             self.decompress(fp, f.folder, ofp, f.uncompressed, f.compressed, src_end)"
            },
            "183": {
                "beforePatchRowNumber": 1333,
                "afterPatchRowNumber": 1329,
                "PatchRowcode": "                             dst: str = ofp.read().decode(\"utf-8\")"
            },
            "184": {
                "beforePatchRowNumber": 1334,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            # fileish.unlink(missing_ok=True) > py3.7"
            },
            "185": {
                "beforePatchRowNumber": 1335,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            if fileish.exists():"
            },
            "186": {
                "beforePatchRowNumber": 1336,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                fileish.unlink()"
            },
            "187": {
                "beforePatchRowNumber": 1337,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            if sys.platform == \"win32\":  # hint for mypy"
            },
            "188": {
                "beforePatchRowNumber": 1338,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                _winapi.CreateJunction(str(fileish), dst)  # noqa"
            },
            "189": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1330,
                "PatchRowcode": "+                            if is_target_path_valid(path, fileish.parent.joinpath(dst)):"
            },
            "190": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1331,
                "PatchRowcode": "+                                # fileish.unlink(missing_ok=True) > py3.7"
            },
            "191": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1332,
                "PatchRowcode": "+                                if fileish.exists():"
            },
            "192": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1333,
                "PatchRowcode": "+                                    fileish.unlink()"
            },
            "193": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1334,
                "PatchRowcode": "+                                if sys.platform == \"win32\":  # hint for mypy"
            },
            "194": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1335,
                "PatchRowcode": "+                                    _winapi.CreateJunction(str(fileish), dst)  # noqa"
            },
            "195": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1336,
                "PatchRowcode": "+                            else:"
            },
            "196": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1337,
                "PatchRowcode": "+                                raise Bad7zFile(\"Junction point out of target directory.\")"
            },
            "197": {
                "beforePatchRowNumber": 1339,
                "afterPatchRowNumber": 1338,
                "PatchRowcode": "                     elif f.is_symlink and not isinstance(fileish, MemIO):"
            },
            "198": {
                "beforePatchRowNumber": 1340,
                "afterPatchRowNumber": 1339,
                "PatchRowcode": "                         with io.BytesIO() as omfp:"
            },
            "199": {
                "beforePatchRowNumber": 1341,
                "afterPatchRowNumber": 1340,
                "PatchRowcode": "                             self.decompress(fp, f.folder, omfp, f.uncompressed, f.compressed, src_end)"
            },
            "200": {
                "beforePatchRowNumber": 1342,
                "afterPatchRowNumber": 1341,
                "PatchRowcode": "                             omfp.seek(0)"
            },
            "201": {
                "beforePatchRowNumber": 1343,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            sym_target = pathlib.Path(omfp.read().decode(\"utf-8\"))"
            },
            "202": {
                "beforePatchRowNumber": 1344,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            # fileish.unlink(missing_ok=True) > py3.7"
            },
            "203": {
                "beforePatchRowNumber": 1345,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            if fileish.exists():"
            },
            "204": {
                "beforePatchRowNumber": 1346,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                fileish.unlink()"
            },
            "205": {
                "beforePatchRowNumber": 1347,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            fileish.symlink_to(sym_target)"
            },
            "206": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1342,
                "PatchRowcode": "+                            dst = omfp.read().decode(\"utf-8\")"
            },
            "207": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1343,
                "PatchRowcode": "+                            # check sym_target points inside an archive target?"
            },
            "208": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1344,
                "PatchRowcode": "+                            if is_target_path_valid(path, fileish.parent.joinpath(dst)):"
            },
            "209": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1345,
                "PatchRowcode": "+                                sym_target = pathlib.Path(dst)"
            },
            "210": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1346,
                "PatchRowcode": "+                                # fileish.unlink(missing_ok=True) > py3.7"
            },
            "211": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1347,
                "PatchRowcode": "+                                if fileish.exists():"
            },
            "212": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1348,
                "PatchRowcode": "+                                    fileish.unlink()"
            },
            "213": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1349,
                "PatchRowcode": "+                                fileish.symlink_to(sym_target)"
            },
            "214": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1350,
                "PatchRowcode": "+                            else:"
            },
            "215": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1351,
                "PatchRowcode": "+                                raise Bad7zFile(\"Symlink point out of target directory.\")"
            },
            "216": {
                "beforePatchRowNumber": 1348,
                "afterPatchRowNumber": 1352,
                "PatchRowcode": "                     else:"
            },
            "217": {
                "beforePatchRowNumber": 1349,
                "afterPatchRowNumber": 1353,
                "PatchRowcode": "                         with fileish.open(mode=\"wb\") as obfp:"
            },
            "218": {
                "beforePatchRowNumber": 1350,
                "afterPatchRowNumber": 1354,
                "PatchRowcode": "                             crc32 = self.decompress(fp, f.folder, obfp, f.uncompressed, f.compressed, src_end)"
            }
        },
        "frontPatchFile": [
            "#!/usr/bin/python -u",
            "#",
            "# p7zr library",
            "#",
            "# Copyright (c) 2019-2021 Hiroshi Miura <miurahr@linux.com>",
            "# Copyright (c) 2004-2015 by Joachim Bauch, mail@joachim-bauch.de",
            "# 7-Zip Copyright (C) 1999-2010 Igor Pavlov",
            "# LZMA SDK Copyright (C) 1999-2010 Igor Pavlov",
            "#",
            "# This library is free software; you can redistribute it and/or",
            "# modify it under the terms of the GNU Lesser General Public",
            "# License as published by the Free Software Foundation; either",
            "# version 2.1 of the License, or (at your option) any later version.",
            "#",
            "# This library is distributed in the hope that it will be useful,",
            "# but WITHOUT ANY WARRANTY; without even the implied warranty of",
            "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU",
            "# Lesser General Public License for more details.",
            "#",
            "# You should have received a copy of the GNU Lesser General Public",
            "# License along with this library; if not, write to the Free Software",
            "# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA",
            "#",
            "#",
            "\"\"\"Read 7zip format archives.\"\"\"",
            "import collections.abc",
            "import contextlib",
            "import datetime",
            "import errno",
            "import functools",
            "import gc",
            "import io",
            "import os",
            "import pathlib",
            "import queue",
            "import stat",
            "import sys",
            "from multiprocessing import Process",
            "from threading import Thread",
            "from typing import IO, Any, BinaryIO, Dict, List, Optional, Tuple, Type, Union",
            "",
            "import multivolumefile",
            "",
            "from py7zr.archiveinfo import Folder, Header, SignatureHeader",
            "from py7zr.callbacks import ExtractCallback",
            "from py7zr.compressor import SupportedMethods, get_methods_names",
            "from py7zr.exceptions import Bad7zFile, CrcError, DecompressionError, InternalError, UnsupportedCompressionMethodError",
            "from py7zr.helpers import (",
            "    ArchiveTimestamp,",
            "    MemIO,",
            "    NullIO,",
            "    calculate_crc32,",
            "    filetime_to_dt,",
            "    readlink,",
            "    remove_relative_path_marker,",
            ")",
            "from py7zr.properties import DEFAULT_FILTERS, FILTER_DEFLATE64, MAGIC_7Z, get_default_blocksize, get_memory_limit",
            "from py7zr.win32compat import is_windows_native_python, is_windows_unc_path",
            "",
            "if sys.platform.startswith(\"win\"):",
            "    import _winapi",
            "",
            "FILE_ATTRIBUTE_UNIX_EXTENSION = 0x8000",
            "FILE_ATTRIBUTE_WINDOWS_MASK = 0x07FFF",
            "",
            "",
            "class ArchiveFile:",
            "    \"\"\"Represent each files metadata inside archive file.",
            "    It holds file properties; filename, permissions, and type whether",
            "    it is directory, link or normal file.",
            "",
            "    Instances of the :class:`ArchiveFile` class are returned by iterating :attr:`files_list` of",
            "    :class:`SevenZipFile` objects.",
            "    Each object stores information about a single member of the 7z archive. Most of users use :meth:`extractall()`.",
            "",
            "    The class also hold an archive parameter where file is exist in",
            "    archive file folder(container).\"\"\"",
            "",
            "    def __init__(self, id: int, file_info: Dict[str, Any]) -> None:",
            "        self.id = id",
            "        self._file_info = file_info",
            "",
            "    def file_properties(self) -> Dict[str, Any]:",
            "        \"\"\"Return file properties as a hash object. Following keys are included: \u2018readonly\u2019, \u2018is_directory\u2019,",
            "        \u2018posix_mode\u2019, \u2018archivable\u2019, \u2018emptystream\u2019, \u2018filename\u2019, \u2018creationtime\u2019, \u2018lastaccesstime\u2019,",
            "        \u2018lastwritetime\u2019, \u2018attributes\u2019",
            "        \"\"\"",
            "        properties = self._file_info",
            "        if properties is not None:",
            "            properties[\"readonly\"] = self.readonly",
            "            properties[\"posix_mode\"] = self.posix_mode",
            "            properties[\"archivable\"] = self.archivable",
            "            properties[\"is_directory\"] = self.is_directory",
            "        return properties",
            "",
            "    def _get_property(self, key: str) -> Any:",
            "        try:",
            "            return self._file_info[key]",
            "        except KeyError:",
            "            return None",
            "",
            "    @property",
            "    def origin(self) -> pathlib.Path:",
            "        return self._get_property(\"origin\")",
            "",
            "    @property",
            "    def folder(self) -> Folder:",
            "        return self._get_property(\"folder\")",
            "",
            "    @property",
            "    def filename(self) -> str:",
            "        \"\"\"return filename of archive file.\"\"\"",
            "        return self._get_property(\"filename\")",
            "",
            "    @property",
            "    def emptystream(self) -> bool:",
            "        \"\"\"True if file is empty(0-byte file), otherwise False\"\"\"",
            "        return self._get_property(\"emptystream\")",
            "",
            "    @property",
            "    def uncompressed(self) -> List[int]:",
            "        return self._get_property(\"uncompressed\")",
            "",
            "    @property",
            "    def compressed(self) -> Optional[int]:",
            "        \"\"\"Compressed size\"\"\"",
            "        return self._get_property(\"compressed\")",
            "",
            "    @property",
            "    def crc32(self) -> Optional[int]:",
            "        \"\"\"CRC of archived file(optional)\"\"\"",
            "        return self._get_property(\"digest\")",
            "",
            "    def _test_attribute(self, target_bit: int) -> bool:",
            "        attributes = self._get_property(\"attributes\")",
            "        if attributes is None:",
            "            return False",
            "        return attributes & target_bit == target_bit",
            "",
            "    @property",
            "    def archivable(self) -> bool:",
            "        \"\"\"File has a Windows `archive` flag.\"\"\"",
            "        if hasattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\"):",
            "            return self._test_attribute(getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\"))",
            "        return False",
            "",
            "    @property",
            "    def is_directory(self) -> bool:",
            "        \"\"\"True if file is a directory, otherwise False.\"\"\"",
            "        if hasattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\"):",
            "            return self._test_attribute(getattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\"))",
            "        return False",
            "",
            "    @property",
            "    def readonly(self) -> bool:",
            "        \"\"\"True if file is readonly, otherwise False.\"\"\"",
            "        if hasattr(stat, \"FILE_ATTRIBUTE_READONLY\"):",
            "            return self._test_attribute(getattr(stat, \"FILE_ATTRIBUTE_READONLY\"))",
            "        return False",
            "",
            "    def _get_unix_extension(self) -> Optional[int]:",
            "        attributes = self._get_property(\"attributes\")",
            "        if self._test_attribute(FILE_ATTRIBUTE_UNIX_EXTENSION):",
            "            return attributes >> 16",
            "        return None",
            "",
            "    def data(self) -> Optional[BinaryIO]:",
            "        return self._get_property(\"data\")",
            "",
            "    def has_strdata(self) -> bool:",
            "        \"\"\"True if file content is set by writestr() method otherwise False.\"\"\"",
            "        return \"data\" in self._file_info",
            "",
            "    @property",
            "    def is_symlink(self) -> bool:",
            "        \"\"\"True if file is a symbolic link, otherwise False.\"\"\"",
            "        e = self._get_unix_extension()",
            "        if e is not None:",
            "            return stat.S_ISLNK(e)",
            "        if hasattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\"):",
            "            return self._test_attribute(getattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\"))",
            "        return False",
            "",
            "    @property",
            "    def is_junction(self) -> bool:",
            "        \"\"\"True if file is a junction/reparse point on windows, otherwise False.\"\"\"",
            "        if hasattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\"):",
            "            return self._test_attribute(",
            "                getattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\") | getattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\")",
            "            )",
            "        return False",
            "",
            "    @property",
            "    def is_socket(self) -> bool:",
            "        \"\"\"True if file is a socket, otherwise False.\"\"\"",
            "        e = self._get_unix_extension()",
            "        if e is not None:",
            "            return stat.S_ISSOCK(e)",
            "        return False",
            "",
            "    @property",
            "    def lastwritetime(self) -> Optional[ArchiveTimestamp]:",
            "        \"\"\"Return last written timestamp of a file.\"\"\"",
            "        return self._get_property(\"lastwritetime\")",
            "",
            "    @property",
            "    def posix_mode(self) -> Optional[int]:",
            "        \"\"\"",
            "        posix mode when a member has a unix extension property, or None",
            "        :return: Return file stat mode can be set by os.chmod()",
            "        \"\"\"",
            "        e = self._get_unix_extension()",
            "        if e is not None:",
            "            return stat.S_IMODE(e)",
            "        return None",
            "",
            "    @property",
            "    def st_fmt(self) -> Optional[int]:",
            "        \"\"\"",
            "        :return: Return the portion of the file mode that describes the file type",
            "        \"\"\"",
            "        e = self._get_unix_extension()",
            "        if e is not None:",
            "            return stat.S_IFMT(e)",
            "        return None",
            "",
            "",
            "class ArchiveFileList(collections.abc.Iterable):",
            "    \"\"\"Iteratable container of ArchiveFile.\"\"\"",
            "",
            "    def __init__(self, offset: int = 0):",
            "        self.files_list: List[dict] = []",
            "        self.index = 0",
            "        self.offset = offset",
            "",
            "    def append(self, file_info: Dict[str, Any]) -> None:",
            "        self.files_list.append(file_info)",
            "",
            "    def __len__(self) -> int:",
            "        return len(self.files_list)",
            "",
            "    def __iter__(self) -> \"ArchiveFileListIterator\":",
            "        return ArchiveFileListIterator(self)",
            "",
            "    def __getitem__(self, index):",
            "        if index > len(self.files_list):",
            "            raise IndexError",
            "        if index < 0:",
            "            raise IndexError",
            "        res = ArchiveFile(index + self.offset, self.files_list[index])",
            "        return res",
            "",
            "",
            "class ArchiveFileListIterator(collections.abc.Iterator):",
            "    def __init__(self, archive_file_list):",
            "        self._archive_file_list = archive_file_list",
            "        self._index = 0",
            "",
            "    def __next__(self) -> ArchiveFile:",
            "        if self._index == len(self._archive_file_list):",
            "            raise StopIteration",
            "        res = self._archive_file_list[self._index]",
            "        self._index += 1",
            "        return res",
            "",
            "",
            "# ------------------",
            "# Exported Classes",
            "# ------------------",
            "class ArchiveInfo:",
            "    \"\"\"Hold archive information\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        filename: str,",
            "        stat: os.stat_result,",
            "        header_size: int,",
            "        method_names: List[str],",
            "        solid: bool,",
            "        blocks: int,",
            "        uncompressed: List[int],",
            "    ):",
            "        self.stat = stat",
            "        self.filename = filename",
            "        self.size = stat.st_size",
            "        self.header_size = header_size",
            "        self.method_names = method_names",
            "        self.solid = solid",
            "        self.blocks = blocks",
            "        self.uncompressed = uncompressed",
            "",
            "",
            "class FileInfo:",
            "    \"\"\"Hold archived file information.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        filename,",
            "        compressed,",
            "        uncompressed,",
            "        archivable,",
            "        is_directory,",
            "        creationtime,",
            "        crc32,",
            "    ):",
            "        self.filename = filename",
            "        self.compressed = compressed",
            "        self.uncompressed = uncompressed",
            "        self.archivable = archivable",
            "        self.is_directory = is_directory",
            "        self.creationtime = creationtime",
            "        self.crc32 = crc32",
            "",
            "",
            "class SevenZipFile(contextlib.AbstractContextManager):",
            "    \"\"\"The SevenZipFile Class provides an interface to 7z archives.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        file: Union[BinaryIO, str, pathlib.Path],",
            "        mode: str = \"r\",",
            "        *,",
            "        filters: Optional[List[Dict[str, int]]] = None,",
            "        dereference=False,",
            "        password: Optional[str] = None,",
            "        header_encryption: bool = False,",
            "        blocksize: Optional[int] = None,",
            "        mp: bool = False,",
            "    ) -> None:",
            "        if mode not in (\"r\", \"w\", \"x\", \"a\"):",
            "            raise ValueError(\"ZipFile requires mode 'r', 'w', 'x', or 'a'\")",
            "        self.fp: BinaryIO",
            "        self.mp = mp",
            "        self.password_protected = password is not None",
            "        if blocksize:",
            "            self._block_size = blocksize",
            "        else:",
            "            self._block_size = get_default_blocksize()",
            "        # Check if we were passed a file-like object or not",
            "        if isinstance(file, str):",
            "            self._filePassed: bool = False",
            "            self.filename: str = file",
            "            if mode == \"r\":",
            "                self.fp = open(file, \"rb\")",
            "            elif mode == \"w\":",
            "                self.fp = open(file, \"w+b\")",
            "            elif mode == \"x\":",
            "                self.fp = open(file, \"x+b\")",
            "            elif mode == \"a\":",
            "                self.fp = open(file, \"r+b\")",
            "            else:",
            "                raise ValueError(\"File open error.\")",
            "            self.mode = mode",
            "        elif isinstance(file, pathlib.Path):",
            "            self._filePassed = False",
            "            self.filename = str(file)",
            "            if mode == \"r\":",
            "                self.fp = file.open(mode=\"rb\")  # noqa   # typeshed issue: 2911",
            "            elif mode == \"w\":",
            "                self.fp = file.open(mode=\"w+b\")  # noqa",
            "            elif mode == \"x\":",
            "                self.fp = file.open(mode=\"x+b\")  # noqa",
            "            elif mode == \"a\":",
            "                self.fp = file.open(mode=\"r+b\")  # noqa",
            "            else:",
            "                raise ValueError(\"File open error.\")",
            "            self.mode = mode",
            "        elif isinstance(file, multivolumefile.MultiVolume):",
            "            self._filePassed = True",
            "            self.fp = file",
            "            self.filename = None",
            "            self.mode = mode  # noqa",
            "        elif isinstance(file, io.IOBase):",
            "            self._filePassed = True",
            "            self.fp = file",
            "            self.filename = getattr(file, \"name\", None)",
            "            self.mode = mode  # noqa",
            "        else:",
            "            raise TypeError(\"invalid file: {}\".format(type(file)))",
            "        self.encoded_header_mode = True",
            "        self.header_encryption = header_encryption",
            "        self._fileRefCnt = 1",
            "        try:",
            "            if mode == \"r\":",
            "                self._real_get_contents(password)",
            "                self.fp.seek(self.afterheader)  # seek into start of payload and prepare worker to extract",
            "                self.worker = Worker(self.files, self.afterheader, self.header, self.mp)",
            "            elif mode in \"w\":",
            "                self._prepare_write(filters, password)",
            "            elif mode in \"x\":",
            "                raise NotImplementedError",
            "            elif mode == \"a\":",
            "                self._real_get_contents(password)",
            "                self._prepare_append(filters, password)",
            "            else:",
            "                raise ValueError(\"Mode must be 'r', 'w', 'x', or 'a'\")",
            "        except Exception as e:",
            "            self._fpclose()",
            "            raise e",
            "        self._dict: Dict[str, IO[Any]] = {}",
            "        self.dereference = dereference",
            "        self.reporterd: Optional[Thread] = None",
            "        self.q: queue.Queue[Any] = queue.Queue()",
            "",
            "    def __enter__(self):",
            "        return self",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        self.close()",
            "",
            "    def _fpclose(self) -> None:",
            "        assert self._fileRefCnt > 0",
            "        self._fileRefCnt -= 1",
            "        if not self._fileRefCnt and not self._filePassed:",
            "            self.fp.close()",
            "",
            "    def _real_get_contents(self, password) -> None:",
            "        if not self._check_7zfile(self.fp):",
            "            raise Bad7zFile(\"not a 7z file\")",
            "        self.sig_header = SignatureHeader.retrieve(self.fp)",
            "        self.afterheader: int = self.fp.tell()",
            "        self.fp.seek(self.sig_header.nextheaderofs, os.SEEK_CUR)",
            "        buffer = io.BytesIO(self.fp.read(self.sig_header.nextheadersize))",
            "        if self.sig_header.nextheadercrc != calculate_crc32(buffer.getvalue()):",
            "            raise Bad7zFile(\"invalid header data\")",
            "        header = Header.retrieve(self.fp, buffer, self.afterheader, password)",
            "        if header is None:",
            "            return",
            "        header._initilized = True",
            "        self.header = header",
            "        header.size += 32 + self.sig_header.nextheadersize",
            "        buffer.close()",
            "        self.files = ArchiveFileList()",
            "        if getattr(self.header, \"files_info\", None) is None:",
            "            return",
            "        # Initialize references for convenience",
            "        if hasattr(self.header, \"main_streams\") and self.header.main_streams is not None:",
            "            folders = self.header.main_streams.unpackinfo.folders",
            "            for folder in folders:",
            "                folder.password = password",
            "            packinfo = self.header.main_streams.packinfo",
            "            packsizes = packinfo.packsizes",
            "            subinfo = self.header.main_streams.substreamsinfo",
            "            if subinfo is not None and subinfo.unpacksizes is not None:",
            "                unpacksizes = subinfo.unpacksizes",
            "            else:",
            "                unpacksizes = [x.unpacksizes[-1] for x in folders]",
            "        else:",
            "            subinfo = None",
            "            folders = None",
            "            packinfo = None",
            "            packsizes = []",
            "            unpacksizes = [0]",
            "",
            "        pstat = self.ParseStatus()",
            "        pstat.src_pos = self.afterheader",
            "        file_in_solid = 0",
            "",
            "        for file_id, file_info in enumerate(self.header.files_info.files):",
            "            if not file_info[\"emptystream\"] and folders is not None:",
            "                folder = folders[pstat.folder]",
            "                numinstreams = max([coder.get(\"numinstreams\", 1) for coder in folder.coders])",
            "                (maxsize, compressed, uncompressed, packsize, solid,) = self._get_fileinfo_sizes(",
            "                    pstat,",
            "                    subinfo,",
            "                    packinfo,",
            "                    folder,",
            "                    packsizes,",
            "                    unpacksizes,",
            "                    file_in_solid,",
            "                    numinstreams,",
            "                )",
            "                pstat.input += 1",
            "                folder.solid = solid",
            "                file_info[\"folder\"] = folder",
            "                file_info[\"maxsize\"] = maxsize",
            "                file_info[\"compressed\"] = compressed",
            "                file_info[\"uncompressed\"] = uncompressed",
            "                file_info[\"packsizes\"] = packsize",
            "                if subinfo.digestsdefined[pstat.outstreams]:",
            "                    file_info[\"digest\"] = subinfo.digests[pstat.outstreams]",
            "                if folder is None:",
            "                    pstat.src_pos += file_info[\"compressed\"]",
            "                else:",
            "                    if folder.solid:",
            "                        file_in_solid += 1",
            "                    pstat.outstreams += 1",
            "                    if folder.files is None:",
            "                        folder.files = ArchiveFileList(offset=file_id)",
            "                    folder.files.append(file_info)",
            "                    if pstat.input >= subinfo.num_unpackstreams_folders[pstat.folder]:",
            "                        file_in_solid = 0",
            "                        pstat.src_pos += sum(packinfo.packsizes[pstat.stream : pstat.stream + numinstreams])",
            "                        pstat.folder += 1",
            "                        pstat.stream += numinstreams",
            "                        pstat.input = 0",
            "            else:",
            "                file_info[\"folder\"] = None",
            "                file_info[\"maxsize\"] = 0",
            "                file_info[\"compressed\"] = 0",
            "                file_info[\"uncompressed\"] = 0",
            "                file_info[\"packsizes\"] = [0]",
            "",
            "            if \"filename\" not in file_info:",
            "                # compressed file is stored without a name, generate one",
            "                try:",
            "                    basefilename = self.filename",
            "                except AttributeError:",
            "                    # 7z archive file doesn't have a name",
            "                    file_info[\"filename\"] = \"contents\"",
            "                else:",
            "                    if basefilename is not None:",
            "                        fn, ext = os.path.splitext(os.path.basename(basefilename))",
            "                        file_info[\"filename\"] = fn",
            "                    else:",
            "                        file_info[\"filename\"] = \"contents\"",
            "            self.files.append(file_info)",
            "        if not self.password_protected and self.header.main_streams is not None:",
            "            # Check specified coders have a crypt method or not.",
            "            self.password_protected = any(",
            "                [SupportedMethods.needs_password(folder.coders) for folder in self.header.main_streams.unpackinfo.folders]",
            "            )",
            "",
            "    def _extract(",
            "        self,",
            "        path: Optional[Any] = None,",
            "        targets: Optional[List[str]] = None,",
            "        return_dict: bool = False,",
            "        callback: Optional[ExtractCallback] = None,",
            "    ) -> Optional[Dict[str, IO[Any]]]:",
            "        if callback is None:",
            "            pass",
            "        elif isinstance(callback, ExtractCallback):",
            "            self.reporterd = Thread(target=self.reporter, args=(callback,), daemon=True)",
            "            self.reporterd.start()",
            "        else:",
            "            raise ValueError(\"Callback specified is not an instance of subclass of py7zr.callbacks.ExtractCallback class\")",
            "        target_files: List[Tuple[pathlib.Path, Dict[str, Any]]] = []",
            "        target_dirs: List[pathlib.Path] = []",
            "        if path is not None:",
            "            if isinstance(path, str):",
            "                path = pathlib.Path(path)",
            "            try:",
            "                if not path.exists():",
            "                    path.mkdir(parents=True)",
            "                else:",
            "                    pass",
            "            except OSError as e:",
            "                if e.errno == errno.EEXIST and path.is_dir():",
            "                    pass",
            "                else:",
            "                    raise e",
            "        fnames: List[str] = []  # check duplicated filename in one archive?",
            "        self.q.put((\"pre\", None, None))",
            "        for f in self.files:",
            "            # When archive has a multiple files which have same name",
            "            # To guarantee order of archive, multi-thread decompression becomes off.",
            "            # Currently always overwrite by latter archives.",
            "            # TODO: provide option to select overwrite or skip.",
            "            if f.filename not in fnames:",
            "                outname = f.filename",
            "            else:",
            "                i = 0",
            "                while True:",
            "                    outname = f.filename + \"_%d\" % i",
            "                    if outname not in fnames:",
            "                        break",
            "                    i += 1",
            "            fnames.append(outname)",
            "            # check f.filename has invalid directory traversals",
            "            if path is None:",
            "                # do following but is_relative_to introduced in py 3.9",
            "                # so I replaced it with relative_to. when condition is not satisfied, raise ValueError",
            "                # if not pathlib.Path(...).joinpath(remove_relative_path_marker(outname)).is_relative_to(...):",
            "                #    raise Bad7zFile",
            "                try:",
            "                    pathlib.Path(os.getcwd()).joinpath(remove_relative_path_marker(outname)).relative_to(os.getcwd())",
            "                except ValueError:",
            "                    raise Bad7zFile",
            "                outfilename = pathlib.Path(remove_relative_path_marker(outname))",
            "            else:",
            "                outfilename = path.joinpath(remove_relative_path_marker(outname))",
            "                try:",
            "                    outfilename.relative_to(path)",
            "                except ValueError:",
            "                    raise Bad7zFile",
            "            # When python on Windows and not python on Cygwin,",
            "            # Add win32 file namespace to exceed microsoft windows",
            "            # path length limitation to 260 bytes",
            "            # ref.",
            "            # https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file",
            "            # In editions of Windows before Windows 10 version 1607,",
            "            # the maximum length for a path is MAX_PATH, which is defined as",
            "            # 260 characters. In later versions of Windows, changing a registry key",
            "            # or select option when python installation is required to remove the limit.",
            "            if is_windows_native_python() and outfilename.is_absolute() and not is_windows_unc_path(outfilename):",
            "                outfilename = pathlib.WindowsPath(\"\\\\\\\\?\\\\\" + str(outfilename))",
            "            if targets is not None and f.filename not in targets:",
            "                self.worker.register_filelike(f.id, None)",
            "                continue",
            "            if return_dict:",
            "                if f.is_directory or f.is_socket:",
            "                    # ignore special files and directories",
            "                    pass",
            "                else:",
            "                    fname = outfilename.as_posix()",
            "                    _buf = io.BytesIO()",
            "                    self._dict[fname] = _buf",
            "                    self.worker.register_filelike(f.id, MemIO(_buf))",
            "            elif f.is_directory:",
            "                if not outfilename.exists():",
            "                    target_dirs.append(outfilename)",
            "                    target_files.append((outfilename, f.file_properties()))",
            "                else:",
            "                    pass",
            "            elif f.is_socket:",
            "                pass  # TODO: implement me.",
            "            elif f.is_symlink or f.is_junction:",
            "                self.worker.register_filelike(f.id, outfilename)",
            "            else:",
            "                self.worker.register_filelike(f.id, outfilename)",
            "                target_files.append((outfilename, f.file_properties()))",
            "        for target_dir in sorted(target_dirs):",
            "            try:",
            "                target_dir.mkdir(parents=True)",
            "            except FileExistsError:",
            "                if target_dir.is_dir():",
            "                    pass",
            "                elif target_dir.is_file():",
            "                    raise DecompressionError(\"Directory {} is existed as a normal file.\".format(str(target_dir)))",
            "                else:",
            "                    raise DecompressionError(\"Directory {} making fails on unknown condition.\".format(str(target_dir)))",
            "",
            "        if callback is not None:",
            "            self.worker.extract(",
            "                self.fp,",
            "                parallel=(not self.password_protected and not self._filePassed),",
            "                q=self.q,",
            "            )",
            "        else:",
            "            self.worker.extract(",
            "                self.fp,",
            "                parallel=(not self.password_protected and not self._filePassed),",
            "            )",
            "",
            "        self.q.put((\"post\", None, None))",
            "        # early return when dict specified",
            "        if return_dict:",
            "            return self._dict",
            "        # set file properties",
            "        for outfilename, properties in target_files:",
            "            # mtime",
            "            lastmodified = None",
            "            try:",
            "                lastmodified = ArchiveTimestamp(properties[\"lastwritetime\"]).totimestamp()",
            "            except KeyError:",
            "                pass",
            "            if lastmodified is not None:",
            "                os.utime(str(outfilename), times=(lastmodified, lastmodified))",
            "            if os.name == \"posix\":",
            "                st_mode = properties[\"posix_mode\"]",
            "                if st_mode is not None:",
            "                    outfilename.chmod(st_mode)",
            "                    continue",
            "            # fallback: only set readonly if specified",
            "            if properties[\"readonly\"] and not properties[\"is_directory\"]:",
            "                ro_mask = 0o777 ^ (stat.S_IWRITE | stat.S_IWGRP | stat.S_IWOTH)",
            "                outfilename.chmod(outfilename.stat().st_mode & ro_mask)",
            "        return None",
            "",
            "    def _prepare_append(self, filters, password):",
            "        if password is not None and filters is None:",
            "            filters = DEFAULT_FILTERS.ENCRYPTED_ARCHIVE_FILTER",
            "        elif filters is None:",
            "            filters = DEFAULT_FILTERS.ARCHIVE_FILTER",
            "        else:",
            "            for f in filters:",
            "                if f[\"id\"] == FILTER_DEFLATE64:",
            "                    raise UnsupportedCompressionMethodError(filters, \"Compression with deflate64 is not supported.\")",
            "        self.header.filters = filters",
            "        self.header.password = password",
            "        if self.header.main_streams is not None:",
            "            pos = self.afterheader + self.header.main_streams.packinfo.packpositions[-1]",
            "        else:",
            "            pos = self.afterheader",
            "        self.fp.seek(pos)",
            "        self.worker = Worker(self.files, pos, self.header, self.mp)",
            "",
            "    def _prepare_write(self, filters, password):",
            "        if password is not None and filters is None:",
            "            filters = DEFAULT_FILTERS.ENCRYPTED_ARCHIVE_FILTER",
            "        elif filters is None:",
            "            filters = DEFAULT_FILTERS.ARCHIVE_FILTER",
            "        self.files = ArchiveFileList()",
            "        self.sig_header = SignatureHeader()",
            "        self.sig_header._write_skelton(self.fp)",
            "        self.afterheader = self.fp.tell()",
            "        self.header = Header.build_header(filters, password)",
            "        self.fp.seek(self.afterheader)",
            "        self.worker = Worker(self.files, self.afterheader, self.header, self.mp)",
            "",
            "    def _write_flush(self):",
            "        if self.header._initialized:",
            "            folder = self.header.main_streams.unpackinfo.folders[-1]",
            "            self.worker.flush_archive(self.fp, folder)",
            "        self._write_header()",
            "",
            "    def _write_header(self):",
            "        \"\"\"Write header and update signature header.\"\"\"",
            "        (header_pos, header_len, header_crc) = self.header.write(",
            "            self.fp,",
            "            self.afterheader,",
            "            encoded=self.encoded_header_mode,",
            "            encrypted=self.header_encryption,",
            "        )",
            "        self.sig_header.nextheaderofs = header_pos - self.afterheader",
            "        self.sig_header.calccrc(header_len, header_crc)",
            "        self.sig_header.write(self.fp)",
            "",
            "    def _writeall(self, path, arcname):",
            "        try:",
            "            if path.is_symlink() and not self.dereference:",
            "                self.write(path, arcname)",
            "            elif path.is_file():",
            "                self.write(path, arcname)",
            "            elif path.is_dir():",
            "                if not path.samefile(\".\"):",
            "                    self.write(path, arcname)",
            "                for nm in sorted(os.listdir(str(path))):",
            "                    arc = os.path.join(arcname, nm) if arcname is not None else None",
            "                    self._writeall(path.joinpath(nm), arc)",
            "            else:",
            "                return  # pathlib ignores ELOOP and return False for is_*().",
            "        except OSError as ose:",
            "            if self.dereference and ose.errno in [errno.ELOOP]:",
            "                return  # ignore ELOOP here, this resulted to stop looped symlink reference.",
            "            elif self.dereference and sys.platform == \"win32\" and ose.errno in [errno.ENOENT]:",
            "                return  # ignore ENOENT which is happened when a case of ELOOP on windows.",
            "            else:",
            "                raise",
            "",
            "    class ParseStatus:",
            "        def __init__(self, src_pos=0):",
            "            self.src_pos = src_pos",
            "            self.folder = 0  # 7zip folder where target stored",
            "            self.outstreams = 0  # output stream count",
            "            self.input = 0  # unpack stream count in each folder",
            "            self.stream = 0  # target input stream position",
            "",
            "    def _get_fileinfo_sizes(",
            "        self,",
            "        pstat,",
            "        subinfo,",
            "        packinfo,",
            "        folder,",
            "        packsizes,",
            "        unpacksizes,",
            "        file_in_solid,",
            "        numinstreams,",
            "    ):",
            "        if pstat.input == 0:",
            "            folder.solid = subinfo.num_unpackstreams_folders[pstat.folder] > 1",
            "        maxsize = (folder.solid and packinfo.packsizes[pstat.stream]) or None",
            "        uncompressed = unpacksizes[pstat.outstreams]",
            "        if file_in_solid > 0:",
            "            compressed = None",
            "        elif pstat.stream < len(packsizes):  # file is compressed",
            "            compressed = packsizes[pstat.stream]",
            "        else:  # file is not compressed",
            "            compressed = uncompressed",
            "        packsize = packsizes[pstat.stream : pstat.stream + numinstreams]",
            "        return maxsize, compressed, uncompressed, packsize, folder.solid",
            "",
            "    def set_encoded_header_mode(self, mode: bool) -> None:",
            "        if mode:",
            "            self.encoded_header_mode = True",
            "        else:",
            "            self.encoded_header_mode = False",
            "            self.header_encryption = False",
            "",
            "    def set_encrypted_header(self, mode: bool) -> None:",
            "        if mode:",
            "            self.encoded_header_mode = True",
            "            self.header_encryption = True",
            "        else:",
            "            self.header_encryption = False",
            "",
            "    @staticmethod",
            "    def _check_7zfile(fp: Union[BinaryIO, io.BufferedReader, io.IOBase]) -> bool:",
            "        result = MAGIC_7Z == fp.read(len(MAGIC_7Z))[: len(MAGIC_7Z)]",
            "        fp.seek(-len(MAGIC_7Z), 1)",
            "        return result",
            "",
            "    def _get_method_names(self) -> List[str]:",
            "        try:",
            "            return get_methods_names([folder.coders for folder in self.header.main_streams.unpackinfo.folders])",
            "        except KeyError:",
            "            raise UnsupportedCompressionMethodError(self.header.main_streams.unpackinfo.folders, \"Unknown method\")",
            "",
            "    def _read_digest(self, pos: int, size: int) -> int:",
            "        self.fp.seek(pos)",
            "        remaining_size = size",
            "        digest = 0",
            "        while remaining_size > 0:",
            "            block = min(self._block_size, remaining_size)",
            "            digest = calculate_crc32(self.fp.read(block), digest)",
            "            remaining_size -= block",
            "        return digest",
            "",
            "    def _is_solid(self):",
            "        for f in self.header.main_streams.substreamsinfo.num_unpackstreams_folders:",
            "            if f > 1:",
            "                return True",
            "        return False",
            "",
            "    def _var_release(self):",
            "        self._dict = None",
            "        self.worker.close()",
            "        del self.worker",
            "        del self.files",
            "        del self.header",
            "        del self.sig_header",
            "        gc.collect()",
            "",
            "    @staticmethod",
            "    def _make_file_info(target: pathlib.Path, arcname: Optional[str] = None, dereference=False) -> Dict[str, Any]:",
            "        f: Dict[str, Any] = {}",
            "        f[\"origin\"] = target",
            "        if arcname is not None:",
            "            f[\"filename\"] = pathlib.Path(arcname).as_posix()",
            "        else:",
            "            f[\"filename\"] = target.as_posix()",
            "        if sys.platform == \"win32\":",
            "            fstat = target.lstat()",
            "            if target.is_symlink():",
            "                if dereference:",
            "                    fstat = target.stat()",
            "                    if stat.S_ISDIR(fstat.st_mode):",
            "                        f[\"emptystream\"] = True",
            "                        f[\"attributes\"] = fstat.st_file_attributes & FILE_ATTRIBUTE_WINDOWS_MASK  # noqa",
            "                    else:",
            "                        f[\"emptystream\"] = False",
            "                        f[\"attributes\"] = stat.FILE_ATTRIBUTE_ARCHIVE  # noqa",
            "                        f[\"uncompressed\"] = fstat.st_size",
            "                else:",
            "                    f[\"emptystream\"] = False",
            "                    f[\"attributes\"] = fstat.st_file_attributes & FILE_ATTRIBUTE_WINDOWS_MASK  # noqa",
            "                    # TODO: handle junctions",
            "                    # f['attributes'] |= stat.FILE_ATTRIBUTE_REPARSE_POINT  # noqa",
            "            elif target.is_dir():",
            "                f[\"emptystream\"] = True",
            "                f[\"attributes\"] = fstat.st_file_attributes & FILE_ATTRIBUTE_WINDOWS_MASK  # noqa",
            "            elif target.is_file():",
            "                f[\"emptystream\"] = False",
            "                f[\"attributes\"] = stat.FILE_ATTRIBUTE_ARCHIVE  # noqa",
            "                f[\"uncompressed\"] = fstat.st_size",
            "        elif (",
            "            sys.platform == \"darwin\"",
            "            or sys.platform.startswith(\"linux\")",
            "            or sys.platform.startswith(\"freebsd\")",
            "            or sys.platform.startswith(\"netbsd\")",
            "            or sys.platform.startswith(\"sunos\")",
            "            or sys.platform == \"aix\"",
            "        ):",
            "            fstat = target.lstat()",
            "            if target.is_symlink():",
            "                if dereference:",
            "                    fstat = target.stat()",
            "                    if stat.S_ISDIR(fstat.st_mode):",
            "                        f[\"emptystream\"] = True",
            "                        f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\")",
            "                        f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IFDIR << 16)",
            "                        f[\"attributes\"] |= stat.S_IMODE(fstat.st_mode) << 16",
            "                    else:",
            "                        f[\"emptystream\"] = False",
            "                        f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\")",
            "                        f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IMODE(fstat.st_mode) << 16)",
            "                else:",
            "                    f[\"emptystream\"] = False",
            "                    f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\") | getattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\")",
            "                    f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IFLNK << 16)",
            "                    f[\"attributes\"] |= stat.S_IMODE(fstat.st_mode) << 16",
            "            elif target.is_dir():",
            "                f[\"emptystream\"] = True",
            "                f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\")",
            "                f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IFDIR << 16)",
            "                f[\"attributes\"] |= stat.S_IMODE(fstat.st_mode) << 16",
            "            elif target.is_file():",
            "                f[\"emptystream\"] = False",
            "                f[\"uncompressed\"] = fstat.st_size",
            "                f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\")",
            "                f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IMODE(fstat.st_mode) << 16)",
            "        else:",
            "            fstat = target.stat()",
            "            if target.is_dir():",
            "                f[\"emptystream\"] = True",
            "                f[\"attributes\"] = stat.FILE_ATTRIBUTE_DIRECTORY",
            "            elif target.is_file():",
            "                f[\"emptystream\"] = False",
            "                f[\"uncompressed\"] = fstat.st_size",
            "                f[\"attributes\"] = stat.FILE_ATTRIBUTE_ARCHIVE",
            "",
            "        f[\"creationtime\"] = ArchiveTimestamp.from_datetime(fstat.st_ctime)",
            "        f[\"lastwritetime\"] = ArchiveTimestamp.from_datetime(fstat.st_mtime)",
            "        f[\"lastaccesstime\"] = ArchiveTimestamp.from_datetime(fstat.st_atime)",
            "        return f",
            "",
            "    def _make_file_info_from_name(self, bio, size: int, arcname: str) -> Dict[str, Any]:",
            "        f: Dict[str, Any] = {}",
            "        f[\"origin\"] = None",
            "        f[\"data\"] = bio",
            "        f[\"filename\"] = pathlib.Path(arcname).as_posix()",
            "        f[\"uncompressed\"] = size",
            "        f[\"emptystream\"] = size == 0",
            "        f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\")",
            "        f[\"creationtime\"] = ArchiveTimestamp.from_now()",
            "        f[\"lastwritetime\"] = ArchiveTimestamp.from_now()",
            "        return f",
            "",
            "    # --------------------------------------------------------------------------",
            "    # The public methods which SevenZipFile provides:",
            "    def getnames(self) -> List[str]:",
            "        \"\"\"Return the members of the archive as a list of their names. It has",
            "        the same order as the list returned by getmembers().",
            "        \"\"\"",
            "        return list(map(lambda x: x.filename, self.files))",
            "",
            "    def archiveinfo(self) -> ArchiveInfo:",
            "        total_uncompressed = functools.reduce(lambda x, y: x + y, [f.uncompressed for f in self.files])",
            "        if isinstance(self.fp, multivolumefile.MultiVolume):",
            "            fname = self.fp.name",
            "            fstat = self.fp.stat()",
            "        else:",
            "            fname = self.filename",
            "            assert fname is not None",
            "            fstat = os.stat(fname)",
            "        return ArchiveInfo(",
            "            fname,",
            "            fstat,",
            "            self.header.size,",
            "            self._get_method_names(),",
            "            self._is_solid(),",
            "            len(self.header.main_streams.unpackinfo.folders),",
            "            total_uncompressed,",
            "        )",
            "",
            "    def needs_password(self) -> bool:",
            "        return self.password_protected",
            "",
            "    def list(self) -> List[FileInfo]:",
            "        \"\"\"Returns contents information\"\"\"",
            "        alist: List[FileInfo] = []",
            "        lastmodified: Optional[datetime.datetime] = None",
            "        for f in self.files:",
            "            if f.lastwritetime is not None:",
            "                lastmodified = filetime_to_dt(f.lastwritetime)",
            "            alist.append(",
            "                FileInfo(",
            "                    f.filename,",
            "                    f.compressed,",
            "                    f.uncompressed,",
            "                    f.archivable,",
            "                    f.is_directory,",
            "                    lastmodified,",
            "                    f.crc32,",
            "                )",
            "            )",
            "        return alist",
            "",
            "    def readall(self) -> Optional[Dict[str, IO[Any]]]:",
            "        self._dict = {}",
            "        return self._extract(path=None, return_dict=True)",
            "",
            "    def extractall(self, path: Optional[Any] = None, callback: Optional[ExtractCallback] = None) -> None:",
            "        \"\"\"Extract all members from the archive to the current working",
            "        directory and set owner, modification time and permissions on",
            "        directories afterwards. ``path`` specifies a different directory",
            "        to extract to.",
            "        \"\"\"",
            "        self._extract(path=path, return_dict=False, callback=callback)",
            "",
            "    def read(self, targets: Optional[List[str]] = None) -> Optional[Dict[str, IO[Any]]]:",
            "        self._dict = {}",
            "        return self._extract(path=None, targets=targets, return_dict=True)",
            "",
            "    def extract(self, path: Optional[Any] = None, targets: Optional[List[str]] = None) -> None:",
            "        self._extract(path, targets, return_dict=False)",
            "",
            "    def reporter(self, callback: ExtractCallback):",
            "        while True:",
            "            try:",
            "                item: Optional[Tuple[str, str, str]] = self.q.get(timeout=1)",
            "            except queue.Empty:",
            "                pass",
            "            else:",
            "                if item is None:",
            "                    break",
            "                elif item[0] == \"s\":",
            "                    callback.report_start(item[1], item[2])",
            "                elif item[0] == \"e\":",
            "                    callback.report_end(item[1], item[2])",
            "                elif item[0] == \"pre\":",
            "                    callback.report_start_preparation()",
            "                elif item[0] == \"post\":",
            "                    callback.report_postprocess()",
            "                elif item[0] == \"w\":",
            "                    callback.report_warning(item[1])",
            "                else:",
            "                    pass",
            "                self.q.task_done()",
            "",
            "    def writeall(self, path: Union[pathlib.Path, str], arcname: Optional[str] = None):",
            "        \"\"\"Write files in target path into archive.\"\"\"",
            "        if isinstance(path, str):",
            "            path = pathlib.Path(path)",
            "        if not path.exists():",
            "            raise ValueError(\"specified path does not exist.\")",
            "        if path.is_dir() or path.is_file():",
            "            self._writeall(path, arcname)",
            "        else:",
            "            raise ValueError(\"specified path is not a directory or a file\")",
            "",
            "    def write(self, file: Union[pathlib.Path, str], arcname: Optional[str] = None):",
            "        \"\"\"Write single target file into archive.\"\"\"",
            "        if isinstance(file, str):",
            "            path = pathlib.Path(file)",
            "        elif isinstance(file, pathlib.Path):",
            "            path = file",
            "        else:",
            "            raise ValueError(\"Unsupported file type.\")",
            "        folder = self.header.initialize()",
            "        file_info = self._make_file_info(path, arcname, self.dereference)",
            "        self.header.files_info.files.append(file_info)",
            "        self.header.files_info.emptyfiles.append(file_info[\"emptystream\"])",
            "        self.files.append(file_info)",
            "        self.worker.archive(self.fp, self.files, folder, deref=self.dereference)",
            "",
            "    def writed(self, targets: Dict[str, IO[Any]]) -> None:",
            "        for target, input in targets.items():",
            "            self.writef(input, target)",
            "",
            "    def writef(self, bio: IO[Any], arcname: str):",
            "        if isinstance(bio, io.BytesIO):",
            "            size = bio.getbuffer().nbytes",
            "        elif isinstance(bio, io.TextIOBase):",
            "            # First check whether is it Text?",
            "            raise ValueError(\"Unsupported file object type: please open file with Binary mode.\")",
            "        elif isinstance(bio, io.BufferedIOBase):",
            "            # come here when subtype of io.BufferedIOBase that don't have __sizeof__ (eg. pypy)",
            "            # alternative for `size = bio.__sizeof__()`",
            "            current = bio.tell()",
            "            bio.seek(0, os.SEEK_END)",
            "            last = bio.tell()",
            "            bio.seek(current, os.SEEK_SET)",
            "            size = last - current",
            "        else:",
            "            raise ValueError(\"Wrong argument passed for argument bio.\")",
            "        if size > 0:",
            "            folder = self.header.initialize()",
            "            file_info = self._make_file_info_from_name(bio, size, arcname)",
            "            self.header.files_info.files.append(file_info)",
            "            self.header.files_info.emptyfiles.append(file_info[\"emptystream\"])",
            "            self.files.append(file_info)",
            "            self.worker.archive(self.fp, self.files, folder, deref=False)",
            "        else:",
            "            file_info = self._make_file_info_from_name(bio, size, arcname)",
            "            self.header.files_info.files.append(file_info)",
            "            self.header.files_info.emptyfiles.append(file_info[\"emptystream\"])",
            "            self.files.append(file_info)",
            "",
            "    def writestr(self, data: Union[str, bytes, bytearray, memoryview], arcname: str):",
            "        if not isinstance(arcname, str):",
            "            raise ValueError(\"Unsupported arcname\")",
            "        if isinstance(data, str):",
            "            self.writef(io.BytesIO(data.encode(\"UTF-8\")), arcname)",
            "        elif isinstance(data, bytes) or isinstance(data, bytearray) or isinstance(data, memoryview):",
            "            self.writef(io.BytesIO(data), arcname)",
            "        else:",
            "            raise ValueError(\"Unsupported data type.\")",
            "",
            "    def close(self):",
            "        \"\"\"Flush all the data into archive and close it.",
            "        When close py7zr start reading target and writing actual archive file.",
            "        \"\"\"",
            "        if \"w\" in self.mode:",
            "            self._write_flush()",
            "        if \"a\" in self.mode:",
            "            self._write_flush()",
            "        if \"r\" in self.mode:",
            "            if self.reporterd is not None:",
            "                self.q.put_nowait(None)",
            "                self.reporterd.join(1)",
            "                if self.reporterd.is_alive():",
            "                    raise InternalError(\"Progress report thread terminate error.\")",
            "                self.reporterd = None",
            "        self._fpclose()",
            "        self._var_release()",
            "",
            "    def reset(self) -> None:",
            "        \"\"\"",
            "        When read mode, it reset file pointer, decompress worker and decompressor",
            "        \"\"\"",
            "        if self.mode == \"r\":",
            "            self.fp.seek(self.afterheader)",
            "            self.worker = Worker(self.files, self.afterheader, self.header, self.mp)",
            "            if self.header.main_streams is not None and self.header.main_streams.unpackinfo.numfolders > 0:",
            "                for i, folder in enumerate(self.header.main_streams.unpackinfo.folders):",
            "                    folder.decompressor = None",
            "",
            "    def test(self) -> Optional[bool]:",
            "        self.fp.seek(self.afterheader)",
            "        self.worker = Worker(self.files, self.afterheader, self.header, self.mp)",
            "        crcs: Optional[List[int]] = self.header.main_streams.packinfo.crcs",
            "        if crcs is None or len(crcs) == 0:",
            "            return None",
            "        packpos = self.afterheader + self.header.main_streams.packinfo.packpos",
            "        packsizes = self.header.main_streams.packinfo.packsizes",
            "        digestdefined = self.header.main_streams.packinfo.digestdefined",
            "        j = 0",
            "        for i, d in enumerate(digestdefined):",
            "            if d:",
            "                if self._read_digest(packpos, packsizes[i]) != crcs[j]:",
            "                    return False",
            "                j += 1",
            "            packpos += packsizes[i]",
            "        return True",
            "",
            "    def testzip(self) -> Optional[str]:",
            "        self.fp.seek(self.afterheader)",
            "        self.worker = Worker(self.files, self.afterheader, self.header, self.mp)",
            "        for f in self.files:",
            "            self.worker.register_filelike(f.id, None)",
            "        try:",
            "            self.worker.extract(self.fp, parallel=(not self.password_protected), skip_notarget=False)  # TODO: print progress",
            "        except CrcError as crce:",
            "            return crce.args[2]",
            "        else:",
            "            return None",
            "",
            "",
            "# --------------------",
            "# exported functions",
            "# --------------------",
            "def is_7zfile(file: Union[BinaryIO, str, pathlib.Path]) -> bool:",
            "    \"\"\"Quickly see if a file is a 7Z file by checking the magic number.",
            "    The file argument may be a filename or file-like object too.",
            "    \"\"\"",
            "    result = False",
            "    try:",
            "        if (isinstance(file, BinaryIO) or isinstance(file, io.BufferedReader) or isinstance(file, io.IOBase)) and hasattr(",
            "            file, \"read\"",
            "        ):",
            "            result = SevenZipFile._check_7zfile(file)",
            "        elif isinstance(file, str):",
            "            with open(file, \"rb\") as fp:",
            "                result = SevenZipFile._check_7zfile(fp)",
            "        elif isinstance(file, pathlib.Path) or isinstance(file, pathlib.PosixPath) or isinstance(file, pathlib.WindowsPath):",
            "            with file.open(mode=\"rb\") as fp:  # noqa",
            "                result = SevenZipFile._check_7zfile(fp)",
            "        else:",
            "            raise TypeError(\"invalid type: file should be str, pathlib.Path or BinaryIO, but {}\".format(type(file)))",
            "    except OSError:",
            "        pass",
            "    return result",
            "",
            "",
            "def unpack_7zarchive(archive, path, extra=None):",
            "    \"\"\"",
            "    Function for registering with shutil.register_unpack_format().",
            "    \"\"\"",
            "    arc = SevenZipFile(archive)",
            "    arc.extractall(path)",
            "    arc.close()",
            "",
            "",
            "def pack_7zarchive(base_name, base_dir, owner=None, group=None, dry_run=None, logger=None):",
            "    \"\"\"",
            "    Function for registering with shutil.register_archive_format().",
            "    \"\"\"",
            "    target_name = \"{}.7z\".format(base_name)",
            "    with SevenZipFile(target_name, mode=\"w\") as archive:",
            "        archive.writeall(path=base_dir)",
            "    return target_name",
            "",
            "",
            "class Worker:",
            "    \"\"\"",
            "    Extract worker class to invoke handler.",
            "    \"\"\"",
            "",
            "    def __init__(self, files, src_start: int, header, mp=False) -> None:",
            "        self.target_filepath: Dict[int, Union[MemIO, pathlib.Path, None]] = {}",
            "        self.files = files",
            "        self.src_start = src_start",
            "        self.header = header",
            "        self.current_file_index = len(self.files)",
            "        self.last_file_index = len(self.files)",
            "        if mp:",
            "            self.concurrent: Union[Type[Thread], Type[Process]] = Process",
            "        else:",
            "            self.concurrent = Thread",
            "",
            "    def extract(self, fp: BinaryIO, parallel: bool, skip_notarget=True, q=None) -> None:",
            "        \"\"\"Extract worker method to handle 7zip folder and decompress each files.\"\"\"",
            "        if hasattr(self.header, \"main_streams\") and self.header.main_streams is not None:",
            "            src_end = self.src_start + self.header.main_streams.packinfo.packpositions[-1]",
            "            numfolders = self.header.main_streams.unpackinfo.numfolders",
            "            if numfolders == 1:",
            "                self.extract_single(",
            "                    fp,",
            "                    self.files,",
            "                    self.src_start,",
            "                    src_end,",
            "                    q,",
            "                    skip_notarget=skip_notarget,",
            "                )",
            "            else:",
            "                folders = self.header.main_streams.unpackinfo.folders",
            "                positions = self.header.main_streams.packinfo.packpositions",
            "                empty_files = [f for f in self.files if f.emptystream]",
            "                if not parallel:",
            "                    self.extract_single(fp, empty_files, 0, 0, q)",
            "                    for i in range(numfolders):",
            "                        if skip_notarget:",
            "                            if not any([self.target_filepath.get(f.id, None) for f in folders[i].files]):",
            "                                continue",
            "                        self.extract_single(",
            "                            fp,",
            "                            folders[i].files,",
            "                            self.src_start + positions[i],",
            "                            self.src_start + positions[i + 1],",
            "                            q,",
            "                            skip_notarget=skip_notarget,",
            "                        )",
            "                else:",
            "                    if getattr(fp, \"name\", None) is None:",
            "                        raise InternalError(\"Caught unknown variable status error\")",
            "                    filename: str = getattr(fp, \"name\", \"\")  # do not become \"\" but it is for type check.",
            "                    self.extract_single(open(filename, \"rb\"), empty_files, 0, 0, q)",
            "                    concurrent_tasks = []",
            "                    exc_q: queue.Queue = queue.Queue()",
            "                    for i in range(numfolders):",
            "                        if skip_notarget:",
            "                            if not any([self.target_filepath.get(f.id, None) for f in folders[i].files]):",
            "                                continue",
            "                        p = self.concurrent(",
            "                            target=self.extract_single,",
            "                            args=(",
            "                                filename,",
            "                                folders[i].files,",
            "                                self.src_start + positions[i],",
            "                                self.src_start + positions[i + 1],",
            "                                q,",
            "                                exc_q,",
            "                                skip_notarget,",
            "                            ),",
            "                        )",
            "                        p.start()",
            "                        concurrent_tasks.append(p)",
            "                    for p in concurrent_tasks:",
            "                        p.join()",
            "                    if exc_q.empty():",
            "                        pass",
            "                    else:",
            "                        exc_info = exc_q.get()",
            "                        raise exc_info[1].with_traceback(exc_info[2])",
            "        else:",
            "            empty_files = [f for f in self.files if f.emptystream]",
            "            self.extract_single(fp, empty_files, 0, 0, q)",
            "",
            "    def extract_single(",
            "        self,",
            "        fp: Union[BinaryIO, str],",
            "        files,",
            "        src_start: int,",
            "        src_end: int,",
            "        q: Optional[queue.Queue],",
            "        exc_q: Optional[queue.Queue] = None,",
            "        skip_notarget=True,",
            "    ) -> None:",
            "        \"\"\"",
            "        Single thread extractor that takes file lists in single 7zip folder.",
            "        \"\"\"",
            "        if files is None:",
            "            return",
            "        try:",
            "            if isinstance(fp, str):",
            "                fp = open(fp, \"rb\")",
            "            fp.seek(src_start)",
            "            self._extract_single(fp, files, src_end, q, skip_notarget)",
            "        except Exception as e:",
            "            if exc_q is None:",
            "                raise e",
            "            else:",
            "                exc_tuple = sys.exc_info()",
            "                exc_q.put(exc_tuple)",
            "",
            "    def _extract_single(",
            "        self,",
            "        fp: BinaryIO,",
            "        files,",
            "        src_end: int,",
            "        q: Optional[queue.Queue],",
            "        skip_notarget=True,",
            "    ) -> None:",
            "        \"\"\"",
            "        Single thread extractor that takes file lists in single 7zip folder.",
            "        this may raise exception.",
            "        \"\"\"",
            "        just_check: List[ArchiveFile] = []",
            "        for f in files:",
            "            if q is not None:",
            "                q.put(",
            "                    (",
            "                        \"s\",",
            "                        str(f.filename),",
            "                        str(f.compressed) if f.compressed is not None else \"0\",",
            "                    )",
            "                )",
            "            fileish = self.target_filepath.get(f.id, None)",
            "            if fileish is None:",
            "                if not f.emptystream:",
            "                    just_check.append(f)",
            "            else:",
            "                # delayed execution of crc check.",
            "                self._check(fp, just_check, src_end)",
            "                just_check = []",
            "                fileish.parent.mkdir(parents=True, exist_ok=True)",
            "                if not f.emptystream:",
            "                    if f.is_junction and not isinstance(fileish, MemIO) and sys.platform == \"win32\":",
            "                        with io.BytesIO() as ofp:",
            "                            self.decompress(fp, f.folder, ofp, f.uncompressed, f.compressed, src_end)",
            "                            dst: str = ofp.read().decode(\"utf-8\")",
            "                            # fileish.unlink(missing_ok=True) > py3.7",
            "                            if fileish.exists():",
            "                                fileish.unlink()",
            "                            if sys.platform == \"win32\":  # hint for mypy",
            "                                _winapi.CreateJunction(str(fileish), dst)  # noqa",
            "                    elif f.is_symlink and not isinstance(fileish, MemIO):",
            "                        with io.BytesIO() as omfp:",
            "                            self.decompress(fp, f.folder, omfp, f.uncompressed, f.compressed, src_end)",
            "                            omfp.seek(0)",
            "                            sym_target = pathlib.Path(omfp.read().decode(\"utf-8\"))",
            "                            # fileish.unlink(missing_ok=True) > py3.7",
            "                            if fileish.exists():",
            "                                fileish.unlink()",
            "                            fileish.symlink_to(sym_target)",
            "                    else:",
            "                        with fileish.open(mode=\"wb\") as obfp:",
            "                            crc32 = self.decompress(fp, f.folder, obfp, f.uncompressed, f.compressed, src_end)",
            "                            obfp.seek(0)",
            "                            if f.crc32 is not None and crc32 != f.crc32:",
            "                                raise CrcError(crc32, f.crc32, f.filename)",
            "                else:",
            "                    # just create empty file",
            "                    if not isinstance(fileish, MemIO):",
            "                        fileish.touch()",
            "                    else:",
            "                        with fileish.open() as ofp:",
            "                            pass",
            "            if q is not None:",
            "                q.put((\"e\", str(f.filename), str(f.uncompressed)))",
            "        if not skip_notarget:",
            "            # delayed execution of crc check.",
            "            self._check(fp, just_check, src_end)",
            "",
            "    def _check(self, fp, check_target, src_end):",
            "        \"\"\"",
            "        delayed execution of crc check.",
            "        \"\"\"",
            "        for f in check_target:",
            "            with NullIO() as ofp:",
            "                crc32 = self.decompress(fp, f.folder, ofp, f.uncompressed, f.compressed, src_end)",
            "            if f.crc32 is not None and crc32 != f.crc32:",
            "                raise CrcError(crc32, f.crc32, f.filename)",
            "",
            "    def decompress(",
            "        self,",
            "        fp: BinaryIO,",
            "        folder,",
            "        fq: IO[Any],",
            "        size: int,",
            "        compressed_size: Optional[int],",
            "        src_end: int,",
            "    ) -> int:",
            "        \"\"\"",
            "        decompressor wrapper called from extract method.",
            "",
            "        :parameter fp: archive source file pointer",
            "        :parameter folder: Folder object that have decompressor object.",
            "        :parameter fq: output file pathlib.Path",
            "        :parameter size: uncompressed size of target file.",
            "        :parameter compressed_size: compressed size of target file.",
            "        :parameter src_end: end position of the folder",
            "",
            "        :returns None",
            "",
            "        \"\"\"",
            "        assert folder is not None",
            "        out_remaining = size",
            "        max_block_size = get_memory_limit()",
            "        crc32 = 0",
            "        decompressor = folder.get_decompressor(compressed_size)",
            "        while out_remaining > 0:",
            "            tmp = decompressor.decompress(fp, min(out_remaining, max_block_size))",
            "            if len(tmp) > 0:",
            "                out_remaining -= len(tmp)",
            "                fq.write(tmp)",
            "                crc32 = calculate_crc32(tmp, crc32)",
            "            if out_remaining <= 0:",
            "                break",
            "        if fp.tell() >= src_end:",
            "            if decompressor.crc is not None and not decompressor.check_crc():",
            "                raise CrcError(decompressor.crc, decompressor.digest, None)",
            "        return crc32",
            "",
            "    def _find_link_target(self, target):",
            "        \"\"\"",
            "        Find the target member of a symlink or hardlink member in the archive.",
            "        \"\"\"",
            "        targetname: str = target.as_posix()",
            "        linkname = readlink(targetname)",
            "        # Check windows full path symlinks",
            "        if linkname.startswith(\"\\\\\\\\?\\\\\"):",
            "            linkname = linkname[4:]",
            "        # normalize as posix style",
            "        linkname: str = pathlib.Path(linkname).as_posix()",
            "        member = None",
            "        for j in range(len(self.files)):",
            "            if linkname == self.files[j].origin.as_posix():",
            "                # FIXME: when API user specify arcname, it will break",
            "                member = os.path.relpath(linkname, os.path.dirname(targetname))",
            "                break",
            "        if member is None:",
            "            member = linkname",
            "        return member",
            "",
            "    def _after_write(self, insize, foutsize, crc):",
            "        self.header.main_streams.substreamsinfo.digestsdefined.append(True)",
            "        self.header.main_streams.substreamsinfo.digests.append(crc)",
            "        if self.header.main_streams.substreamsinfo.unpacksizes is None:",
            "            self.header.main_streams.substreamsinfo.unpacksizes = [insize]",
            "        else:",
            "            self.header.main_streams.substreamsinfo.unpacksizes.append(insize)",
            "        if self.header.main_streams.substreamsinfo.num_unpackstreams_folders is None:",
            "            self.header.main_streams.substreamsinfo.num_unpackstreams_folders = [1]",
            "        else:",
            "            self.header.main_streams.substreamsinfo.num_unpackstreams_folders[-1] += 1",
            "        return foutsize, crc",
            "",
            "    def write(self, fp: BinaryIO, f, assym, folder):",
            "        compressor = folder.get_compressor()",
            "        if assym:",
            "            link_target: str = self._find_link_target(f.origin)",
            "            tgt: bytes = link_target.encode(\"utf-8\")",
            "            fd = io.BytesIO(tgt)",
            "            insize, foutsize, crc = compressor.compress(fd, fp)",
            "            fd.close()",
            "        else:",
            "            with f.origin.open(mode=\"rb\") as fd:",
            "                insize, foutsize, crc = compressor.compress(fd, fp)",
            "        return self._after_write(insize, foutsize, crc)",
            "",
            "    def writestr(self, fp: BinaryIO, f, folder):",
            "        compressor = folder.get_compressor()",
            "        insize, foutsize, crc = compressor.compress(f.data(), fp)",
            "        return self._after_write(insize, foutsize, crc)",
            "",
            "    def flush_archive(self, fp, folder):",
            "        compressor = folder.get_compressor()",
            "        foutsize = compressor.flush(fp)",
            "        if len(self.files) > 0:",
            "            if \"maxsize\" in self.header.files_info.files[self.last_file_index]:",
            "                self.header.files_info.files[self.last_file_index][\"maxsize\"] += foutsize",
            "            else:",
            "                self.header.files_info.files[self.last_file_index][\"maxsize\"] = foutsize",
            "        # Update size data in header",
            "        self.header.main_streams.packinfo.numstreams += 1",
            "        if self.header.main_streams.packinfo.enable_digests:",
            "            self.header.main_streams.packinfo.crcs.append(compressor.digest)",
            "            self.header.main_streams.packinfo.digestdefined.append(True)",
            "        self.header.main_streams.packinfo.packsizes.append(compressor.packsize)",
            "        folder.unpacksizes = compressor.unpacksizes",
            "",
            "    def archive(self, fp: BinaryIO, files, folder, deref=False):",
            "        \"\"\"Run archive task for specified 7zip folder.\"\"\"",
            "        f = files[self.current_file_index]",
            "        if f.has_strdata():",
            "            foutsize, crc = self.writestr(fp, f, folder)",
            "            self.header.files_info.files[self.current_file_index][\"maxsize\"] = foutsize",
            "            self.header.files_info.files[self.current_file_index][\"digest\"] = crc",
            "            self.last_file_index = self.current_file_index",
            "        elif (f.is_symlink and not deref) or not f.emptystream:",
            "            foutsize, crc = self.write(fp, f, (f.is_symlink and not deref), folder)",
            "            self.header.files_info.files[self.current_file_index][\"maxsize\"] = foutsize",
            "            self.header.files_info.files[self.current_file_index][\"digest\"] = crc",
            "            self.last_file_index = self.current_file_index",
            "        self.current_file_index += 1",
            "",
            "    def register_filelike(self, id: int, fileish: Union[MemIO, pathlib.Path, None]) -> None:",
            "        \"\"\"register file-ish to worker.\"\"\"",
            "        self.target_filepath[id] = fileish",
            "",
            "    def close(self):",
            "        del self.header",
            "        del self.files",
            "        del self.concurrent"
        ],
        "afterPatchFile": [
            "#!/usr/bin/python -u",
            "#",
            "# p7zr library",
            "#",
            "# Copyright (c) 2019-2021 Hiroshi Miura <miurahr@linux.com>",
            "# Copyright (c) 2004-2015 by Joachim Bauch, mail@joachim-bauch.de",
            "# 7-Zip Copyright (C) 1999-2010 Igor Pavlov",
            "# LZMA SDK Copyright (C) 1999-2010 Igor Pavlov",
            "#",
            "# This library is free software; you can redistribute it and/or",
            "# modify it under the terms of the GNU Lesser General Public",
            "# License as published by the Free Software Foundation; either",
            "# version 2.1 of the License, or (at your option) any later version.",
            "#",
            "# This library is distributed in the hope that it will be useful,",
            "# but WITHOUT ANY WARRANTY; without even the implied warranty of",
            "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU",
            "# Lesser General Public License for more details.",
            "#",
            "# You should have received a copy of the GNU Lesser General Public",
            "# License along with this library; if not, write to the Free Software",
            "# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA",
            "#",
            "#",
            "\"\"\"Read 7zip format archives.\"\"\"",
            "import collections.abc",
            "import contextlib",
            "import datetime",
            "import errno",
            "import functools",
            "import gc",
            "import io",
            "import os",
            "import pathlib",
            "import queue",
            "import stat",
            "import sys",
            "from multiprocessing import Process",
            "from threading import Thread",
            "from typing import IO, Any, BinaryIO, Dict, List, Optional, Tuple, Type, Union",
            "",
            "import multivolumefile",
            "",
            "from py7zr.archiveinfo import Folder, Header, SignatureHeader",
            "from py7zr.callbacks import ExtractCallback",
            "from py7zr.compressor import SupportedMethods, get_methods_names",
            "from py7zr.exceptions import Bad7zFile, CrcError, DecompressionError, InternalError, UnsupportedCompressionMethodError",
            "from py7zr.helpers import (",
            "    ArchiveTimestamp,",
            "    MemIO,",
            "    NullIO,",
            "    calculate_crc32,",
            "    check_archive_path,",
            "    filetime_to_dt,",
            "    get_sanitized_output_path,",
            "    is_target_path_valid,",
            "    readlink,",
            ")",
            "from py7zr.properties import DEFAULT_FILTERS, FILTER_DEFLATE64, MAGIC_7Z, get_default_blocksize, get_memory_limit",
            "",
            "if sys.platform.startswith(\"win\"):",
            "    import _winapi",
            "",
            "FILE_ATTRIBUTE_UNIX_EXTENSION = 0x8000",
            "FILE_ATTRIBUTE_WINDOWS_MASK = 0x07FFF",
            "",
            "",
            "class ArchiveFile:",
            "    \"\"\"Represent each files metadata inside archive file.",
            "    It holds file properties; filename, permissions, and type whether",
            "    it is directory, link or normal file.",
            "",
            "    Instances of the :class:`ArchiveFile` class are returned by iterating :attr:`files_list` of",
            "    :class:`SevenZipFile` objects.",
            "    Each object stores information about a single member of the 7z archive. Most of users use :meth:`extractall()`.",
            "",
            "    The class also hold an archive parameter where file is exist in",
            "    archive file folder(container).\"\"\"",
            "",
            "    def __init__(self, id: int, file_info: Dict[str, Any]) -> None:",
            "        self.id = id",
            "        self._file_info = file_info",
            "",
            "    def file_properties(self) -> Dict[str, Any]:",
            "        \"\"\"Return file properties as a hash object. Following keys are included: \u2018readonly\u2019, \u2018is_directory\u2019,",
            "        \u2018posix_mode\u2019, \u2018archivable\u2019, \u2018emptystream\u2019, \u2018filename\u2019, \u2018creationtime\u2019, \u2018lastaccesstime\u2019,",
            "        \u2018lastwritetime\u2019, \u2018attributes\u2019",
            "        \"\"\"",
            "        properties = self._file_info",
            "        if properties is not None:",
            "            properties[\"readonly\"] = self.readonly",
            "            properties[\"posix_mode\"] = self.posix_mode",
            "            properties[\"archivable\"] = self.archivable",
            "            properties[\"is_directory\"] = self.is_directory",
            "        return properties",
            "",
            "    def _get_property(self, key: str) -> Any:",
            "        try:",
            "            return self._file_info[key]",
            "        except KeyError:",
            "            return None",
            "",
            "    @property",
            "    def origin(self) -> pathlib.Path:",
            "        return self._get_property(\"origin\")",
            "",
            "    @property",
            "    def folder(self) -> Folder:",
            "        return self._get_property(\"folder\")",
            "",
            "    @property",
            "    def filename(self) -> str:",
            "        \"\"\"return filename of archive file.\"\"\"",
            "        return self._get_property(\"filename\")",
            "",
            "    @property",
            "    def emptystream(self) -> bool:",
            "        \"\"\"True if file is empty(0-byte file), otherwise False\"\"\"",
            "        return self._get_property(\"emptystream\")",
            "",
            "    @property",
            "    def uncompressed(self) -> List[int]:",
            "        return self._get_property(\"uncompressed\")",
            "",
            "    @property",
            "    def compressed(self) -> Optional[int]:",
            "        \"\"\"Compressed size\"\"\"",
            "        return self._get_property(\"compressed\")",
            "",
            "    @property",
            "    def crc32(self) -> Optional[int]:",
            "        \"\"\"CRC of archived file(optional)\"\"\"",
            "        return self._get_property(\"digest\")",
            "",
            "    def _test_attribute(self, target_bit: int) -> bool:",
            "        attributes = self._get_property(\"attributes\")",
            "        if attributes is None:",
            "            return False",
            "        return attributes & target_bit == target_bit",
            "",
            "    @property",
            "    def archivable(self) -> bool:",
            "        \"\"\"File has a Windows `archive` flag.\"\"\"",
            "        if hasattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\"):",
            "            return self._test_attribute(getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\"))",
            "        return False",
            "",
            "    @property",
            "    def is_directory(self) -> bool:",
            "        \"\"\"True if file is a directory, otherwise False.\"\"\"",
            "        if hasattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\"):",
            "            return self._test_attribute(getattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\"))",
            "        return False",
            "",
            "    @property",
            "    def readonly(self) -> bool:",
            "        \"\"\"True if file is readonly, otherwise False.\"\"\"",
            "        if hasattr(stat, \"FILE_ATTRIBUTE_READONLY\"):",
            "            return self._test_attribute(getattr(stat, \"FILE_ATTRIBUTE_READONLY\"))",
            "        return False",
            "",
            "    def _get_unix_extension(self) -> Optional[int]:",
            "        attributes = self._get_property(\"attributes\")",
            "        if self._test_attribute(FILE_ATTRIBUTE_UNIX_EXTENSION):",
            "            return attributes >> 16",
            "        return None",
            "",
            "    def data(self) -> Optional[BinaryIO]:",
            "        return self._get_property(\"data\")",
            "",
            "    def has_strdata(self) -> bool:",
            "        \"\"\"True if file content is set by writestr() method otherwise False.\"\"\"",
            "        return \"data\" in self._file_info",
            "",
            "    @property",
            "    def is_symlink(self) -> bool:",
            "        \"\"\"True if file is a symbolic link, otherwise False.\"\"\"",
            "        e = self._get_unix_extension()",
            "        if e is not None:",
            "            return stat.S_ISLNK(e)",
            "        if hasattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\"):",
            "            return self._test_attribute(getattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\"))",
            "        return False",
            "",
            "    @property",
            "    def is_junction(self) -> bool:",
            "        \"\"\"True if file is a junction/reparse point on windows, otherwise False.\"\"\"",
            "        if hasattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\"):",
            "            return self._test_attribute(",
            "                getattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\") | getattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\")",
            "            )",
            "        return False",
            "",
            "    @property",
            "    def is_socket(self) -> bool:",
            "        \"\"\"True if file is a socket, otherwise False.\"\"\"",
            "        e = self._get_unix_extension()",
            "        if e is not None:",
            "            return stat.S_ISSOCK(e)",
            "        return False",
            "",
            "    @property",
            "    def lastwritetime(self) -> Optional[ArchiveTimestamp]:",
            "        \"\"\"Return last written timestamp of a file.\"\"\"",
            "        return self._get_property(\"lastwritetime\")",
            "",
            "    @property",
            "    def posix_mode(self) -> Optional[int]:",
            "        \"\"\"",
            "        posix mode when a member has a unix extension property, or None",
            "        :return: Return file stat mode can be set by os.chmod()",
            "        \"\"\"",
            "        e = self._get_unix_extension()",
            "        if e is not None:",
            "            return stat.S_IMODE(e)",
            "        return None",
            "",
            "    @property",
            "    def st_fmt(self) -> Optional[int]:",
            "        \"\"\"",
            "        :return: Return the portion of the file mode that describes the file type",
            "        \"\"\"",
            "        e = self._get_unix_extension()",
            "        if e is not None:",
            "            return stat.S_IFMT(e)",
            "        return None",
            "",
            "",
            "class ArchiveFileList(collections.abc.Iterable):",
            "    \"\"\"Iteratable container of ArchiveFile.\"\"\"",
            "",
            "    def __init__(self, offset: int = 0):",
            "        self.files_list: List[dict] = []",
            "        self.index = 0",
            "        self.offset = offset",
            "",
            "    def append(self, file_info: Dict[str, Any]) -> None:",
            "        self.files_list.append(file_info)",
            "",
            "    def __len__(self) -> int:",
            "        return len(self.files_list)",
            "",
            "    def __iter__(self) -> \"ArchiveFileListIterator\":",
            "        return ArchiveFileListIterator(self)",
            "",
            "    def __getitem__(self, index):",
            "        if index > len(self.files_list):",
            "            raise IndexError",
            "        if index < 0:",
            "            raise IndexError",
            "        res = ArchiveFile(index + self.offset, self.files_list[index])",
            "        return res",
            "",
            "",
            "class ArchiveFileListIterator(collections.abc.Iterator):",
            "    def __init__(self, archive_file_list):",
            "        self._archive_file_list = archive_file_list",
            "        self._index = 0",
            "",
            "    def __next__(self) -> ArchiveFile:",
            "        if self._index == len(self._archive_file_list):",
            "            raise StopIteration",
            "        res = self._archive_file_list[self._index]",
            "        self._index += 1",
            "        return res",
            "",
            "",
            "# ------------------",
            "# Exported Classes",
            "# ------------------",
            "class ArchiveInfo:",
            "    \"\"\"Hold archive information\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        filename: str,",
            "        stat: os.stat_result,",
            "        header_size: int,",
            "        method_names: List[str],",
            "        solid: bool,",
            "        blocks: int,",
            "        uncompressed: List[int],",
            "    ):",
            "        self.stat = stat",
            "        self.filename = filename",
            "        self.size = stat.st_size",
            "        self.header_size = header_size",
            "        self.method_names = method_names",
            "        self.solid = solid",
            "        self.blocks = blocks",
            "        self.uncompressed = uncompressed",
            "",
            "",
            "class FileInfo:",
            "    \"\"\"Hold archived file information.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        filename,",
            "        compressed,",
            "        uncompressed,",
            "        archivable,",
            "        is_directory,",
            "        creationtime,",
            "        crc32,",
            "    ):",
            "        self.filename = filename",
            "        self.compressed = compressed",
            "        self.uncompressed = uncompressed",
            "        self.archivable = archivable",
            "        self.is_directory = is_directory",
            "        self.creationtime = creationtime",
            "        self.crc32 = crc32",
            "",
            "",
            "class SevenZipFile(contextlib.AbstractContextManager):",
            "    \"\"\"The SevenZipFile Class provides an interface to 7z archives.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        file: Union[BinaryIO, str, pathlib.Path],",
            "        mode: str = \"r\",",
            "        *,",
            "        filters: Optional[List[Dict[str, int]]] = None,",
            "        dereference=False,",
            "        password: Optional[str] = None,",
            "        header_encryption: bool = False,",
            "        blocksize: Optional[int] = None,",
            "        mp: bool = False,",
            "    ) -> None:",
            "        if mode not in (\"r\", \"w\", \"x\", \"a\"):",
            "            raise ValueError(\"ZipFile requires mode 'r', 'w', 'x', or 'a'\")",
            "        self.fp: BinaryIO",
            "        self.mp = mp",
            "        self.password_protected = password is not None",
            "        if blocksize:",
            "            self._block_size = blocksize",
            "        else:",
            "            self._block_size = get_default_blocksize()",
            "        # Check if we were passed a file-like object or not",
            "        if isinstance(file, str):",
            "            self._filePassed: bool = False",
            "            self.filename: str = file",
            "            if mode == \"r\":",
            "                self.fp = open(file, \"rb\")",
            "            elif mode == \"w\":",
            "                self.fp = open(file, \"w+b\")",
            "            elif mode == \"x\":",
            "                self.fp = open(file, \"x+b\")",
            "            elif mode == \"a\":",
            "                self.fp = open(file, \"r+b\")",
            "            else:",
            "                raise ValueError(\"File open error.\")",
            "            self.mode = mode",
            "        elif isinstance(file, pathlib.Path):",
            "            self._filePassed = False",
            "            self.filename = str(file)",
            "            if mode == \"r\":",
            "                self.fp = file.open(mode=\"rb\")  # noqa   # typeshed issue: 2911",
            "            elif mode == \"w\":",
            "                self.fp = file.open(mode=\"w+b\")  # noqa",
            "            elif mode == \"x\":",
            "                self.fp = file.open(mode=\"x+b\")  # noqa",
            "            elif mode == \"a\":",
            "                self.fp = file.open(mode=\"r+b\")  # noqa",
            "            else:",
            "                raise ValueError(\"File open error.\")",
            "            self.mode = mode",
            "        elif isinstance(file, multivolumefile.MultiVolume):",
            "            self._filePassed = True",
            "            self.fp = file",
            "            self.filename = None",
            "            self.mode = mode  # noqa",
            "        elif isinstance(file, io.IOBase):",
            "            self._filePassed = True",
            "            self.fp = file",
            "            self.filename = getattr(file, \"name\", None)",
            "            self.mode = mode  # noqa",
            "        else:",
            "            raise TypeError(\"invalid file: {}\".format(type(file)))",
            "        self.encoded_header_mode = True",
            "        self.header_encryption = header_encryption",
            "        self._fileRefCnt = 1",
            "        try:",
            "            if mode == \"r\":",
            "                self._real_get_contents(password)",
            "                self.fp.seek(self.afterheader)  # seek into start of payload and prepare worker to extract",
            "                self.worker = Worker(self.files, self.afterheader, self.header, self.mp)",
            "            elif mode in \"w\":",
            "                self._prepare_write(filters, password)",
            "            elif mode in \"x\":",
            "                raise NotImplementedError",
            "            elif mode == \"a\":",
            "                self._real_get_contents(password)",
            "                self._prepare_append(filters, password)",
            "            else:",
            "                raise ValueError(\"Mode must be 'r', 'w', 'x', or 'a'\")",
            "        except Exception as e:",
            "            self._fpclose()",
            "            raise e",
            "        self._dict: Dict[str, IO[Any]] = {}",
            "        self.dereference = dereference",
            "        self.reporterd: Optional[Thread] = None",
            "        self.q: queue.Queue[Any] = queue.Queue()",
            "",
            "    def __enter__(self):",
            "        return self",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        self.close()",
            "",
            "    def _fpclose(self) -> None:",
            "        assert self._fileRefCnt > 0",
            "        self._fileRefCnt -= 1",
            "        if not self._fileRefCnt and not self._filePassed:",
            "            self.fp.close()",
            "",
            "    def _real_get_contents(self, password) -> None:",
            "        if not self._check_7zfile(self.fp):",
            "            raise Bad7zFile(\"not a 7z file\")",
            "        self.sig_header = SignatureHeader.retrieve(self.fp)",
            "        self.afterheader: int = self.fp.tell()",
            "        self.fp.seek(self.sig_header.nextheaderofs, os.SEEK_CUR)",
            "        buffer = io.BytesIO(self.fp.read(self.sig_header.nextheadersize))",
            "        if self.sig_header.nextheadercrc != calculate_crc32(buffer.getvalue()):",
            "            raise Bad7zFile(\"invalid header data\")",
            "        header = Header.retrieve(self.fp, buffer, self.afterheader, password)",
            "        if header is None:",
            "            return",
            "        header._initilized = True",
            "        self.header = header",
            "        header.size += 32 + self.sig_header.nextheadersize",
            "        buffer.close()",
            "        self.files = ArchiveFileList()",
            "        if getattr(self.header, \"files_info\", None) is None:",
            "            return",
            "        # Initialize references for convenience",
            "        if hasattr(self.header, \"main_streams\") and self.header.main_streams is not None:",
            "            folders = self.header.main_streams.unpackinfo.folders",
            "            for folder in folders:",
            "                folder.password = password",
            "            packinfo = self.header.main_streams.packinfo",
            "            packsizes = packinfo.packsizes",
            "            subinfo = self.header.main_streams.substreamsinfo",
            "            if subinfo is not None and subinfo.unpacksizes is not None:",
            "                unpacksizes = subinfo.unpacksizes",
            "            else:",
            "                unpacksizes = [x.unpacksizes[-1] for x in folders]",
            "        else:",
            "            subinfo = None",
            "            folders = None",
            "            packinfo = None",
            "            packsizes = []",
            "            unpacksizes = [0]",
            "",
            "        pstat = self.ParseStatus()",
            "        pstat.src_pos = self.afterheader",
            "        file_in_solid = 0",
            "",
            "        for file_id, file_info in enumerate(self.header.files_info.files):",
            "            if not file_info[\"emptystream\"] and folders is not None:",
            "                folder = folders[pstat.folder]",
            "                numinstreams = max([coder.get(\"numinstreams\", 1) for coder in folder.coders])",
            "                (maxsize, compressed, uncompressed, packsize, solid,) = self._get_fileinfo_sizes(",
            "                    pstat,",
            "                    subinfo,",
            "                    packinfo,",
            "                    folder,",
            "                    packsizes,",
            "                    unpacksizes,",
            "                    file_in_solid,",
            "                    numinstreams,",
            "                )",
            "                pstat.input += 1",
            "                folder.solid = solid",
            "                file_info[\"folder\"] = folder",
            "                file_info[\"maxsize\"] = maxsize",
            "                file_info[\"compressed\"] = compressed",
            "                file_info[\"uncompressed\"] = uncompressed",
            "                file_info[\"packsizes\"] = packsize",
            "                if subinfo.digestsdefined[pstat.outstreams]:",
            "                    file_info[\"digest\"] = subinfo.digests[pstat.outstreams]",
            "                if folder is None:",
            "                    pstat.src_pos += file_info[\"compressed\"]",
            "                else:",
            "                    if folder.solid:",
            "                        file_in_solid += 1",
            "                    pstat.outstreams += 1",
            "                    if folder.files is None:",
            "                        folder.files = ArchiveFileList(offset=file_id)",
            "                    folder.files.append(file_info)",
            "                    if pstat.input >= subinfo.num_unpackstreams_folders[pstat.folder]:",
            "                        file_in_solid = 0",
            "                        pstat.src_pos += sum(packinfo.packsizes[pstat.stream : pstat.stream + numinstreams])",
            "                        pstat.folder += 1",
            "                        pstat.stream += numinstreams",
            "                        pstat.input = 0",
            "            else:",
            "                file_info[\"folder\"] = None",
            "                file_info[\"maxsize\"] = 0",
            "                file_info[\"compressed\"] = 0",
            "                file_info[\"uncompressed\"] = 0",
            "                file_info[\"packsizes\"] = [0]",
            "",
            "            if \"filename\" not in file_info:",
            "                # compressed file is stored without a name, generate one",
            "                try:",
            "                    basefilename = self.filename",
            "                except AttributeError:",
            "                    # 7z archive file doesn't have a name",
            "                    file_info[\"filename\"] = \"contents\"",
            "                else:",
            "                    if basefilename is not None:",
            "                        fn, ext = os.path.splitext(os.path.basename(basefilename))",
            "                        file_info[\"filename\"] = fn",
            "                    else:",
            "                        file_info[\"filename\"] = \"contents\"",
            "            self.files.append(file_info)",
            "        if not self.password_protected and self.header.main_streams is not None:",
            "            # Check specified coders have a crypt method or not.",
            "            self.password_protected = any(",
            "                [SupportedMethods.needs_password(folder.coders) for folder in self.header.main_streams.unpackinfo.folders]",
            "            )",
            "",
            "    def _extract(",
            "        self,",
            "        path: Optional[Any] = None,",
            "        targets: Optional[List[str]] = None,",
            "        return_dict: bool = False,",
            "        callback: Optional[ExtractCallback] = None,",
            "    ) -> Optional[Dict[str, IO[Any]]]:",
            "        if callback is None:",
            "            pass",
            "        elif isinstance(callback, ExtractCallback):",
            "            self.reporterd = Thread(target=self.reporter, args=(callback,), daemon=True)",
            "            self.reporterd.start()",
            "        else:",
            "            raise ValueError(\"Callback specified is not an instance of subclass of py7zr.callbacks.ExtractCallback class\")",
            "        target_files: List[Tuple[pathlib.Path, Dict[str, Any]]] = []",
            "        target_dirs: List[pathlib.Path] = []",
            "        if path is not None:",
            "            if isinstance(path, str):",
            "                path = pathlib.Path(path)",
            "            try:",
            "                if not path.exists():",
            "                    path.mkdir(parents=True)",
            "                else:",
            "                    pass",
            "            except OSError as e:",
            "                if e.errno == errno.EEXIST and path.is_dir():",
            "                    pass",
            "                else:",
            "                    raise e",
            "        fnames: List[str] = []  # check duplicated filename in one archive?",
            "        self.q.put((\"pre\", None, None))",
            "        for f in self.files:",
            "            # When archive has a multiple files which have same name",
            "            # To guarantee order of archive, multi-thread decompression becomes off.",
            "            # Currently always overwrite by latter archives.",
            "            # TODO: provide option to select overwrite or skip.",
            "            if f.filename not in fnames:",
            "                outname = f.filename",
            "            else:",
            "                i = 0",
            "                while True:",
            "                    outname = f.filename + \"_%d\" % i",
            "                    if outname not in fnames:",
            "                        break",
            "                    i += 1",
            "            fnames.append(outname)",
            "            if path is None or path.is_absolute():",
            "                outfilename = get_sanitized_output_path(outname, path)",
            "            else:",
            "                outfilename = get_sanitized_output_path(outname, pathlib.Path(os.getcwd()).joinpath(path))",
            "            if targets is not None and f.filename not in targets:",
            "                self.worker.register_filelike(f.id, None)",
            "                continue",
            "            if return_dict:",
            "                if f.is_directory or f.is_socket:",
            "                    # ignore special files and directories",
            "                    pass",
            "                else:",
            "                    fname = outfilename.as_posix()",
            "                    _buf = io.BytesIO()",
            "                    self._dict[fname] = _buf",
            "                    self.worker.register_filelike(f.id, MemIO(_buf))",
            "            elif f.is_directory:",
            "                if not outfilename.exists():",
            "                    target_dirs.append(outfilename)",
            "                    target_files.append((outfilename, f.file_properties()))",
            "                else:",
            "                    pass",
            "            elif f.is_socket:",
            "                pass  # TODO: implement me.",
            "            elif f.is_symlink or f.is_junction:",
            "                self.worker.register_filelike(f.id, outfilename)",
            "            else:",
            "                self.worker.register_filelike(f.id, outfilename)",
            "                target_files.append((outfilename, f.file_properties()))",
            "        for target_dir in sorted(target_dirs):",
            "            try:",
            "                target_dir.mkdir(parents=True)",
            "            except FileExistsError:",
            "                if target_dir.is_dir():",
            "                    pass",
            "                elif target_dir.is_file():",
            "                    raise DecompressionError(\"Directory {} is existed as a normal file.\".format(str(target_dir)))",
            "                else:",
            "                    raise DecompressionError(\"Directory {} making fails on unknown condition.\".format(str(target_dir)))",
            "",
            "        if callback is not None:",
            "            self.worker.extract(",
            "                self.fp,",
            "                path,",
            "                parallel=(not self.password_protected and not self._filePassed),",
            "                q=self.q,",
            "            )",
            "        else:",
            "            self.worker.extract(",
            "                self.fp,",
            "                path,",
            "                parallel=(not self.password_protected and not self._filePassed),",
            "            )",
            "",
            "        self.q.put((\"post\", None, None))",
            "        # early return when dict specified",
            "        if return_dict:",
            "            return self._dict",
            "        # set file properties",
            "        for outfilename, properties in target_files:",
            "            # mtime",
            "            lastmodified = None",
            "            try:",
            "                lastmodified = ArchiveTimestamp(properties[\"lastwritetime\"]).totimestamp()",
            "            except KeyError:",
            "                pass",
            "            if lastmodified is not None:",
            "                os.utime(str(outfilename), times=(lastmodified, lastmodified))",
            "            if os.name == \"posix\":",
            "                st_mode = properties[\"posix_mode\"]",
            "                if st_mode is not None:",
            "                    outfilename.chmod(st_mode)",
            "                    continue",
            "            # fallback: only set readonly if specified",
            "            if properties[\"readonly\"] and not properties[\"is_directory\"]:",
            "                ro_mask = 0o777 ^ (stat.S_IWRITE | stat.S_IWGRP | stat.S_IWOTH)",
            "                outfilename.chmod(outfilename.stat().st_mode & ro_mask)",
            "        return None",
            "",
            "    def _prepare_append(self, filters, password):",
            "        if password is not None and filters is None:",
            "            filters = DEFAULT_FILTERS.ENCRYPTED_ARCHIVE_FILTER",
            "        elif filters is None:",
            "            filters = DEFAULT_FILTERS.ARCHIVE_FILTER",
            "        else:",
            "            for f in filters:",
            "                if f[\"id\"] == FILTER_DEFLATE64:",
            "                    raise UnsupportedCompressionMethodError(filters, \"Compression with deflate64 is not supported.\")",
            "        self.header.filters = filters",
            "        self.header.password = password",
            "        if self.header.main_streams is not None:",
            "            pos = self.afterheader + self.header.main_streams.packinfo.packpositions[-1]",
            "        else:",
            "            pos = self.afterheader",
            "        self.fp.seek(pos)",
            "        self.worker = Worker(self.files, pos, self.header, self.mp)",
            "",
            "    def _prepare_write(self, filters, password):",
            "        if password is not None and filters is None:",
            "            filters = DEFAULT_FILTERS.ENCRYPTED_ARCHIVE_FILTER",
            "        elif filters is None:",
            "            filters = DEFAULT_FILTERS.ARCHIVE_FILTER",
            "        self.files = ArchiveFileList()",
            "        self.sig_header = SignatureHeader()",
            "        self.sig_header._write_skelton(self.fp)",
            "        self.afterheader = self.fp.tell()",
            "        self.header = Header.build_header(filters, password)",
            "        self.fp.seek(self.afterheader)",
            "        self.worker = Worker(self.files, self.afterheader, self.header, self.mp)",
            "",
            "    def _write_flush(self):",
            "        if self.header._initialized:",
            "            folder = self.header.main_streams.unpackinfo.folders[-1]",
            "            self.worker.flush_archive(self.fp, folder)",
            "        self._write_header()",
            "",
            "    def _write_header(self):",
            "        \"\"\"Write header and update signature header.\"\"\"",
            "        (header_pos, header_len, header_crc) = self.header.write(",
            "            self.fp,",
            "            self.afterheader,",
            "            encoded=self.encoded_header_mode,",
            "            encrypted=self.header_encryption,",
            "        )",
            "        self.sig_header.nextheaderofs = header_pos - self.afterheader",
            "        self.sig_header.calccrc(header_len, header_crc)",
            "        self.sig_header.write(self.fp)",
            "",
            "    def _writeall(self, path, arcname):",
            "        try:",
            "            if path.is_symlink() and not self.dereference:",
            "                self.write(path, arcname)",
            "            elif path.is_file():",
            "                self.write(path, arcname)",
            "            elif path.is_dir():",
            "                if not path.samefile(\".\"):",
            "                    self.write(path, arcname)",
            "                for nm in sorted(os.listdir(str(path))):",
            "                    arc = os.path.join(arcname, nm) if arcname is not None else None",
            "                    self._writeall(path.joinpath(nm), arc)",
            "            else:",
            "                return  # pathlib ignores ELOOP and return False for is_*().",
            "        except OSError as ose:",
            "            if self.dereference and ose.errno in [errno.ELOOP]:",
            "                return  # ignore ELOOP here, this resulted to stop looped symlink reference.",
            "            elif self.dereference and sys.platform == \"win32\" and ose.errno in [errno.ENOENT]:",
            "                return  # ignore ENOENT which is happened when a case of ELOOP on windows.",
            "            else:",
            "                raise",
            "",
            "    class ParseStatus:",
            "        def __init__(self, src_pos=0):",
            "            self.src_pos = src_pos",
            "            self.folder = 0  # 7zip folder where target stored",
            "            self.outstreams = 0  # output stream count",
            "            self.input = 0  # unpack stream count in each folder",
            "            self.stream = 0  # target input stream position",
            "",
            "    def _get_fileinfo_sizes(",
            "        self,",
            "        pstat,",
            "        subinfo,",
            "        packinfo,",
            "        folder,",
            "        packsizes,",
            "        unpacksizes,",
            "        file_in_solid,",
            "        numinstreams,",
            "    ):",
            "        if pstat.input == 0:",
            "            folder.solid = subinfo.num_unpackstreams_folders[pstat.folder] > 1",
            "        maxsize = (folder.solid and packinfo.packsizes[pstat.stream]) or None",
            "        uncompressed = unpacksizes[pstat.outstreams]",
            "        if file_in_solid > 0:",
            "            compressed = None",
            "        elif pstat.stream < len(packsizes):  # file is compressed",
            "            compressed = packsizes[pstat.stream]",
            "        else:  # file is not compressed",
            "            compressed = uncompressed",
            "        packsize = packsizes[pstat.stream : pstat.stream + numinstreams]",
            "        return maxsize, compressed, uncompressed, packsize, folder.solid",
            "",
            "    def set_encoded_header_mode(self, mode: bool) -> None:",
            "        if mode:",
            "            self.encoded_header_mode = True",
            "        else:",
            "            self.encoded_header_mode = False",
            "            self.header_encryption = False",
            "",
            "    def set_encrypted_header(self, mode: bool) -> None:",
            "        if mode:",
            "            self.encoded_header_mode = True",
            "            self.header_encryption = True",
            "        else:",
            "            self.header_encryption = False",
            "",
            "    @staticmethod",
            "    def _check_7zfile(fp: Union[BinaryIO, io.BufferedReader, io.IOBase]) -> bool:",
            "        result = MAGIC_7Z == fp.read(len(MAGIC_7Z))[: len(MAGIC_7Z)]",
            "        fp.seek(-len(MAGIC_7Z), 1)",
            "        return result",
            "",
            "    def _get_method_names(self) -> List[str]:",
            "        try:",
            "            return get_methods_names([folder.coders for folder in self.header.main_streams.unpackinfo.folders])",
            "        except KeyError:",
            "            raise UnsupportedCompressionMethodError(self.header.main_streams.unpackinfo.folders, \"Unknown method\")",
            "",
            "    def _read_digest(self, pos: int, size: int) -> int:",
            "        self.fp.seek(pos)",
            "        remaining_size = size",
            "        digest = 0",
            "        while remaining_size > 0:",
            "            block = min(self._block_size, remaining_size)",
            "            digest = calculate_crc32(self.fp.read(block), digest)",
            "            remaining_size -= block",
            "        return digest",
            "",
            "    def _is_solid(self):",
            "        for f in self.header.main_streams.substreamsinfo.num_unpackstreams_folders:",
            "            if f > 1:",
            "                return True",
            "        return False",
            "",
            "    def _var_release(self):",
            "        self._dict = None",
            "        self.worker.close()",
            "        del self.worker",
            "        del self.files",
            "        del self.header",
            "        del self.sig_header",
            "        gc.collect()",
            "",
            "    @staticmethod",
            "    def _make_file_info(target: pathlib.Path, arcname: Optional[str] = None, dereference=False) -> Dict[str, Any]:",
            "        f: Dict[str, Any] = {}",
            "        f[\"origin\"] = target",
            "        if arcname is not None:",
            "            f[\"filename\"] = pathlib.Path(arcname).as_posix()",
            "        else:",
            "            f[\"filename\"] = target.as_posix()",
            "        if sys.platform == \"win32\":",
            "            fstat = target.lstat()",
            "            if target.is_symlink():",
            "                if dereference:",
            "                    fstat = target.stat()",
            "                    if stat.S_ISDIR(fstat.st_mode):",
            "                        f[\"emptystream\"] = True",
            "                        f[\"attributes\"] = fstat.st_file_attributes & FILE_ATTRIBUTE_WINDOWS_MASK  # noqa",
            "                    else:",
            "                        f[\"emptystream\"] = False",
            "                        f[\"attributes\"] = stat.FILE_ATTRIBUTE_ARCHIVE  # noqa",
            "                        f[\"uncompressed\"] = fstat.st_size",
            "                else:",
            "                    f[\"emptystream\"] = False",
            "                    f[\"attributes\"] = fstat.st_file_attributes & FILE_ATTRIBUTE_WINDOWS_MASK  # noqa",
            "                    # TODO: handle junctions",
            "                    # f['attributes'] |= stat.FILE_ATTRIBUTE_REPARSE_POINT  # noqa",
            "            elif target.is_dir():",
            "                f[\"emptystream\"] = True",
            "                f[\"attributes\"] = fstat.st_file_attributes & FILE_ATTRIBUTE_WINDOWS_MASK  # noqa",
            "            elif target.is_file():",
            "                f[\"emptystream\"] = False",
            "                f[\"attributes\"] = stat.FILE_ATTRIBUTE_ARCHIVE  # noqa",
            "                f[\"uncompressed\"] = fstat.st_size",
            "        elif (",
            "            sys.platform == \"darwin\"",
            "            or sys.platform.startswith(\"linux\")",
            "            or sys.platform.startswith(\"freebsd\")",
            "            or sys.platform.startswith(\"netbsd\")",
            "            or sys.platform.startswith(\"sunos\")",
            "            or sys.platform == \"aix\"",
            "        ):",
            "            fstat = target.lstat()",
            "            if target.is_symlink():",
            "                if dereference:",
            "                    fstat = target.stat()",
            "                    if stat.S_ISDIR(fstat.st_mode):",
            "                        f[\"emptystream\"] = True",
            "                        f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\")",
            "                        f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IFDIR << 16)",
            "                        f[\"attributes\"] |= stat.S_IMODE(fstat.st_mode) << 16",
            "                    else:",
            "                        f[\"emptystream\"] = False",
            "                        f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\")",
            "                        f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IMODE(fstat.st_mode) << 16)",
            "                else:",
            "                    f[\"emptystream\"] = False",
            "                    f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\") | getattr(stat, \"FILE_ATTRIBUTE_REPARSE_POINT\")",
            "                    f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IFLNK << 16)",
            "                    f[\"attributes\"] |= stat.S_IMODE(fstat.st_mode) << 16",
            "            elif target.is_dir():",
            "                f[\"emptystream\"] = True",
            "                f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_DIRECTORY\")",
            "                f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IFDIR << 16)",
            "                f[\"attributes\"] |= stat.S_IMODE(fstat.st_mode) << 16",
            "            elif target.is_file():",
            "                f[\"emptystream\"] = False",
            "                f[\"uncompressed\"] = fstat.st_size",
            "                f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\")",
            "                f[\"attributes\"] |= FILE_ATTRIBUTE_UNIX_EXTENSION | (stat.S_IMODE(fstat.st_mode) << 16)",
            "        else:",
            "            fstat = target.stat()",
            "            if target.is_dir():",
            "                f[\"emptystream\"] = True",
            "                f[\"attributes\"] = stat.FILE_ATTRIBUTE_DIRECTORY",
            "            elif target.is_file():",
            "                f[\"emptystream\"] = False",
            "                f[\"uncompressed\"] = fstat.st_size",
            "                f[\"attributes\"] = stat.FILE_ATTRIBUTE_ARCHIVE",
            "",
            "        f[\"creationtime\"] = ArchiveTimestamp.from_datetime(fstat.st_ctime)",
            "        f[\"lastwritetime\"] = ArchiveTimestamp.from_datetime(fstat.st_mtime)",
            "        f[\"lastaccesstime\"] = ArchiveTimestamp.from_datetime(fstat.st_atime)",
            "        return f",
            "",
            "    def _make_file_info_from_name(self, bio, size: int, arcname: str) -> Dict[str, Any]:",
            "        f: Dict[str, Any] = {}",
            "        f[\"origin\"] = None",
            "        f[\"data\"] = bio",
            "        f[\"filename\"] = pathlib.Path(arcname).as_posix()",
            "        f[\"uncompressed\"] = size",
            "        f[\"emptystream\"] = size == 0",
            "        f[\"attributes\"] = getattr(stat, \"FILE_ATTRIBUTE_ARCHIVE\")",
            "        f[\"creationtime\"] = ArchiveTimestamp.from_now()",
            "        f[\"lastwritetime\"] = ArchiveTimestamp.from_now()",
            "        return f",
            "",
            "    # --------------------------------------------------------------------------",
            "    # The public methods which SevenZipFile provides:",
            "    def getnames(self) -> List[str]:",
            "        \"\"\"Return the members of the archive as a list of their names. It has",
            "        the same order as the list returned by getmembers().",
            "        \"\"\"",
            "        return list(map(lambda x: x.filename, self.files))",
            "",
            "    def archiveinfo(self) -> ArchiveInfo:",
            "        total_uncompressed = functools.reduce(lambda x, y: x + y, [f.uncompressed for f in self.files])",
            "        if isinstance(self.fp, multivolumefile.MultiVolume):",
            "            fname = self.fp.name",
            "            fstat = self.fp.stat()",
            "        else:",
            "            fname = self.filename",
            "            assert fname is not None",
            "            fstat = os.stat(fname)",
            "        return ArchiveInfo(",
            "            fname,",
            "            fstat,",
            "            self.header.size,",
            "            self._get_method_names(),",
            "            self._is_solid(),",
            "            len(self.header.main_streams.unpackinfo.folders),",
            "            total_uncompressed,",
            "        )",
            "",
            "    def needs_password(self) -> bool:",
            "        return self.password_protected",
            "",
            "    def list(self) -> List[FileInfo]:",
            "        \"\"\"Returns contents information\"\"\"",
            "        alist: List[FileInfo] = []",
            "        lastmodified: Optional[datetime.datetime] = None",
            "        for f in self.files:",
            "            if f.lastwritetime is not None:",
            "                lastmodified = filetime_to_dt(f.lastwritetime)",
            "            alist.append(",
            "                FileInfo(",
            "                    f.filename,",
            "                    f.compressed,",
            "                    f.uncompressed,",
            "                    f.archivable,",
            "                    f.is_directory,",
            "                    lastmodified,",
            "                    f.crc32,",
            "                )",
            "            )",
            "        return alist",
            "",
            "    def readall(self) -> Optional[Dict[str, IO[Any]]]:",
            "        self._dict = {}",
            "        return self._extract(path=None, return_dict=True)",
            "",
            "    def extractall(self, path: Optional[Any] = None, callback: Optional[ExtractCallback] = None) -> None:",
            "        \"\"\"Extract all members from the archive to the current working",
            "        directory and set owner, modification time and permissions on",
            "        directories afterwards. ``path`` specifies a different directory",
            "        to extract to.",
            "        \"\"\"",
            "        self._extract(path=path, return_dict=False, callback=callback)",
            "",
            "    def read(self, targets: Optional[List[str]] = None) -> Optional[Dict[str, IO[Any]]]:",
            "        self._dict = {}",
            "        return self._extract(path=None, targets=targets, return_dict=True)",
            "",
            "    def extract(self, path: Optional[Any] = None, targets: Optional[List[str]] = None) -> None:",
            "        self._extract(path, targets, return_dict=False)",
            "",
            "    def reporter(self, callback: ExtractCallback):",
            "        while True:",
            "            try:",
            "                item: Optional[Tuple[str, str, str]] = self.q.get(timeout=1)",
            "            except queue.Empty:",
            "                pass",
            "            else:",
            "                if item is None:",
            "                    break",
            "                elif item[0] == \"s\":",
            "                    callback.report_start(item[1], item[2])",
            "                elif item[0] == \"e\":",
            "                    callback.report_end(item[1], item[2])",
            "                elif item[0] == \"pre\":",
            "                    callback.report_start_preparation()",
            "                elif item[0] == \"post\":",
            "                    callback.report_postprocess()",
            "                elif item[0] == \"w\":",
            "                    callback.report_warning(item[1])",
            "                else:",
            "                    pass",
            "                self.q.task_done()",
            "",
            "    def writeall(self, path: Union[pathlib.Path, str], arcname: Optional[str] = None):",
            "        \"\"\"Write files in target path into archive.\"\"\"",
            "        if isinstance(path, str):",
            "            path = pathlib.Path(path)",
            "        if not path.exists():",
            "            raise ValueError(\"specified path does not exist.\")",
            "        if path.is_dir() or path.is_file():",
            "            self._writeall(path, arcname)",
            "        else:",
            "            raise ValueError(\"specified path is not a directory or a file\")",
            "",
            "    def write(self, file: Union[pathlib.Path, str], arcname: Optional[str] = None):",
            "        \"\"\"Write single target file into archive.\"\"\"",
            "        if isinstance(file, str):",
            "            path = pathlib.Path(file)",
            "        elif isinstance(file, pathlib.Path):",
            "            path = file",
            "        else:",
            "            raise ValueError(\"Unsupported file type.\")",
            "        folder = self.header.initialize()",
            "        file_info = self._make_file_info(path, arcname, self.dereference)",
            "        self.header.files_info.files.append(file_info)",
            "        self.header.files_info.emptyfiles.append(file_info[\"emptystream\"])",
            "        self.files.append(file_info)",
            "        self.worker.archive(self.fp, self.files, folder, deref=self.dereference)",
            "",
            "    def writed(self, targets: Dict[str, IO[Any]]) -> None:",
            "        for target, input in targets.items():",
            "            self.writef(input, target)",
            "",
            "    def writef(self, bio: IO[Any], arcname: str):",
            "        if not check_archive_path(arcname):",
            "            raise ValueError(f\"Specified path is bad: {arcname}\")",
            "        return self._writef(bio, arcname)",
            "",
            "    def _writef(self, bio: IO[Any], arcname: str):",
            "        if isinstance(bio, io.BytesIO):",
            "            size = bio.getbuffer().nbytes",
            "        elif isinstance(bio, io.TextIOBase):",
            "            # First check whether is it Text?",
            "            raise ValueError(\"Unsupported file object type: please open file with Binary mode.\")",
            "        elif isinstance(bio, io.BufferedIOBase):",
            "            # come here when subtype of io.BufferedIOBase that don't have __sizeof__ (eg. pypy)",
            "            # alternative for `size = bio.__sizeof__()`",
            "            current = bio.tell()",
            "            bio.seek(0, os.SEEK_END)",
            "            last = bio.tell()",
            "            bio.seek(current, os.SEEK_SET)",
            "            size = last - current",
            "        else:",
            "            raise ValueError(\"Wrong argument passed for argument bio.\")",
            "        if size > 0:",
            "            folder = self.header.initialize()",
            "            file_info = self._make_file_info_from_name(bio, size, arcname)",
            "            self.header.files_info.files.append(file_info)",
            "            self.header.files_info.emptyfiles.append(file_info[\"emptystream\"])",
            "            self.files.append(file_info)",
            "            self.worker.archive(self.fp, self.files, folder, deref=False)",
            "        else:",
            "            file_info = self._make_file_info_from_name(bio, size, arcname)",
            "            self.header.files_info.files.append(file_info)",
            "            self.header.files_info.emptyfiles.append(file_info[\"emptystream\"])",
            "            self.files.append(file_info)",
            "",
            "    def writestr(self, data: Union[str, bytes, bytearray, memoryview], arcname: str):",
            "        if not check_archive_path(arcname):",
            "            raise ValueError(f\"Specified path is bad: {arcname}\")",
            "        return self._writestr(data, arcname)",
            "",
            "    def _writestr(self, data: Union[str, bytes, bytearray, memoryview], arcname: str):",
            "        if not isinstance(arcname, str):",
            "            raise ValueError(\"Unsupported arcname\")",
            "        if isinstance(data, str):",
            "            self._writef(io.BytesIO(data.encode(\"UTF-8\")), arcname)",
            "        elif isinstance(data, bytes) or isinstance(data, bytearray) or isinstance(data, memoryview):",
            "            self._writef(io.BytesIO(bytes(data)), arcname)",
            "        else:",
            "            raise ValueError(\"Unsupported data type.\")",
            "",
            "    def close(self):",
            "        \"\"\"Flush all the data into archive and close it.",
            "        When close py7zr start reading target and writing actual archive file.",
            "        \"\"\"",
            "        if \"w\" in self.mode:",
            "            self._write_flush()",
            "        if \"a\" in self.mode:",
            "            self._write_flush()",
            "        if \"r\" in self.mode:",
            "            if self.reporterd is not None:",
            "                self.q.put_nowait(None)",
            "                self.reporterd.join(1)",
            "                if self.reporterd.is_alive():",
            "                    raise InternalError(\"Progress report thread terminate error.\")",
            "                self.reporterd = None",
            "        self._fpclose()",
            "        self._var_release()",
            "",
            "    def reset(self) -> None:",
            "        \"\"\"",
            "        When read mode, it reset file pointer, decompress worker and decompressor",
            "        \"\"\"",
            "        if self.mode == \"r\":",
            "            self.fp.seek(self.afterheader)",
            "            self.worker = Worker(self.files, self.afterheader, self.header, self.mp)",
            "            if self.header.main_streams is not None and self.header.main_streams.unpackinfo.numfolders > 0:",
            "                for i, folder in enumerate(self.header.main_streams.unpackinfo.folders):",
            "                    folder.decompressor = None",
            "",
            "    def test(self) -> Optional[bool]:",
            "        self.fp.seek(self.afterheader)",
            "        self.worker = Worker(self.files, self.afterheader, self.header, self.mp)",
            "        crcs: Optional[List[int]] = self.header.main_streams.packinfo.crcs",
            "        if crcs is None or len(crcs) == 0:",
            "            return None",
            "        packpos = self.afterheader + self.header.main_streams.packinfo.packpos",
            "        packsizes = self.header.main_streams.packinfo.packsizes",
            "        digestdefined = self.header.main_streams.packinfo.digestdefined",
            "        j = 0",
            "        for i, d in enumerate(digestdefined):",
            "            if d:",
            "                if self._read_digest(packpos, packsizes[i]) != crcs[j]:",
            "                    return False",
            "                j += 1",
            "            packpos += packsizes[i]",
            "        return True",
            "",
            "    def testzip(self) -> Optional[str]:",
            "        self.fp.seek(self.afterheader)",
            "        self.worker = Worker(self.files, self.afterheader, self.header, self.mp)",
            "        for f in self.files:",
            "            self.worker.register_filelike(f.id, None)",
            "        try:",
            "            self.worker.extract(",
            "                self.fp, None, parallel=(not self.password_protected), skip_notarget=False",
            "            )  # TODO: print progress",
            "        except CrcError as crce:",
            "            return crce.args[2]",
            "        else:",
            "            return None",
            "",
            "",
            "# --------------------",
            "# exported functions",
            "# --------------------",
            "def is_7zfile(file: Union[BinaryIO, str, pathlib.Path]) -> bool:",
            "    \"\"\"Quickly see if a file is a 7Z file by checking the magic number.",
            "    The file argument may be a filename or file-like object too.",
            "    \"\"\"",
            "    result = False",
            "    try:",
            "        if (isinstance(file, BinaryIO) or isinstance(file, io.BufferedReader) or isinstance(file, io.IOBase)) and hasattr(",
            "            file, \"read\"",
            "        ):",
            "            result = SevenZipFile._check_7zfile(file)",
            "        elif isinstance(file, str):",
            "            with open(file, \"rb\") as fp:",
            "                result = SevenZipFile._check_7zfile(fp)",
            "        elif isinstance(file, pathlib.Path) or isinstance(file, pathlib.PosixPath) or isinstance(file, pathlib.WindowsPath):",
            "            with file.open(mode=\"rb\") as fp:  # noqa",
            "                result = SevenZipFile._check_7zfile(fp)",
            "        else:",
            "            raise TypeError(\"invalid type: file should be str, pathlib.Path or BinaryIO, but {}\".format(type(file)))",
            "    except OSError:",
            "        pass",
            "    return result",
            "",
            "",
            "def unpack_7zarchive(archive, path, extra=None):",
            "    \"\"\"",
            "    Function for registering with shutil.register_unpack_format().",
            "    \"\"\"",
            "    arc = SevenZipFile(archive)",
            "    arc.extractall(path)",
            "    arc.close()",
            "",
            "",
            "def pack_7zarchive(base_name, base_dir, owner=None, group=None, dry_run=None, logger=None):",
            "    \"\"\"",
            "    Function for registering with shutil.register_archive_format().",
            "    \"\"\"",
            "    target_name = \"{}.7z\".format(base_name)",
            "    with SevenZipFile(target_name, mode=\"w\") as archive:",
            "        archive.writeall(path=base_dir)",
            "    return target_name",
            "",
            "",
            "class Worker:",
            "    \"\"\"",
            "    Extract worker class to invoke handler.",
            "    \"\"\"",
            "",
            "    def __init__(self, files, src_start: int, header, mp=False) -> None:",
            "        self.target_filepath: Dict[int, Union[MemIO, pathlib.Path, None]] = {}",
            "        self.files = files",
            "        self.src_start = src_start",
            "        self.header = header",
            "        self.current_file_index = len(self.files)",
            "        self.last_file_index = len(self.files)",
            "        if mp:",
            "            self.concurrent: Union[Type[Thread], Type[Process]] = Process",
            "        else:",
            "            self.concurrent = Thread",
            "",
            "    def extract(self, fp: BinaryIO, path: Optional[pathlib.Path], parallel: bool, skip_notarget=True, q=None) -> None:",
            "        \"\"\"Extract worker method to handle 7zip folder and decompress each files.\"\"\"",
            "        if hasattr(self.header, \"main_streams\") and self.header.main_streams is not None:",
            "            src_end = self.src_start + self.header.main_streams.packinfo.packpositions[-1]",
            "            numfolders = self.header.main_streams.unpackinfo.numfolders",
            "            if numfolders == 1:",
            "                self.extract_single(",
            "                    fp,",
            "                    self.files,",
            "                    path,",
            "                    self.src_start,",
            "                    src_end,",
            "                    q,",
            "                    skip_notarget=skip_notarget,",
            "                )",
            "            else:",
            "                folders = self.header.main_streams.unpackinfo.folders",
            "                positions = self.header.main_streams.packinfo.packpositions",
            "                empty_files = [f for f in self.files if f.emptystream]",
            "                if not parallel:",
            "                    self.extract_single(fp, empty_files, path, 0, 0, q)",
            "                    for i in range(numfolders):",
            "                        if skip_notarget:",
            "                            if not any([self.target_filepath.get(f.id, None) for f in folders[i].files]):",
            "                                continue",
            "                        self.extract_single(",
            "                            fp,",
            "                            folders[i].files,",
            "                            path,",
            "                            self.src_start + positions[i],",
            "                            self.src_start + positions[i + 1],",
            "                            q,",
            "                            skip_notarget=skip_notarget,",
            "                        )",
            "                else:",
            "                    if getattr(fp, \"name\", None) is None:",
            "                        raise InternalError(\"Caught unknown variable status error\")",
            "                    filename: str = getattr(fp, \"name\", \"\")  # do not become \"\" but it is for type check.",
            "                    self.extract_single(open(filename, \"rb\"), empty_files, path, 0, 0, q)",
            "                    concurrent_tasks = []",
            "                    exc_q: queue.Queue = queue.Queue()",
            "                    for i in range(numfolders):",
            "                        if skip_notarget:",
            "                            if not any([self.target_filepath.get(f.id, None) for f in folders[i].files]):",
            "                                continue",
            "                        p = self.concurrent(",
            "                            target=self.extract_single,",
            "                            args=(",
            "                                filename,",
            "                                folders[i].files,",
            "                                path,",
            "                                self.src_start + positions[i],",
            "                                self.src_start + positions[i + 1],",
            "                                q,",
            "                                exc_q,",
            "                                skip_notarget,",
            "                            ),",
            "                        )",
            "                        p.start()",
            "                        concurrent_tasks.append(p)",
            "                    for p in concurrent_tasks:",
            "                        p.join()",
            "                    if exc_q.empty():",
            "                        pass",
            "                    else:",
            "                        exc_info = exc_q.get()",
            "                        raise exc_info[1].with_traceback(exc_info[2])",
            "        else:",
            "            empty_files = [f for f in self.files if f.emptystream]",
            "            self.extract_single(fp, empty_files, path, 0, 0, q)",
            "",
            "    def extract_single(",
            "        self,",
            "        fp: Union[BinaryIO, str],",
            "        files,",
            "        path,",
            "        src_start: int,",
            "        src_end: int,",
            "        q: Optional[queue.Queue],",
            "        exc_q: Optional[queue.Queue] = None,",
            "        skip_notarget=True,",
            "    ) -> None:",
            "        \"\"\"",
            "        Single thread extractor that takes file lists in single 7zip folder.",
            "        \"\"\"",
            "        if files is None:",
            "            return",
            "        try:",
            "            if isinstance(fp, str):",
            "                fp = open(fp, \"rb\")",
            "            fp.seek(src_start)",
            "            self._extract_single(fp, files, path, src_end, q, skip_notarget)",
            "        except Exception as e:",
            "            if exc_q is None:",
            "                raise e",
            "            else:",
            "                exc_tuple = sys.exc_info()",
            "                exc_q.put(exc_tuple)",
            "",
            "    def _extract_single(",
            "        self,",
            "        fp: BinaryIO,",
            "        files,",
            "        path,",
            "        src_end: int,",
            "        q: Optional[queue.Queue],",
            "        skip_notarget=True,",
            "    ) -> None:",
            "        \"\"\"",
            "        Single thread extractor that takes file lists in single 7zip folder.",
            "        this may raise exception.",
            "        \"\"\"",
            "        just_check: List[ArchiveFile] = []",
            "        for f in files:",
            "            if q is not None:",
            "                q.put(",
            "                    (",
            "                        \"s\",",
            "                        str(f.filename),",
            "                        str(f.compressed) if f.compressed is not None else \"0\",",
            "                    )",
            "                )",
            "            fileish = self.target_filepath.get(f.id, None)",
            "            if fileish is None:",
            "                if not f.emptystream:",
            "                    just_check.append(f)",
            "            else:",
            "                # delayed execution of crc check.",
            "                self._check(fp, just_check, src_end)",
            "                just_check = []",
            "                fileish.parent.mkdir(parents=True, exist_ok=True)",
            "                if not f.emptystream:",
            "                    if f.is_junction and not isinstance(fileish, MemIO) and sys.platform == \"win32\":",
            "                        with io.BytesIO() as ofp:",
            "                            self.decompress(fp, f.folder, ofp, f.uncompressed, f.compressed, src_end)",
            "                            dst: str = ofp.read().decode(\"utf-8\")",
            "                            if is_target_path_valid(path, fileish.parent.joinpath(dst)):",
            "                                # fileish.unlink(missing_ok=True) > py3.7",
            "                                if fileish.exists():",
            "                                    fileish.unlink()",
            "                                if sys.platform == \"win32\":  # hint for mypy",
            "                                    _winapi.CreateJunction(str(fileish), dst)  # noqa",
            "                            else:",
            "                                raise Bad7zFile(\"Junction point out of target directory.\")",
            "                    elif f.is_symlink and not isinstance(fileish, MemIO):",
            "                        with io.BytesIO() as omfp:",
            "                            self.decompress(fp, f.folder, omfp, f.uncompressed, f.compressed, src_end)",
            "                            omfp.seek(0)",
            "                            dst = omfp.read().decode(\"utf-8\")",
            "                            # check sym_target points inside an archive target?",
            "                            if is_target_path_valid(path, fileish.parent.joinpath(dst)):",
            "                                sym_target = pathlib.Path(dst)",
            "                                # fileish.unlink(missing_ok=True) > py3.7",
            "                                if fileish.exists():",
            "                                    fileish.unlink()",
            "                                fileish.symlink_to(sym_target)",
            "                            else:",
            "                                raise Bad7zFile(\"Symlink point out of target directory.\")",
            "                    else:",
            "                        with fileish.open(mode=\"wb\") as obfp:",
            "                            crc32 = self.decompress(fp, f.folder, obfp, f.uncompressed, f.compressed, src_end)",
            "                            obfp.seek(0)",
            "                            if f.crc32 is not None and crc32 != f.crc32:",
            "                                raise CrcError(crc32, f.crc32, f.filename)",
            "                else:",
            "                    # just create empty file",
            "                    if not isinstance(fileish, MemIO):",
            "                        fileish.touch()",
            "                    else:",
            "                        with fileish.open() as ofp:",
            "                            pass",
            "            if q is not None:",
            "                q.put((\"e\", str(f.filename), str(f.uncompressed)))",
            "        if not skip_notarget:",
            "            # delayed execution of crc check.",
            "            self._check(fp, just_check, src_end)",
            "",
            "    def _check(self, fp, check_target, src_end):",
            "        \"\"\"",
            "        delayed execution of crc check.",
            "        \"\"\"",
            "        for f in check_target:",
            "            with NullIO() as ofp:",
            "                crc32 = self.decompress(fp, f.folder, ofp, f.uncompressed, f.compressed, src_end)",
            "            if f.crc32 is not None and crc32 != f.crc32:",
            "                raise CrcError(crc32, f.crc32, f.filename)",
            "",
            "    def decompress(",
            "        self,",
            "        fp: BinaryIO,",
            "        folder,",
            "        fq: IO[Any],",
            "        size: int,",
            "        compressed_size: Optional[int],",
            "        src_end: int,",
            "    ) -> int:",
            "        \"\"\"",
            "        decompressor wrapper called from extract method.",
            "",
            "        :parameter fp: archive source file pointer",
            "        :parameter folder: Folder object that have decompressor object.",
            "        :parameter fq: output file pathlib.Path",
            "        :parameter size: uncompressed size of target file.",
            "        :parameter compressed_size: compressed size of target file.",
            "        :parameter src_end: end position of the folder",
            "",
            "        :returns None",
            "",
            "        \"\"\"",
            "        assert folder is not None",
            "        out_remaining = size",
            "        max_block_size = get_memory_limit()",
            "        crc32 = 0",
            "        decompressor = folder.get_decompressor(compressed_size)",
            "        while out_remaining > 0:",
            "            tmp = decompressor.decompress(fp, min(out_remaining, max_block_size))",
            "            if len(tmp) > 0:",
            "                out_remaining -= len(tmp)",
            "                fq.write(tmp)",
            "                crc32 = calculate_crc32(tmp, crc32)",
            "            if out_remaining <= 0:",
            "                break",
            "        if fp.tell() >= src_end:",
            "            if decompressor.crc is not None and not decompressor.check_crc():",
            "                raise CrcError(decompressor.crc, decompressor.digest, None)",
            "        return crc32",
            "",
            "    def _find_link_target(self, target):",
            "        \"\"\"",
            "        Find the target member of a symlink or hardlink member in the archive.",
            "        \"\"\"",
            "        targetname: str = target.as_posix()",
            "        linkname = readlink(targetname)",
            "        # Check windows full path symlinks",
            "        if linkname.startswith(\"\\\\\\\\?\\\\\"):",
            "            linkname = linkname[4:]",
            "        # normalize as posix style",
            "        linkname: str = pathlib.Path(linkname).as_posix()",
            "        member = None",
            "        for j in range(len(self.files)):",
            "            if linkname == self.files[j].origin.as_posix():",
            "                # FIXME: when API user specify arcname, it will break",
            "                member = os.path.relpath(linkname, os.path.dirname(targetname))",
            "                break",
            "        if member is None:",
            "            member = linkname",
            "        return member",
            "",
            "    def _after_write(self, insize, foutsize, crc):",
            "        self.header.main_streams.substreamsinfo.digestsdefined.append(True)",
            "        self.header.main_streams.substreamsinfo.digests.append(crc)",
            "        if self.header.main_streams.substreamsinfo.unpacksizes is None:",
            "            self.header.main_streams.substreamsinfo.unpacksizes = [insize]",
            "        else:",
            "            self.header.main_streams.substreamsinfo.unpacksizes.append(insize)",
            "        if self.header.main_streams.substreamsinfo.num_unpackstreams_folders is None:",
            "            self.header.main_streams.substreamsinfo.num_unpackstreams_folders = [1]",
            "        else:",
            "            self.header.main_streams.substreamsinfo.num_unpackstreams_folders[-1] += 1",
            "        return foutsize, crc",
            "",
            "    def write(self, fp: BinaryIO, f, assym, folder):",
            "        compressor = folder.get_compressor()",
            "        if assym:",
            "            link_target: str = self._find_link_target(f.origin)",
            "            tgt: bytes = link_target.encode(\"utf-8\")",
            "            fd = io.BytesIO(tgt)",
            "            insize, foutsize, crc = compressor.compress(fd, fp)",
            "            fd.close()",
            "        else:",
            "            with f.origin.open(mode=\"rb\") as fd:",
            "                insize, foutsize, crc = compressor.compress(fd, fp)",
            "        return self._after_write(insize, foutsize, crc)",
            "",
            "    def writestr(self, fp: BinaryIO, f, folder):",
            "        compressor = folder.get_compressor()",
            "        insize, foutsize, crc = compressor.compress(f.data(), fp)",
            "        return self._after_write(insize, foutsize, crc)",
            "",
            "    def flush_archive(self, fp, folder):",
            "        compressor = folder.get_compressor()",
            "        foutsize = compressor.flush(fp)",
            "        if len(self.files) > 0:",
            "            if \"maxsize\" in self.header.files_info.files[self.last_file_index]:",
            "                self.header.files_info.files[self.last_file_index][\"maxsize\"] += foutsize",
            "            else:",
            "                self.header.files_info.files[self.last_file_index][\"maxsize\"] = foutsize",
            "        # Update size data in header",
            "        self.header.main_streams.packinfo.numstreams += 1",
            "        if self.header.main_streams.packinfo.enable_digests:",
            "            self.header.main_streams.packinfo.crcs.append(compressor.digest)",
            "            self.header.main_streams.packinfo.digestdefined.append(True)",
            "        self.header.main_streams.packinfo.packsizes.append(compressor.packsize)",
            "        folder.unpacksizes = compressor.unpacksizes",
            "",
            "    def archive(self, fp: BinaryIO, files, folder, deref=False):",
            "        \"\"\"Run archive task for specified 7zip folder.\"\"\"",
            "        f = files[self.current_file_index]",
            "        if f.has_strdata():",
            "            foutsize, crc = self.writestr(fp, f, folder)",
            "            self.header.files_info.files[self.current_file_index][\"maxsize\"] = foutsize",
            "            self.header.files_info.files[self.current_file_index][\"digest\"] = crc",
            "            self.last_file_index = self.current_file_index",
            "        elif (f.is_symlink and not deref) or not f.emptystream:",
            "            foutsize, crc = self.write(fp, f, (f.is_symlink and not deref), folder)",
            "            self.header.files_info.files[self.current_file_index][\"maxsize\"] = foutsize",
            "            self.header.files_info.files[self.current_file_index][\"digest\"] = crc",
            "            self.last_file_index = self.current_file_index",
            "        self.current_file_index += 1",
            "",
            "    def register_filelike(self, id: int, fileish: Union[MemIO, pathlib.Path, None]) -> None:",
            "        \"\"\"register file-ish to worker.\"\"\"",
            "        self.target_filepath[id] = fileish",
            "",
            "    def close(self):",
            "        del self.header",
            "        del self.files",
            "        del self.concurrent"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "-1",
            "-1",
            "0",
            "2",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "55": [],
            "58": [],
            "570": [
                "SevenZipFile",
                "_extract"
            ],
            "571": [
                "SevenZipFile",
                "_extract"
            ],
            "572": [
                "SevenZipFile",
                "_extract"
            ],
            "573": [
                "SevenZipFile",
                "_extract"
            ],
            "574": [
                "SevenZipFile",
                "_extract"
            ],
            "575": [
                "SevenZipFile",
                "_extract"
            ],
            "576": [
                "SevenZipFile",
                "_extract"
            ],
            "577": [
                "SevenZipFile",
                "_extract"
            ],
            "578": [
                "SevenZipFile",
                "_extract"
            ],
            "579": [
                "SevenZipFile",
                "_extract"
            ],
            "580": [
                "SevenZipFile",
                "_extract"
            ],
            "582": [
                "SevenZipFile",
                "_extract"
            ],
            "583": [
                "SevenZipFile",
                "_extract"
            ],
            "584": [
                "SevenZipFile",
                "_extract"
            ],
            "585": [
                "SevenZipFile",
                "_extract"
            ],
            "586": [
                "SevenZipFile",
                "_extract"
            ],
            "587": [
                "SevenZipFile",
                "_extract"
            ],
            "588": [
                "SevenZipFile",
                "_extract"
            ],
            "589": [
                "SevenZipFile",
                "_extract"
            ],
            "590": [
                "SevenZipFile",
                "_extract"
            ],
            "591": [
                "SevenZipFile",
                "_extract"
            ],
            "592": [
                "SevenZipFile",
                "_extract"
            ],
            "593": [
                "SevenZipFile",
                "_extract"
            ],
            "594": [
                "SevenZipFile",
                "_extract"
            ],
            "595": [
                "SevenZipFile",
                "_extract"
            ],
            "596": [
                "SevenZipFile",
                "_extract"
            ],
            "597": [
                "SevenZipFile",
                "_extract"
            ],
            "1075": [
                "SevenZipFile",
                "writestr"
            ],
            "1077": [
                "SevenZipFile",
                "writestr"
            ],
            "1134": [
                "SevenZipFile",
                "testzip"
            ],
            "1203": [
                "Worker",
                "extract"
            ],
            "1222": [
                "Worker",
                "extract"
            ],
            "1239": [
                "Worker",
                "extract"
            ],
            "1269": [
                "Worker",
                "extract"
            ],
            "1290": [
                "Worker",
                "extract_single"
            ],
            "1334": [
                "Worker",
                "_extract_single"
            ],
            "1335": [
                "Worker",
                "_extract_single"
            ],
            "1336": [
                "Worker",
                "_extract_single"
            ],
            "1337": [
                "Worker",
                "_extract_single"
            ],
            "1338": [
                "Worker",
                "_extract_single"
            ],
            "1343": [
                "Worker",
                "_extract_single"
            ],
            "1344": [
                "Worker",
                "_extract_single"
            ],
            "1345": [
                "Worker",
                "_extract_single"
            ],
            "1346": [
                "Worker",
                "_extract_single"
            ],
            "1347": [
                "Worker",
                "_extract_single"
            ]
        },
        "addLocation": []
    }
}