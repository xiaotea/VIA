{
    "airflow/utils/log/file_task_handler.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " from typing import TYPE_CHECKING, Optional"
            },
            "1": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " import httpx"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+from itsdangerous import TimedJSONWebSignatureSerializer"
            },
            "4": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " from airflow.configuration import AirflowConfigException, conf"
            },
            "6": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from airflow.utils.helpers import parse_template_string"
            },
            "7": {
                "beforePatchRowNumber": 172,
                "afterPatchRowNumber": 173,
                "PatchRowcode": "                 except (AirflowConfigException, ValueError):"
            },
            "8": {
                "beforePatchRowNumber": 173,
                "afterPatchRowNumber": 174,
                "PatchRowcode": "                     pass"
            },
            "9": {
                "beforePatchRowNumber": 174,
                "afterPatchRowNumber": 175,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 175,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                response = httpx.get(url, timeout=timeout)"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 176,
                "PatchRowcode": "+                signer = TimedJSONWebSignatureSerializer("
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 177,
                "PatchRowcode": "+                    secret_key=conf.get('webserver', 'secret_key'),"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 178,
                "PatchRowcode": "+                    algorithm_name='HS512',"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 179,
                "PatchRowcode": "+                    expires_in=conf.getint('webserver', 'log_request_clock_grace', fallback=30),"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 180,
                "PatchRowcode": "+                )"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 181,
                "PatchRowcode": "+"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 182,
                "PatchRowcode": "+                response = httpx.get(url, timeout=timeout, headers={'Authorization': signer.dumps({})})"
            },
            "18": {
                "beforePatchRowNumber": 176,
                "afterPatchRowNumber": 183,
                "PatchRowcode": "                 response.encoding = \"utf-8\""
            },
            "19": {
                "beforePatchRowNumber": 177,
                "afterPatchRowNumber": 184,
                "PatchRowcode": " "
            },
            "20": {
                "beforePatchRowNumber": 178,
                "afterPatchRowNumber": 185,
                "PatchRowcode": "                 # Check if the resource was properly fetched"
            }
        },
        "frontPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"File logging handler for tasks.\"\"\"",
            "import logging",
            "import os",
            "from pathlib import Path",
            "from typing import TYPE_CHECKING, Optional",
            "",
            "import httpx",
            "",
            "from airflow.configuration import AirflowConfigException, conf",
            "from airflow.utils.helpers import parse_template_string",
            "",
            "if TYPE_CHECKING:",
            "    from airflow.models import TaskInstance",
            "",
            "",
            "class FileTaskHandler(logging.Handler):",
            "    \"\"\"",
            "    FileTaskHandler is a python log handler that handles and reads",
            "    task instance logs. It creates and delegates log handling",
            "    to `logging.FileHandler` after receiving task instance context.",
            "    It reads logs from task instance's host machine.",
            "",
            "    :param base_log_folder: Base log folder to place logs.",
            "    :param filename_template: template filename string",
            "    \"\"\"",
            "",
            "    def __init__(self, base_log_folder: str, filename_template: str):",
            "        super().__init__()",
            "        self.handler = None  # type: Optional[logging.FileHandler]",
            "        self.local_base = base_log_folder",
            "        self.filename_template, self.filename_jinja_template = parse_template_string(filename_template)",
            "",
            "    def set_context(self, ti: \"TaskInstance\"):",
            "        \"\"\"",
            "        Provide task_instance context to airflow task handler.",
            "",
            "        :param ti: task instance object",
            "        \"\"\"",
            "        local_loc = self._init_file(ti)",
            "        self.handler = logging.FileHandler(local_loc, encoding='utf-8')",
            "        if self.formatter:",
            "            self.handler.setFormatter(self.formatter)",
            "        self.handler.setLevel(self.level)",
            "",
            "    def emit(self, record):",
            "        if self.handler:",
            "            self.handler.emit(record)",
            "",
            "    def flush(self):",
            "        if self.handler:",
            "            self.handler.flush()",
            "",
            "    def close(self):",
            "        if self.handler:",
            "            self.handler.close()",
            "",
            "    def _render_filename(self, ti, try_number):",
            "        if self.filename_jinja_template:",
            "            if hasattr(ti, 'task'):",
            "                jinja_context = ti.get_template_context()",
            "                jinja_context['try_number'] = try_number",
            "            else:",
            "                jinja_context = {",
            "                    'ti': ti,",
            "                    'ts': ti.execution_date.isoformat(),",
            "                    'try_number': try_number,",
            "                }",
            "            return self.filename_jinja_template.render(**jinja_context)",
            "",
            "        return self.filename_template.format(",
            "            dag_id=ti.dag_id,",
            "            task_id=ti.task_id,",
            "            execution_date=ti.execution_date.isoformat(),",
            "            try_number=try_number,",
            "        )",
            "",
            "    def _read_grouped_logs(self):",
            "        return False",
            "",
            "    def _read(self, ti, try_number, metadata=None):",
            "        \"\"\"",
            "        Template method that contains custom logic of reading",
            "        logs given the try_number.",
            "",
            "        :param ti: task instance record",
            "        :param try_number: current try_number to read log from",
            "        :param metadata: log metadata,",
            "                         can be used for steaming log reading and auto-tailing.",
            "        :return: log message as a string and metadata.",
            "        \"\"\"",
            "        # Task instance here might be different from task instance when",
            "        # initializing the handler. Thus explicitly getting log location",
            "        # is needed to get correct log path.",
            "        log_relative_path = self._render_filename(ti, try_number)",
            "        location = os.path.join(self.local_base, log_relative_path)",
            "",
            "        log = \"\"",
            "",
            "        if os.path.exists(location):",
            "            try:",
            "                with open(location) as file:",
            "                    log += f\"*** Reading local file: {location}\\n\"",
            "                    log += \"\".join(file.readlines())",
            "            except Exception as e:",
            "                log = f\"*** Failed to load local log file: {location}\\n\"",
            "                log += f\"*** {str(e)}\\n\"",
            "        elif conf.get('core', 'executor') == 'KubernetesExecutor':",
            "            try:",
            "                from airflow.kubernetes.kube_client import get_kube_client",
            "",
            "                kube_client = get_kube_client()",
            "",
            "                if len(ti.hostname) >= 63:",
            "                    # Kubernetes takes the pod name and truncates it for the hostname. This truncated hostname",
            "                    # is returned for the fqdn to comply with the 63 character limit imposed by DNS standards",
            "                    # on any label of a FQDN.",
            "                    pod_list = kube_client.list_namespaced_pod(conf.get('kubernetes', 'namespace'))",
            "                    matches = [",
            "                        pod.metadata.name",
            "                        for pod in pod_list.items",
            "                        if pod.metadata.name.startswith(ti.hostname)",
            "                    ]",
            "                    if len(matches) == 1:",
            "                        if len(matches[0]) > len(ti.hostname):",
            "                            ti.hostname = matches[0]",
            "",
            "                log += '*** Trying to get logs (last 100 lines) from worker pod {} ***\\n\\n'.format(",
            "                    ti.hostname",
            "                )",
            "",
            "                res = kube_client.read_namespaced_pod_log(",
            "                    name=ti.hostname,",
            "                    namespace=conf.get('kubernetes', 'namespace'),",
            "                    container='base',",
            "                    follow=False,",
            "                    tail_lines=100,",
            "                    _preload_content=False,",
            "                )",
            "",
            "                for line in res:",
            "                    log += line.decode()",
            "",
            "            except Exception as f:",
            "                log += f'*** Unable to fetch logs from worker pod {ti.hostname} ***\\n{str(f)}\\n\\n'",
            "        else:",
            "            url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(",
            "                ti=ti, worker_log_server_port=conf.get('celery', 'WORKER_LOG_SERVER_PORT')",
            "            )",
            "            log += f\"*** Log file does not exist: {location}\\n\"",
            "            log += f\"*** Fetching from: {url}\\n\"",
            "            try:",
            "                timeout = None  # No timeout",
            "                try:",
            "                    timeout = conf.getint('webserver', 'log_fetch_timeout_sec')",
            "                except (AirflowConfigException, ValueError):",
            "                    pass",
            "",
            "                response = httpx.get(url, timeout=timeout)",
            "                response.encoding = \"utf-8\"",
            "",
            "                # Check if the resource was properly fetched",
            "                response.raise_for_status()",
            "",
            "                log += '\\n' + response.text",
            "            except Exception as e:",
            "                log += f\"*** Failed to fetch log file from worker. {str(e)}\\n\"",
            "",
            "        return log, {'end_of_log': True}",
            "",
            "    def read(self, task_instance, try_number=None, metadata=None):",
            "        \"\"\"",
            "        Read logs of given task instance from local machine.",
            "",
            "        :param task_instance: task instance object",
            "        :param try_number: task instance try_number to read logs from. If None",
            "                           it returns all logs separated by try_number",
            "        :param metadata: log metadata,",
            "                         can be used for steaming log reading and auto-tailing.",
            "        :return: a list of listed tuples which order log string by host",
            "        \"\"\"",
            "        # Task instance increments its try number when it starts to run.",
            "        # So the log for a particular task try will only show up when",
            "        # try number gets incremented in DB, i.e logs produced the time",
            "        # after cli run and before try_number + 1 in DB will not be displayed.",
            "",
            "        if try_number is None:",
            "            next_try = task_instance.next_try_number",
            "            try_numbers = list(range(1, next_try))",
            "        elif try_number < 1:",
            "            logs = [",
            "                [('default_host', f'Error fetching the logs. Try number {try_number} is invalid.')],",
            "            ]",
            "            return logs, [{'end_of_log': True}]",
            "        else:",
            "            try_numbers = [try_number]",
            "",
            "        logs = [''] * len(try_numbers)",
            "        metadata_array = [{}] * len(try_numbers)",
            "        for i, try_number_element in enumerate(try_numbers):",
            "            log, metadata = self._read(task_instance, try_number_element, metadata)",
            "            # es_task_handler return logs grouped by host. wrap other handler returning log string",
            "            # with default/ empty host so that UI can render the response in the same way",
            "            logs[i] = log if self._read_grouped_logs() else [(task_instance.hostname, log)]",
            "            metadata_array[i] = metadata",
            "",
            "        return logs, metadata_array",
            "",
            "    def _init_file(self, ti):",
            "        \"\"\"",
            "        Create log directory and give it correct permissions.",
            "",
            "        :param ti: task instance object",
            "        :return: relative log path of the given task instance",
            "        \"\"\"",
            "        # To handle log writing when tasks are impersonated, the log files need to",
            "        # be writable by the user that runs the Airflow command and the user",
            "        # that is impersonated. This is mainly to handle corner cases with the",
            "        # SubDagOperator. When the SubDagOperator is run, all of the operators",
            "        # run under the impersonated user and create appropriate log files",
            "        # as the impersonated user. However, if the user manually runs tasks",
            "        # of the SubDagOperator through the UI, then the log files are created",
            "        # by the user that runs the Airflow command. For example, the Airflow",
            "        # run command may be run by the `airflow_sudoable` user, but the Airflow",
            "        # tasks may be run by the `airflow` user. If the log files are not",
            "        # writable by both users, then it's possible that re-running a task",
            "        # via the UI (or vice versa) results in a permission error as the task",
            "        # tries to write to a log file created by the other user.",
            "        relative_path = self._render_filename(ti, ti.try_number)",
            "        full_path = os.path.join(self.local_base, relative_path)",
            "        directory = os.path.dirname(full_path)",
            "        # Create the log file and give it group writable permissions",
            "        # TODO(aoen): Make log dirs and logs globally readable for now since the SubDag",
            "        # operator is not compatible with impersonation (e.g. if a Celery executor is used",
            "        # for a SubDag operator and the SubDag operator has a different owner than the",
            "        # parent DAG)",
            "        Path(directory).mkdir(mode=0o777, parents=True, exist_ok=True)",
            "",
            "        if not os.path.exists(full_path):",
            "            open(full_path, \"a\").close()",
            "            # TODO: Investigate using 444 instead of 666.",
            "            try:",
            "                os.chmod(full_path, 0o666)",
            "            except OSError:",
            "                logging.warning(\"OSError while change ownership of the log file\")",
            "",
            "        return full_path"
        ],
        "afterPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"File logging handler for tasks.\"\"\"",
            "import logging",
            "import os",
            "from pathlib import Path",
            "from typing import TYPE_CHECKING, Optional",
            "",
            "import httpx",
            "from itsdangerous import TimedJSONWebSignatureSerializer",
            "",
            "from airflow.configuration import AirflowConfigException, conf",
            "from airflow.utils.helpers import parse_template_string",
            "",
            "if TYPE_CHECKING:",
            "    from airflow.models import TaskInstance",
            "",
            "",
            "class FileTaskHandler(logging.Handler):",
            "    \"\"\"",
            "    FileTaskHandler is a python log handler that handles and reads",
            "    task instance logs. It creates and delegates log handling",
            "    to `logging.FileHandler` after receiving task instance context.",
            "    It reads logs from task instance's host machine.",
            "",
            "    :param base_log_folder: Base log folder to place logs.",
            "    :param filename_template: template filename string",
            "    \"\"\"",
            "",
            "    def __init__(self, base_log_folder: str, filename_template: str):",
            "        super().__init__()",
            "        self.handler = None  # type: Optional[logging.FileHandler]",
            "        self.local_base = base_log_folder",
            "        self.filename_template, self.filename_jinja_template = parse_template_string(filename_template)",
            "",
            "    def set_context(self, ti: \"TaskInstance\"):",
            "        \"\"\"",
            "        Provide task_instance context to airflow task handler.",
            "",
            "        :param ti: task instance object",
            "        \"\"\"",
            "        local_loc = self._init_file(ti)",
            "        self.handler = logging.FileHandler(local_loc, encoding='utf-8')",
            "        if self.formatter:",
            "            self.handler.setFormatter(self.formatter)",
            "        self.handler.setLevel(self.level)",
            "",
            "    def emit(self, record):",
            "        if self.handler:",
            "            self.handler.emit(record)",
            "",
            "    def flush(self):",
            "        if self.handler:",
            "            self.handler.flush()",
            "",
            "    def close(self):",
            "        if self.handler:",
            "            self.handler.close()",
            "",
            "    def _render_filename(self, ti, try_number):",
            "        if self.filename_jinja_template:",
            "            if hasattr(ti, 'task'):",
            "                jinja_context = ti.get_template_context()",
            "                jinja_context['try_number'] = try_number",
            "            else:",
            "                jinja_context = {",
            "                    'ti': ti,",
            "                    'ts': ti.execution_date.isoformat(),",
            "                    'try_number': try_number,",
            "                }",
            "            return self.filename_jinja_template.render(**jinja_context)",
            "",
            "        return self.filename_template.format(",
            "            dag_id=ti.dag_id,",
            "            task_id=ti.task_id,",
            "            execution_date=ti.execution_date.isoformat(),",
            "            try_number=try_number,",
            "        )",
            "",
            "    def _read_grouped_logs(self):",
            "        return False",
            "",
            "    def _read(self, ti, try_number, metadata=None):",
            "        \"\"\"",
            "        Template method that contains custom logic of reading",
            "        logs given the try_number.",
            "",
            "        :param ti: task instance record",
            "        :param try_number: current try_number to read log from",
            "        :param metadata: log metadata,",
            "                         can be used for steaming log reading and auto-tailing.",
            "        :return: log message as a string and metadata.",
            "        \"\"\"",
            "        # Task instance here might be different from task instance when",
            "        # initializing the handler. Thus explicitly getting log location",
            "        # is needed to get correct log path.",
            "        log_relative_path = self._render_filename(ti, try_number)",
            "        location = os.path.join(self.local_base, log_relative_path)",
            "",
            "        log = \"\"",
            "",
            "        if os.path.exists(location):",
            "            try:",
            "                with open(location) as file:",
            "                    log += f\"*** Reading local file: {location}\\n\"",
            "                    log += \"\".join(file.readlines())",
            "            except Exception as e:",
            "                log = f\"*** Failed to load local log file: {location}\\n\"",
            "                log += f\"*** {str(e)}\\n\"",
            "        elif conf.get('core', 'executor') == 'KubernetesExecutor':",
            "            try:",
            "                from airflow.kubernetes.kube_client import get_kube_client",
            "",
            "                kube_client = get_kube_client()",
            "",
            "                if len(ti.hostname) >= 63:",
            "                    # Kubernetes takes the pod name and truncates it for the hostname. This truncated hostname",
            "                    # is returned for the fqdn to comply with the 63 character limit imposed by DNS standards",
            "                    # on any label of a FQDN.",
            "                    pod_list = kube_client.list_namespaced_pod(conf.get('kubernetes', 'namespace'))",
            "                    matches = [",
            "                        pod.metadata.name",
            "                        for pod in pod_list.items",
            "                        if pod.metadata.name.startswith(ti.hostname)",
            "                    ]",
            "                    if len(matches) == 1:",
            "                        if len(matches[0]) > len(ti.hostname):",
            "                            ti.hostname = matches[0]",
            "",
            "                log += '*** Trying to get logs (last 100 lines) from worker pod {} ***\\n\\n'.format(",
            "                    ti.hostname",
            "                )",
            "",
            "                res = kube_client.read_namespaced_pod_log(",
            "                    name=ti.hostname,",
            "                    namespace=conf.get('kubernetes', 'namespace'),",
            "                    container='base',",
            "                    follow=False,",
            "                    tail_lines=100,",
            "                    _preload_content=False,",
            "                )",
            "",
            "                for line in res:",
            "                    log += line.decode()",
            "",
            "            except Exception as f:",
            "                log += f'*** Unable to fetch logs from worker pod {ti.hostname} ***\\n{str(f)}\\n\\n'",
            "        else:",
            "            url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(",
            "                ti=ti, worker_log_server_port=conf.get('celery', 'WORKER_LOG_SERVER_PORT')",
            "            )",
            "            log += f\"*** Log file does not exist: {location}\\n\"",
            "            log += f\"*** Fetching from: {url}\\n\"",
            "            try:",
            "                timeout = None  # No timeout",
            "                try:",
            "                    timeout = conf.getint('webserver', 'log_fetch_timeout_sec')",
            "                except (AirflowConfigException, ValueError):",
            "                    pass",
            "",
            "                signer = TimedJSONWebSignatureSerializer(",
            "                    secret_key=conf.get('webserver', 'secret_key'),",
            "                    algorithm_name='HS512',",
            "                    expires_in=conf.getint('webserver', 'log_request_clock_grace', fallback=30),",
            "                )",
            "",
            "                response = httpx.get(url, timeout=timeout, headers={'Authorization': signer.dumps({})})",
            "                response.encoding = \"utf-8\"",
            "",
            "                # Check if the resource was properly fetched",
            "                response.raise_for_status()",
            "",
            "                log += '\\n' + response.text",
            "            except Exception as e:",
            "                log += f\"*** Failed to fetch log file from worker. {str(e)}\\n\"",
            "",
            "        return log, {'end_of_log': True}",
            "",
            "    def read(self, task_instance, try_number=None, metadata=None):",
            "        \"\"\"",
            "        Read logs of given task instance from local machine.",
            "",
            "        :param task_instance: task instance object",
            "        :param try_number: task instance try_number to read logs from. If None",
            "                           it returns all logs separated by try_number",
            "        :param metadata: log metadata,",
            "                         can be used for steaming log reading and auto-tailing.",
            "        :return: a list of listed tuples which order log string by host",
            "        \"\"\"",
            "        # Task instance increments its try number when it starts to run.",
            "        # So the log for a particular task try will only show up when",
            "        # try number gets incremented in DB, i.e logs produced the time",
            "        # after cli run and before try_number + 1 in DB will not be displayed.",
            "",
            "        if try_number is None:",
            "            next_try = task_instance.next_try_number",
            "            try_numbers = list(range(1, next_try))",
            "        elif try_number < 1:",
            "            logs = [",
            "                [('default_host', f'Error fetching the logs. Try number {try_number} is invalid.')],",
            "            ]",
            "            return logs, [{'end_of_log': True}]",
            "        else:",
            "            try_numbers = [try_number]",
            "",
            "        logs = [''] * len(try_numbers)",
            "        metadata_array = [{}] * len(try_numbers)",
            "        for i, try_number_element in enumerate(try_numbers):",
            "            log, metadata = self._read(task_instance, try_number_element, metadata)",
            "            # es_task_handler return logs grouped by host. wrap other handler returning log string",
            "            # with default/ empty host so that UI can render the response in the same way",
            "            logs[i] = log if self._read_grouped_logs() else [(task_instance.hostname, log)]",
            "            metadata_array[i] = metadata",
            "",
            "        return logs, metadata_array",
            "",
            "    def _init_file(self, ti):",
            "        \"\"\"",
            "        Create log directory and give it correct permissions.",
            "",
            "        :param ti: task instance object",
            "        :return: relative log path of the given task instance",
            "        \"\"\"",
            "        # To handle log writing when tasks are impersonated, the log files need to",
            "        # be writable by the user that runs the Airflow command and the user",
            "        # that is impersonated. This is mainly to handle corner cases with the",
            "        # SubDagOperator. When the SubDagOperator is run, all of the operators",
            "        # run under the impersonated user and create appropriate log files",
            "        # as the impersonated user. However, if the user manually runs tasks",
            "        # of the SubDagOperator through the UI, then the log files are created",
            "        # by the user that runs the Airflow command. For example, the Airflow",
            "        # run command may be run by the `airflow_sudoable` user, but the Airflow",
            "        # tasks may be run by the `airflow` user. If the log files are not",
            "        # writable by both users, then it's possible that re-running a task",
            "        # via the UI (or vice versa) results in a permission error as the task",
            "        # tries to write to a log file created by the other user.",
            "        relative_path = self._render_filename(ti, ti.try_number)",
            "        full_path = os.path.join(self.local_base, relative_path)",
            "        directory = os.path.dirname(full_path)",
            "        # Create the log file and give it group writable permissions",
            "        # TODO(aoen): Make log dirs and logs globally readable for now since the SubDag",
            "        # operator is not compatible with impersonation (e.g. if a Celery executor is used",
            "        # for a SubDag operator and the SubDag operator has a different owner than the",
            "        # parent DAG)",
            "        Path(directory).mkdir(mode=0o777, parents=True, exist_ok=True)",
            "",
            "        if not os.path.exists(full_path):",
            "            open(full_path, \"a\").close()",
            "            # TODO: Investigate using 444 instead of 666.",
            "            try:",
            "                os.chmod(full_path, 0o666)",
            "            except OSError:",
            "                logging.warning(\"OSError while change ownership of the log file\")",
            "",
            "        return full_path"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "175": [
                "FileTaskHandler",
                "_read"
            ]
        },
        "addLocation": []
    },
    "airflow/utils/serve_logs.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " \"\"\"Serve logs process\"\"\""
            },
            "2": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " import os"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 20,
                "PatchRowcode": "+import time"
            },
            "4": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import flask"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 22,
                "PatchRowcode": "+from flask import Flask, abort, request, send_from_directory"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 23,
                "PatchRowcode": "+from itsdangerous import TimedJSONWebSignatureSerializer"
            },
            "8": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " from setproctitle import setproctitle"
            },
            "9": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from airflow.configuration import conf"
            },
            "11": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def serve_logs():"
            },
            "14": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    \"\"\"Serves logs generated by Worker\"\"\""
            },
            "15": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    print(\"Starting flask\")"
            },
            "16": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    flask_app = flask.Flask(__name__)"
            },
            "17": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    setproctitle(\"airflow serve-logs\")"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+def flask_app():"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+    flask_app = Flask(__name__)"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+    max_request_age = conf.getint('webserver', 'log_request_clock_grace', fallback=30)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+    log_directory = os.path.expanduser(conf.get('logging', 'BASE_LOG_FOLDER'))"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+    signer = TimedJSONWebSignatureSerializer("
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+        secret_key=conf.get('webserver', 'secret_key'),"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 36,
                "PatchRowcode": "+        algorithm_name='HS512',"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+        expires_in=max_request_age,"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 38,
                "PatchRowcode": "+    )"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 39,
                "PatchRowcode": "+"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 40,
                "PatchRowcode": "+    # Prevent direct access to the logs port"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 41,
                "PatchRowcode": "+    @flask_app.before_request"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+    def validate_pre_signed_url():"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+        try:"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+            auth = request.headers['Authorization']"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+            # We don't actually care about the payload, just that the signature"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+            # was valid and the `exp` claim is correct"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+            _, headers = signer.loads(auth, return_header=True)"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+            issued_at = int(headers['iat'])"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+            expires_at = int(headers['exp'])"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+        except Exception as e:"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+            print(e)"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 54,
                "PatchRowcode": "+            abort(403)"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+        # Validate the `iat` and `exp` are within `max_request_age` of now."
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 56,
                "PatchRowcode": "+        now = int(time.time())"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 57,
                "PatchRowcode": "+        if abs(now - issued_at) > max_request_age:"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 58,
                "PatchRowcode": "+            abort(403)"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+        if abs(now - expires_at) > max_request_age:"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 60,
                "PatchRowcode": "+            abort(403)"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 61,
                "PatchRowcode": "+        if issued_at > expires_at or expires_at - issued_at > max_request_age:"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+            abort(403)"
            },
            "52": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 63,
                "PatchRowcode": " "
            },
            "53": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 64,
                "PatchRowcode": "     @flask_app.route('/log/<path:filename>')"
            },
            "54": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 65,
                "PatchRowcode": "     def serve_logs_view(filename):"
            },
            "55": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        log_directory = os.path.expanduser(conf.get('logging', 'BASE_LOG_FOLDER'))"
            },
            "56": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return flask.send_from_directory("
            },
            "57": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            log_directory, filename, mimetype=\"application/json\", as_attachment=False"
            },
            "58": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        )"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+        return send_from_directory(log_directory, filename, mimetype=\"application/json\", as_attachment=False)"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 68,
                "PatchRowcode": "+    return flask_app"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 69,
                "PatchRowcode": "+"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 70,
                "PatchRowcode": "+"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 71,
                "PatchRowcode": "+def serve_logs():"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+    \"\"\"Serves logs generated by Worker\"\"\""
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 73,
                "PatchRowcode": "+    setproctitle(\"airflow serve-logs\")"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+    app = flask_app()"
            },
            "68": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 75,
                "PatchRowcode": " "
            },
            "69": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 76,
                "PatchRowcode": "     worker_log_server_port = conf.getint('celery', 'WORKER_LOG_SERVER_PORT')"
            },
            "70": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    flask_app.run(host='0.0.0.0', port=worker_log_server_port)"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+    app.run(host='0.0.0.0', port=worker_log_server_port)"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "\"\"\"Serve logs process\"\"\"",
            "import os",
            "",
            "import flask",
            "from setproctitle import setproctitle",
            "",
            "from airflow.configuration import conf",
            "",
            "",
            "def serve_logs():",
            "    \"\"\"Serves logs generated by Worker\"\"\"",
            "    print(\"Starting flask\")",
            "    flask_app = flask.Flask(__name__)",
            "    setproctitle(\"airflow serve-logs\")",
            "",
            "    @flask_app.route('/log/<path:filename>')",
            "    def serve_logs_view(filename):",
            "        log_directory = os.path.expanduser(conf.get('logging', 'BASE_LOG_FOLDER'))",
            "        return flask.send_from_directory(",
            "            log_directory, filename, mimetype=\"application/json\", as_attachment=False",
            "        )",
            "",
            "    worker_log_server_port = conf.getint('celery', 'WORKER_LOG_SERVER_PORT')",
            "    flask_app.run(host='0.0.0.0', port=worker_log_server_port)"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "\"\"\"Serve logs process\"\"\"",
            "import os",
            "import time",
            "",
            "from flask import Flask, abort, request, send_from_directory",
            "from itsdangerous import TimedJSONWebSignatureSerializer",
            "from setproctitle import setproctitle",
            "",
            "from airflow.configuration import conf",
            "",
            "",
            "def flask_app():",
            "    flask_app = Flask(__name__)",
            "    max_request_age = conf.getint('webserver', 'log_request_clock_grace', fallback=30)",
            "    log_directory = os.path.expanduser(conf.get('logging', 'BASE_LOG_FOLDER'))",
            "",
            "    signer = TimedJSONWebSignatureSerializer(",
            "        secret_key=conf.get('webserver', 'secret_key'),",
            "        algorithm_name='HS512',",
            "        expires_in=max_request_age,",
            "    )",
            "",
            "    # Prevent direct access to the logs port",
            "    @flask_app.before_request",
            "    def validate_pre_signed_url():",
            "        try:",
            "            auth = request.headers['Authorization']",
            "",
            "            # We don't actually care about the payload, just that the signature",
            "            # was valid and the `exp` claim is correct",
            "            _, headers = signer.loads(auth, return_header=True)",
            "",
            "            issued_at = int(headers['iat'])",
            "            expires_at = int(headers['exp'])",
            "        except Exception as e:",
            "            print(e)",
            "            abort(403)",
            "        # Validate the `iat` and `exp` are within `max_request_age` of now.",
            "        now = int(time.time())",
            "        if abs(now - issued_at) > max_request_age:",
            "            abort(403)",
            "        if abs(now - expires_at) > max_request_age:",
            "            abort(403)",
            "        if issued_at > expires_at or expires_at - issued_at > max_request_age:",
            "            abort(403)",
            "",
            "    @flask_app.route('/log/<path:filename>')",
            "    def serve_logs_view(filename):",
            "        return send_from_directory(log_directory, filename, mimetype=\"application/json\", as_attachment=False)",
            "",
            "    return flask_app",
            "",
            "",
            "def serve_logs():",
            "    \"\"\"Serves logs generated by Worker\"\"\"",
            "    setproctitle(\"airflow serve-logs\")",
            "    app = flask_app()",
            "",
            "    worker_log_server_port = conf.getint('celery', 'WORKER_LOG_SERVER_PORT')",
            "    app.run(host='0.0.0.0', port=worker_log_server_port)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "21": [],
            "27": [
                "serve_logs"
            ],
            "28": [
                "serve_logs"
            ],
            "29": [
                "serve_logs"
            ],
            "30": [
                "serve_logs"
            ],
            "31": [
                "serve_logs"
            ],
            "35": [
                "serve_logs",
                "serve_logs_view"
            ],
            "36": [
                "serve_logs",
                "serve_logs_view"
            ],
            "37": [
                "serve_logs",
                "serve_logs_view"
            ],
            "38": [
                "serve_logs",
                "serve_logs_view"
            ],
            "41": [
                "serve_logs"
            ]
        },
        "addLocation": []
    }
}