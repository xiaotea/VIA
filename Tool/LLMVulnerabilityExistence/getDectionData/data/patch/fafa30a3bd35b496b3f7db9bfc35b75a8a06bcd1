{
    "S3Scanner/S3Service.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " from botocore.client import Config\r"
            },
            "1": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " import datetime\r"
            },
            "2": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " from S3Scanner.exceptions import AccessDeniedException, InvalidEndpointException, BucketMightNotExistException\r"
            },
            "3": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from os.path import normpath\r"
            },
            "4": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " import pathlib\r"
            },
            "5": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " from concurrent.futures import ThreadPoolExecutor, as_completed\r"
            },
            "6": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " from functools import partial\r"
            },
            "7": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " from urllib3 import disable_warnings\r"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 17,
                "PatchRowcode": "+import os\r"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 18,
                "PatchRowcode": "+\r"
            },
            "10": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " \r"
            },
            "11": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " ALL_USERS_URI = 'uri=http://acs.amazonaws.com/groups/global/AllUsers'\r"
            },
            "12": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " AUTH_USERS_URI = 'uri=http://acs.amazonaws.com/groups/global/AuthenticatedUsers'\r"
            },
            "13": {
                "beforePatchRowNumber": 284,
                "afterPatchRowNumber": 285,
                "PatchRowcode": " \r"
            },
            "14": {
                "beforePatchRowNumber": 285,
                "afterPatchRowNumber": 286,
                "PatchRowcode": "             for future in as_completed(futures):\r"
            },
            "15": {
                "beforePatchRowNumber": 286,
                "afterPatchRowNumber": 287,
                "PatchRowcode": "                 if future.exception():\r"
            },
            "16": {
                "beforePatchRowNumber": 287,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    print(f\"{bucket.name} | Download failed: {futures[future]}\")\r"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 288,
                "PatchRowcode": "+                    print(f\"{bucket.name} | Download failed: {futures[future]} | {future.exception()}\")\r"
            },
            "18": {
                "beforePatchRowNumber": 288,
                "afterPatchRowNumber": 289,
                "PatchRowcode": " \r"
            },
            "19": {
                "beforePatchRowNumber": 289,
                "afterPatchRowNumber": 290,
                "PatchRowcode": "         print(f\"{bucket.name} | Dumping completed\")\r"
            },
            "20": {
                "beforePatchRowNumber": 290,
                "afterPatchRowNumber": 291,
                "PatchRowcode": " \r"
            },
            "21": {
                "beforePatchRowNumber": 291,
                "afterPatchRowNumber": 292,
                "PatchRowcode": "     def download_file(self, dest_directory, bucket, verbose, obj):\r"
            },
            "22": {
                "beforePatchRowNumber": 292,
                "afterPatchRowNumber": 293,
                "PatchRowcode": "         \"\"\"\r"
            },
            "23": {
                "beforePatchRowNumber": 293,
                "afterPatchRowNumber": 294,
                "PatchRowcode": "         Download `obj` from `bucket` into `dest_directory`\r"
            },
            "24": {
                "beforePatchRowNumber": 294,
                "afterPatchRowNumber": 295,
                "PatchRowcode": " \r"
            },
            "25": {
                "beforePatchRowNumber": 295,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        :param str dest_directory: Directory to store the object into\r"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 296,
                "PatchRowcode": "+        :param str dest_directory: Directory to store the object into. _Must_ end in a slash\r"
            },
            "27": {
                "beforePatchRowNumber": 296,
                "afterPatchRowNumber": 297,
                "PatchRowcode": "         :param S3Bucket bucket: Bucket to download the object from\r"
            },
            "28": {
                "beforePatchRowNumber": 297,
                "afterPatchRowNumber": 298,
                "PatchRowcode": "         :param bool verbose: Output verbose messages to the user\r"
            },
            "29": {
                "beforePatchRowNumber": 298,
                "afterPatchRowNumber": 299,
                "PatchRowcode": "         :param S3BucketObject obj: Object to downlaod\r"
            },
            "30": {
                "beforePatchRowNumber": 299,
                "afterPatchRowNumber": 300,
                "PatchRowcode": "         :return: None\r"
            },
            "31": {
                "beforePatchRowNumber": 300,
                "afterPatchRowNumber": 301,
                "PatchRowcode": "         \"\"\"\r"
            },
            "32": {
                "beforePatchRowNumber": 301,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        dest_file_path = pathlib.Path(normpath(dest_directory + obj.key))\r"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 302,
                "PatchRowcode": "+        dest_file_path = pathlib.Path(os.path.normpath(os.path.join(dest_directory, obj.key)))\r"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 303,
                "PatchRowcode": "+\r"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 304,
                "PatchRowcode": "+        if not self.is_safe_file_to_download(obj.key, dest_directory):\r"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 305,
                "PatchRowcode": "+            print(f\"{bucket.name} | Skipping file {obj.key}. File references a parent directory.\")\r"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 306,
                "PatchRowcode": "+            return\r"
            },
            "38": {
                "beforePatchRowNumber": 302,
                "afterPatchRowNumber": 307,
                "PatchRowcode": "         if dest_file_path.exists():\r"
            },
            "39": {
                "beforePatchRowNumber": 303,
                "afterPatchRowNumber": 308,
                "PatchRowcode": "             if dest_file_path.stat().st_size == obj.size:\r"
            },
            "40": {
                "beforePatchRowNumber": 304,
                "afterPatchRowNumber": 309,
                "PatchRowcode": "                 if verbose:\r"
            },
            "41": {
                "beforePatchRowNumber": 342,
                "afterPatchRowNumber": 347,
                "PatchRowcode": "                 raise AccessDeniedException(\"AccessDenied while enumerating bucket objects\")\r"
            },
            "42": {
                "beforePatchRowNumber": 343,
                "afterPatchRowNumber": 348,
                "PatchRowcode": "         bucket.objects_enumerated = True\r"
            },
            "43": {
                "beforePatchRowNumber": 344,
                "afterPatchRowNumber": 349,
                "PatchRowcode": " \r"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 350,
                "PatchRowcode": "+    def is_safe_file_to_download(self, file_to_check, dest_directory):\r"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 351,
                "PatchRowcode": "+        \"\"\"\r"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 352,
                "PatchRowcode": "+        Check if bucket object would be saved outside of `dest_directory` if downloaded.\r"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 353,
                "PatchRowcode": "+        AWS allows object keys to include relative path characters like '../' which can lead to a \r"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 354,
                "PatchRowcode": "+        path traversal-like issue where objects get saved outside of the intended directory.\r"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 355,
                "PatchRowcode": "+\r"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 356,
                "PatchRowcode": "+        :param string file_to_check: Bucket object key\r"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 357,
                "PatchRowcode": "+        :param string dest_directory: Path to directory to save file in\r"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 358,
                "PatchRowcode": "+        :return: bool\r"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 359,
                "PatchRowcode": "+        \"\"\"\r"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 360,
                "PatchRowcode": "+        file_to_check = os.path.abspath(os.path.join(dest_directory, file_to_check))\r"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 361,
                "PatchRowcode": "+        safe_dir = os.path.abspath(dest_directory)\r"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 362,
                "PatchRowcode": "+        return os.path.commonpath([safe_dir]) == os.path.commonpath([safe_dir, file_to_check])\r"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 363,
                "PatchRowcode": "+\r"
            },
            "58": {
                "beforePatchRowNumber": 345,
                "afterPatchRowNumber": 364,
                "PatchRowcode": "     def parse_found_acl(self, bucket):\r"
            },
            "59": {
                "beforePatchRowNumber": 346,
                "afterPatchRowNumber": 365,
                "PatchRowcode": "         \"\"\"\r"
            },
            "60": {
                "beforePatchRowNumber": 347,
                "afterPatchRowNumber": 366,
                "PatchRowcode": "         Translate ACL grants into permission properties. If we were able to read the ACLs, we should be able to skip\r"
            }
        },
        "frontPatchFile": [
            "\"\"\"\r",
            "    This will be a service that the client program will instantiate to then call methods\r",
            "    passing buckets\r",
            "\"\"\"\r",
            "from boto3 import client  # TODO: Limit import to just boto3.client, probably\r",
            "from S3Scanner.S3Bucket import S3Bucket, BucketExists, Permission, S3BucketObject\r",
            "from botocore.exceptions import ClientError\r",
            "import botocore.session\r",
            "from botocore import UNSIGNED\r",
            "from botocore.client import Config\r",
            "import datetime\r",
            "from S3Scanner.exceptions import AccessDeniedException, InvalidEndpointException, BucketMightNotExistException\r",
            "from os.path import normpath\r",
            "import pathlib\r",
            "from concurrent.futures import ThreadPoolExecutor, as_completed\r",
            "from functools import partial\r",
            "from urllib3 import disable_warnings\r",
            "\r",
            "ALL_USERS_URI = 'uri=http://acs.amazonaws.com/groups/global/AllUsers'\r",
            "AUTH_USERS_URI = 'uri=http://acs.amazonaws.com/groups/global/AuthenticatedUsers'\r",
            "\r",
            "\r",
            "class S3Service:\r",
            "    def __init__(self, forceNoCreds=False, endpoint_url='https://s3.amazonaws.com', verify_ssl=True,\r",
            "                 endpoint_address_style='path'):\r",
            "        \"\"\"\r",
            "        Service constructor\r",
            "\r",
            "        :param forceNoCreds: (Boolean) Force the service to not use credentials, even if the user has creds configured\r",
            "        :param endpoint_url: (String) URL of S3 endpoint to use. Must include http(s):// scheme\r",
            "        :param verify_ssl: (Boolean) Whether of not to verify ssl. Set to false if endpoint is http\r",
            "        :param endpoint_address_style: (String) Addressing style of the endpoint. Must be 'path' or 'vhost'\r",
            "        :returns None\r",
            "        \"\"\"\r",
            "        self.endpoint_url = endpoint_url\r",
            "        self.endpoint_address_style = 'path' if endpoint_address_style == 'path' else 'virtual'\r",
            "        use_ssl = True if self.endpoint_url.startswith('http://') else False\r",
            "\r",
            "        if not verify_ssl:\r",
            "            disable_warnings()\r",
            "\r",
            "        # DEBUG\r",
            "        # boto3.set_stream_logger(name='botocore')\r",
            "\r",
            "        # Validate the endpoint if it's not the default of AWS\r",
            "        if self.endpoint_url != 'https://s3.amazonaws.com':\r",
            "            if not self.validate_endpoint_url(use_ssl, verify_ssl, endpoint_address_style):\r",
            "                raise InvalidEndpointException(message=f\"Endpoint '{self.endpoint_url}' does not appear to be S3-compliant\")\r",
            "\r",
            "        # Check for AWS credentials\r",
            "        session = botocore.session.get_session()\r",
            "        if forceNoCreds or session.get_credentials() is None or session.get_credentials().access_key is None:\r",
            "            self.aws_creds_configured = False\r",
            "            self.s3_client = client('s3',\r",
            "                                          config=Config(signature_version=UNSIGNED, s3={'addressing_style': self.endpoint_address_style}, connect_timeout=3,\r",
            "                                         retries={'max_attempts': 2}),\r",
            "                                          endpoint_url=self.endpoint_url, use_ssl=use_ssl, verify=verify_ssl)\r",
            "        else:\r",
            "            self.aws_creds_configured = True\r",
            "            self.s3_client = client('s3', config=Config(s3={'addressing_style': self.endpoint_address_style}, connect_timeout=3,\r",
            "                                         retries={'max_attempts': 2}),\r",
            "                                          endpoint_url=self.endpoint_url, use_ssl=use_ssl, verify=verify_ssl)\r",
            "\r",
            "        del session  # No longer needed\r",
            "\r",
            "    def check_bucket_exists(self, bucket):\r",
            "        \"\"\"\r",
            "        Checks if a bucket exists. Sets `exists` property of `bucket`\r",
            "\r",
            "        :param S3Bucket bucket: Bucket to check\r",
            "        :raises ValueError: If `bucket` is not an s3Bucket object\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "        if not isinstance(bucket, S3Bucket):\r",
            "            raise ValueError(\"Passed object was not type S3Bucket\")\r",
            "\r",
            "        bucket_exists = True\r",
            "\r",
            "        try:\r",
            "            self.s3_client.head_bucket(Bucket=bucket.name)\r",
            "        except ClientError as e:\r",
            "            if e.response['Error']['Code'] == '404':\r",
            "                bucket_exists = False\r",
            "\r",
            "        bucket.exists = BucketExists.YES if bucket_exists else BucketExists.NO\r",
            "\r",
            "    def check_perm_read_acl(self, bucket):\r",
            "        \"\"\"\r",
            "        Check for the READACP permission on `bucket` by trying to get the bucket ACL\r",
            "\r",
            "        :param S3Bucket bucket: Bucket to check permission of\r",
            "        :raises BucketMightNotExistException: If `bucket` existence hasn't been checked\r",
            "        :raises ClientError: If we encounter an unexpected ClientError from boto client\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "\r",
            "        if bucket.exists != BucketExists.YES:\r",
            "            raise BucketMightNotExistException()\r",
            "\r",
            "        try:\r",
            "            bucket.foundACL = self.s3_client.get_bucket_acl(Bucket=bucket.name)\r",
            "            self.parse_found_acl(bucket)  # If we can read ACLs, we know the rest of the permissions\r",
            "        except ClientError as e:\r",
            "            if e.response['Error']['Code'] == \"AccessDenied\" or e.response['Error']['Code'] == \"AllAccessDisabled\":\r",
            "                if self.aws_creds_configured:\r",
            "                    bucket.AuthUsersReadACP = Permission.DENIED\r",
            "                else:\r",
            "                    bucket.AllUsersReadACP = Permission.DENIED\r",
            "            else:\r",
            "                raise e\r",
            "\r",
            "    def check_perm_read(self, bucket):\r",
            "        \"\"\"\r",
            "        Checks for the READ permission on the bucket by attempting to list the objects.\r",
            "        Sets the `AllUsersRead` and/or `AuthUsersRead` property of `bucket`.\r",
            "\r",
            "        :param S3Bucket bucket: Bucket to check permission of\r",
            "        :raises BucketMightNotExistException: If `bucket` existence hasn't been checked\r",
            "        :raises ClientError: If we encounter an unexpected ClientError from boto client\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "        if bucket.exists != BucketExists.YES:\r",
            "            raise BucketMightNotExistException()\r",
            "\r",
            "        list_bucket_perm_allowed = True\r",
            "        try:\r",
            "            self.s3_client.list_objects_v2(Bucket=bucket.name, MaxKeys=0)  # TODO: Compare this to doing a HeadBucket\r",
            "        except ClientError as e:\r",
            "            if e.response['Error']['Code'] == \"AccessDenied\" or e.response['Error']['Code'] == \"AllAccessDisabled\":\r",
            "                list_bucket_perm_allowed = False\r",
            "            else:\r",
            "                print(f\"ERROR: Error while checking bucket {bucket.name}\")\r",
            "                raise e\r",
            "        if self.aws_creds_configured:\r",
            "            # Don't mark AuthUsersRead as Allowed if it's only implicitly allowed due to AllUsersRead being allowed\r",
            "            # We only want to make AuthUsersRead as Allowed if that permission is explicitly set for AuthUsers\r",
            "            if bucket.AllUsersRead != Permission.ALLOWED:\r",
            "                bucket.AuthUsersRead = Permission.ALLOWED if list_bucket_perm_allowed else Permission.DENIED\r",
            "        else:\r",
            "            bucket.AllUsersRead = Permission.ALLOWED if list_bucket_perm_allowed else Permission.DENIED\r",
            "\r",
            "    def check_perm_write(self, bucket):\r",
            "        \"\"\"\r",
            "        Check for WRITE permission by trying to upload an empty file to the bucket.\r",
            "        File is named the current timestamp to ensure we're not overwriting an existing file in the bucket.\r",
            "\r",
            "        NOTE: If writing to bucket succeeds using an AuthUser, only mark AuthUsersWrite as Allowed if AllUsers are\r",
            "        Denied. Writing can succeed if AuthUsers are implicitly allowed due to AllUsers being allowed, but we only want\r",
            "        to mark AuthUsers as Allowed if they are explicitly granted. If AllUsersWrite is Allowed and the write is\r",
            "        successful by an AuthUser, we have no way of knowing if AuthUsers were granted permission explicitly\r",
            "\r",
            "        :param S3Bucket bucket: Bucket to check permission of\r",
            "        :raises BucketMightNotExistException: If `bucket` existence hasn't been checked\r",
            "        :raises ClientError: If we encounter an unexpected ClientError from boto client\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "        if bucket.exists != BucketExists.YES:\r",
            "            raise BucketMightNotExistException()\r",
            "\r",
            "        timestamp_file = str(datetime.datetime.now().timestamp()) + '.txt'\r",
            "\r",
            "        try:\r",
            "            # Try to create a new empty file with a key of the timestamp\r",
            "            self.s3_client.put_object(Bucket=bucket.name, Key=timestamp_file, Body=b'')\r",
            "\r",
            "            if self.aws_creds_configured:\r",
            "                if bucket.AllUsersWrite != Permission.ALLOWED:  # If AllUsers have Write permission, don't mark AuthUsers as Allowed\r",
            "                    bucket.AuthUsersWrite = Permission.ALLOWED\r",
            "                else:\r",
            "                    bucket.AuthUsersWrite = Permission.UNKNOWN\r",
            "            else:\r",
            "                bucket.AllUsersWrite = Permission.ALLOWED\r",
            "\r",
            "            # Delete the temporary file\r",
            "            self.s3_client.delete_object(Bucket=bucket.name, Key=timestamp_file)\r",
            "        except ClientError as e:\r",
            "            if e.response['Error']['Code'] == \"AccessDenied\" or e.response['Error']['Code'] == \"AllAccessDisabled\":\r",
            "                if self.aws_creds_configured:\r",
            "                    bucket.AuthUsersWrite = Permission.DENIED\r",
            "                else:\r",
            "                    bucket.AllUsersWrite = Permission.DENIED\r",
            "            else:\r",
            "                raise e\r",
            "\r",
            "    def check_perm_write_acl(self, bucket):\r",
            "        \"\"\"\r",
            "        Checks for WRITE_ACP permission by attempting to set an ACL on the bucket.\r",
            "        WARNING: Potentially destructive - make sure to run this check last as it will include all discovered\r",
            "        permissions in the ACL it tries to set, thus ensuring minimal disruption for the bucket owner.\r",
            "\r",
            "        :param S3Bucket bucket: Bucket to check permission of\r",
            "        :raises BucketMightNotExistException: If `bucket` existence hasn't been checked\r",
            "        :raises ClientError: If we encounter an unexpected ClientError from boto client\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "        if bucket.exists != BucketExists.YES:\r",
            "            raise BucketMightNotExistException()\r",
            "\r",
            "        # TODO: See if there's a way to simplify this section\r",
            "        readURIs = []\r",
            "        writeURIs = []\r",
            "        readAcpURIs = []\r",
            "        writeAcpURIs = []\r",
            "        fullControlURIs = []\r",
            "\r",
            "        if bucket.AuthUsersRead == Permission.ALLOWED:\r",
            "            readURIs.append(AUTH_USERS_URI)\r",
            "        if bucket.AuthUsersWrite == Permission.ALLOWED:\r",
            "            writeURIs.append(AUTH_USERS_URI)\r",
            "        if bucket.AuthUsersReadACP == Permission.ALLOWED:\r",
            "            readAcpURIs.append(AUTH_USERS_URI)\r",
            "        if bucket.AuthUsersWriteACP == Permission.ALLOWED:\r",
            "            writeAcpURIs.append(AUTH_USERS_URI)\r",
            "        if bucket.AuthUsersFullControl == Permission.ALLOWED:\r",
            "            fullControlURIs.append(AUTH_USERS_URI)\r",
            "\r",
            "        if bucket.AllUsersRead == Permission.ALLOWED:\r",
            "            readURIs.append(ALL_USERS_URI)\r",
            "        if bucket.AllUsersWrite == Permission.ALLOWED:\r",
            "            writeURIs.append(ALL_USERS_URI)\r",
            "        if bucket.AllUsersReadACP == Permission.ALLOWED:\r",
            "            readAcpURIs.append(ALL_USERS_URI)\r",
            "        if bucket.AllUsersWriteACP == Permission.ALLOWED:\r",
            "            writeAcpURIs.append(ALL_USERS_URI)\r",
            "        if bucket.AllUsersFullControl == Permission.ALLOWED:\r",
            "            fullControlURIs.append(ALL_USERS_URI)\r",
            "\r",
            "        if self.aws_creds_configured:   # Otherwise AWS will return \"Request was missing a required header\"\r",
            "            writeAcpURIs.append(AUTH_USERS_URI)\r",
            "        else:\r",
            "            writeAcpURIs.append(ALL_USERS_URI)\r",
            "        args = {'Bucket': bucket.name}\r",
            "        if len(readURIs) > 0:\r",
            "            args['GrantRead'] = ','.join(readURIs)\r",
            "        if len(writeURIs) > 0:\r",
            "            args['GrantWrite'] = ','.join(writeURIs)\r",
            "        if len(readAcpURIs) > 0:\r",
            "            args['GrantReadACP'] = ','.join(readAcpURIs)\r",
            "        if len(writeAcpURIs) > 0:\r",
            "            args['GrantWriteACP'] = ','.join(writeAcpURIs)\r",
            "        if len(fullControlURIs) > 0:\r",
            "            args['GrantFullControl'] = ','.join(fullControlURIs)\r",
            "        try:\r",
            "            self.s3_client.put_bucket_acl(**args)\r",
            "            if self.aws_creds_configured:\r",
            "                # Don't mark AuthUsersWriteACP as Allowed if it's due to implicit permission via AllUsersWriteACP\r",
            "                # Only mark it as allowed if the AuthUsers group is explicitly allowed\r",
            "                if bucket.AllUsersWriteACP != Permission.ALLOWED:\r",
            "                    bucket.AuthUsersWriteACP = Permission.ALLOWED\r",
            "                else:\r",
            "                    bucket.AuthUsersWriteACP = Permission.UNKNOWN\r",
            "            else:\r",
            "                bucket.AllUsersWriteACP = Permission.ALLOWED\r",
            "        except ClientError as e:\r",
            "            if e.response['Error']['Code'] == \"AccessDenied\" or e.response['Error']['Code'] == \"AllAccessDisabled\":\r",
            "                if self.aws_creds_configured:\r",
            "                    bucket.AuthUsersWriteACP = Permission.DENIED\r",
            "                else:\r",
            "                    bucket.AllUsersWriteACP = Permission.DENIED\r",
            "            else:\r",
            "                raise e\r",
            "\r",
            "    def dump_bucket_multithread(self, bucket, dest_directory, verbose=False, threads=4):\r",
            "        \"\"\"\r",
            "        Takes a bucket and downloads all the objects to a local folder.\r",
            "        If the object exists locally and is the same size as the remote object, the object is skipped.\r",
            "        If the object exists locally and is a different size then the remote object, the local object is overwritten.\r",
            "\r",
            "        :param S3Bucket bucket: Bucket whose contents we want to dump\r",
            "        :param str dest_directory: Folder to save the objects to. Must include trailing slash\r",
            "        :param bool verbose: Output verbose messages to the user\r",
            "        :param int threads: Number of threads to use while dumping\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "        # TODO: Let the user choose whether or not to overwrite local files if different\r",
            "\r",
            "        print(f\"{bucket.name} | Dumping contents using 4 threads...\")\r",
            "        func = partial(self.download_file, dest_directory, bucket, verbose)\r",
            "\r",
            "        with ThreadPoolExecutor(max_workers=threads) as executor:\r",
            "            futures = {\r",
            "                executor.submit(func, obj): obj for obj in bucket.objects\r",
            "            }\r",
            "\r",
            "            for future in as_completed(futures):\r",
            "                if future.exception():\r",
            "                    print(f\"{bucket.name} | Download failed: {futures[future]}\")\r",
            "\r",
            "        print(f\"{bucket.name} | Dumping completed\")\r",
            "\r",
            "    def download_file(self, dest_directory, bucket, verbose, obj):\r",
            "        \"\"\"\r",
            "        Download `obj` from `bucket` into `dest_directory`\r",
            "\r",
            "        :param str dest_directory: Directory to store the object into\r",
            "        :param S3Bucket bucket: Bucket to download the object from\r",
            "        :param bool verbose: Output verbose messages to the user\r",
            "        :param S3BucketObject obj: Object to downlaod\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "        dest_file_path = pathlib.Path(normpath(dest_directory + obj.key))\r",
            "        if dest_file_path.exists():\r",
            "            if dest_file_path.stat().st_size == obj.size:\r",
            "                if verbose:\r",
            "                    print(f\"{bucket.name} | Skipping {obj.key} - already downloaded\")\r",
            "                return\r",
            "            else:\r",
            "                if verbose:\r",
            "                    print(f\"{bucket.name} | Re-downloading {obj.key} - local size differs from remote\")\r",
            "        else:\r",
            "            if verbose:\r",
            "                print(f\"{bucket.name} | Downloading {obj.key}\")\r",
            "        dest_file_path.parent.mkdir(parents=True, exist_ok=True)  # Equivalent to `mkdir -p`\r",
            "        self.s3_client.download_file(bucket.name, obj.key, str(dest_file_path))\r",
            "\r",
            "    def enumerate_bucket_objects(self, bucket):\r",
            "        \"\"\"\r",
            "        Enumerate all the objects in a bucket. Sets the `BucketSize`, `objects`, and `objects_enumerated` properties\r",
            "        of `bucket`.\r",
            "\r",
            "        :param S3Bucket bucket: Bucket to enumerate objects of\r",
            "        :raises Exception: If the bucket doesn't exist\r",
            "        :raises AccessDeniedException: If we are denied access to the bucket objects\r",
            "        :raises ClientError: If we encounter an unexpected ClientError from boto client\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "        if bucket.exists == BucketExists.UNKNOWN:\r",
            "            self.check_bucket_exists(bucket)\r",
            "        if bucket.exists == BucketExists.NO:\r",
            "            raise Exception(\"Bucket doesn't exist\")\r",
            "\r",
            "        try:\r",
            "            for page in self.s3_client.get_paginator(\"list_objects_v2\").paginate(Bucket=bucket.name):\r",
            "                if 'Contents' not in page:  # No items in this bucket\r",
            "                    bucket.objects_enumerated = True\r",
            "                    return\r",
            "                for item in page['Contents']:\r",
            "                    obj = S3BucketObject(key=item['Key'], last_modified=item['LastModified'], size=item['Size'])\r",
            "                    bucket.add_object(obj)\r",
            "        except ClientError as e:\r",
            "            if e.response['Error']['Code'] == \"AccessDenied\" or e.response['Error']['Code'] == \"AllAccessDisabled\":\r",
            "                raise AccessDeniedException(\"AccessDenied while enumerating bucket objects\")\r",
            "        bucket.objects_enumerated = True\r",
            "\r",
            "    def parse_found_acl(self, bucket):\r",
            "        \"\"\"\r",
            "        Translate ACL grants into permission properties. If we were able to read the ACLs, we should be able to skip\r",
            "        manually checking most permissions\r",
            "\r",
            "        :param S3Bucket bucket: Bucket whose ACLs we want to parse\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "        if bucket.foundACL is None:\r",
            "            return\r",
            "\r",
            "        if 'Grants' in bucket.foundACL:\r",
            "            for grant in bucket.foundACL['Grants']:\r",
            "                if grant['Grantee']['Type'] == 'Group':\r",
            "                    if 'URI' in grant['Grantee'] and grant['Grantee']['URI'] == 'http://acs.amazonaws.com/groups/global/AuthenticatedUsers':\r",
            "                        # Permissions have been given to the AuthUsers group\r",
            "                        if grant['Permission'] == 'FULL_CONTROL':\r",
            "                            bucket.AuthUsersRead = Permission.ALLOWED\r",
            "                            bucket.AuthUsersWrite = Permission.ALLOWED\r",
            "                            bucket.AuthUsersReadACP = Permission.ALLOWED\r",
            "                            bucket.AuthUsersWriteACP = Permission.ALLOWED\r",
            "                            bucket.AuthUsersFullControl = Permission.ALLOWED\r",
            "                        elif grant['Permission'] == 'READ':\r",
            "                            bucket.AuthUsersRead = Permission.ALLOWED\r",
            "                        elif grant['Permission'] == 'READ_ACP':\r",
            "                            bucket.AuthUsersReadACP = Permission.ALLOWED\r",
            "                        elif grant['Permission'] == 'WRITE':\r",
            "                            bucket.AuthUsersWrite = Permission.ALLOWED\r",
            "                        elif grant['Permission'] == 'WRITE_ACP':\r",
            "                            bucket.AuthUsersWriteACP = Permission.ALLOWED\r",
            "\r",
            "                    elif 'URI' in grant['Grantee'] and grant['Grantee']['URI'] == 'http://acs.amazonaws.com/groups/global/AllUsers':\r",
            "                        # Permissions have been given to the AllUsers group\r",
            "                        if grant['Permission'] == 'FULL_CONTROL':\r",
            "                            bucket.AllUsersRead = Permission.ALLOWED\r",
            "                            bucket.AllUsersWrite = Permission.ALLOWED\r",
            "                            bucket.AllUsersReadACP = Permission.ALLOWED\r",
            "                            bucket.AllUsersWriteACP = Permission.ALLOWED\r",
            "                            bucket.AllUsersFullControl = Permission.ALLOWED\r",
            "                        elif grant['Permission'] == 'READ':\r",
            "                            bucket.AllUsersRead = Permission.ALLOWED\r",
            "                        elif grant['Permission'] == 'READ_ACP':\r",
            "                            bucket.AllUsersReadACP = Permission.ALLOWED\r",
            "                        elif grant['Permission'] == 'WRITE':\r",
            "                            bucket.AllUsersWrite = Permission.ALLOWED\r",
            "                        elif grant['Permission'] == 'WRITE_ACP':\r",
            "                            bucket.AllUsersWriteACP = Permission.ALLOWED\r",
            "\r",
            "            # All permissions not explicitly granted in the ACL are denied\r",
            "            # TODO: Simplify this\r",
            "            if bucket.AuthUsersRead == Permission.UNKNOWN:\r",
            "                bucket.AuthUsersRead = Permission.DENIED\r",
            "\r",
            "            if bucket.AuthUsersWrite == Permission.UNKNOWN:\r",
            "                bucket.AuthUsersWrite = Permission.DENIED\r",
            "\r",
            "            if bucket.AuthUsersReadACP == Permission.UNKNOWN:\r",
            "                bucket.AuthUsersReadACP = Permission.DENIED\r",
            "\r",
            "            if bucket.AuthUsersWriteACP == Permission.UNKNOWN:\r",
            "                bucket.AuthUsersWriteACP = Permission.DENIED\r",
            "\r",
            "            if bucket.AuthUsersFullControl == Permission.UNKNOWN:\r",
            "                bucket.AuthUsersFullControl = Permission.DENIED\r",
            "\r",
            "            if bucket.AllUsersRead == Permission.UNKNOWN:\r",
            "                bucket.AllUsersRead = Permission.DENIED\r",
            "\r",
            "            if bucket.AllUsersWrite == Permission.UNKNOWN:\r",
            "                bucket.AllUsersWrite = Permission.DENIED\r",
            "\r",
            "            if bucket.AllUsersReadACP == Permission.UNKNOWN:\r",
            "                bucket.AllUsersReadACP = Permission.DENIED\r",
            "\r",
            "            if bucket.AllUsersWriteACP == Permission.UNKNOWN:\r",
            "                bucket.AllUsersWriteACP = Permission.DENIED\r",
            "\r",
            "            if bucket.AllUsersFullControl == Permission.UNKNOWN:\r",
            "                bucket.AllUsersFullControl = Permission.DENIED\r",
            "\r",
            "    def validate_endpoint_url(self, use_ssl=True, verify_ssl=True, endpoint_address_style='path'):\r",
            "        \"\"\"\r",
            "        Verify the user-supplied endpoint URL is S3-compliant by trying to list a maximum of 0 keys from a bucket which\r",
            "        is extremely unlikely to exist.\r",
            "\r",
            "        Note: Most S3-compliant services will return an error code of \"NoSuchBucket\". Some services which require auth\r",
            "        for most operations (like Minio) will return an error code of \"AccessDenied\" instead\r",
            "\r",
            "        :param bool use_ssl: Whether or not the endpoint serves HTTP over SSL\r",
            "        :param bool verify_ssl: Whether or not to verify the SSL connection.\r",
            "        :param str endpoint_address_style: Addressing style of endpoint. Must be either 'path' or 'vhost'\r",
            "        :return: bool: Whether or not the server responded in an S3-compliant way\r",
            "        \"\"\"\r",
            "\r",
            "        # We always want to verify the endpoint using no creds\r",
            "        # so if the s3_client has creds configured, make a new anonymous client\r",
            "\r",
            "        addressing_style = 'virtual' if endpoint_address_style == 'vhost' else 'path'\r",
            "\r",
            "        validation_client = client('s3', config=Config(signature_version=UNSIGNED,\r",
            "                                         s3={'addressing_style': addressing_style}, connect_timeout=3,\r",
            "                                         retries={'max_attempts': 0}), endpoint_url=self.endpoint_url, use_ssl=use_ssl,\r",
            "                                         verify=verify_ssl)\r",
            "\r",
            "        non_existent_bucket = 's3scanner-' + str(datetime.datetime.now())[0:10]\r",
            "        try:\r",
            "            validation_client.list_objects_v2(Bucket=non_existent_bucket, MaxKeys=0)\r",
            "        except ClientError as e:\r",
            "            if e.response['Error']['Code'] == 'NoSuchBucket' or e.response['Error']['Code'] == 'AccessDenied':\r",
            "                return True\r",
            "            return False\r",
            "        except botocore.exceptions.ConnectTimeoutError:\r",
            "            return False\r",
            "\r",
            "        # If we get here, the bucket either existed (unlikely) or the server returned a 200 for some reason\r",
            "        return False\r"
        ],
        "afterPatchFile": [
            "\"\"\"\r",
            "    This will be a service that the client program will instantiate to then call methods\r",
            "    passing buckets\r",
            "\"\"\"\r",
            "from boto3 import client  # TODO: Limit import to just boto3.client, probably\r",
            "from S3Scanner.S3Bucket import S3Bucket, BucketExists, Permission, S3BucketObject\r",
            "from botocore.exceptions import ClientError\r",
            "import botocore.session\r",
            "from botocore import UNSIGNED\r",
            "from botocore.client import Config\r",
            "import datetime\r",
            "from S3Scanner.exceptions import AccessDeniedException, InvalidEndpointException, BucketMightNotExistException\r",
            "import pathlib\r",
            "from concurrent.futures import ThreadPoolExecutor, as_completed\r",
            "from functools import partial\r",
            "from urllib3 import disable_warnings\r",
            "import os\r",
            "\r",
            "\r",
            "ALL_USERS_URI = 'uri=http://acs.amazonaws.com/groups/global/AllUsers'\r",
            "AUTH_USERS_URI = 'uri=http://acs.amazonaws.com/groups/global/AuthenticatedUsers'\r",
            "\r",
            "\r",
            "class S3Service:\r",
            "    def __init__(self, forceNoCreds=False, endpoint_url='https://s3.amazonaws.com', verify_ssl=True,\r",
            "                 endpoint_address_style='path'):\r",
            "        \"\"\"\r",
            "        Service constructor\r",
            "\r",
            "        :param forceNoCreds: (Boolean) Force the service to not use credentials, even if the user has creds configured\r",
            "        :param endpoint_url: (String) URL of S3 endpoint to use. Must include http(s):// scheme\r",
            "        :param verify_ssl: (Boolean) Whether of not to verify ssl. Set to false if endpoint is http\r",
            "        :param endpoint_address_style: (String) Addressing style of the endpoint. Must be 'path' or 'vhost'\r",
            "        :returns None\r",
            "        \"\"\"\r",
            "        self.endpoint_url = endpoint_url\r",
            "        self.endpoint_address_style = 'path' if endpoint_address_style == 'path' else 'virtual'\r",
            "        use_ssl = True if self.endpoint_url.startswith('http://') else False\r",
            "\r",
            "        if not verify_ssl:\r",
            "            disable_warnings()\r",
            "\r",
            "        # DEBUG\r",
            "        # boto3.set_stream_logger(name='botocore')\r",
            "\r",
            "        # Validate the endpoint if it's not the default of AWS\r",
            "        if self.endpoint_url != 'https://s3.amazonaws.com':\r",
            "            if not self.validate_endpoint_url(use_ssl, verify_ssl, endpoint_address_style):\r",
            "                raise InvalidEndpointException(message=f\"Endpoint '{self.endpoint_url}' does not appear to be S3-compliant\")\r",
            "\r",
            "        # Check for AWS credentials\r",
            "        session = botocore.session.get_session()\r",
            "        if forceNoCreds or session.get_credentials() is None or session.get_credentials().access_key is None:\r",
            "            self.aws_creds_configured = False\r",
            "            self.s3_client = client('s3',\r",
            "                                          config=Config(signature_version=UNSIGNED, s3={'addressing_style': self.endpoint_address_style}, connect_timeout=3,\r",
            "                                         retries={'max_attempts': 2}),\r",
            "                                          endpoint_url=self.endpoint_url, use_ssl=use_ssl, verify=verify_ssl)\r",
            "        else:\r",
            "            self.aws_creds_configured = True\r",
            "            self.s3_client = client('s3', config=Config(s3={'addressing_style': self.endpoint_address_style}, connect_timeout=3,\r",
            "                                         retries={'max_attempts': 2}),\r",
            "                                          endpoint_url=self.endpoint_url, use_ssl=use_ssl, verify=verify_ssl)\r",
            "\r",
            "        del session  # No longer needed\r",
            "\r",
            "    def check_bucket_exists(self, bucket):\r",
            "        \"\"\"\r",
            "        Checks if a bucket exists. Sets `exists` property of `bucket`\r",
            "\r",
            "        :param S3Bucket bucket: Bucket to check\r",
            "        :raises ValueError: If `bucket` is not an s3Bucket object\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "        if not isinstance(bucket, S3Bucket):\r",
            "            raise ValueError(\"Passed object was not type S3Bucket\")\r",
            "\r",
            "        bucket_exists = True\r",
            "\r",
            "        try:\r",
            "            self.s3_client.head_bucket(Bucket=bucket.name)\r",
            "        except ClientError as e:\r",
            "            if e.response['Error']['Code'] == '404':\r",
            "                bucket_exists = False\r",
            "\r",
            "        bucket.exists = BucketExists.YES if bucket_exists else BucketExists.NO\r",
            "\r",
            "    def check_perm_read_acl(self, bucket):\r",
            "        \"\"\"\r",
            "        Check for the READACP permission on `bucket` by trying to get the bucket ACL\r",
            "\r",
            "        :param S3Bucket bucket: Bucket to check permission of\r",
            "        :raises BucketMightNotExistException: If `bucket` existence hasn't been checked\r",
            "        :raises ClientError: If we encounter an unexpected ClientError from boto client\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "\r",
            "        if bucket.exists != BucketExists.YES:\r",
            "            raise BucketMightNotExistException()\r",
            "\r",
            "        try:\r",
            "            bucket.foundACL = self.s3_client.get_bucket_acl(Bucket=bucket.name)\r",
            "            self.parse_found_acl(bucket)  # If we can read ACLs, we know the rest of the permissions\r",
            "        except ClientError as e:\r",
            "            if e.response['Error']['Code'] == \"AccessDenied\" or e.response['Error']['Code'] == \"AllAccessDisabled\":\r",
            "                if self.aws_creds_configured:\r",
            "                    bucket.AuthUsersReadACP = Permission.DENIED\r",
            "                else:\r",
            "                    bucket.AllUsersReadACP = Permission.DENIED\r",
            "            else:\r",
            "                raise e\r",
            "\r",
            "    def check_perm_read(self, bucket):\r",
            "        \"\"\"\r",
            "        Checks for the READ permission on the bucket by attempting to list the objects.\r",
            "        Sets the `AllUsersRead` and/or `AuthUsersRead` property of `bucket`.\r",
            "\r",
            "        :param S3Bucket bucket: Bucket to check permission of\r",
            "        :raises BucketMightNotExistException: If `bucket` existence hasn't been checked\r",
            "        :raises ClientError: If we encounter an unexpected ClientError from boto client\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "        if bucket.exists != BucketExists.YES:\r",
            "            raise BucketMightNotExistException()\r",
            "\r",
            "        list_bucket_perm_allowed = True\r",
            "        try:\r",
            "            self.s3_client.list_objects_v2(Bucket=bucket.name, MaxKeys=0)  # TODO: Compare this to doing a HeadBucket\r",
            "        except ClientError as e:\r",
            "            if e.response['Error']['Code'] == \"AccessDenied\" or e.response['Error']['Code'] == \"AllAccessDisabled\":\r",
            "                list_bucket_perm_allowed = False\r",
            "            else:\r",
            "                print(f\"ERROR: Error while checking bucket {bucket.name}\")\r",
            "                raise e\r",
            "        if self.aws_creds_configured:\r",
            "            # Don't mark AuthUsersRead as Allowed if it's only implicitly allowed due to AllUsersRead being allowed\r",
            "            # We only want to make AuthUsersRead as Allowed if that permission is explicitly set for AuthUsers\r",
            "            if bucket.AllUsersRead != Permission.ALLOWED:\r",
            "                bucket.AuthUsersRead = Permission.ALLOWED if list_bucket_perm_allowed else Permission.DENIED\r",
            "        else:\r",
            "            bucket.AllUsersRead = Permission.ALLOWED if list_bucket_perm_allowed else Permission.DENIED\r",
            "\r",
            "    def check_perm_write(self, bucket):\r",
            "        \"\"\"\r",
            "        Check for WRITE permission by trying to upload an empty file to the bucket.\r",
            "        File is named the current timestamp to ensure we're not overwriting an existing file in the bucket.\r",
            "\r",
            "        NOTE: If writing to bucket succeeds using an AuthUser, only mark AuthUsersWrite as Allowed if AllUsers are\r",
            "        Denied. Writing can succeed if AuthUsers are implicitly allowed due to AllUsers being allowed, but we only want\r",
            "        to mark AuthUsers as Allowed if they are explicitly granted. If AllUsersWrite is Allowed and the write is\r",
            "        successful by an AuthUser, we have no way of knowing if AuthUsers were granted permission explicitly\r",
            "\r",
            "        :param S3Bucket bucket: Bucket to check permission of\r",
            "        :raises BucketMightNotExistException: If `bucket` existence hasn't been checked\r",
            "        :raises ClientError: If we encounter an unexpected ClientError from boto client\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "        if bucket.exists != BucketExists.YES:\r",
            "            raise BucketMightNotExistException()\r",
            "\r",
            "        timestamp_file = str(datetime.datetime.now().timestamp()) + '.txt'\r",
            "\r",
            "        try:\r",
            "            # Try to create a new empty file with a key of the timestamp\r",
            "            self.s3_client.put_object(Bucket=bucket.name, Key=timestamp_file, Body=b'')\r",
            "\r",
            "            if self.aws_creds_configured:\r",
            "                if bucket.AllUsersWrite != Permission.ALLOWED:  # If AllUsers have Write permission, don't mark AuthUsers as Allowed\r",
            "                    bucket.AuthUsersWrite = Permission.ALLOWED\r",
            "                else:\r",
            "                    bucket.AuthUsersWrite = Permission.UNKNOWN\r",
            "            else:\r",
            "                bucket.AllUsersWrite = Permission.ALLOWED\r",
            "\r",
            "            # Delete the temporary file\r",
            "            self.s3_client.delete_object(Bucket=bucket.name, Key=timestamp_file)\r",
            "        except ClientError as e:\r",
            "            if e.response['Error']['Code'] == \"AccessDenied\" or e.response['Error']['Code'] == \"AllAccessDisabled\":\r",
            "                if self.aws_creds_configured:\r",
            "                    bucket.AuthUsersWrite = Permission.DENIED\r",
            "                else:\r",
            "                    bucket.AllUsersWrite = Permission.DENIED\r",
            "            else:\r",
            "                raise e\r",
            "\r",
            "    def check_perm_write_acl(self, bucket):\r",
            "        \"\"\"\r",
            "        Checks for WRITE_ACP permission by attempting to set an ACL on the bucket.\r",
            "        WARNING: Potentially destructive - make sure to run this check last as it will include all discovered\r",
            "        permissions in the ACL it tries to set, thus ensuring minimal disruption for the bucket owner.\r",
            "\r",
            "        :param S3Bucket bucket: Bucket to check permission of\r",
            "        :raises BucketMightNotExistException: If `bucket` existence hasn't been checked\r",
            "        :raises ClientError: If we encounter an unexpected ClientError from boto client\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "        if bucket.exists != BucketExists.YES:\r",
            "            raise BucketMightNotExistException()\r",
            "\r",
            "        # TODO: See if there's a way to simplify this section\r",
            "        readURIs = []\r",
            "        writeURIs = []\r",
            "        readAcpURIs = []\r",
            "        writeAcpURIs = []\r",
            "        fullControlURIs = []\r",
            "\r",
            "        if bucket.AuthUsersRead == Permission.ALLOWED:\r",
            "            readURIs.append(AUTH_USERS_URI)\r",
            "        if bucket.AuthUsersWrite == Permission.ALLOWED:\r",
            "            writeURIs.append(AUTH_USERS_URI)\r",
            "        if bucket.AuthUsersReadACP == Permission.ALLOWED:\r",
            "            readAcpURIs.append(AUTH_USERS_URI)\r",
            "        if bucket.AuthUsersWriteACP == Permission.ALLOWED:\r",
            "            writeAcpURIs.append(AUTH_USERS_URI)\r",
            "        if bucket.AuthUsersFullControl == Permission.ALLOWED:\r",
            "            fullControlURIs.append(AUTH_USERS_URI)\r",
            "\r",
            "        if bucket.AllUsersRead == Permission.ALLOWED:\r",
            "            readURIs.append(ALL_USERS_URI)\r",
            "        if bucket.AllUsersWrite == Permission.ALLOWED:\r",
            "            writeURIs.append(ALL_USERS_URI)\r",
            "        if bucket.AllUsersReadACP == Permission.ALLOWED:\r",
            "            readAcpURIs.append(ALL_USERS_URI)\r",
            "        if bucket.AllUsersWriteACP == Permission.ALLOWED:\r",
            "            writeAcpURIs.append(ALL_USERS_URI)\r",
            "        if bucket.AllUsersFullControl == Permission.ALLOWED:\r",
            "            fullControlURIs.append(ALL_USERS_URI)\r",
            "\r",
            "        if self.aws_creds_configured:   # Otherwise AWS will return \"Request was missing a required header\"\r",
            "            writeAcpURIs.append(AUTH_USERS_URI)\r",
            "        else:\r",
            "            writeAcpURIs.append(ALL_USERS_URI)\r",
            "        args = {'Bucket': bucket.name}\r",
            "        if len(readURIs) > 0:\r",
            "            args['GrantRead'] = ','.join(readURIs)\r",
            "        if len(writeURIs) > 0:\r",
            "            args['GrantWrite'] = ','.join(writeURIs)\r",
            "        if len(readAcpURIs) > 0:\r",
            "            args['GrantReadACP'] = ','.join(readAcpURIs)\r",
            "        if len(writeAcpURIs) > 0:\r",
            "            args['GrantWriteACP'] = ','.join(writeAcpURIs)\r",
            "        if len(fullControlURIs) > 0:\r",
            "            args['GrantFullControl'] = ','.join(fullControlURIs)\r",
            "        try:\r",
            "            self.s3_client.put_bucket_acl(**args)\r",
            "            if self.aws_creds_configured:\r",
            "                # Don't mark AuthUsersWriteACP as Allowed if it's due to implicit permission via AllUsersWriteACP\r",
            "                # Only mark it as allowed if the AuthUsers group is explicitly allowed\r",
            "                if bucket.AllUsersWriteACP != Permission.ALLOWED:\r",
            "                    bucket.AuthUsersWriteACP = Permission.ALLOWED\r",
            "                else:\r",
            "                    bucket.AuthUsersWriteACP = Permission.UNKNOWN\r",
            "            else:\r",
            "                bucket.AllUsersWriteACP = Permission.ALLOWED\r",
            "        except ClientError as e:\r",
            "            if e.response['Error']['Code'] == \"AccessDenied\" or e.response['Error']['Code'] == \"AllAccessDisabled\":\r",
            "                if self.aws_creds_configured:\r",
            "                    bucket.AuthUsersWriteACP = Permission.DENIED\r",
            "                else:\r",
            "                    bucket.AllUsersWriteACP = Permission.DENIED\r",
            "            else:\r",
            "                raise e\r",
            "\r",
            "    def dump_bucket_multithread(self, bucket, dest_directory, verbose=False, threads=4):\r",
            "        \"\"\"\r",
            "        Takes a bucket and downloads all the objects to a local folder.\r",
            "        If the object exists locally and is the same size as the remote object, the object is skipped.\r",
            "        If the object exists locally and is a different size then the remote object, the local object is overwritten.\r",
            "\r",
            "        :param S3Bucket bucket: Bucket whose contents we want to dump\r",
            "        :param str dest_directory: Folder to save the objects to. Must include trailing slash\r",
            "        :param bool verbose: Output verbose messages to the user\r",
            "        :param int threads: Number of threads to use while dumping\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "        # TODO: Let the user choose whether or not to overwrite local files if different\r",
            "\r",
            "        print(f\"{bucket.name} | Dumping contents using 4 threads...\")\r",
            "        func = partial(self.download_file, dest_directory, bucket, verbose)\r",
            "\r",
            "        with ThreadPoolExecutor(max_workers=threads) as executor:\r",
            "            futures = {\r",
            "                executor.submit(func, obj): obj for obj in bucket.objects\r",
            "            }\r",
            "\r",
            "            for future in as_completed(futures):\r",
            "                if future.exception():\r",
            "                    print(f\"{bucket.name} | Download failed: {futures[future]} | {future.exception()}\")\r",
            "\r",
            "        print(f\"{bucket.name} | Dumping completed\")\r",
            "\r",
            "    def download_file(self, dest_directory, bucket, verbose, obj):\r",
            "        \"\"\"\r",
            "        Download `obj` from `bucket` into `dest_directory`\r",
            "\r",
            "        :param str dest_directory: Directory to store the object into. _Must_ end in a slash\r",
            "        :param S3Bucket bucket: Bucket to download the object from\r",
            "        :param bool verbose: Output verbose messages to the user\r",
            "        :param S3BucketObject obj: Object to downlaod\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "        dest_file_path = pathlib.Path(os.path.normpath(os.path.join(dest_directory, obj.key)))\r",
            "\r",
            "        if not self.is_safe_file_to_download(obj.key, dest_directory):\r",
            "            print(f\"{bucket.name} | Skipping file {obj.key}. File references a parent directory.\")\r",
            "            return\r",
            "        if dest_file_path.exists():\r",
            "            if dest_file_path.stat().st_size == obj.size:\r",
            "                if verbose:\r",
            "                    print(f\"{bucket.name} | Skipping {obj.key} - already downloaded\")\r",
            "                return\r",
            "            else:\r",
            "                if verbose:\r",
            "                    print(f\"{bucket.name} | Re-downloading {obj.key} - local size differs from remote\")\r",
            "        else:\r",
            "            if verbose:\r",
            "                print(f\"{bucket.name} | Downloading {obj.key}\")\r",
            "        dest_file_path.parent.mkdir(parents=True, exist_ok=True)  # Equivalent to `mkdir -p`\r",
            "        self.s3_client.download_file(bucket.name, obj.key, str(dest_file_path))\r",
            "\r",
            "    def enumerate_bucket_objects(self, bucket):\r",
            "        \"\"\"\r",
            "        Enumerate all the objects in a bucket. Sets the `BucketSize`, `objects`, and `objects_enumerated` properties\r",
            "        of `bucket`.\r",
            "\r",
            "        :param S3Bucket bucket: Bucket to enumerate objects of\r",
            "        :raises Exception: If the bucket doesn't exist\r",
            "        :raises AccessDeniedException: If we are denied access to the bucket objects\r",
            "        :raises ClientError: If we encounter an unexpected ClientError from boto client\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "        if bucket.exists == BucketExists.UNKNOWN:\r",
            "            self.check_bucket_exists(bucket)\r",
            "        if bucket.exists == BucketExists.NO:\r",
            "            raise Exception(\"Bucket doesn't exist\")\r",
            "\r",
            "        try:\r",
            "            for page in self.s3_client.get_paginator(\"list_objects_v2\").paginate(Bucket=bucket.name):\r",
            "                if 'Contents' not in page:  # No items in this bucket\r",
            "                    bucket.objects_enumerated = True\r",
            "                    return\r",
            "                for item in page['Contents']:\r",
            "                    obj = S3BucketObject(key=item['Key'], last_modified=item['LastModified'], size=item['Size'])\r",
            "                    bucket.add_object(obj)\r",
            "        except ClientError as e:\r",
            "            if e.response['Error']['Code'] == \"AccessDenied\" or e.response['Error']['Code'] == \"AllAccessDisabled\":\r",
            "                raise AccessDeniedException(\"AccessDenied while enumerating bucket objects\")\r",
            "        bucket.objects_enumerated = True\r",
            "\r",
            "    def is_safe_file_to_download(self, file_to_check, dest_directory):\r",
            "        \"\"\"\r",
            "        Check if bucket object would be saved outside of `dest_directory` if downloaded.\r",
            "        AWS allows object keys to include relative path characters like '../' which can lead to a \r",
            "        path traversal-like issue where objects get saved outside of the intended directory.\r",
            "\r",
            "        :param string file_to_check: Bucket object key\r",
            "        :param string dest_directory: Path to directory to save file in\r",
            "        :return: bool\r",
            "        \"\"\"\r",
            "        file_to_check = os.path.abspath(os.path.join(dest_directory, file_to_check))\r",
            "        safe_dir = os.path.abspath(dest_directory)\r",
            "        return os.path.commonpath([safe_dir]) == os.path.commonpath([safe_dir, file_to_check])\r",
            "\r",
            "    def parse_found_acl(self, bucket):\r",
            "        \"\"\"\r",
            "        Translate ACL grants into permission properties. If we were able to read the ACLs, we should be able to skip\r",
            "        manually checking most permissions\r",
            "\r",
            "        :param S3Bucket bucket: Bucket whose ACLs we want to parse\r",
            "        :return: None\r",
            "        \"\"\"\r",
            "        if bucket.foundACL is None:\r",
            "            return\r",
            "\r",
            "        if 'Grants' in bucket.foundACL:\r",
            "            for grant in bucket.foundACL['Grants']:\r",
            "                if grant['Grantee']['Type'] == 'Group':\r",
            "                    if 'URI' in grant['Grantee'] and grant['Grantee']['URI'] == 'http://acs.amazonaws.com/groups/global/AuthenticatedUsers':\r",
            "                        # Permissions have been given to the AuthUsers group\r",
            "                        if grant['Permission'] == 'FULL_CONTROL':\r",
            "                            bucket.AuthUsersRead = Permission.ALLOWED\r",
            "                            bucket.AuthUsersWrite = Permission.ALLOWED\r",
            "                            bucket.AuthUsersReadACP = Permission.ALLOWED\r",
            "                            bucket.AuthUsersWriteACP = Permission.ALLOWED\r",
            "                            bucket.AuthUsersFullControl = Permission.ALLOWED\r",
            "                        elif grant['Permission'] == 'READ':\r",
            "                            bucket.AuthUsersRead = Permission.ALLOWED\r",
            "                        elif grant['Permission'] == 'READ_ACP':\r",
            "                            bucket.AuthUsersReadACP = Permission.ALLOWED\r",
            "                        elif grant['Permission'] == 'WRITE':\r",
            "                            bucket.AuthUsersWrite = Permission.ALLOWED\r",
            "                        elif grant['Permission'] == 'WRITE_ACP':\r",
            "                            bucket.AuthUsersWriteACP = Permission.ALLOWED\r",
            "\r",
            "                    elif 'URI' in grant['Grantee'] and grant['Grantee']['URI'] == 'http://acs.amazonaws.com/groups/global/AllUsers':\r",
            "                        # Permissions have been given to the AllUsers group\r",
            "                        if grant['Permission'] == 'FULL_CONTROL':\r",
            "                            bucket.AllUsersRead = Permission.ALLOWED\r",
            "                            bucket.AllUsersWrite = Permission.ALLOWED\r",
            "                            bucket.AllUsersReadACP = Permission.ALLOWED\r",
            "                            bucket.AllUsersWriteACP = Permission.ALLOWED\r",
            "                            bucket.AllUsersFullControl = Permission.ALLOWED\r",
            "                        elif grant['Permission'] == 'READ':\r",
            "                            bucket.AllUsersRead = Permission.ALLOWED\r",
            "                        elif grant['Permission'] == 'READ_ACP':\r",
            "                            bucket.AllUsersReadACP = Permission.ALLOWED\r",
            "                        elif grant['Permission'] == 'WRITE':\r",
            "                            bucket.AllUsersWrite = Permission.ALLOWED\r",
            "                        elif grant['Permission'] == 'WRITE_ACP':\r",
            "                            bucket.AllUsersWriteACP = Permission.ALLOWED\r",
            "\r",
            "            # All permissions not explicitly granted in the ACL are denied\r",
            "            # TODO: Simplify this\r",
            "            if bucket.AuthUsersRead == Permission.UNKNOWN:\r",
            "                bucket.AuthUsersRead = Permission.DENIED\r",
            "\r",
            "            if bucket.AuthUsersWrite == Permission.UNKNOWN:\r",
            "                bucket.AuthUsersWrite = Permission.DENIED\r",
            "\r",
            "            if bucket.AuthUsersReadACP == Permission.UNKNOWN:\r",
            "                bucket.AuthUsersReadACP = Permission.DENIED\r",
            "\r",
            "            if bucket.AuthUsersWriteACP == Permission.UNKNOWN:\r",
            "                bucket.AuthUsersWriteACP = Permission.DENIED\r",
            "\r",
            "            if bucket.AuthUsersFullControl == Permission.UNKNOWN:\r",
            "                bucket.AuthUsersFullControl = Permission.DENIED\r",
            "\r",
            "            if bucket.AllUsersRead == Permission.UNKNOWN:\r",
            "                bucket.AllUsersRead = Permission.DENIED\r",
            "\r",
            "            if bucket.AllUsersWrite == Permission.UNKNOWN:\r",
            "                bucket.AllUsersWrite = Permission.DENIED\r",
            "\r",
            "            if bucket.AllUsersReadACP == Permission.UNKNOWN:\r",
            "                bucket.AllUsersReadACP = Permission.DENIED\r",
            "\r",
            "            if bucket.AllUsersWriteACP == Permission.UNKNOWN:\r",
            "                bucket.AllUsersWriteACP = Permission.DENIED\r",
            "\r",
            "            if bucket.AllUsersFullControl == Permission.UNKNOWN:\r",
            "                bucket.AllUsersFullControl = Permission.DENIED\r",
            "\r",
            "    def validate_endpoint_url(self, use_ssl=True, verify_ssl=True, endpoint_address_style='path'):\r",
            "        \"\"\"\r",
            "        Verify the user-supplied endpoint URL is S3-compliant by trying to list a maximum of 0 keys from a bucket which\r",
            "        is extremely unlikely to exist.\r",
            "\r",
            "        Note: Most S3-compliant services will return an error code of \"NoSuchBucket\". Some services which require auth\r",
            "        for most operations (like Minio) will return an error code of \"AccessDenied\" instead\r",
            "\r",
            "        :param bool use_ssl: Whether or not the endpoint serves HTTP over SSL\r",
            "        :param bool verify_ssl: Whether or not to verify the SSL connection.\r",
            "        :param str endpoint_address_style: Addressing style of endpoint. Must be either 'path' or 'vhost'\r",
            "        :return: bool: Whether or not the server responded in an S3-compliant way\r",
            "        \"\"\"\r",
            "\r",
            "        # We always want to verify the endpoint using no creds\r",
            "        # so if the s3_client has creds configured, make a new anonymous client\r",
            "\r",
            "        addressing_style = 'virtual' if endpoint_address_style == 'vhost' else 'path'\r",
            "\r",
            "        validation_client = client('s3', config=Config(signature_version=UNSIGNED,\r",
            "                                         s3={'addressing_style': addressing_style}, connect_timeout=3,\r",
            "                                         retries={'max_attempts': 0}), endpoint_url=self.endpoint_url, use_ssl=use_ssl,\r",
            "                                         verify=verify_ssl)\r",
            "\r",
            "        non_existent_bucket = 's3scanner-' + str(datetime.datetime.now())[0:10]\r",
            "        try:\r",
            "            validation_client.list_objects_v2(Bucket=non_existent_bucket, MaxKeys=0)\r",
            "        except ClientError as e:\r",
            "            if e.response['Error']['Code'] == 'NoSuchBucket' or e.response['Error']['Code'] == 'AccessDenied':\r",
            "                return True\r",
            "            return False\r",
            "        except botocore.exceptions.ConnectTimeoutError:\r",
            "            return False\r",
            "\r",
            "        # If we get here, the bucket either existed (unlikely) or the server returned a 200 for some reason\r",
            "        return False\r"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "13": [],
            "287": [
                "S3Service",
                "dump_bucket_multithread"
            ],
            "295": [
                "S3Service",
                "download_file"
            ],
            "301": [
                "S3Service",
                "download_file"
            ]
        },
        "addLocation": []
    },
    "S3Scanner/__main__.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " from concurrent.futures import ThreadPoolExecutor, as_completed"
            },
            "1": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " from .exceptions import InvalidEndpointException"
            },
            "2": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-CURRENT_VERSION = '2.0.1'"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 19,
                "PatchRowcode": "+CURRENT_VERSION = '2.0.2'"
            },
            "5": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " AWS_ENDPOINT = 'https://s3.amazonaws.com'"
            },
            "6": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "#########",
            "#",
            "# S3scanner - Audit unsecured S3 buckets",
            "# ",
            "# Author:  Dan Salmon (twitter.com/bltjetpack, github.com/sa7mon)",
            "# Created: 6/19/17",
            "# License: MIT",
            "#",
            "#########",
            "",
            "import argparse",
            "from os import path",
            "from sys import exit",
            "from .S3Bucket import S3Bucket, BucketExists, Permission",
            "from .S3Service import S3Service",
            "from concurrent.futures import ThreadPoolExecutor, as_completed",
            "from .exceptions import InvalidEndpointException",
            "",
            "CURRENT_VERSION = '2.0.1'",
            "AWS_ENDPOINT = 'https://s3.amazonaws.com'",
            "",
            "",
            "# We want to use both formatter classes, so a custom class it is",
            "class CustomFormatter(argparse.RawTextHelpFormatter, argparse.RawDescriptionHelpFormatter):",
            "    pass",
            "",
            "",
            "def load_bucket_names_from_file(file_name):",
            "    \"\"\"",
            "    Load in bucket names from a text file",
            "",
            "    :param str file_name: Path to text file",
            "    :return: set: All lines of text file",
            "    \"\"\"",
            "    buckets = set()",
            "    if path.isfile(file_name):",
            "        with open(file_name, 'r') as f:",
            "            for line in f:",
            "                line = line.rstrip()  # Remove any extra whitespace",
            "                buckets.add(line)",
            "        return buckets",
            "    else:",
            "        print(\"Error: '%s' is not a file\" % file_name)",
            "        exit(1)",
            "",
            "",
            "def scan_single_bucket(s3service, anons3service, do_dangerous, bucket_name):",
            "    \"\"\"",
            "    Scans a single bucket for permission issues. Exists on its own so we can do multi-threading",
            "",
            "    :param S3Service s3service: S3Service with credentials to use for scanning",
            "    :param S3Service anonS3Service: S3Service without credentials to use for scanning",
            "    :param bool do_dangerous: Whether or not to do dangerous checks",
            "    :param str bucket_name: Name of bucket to check",
            "    :return: None",
            "    \"\"\"",
            "    try:",
            "        b = S3Bucket(bucket_name)",
            "    except ValueError as ve:",
            "        if str(ve) == \"Invalid bucket name\":",
            "            print(f\"{bucket_name} | bucket_invalid_name\")",
            "            return",
            "        else:",
            "            print(f\"{bucket_name} | {str(ve)}\")",
            "            return",
            "",
            "    # Check if bucket exists first",
            "    # Use credentials if configured and if we're hitting AWS. Otherwise, check anonymously",
            "    if s3service.endpoint_url == AWS_ENDPOINT:",
            "        s3service.check_bucket_exists(b)",
            "    else:",
            "        anons3service.check_bucket_exists(b)",
            "",
            "    if b.exists == BucketExists.NO:",
            "        print(f\"{b.name} | bucket_not_exist\")",
            "        return",
            "    checkAllUsersPerms = True",
            "    checkAuthUsersPerms = True",
            "",
            "    # 1. Check for ReadACP",
            "    anons3service.check_perm_read_acl(b)  # Check for AllUsers",
            "    if s3service.aws_creds_configured and s3service.endpoint_url == AWS_ENDPOINT:",
            "        s3service.check_perm_read_acl(b)  # Check for AuthUsers",
            "",
            "    # If FullControl is allowed for either AllUsers or AnonUsers, skip the remainder of those tests",
            "    if b.AuthUsersFullControl == Permission.ALLOWED:",
            "        checkAuthUsersPerms = False",
            "    if b.AllUsersFullControl == Permission.ALLOWED:",
            "        checkAllUsersPerms = False",
            "",
            "    # 2. Check for Read",
            "    if checkAllUsersPerms:",
            "        anons3service.check_perm_read(b)",
            "    if s3service.aws_creds_configured and checkAuthUsersPerms and s3service.endpoint_url == AWS_ENDPOINT:",
            "        s3service.check_perm_read(b)",
            "",
            "    # Do dangerous/destructive checks",
            "    if do_dangerous:",
            "        # 3. Check for Write",
            "        if checkAllUsersPerms:",
            "            anons3service.check_perm_write(b)",
            "        if s3service.aws_creds_configured and checkAuthUsersPerms:",
            "            s3service.check_perm_write(b)",
            "",
            "        # 4. Check for WriteACP",
            "        if checkAllUsersPerms:",
            "            anons3service.check_perm_write_acl(b)",
            "        if s3service.aws_creds_configured and checkAuthUsersPerms:",
            "            s3service.check_perm_write_acl(b)",
            "",
            "    print(f\"{b.name} | bucket_exists | {b.get_human_readable_permissions()}\")",
            "",
            "",
            "def main():",
            "    # Instantiate the parser",
            "    parser = argparse.ArgumentParser(description='s3scanner: Audit unsecured S3 buckets\\n'",
            "                                                 '           by Dan Salmon - github.com/sa7mon, @bltjetpack\\n',",
            "                                     prog='s3scanner', allow_abbrev=False, formatter_class=CustomFormatter)",
            "    # Declare arguments",
            "    parser.add_argument('--version', action='version', version=CURRENT_VERSION,",
            "                        help='Display the current version of this tool')",
            "    parser.add_argument('--threads', '-t', type=int, default=4, dest='threads', help='Number of threads to use. Default: 4',",
            "                        metavar='n')",
            "    parser.add_argument('--endpoint-url', '-u', dest='endpoint_url',",
            "                        help='URL of S3-compliant API. Default: https://s3.amazonaws.com',",
            "                        default='https://s3.amazonaws.com')",
            "    parser.add_argument('--endpoint-address-style', '-s', dest='endpoint_address_style', choices=['path', 'vhost'],",
            "                        default='path', help='Address style to use for the endpoint. Default: path')",
            "    parser.add_argument('--insecure', '-i', dest='verify_ssl', action='store_false', help='Do not verify SSL')",
            "    subparsers = parser.add_subparsers(title='mode', dest='mode', help='(Must choose one)')",
            "",
            "    # Scan mode",
            "    parser_scan = subparsers.add_parser('scan', help='Scan bucket permissions')",
            "    parser_scan.add_argument('--dangerous', action='store_true',",
            "                             help='Include Write and WriteACP permissions checks (potentially destructive)')",
            "    parser_group = parser_scan.add_mutually_exclusive_group(required=True)",
            "    parser_group.add_argument('--buckets-file', '-f', dest='buckets_file',",
            "                              help='Name of text file containing bucket names to check', metavar='file')",
            "    parser_group.add_argument('--bucket', '-b', dest='bucket', help='Name of bucket to check', metavar='bucket')",
            "    # TODO: Get help output to not repeat metavar names - i.e. `--bucket FILE, -f FILE`",
            "    #   https://stackoverflow.com/a/9643162/2307994",
            "",
            "    # Dump mode",
            "    parser_dump = subparsers.add_parser('dump', help='Dump the contents of buckets')",
            "    parser_dump.add_argument('--dump-dir', '-d', required=True, dest='dump_dir', help='Directory to dump bucket into')",
            "    dump_parser_group = parser_dump.add_mutually_exclusive_group(required=True)",
            "    dump_parser_group.add_argument('--buckets-file', '-f', dest='dump_buckets_file',",
            "                                   help='Name of text file containing bucket names to check', metavar='file')",
            "    dump_parser_group.add_argument('--bucket', '-b', dest='dump_bucket', help='Name of bucket to check', metavar='bucket')",
            "    parser_dump.add_argument('--verbose', '-v', dest='dump_verbose', action='store_true',",
            "                             help='Enable verbose output while dumping bucket(s)')",
            "",
            "    # Parse the args",
            "    args = parser.parse_args()",
            "",
            "    if 'http://' not in args.endpoint_url and 'https://' not in args.endpoint_url:",
            "        print(\"Error: endpoint_url must start with http:// or https:// scheme\")",
            "        exit(1)",
            "",
            "    s3service = None",
            "    anons3service = None",
            "    try:",
            "        s3service = S3Service(endpoint_url=args.endpoint_url, verify_ssl=args.verify_ssl, endpoint_address_style=args.endpoint_address_style)",
            "        anons3service = S3Service(forceNoCreds=True, endpoint_url=args.endpoint_url, verify_ssl=args.verify_ssl, endpoint_address_style=args.endpoint_address_style)",
            "    except InvalidEndpointException as e:",
            "        print(f\"Error: {e.message}\")",
            "        exit(1)",
            "",
            "    if s3service.aws_creds_configured is False:",
            "        print(\"Warning: AWS credentials not configured - functionality will be limited. Run:\"",
            "              \" `aws configure` to fix this.\\n\")",
            "",
            "    bucketsIn = set()",
            "",
            "    if args.mode == 'scan':",
            "        if args.buckets_file is not None:",
            "            bucketsIn = load_bucket_names_from_file(args.buckets_file)",
            "        elif args.bucket is not None:",
            "            bucketsIn.add(args.bucket)",
            "",
            "        if args.dangerous:",
            "            print(\"INFO: Including dangerous checks. WARNING: This may change bucket ACL destructively\")",
            "",
            "        with ThreadPoolExecutor(max_workers=args.threads) as executor:",
            "            futures = {",
            "                executor.submit(scan_single_bucket, s3service, anons3service, args.dangerous, bucketName): bucketName for bucketName in bucketsIn",
            "            }",
            "            for future in as_completed(futures):",
            "                if future.exception():",
            "                    print(f\"Bucket scan raised exception: {futures[future]} - {future.exception()}\")",
            "",
            "    elif args.mode == 'dump':",
            "        if args.dump_dir is None or not path.isdir(args.dump_dir):",
            "            print(\"Error: Given --dump-dir does not exist or is not a directory\")",
            "            exit(1)",
            "        if args.dump_buckets_file is not None:",
            "            bucketsIn = load_bucket_names_from_file(args.dump_buckets_file)",
            "        elif args.dump_bucket is not None:",
            "            bucketsIn.add(args.dump_bucket)",
            "",
            "        for bucketName in bucketsIn:",
            "            try:",
            "                b = S3Bucket(bucketName)",
            "            except ValueError as ve:",
            "                if str(ve) == \"Invalid bucket name\":",
            "                    print(f\"{bucketName} | bucket_name_invalid\")",
            "                    continue",
            "                else:",
            "                    print(f\"{bucketName} | {str(ve)}\")",
            "                    continue",
            "",
            "            # Check if bucket exists first",
            "            s3service.check_bucket_exists(b)",
            "",
            "            if b.exists == BucketExists.NO:",
            "                print(f\"{b.name} | bucket_not_exist\")",
            "                continue",
            "",
            "            s3service.check_perm_read(b)",
            "",
            "            if b.AuthUsersRead != Permission.ALLOWED:",
            "                anons3service.check_perm_read(b)",
            "                if b.AllUsersRead != Permission.ALLOWED:",
            "                    print(f\"{b.name} | Error: no read permissions\")",
            "                else:",
            "                    # Dump bucket without creds",
            "                    print(f\"{b.name} | Enumerating bucket objects...\")",
            "                    anons3service.enumerate_bucket_objects(b)",
            "                    print(f\"{b.name} | Total Objects: {str(len(b.objects))}, Total Size: {b.get_human_readable_size()}\")",
            "                    anons3service.dump_bucket_multithread(bucket=b, dest_directory=args.dump_dir,",
            "                                                          verbose=args.dump_verbose, threads=args.threads)",
            "            else:",
            "                # Dump bucket with creds",
            "                print(f\"{b.name} | Enumerating bucket objects...\")",
            "                s3service.enumerate_bucket_objects(b)",
            "                print(f\"{b.name} | Total Objects: {str(len(b.objects))}, Total Size: {b.get_human_readable_size()}\")",
            "                s3service.dump_bucket_multithread(bucket=b, dest_directory=args.dump_dir,",
            "                                                  verbose=args.dump_verbose, threads=args.threads)",
            "    else:",
            "        print(\"Invalid mode\")",
            "        parser.print_help()",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    main()"
        ],
        "afterPatchFile": [
            "#########",
            "#",
            "# S3scanner - Audit unsecured S3 buckets",
            "# ",
            "# Author:  Dan Salmon (twitter.com/bltjetpack, github.com/sa7mon)",
            "# Created: 6/19/17",
            "# License: MIT",
            "#",
            "#########",
            "",
            "import argparse",
            "from os import path",
            "from sys import exit",
            "from .S3Bucket import S3Bucket, BucketExists, Permission",
            "from .S3Service import S3Service",
            "from concurrent.futures import ThreadPoolExecutor, as_completed",
            "from .exceptions import InvalidEndpointException",
            "",
            "CURRENT_VERSION = '2.0.2'",
            "AWS_ENDPOINT = 'https://s3.amazonaws.com'",
            "",
            "",
            "# We want to use both formatter classes, so a custom class it is",
            "class CustomFormatter(argparse.RawTextHelpFormatter, argparse.RawDescriptionHelpFormatter):",
            "    pass",
            "",
            "",
            "def load_bucket_names_from_file(file_name):",
            "    \"\"\"",
            "    Load in bucket names from a text file",
            "",
            "    :param str file_name: Path to text file",
            "    :return: set: All lines of text file",
            "    \"\"\"",
            "    buckets = set()",
            "    if path.isfile(file_name):",
            "        with open(file_name, 'r') as f:",
            "            for line in f:",
            "                line = line.rstrip()  # Remove any extra whitespace",
            "                buckets.add(line)",
            "        return buckets",
            "    else:",
            "        print(\"Error: '%s' is not a file\" % file_name)",
            "        exit(1)",
            "",
            "",
            "def scan_single_bucket(s3service, anons3service, do_dangerous, bucket_name):",
            "    \"\"\"",
            "    Scans a single bucket for permission issues. Exists on its own so we can do multi-threading",
            "",
            "    :param S3Service s3service: S3Service with credentials to use for scanning",
            "    :param S3Service anonS3Service: S3Service without credentials to use for scanning",
            "    :param bool do_dangerous: Whether or not to do dangerous checks",
            "    :param str bucket_name: Name of bucket to check",
            "    :return: None",
            "    \"\"\"",
            "    try:",
            "        b = S3Bucket(bucket_name)",
            "    except ValueError as ve:",
            "        if str(ve) == \"Invalid bucket name\":",
            "            print(f\"{bucket_name} | bucket_invalid_name\")",
            "            return",
            "        else:",
            "            print(f\"{bucket_name} | {str(ve)}\")",
            "            return",
            "",
            "    # Check if bucket exists first",
            "    # Use credentials if configured and if we're hitting AWS. Otherwise, check anonymously",
            "    if s3service.endpoint_url == AWS_ENDPOINT:",
            "        s3service.check_bucket_exists(b)",
            "    else:",
            "        anons3service.check_bucket_exists(b)",
            "",
            "    if b.exists == BucketExists.NO:",
            "        print(f\"{b.name} | bucket_not_exist\")",
            "        return",
            "    checkAllUsersPerms = True",
            "    checkAuthUsersPerms = True",
            "",
            "    # 1. Check for ReadACP",
            "    anons3service.check_perm_read_acl(b)  # Check for AllUsers",
            "    if s3service.aws_creds_configured and s3service.endpoint_url == AWS_ENDPOINT:",
            "        s3service.check_perm_read_acl(b)  # Check for AuthUsers",
            "",
            "    # If FullControl is allowed for either AllUsers or AnonUsers, skip the remainder of those tests",
            "    if b.AuthUsersFullControl == Permission.ALLOWED:",
            "        checkAuthUsersPerms = False",
            "    if b.AllUsersFullControl == Permission.ALLOWED:",
            "        checkAllUsersPerms = False",
            "",
            "    # 2. Check for Read",
            "    if checkAllUsersPerms:",
            "        anons3service.check_perm_read(b)",
            "    if s3service.aws_creds_configured and checkAuthUsersPerms and s3service.endpoint_url == AWS_ENDPOINT:",
            "        s3service.check_perm_read(b)",
            "",
            "    # Do dangerous/destructive checks",
            "    if do_dangerous:",
            "        # 3. Check for Write",
            "        if checkAllUsersPerms:",
            "            anons3service.check_perm_write(b)",
            "        if s3service.aws_creds_configured and checkAuthUsersPerms:",
            "            s3service.check_perm_write(b)",
            "",
            "        # 4. Check for WriteACP",
            "        if checkAllUsersPerms:",
            "            anons3service.check_perm_write_acl(b)",
            "        if s3service.aws_creds_configured and checkAuthUsersPerms:",
            "            s3service.check_perm_write_acl(b)",
            "",
            "    print(f\"{b.name} | bucket_exists | {b.get_human_readable_permissions()}\")",
            "",
            "",
            "def main():",
            "    # Instantiate the parser",
            "    parser = argparse.ArgumentParser(description='s3scanner: Audit unsecured S3 buckets\\n'",
            "                                                 '           by Dan Salmon - github.com/sa7mon, @bltjetpack\\n',",
            "                                     prog='s3scanner', allow_abbrev=False, formatter_class=CustomFormatter)",
            "    # Declare arguments",
            "    parser.add_argument('--version', action='version', version=CURRENT_VERSION,",
            "                        help='Display the current version of this tool')",
            "    parser.add_argument('--threads', '-t', type=int, default=4, dest='threads', help='Number of threads to use. Default: 4',",
            "                        metavar='n')",
            "    parser.add_argument('--endpoint-url', '-u', dest='endpoint_url',",
            "                        help='URL of S3-compliant API. Default: https://s3.amazonaws.com',",
            "                        default='https://s3.amazonaws.com')",
            "    parser.add_argument('--endpoint-address-style', '-s', dest='endpoint_address_style', choices=['path', 'vhost'],",
            "                        default='path', help='Address style to use for the endpoint. Default: path')",
            "    parser.add_argument('--insecure', '-i', dest='verify_ssl', action='store_false', help='Do not verify SSL')",
            "    subparsers = parser.add_subparsers(title='mode', dest='mode', help='(Must choose one)')",
            "",
            "    # Scan mode",
            "    parser_scan = subparsers.add_parser('scan', help='Scan bucket permissions')",
            "    parser_scan.add_argument('--dangerous', action='store_true',",
            "                             help='Include Write and WriteACP permissions checks (potentially destructive)')",
            "    parser_group = parser_scan.add_mutually_exclusive_group(required=True)",
            "    parser_group.add_argument('--buckets-file', '-f', dest='buckets_file',",
            "                              help='Name of text file containing bucket names to check', metavar='file')",
            "    parser_group.add_argument('--bucket', '-b', dest='bucket', help='Name of bucket to check', metavar='bucket')",
            "    # TODO: Get help output to not repeat metavar names - i.e. `--bucket FILE, -f FILE`",
            "    #   https://stackoverflow.com/a/9643162/2307994",
            "",
            "    # Dump mode",
            "    parser_dump = subparsers.add_parser('dump', help='Dump the contents of buckets')",
            "    parser_dump.add_argument('--dump-dir', '-d', required=True, dest='dump_dir', help='Directory to dump bucket into')",
            "    dump_parser_group = parser_dump.add_mutually_exclusive_group(required=True)",
            "    dump_parser_group.add_argument('--buckets-file', '-f', dest='dump_buckets_file',",
            "                                   help='Name of text file containing bucket names to check', metavar='file')",
            "    dump_parser_group.add_argument('--bucket', '-b', dest='dump_bucket', help='Name of bucket to check', metavar='bucket')",
            "    parser_dump.add_argument('--verbose', '-v', dest='dump_verbose', action='store_true',",
            "                             help='Enable verbose output while dumping bucket(s)')",
            "",
            "    # Parse the args",
            "    args = parser.parse_args()",
            "",
            "    if 'http://' not in args.endpoint_url and 'https://' not in args.endpoint_url:",
            "        print(\"Error: endpoint_url must start with http:// or https:// scheme\")",
            "        exit(1)",
            "",
            "    s3service = None",
            "    anons3service = None",
            "    try:",
            "        s3service = S3Service(endpoint_url=args.endpoint_url, verify_ssl=args.verify_ssl, endpoint_address_style=args.endpoint_address_style)",
            "        anons3service = S3Service(forceNoCreds=True, endpoint_url=args.endpoint_url, verify_ssl=args.verify_ssl, endpoint_address_style=args.endpoint_address_style)",
            "    except InvalidEndpointException as e:",
            "        print(f\"Error: {e.message}\")",
            "        exit(1)",
            "",
            "    if s3service.aws_creds_configured is False:",
            "        print(\"Warning: AWS credentials not configured - functionality will be limited. Run:\"",
            "              \" `aws configure` to fix this.\\n\")",
            "",
            "    bucketsIn = set()",
            "",
            "    if args.mode == 'scan':",
            "        if args.buckets_file is not None:",
            "            bucketsIn = load_bucket_names_from_file(args.buckets_file)",
            "        elif args.bucket is not None:",
            "            bucketsIn.add(args.bucket)",
            "",
            "        if args.dangerous:",
            "            print(\"INFO: Including dangerous checks. WARNING: This may change bucket ACL destructively\")",
            "",
            "        with ThreadPoolExecutor(max_workers=args.threads) as executor:",
            "            futures = {",
            "                executor.submit(scan_single_bucket, s3service, anons3service, args.dangerous, bucketName): bucketName for bucketName in bucketsIn",
            "            }",
            "            for future in as_completed(futures):",
            "                if future.exception():",
            "                    print(f\"Bucket scan raised exception: {futures[future]} - {future.exception()}\")",
            "",
            "    elif args.mode == 'dump':",
            "        if args.dump_dir is None or not path.isdir(args.dump_dir):",
            "            print(\"Error: Given --dump-dir does not exist or is not a directory\")",
            "            exit(1)",
            "        if args.dump_buckets_file is not None:",
            "            bucketsIn = load_bucket_names_from_file(args.dump_buckets_file)",
            "        elif args.dump_bucket is not None:",
            "            bucketsIn.add(args.dump_bucket)",
            "",
            "        for bucketName in bucketsIn:",
            "            try:",
            "                b = S3Bucket(bucketName)",
            "            except ValueError as ve:",
            "                if str(ve) == \"Invalid bucket name\":",
            "                    print(f\"{bucketName} | bucket_name_invalid\")",
            "                    continue",
            "                else:",
            "                    print(f\"{bucketName} | {str(ve)}\")",
            "                    continue",
            "",
            "            # Check if bucket exists first",
            "            s3service.check_bucket_exists(b)",
            "",
            "            if b.exists == BucketExists.NO:",
            "                print(f\"{b.name} | bucket_not_exist\")",
            "                continue",
            "",
            "            s3service.check_perm_read(b)",
            "",
            "            if b.AuthUsersRead != Permission.ALLOWED:",
            "                anons3service.check_perm_read(b)",
            "                if b.AllUsersRead != Permission.ALLOWED:",
            "                    print(f\"{b.name} | Error: no read permissions\")",
            "                else:",
            "                    # Dump bucket without creds",
            "                    print(f\"{b.name} | Enumerating bucket objects...\")",
            "                    anons3service.enumerate_bucket_objects(b)",
            "                    print(f\"{b.name} | Total Objects: {str(len(b.objects))}, Total Size: {b.get_human_readable_size()}\")",
            "                    anons3service.dump_bucket_multithread(bucket=b, dest_directory=args.dump_dir,",
            "                                                          verbose=args.dump_verbose, threads=args.threads)",
            "            else:",
            "                # Dump bucket with creds",
            "                print(f\"{b.name} | Enumerating bucket objects...\")",
            "                s3service.enumerate_bucket_objects(b)",
            "                print(f\"{b.name} | Total Objects: {str(len(b.objects))}, Total Size: {b.get_human_readable_size()}\")",
            "                s3service.dump_bucket_multithread(bucket=b, dest_directory=args.dump_dir,",
            "                                                  verbose=args.dump_verbose, threads=args.threads)",
            "    else:",
            "        print(\"Invalid mode\")",
            "        parser.print_help()",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    main()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "19": [
                "CURRENT_VERSION"
            ]
        },
        "addLocation": []
    }
}