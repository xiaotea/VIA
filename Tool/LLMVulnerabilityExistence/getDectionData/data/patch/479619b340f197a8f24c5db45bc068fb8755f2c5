{
    "scrapy/spiders/feed.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " from scrapy.exceptions import NotConfigured, NotSupported"
            },
            "1": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " from scrapy.selector import Selector"
            },
            "2": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " from scrapy.spiders import Spider"
            },
            "3": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from scrapy.utils.iterators import csviter, xmliter"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 10,
                "PatchRowcode": "+from scrapy.utils.iterators import csviter, xmliter_lxml"
            },
            "5": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " from scrapy.utils.spider import iterate_spider_output"
            },
            "6": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 84,
                "PatchRowcode": "         return self.parse_nodes(response, nodes)"
            },
            "9": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 85,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 86,
                "PatchRowcode": "     def _iternodes(self, response):"
            },
            "11": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        for node in xmliter(response, self.itertag):"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+        for node in xmliter_lxml(response, self.itertag):"
            },
            "13": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": 88,
                "PatchRowcode": "             self._register_namespaces(node)"
            },
            "14": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": 89,
                "PatchRowcode": "             yield node"
            },
            "15": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 90,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "This module implements the XMLFeedSpider which is the recommended spider to use",
            "for scraping from an XML feed.",
            "",
            "See documentation in docs/topics/spiders.rst",
            "\"\"\"",
            "from scrapy.exceptions import NotConfigured, NotSupported",
            "from scrapy.selector import Selector",
            "from scrapy.spiders import Spider",
            "from scrapy.utils.iterators import csviter, xmliter",
            "from scrapy.utils.spider import iterate_spider_output",
            "",
            "",
            "class XMLFeedSpider(Spider):",
            "    \"\"\"",
            "    This class intends to be the base class for spiders that scrape",
            "    from XML feeds.",
            "",
            "    You can choose whether to parse the file using the 'iternodes' iterator, an",
            "    'xml' selector, or an 'html' selector.  In most cases, it's convenient to",
            "    use iternodes, since it's a faster and cleaner.",
            "    \"\"\"",
            "",
            "    iterator = \"iternodes\"",
            "    itertag = \"item\"",
            "    namespaces = ()",
            "",
            "    def process_results(self, response, results):",
            "        \"\"\"This overridable method is called for each result (item or request)",
            "        returned by the spider, and it's intended to perform any last time",
            "        processing required before returning the results to the framework core,",
            "        for example setting the item GUIDs. It receives a list of results and",
            "        the response which originated that results. It must return a list of",
            "        results (items or requests).",
            "        \"\"\"",
            "        return results",
            "",
            "    def adapt_response(self, response):",
            "        \"\"\"You can override this function in order to make any changes you want",
            "        to into the feed before parsing it. This function must return a",
            "        response.",
            "        \"\"\"",
            "        return response",
            "",
            "    def parse_node(self, response, selector):",
            "        \"\"\"This method must be overridden with your custom spider functionality\"\"\"",
            "        if hasattr(self, \"parse_item\"):  # backward compatibility",
            "            return self.parse_item(response, selector)",
            "        raise NotImplementedError",
            "",
            "    def parse_nodes(self, response, nodes):",
            "        \"\"\"This method is called for the nodes matching the provided tag name",
            "        (itertag). Receives the response and an Selector for each node.",
            "        Overriding this method is mandatory. Otherwise, you spider won't work.",
            "        This method must return either an item, a request, or a list",
            "        containing any of them.",
            "        \"\"\"",
            "",
            "        for selector in nodes:",
            "            ret = iterate_spider_output(self.parse_node(response, selector))",
            "            for result_item in self.process_results(response, ret):",
            "                yield result_item",
            "",
            "    def _parse(self, response, **kwargs):",
            "        if not hasattr(self, \"parse_node\"):",
            "            raise NotConfigured(",
            "                \"You must define parse_node method in order to scrape this XML feed\"",
            "            )",
            "",
            "        response = self.adapt_response(response)",
            "        if self.iterator == \"iternodes\":",
            "            nodes = self._iternodes(response)",
            "        elif self.iterator == \"xml\":",
            "            selector = Selector(response, type=\"xml\")",
            "            self._register_namespaces(selector)",
            "            nodes = selector.xpath(f\"//{self.itertag}\")",
            "        elif self.iterator == \"html\":",
            "            selector = Selector(response, type=\"html\")",
            "            self._register_namespaces(selector)",
            "            nodes = selector.xpath(f\"//{self.itertag}\")",
            "        else:",
            "            raise NotSupported(\"Unsupported node iterator\")",
            "",
            "        return self.parse_nodes(response, nodes)",
            "",
            "    def _iternodes(self, response):",
            "        for node in xmliter(response, self.itertag):",
            "            self._register_namespaces(node)",
            "            yield node",
            "",
            "    def _register_namespaces(self, selector):",
            "        for prefix, uri in self.namespaces:",
            "            selector.register_namespace(prefix, uri)",
            "",
            "",
            "class CSVFeedSpider(Spider):",
            "    \"\"\"Spider for parsing CSV feeds.",
            "    It receives a CSV file in a response; iterates through each of its rows,",
            "    and calls parse_row with a dict containing each field's data.",
            "",
            "    You can set some options regarding the CSV file, such as the delimiter, quotechar",
            "    and the file's headers.",
            "    \"\"\"",
            "",
            "    delimiter = (",
            "        None  # When this is None, python's csv module's default delimiter is used",
            "    )",
            "    quotechar = (",
            "        None  # When this is None, python's csv module's default quotechar is used",
            "    )",
            "    headers = None",
            "",
            "    def process_results(self, response, results):",
            "        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"",
            "        return results",
            "",
            "    def adapt_response(self, response):",
            "        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"",
            "        return response",
            "",
            "    def parse_row(self, response, row):",
            "        \"\"\"This method must be overridden with your custom spider functionality\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def parse_rows(self, response):",
            "        \"\"\"Receives a response and a dict (representing each row) with a key for",
            "        each provided (or detected) header of the CSV file.  This spider also",
            "        gives the opportunity to override adapt_response and",
            "        process_results methods for pre and post-processing purposes.",
            "        \"\"\"",
            "",
            "        for row in csviter(",
            "            response, self.delimiter, self.headers, quotechar=self.quotechar",
            "        ):",
            "            ret = iterate_spider_output(self.parse_row(response, row))",
            "            for result_item in self.process_results(response, ret):",
            "                yield result_item",
            "",
            "    def _parse(self, response, **kwargs):",
            "        if not hasattr(self, \"parse_row\"):",
            "            raise NotConfigured(",
            "                \"You must define parse_row method in order to scrape this CSV feed\"",
            "            )",
            "        response = self.adapt_response(response)",
            "        return self.parse_rows(response)"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "This module implements the XMLFeedSpider which is the recommended spider to use",
            "for scraping from an XML feed.",
            "",
            "See documentation in docs/topics/spiders.rst",
            "\"\"\"",
            "from scrapy.exceptions import NotConfigured, NotSupported",
            "from scrapy.selector import Selector",
            "from scrapy.spiders import Spider",
            "from scrapy.utils.iterators import csviter, xmliter_lxml",
            "from scrapy.utils.spider import iterate_spider_output",
            "",
            "",
            "class XMLFeedSpider(Spider):",
            "    \"\"\"",
            "    This class intends to be the base class for spiders that scrape",
            "    from XML feeds.",
            "",
            "    You can choose whether to parse the file using the 'iternodes' iterator, an",
            "    'xml' selector, or an 'html' selector.  In most cases, it's convenient to",
            "    use iternodes, since it's a faster and cleaner.",
            "    \"\"\"",
            "",
            "    iterator = \"iternodes\"",
            "    itertag = \"item\"",
            "    namespaces = ()",
            "",
            "    def process_results(self, response, results):",
            "        \"\"\"This overridable method is called for each result (item or request)",
            "        returned by the spider, and it's intended to perform any last time",
            "        processing required before returning the results to the framework core,",
            "        for example setting the item GUIDs. It receives a list of results and",
            "        the response which originated that results. It must return a list of",
            "        results (items or requests).",
            "        \"\"\"",
            "        return results",
            "",
            "    def adapt_response(self, response):",
            "        \"\"\"You can override this function in order to make any changes you want",
            "        to into the feed before parsing it. This function must return a",
            "        response.",
            "        \"\"\"",
            "        return response",
            "",
            "    def parse_node(self, response, selector):",
            "        \"\"\"This method must be overridden with your custom spider functionality\"\"\"",
            "        if hasattr(self, \"parse_item\"):  # backward compatibility",
            "            return self.parse_item(response, selector)",
            "        raise NotImplementedError",
            "",
            "    def parse_nodes(self, response, nodes):",
            "        \"\"\"This method is called for the nodes matching the provided tag name",
            "        (itertag). Receives the response and an Selector for each node.",
            "        Overriding this method is mandatory. Otherwise, you spider won't work.",
            "        This method must return either an item, a request, or a list",
            "        containing any of them.",
            "        \"\"\"",
            "",
            "        for selector in nodes:",
            "            ret = iterate_spider_output(self.parse_node(response, selector))",
            "            for result_item in self.process_results(response, ret):",
            "                yield result_item",
            "",
            "    def _parse(self, response, **kwargs):",
            "        if not hasattr(self, \"parse_node\"):",
            "            raise NotConfigured(",
            "                \"You must define parse_node method in order to scrape this XML feed\"",
            "            )",
            "",
            "        response = self.adapt_response(response)",
            "        if self.iterator == \"iternodes\":",
            "            nodes = self._iternodes(response)",
            "        elif self.iterator == \"xml\":",
            "            selector = Selector(response, type=\"xml\")",
            "            self._register_namespaces(selector)",
            "            nodes = selector.xpath(f\"//{self.itertag}\")",
            "        elif self.iterator == \"html\":",
            "            selector = Selector(response, type=\"html\")",
            "            self._register_namespaces(selector)",
            "            nodes = selector.xpath(f\"//{self.itertag}\")",
            "        else:",
            "            raise NotSupported(\"Unsupported node iterator\")",
            "",
            "        return self.parse_nodes(response, nodes)",
            "",
            "    def _iternodes(self, response):",
            "        for node in xmliter_lxml(response, self.itertag):",
            "            self._register_namespaces(node)",
            "            yield node",
            "",
            "    def _register_namespaces(self, selector):",
            "        for prefix, uri in self.namespaces:",
            "            selector.register_namespace(prefix, uri)",
            "",
            "",
            "class CSVFeedSpider(Spider):",
            "    \"\"\"Spider for parsing CSV feeds.",
            "    It receives a CSV file in a response; iterates through each of its rows,",
            "    and calls parse_row with a dict containing each field's data.",
            "",
            "    You can set some options regarding the CSV file, such as the delimiter, quotechar",
            "    and the file's headers.",
            "    \"\"\"",
            "",
            "    delimiter = (",
            "        None  # When this is None, python's csv module's default delimiter is used",
            "    )",
            "    quotechar = (",
            "        None  # When this is None, python's csv module's default quotechar is used",
            "    )",
            "    headers = None",
            "",
            "    def process_results(self, response, results):",
            "        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"",
            "        return results",
            "",
            "    def adapt_response(self, response):",
            "        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"",
            "        return response",
            "",
            "    def parse_row(self, response, row):",
            "        \"\"\"This method must be overridden with your custom spider functionality\"\"\"",
            "        raise NotImplementedError",
            "",
            "    def parse_rows(self, response):",
            "        \"\"\"Receives a response and a dict (representing each row) with a key for",
            "        each provided (or detected) header of the CSV file.  This spider also",
            "        gives the opportunity to override adapt_response and",
            "        process_results methods for pre and post-processing purposes.",
            "        \"\"\"",
            "",
            "        for row in csviter(",
            "            response, self.delimiter, self.headers, quotechar=self.quotechar",
            "        ):",
            "            ret = iterate_spider_output(self.parse_row(response, row))",
            "            for result_item in self.process_results(response, ret):",
            "                yield result_item",
            "",
            "    def _parse(self, response, **kwargs):",
            "        if not hasattr(self, \"parse_row\"):",
            "            raise NotConfigured(",
            "                \"You must define parse_row method in order to scrape this CSV feed\"",
            "            )",
            "        response = self.adapt_response(response)",
            "        return self.parse_rows(response)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "10": [],
            "87": [
                "XMLFeedSpider",
                "_iternodes"
            ]
        },
        "addLocation": []
    },
    "scrapy/utils/iterators.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": "     cast,"
            },
            "1": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": "     overload,"
            },
            "2": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " )"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 19,
                "PatchRowcode": "+from warnings import warn"
            },
            "4": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 21,
                "PatchRowcode": "+from lxml import etree"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 22,
                "PatchRowcode": "+"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 23,
                "PatchRowcode": "+from scrapy.exceptions import ScrapyDeprecationWarning"
            },
            "8": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " from scrapy.http import Response, TextResponse"
            },
            "9": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " from scrapy.selector import Selector"
            },
            "10": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from scrapy.utils.python import re_rsearch, to_unicode"
            },
            "11": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 42,
                "PatchRowcode": "     - a unicode string"
            },
            "12": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 43,
                "PatchRowcode": "     - a string encoded as utf-8"
            },
            "13": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 44,
                "PatchRowcode": "     \"\"\""
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+    warn("
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+        ("
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+            \"xmliter is deprecated and its use strongly discouraged because \""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+            \"it is vulnerable to ReDoS attacks. Use xmliter_lxml instead. See \""
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+            \"https://github.com/scrapy/scrapy/security/advisories/GHSA-cc65-xxvf-f7r9\""
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+        ),"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+        ScrapyDeprecationWarning,"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+        stacklevel=2,"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+    )"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 54,
                "PatchRowcode": "+"
            },
            "24": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 55,
                "PatchRowcode": "     nodename_patt = re.escape(nodename)"
            },
            "25": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 56,
                "PatchRowcode": " "
            },
            "26": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 57,
                "PatchRowcode": "     DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.S)"
            },
            "27": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 95,
                "PatchRowcode": "     namespace: Optional[str] = None,"
            },
            "28": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 96,
                "PatchRowcode": "     prefix: str = \"x\","
            },
            "29": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 97,
                "PatchRowcode": " ) -> Generator[Selector, Any, None]:"
            },
            "30": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    from lxml import etree"
            },
            "31": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "32": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 98,
                "PatchRowcode": "     reader = _StreamReader(obj)"
            },
            "33": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": 99,
                "PatchRowcode": "     tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename"
            },
            "34": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": 100,
                "PatchRowcode": "     iterable = etree.iterparse("
            },
            "35": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        cast(\"SupportsReadClose[bytes]\", reader), tag=tag, encoding=reader.encoding"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+        cast(\"SupportsReadClose[bytes]\", reader),"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+        encoding=reader.encoding,"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+        events=(\"end\", \"start-ns\"),"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 104,
                "PatchRowcode": "+        huge_tree=True,"
            },
            "40": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 105,
                "PatchRowcode": "     )"
            },
            "41": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 106,
                "PatchRowcode": "     selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)"
            },
            "42": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    for _, node in iterable:"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+    needs_namespace_resolution = not namespace and \":\" in nodename"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 108,
                "PatchRowcode": "+    if needs_namespace_resolution:"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+        prefix, nodename = nodename.split(\":\", maxsplit=1)"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 110,
                "PatchRowcode": "+    for event, data in iterable:"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 111,
                "PatchRowcode": "+        if event == \"start-ns\":"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 112,
                "PatchRowcode": "+            assert isinstance(data, tuple)"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 113,
                "PatchRowcode": "+            if needs_namespace_resolution:"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 114,
                "PatchRowcode": "+                _prefix, _namespace = data"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 115,
                "PatchRowcode": "+                if _prefix != prefix:"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+                    continue"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 117,
                "PatchRowcode": "+                namespace = _namespace"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 118,
                "PatchRowcode": "+                needs_namespace_resolution = False"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 119,
                "PatchRowcode": "+                selxpath = f\"//{prefix}:{nodename}\""
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 120,
                "PatchRowcode": "+                tag = f\"{{{namespace}}}{nodename}\""
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 121,
                "PatchRowcode": "+            continue"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 122,
                "PatchRowcode": "+        assert isinstance(data, etree._Element)"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 123,
                "PatchRowcode": "+        node = data"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 124,
                "PatchRowcode": "+        if node.tag != tag:"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 125,
                "PatchRowcode": "+            continue"
            },
            "62": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 126,
                "PatchRowcode": "         nodetext = etree.tostring(node, encoding=\"unicode\")"
            },
            "63": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": 127,
                "PatchRowcode": "         node.clear()"
            },
            "64": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 128,
                "PatchRowcode": "         xs = Selector(text=nodetext, type=\"xml\")"
            }
        },
        "frontPatchFile": [
            "import csv",
            "import logging",
            "import re",
            "from io import StringIO",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Callable,",
            "    Dict,",
            "    Generator,",
            "    Iterable,",
            "    List,",
            "    Literal,",
            "    Optional,",
            "    Union,",
            "    cast,",
            "    overload,",
            ")",
            "",
            "from scrapy.http import Response, TextResponse",
            "from scrapy.selector import Selector",
            "from scrapy.utils.python import re_rsearch, to_unicode",
            "",
            "if TYPE_CHECKING:",
            "    from lxml._types import SupportsReadClose",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "def xmliter(",
            "    obj: Union[Response, str, bytes], nodename: str",
            ") -> Generator[Selector, Any, None]:",
            "    \"\"\"Return a iterator of Selector's over all nodes of a XML document,",
            "       given the name of the node to iterate. Useful for parsing XML feeds.",
            "",
            "    obj can be:",
            "    - a Response object",
            "    - a unicode string",
            "    - a string encoded as utf-8",
            "    \"\"\"",
            "    nodename_patt = re.escape(nodename)",
            "",
            "    DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.S)",
            "    HEADER_END_RE = re.compile(rf\"<\\s*/{nodename_patt}\\s*>\", re.S)",
            "    END_TAG_RE = re.compile(r\"<\\s*/([^\\s>]+)\\s*>\", re.S)",
            "    NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]*)=[^>\\s]+)\", re.S)",
            "    text = _body_or_str(obj)",
            "",
            "    document_header_match = re.search(DOCUMENT_HEADER_RE, text)",
            "    document_header = (",
            "        document_header_match.group().strip() if document_header_match else \"\"",
            "    )",
            "    header_end_idx = re_rsearch(HEADER_END_RE, text)",
            "    header_end = text[header_end_idx[1] :].strip() if header_end_idx else \"\"",
            "    namespaces: Dict[str, str] = {}",
            "    if header_end:",
            "        for tagname in reversed(re.findall(END_TAG_RE, header_end)):",
            "            assert header_end_idx",
            "            tag = re.search(",
            "                rf\"<\\s*{tagname}.*?xmlns[:=][^>]*>\", text[: header_end_idx[1]], re.S",
            "            )",
            "            if tag:",
            "                for x in re.findall(NAMESPACE_RE, tag.group()):",
            "                    namespaces[x[1]] = x[0]",
            "",
            "    r = re.compile(rf\"<{nodename_patt}[\\s>].*?</{nodename_patt}>\", re.DOTALL)",
            "    for match in r.finditer(text):",
            "        nodetext = (",
            "            document_header",
            "            + match.group().replace(",
            "                nodename, f'{nodename} {\" \".join(namespaces.values())}', 1",
            "            )",
            "            + header_end",
            "        )",
            "        yield Selector(text=nodetext, type=\"xml\")",
            "",
            "",
            "def xmliter_lxml(",
            "    obj: Union[Response, str, bytes],",
            "    nodename: str,",
            "    namespace: Optional[str] = None,",
            "    prefix: str = \"x\",",
            ") -> Generator[Selector, Any, None]:",
            "    from lxml import etree",
            "",
            "    reader = _StreamReader(obj)",
            "    tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename",
            "    iterable = etree.iterparse(",
            "        cast(\"SupportsReadClose[bytes]\", reader), tag=tag, encoding=reader.encoding",
            "    )",
            "    selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)",
            "    for _, node in iterable:",
            "        nodetext = etree.tostring(node, encoding=\"unicode\")",
            "        node.clear()",
            "        xs = Selector(text=nodetext, type=\"xml\")",
            "        if namespace:",
            "            xs.register_namespace(prefix, namespace)",
            "        yield xs.xpath(selxpath)[0]",
            "",
            "",
            "class _StreamReader:",
            "    def __init__(self, obj: Union[Response, str, bytes]):",
            "        self._ptr: int = 0",
            "        self._text: Union[str, bytes]",
            "        if isinstance(obj, TextResponse):",
            "            self._text, self.encoding = obj.body, obj.encoding",
            "        elif isinstance(obj, Response):",
            "            self._text, self.encoding = obj.body, \"utf-8\"",
            "        else:",
            "            self._text, self.encoding = obj, \"utf-8\"",
            "        self._is_unicode: bool = isinstance(self._text, str)",
            "        self._is_first_read: bool = True",
            "",
            "    def read(self, n: int = 65535) -> bytes:",
            "        method: Callable[[int], bytes] = (",
            "            self._read_unicode if self._is_unicode else self._read_string",
            "        )",
            "        result = method(n)",
            "        if self._is_first_read:",
            "            self._is_first_read = False",
            "            result = result.lstrip()",
            "        return result",
            "",
            "    def _read_string(self, n: int = 65535) -> bytes:",
            "        s, e = self._ptr, self._ptr + n",
            "        self._ptr = e",
            "        return cast(bytes, self._text)[s:e]",
            "",
            "    def _read_unicode(self, n: int = 65535) -> bytes:",
            "        s, e = self._ptr, self._ptr + n",
            "        self._ptr = e",
            "        return cast(str, self._text)[s:e].encode(\"utf-8\")",
            "",
            "",
            "def csviter(",
            "    obj: Union[Response, str, bytes],",
            "    delimiter: Optional[str] = None,",
            "    headers: Optional[List[str]] = None,",
            "    encoding: Optional[str] = None,",
            "    quotechar: Optional[str] = None,",
            ") -> Generator[Dict[str, str], Any, None]:",
            "    \"\"\"Returns an iterator of dictionaries from the given csv object",
            "",
            "    obj can be:",
            "    - a Response object",
            "    - a unicode string",
            "    - a string encoded as utf-8",
            "",
            "    delimiter is the character used to separate fields on the given obj.",
            "",
            "    headers is an iterable that when provided offers the keys",
            "    for the returned dictionaries, if not the first row is used.",
            "",
            "    quotechar is the character used to enclosure fields on the given obj.",
            "    \"\"\"",
            "",
            "    encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or \"utf-8\"",
            "",
            "    def row_to_unicode(row_: Iterable) -> List[str]:",
            "        return [to_unicode(field, encoding) for field in row_]",
            "",
            "    lines = StringIO(_body_or_str(obj, unicode=True))",
            "",
            "    kwargs: Dict[str, Any] = {}",
            "    if delimiter:",
            "        kwargs[\"delimiter\"] = delimiter",
            "    if quotechar:",
            "        kwargs[\"quotechar\"] = quotechar",
            "    csv_r = csv.reader(lines, **kwargs)",
            "",
            "    if not headers:",
            "        try:",
            "            row = next(csv_r)",
            "        except StopIteration:",
            "            return",
            "        headers = row_to_unicode(row)",
            "",
            "    for row in csv_r:",
            "        row = row_to_unicode(row)",
            "        if len(row) != len(headers):",
            "            logger.warning(",
            "                \"ignoring row %(csvlnum)d (length: %(csvrow)d, \"",
            "                \"should be: %(csvheader)d)\",",
            "                {",
            "                    \"csvlnum\": csv_r.line_num,",
            "                    \"csvrow\": len(row),",
            "                    \"csvheader\": len(headers),",
            "                },",
            "            )",
            "            continue",
            "        yield dict(zip(headers, row))",
            "",
            "",
            "@overload",
            "def _body_or_str(obj: Union[Response, str, bytes]) -> str:",
            "    ...",
            "",
            "",
            "@overload",
            "def _body_or_str(obj: Union[Response, str, bytes], unicode: Literal[True]) -> str:",
            "    ...",
            "",
            "",
            "@overload",
            "def _body_or_str(obj: Union[Response, str, bytes], unicode: Literal[False]) -> bytes:",
            "    ...",
            "",
            "",
            "def _body_or_str(",
            "    obj: Union[Response, str, bytes], unicode: bool = True",
            ") -> Union[str, bytes]:",
            "    expected_types = (Response, str, bytes)",
            "    if not isinstance(obj, expected_types):",
            "        expected_types_str = \" or \".join(t.__name__ for t in expected_types)",
            "        raise TypeError(",
            "            f\"Object {obj!r} must be {expected_types_str}, not {type(obj).__name__}\"",
            "        )",
            "    if isinstance(obj, Response):",
            "        if not unicode:",
            "            return cast(bytes, obj.body)",
            "        if isinstance(obj, TextResponse):",
            "            return obj.text",
            "        return cast(bytes, obj.body).decode(\"utf-8\")",
            "    if isinstance(obj, str):",
            "        return obj if unicode else obj.encode(\"utf-8\")",
            "    return obj.decode(\"utf-8\") if unicode else obj"
        ],
        "afterPatchFile": [
            "import csv",
            "import logging",
            "import re",
            "from io import StringIO",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Callable,",
            "    Dict,",
            "    Generator,",
            "    Iterable,",
            "    List,",
            "    Literal,",
            "    Optional,",
            "    Union,",
            "    cast,",
            "    overload,",
            ")",
            "from warnings import warn",
            "",
            "from lxml import etree",
            "",
            "from scrapy.exceptions import ScrapyDeprecationWarning",
            "from scrapy.http import Response, TextResponse",
            "from scrapy.selector import Selector",
            "from scrapy.utils.python import re_rsearch, to_unicode",
            "",
            "if TYPE_CHECKING:",
            "    from lxml._types import SupportsReadClose",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "def xmliter(",
            "    obj: Union[Response, str, bytes], nodename: str",
            ") -> Generator[Selector, Any, None]:",
            "    \"\"\"Return a iterator of Selector's over all nodes of a XML document,",
            "       given the name of the node to iterate. Useful for parsing XML feeds.",
            "",
            "    obj can be:",
            "    - a Response object",
            "    - a unicode string",
            "    - a string encoded as utf-8",
            "    \"\"\"",
            "    warn(",
            "        (",
            "            \"xmliter is deprecated and its use strongly discouraged because \"",
            "            \"it is vulnerable to ReDoS attacks. Use xmliter_lxml instead. See \"",
            "            \"https://github.com/scrapy/scrapy/security/advisories/GHSA-cc65-xxvf-f7r9\"",
            "        ),",
            "        ScrapyDeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "",
            "    nodename_patt = re.escape(nodename)",
            "",
            "    DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.S)",
            "    HEADER_END_RE = re.compile(rf\"<\\s*/{nodename_patt}\\s*>\", re.S)",
            "    END_TAG_RE = re.compile(r\"<\\s*/([^\\s>]+)\\s*>\", re.S)",
            "    NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]*)=[^>\\s]+)\", re.S)",
            "    text = _body_or_str(obj)",
            "",
            "    document_header_match = re.search(DOCUMENT_HEADER_RE, text)",
            "    document_header = (",
            "        document_header_match.group().strip() if document_header_match else \"\"",
            "    )",
            "    header_end_idx = re_rsearch(HEADER_END_RE, text)",
            "    header_end = text[header_end_idx[1] :].strip() if header_end_idx else \"\"",
            "    namespaces: Dict[str, str] = {}",
            "    if header_end:",
            "        for tagname in reversed(re.findall(END_TAG_RE, header_end)):",
            "            assert header_end_idx",
            "            tag = re.search(",
            "                rf\"<\\s*{tagname}.*?xmlns[:=][^>]*>\", text[: header_end_idx[1]], re.S",
            "            )",
            "            if tag:",
            "                for x in re.findall(NAMESPACE_RE, tag.group()):",
            "                    namespaces[x[1]] = x[0]",
            "",
            "    r = re.compile(rf\"<{nodename_patt}[\\s>].*?</{nodename_patt}>\", re.DOTALL)",
            "    for match in r.finditer(text):",
            "        nodetext = (",
            "            document_header",
            "            + match.group().replace(",
            "                nodename, f'{nodename} {\" \".join(namespaces.values())}', 1",
            "            )",
            "            + header_end",
            "        )",
            "        yield Selector(text=nodetext, type=\"xml\")",
            "",
            "",
            "def xmliter_lxml(",
            "    obj: Union[Response, str, bytes],",
            "    nodename: str,",
            "    namespace: Optional[str] = None,",
            "    prefix: str = \"x\",",
            ") -> Generator[Selector, Any, None]:",
            "    reader = _StreamReader(obj)",
            "    tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename",
            "    iterable = etree.iterparse(",
            "        cast(\"SupportsReadClose[bytes]\", reader),",
            "        encoding=reader.encoding,",
            "        events=(\"end\", \"start-ns\"),",
            "        huge_tree=True,",
            "    )",
            "    selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)",
            "    needs_namespace_resolution = not namespace and \":\" in nodename",
            "    if needs_namespace_resolution:",
            "        prefix, nodename = nodename.split(\":\", maxsplit=1)",
            "    for event, data in iterable:",
            "        if event == \"start-ns\":",
            "            assert isinstance(data, tuple)",
            "            if needs_namespace_resolution:",
            "                _prefix, _namespace = data",
            "                if _prefix != prefix:",
            "                    continue",
            "                namespace = _namespace",
            "                needs_namespace_resolution = False",
            "                selxpath = f\"//{prefix}:{nodename}\"",
            "                tag = f\"{{{namespace}}}{nodename}\"",
            "            continue",
            "        assert isinstance(data, etree._Element)",
            "        node = data",
            "        if node.tag != tag:",
            "            continue",
            "        nodetext = etree.tostring(node, encoding=\"unicode\")",
            "        node.clear()",
            "        xs = Selector(text=nodetext, type=\"xml\")",
            "        if namespace:",
            "            xs.register_namespace(prefix, namespace)",
            "        yield xs.xpath(selxpath)[0]",
            "",
            "",
            "class _StreamReader:",
            "    def __init__(self, obj: Union[Response, str, bytes]):",
            "        self._ptr: int = 0",
            "        self._text: Union[str, bytes]",
            "        if isinstance(obj, TextResponse):",
            "            self._text, self.encoding = obj.body, obj.encoding",
            "        elif isinstance(obj, Response):",
            "            self._text, self.encoding = obj.body, \"utf-8\"",
            "        else:",
            "            self._text, self.encoding = obj, \"utf-8\"",
            "        self._is_unicode: bool = isinstance(self._text, str)",
            "        self._is_first_read: bool = True",
            "",
            "    def read(self, n: int = 65535) -> bytes:",
            "        method: Callable[[int], bytes] = (",
            "            self._read_unicode if self._is_unicode else self._read_string",
            "        )",
            "        result = method(n)",
            "        if self._is_first_read:",
            "            self._is_first_read = False",
            "            result = result.lstrip()",
            "        return result",
            "",
            "    def _read_string(self, n: int = 65535) -> bytes:",
            "        s, e = self._ptr, self._ptr + n",
            "        self._ptr = e",
            "        return cast(bytes, self._text)[s:e]",
            "",
            "    def _read_unicode(self, n: int = 65535) -> bytes:",
            "        s, e = self._ptr, self._ptr + n",
            "        self._ptr = e",
            "        return cast(str, self._text)[s:e].encode(\"utf-8\")",
            "",
            "",
            "def csviter(",
            "    obj: Union[Response, str, bytes],",
            "    delimiter: Optional[str] = None,",
            "    headers: Optional[List[str]] = None,",
            "    encoding: Optional[str] = None,",
            "    quotechar: Optional[str] = None,",
            ") -> Generator[Dict[str, str], Any, None]:",
            "    \"\"\"Returns an iterator of dictionaries from the given csv object",
            "",
            "    obj can be:",
            "    - a Response object",
            "    - a unicode string",
            "    - a string encoded as utf-8",
            "",
            "    delimiter is the character used to separate fields on the given obj.",
            "",
            "    headers is an iterable that when provided offers the keys",
            "    for the returned dictionaries, if not the first row is used.",
            "",
            "    quotechar is the character used to enclosure fields on the given obj.",
            "    \"\"\"",
            "",
            "    encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or \"utf-8\"",
            "",
            "    def row_to_unicode(row_: Iterable) -> List[str]:",
            "        return [to_unicode(field, encoding) for field in row_]",
            "",
            "    lines = StringIO(_body_or_str(obj, unicode=True))",
            "",
            "    kwargs: Dict[str, Any] = {}",
            "    if delimiter:",
            "        kwargs[\"delimiter\"] = delimiter",
            "    if quotechar:",
            "        kwargs[\"quotechar\"] = quotechar",
            "    csv_r = csv.reader(lines, **kwargs)",
            "",
            "    if not headers:",
            "        try:",
            "            row = next(csv_r)",
            "        except StopIteration:",
            "            return",
            "        headers = row_to_unicode(row)",
            "",
            "    for row in csv_r:",
            "        row = row_to_unicode(row)",
            "        if len(row) != len(headers):",
            "            logger.warning(",
            "                \"ignoring row %(csvlnum)d (length: %(csvrow)d, \"",
            "                \"should be: %(csvheader)d)\",",
            "                {",
            "                    \"csvlnum\": csv_r.line_num,",
            "                    \"csvrow\": len(row),",
            "                    \"csvheader\": len(headers),",
            "                },",
            "            )",
            "            continue",
            "        yield dict(zip(headers, row))",
            "",
            "",
            "@overload",
            "def _body_or_str(obj: Union[Response, str, bytes]) -> str:",
            "    ...",
            "",
            "",
            "@overload",
            "def _body_or_str(obj: Union[Response, str, bytes], unicode: Literal[True]) -> str:",
            "    ...",
            "",
            "",
            "@overload",
            "def _body_or_str(obj: Union[Response, str, bytes], unicode: Literal[False]) -> bytes:",
            "    ...",
            "",
            "",
            "def _body_or_str(",
            "    obj: Union[Response, str, bytes], unicode: bool = True",
            ") -> Union[str, bytes]:",
            "    expected_types = (Response, str, bytes)",
            "    if not isinstance(obj, expected_types):",
            "        expected_types_str = \" or \".join(t.__name__ for t in expected_types)",
            "        raise TypeError(",
            "            f\"Object {obj!r} must be {expected_types_str}, not {type(obj).__name__}\"",
            "        )",
            "    if isinstance(obj, Response):",
            "        if not unicode:",
            "            return cast(bytes, obj.body)",
            "        if isinstance(obj, TextResponse):",
            "            return obj.text",
            "        return cast(bytes, obj.body).decode(\"utf-8\")",
            "    if isinstance(obj, str):",
            "        return obj if unicode else obj.encode(\"utf-8\")",
            "    return obj.decode(\"utf-8\") if unicode else obj"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "84": [
                "xmliter_lxml"
            ],
            "85": [
                "xmliter_lxml"
            ],
            "89": [
                "xmliter_lxml"
            ],
            "92": [
                "xmliter_lxml"
            ]
        },
        "addLocation": [
            "nova.virt.libvirt.driver.LibvirtDriver"
        ]
    },
    "scrapy/utils/response.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 74,
                "PatchRowcode": "     return b\"\".join(values)"
            },
            "1": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 75,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 76,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+def _remove_html_comments(body):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+    start = body.find(b\"<!--\")"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+    while start != -1:"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 80,
                "PatchRowcode": "+        end = body.find(b\"-->\", start + 1)"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+        if end == -1:"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 82,
                "PatchRowcode": "+            return body[:start]"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 83,
                "PatchRowcode": "+        else:"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 84,
                "PatchRowcode": "+            body = body[:start] + body[end + 3 :]"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 85,
                "PatchRowcode": "+            start = body.find(b\"<!--\")"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+    return body"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+"
            },
            "15": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 89,
                "PatchRowcode": " def open_in_browser("
            },
            "16": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 90,
                "PatchRowcode": "     response: Union["
            },
            "17": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 91,
                "PatchRowcode": "         \"scrapy.http.response.html.HtmlResponse\","
            },
            "18": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": 92,
                "PatchRowcode": "         \"scrapy.http.response.text.TextResponse\","
            },
            "19": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 93,
                "PatchRowcode": "     ],"
            },
            "20": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 94,
                "PatchRowcode": "     _openfunc: Callable[[str], Any] = webbrowser.open,"
            },
            "21": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 95,
                "PatchRowcode": " ) -> Any:"
            },
            "22": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    \"\"\"Open the given response in a local web browser, populating the <base>"
            },
            "23": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    tag for external links to work"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+    \"\"\"Open *response* in a local web browser, adjusting the `base tag`_ for"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+    external links to work, e.g. so that images and styles are displayed."
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+    .. _base tag: https://www.w3schools.com/tags/tag_base.asp"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+    For example:"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+    .. code-block:: python"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 104,
                "PatchRowcode": "+"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 105,
                "PatchRowcode": "+        from scrapy.utils.response import open_in_browser"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 108,
                "PatchRowcode": "+        def parse_details(self, response):"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+            if \"item name\" not in response.body:"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 110,
                "PatchRowcode": "+                open_in_browser(response)"
            },
            "39": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 111,
                "PatchRowcode": "     \"\"\""
            },
            "40": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": 112,
                "PatchRowcode": "     from scrapy.http import HtmlResponse, TextResponse"
            },
            "41": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": 113,
                "PatchRowcode": " "
            },
            "42": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "     # XXX: this implementation is a bit dirty and could be improved"
            },
            "43": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 115,
                "PatchRowcode": "     body = response.body"
            },
            "44": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 116,
                "PatchRowcode": "     if isinstance(response, HtmlResponse):"
            },
            "45": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 117,
                "PatchRowcode": "         if b\"<base\" not in body:"
            },
            "46": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            repl = rf'\\1<base href=\"{response.url}\">'"
            },
            "47": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            body = re.sub(b\"<!--.*?-->\", b\"\", body, flags=re.DOTALL)"
            },
            "48": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            body = re.sub(rb\"(<head(?:>|\\s.*?>))\", to_bytes(repl), body)"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 118,
                "PatchRowcode": "+            _remove_html_comments(body)"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 119,
                "PatchRowcode": "+            repl = rf'\\0<base href=\"{response.url}\">'"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 120,
                "PatchRowcode": "+            body = re.sub(rb\"<head(?:[^<>]*?>)\", to_bytes(repl), body, count=1)"
            },
            "52": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 121,
                "PatchRowcode": "         ext = \".html\""
            },
            "53": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 122,
                "PatchRowcode": "     elif isinstance(response, TextResponse):"
            },
            "54": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": 123,
                "PatchRowcode": "         ext = \".txt\""
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "This module provides some useful functions for working with",
            "scrapy.http.Response objects",
            "\"\"\"",
            "import os",
            "import re",
            "import tempfile",
            "import webbrowser",
            "from typing import Any, Callable, Iterable, Tuple, Union",
            "from weakref import WeakKeyDictionary",
            "",
            "from twisted.web import http",
            "from w3lib import html",
            "",
            "import scrapy",
            "from scrapy.http.response import Response",
            "from scrapy.utils.decorators import deprecated",
            "from scrapy.utils.python import to_bytes, to_unicode",
            "",
            "_baseurl_cache: \"WeakKeyDictionary[Response, str]\" = WeakKeyDictionary()",
            "",
            "",
            "def get_base_url(response: \"scrapy.http.response.text.TextResponse\") -> str:",
            "    \"\"\"Return the base url of the given response, joined with the response url\"\"\"",
            "    if response not in _baseurl_cache:",
            "        text = response.text[0:4096]",
            "        _baseurl_cache[response] = html.get_base_url(",
            "            text, response.url, response.encoding",
            "        )",
            "    return _baseurl_cache[response]",
            "",
            "",
            "_metaref_cache: \"WeakKeyDictionary[Response, Union[Tuple[None, None], Tuple[float, str]]]\" = (",
            "    WeakKeyDictionary()",
            ")",
            "",
            "",
            "def get_meta_refresh(",
            "    response: \"scrapy.http.response.text.TextResponse\",",
            "    ignore_tags: Iterable[str] = (\"script\", \"noscript\"),",
            ") -> Union[Tuple[None, None], Tuple[float, str]]:",
            "    \"\"\"Parse the http-equiv refresh parameter from the given response\"\"\"",
            "    if response not in _metaref_cache:",
            "        text = response.text[0:4096]",
            "        _metaref_cache[response] = html.get_meta_refresh(",
            "            text, response.url, response.encoding, ignore_tags=ignore_tags",
            "        )",
            "    return _metaref_cache[response]",
            "",
            "",
            "def response_status_message(status: Union[bytes, float, int, str]) -> str:",
            "    \"\"\"Return status code plus status text descriptive message\"\"\"",
            "    status_int = int(status)",
            "    message = http.RESPONSES.get(status_int, \"Unknown Status\")",
            "    return f\"{status_int} {to_unicode(message)}\"",
            "",
            "",
            "@deprecated",
            "def response_httprepr(response: Response) -> bytes:",
            "    \"\"\"Return raw HTTP representation (as bytes) of the given response. This",
            "    is provided only for reference, since it's not the exact stream of bytes",
            "    that was received (that's not exposed by Twisted).",
            "    \"\"\"",
            "    values = [",
            "        b\"HTTP/1.1 \",",
            "        to_bytes(str(response.status)),",
            "        b\" \",",
            "        to_bytes(http.RESPONSES.get(response.status, b\"\")),",
            "        b\"\\r\\n\",",
            "    ]",
            "    if response.headers:",
            "        values.extend([response.headers.to_string(), b\"\\r\\n\"])",
            "    values.extend([b\"\\r\\n\", response.body])",
            "    return b\"\".join(values)",
            "",
            "",
            "def open_in_browser(",
            "    response: Union[",
            "        \"scrapy.http.response.html.HtmlResponse\",",
            "        \"scrapy.http.response.text.TextResponse\",",
            "    ],",
            "    _openfunc: Callable[[str], Any] = webbrowser.open,",
            ") -> Any:",
            "    \"\"\"Open the given response in a local web browser, populating the <base>",
            "    tag for external links to work",
            "    \"\"\"",
            "    from scrapy.http import HtmlResponse, TextResponse",
            "",
            "    # XXX: this implementation is a bit dirty and could be improved",
            "    body = response.body",
            "    if isinstance(response, HtmlResponse):",
            "        if b\"<base\" not in body:",
            "            repl = rf'\\1<base href=\"{response.url}\">'",
            "            body = re.sub(b\"<!--.*?-->\", b\"\", body, flags=re.DOTALL)",
            "            body = re.sub(rb\"(<head(?:>|\\s.*?>))\", to_bytes(repl), body)",
            "        ext = \".html\"",
            "    elif isinstance(response, TextResponse):",
            "        ext = \".txt\"",
            "    else:",
            "        raise TypeError(\"Unsupported response type: \" f\"{response.__class__.__name__}\")",
            "    fd, fname = tempfile.mkstemp(ext)",
            "    os.write(fd, body)",
            "    os.close(fd)",
            "    return _openfunc(f\"file://{fname}\")"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "This module provides some useful functions for working with",
            "scrapy.http.Response objects",
            "\"\"\"",
            "import os",
            "import re",
            "import tempfile",
            "import webbrowser",
            "from typing import Any, Callable, Iterable, Tuple, Union",
            "from weakref import WeakKeyDictionary",
            "",
            "from twisted.web import http",
            "from w3lib import html",
            "",
            "import scrapy",
            "from scrapy.http.response import Response",
            "from scrapy.utils.decorators import deprecated",
            "from scrapy.utils.python import to_bytes, to_unicode",
            "",
            "_baseurl_cache: \"WeakKeyDictionary[Response, str]\" = WeakKeyDictionary()",
            "",
            "",
            "def get_base_url(response: \"scrapy.http.response.text.TextResponse\") -> str:",
            "    \"\"\"Return the base url of the given response, joined with the response url\"\"\"",
            "    if response not in _baseurl_cache:",
            "        text = response.text[0:4096]",
            "        _baseurl_cache[response] = html.get_base_url(",
            "            text, response.url, response.encoding",
            "        )",
            "    return _baseurl_cache[response]",
            "",
            "",
            "_metaref_cache: \"WeakKeyDictionary[Response, Union[Tuple[None, None], Tuple[float, str]]]\" = (",
            "    WeakKeyDictionary()",
            ")",
            "",
            "",
            "def get_meta_refresh(",
            "    response: \"scrapy.http.response.text.TextResponse\",",
            "    ignore_tags: Iterable[str] = (\"script\", \"noscript\"),",
            ") -> Union[Tuple[None, None], Tuple[float, str]]:",
            "    \"\"\"Parse the http-equiv refresh parameter from the given response\"\"\"",
            "    if response not in _metaref_cache:",
            "        text = response.text[0:4096]",
            "        _metaref_cache[response] = html.get_meta_refresh(",
            "            text, response.url, response.encoding, ignore_tags=ignore_tags",
            "        )",
            "    return _metaref_cache[response]",
            "",
            "",
            "def response_status_message(status: Union[bytes, float, int, str]) -> str:",
            "    \"\"\"Return status code plus status text descriptive message\"\"\"",
            "    status_int = int(status)",
            "    message = http.RESPONSES.get(status_int, \"Unknown Status\")",
            "    return f\"{status_int} {to_unicode(message)}\"",
            "",
            "",
            "@deprecated",
            "def response_httprepr(response: Response) -> bytes:",
            "    \"\"\"Return raw HTTP representation (as bytes) of the given response. This",
            "    is provided only for reference, since it's not the exact stream of bytes",
            "    that was received (that's not exposed by Twisted).",
            "    \"\"\"",
            "    values = [",
            "        b\"HTTP/1.1 \",",
            "        to_bytes(str(response.status)),",
            "        b\" \",",
            "        to_bytes(http.RESPONSES.get(response.status, b\"\")),",
            "        b\"\\r\\n\",",
            "    ]",
            "    if response.headers:",
            "        values.extend([response.headers.to_string(), b\"\\r\\n\"])",
            "    values.extend([b\"\\r\\n\", response.body])",
            "    return b\"\".join(values)",
            "",
            "",
            "def _remove_html_comments(body):",
            "    start = body.find(b\"<!--\")",
            "    while start != -1:",
            "        end = body.find(b\"-->\", start + 1)",
            "        if end == -1:",
            "            return body[:start]",
            "        else:",
            "            body = body[:start] + body[end + 3 :]",
            "            start = body.find(b\"<!--\")",
            "    return body",
            "",
            "",
            "def open_in_browser(",
            "    response: Union[",
            "        \"scrapy.http.response.html.HtmlResponse\",",
            "        \"scrapy.http.response.text.TextResponse\",",
            "    ],",
            "    _openfunc: Callable[[str], Any] = webbrowser.open,",
            ") -> Any:",
            "    \"\"\"Open *response* in a local web browser, adjusting the `base tag`_ for",
            "    external links to work, e.g. so that images and styles are displayed.",
            "",
            "    .. _base tag: https://www.w3schools.com/tags/tag_base.asp",
            "",
            "    For example:",
            "",
            "    .. code-block:: python",
            "",
            "        from scrapy.utils.response import open_in_browser",
            "",
            "",
            "        def parse_details(self, response):",
            "            if \"item name\" not in response.body:",
            "                open_in_browser(response)",
            "    \"\"\"",
            "    from scrapy.http import HtmlResponse, TextResponse",
            "",
            "    # XXX: this implementation is a bit dirty and could be improved",
            "    body = response.body",
            "    if isinstance(response, HtmlResponse):",
            "        if b\"<base\" not in body:",
            "            _remove_html_comments(body)",
            "            repl = rf'\\0<base href=\"{response.url}\">'",
            "            body = re.sub(rb\"<head(?:[^<>]*?>)\", to_bytes(repl), body, count=1)",
            "        ext = \".html\"",
            "    elif isinstance(response, TextResponse):",
            "        ext = \".txt\"",
            "    else:",
            "        raise TypeError(\"Unsupported response type: \" f\"{response.__class__.__name__}\")",
            "    fd, fname = tempfile.mkstemp(ext)",
            "    os.write(fd, body)",
            "    os.close(fd)",
            "    return _openfunc(f\"file://{fname}\")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "84": [
                "open_in_browser"
            ],
            "85": [
                "open_in_browser"
            ],
            "93": [
                "open_in_browser"
            ],
            "94": [
                "open_in_browser"
            ],
            "95": [
                "open_in_browser"
            ]
        },
        "addLocation": [
            "scrapy.utils.response.open_in_browser",
            "nova.virt.libvirt.driver.LibvirtDriver"
        ]
    }
}