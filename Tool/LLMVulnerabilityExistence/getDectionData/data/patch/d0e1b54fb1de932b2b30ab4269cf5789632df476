{
    "neutron/db/securitygroups_db.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 861,
                "afterPatchRowNumber": 861,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 862,
                "afterPatchRowNumber": 862,
                "PatchRowcode": "         :returns: the default security group id for given tenant."
            },
            "2": {
                "beforePatchRowNumber": 863,
                "afterPatchRowNumber": 863,
                "PatchRowcode": "         \"\"\""
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 864,
                "PatchRowcode": "+        # Do not allow a tenant to create a default SG for another one."
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 865,
                "PatchRowcode": "+        # See Bug 1987410."
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 866,
                "PatchRowcode": "+        if tenant_id != context.tenant_id and not context.is_admin:"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 867,
                "PatchRowcode": "+            return"
            },
            "7": {
                "beforePatchRowNumber": 864,
                "afterPatchRowNumber": 868,
                "PatchRowcode": "         if not extensions.is_extension_supported(self, 'security-group'):"
            },
            "8": {
                "beforePatchRowNumber": 865,
                "afterPatchRowNumber": 869,
                "PatchRowcode": "             return"
            },
            "9": {
                "beforePatchRowNumber": 866,
                "afterPatchRowNumber": 870,
                "PatchRowcode": "         default_group_id = self._get_default_sg_id(context, tenant_id)"
            }
        },
        "frontPatchFile": [
            "# Copyright 2012 VMware, Inc.  All rights reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "import netaddr",
            "from neutron_lib.api.definitions import port as port_def",
            "from neutron_lib.api import extensions",
            "from neutron_lib.api import validators",
            "from neutron_lib.callbacks import events",
            "from neutron_lib.callbacks import exceptions",
            "from neutron_lib.callbacks import registry",
            "from neutron_lib.callbacks import resources",
            "from neutron_lib import constants",
            "from neutron_lib import context as context_lib",
            "from neutron_lib.db import api as db_api",
            "from neutron_lib.db import model_query",
            "from neutron_lib.db import resource_extend",
            "from neutron_lib.db import utils as db_utils",
            "from neutron_lib import exceptions as n_exc",
            "from neutron_lib.objects import exceptions as obj_exc",
            "from neutron_lib.utils import helpers",
            "from neutron_lib.utils import net",
            "from oslo_log import log as logging",
            "from oslo_utils import uuidutils",
            "from sqlalchemy.orm import scoped_session",
            "",
            "from neutron._i18n import _",
            "from neutron.common import _constants as const",
            "from neutron.db.models import securitygroup as sg_models",
            "from neutron.db import rbac_db_mixin as rbac_mixin",
            "from neutron.extensions import securitygroup as ext_sg",
            "from neutron.objects import base as base_obj",
            "from neutron.objects import ports as port_obj",
            "from neutron.objects import securitygroup as sg_obj",
            "from neutron import quota",
            "",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "DEFAULT_SG_DESCRIPTION = _('Default security group')",
            "",
            "",
            "@resource_extend.has_resource_extenders",
            "@registry.has_registry_receivers",
            "class SecurityGroupDbMixin(ext_sg.SecurityGroupPluginBase,",
            "                           rbac_mixin.RbacPluginMixin):",
            "    \"\"\"Mixin class to add security group to db_base_plugin_v2.\"\"\"",
            "",
            "    __native_bulk_support = True",
            "",
            "    def create_security_group_bulk(self, context, security_groups):",
            "        return self._create_bulk('security_group', context,",
            "                                 security_groups)",
            "",
            "    def _registry_notify(self, res, event, id=None, exc_cls=None, **kwargs):",
            "        # NOTE(armax): a callback exception here will prevent the request",
            "        # from being processed. This is a hook point for backend's validation;",
            "        # we raise to propagate the reason for the failure.",
            "        try:",
            "            if 'payload' in kwargs:",
            "                # TODO(boden): remove shim once all callbacks use payloads",
            "                registry.publish(res, event, self, payload=kwargs['payload'])",
            "            else:",
            "                registry.notify(res, event, self, **kwargs)",
            "        except exceptions.CallbackFailure as e:",
            "            if exc_cls:",
            "                reason = (_('cannot perform %(event)s due to %(reason)s') %",
            "                          {'event': event, 'reason': e})",
            "                raise exc_cls(reason=reason, id=id)",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def create_security_group(self, context, security_group, default_sg=False):",
            "        \"\"\"Create security group.",
            "",
            "        If default_sg is true that means we are a default security group for",
            "        a given tenant if it does not exist.",
            "        \"\"\"",
            "        s = security_group['security_group']",
            "        kwargs = {",
            "            'context': context,",
            "            'security_group': s,",
            "            'is_default': default_sg,",
            "        }",
            "        self._registry_notify(resources.SECURITY_GROUP, events.BEFORE_CREATE,",
            "                              exc_cls=ext_sg.SecurityGroupConflict,",
            "                              payload=events.DBEventPayload(",
            "                                  context, metadata={'is_default': default_sg},",
            "                                  request_body=security_group,",
            "                                  desired_state=s))",
            "",
            "        tenant_id = s['tenant_id']",
            "        stateful = s.get('stateful', True)",
            "",
            "        if not default_sg:",
            "            self._ensure_default_security_group(context, tenant_id)",
            "        else:",
            "            existing_def_sg_id = self._get_default_sg_id(context, tenant_id)",
            "            if existing_def_sg_id is not None:",
            "                # default already exists, return it",
            "                return self.get_security_group(context, existing_def_sg_id)",
            "",
            "        with db_api.CONTEXT_WRITER.using(context):",
            "            sg = sg_obj.SecurityGroup(",
            "                context, id=s.get('id') or uuidutils.generate_uuid(),",
            "                description=s['description'], project_id=tenant_id,",
            "                name=s['name'], is_default=default_sg, stateful=stateful)",
            "            sg.create()",
            "",
            "            delta = len(ext_sg.sg_supported_ethertypes)",
            "            delta = delta * 2 if default_sg else delta",
            "            reservation = quota.QUOTAS.make_reservation(",
            "                context, tenant_id, {'security_group_rule': delta},",
            "                self)",
            "",
            "            for ethertype in ext_sg.sg_supported_ethertypes:",
            "                if default_sg:",
            "                    # Allow intercommunication",
            "                    ingress_rule = sg_obj.SecurityGroupRule(",
            "                        context, id=uuidutils.generate_uuid(),",
            "                        project_id=tenant_id, security_group_id=sg.id,",
            "                        direction='ingress', ethertype=ethertype,",
            "                        remote_group_id=sg.id)",
            "                    ingress_rule.create()",
            "                    sg.rules.append(ingress_rule)",
            "",
            "                egress_rule = sg_obj.SecurityGroupRule(",
            "                    context, id=uuidutils.generate_uuid(),",
            "                    project_id=tenant_id, security_group_id=sg.id,",
            "                    direction='egress', ethertype=ethertype)",
            "                egress_rule.create()",
            "                sg.rules.append(egress_rule)",
            "            sg.obj_reset_changes(['rules'])",
            "",
            "            quota.QUOTAS.commit_reservation(context,",
            "                                            reservation.reservation_id)",
            "",
            "            # fetch sg from db to load the sg rules with sg model.",
            "            sg = sg_obj.SecurityGroup.get_object(context, id=sg.id)",
            "            secgroup_dict = self._make_security_group_dict(sg)",
            "            kwargs['security_group'] = secgroup_dict",
            "            self._registry_notify(resources.SECURITY_GROUP,",
            "                                  events.PRECOMMIT_CREATE,",
            "                                  exc_cls=ext_sg.SecurityGroupConflict,",
            "                                  **kwargs)",
            "",
            "        registry.notify(resources.SECURITY_GROUP, events.AFTER_CREATE, self,",
            "                        **kwargs)",
            "        return secgroup_dict",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def get_security_groups(self, context, filters=None, fields=None,",
            "                            sorts=None, limit=None,",
            "                            marker=None, page_reverse=False, default_sg=False):",
            "",
            "        # If default_sg is True do not call _ensure_default_security_group()",
            "        # so this can be done recursively. Context.tenant_id is checked",
            "        # because all the unit tests do not explicitly set the context on",
            "        # GETS. TODO(arosen)  context handling can probably be improved here.",
            "        filters = filters or {}",
            "        if not default_sg and context.tenant_id:",
            "            tenant_id = filters.get('project_id') or filters.get('tenant_id')",
            "            if tenant_id:",
            "                tenant_id = tenant_id[0]",
            "            else:",
            "                tenant_id = context.tenant_id",
            "            self._ensure_default_security_group(context, tenant_id)",
            "",
            "        pager = base_obj.Pager(",
            "            sorts=sorts, limit=limit, marker=marker, page_reverse=page_reverse)",
            "",
            "        sg_objs = sg_obj.SecurityGroup.get_objects(",
            "            context, _pager=pager, validate_filters=False,",
            "            fields=fields, **filters)",
            "",
            "        return [self._make_security_group_dict(obj, fields) for obj in sg_objs]",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def get_security_groups_count(self, context, filters=None):",
            "        filters = filters or {}",
            "        return sg_obj.SecurityGroup.count(",
            "            context, validate_filters=False, **filters)",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def get_security_group(self, context, id, fields=None, tenant_id=None):",
            "        \"\"\"Tenant id is given to handle the case when creating a security",
            "        group rule on behalf of another use.",
            "        \"\"\"",
            "        if tenant_id:",
            "            tmp_context_tenant_id = context.tenant_id",
            "            context.tenant_id = tenant_id",
            "",
            "        try:",
            "            with db_api.CONTEXT_READER.using(context):",
            "                ret = self._make_security_group_dict(self._get_security_group(",
            "                                                     context, id,",
            "                                                     fields=fields),",
            "                                                     fields)",
            "                if (fields is None or len(fields) == 0 or",
            "                   'security_group_rules' in fields):",
            "                    rules = self.get_security_group_rules(",
            "                        context_lib.get_admin_context(),",
            "                        {'security_group_id': [id]})",
            "                    ret['security_group_rules'] = rules",
            "",
            "        finally:",
            "            if tenant_id:",
            "                context.tenant_id = tmp_context_tenant_id",
            "        return ret",
            "",
            "    def _get_security_group(self, context, id, fields=None):",
            "        sg = sg_obj.SecurityGroup.get_object(context, fields=fields, id=id)",
            "        if sg is None:",
            "            raise ext_sg.SecurityGroupNotFound(id=id)",
            "        return sg",
            "",
            "    def _check_security_group(self, context, id, tenant_id=None):",
            "        if tenant_id:",
            "            tmp_context_tenant_id = context.tenant_id",
            "            context.tenant_id = tenant_id",
            "",
            "        try:",
            "            if not sg_obj.SecurityGroup.objects_exist(context, id=id):",
            "                raise ext_sg.SecurityGroupNotFound(id=id)",
            "        finally:",
            "            if tenant_id:",
            "                context.tenant_id = tmp_context_tenant_id",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def delete_security_group(self, context, id):",
            "        filters = {'security_group_id': [id]}",
            "        with db_api.CONTEXT_READER.using(context):",
            "            ports = self._get_port_security_group_bindings(context, filters)",
            "            if ports:",
            "                raise ext_sg.SecurityGroupInUse(id=id)",
            "            # confirm security group exists",
            "            sg = self._get_security_group(context, id, fields=['id', 'name'])",
            "",
            "            if sg['name'] == 'default' and not context.is_admin:",
            "                raise ext_sg.SecurityGroupCannotRemoveDefault()",
            "        kwargs = {",
            "            'context': context,",
            "            'security_group_id': id,",
            "            'security_group': sg,",
            "        }",
            "        self._registry_notify(resources.SECURITY_GROUP,",
            "                              events.BEFORE_DELETE,",
            "                              exc_cls=ext_sg.SecurityGroupInUse, id=id,",
            "                              payload=events.DBEventPayload(",
            "                                  context, states=(sg,), resource_id=id))",
            "",
            "        with db_api.CONTEXT_WRITER.using(context):",
            "            # pass security_group_rule_ids to ensure",
            "            # consistency with deleted rules",
            "            # get security_group_bindings and security_group one more time",
            "            # so that they will be attached for session where sg will be",
            "            # deleted",
            "            ports = self._get_port_security_group_bindings(context, filters)",
            "            sg = self._get_security_group(context, id)",
            "            kwargs['security_group_rule_ids'] = [r['id'] for r in sg.rules]",
            "            kwargs['security_group'] = self._make_security_group_dict(sg)",
            "            self._registry_notify(resources.SECURITY_GROUP,",
            "                                  events.PRECOMMIT_DELETE,",
            "                                  exc_cls=ext_sg.SecurityGroupInUse, id=id,",
            "                                  **kwargs)",
            "            sg.delete()",
            "",
            "        kwargs.pop('security_group')",
            "        kwargs['name'] = sg['name']",
            "        registry.notify(resources.SECURITY_GROUP, events.AFTER_DELETE,",
            "                        self, **kwargs)",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def update_security_group(self, context, id, security_group):",
            "        s = security_group['security_group']",
            "",
            "        if 'stateful' in s:",
            "            with db_api.CONTEXT_READER.using(context):",
            "                sg = self._get_security_group(context, id)",
            "                if s['stateful'] != sg['stateful']:",
            "                    filters = {'security_group_id': [id]}",
            "                    ports = self._get_port_security_group_bindings(context,",
            "                                                                   filters)",
            "                    if ports:",
            "                        raise ext_sg.SecurityGroupInUse(id=id)",
            "",
            "        kwargs = {",
            "            'context': context,",
            "            'security_group_id': id,",
            "            'security_group': s,",
            "        }",
            "        self._registry_notify(resources.SECURITY_GROUP, events.BEFORE_UPDATE,",
            "                              exc_cls=ext_sg.SecurityGroupConflict, **kwargs)",
            "",
            "        with db_api.CONTEXT_WRITER.using(context):",
            "            sg = self._get_security_group(context, id)",
            "            if sg.name == 'default' and 'name' in s:",
            "                raise ext_sg.SecurityGroupCannotUpdateDefault()",
            "            sg_dict = self._make_security_group_dict(sg)",
            "            kwargs['original_security_group'] = sg_dict",
            "            sg.update_fields(s)",
            "            sg.update()",
            "            sg_dict = self._make_security_group_dict(sg)",
            "            kwargs['security_group'] = sg_dict",
            "            self._registry_notify(",
            "                    resources.SECURITY_GROUP,",
            "                    events.PRECOMMIT_UPDATE,",
            "                    exc_cls=ext_sg.SecurityGroupConflict,",
            "                    payload=events.DBEventPayload(",
            "                        context, request_body=s,",
            "                        states=(kwargs['original_security_group'],),",
            "                        resource_id=id, desired_state=sg_dict))",
            "        registry.notify(resources.SECURITY_GROUP, events.AFTER_UPDATE, self,",
            "                        **kwargs)",
            "        return sg_dict",
            "",
            "    def _make_security_group_dict(self, security_group, fields=None):",
            "        res = {'id': security_group['id'],",
            "               'name': security_group['name'],",
            "               'stateful': security_group['stateful'],",
            "               'tenant_id': security_group['tenant_id'],",
            "               'description': security_group['description']}",
            "        if security_group.rules:",
            "            res['security_group_rules'] = [",
            "                self._make_security_group_rule_dict(r.db_obj)",
            "                for r in security_group.rules",
            "            ]",
            "        else:",
            "            res['security_group_rules'] = []",
            "        resource_extend.apply_funcs(ext_sg.SECURITYGROUPS, res,",
            "                                    security_group.db_obj)",
            "        return db_utils.resource_fields(res, fields)",
            "",
            "    @staticmethod",
            "    def _make_security_group_binding_dict(security_group, fields=None):",
            "        res = {'port_id': security_group['port_id'],",
            "               'security_group_id': security_group['security_group_id']}",
            "        return db_utils.resource_fields(res, fields)",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def _create_port_security_group_binding(self, context, port_id,",
            "                                            security_group_id):",
            "        with db_api.CONTEXT_WRITER.using(context):",
            "            db = sg_models.SecurityGroupPortBinding(",
            "                port_id=port_id, security_group_id=security_group_id)",
            "            context.session.add(db)",
            "",
            "    def _get_port_security_group_bindings(self, context,",
            "                                          filters=None, fields=None):",
            "        return model_query.get_collection(",
            "            context, sg_models.SecurityGroupPortBinding,",
            "            self._make_security_group_binding_dict,",
            "            filters=filters, fields=fields)",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def _delete_port_security_group_bindings(self, context, port_id):",
            "        with db_api.CONTEXT_WRITER.using(context):",
            "            query = model_query.query_with_hooks(",
            "                context, sg_models.SecurityGroupPortBinding)",
            "            bindings = query.filter(",
            "                sg_models.SecurityGroupPortBinding.port_id == port_id)",
            "            for binding in bindings:",
            "                context.session.delete(binding)",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def create_security_group_rule_bulk(self, context, security_group_rules):",
            "        return self._create_bulk('security_group_rule', context,",
            "                                 security_group_rules)",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def create_security_group_rule_bulk_native(self, context,",
            "                                               security_group_rules):",
            "        rules = security_group_rules['security_group_rules']",
            "        scoped_session(context.session)",
            "        security_group_id = self._validate_security_group_rules(",
            "            context, security_group_rules)",
            "        with db_api.CONTEXT_WRITER.using(context):",
            "            self._check_for_duplicate_rules(context, security_group_id, rules)",
            "            ret = []",
            "            for rule_dict in rules:",
            "                res_rule_dict = self._create_security_group_rule(",
            "                    context, rule_dict, validate=False)",
            "                ret.append(res_rule_dict)",
            "        for rdict in ret:",
            "            registry.notify(",
            "                resources.SECURITY_GROUP_RULE, events.AFTER_CREATE, self,",
            "                context=context, security_group_rule=rdict)",
            "        return ret",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def create_security_group_rule(self, context, security_group_rule):",
            "        res = self._create_security_group_rule(context, security_group_rule)",
            "        registry.notify(",
            "            resources.SECURITY_GROUP_RULE, events.AFTER_CREATE, self,",
            "            context=context, security_group_rule=res)",
            "        return res",
            "",
            "    def _create_security_group_rule(self, context, security_group_rule,",
            "                                    validate=True):",
            "        if validate:",
            "            sg_id = self._validate_security_group_rule(context,",
            "                                                       security_group_rule)",
            "        rule_dict = security_group_rule['security_group_rule']",
            "        remote_ip_prefix = rule_dict.get('remote_ip_prefix')",
            "        if remote_ip_prefix:",
            "            remote_ip_prefix = net.AuthenticIPNetwork(remote_ip_prefix)",
            "",
            "        protocol = rule_dict.get('protocol')",
            "        if protocol:",
            "            # object expects strings only",
            "            protocol = str(protocol)",
            "",
            "        args = {",
            "            'id': (rule_dict.get('id') or uuidutils.generate_uuid()),",
            "            'project_id': rule_dict['tenant_id'],",
            "            'security_group_id': rule_dict['security_group_id'],",
            "            'direction': rule_dict['direction'],",
            "            'remote_group_id': rule_dict.get('remote_group_id'),",
            "            'ethertype': rule_dict['ethertype'],",
            "            'protocol': protocol,",
            "            'remote_ip_prefix': remote_ip_prefix,",
            "            'description': rule_dict.get('description'),",
            "        }",
            "",
            "        port_range_min = self._safe_int(rule_dict['port_range_min'])",
            "        if port_range_min is not None:",
            "            args['port_range_min'] = port_range_min",
            "",
            "        port_range_max = self._safe_int(rule_dict['port_range_max'])",
            "        if port_range_max is not None:",
            "            args['port_range_max'] = port_range_max",
            "",
            "        kwargs = {",
            "            'context': context,",
            "            'security_group_rule': args",
            "        }",
            "        self._registry_notify(resources.SECURITY_GROUP_RULE,",
            "                              events.BEFORE_CREATE,",
            "                              exc_cls=ext_sg.SecurityGroupConflict, **kwargs)",
            "        with db_api.CONTEXT_WRITER.using(context):",
            "            if validate:",
            "                self._check_for_duplicate_rules(context, sg_id,",
            "                                                [security_group_rule])",
            "            sg_rule = sg_obj.SecurityGroupRule(context, **args)",
            "            sg_rule.create()",
            "",
            "            # fetch sg_rule from db to load the sg rules with sg model",
            "            # otherwise a DetachedInstanceError can occur for model extensions",
            "            sg_rule = sg_obj.SecurityGroupRule.get_object(context,",
            "                                                          id=sg_rule.id)",
            "            res_rule_dict = self._make_security_group_rule_dict(sg_rule.db_obj)",
            "            kwargs['security_group_rule'] = res_rule_dict",
            "            self._registry_notify(",
            "                resources.SECURITY_GROUP_RULE,",
            "                events.PRECOMMIT_CREATE,",
            "                exc_cls=ext_sg.SecurityGroupConflict, **kwargs)",
            "        return res_rule_dict",
            "",
            "    def _get_ip_proto_number(self, protocol):",
            "        if protocol is None:",
            "            return",
            "        # According to bug 1381379, protocol is always set to string to avoid",
            "        # problems with comparing int and string in PostgreSQL. Here this",
            "        # string is converted to int to give an opportunity to use it as",
            "        # before.",
            "        if protocol in constants.IP_PROTOCOL_NAME_ALIASES:",
            "            protocol = constants.IP_PROTOCOL_NAME_ALIASES[protocol]",
            "        return int(constants.IP_PROTOCOL_MAP.get(protocol, protocol))",
            "",
            "    def _get_ip_proto_name_and_num(self, protocol, ethertype=None):",
            "        if protocol is None:",
            "            return",
            "        protocol = str(protocol)",
            "        # Force all legacy IPv6 ICMP protocol names to be 'ipv6-icmp', and",
            "        # protocol number 1 to be 58",
            "        if ethertype == constants.IPv6:",
            "            if protocol in const.IPV6_ICMP_LEGACY_PROTO_LIST:",
            "                protocol = constants.PROTO_NAME_IPV6_ICMP",
            "            elif protocol == str(constants.PROTO_NUM_ICMP):",
            "                protocol = str(constants.PROTO_NUM_IPV6_ICMP)",
            "        if protocol in constants.IP_PROTOCOL_MAP:",
            "            return [protocol, str(constants.IP_PROTOCOL_MAP.get(protocol))]",
            "        elif protocol in constants.IP_PROTOCOL_NUM_TO_NAME_MAP:",
            "            return [constants.IP_PROTOCOL_NUM_TO_NAME_MAP.get(protocol),",
            "                    protocol]",
            "        return [protocol, protocol]",
            "",
            "    def _safe_int(self, port_range):",
            "        if port_range is None:",
            "            return",
            "        try:",
            "            return int(port_range)",
            "        except (ValueError, TypeError):",
            "            msg = \"port range must be an integer\"",
            "            raise n_exc.InvalidInput(error_message=msg)",
            "",
            "    def _validate_port_range(self, rule):",
            "        \"\"\"Check that port_range is valid.\"\"\"",
            "        if rule['port_range_min'] is None and rule['port_range_max'] is None:",
            "            return",
            "        if not rule['protocol']:",
            "            raise ext_sg.SecurityGroupProtocolRequiredWithPorts()",
            "        ip_proto = self._get_ip_proto_number(rule['protocol'])",
            "        # Not all firewall_driver support all these protocols,",
            "        # but being strict here doesn't hurt.",
            "        if (ip_proto in const.SG_PORT_PROTO_NUMS or",
            "                ip_proto in const.SG_PORT_PROTO_NAMES):",
            "            if rule['port_range_min'] == 0 or rule['port_range_max'] == 0:",
            "                raise ext_sg.SecurityGroupInvalidPortValue(port=0)",
            "            if (rule['port_range_min'] is not None and",
            "                    rule['port_range_max'] is not None and",
            "                    rule['port_range_min'] <= rule['port_range_max']):",
            "                # When min/max are the same it is just a single port",
            "                pass",
            "            else:",
            "                raise ext_sg.SecurityGroupInvalidPortRange()",
            "        elif ip_proto in [constants.PROTO_NUM_ICMP,",
            "                          constants.PROTO_NUM_IPV6_ICMP]:",
            "            for attr, field in [('port_range_min', 'type'),",
            "                                ('port_range_max', 'code')]:",
            "                if rule[attr] is not None and not (0 <= rule[attr] <= 255):",
            "                    raise ext_sg.SecurityGroupInvalidIcmpValue(",
            "                        field=field, attr=attr, value=rule[attr])",
            "            if (rule['port_range_min'] is None and",
            "                    rule['port_range_max'] is not None):",
            "                raise ext_sg.SecurityGroupMissingIcmpType(",
            "                    value=rule['port_range_max'])",
            "        else:",
            "            # Only the protocols above support ports, raise otherwise.",
            "            if (rule['port_range_min'] is not None or",
            "                    rule['port_range_max'] is not None):",
            "                port_protocols = (",
            "                    ', '.join(s.upper() for s in const.SG_PORT_PROTO_NAMES))",
            "                raise ext_sg.SecurityGroupInvalidProtocolForPort(",
            "                    protocol=ip_proto, valid_port_protocols=port_protocols)",
            "",
            "    def _make_canonical_port_range(self, rule):",
            "        if (rule['port_range_min'] == constants.PORT_RANGE_MIN and",
            "                rule['port_range_max'] == constants.PORT_RANGE_MAX):",
            "            LOG.info('Project %(project)s added a security group rule '",
            "                     'specifying the entire port range (%(min)s - '",
            "                     '%(max)s). It was automatically converted to not '",
            "                     'have a range to better optimize it for the backend '",
            "                     'security group implementation(s).',",
            "                     {'project': rule['tenant_id'],",
            "                      'min': rule['port_range_min'],",
            "                      'max': rule['port_range_max']})",
            "            rule['port_range_min'] = rule['port_range_max'] = None",
            "",
            "    def _validate_ethertype_and_protocol(self, rule):",
            "        \"\"\"Check if given ethertype and  protocol are valid or not\"\"\"",
            "        if rule['protocol'] in [constants.PROTO_NAME_IPV6_ENCAP,",
            "                                constants.PROTO_NAME_IPV6_FRAG,",
            "                                constants.PROTO_NAME_IPV6_ICMP,",
            "                                constants.PROTO_NAME_IPV6_ICMP_LEGACY,",
            "                                constants.PROTO_NAME_IPV6_NONXT,",
            "                                constants.PROTO_NAME_IPV6_OPTS,",
            "                                constants.PROTO_NAME_IPV6_ROUTE,",
            "                                str(constants.PROTO_NUM_IPV6_ENCAP),",
            "                                str(constants.PROTO_NUM_IPV6_FRAG),",
            "                                str(constants.PROTO_NUM_IPV6_ICMP),",
            "                                str(constants.PROTO_NUM_IPV6_NONXT),",
            "                                str(constants.PROTO_NUM_IPV6_OPTS),",
            "                                str(constants.PROTO_NUM_IPV6_ROUTE)]:",
            "            if rule['ethertype'] == constants.IPv4:",
            "                raise ext_sg.SecurityGroupEthertypeConflictWithProtocol(",
            "                        ethertype=rule['ethertype'], protocol=rule['protocol'])",
            "",
            "    def _validate_single_tenant_and_group(self, security_group_rules):",
            "        \"\"\"Check that all rules belong to the same security group and tenant",
            "        \"\"\"",
            "        sg_groups = set()",
            "        tenants = set()",
            "        for rule_dict in security_group_rules['security_group_rules']:",
            "            rule = rule_dict['security_group_rule']",
            "            sg_groups.add(rule['security_group_id'])",
            "            if len(sg_groups) > 1:",
            "                raise ext_sg.SecurityGroupNotSingleGroupRules()",
            "",
            "            tenants.add(rule['tenant_id'])",
            "            if len(tenants) > 1:",
            "                raise ext_sg.SecurityGroupRulesNotSingleTenant()",
            "        return sg_groups.pop()",
            "",
            "    def _make_canonical_ipv6_icmp_protocol(self, rule):",
            "        if rule.get('ethertype') == constants.IPv6:",
            "            if rule.get('protocol') in const.IPV6_ICMP_LEGACY_PROTO_LIST:",
            "                LOG.info('Project %(project)s added a security group rule '",
            "                         'with legacy IPv6 ICMP protocol name %(protocol)s, '",
            "                         '%(new_protocol)s should be used instead. It was '",
            "                         'automatically converted.',",
            "                         {'project': rule['tenant_id'],",
            "                          'protocol': rule['protocol'],",
            "                          'new_protocol': constants.PROTO_NAME_IPV6_ICMP})",
            "                rule['protocol'] = constants.PROTO_NAME_IPV6_ICMP",
            "            elif rule.get('protocol') == str(constants.PROTO_NUM_ICMP):",
            "                LOG.info('Project %(project)s added a security group rule '",
            "                         'with legacy IPv6 ICMP protocol number %(protocol)s, '",
            "                         '%(new_protocol)s should be used instead. It was '",
            "                         'automatically converted.',",
            "                         {'project': rule['tenant_id'],",
            "                          'protocol': rule['protocol'],",
            "                          'new_protocol': str(constants.PROTO_NUM_IPV6_ICMP)})",
            "                rule['protocol'] = str(constants.PROTO_NUM_IPV6_ICMP)",
            "",
            "    def _validate_security_group_rule(self, context, security_group_rule):",
            "        rule = security_group_rule['security_group_rule']",
            "        self._make_canonical_ipv6_icmp_protocol(rule)",
            "        self._make_canonical_port_range(rule)",
            "        self._validate_port_range(rule)",
            "        self._validate_ip_prefix(rule)",
            "        self._validate_ethertype_and_protocol(rule)",
            "",
            "        if rule['remote_ip_prefix'] and rule['remote_group_id']:",
            "            raise ext_sg.SecurityGroupRemoteGroupAndRemoteIpPrefix()",
            "",
            "        remote_group_id = rule['remote_group_id']",
            "        # Check that remote_group_id exists for tenant",
            "        if remote_group_id:",
            "            self._check_security_group(context, remote_group_id,",
            "                                       tenant_id=rule['tenant_id'])",
            "",
            "        security_group_id = rule['security_group_id']",
            "",
            "        # Confirm that the tenant has permission",
            "        # to add rules to this security group.",
            "        self._check_security_group(context, security_group_id,",
            "                                   tenant_id=rule['tenant_id'])",
            "        return security_group_id",
            "",
            "    @staticmethod",
            "    def _validate_sgs_for_port(security_groups):",
            "        if not security_groups:",
            "            return",
            "        if not len(set(sg.stateful for sg in security_groups)) == 1:",
            "            msg = (\"Cannot apply both stateful and stateless security \"",
            "                   \"groups on the same port at the same time\")",
            "            raise ext_sg.SecurityGroupConflict(reason=msg)",
            "",
            "    def _validate_security_group_rules(self, context, security_group_rules):",
            "        sg_id = self._validate_single_tenant_and_group(security_group_rules)",
            "        for rule in security_group_rules['security_group_rules']:",
            "            self._validate_security_group_rule(context, rule)",
            "        return sg_id",
            "",
            "    def _make_security_group_rule_dict(self, security_group_rule, fields=None):",
            "        res = {'id': security_group_rule['id'],",
            "               'tenant_id': security_group_rule['tenant_id'],",
            "               'security_group_id': security_group_rule['security_group_id'],",
            "               'ethertype': security_group_rule['ethertype'],",
            "               'direction': security_group_rule['direction'],",
            "               'protocol': security_group_rule['protocol'],",
            "               'port_range_min': security_group_rule['port_range_min'],",
            "               'port_range_max': security_group_rule['port_range_max'],",
            "               'remote_ip_prefix': security_group_rule['remote_ip_prefix'],",
            "               'remote_group_id': security_group_rule['remote_group_id']}",
            "",
            "        resource_extend.apply_funcs(ext_sg.SECURITYGROUPRULES, res,",
            "                                    security_group_rule)",
            "        return db_utils.resource_fields(res, fields)",
            "",
            "    def _rule_to_key(self, rule):",
            "        def _normalize_rule_value(key, value):",
            "            # This string is used as a placeholder for str(None), but shorter.",
            "            none_char = '+'",
            "",
            "            if key == 'remote_ip_prefix':",
            "                all_address = ['0.0.0.0/0', '::/0', None]",
            "                if value in all_address:",
            "                    return none_char",
            "            elif value is None:",
            "                return none_char",
            "            elif key == 'protocol':",
            "                return str(self._get_ip_proto_name_and_num(",
            "                               value, ethertype=rule.get('ethertype')))",
            "            return str(value)",
            "",
            "        comparison_keys = [",
            "            'direction',",
            "            'ethertype',",
            "            'port_range_max',",
            "            'port_range_min',",
            "            'protocol',",
            "            'remote_group_id',",
            "            'remote_ip_prefix',",
            "            'security_group_id'",
            "        ]",
            "        return '_'.join([_normalize_rule_value(x, rule.get(x))",
            "                         for x in comparison_keys])",
            "",
            "    def _check_for_duplicate_rules(self, context, security_group_id,",
            "                                   new_security_group_rules):",
            "        # First up, check for any duplicates in the new rules.",
            "        new_rules_set = set()",
            "        for i in new_security_group_rules:",
            "            rule_key = self._rule_to_key(i['security_group_rule'])",
            "            if rule_key in new_rules_set:",
            "                raise ext_sg.DuplicateSecurityGroupRuleInPost(rule=i)",
            "            new_rules_set.add(rule_key)",
            "",
            "        # Now, let's make sure none of the new rules conflict with",
            "        # existing rules; note that we do *not* store the db rules",
            "        # in the set, as we assume they were already checked,",
            "        # when added.",
            "        sg = self.get_security_group(context, security_group_id)",
            "        if sg:",
            "            for i in sg['security_group_rules']:",
            "                rule_key = self._rule_to_key(i)",
            "                if rule_key in new_rules_set:",
            "                    raise ext_sg.SecurityGroupRuleExists(rule_id=i.get('id'))",
            "",
            "    def _validate_ip_prefix(self, rule):",
            "        \"\"\"Check that a valid cidr was specified as remote_ip_prefix",
            "",
            "        No need to check that it is in fact an IP address as this is already",
            "        validated by attribute validators.",
            "        Check that rule ethertype is consistent with remote_ip_prefix ip type.",
            "        Add mask to ip_prefix if absent (192.168.1.10 -> 192.168.1.10/32).",
            "        \"\"\"",
            "        input_prefix = rule['remote_ip_prefix']",
            "        if input_prefix:",
            "            addr = netaddr.IPNetwork(input_prefix)",
            "            # set input_prefix to always include the netmask:",
            "            rule['remote_ip_prefix'] = str(addr)",
            "            # check consistency of ethertype with addr version",
            "            if rule['ethertype'] != \"IPv%d\" % (addr.version):",
            "                raise ext_sg.SecurityGroupRuleParameterConflict(",
            "                    ethertype=rule['ethertype'], cidr=input_prefix)",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def get_security_group_rules(self, context, filters=None, fields=None,",
            "                                 sorts=None, limit=None, marker=None,",
            "                                 page_reverse=False):",
            "        filters = filters or {}",
            "        pager = base_obj.Pager(",
            "            sorts=sorts, marker=marker, limit=limit, page_reverse=page_reverse)",
            "",
            "        project_id = filters.get('project_id') or filters.get('tenant_id')",
            "        if project_id:",
            "            project_id = project_id[0]",
            "        else:",
            "            project_id = context.project_id",
            "        if project_id:",
            "            self._ensure_default_security_group(context, project_id)",
            "",
            "        if not filters and context.project_id and not context.is_admin:",
            "            rule_ids = sg_obj.SecurityGroupRule.get_security_group_rule_ids(",
            "                context.project_id)",
            "            filters = {'id': rule_ids}",
            "",
            "        # NOTE(slaweq): use admin context here to be able to get all rules",
            "        # which fits filters' criteria. Later in policy engine rules will be",
            "        # filtered and only those which are allowed according to policy will",
            "        # be returned",
            "        rule_objs = sg_obj.SecurityGroupRule.get_objects(",
            "            context_lib.get_admin_context(), _pager=pager,",
            "            validate_filters=False, **filters",
            "        )",
            "        return [",
            "            self._make_security_group_rule_dict(obj.db_obj, fields)",
            "            for obj in rule_objs",
            "        ]",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def get_security_group_rule(self, context, id, fields=None):",
            "        # NOTE(slaweq): use admin context here to be able to get all rules",
            "        # which fits filters' criteria. Later in policy engine rules will be",
            "        # filtered and only those which are allowed according to policy will",
            "        # be returned",
            "        security_group_rule = self._get_security_group_rule(",
            "            context_lib.get_admin_context(), id)",
            "        return self._make_security_group_rule_dict(",
            "            security_group_rule.db_obj, fields)",
            "",
            "    def _get_security_group_rule(self, context, id):",
            "        sgr = sg_obj.SecurityGroupRule.get_object(context, id=id)",
            "        if sgr is None:",
            "            raise ext_sg.SecurityGroupRuleNotFound(id=id)",
            "        return sgr",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def delete_security_group_rule(self, context, id):",
            "        kwargs = {",
            "            'context': context,",
            "            'security_group_rule_id': id",
            "        }",
            "        self._registry_notify(resources.SECURITY_GROUP_RULE,",
            "                              events.BEFORE_DELETE, id=id,",
            "                              exc_cls=ext_sg.SecurityGroupRuleInUse, **kwargs)",
            "",
            "        with db_api.CONTEXT_WRITER.using(context):",
            "            sgr = self._get_security_group_rule(context, id)",
            "            kwargs['security_group_id'] = sgr['security_group_id']",
            "            self._registry_notify(resources.SECURITY_GROUP_RULE,",
            "                                  events.PRECOMMIT_DELETE,",
            "                                  exc_cls=ext_sg.SecurityGroupRuleInUse, id=id,",
            "                                  **kwargs)",
            "            sgr.delete()",
            "",
            "        registry.notify(",
            "            resources.SECURITY_GROUP_RULE, events.AFTER_DELETE, self,",
            "            **kwargs)",
            "",
            "    @staticmethod",
            "    @resource_extend.extends([port_def.COLLECTION_NAME])",
            "    def _extend_port_dict_security_group(port_res, port_db):",
            "        # Security group bindings will be retrieved from the SQLAlchemy",
            "        # model. As they're loaded eagerly with ports because of the",
            "        # joined load they will not cause an extra query.",
            "        if isinstance(port_db, port_obj.Port):",
            "            port_res[ext_sg.SECURITYGROUPS] = port_db.security_group_ids",
            "        else:",
            "            security_group_ids = [sec_group_mapping['security_group_id'] for",
            "                                  sec_group_mapping in port_db.security_groups]",
            "            port_res[ext_sg.SECURITYGROUPS] = security_group_ids",
            "        return port_res",
            "",
            "    def _process_port_create_security_group(self, context, port,",
            "                                            security_groups):",
            "        self._validate_sgs_for_port(security_groups)",
            "        if validators.is_attr_set(security_groups):",
            "            for sg in security_groups:",
            "                self._create_port_security_group_binding(context, port['id'],",
            "                                                         sg.id)",
            "        # Convert to list as a set might be passed here and",
            "        # this has to be serialized",
            "        port[ext_sg.SECURITYGROUPS] = ([sg.id for sg in security_groups] if",
            "                                       security_groups else [])",
            "",
            "    def _get_default_sg_id(self, context, tenant_id):",
            "        default_group = sg_obj.DefaultSecurityGroup.get_object(",
            "            context,",
            "            project_id=tenant_id,",
            "        )",
            "        if default_group:",
            "            return default_group.security_group_id",
            "",
            "    @registry.receives(resources.PORT, [events.BEFORE_CREATE,",
            "                                        events.BEFORE_UPDATE])",
            "    @registry.receives(resources.NETWORK, [events.BEFORE_CREATE])",
            "    def _ensure_default_security_group_handler(self, resource, event, trigger,",
            "                                               context, **kwargs):",
            "        if event == events.BEFORE_UPDATE:",
            "            tenant_id = kwargs['original_' + resource]['tenant_id']",
            "        else:",
            "            tenant_id = kwargs[resource]['tenant_id']",
            "        if tenant_id:",
            "            self._ensure_default_security_group(context, tenant_id)",
            "",
            "    def _ensure_default_security_group(self, context, tenant_id):",
            "        \"\"\"Create a default security group if one doesn't exist.",
            "",
            "        :returns: the default security group id for given tenant.",
            "        \"\"\"",
            "        if not extensions.is_extension_supported(self, 'security-group'):",
            "            return",
            "        default_group_id = self._get_default_sg_id(context, tenant_id)",
            "        if default_group_id:",
            "            return default_group_id",
            "",
            "        security_group = {",
            "            'security_group':",
            "                {'name': 'default',",
            "                 'tenant_id': tenant_id,",
            "                 'description': DEFAULT_SG_DESCRIPTION}",
            "        }",
            "        try:",
            "            return self.create_security_group(context, security_group,",
            "                                              default_sg=True)['id']",
            "        except obj_exc.NeutronDbObjectDuplicateEntry:",
            "            return self._get_default_sg_id(context, tenant_id)",
            "",
            "    def _get_security_groups_on_port(self, context, port):",
            "        \"\"\"Check that all security groups on port belong to tenant.",
            "",
            "        :returns: all security groups on port belonging to tenant)",
            "",
            "        \"\"\"",
            "        port = port['port']",
            "        if not validators.is_attr_set(port.get(ext_sg.SECURITYGROUPS)):",
            "            return",
            "        if port.get('device_owner') and net.is_port_trusted(port):",
            "            return",
            "",
            "        port_sg = port.get(ext_sg.SECURITYGROUPS, [])",
            "        tenant_id = port.get('tenant_id')",
            "",
            "        sg_objs = sg_obj.SecurityGroup.get_objects(context, id=port_sg)",
            "",
            "        valid_groups = set(",
            "            g.id for g in sg_objs",
            "            if (context.is_admin or not tenant_id or",
            "                g.tenant_id == tenant_id or",
            "                sg_obj.SecurityGroup.is_shared_with_tenant(",
            "                    context, g.id, tenant_id))",
            "        )",
            "",
            "        requested_groups = set(port_sg)",
            "        port_sg_missing = requested_groups - valid_groups",
            "        if port_sg_missing:",
            "            raise ext_sg.SecurityGroupNotFound(id=', '.join(port_sg_missing))",
            "",
            "        return sg_objs",
            "",
            "    def _ensure_default_security_group_on_port(self, context, port):",
            "        # we don't apply security groups for dhcp, router",
            "        port = port['port']",
            "        if port.get('device_owner') and net.is_port_trusted(port):",
            "            return",
            "        port_sg = port.get(ext_sg.SECURITYGROUPS)",
            "        if port_sg is None or not validators.is_attr_set(port_sg):",
            "            port_project = port.get('tenant_id')",
            "            default_sg = self._ensure_default_security_group(context,",
            "                                                             port_project)",
            "            if default_sg:",
            "                port[ext_sg.SECURITYGROUPS] = [default_sg]",
            "",
            "    def _check_update_deletes_security_groups(self, port):",
            "        \"\"\"Return True if port has as a security group and it's value",
            "        is either [] or not is_attr_set, otherwise return False",
            "        \"\"\"",
            "        if (ext_sg.SECURITYGROUPS in port['port'] and",
            "            not (validators.is_attr_set(",
            "                     port['port'][ext_sg.SECURITYGROUPS]) and",
            "                 port['port'][ext_sg.SECURITYGROUPS] != [])):",
            "            return True",
            "        return False",
            "",
            "    def _check_update_has_security_groups(self, port):",
            "        \"\"\"Return True if port has security_groups attribute set and",
            "        its not empty, or False otherwise.",
            "        This method is called both for port create and port update.",
            "        \"\"\"",
            "        if (ext_sg.SECURITYGROUPS in port['port'] and",
            "            (validators.is_attr_set(port['port'][ext_sg.SECURITYGROUPS]) and",
            "             port['port'][ext_sg.SECURITYGROUPS] != [])):",
            "            return True",
            "        return False",
            "",
            "    def update_security_group_on_port(self, context, id, port,",
            "                                      original_port, updated_port):",
            "        \"\"\"Update security groups on port.",
            "",
            "        This method returns a flag which indicates request notification",
            "        is required and does not perform notification itself.",
            "        It is because another changes for the port may require notification.",
            "        \"\"\"",
            "        need_notify = False",
            "        port_updates = port['port']",
            "        if (ext_sg.SECURITYGROUPS in port_updates and",
            "            not helpers.compare_elements(",
            "                original_port.get(ext_sg.SECURITYGROUPS),",
            "                port_updates[ext_sg.SECURITYGROUPS])):",
            "            # delete the port binding and read it with the new rules",
            "            sgs = self._get_security_groups_on_port(context, port)",
            "            port_updates[ext_sg.SECURITYGROUPS] = [sg.id for sg in sgs]",
            "            self._delete_port_security_group_bindings(context, id)",
            "            self._process_port_create_security_group(",
            "                context,",
            "                updated_port,",
            "                sgs)",
            "            need_notify = True",
            "        else:",
            "            updated_port[ext_sg.SECURITYGROUPS] = (",
            "                original_port[ext_sg.SECURITYGROUPS])",
            "        return need_notify"
        ],
        "afterPatchFile": [
            "# Copyright 2012 VMware, Inc.  All rights reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "import netaddr",
            "from neutron_lib.api.definitions import port as port_def",
            "from neutron_lib.api import extensions",
            "from neutron_lib.api import validators",
            "from neutron_lib.callbacks import events",
            "from neutron_lib.callbacks import exceptions",
            "from neutron_lib.callbacks import registry",
            "from neutron_lib.callbacks import resources",
            "from neutron_lib import constants",
            "from neutron_lib import context as context_lib",
            "from neutron_lib.db import api as db_api",
            "from neutron_lib.db import model_query",
            "from neutron_lib.db import resource_extend",
            "from neutron_lib.db import utils as db_utils",
            "from neutron_lib import exceptions as n_exc",
            "from neutron_lib.objects import exceptions as obj_exc",
            "from neutron_lib.utils import helpers",
            "from neutron_lib.utils import net",
            "from oslo_log import log as logging",
            "from oslo_utils import uuidutils",
            "from sqlalchemy.orm import scoped_session",
            "",
            "from neutron._i18n import _",
            "from neutron.common import _constants as const",
            "from neutron.db.models import securitygroup as sg_models",
            "from neutron.db import rbac_db_mixin as rbac_mixin",
            "from neutron.extensions import securitygroup as ext_sg",
            "from neutron.objects import base as base_obj",
            "from neutron.objects import ports as port_obj",
            "from neutron.objects import securitygroup as sg_obj",
            "from neutron import quota",
            "",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "DEFAULT_SG_DESCRIPTION = _('Default security group')",
            "",
            "",
            "@resource_extend.has_resource_extenders",
            "@registry.has_registry_receivers",
            "class SecurityGroupDbMixin(ext_sg.SecurityGroupPluginBase,",
            "                           rbac_mixin.RbacPluginMixin):",
            "    \"\"\"Mixin class to add security group to db_base_plugin_v2.\"\"\"",
            "",
            "    __native_bulk_support = True",
            "",
            "    def create_security_group_bulk(self, context, security_groups):",
            "        return self._create_bulk('security_group', context,",
            "                                 security_groups)",
            "",
            "    def _registry_notify(self, res, event, id=None, exc_cls=None, **kwargs):",
            "        # NOTE(armax): a callback exception here will prevent the request",
            "        # from being processed. This is a hook point for backend's validation;",
            "        # we raise to propagate the reason for the failure.",
            "        try:",
            "            if 'payload' in kwargs:",
            "                # TODO(boden): remove shim once all callbacks use payloads",
            "                registry.publish(res, event, self, payload=kwargs['payload'])",
            "            else:",
            "                registry.notify(res, event, self, **kwargs)",
            "        except exceptions.CallbackFailure as e:",
            "            if exc_cls:",
            "                reason = (_('cannot perform %(event)s due to %(reason)s') %",
            "                          {'event': event, 'reason': e})",
            "                raise exc_cls(reason=reason, id=id)",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def create_security_group(self, context, security_group, default_sg=False):",
            "        \"\"\"Create security group.",
            "",
            "        If default_sg is true that means we are a default security group for",
            "        a given tenant if it does not exist.",
            "        \"\"\"",
            "        s = security_group['security_group']",
            "        kwargs = {",
            "            'context': context,",
            "            'security_group': s,",
            "            'is_default': default_sg,",
            "        }",
            "        self._registry_notify(resources.SECURITY_GROUP, events.BEFORE_CREATE,",
            "                              exc_cls=ext_sg.SecurityGroupConflict,",
            "                              payload=events.DBEventPayload(",
            "                                  context, metadata={'is_default': default_sg},",
            "                                  request_body=security_group,",
            "                                  desired_state=s))",
            "",
            "        tenant_id = s['tenant_id']",
            "        stateful = s.get('stateful', True)",
            "",
            "        if not default_sg:",
            "            self._ensure_default_security_group(context, tenant_id)",
            "        else:",
            "            existing_def_sg_id = self._get_default_sg_id(context, tenant_id)",
            "            if existing_def_sg_id is not None:",
            "                # default already exists, return it",
            "                return self.get_security_group(context, existing_def_sg_id)",
            "",
            "        with db_api.CONTEXT_WRITER.using(context):",
            "            sg = sg_obj.SecurityGroup(",
            "                context, id=s.get('id') or uuidutils.generate_uuid(),",
            "                description=s['description'], project_id=tenant_id,",
            "                name=s['name'], is_default=default_sg, stateful=stateful)",
            "            sg.create()",
            "",
            "            delta = len(ext_sg.sg_supported_ethertypes)",
            "            delta = delta * 2 if default_sg else delta",
            "            reservation = quota.QUOTAS.make_reservation(",
            "                context, tenant_id, {'security_group_rule': delta},",
            "                self)",
            "",
            "            for ethertype in ext_sg.sg_supported_ethertypes:",
            "                if default_sg:",
            "                    # Allow intercommunication",
            "                    ingress_rule = sg_obj.SecurityGroupRule(",
            "                        context, id=uuidutils.generate_uuid(),",
            "                        project_id=tenant_id, security_group_id=sg.id,",
            "                        direction='ingress', ethertype=ethertype,",
            "                        remote_group_id=sg.id)",
            "                    ingress_rule.create()",
            "                    sg.rules.append(ingress_rule)",
            "",
            "                egress_rule = sg_obj.SecurityGroupRule(",
            "                    context, id=uuidutils.generate_uuid(),",
            "                    project_id=tenant_id, security_group_id=sg.id,",
            "                    direction='egress', ethertype=ethertype)",
            "                egress_rule.create()",
            "                sg.rules.append(egress_rule)",
            "            sg.obj_reset_changes(['rules'])",
            "",
            "            quota.QUOTAS.commit_reservation(context,",
            "                                            reservation.reservation_id)",
            "",
            "            # fetch sg from db to load the sg rules with sg model.",
            "            sg = sg_obj.SecurityGroup.get_object(context, id=sg.id)",
            "            secgroup_dict = self._make_security_group_dict(sg)",
            "            kwargs['security_group'] = secgroup_dict",
            "            self._registry_notify(resources.SECURITY_GROUP,",
            "                                  events.PRECOMMIT_CREATE,",
            "                                  exc_cls=ext_sg.SecurityGroupConflict,",
            "                                  **kwargs)",
            "",
            "        registry.notify(resources.SECURITY_GROUP, events.AFTER_CREATE, self,",
            "                        **kwargs)",
            "        return secgroup_dict",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def get_security_groups(self, context, filters=None, fields=None,",
            "                            sorts=None, limit=None,",
            "                            marker=None, page_reverse=False, default_sg=False):",
            "",
            "        # If default_sg is True do not call _ensure_default_security_group()",
            "        # so this can be done recursively. Context.tenant_id is checked",
            "        # because all the unit tests do not explicitly set the context on",
            "        # GETS. TODO(arosen)  context handling can probably be improved here.",
            "        filters = filters or {}",
            "        if not default_sg and context.tenant_id:",
            "            tenant_id = filters.get('project_id') or filters.get('tenant_id')",
            "            if tenant_id:",
            "                tenant_id = tenant_id[0]",
            "            else:",
            "                tenant_id = context.tenant_id",
            "            self._ensure_default_security_group(context, tenant_id)",
            "",
            "        pager = base_obj.Pager(",
            "            sorts=sorts, limit=limit, marker=marker, page_reverse=page_reverse)",
            "",
            "        sg_objs = sg_obj.SecurityGroup.get_objects(",
            "            context, _pager=pager, validate_filters=False,",
            "            fields=fields, **filters)",
            "",
            "        return [self._make_security_group_dict(obj, fields) for obj in sg_objs]",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def get_security_groups_count(self, context, filters=None):",
            "        filters = filters or {}",
            "        return sg_obj.SecurityGroup.count(",
            "            context, validate_filters=False, **filters)",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def get_security_group(self, context, id, fields=None, tenant_id=None):",
            "        \"\"\"Tenant id is given to handle the case when creating a security",
            "        group rule on behalf of another use.",
            "        \"\"\"",
            "        if tenant_id:",
            "            tmp_context_tenant_id = context.tenant_id",
            "            context.tenant_id = tenant_id",
            "",
            "        try:",
            "            with db_api.CONTEXT_READER.using(context):",
            "                ret = self._make_security_group_dict(self._get_security_group(",
            "                                                     context, id,",
            "                                                     fields=fields),",
            "                                                     fields)",
            "                if (fields is None or len(fields) == 0 or",
            "                   'security_group_rules' in fields):",
            "                    rules = self.get_security_group_rules(",
            "                        context_lib.get_admin_context(),",
            "                        {'security_group_id': [id]})",
            "                    ret['security_group_rules'] = rules",
            "",
            "        finally:",
            "            if tenant_id:",
            "                context.tenant_id = tmp_context_tenant_id",
            "        return ret",
            "",
            "    def _get_security_group(self, context, id, fields=None):",
            "        sg = sg_obj.SecurityGroup.get_object(context, fields=fields, id=id)",
            "        if sg is None:",
            "            raise ext_sg.SecurityGroupNotFound(id=id)",
            "        return sg",
            "",
            "    def _check_security_group(self, context, id, tenant_id=None):",
            "        if tenant_id:",
            "            tmp_context_tenant_id = context.tenant_id",
            "            context.tenant_id = tenant_id",
            "",
            "        try:",
            "            if not sg_obj.SecurityGroup.objects_exist(context, id=id):",
            "                raise ext_sg.SecurityGroupNotFound(id=id)",
            "        finally:",
            "            if tenant_id:",
            "                context.tenant_id = tmp_context_tenant_id",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def delete_security_group(self, context, id):",
            "        filters = {'security_group_id': [id]}",
            "        with db_api.CONTEXT_READER.using(context):",
            "            ports = self._get_port_security_group_bindings(context, filters)",
            "            if ports:",
            "                raise ext_sg.SecurityGroupInUse(id=id)",
            "            # confirm security group exists",
            "            sg = self._get_security_group(context, id, fields=['id', 'name'])",
            "",
            "            if sg['name'] == 'default' and not context.is_admin:",
            "                raise ext_sg.SecurityGroupCannotRemoveDefault()",
            "        kwargs = {",
            "            'context': context,",
            "            'security_group_id': id,",
            "            'security_group': sg,",
            "        }",
            "        self._registry_notify(resources.SECURITY_GROUP,",
            "                              events.BEFORE_DELETE,",
            "                              exc_cls=ext_sg.SecurityGroupInUse, id=id,",
            "                              payload=events.DBEventPayload(",
            "                                  context, states=(sg,), resource_id=id))",
            "",
            "        with db_api.CONTEXT_WRITER.using(context):",
            "            # pass security_group_rule_ids to ensure",
            "            # consistency with deleted rules",
            "            # get security_group_bindings and security_group one more time",
            "            # so that they will be attached for session where sg will be",
            "            # deleted",
            "            ports = self._get_port_security_group_bindings(context, filters)",
            "            sg = self._get_security_group(context, id)",
            "            kwargs['security_group_rule_ids'] = [r['id'] for r in sg.rules]",
            "            kwargs['security_group'] = self._make_security_group_dict(sg)",
            "            self._registry_notify(resources.SECURITY_GROUP,",
            "                                  events.PRECOMMIT_DELETE,",
            "                                  exc_cls=ext_sg.SecurityGroupInUse, id=id,",
            "                                  **kwargs)",
            "            sg.delete()",
            "",
            "        kwargs.pop('security_group')",
            "        kwargs['name'] = sg['name']",
            "        registry.notify(resources.SECURITY_GROUP, events.AFTER_DELETE,",
            "                        self, **kwargs)",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def update_security_group(self, context, id, security_group):",
            "        s = security_group['security_group']",
            "",
            "        if 'stateful' in s:",
            "            with db_api.CONTEXT_READER.using(context):",
            "                sg = self._get_security_group(context, id)",
            "                if s['stateful'] != sg['stateful']:",
            "                    filters = {'security_group_id': [id]}",
            "                    ports = self._get_port_security_group_bindings(context,",
            "                                                                   filters)",
            "                    if ports:",
            "                        raise ext_sg.SecurityGroupInUse(id=id)",
            "",
            "        kwargs = {",
            "            'context': context,",
            "            'security_group_id': id,",
            "            'security_group': s,",
            "        }",
            "        self._registry_notify(resources.SECURITY_GROUP, events.BEFORE_UPDATE,",
            "                              exc_cls=ext_sg.SecurityGroupConflict, **kwargs)",
            "",
            "        with db_api.CONTEXT_WRITER.using(context):",
            "            sg = self._get_security_group(context, id)",
            "            if sg.name == 'default' and 'name' in s:",
            "                raise ext_sg.SecurityGroupCannotUpdateDefault()",
            "            sg_dict = self._make_security_group_dict(sg)",
            "            kwargs['original_security_group'] = sg_dict",
            "            sg.update_fields(s)",
            "            sg.update()",
            "            sg_dict = self._make_security_group_dict(sg)",
            "            kwargs['security_group'] = sg_dict",
            "            self._registry_notify(",
            "                    resources.SECURITY_GROUP,",
            "                    events.PRECOMMIT_UPDATE,",
            "                    exc_cls=ext_sg.SecurityGroupConflict,",
            "                    payload=events.DBEventPayload(",
            "                        context, request_body=s,",
            "                        states=(kwargs['original_security_group'],),",
            "                        resource_id=id, desired_state=sg_dict))",
            "        registry.notify(resources.SECURITY_GROUP, events.AFTER_UPDATE, self,",
            "                        **kwargs)",
            "        return sg_dict",
            "",
            "    def _make_security_group_dict(self, security_group, fields=None):",
            "        res = {'id': security_group['id'],",
            "               'name': security_group['name'],",
            "               'stateful': security_group['stateful'],",
            "               'tenant_id': security_group['tenant_id'],",
            "               'description': security_group['description']}",
            "        if security_group.rules:",
            "            res['security_group_rules'] = [",
            "                self._make_security_group_rule_dict(r.db_obj)",
            "                for r in security_group.rules",
            "            ]",
            "        else:",
            "            res['security_group_rules'] = []",
            "        resource_extend.apply_funcs(ext_sg.SECURITYGROUPS, res,",
            "                                    security_group.db_obj)",
            "        return db_utils.resource_fields(res, fields)",
            "",
            "    @staticmethod",
            "    def _make_security_group_binding_dict(security_group, fields=None):",
            "        res = {'port_id': security_group['port_id'],",
            "               'security_group_id': security_group['security_group_id']}",
            "        return db_utils.resource_fields(res, fields)",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def _create_port_security_group_binding(self, context, port_id,",
            "                                            security_group_id):",
            "        with db_api.CONTEXT_WRITER.using(context):",
            "            db = sg_models.SecurityGroupPortBinding(",
            "                port_id=port_id, security_group_id=security_group_id)",
            "            context.session.add(db)",
            "",
            "    def _get_port_security_group_bindings(self, context,",
            "                                          filters=None, fields=None):",
            "        return model_query.get_collection(",
            "            context, sg_models.SecurityGroupPortBinding,",
            "            self._make_security_group_binding_dict,",
            "            filters=filters, fields=fields)",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def _delete_port_security_group_bindings(self, context, port_id):",
            "        with db_api.CONTEXT_WRITER.using(context):",
            "            query = model_query.query_with_hooks(",
            "                context, sg_models.SecurityGroupPortBinding)",
            "            bindings = query.filter(",
            "                sg_models.SecurityGroupPortBinding.port_id == port_id)",
            "            for binding in bindings:",
            "                context.session.delete(binding)",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def create_security_group_rule_bulk(self, context, security_group_rules):",
            "        return self._create_bulk('security_group_rule', context,",
            "                                 security_group_rules)",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def create_security_group_rule_bulk_native(self, context,",
            "                                               security_group_rules):",
            "        rules = security_group_rules['security_group_rules']",
            "        scoped_session(context.session)",
            "        security_group_id = self._validate_security_group_rules(",
            "            context, security_group_rules)",
            "        with db_api.CONTEXT_WRITER.using(context):",
            "            self._check_for_duplicate_rules(context, security_group_id, rules)",
            "            ret = []",
            "            for rule_dict in rules:",
            "                res_rule_dict = self._create_security_group_rule(",
            "                    context, rule_dict, validate=False)",
            "                ret.append(res_rule_dict)",
            "        for rdict in ret:",
            "            registry.notify(",
            "                resources.SECURITY_GROUP_RULE, events.AFTER_CREATE, self,",
            "                context=context, security_group_rule=rdict)",
            "        return ret",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def create_security_group_rule(self, context, security_group_rule):",
            "        res = self._create_security_group_rule(context, security_group_rule)",
            "        registry.notify(",
            "            resources.SECURITY_GROUP_RULE, events.AFTER_CREATE, self,",
            "            context=context, security_group_rule=res)",
            "        return res",
            "",
            "    def _create_security_group_rule(self, context, security_group_rule,",
            "                                    validate=True):",
            "        if validate:",
            "            sg_id = self._validate_security_group_rule(context,",
            "                                                       security_group_rule)",
            "        rule_dict = security_group_rule['security_group_rule']",
            "        remote_ip_prefix = rule_dict.get('remote_ip_prefix')",
            "        if remote_ip_prefix:",
            "            remote_ip_prefix = net.AuthenticIPNetwork(remote_ip_prefix)",
            "",
            "        protocol = rule_dict.get('protocol')",
            "        if protocol:",
            "            # object expects strings only",
            "            protocol = str(protocol)",
            "",
            "        args = {",
            "            'id': (rule_dict.get('id') or uuidutils.generate_uuid()),",
            "            'project_id': rule_dict['tenant_id'],",
            "            'security_group_id': rule_dict['security_group_id'],",
            "            'direction': rule_dict['direction'],",
            "            'remote_group_id': rule_dict.get('remote_group_id'),",
            "            'ethertype': rule_dict['ethertype'],",
            "            'protocol': protocol,",
            "            'remote_ip_prefix': remote_ip_prefix,",
            "            'description': rule_dict.get('description'),",
            "        }",
            "",
            "        port_range_min = self._safe_int(rule_dict['port_range_min'])",
            "        if port_range_min is not None:",
            "            args['port_range_min'] = port_range_min",
            "",
            "        port_range_max = self._safe_int(rule_dict['port_range_max'])",
            "        if port_range_max is not None:",
            "            args['port_range_max'] = port_range_max",
            "",
            "        kwargs = {",
            "            'context': context,",
            "            'security_group_rule': args",
            "        }",
            "        self._registry_notify(resources.SECURITY_GROUP_RULE,",
            "                              events.BEFORE_CREATE,",
            "                              exc_cls=ext_sg.SecurityGroupConflict, **kwargs)",
            "        with db_api.CONTEXT_WRITER.using(context):",
            "            if validate:",
            "                self._check_for_duplicate_rules(context, sg_id,",
            "                                                [security_group_rule])",
            "            sg_rule = sg_obj.SecurityGroupRule(context, **args)",
            "            sg_rule.create()",
            "",
            "            # fetch sg_rule from db to load the sg rules with sg model",
            "            # otherwise a DetachedInstanceError can occur for model extensions",
            "            sg_rule = sg_obj.SecurityGroupRule.get_object(context,",
            "                                                          id=sg_rule.id)",
            "            res_rule_dict = self._make_security_group_rule_dict(sg_rule.db_obj)",
            "            kwargs['security_group_rule'] = res_rule_dict",
            "            self._registry_notify(",
            "                resources.SECURITY_GROUP_RULE,",
            "                events.PRECOMMIT_CREATE,",
            "                exc_cls=ext_sg.SecurityGroupConflict, **kwargs)",
            "        return res_rule_dict",
            "",
            "    def _get_ip_proto_number(self, protocol):",
            "        if protocol is None:",
            "            return",
            "        # According to bug 1381379, protocol is always set to string to avoid",
            "        # problems with comparing int and string in PostgreSQL. Here this",
            "        # string is converted to int to give an opportunity to use it as",
            "        # before.",
            "        if protocol in constants.IP_PROTOCOL_NAME_ALIASES:",
            "            protocol = constants.IP_PROTOCOL_NAME_ALIASES[protocol]",
            "        return int(constants.IP_PROTOCOL_MAP.get(protocol, protocol))",
            "",
            "    def _get_ip_proto_name_and_num(self, protocol, ethertype=None):",
            "        if protocol is None:",
            "            return",
            "        protocol = str(protocol)",
            "        # Force all legacy IPv6 ICMP protocol names to be 'ipv6-icmp', and",
            "        # protocol number 1 to be 58",
            "        if ethertype == constants.IPv6:",
            "            if protocol in const.IPV6_ICMP_LEGACY_PROTO_LIST:",
            "                protocol = constants.PROTO_NAME_IPV6_ICMP",
            "            elif protocol == str(constants.PROTO_NUM_ICMP):",
            "                protocol = str(constants.PROTO_NUM_IPV6_ICMP)",
            "        if protocol in constants.IP_PROTOCOL_MAP:",
            "            return [protocol, str(constants.IP_PROTOCOL_MAP.get(protocol))]",
            "        elif protocol in constants.IP_PROTOCOL_NUM_TO_NAME_MAP:",
            "            return [constants.IP_PROTOCOL_NUM_TO_NAME_MAP.get(protocol),",
            "                    protocol]",
            "        return [protocol, protocol]",
            "",
            "    def _safe_int(self, port_range):",
            "        if port_range is None:",
            "            return",
            "        try:",
            "            return int(port_range)",
            "        except (ValueError, TypeError):",
            "            msg = \"port range must be an integer\"",
            "            raise n_exc.InvalidInput(error_message=msg)",
            "",
            "    def _validate_port_range(self, rule):",
            "        \"\"\"Check that port_range is valid.\"\"\"",
            "        if rule['port_range_min'] is None and rule['port_range_max'] is None:",
            "            return",
            "        if not rule['protocol']:",
            "            raise ext_sg.SecurityGroupProtocolRequiredWithPorts()",
            "        ip_proto = self._get_ip_proto_number(rule['protocol'])",
            "        # Not all firewall_driver support all these protocols,",
            "        # but being strict here doesn't hurt.",
            "        if (ip_proto in const.SG_PORT_PROTO_NUMS or",
            "                ip_proto in const.SG_PORT_PROTO_NAMES):",
            "            if rule['port_range_min'] == 0 or rule['port_range_max'] == 0:",
            "                raise ext_sg.SecurityGroupInvalidPortValue(port=0)",
            "            if (rule['port_range_min'] is not None and",
            "                    rule['port_range_max'] is not None and",
            "                    rule['port_range_min'] <= rule['port_range_max']):",
            "                # When min/max are the same it is just a single port",
            "                pass",
            "            else:",
            "                raise ext_sg.SecurityGroupInvalidPortRange()",
            "        elif ip_proto in [constants.PROTO_NUM_ICMP,",
            "                          constants.PROTO_NUM_IPV6_ICMP]:",
            "            for attr, field in [('port_range_min', 'type'),",
            "                                ('port_range_max', 'code')]:",
            "                if rule[attr] is not None and not (0 <= rule[attr] <= 255):",
            "                    raise ext_sg.SecurityGroupInvalidIcmpValue(",
            "                        field=field, attr=attr, value=rule[attr])",
            "            if (rule['port_range_min'] is None and",
            "                    rule['port_range_max'] is not None):",
            "                raise ext_sg.SecurityGroupMissingIcmpType(",
            "                    value=rule['port_range_max'])",
            "        else:",
            "            # Only the protocols above support ports, raise otherwise.",
            "            if (rule['port_range_min'] is not None or",
            "                    rule['port_range_max'] is not None):",
            "                port_protocols = (",
            "                    ', '.join(s.upper() for s in const.SG_PORT_PROTO_NAMES))",
            "                raise ext_sg.SecurityGroupInvalidProtocolForPort(",
            "                    protocol=ip_proto, valid_port_protocols=port_protocols)",
            "",
            "    def _make_canonical_port_range(self, rule):",
            "        if (rule['port_range_min'] == constants.PORT_RANGE_MIN and",
            "                rule['port_range_max'] == constants.PORT_RANGE_MAX):",
            "            LOG.info('Project %(project)s added a security group rule '",
            "                     'specifying the entire port range (%(min)s - '",
            "                     '%(max)s). It was automatically converted to not '",
            "                     'have a range to better optimize it for the backend '",
            "                     'security group implementation(s).',",
            "                     {'project': rule['tenant_id'],",
            "                      'min': rule['port_range_min'],",
            "                      'max': rule['port_range_max']})",
            "            rule['port_range_min'] = rule['port_range_max'] = None",
            "",
            "    def _validate_ethertype_and_protocol(self, rule):",
            "        \"\"\"Check if given ethertype and  protocol are valid or not\"\"\"",
            "        if rule['protocol'] in [constants.PROTO_NAME_IPV6_ENCAP,",
            "                                constants.PROTO_NAME_IPV6_FRAG,",
            "                                constants.PROTO_NAME_IPV6_ICMP,",
            "                                constants.PROTO_NAME_IPV6_ICMP_LEGACY,",
            "                                constants.PROTO_NAME_IPV6_NONXT,",
            "                                constants.PROTO_NAME_IPV6_OPTS,",
            "                                constants.PROTO_NAME_IPV6_ROUTE,",
            "                                str(constants.PROTO_NUM_IPV6_ENCAP),",
            "                                str(constants.PROTO_NUM_IPV6_FRAG),",
            "                                str(constants.PROTO_NUM_IPV6_ICMP),",
            "                                str(constants.PROTO_NUM_IPV6_NONXT),",
            "                                str(constants.PROTO_NUM_IPV6_OPTS),",
            "                                str(constants.PROTO_NUM_IPV6_ROUTE)]:",
            "            if rule['ethertype'] == constants.IPv4:",
            "                raise ext_sg.SecurityGroupEthertypeConflictWithProtocol(",
            "                        ethertype=rule['ethertype'], protocol=rule['protocol'])",
            "",
            "    def _validate_single_tenant_and_group(self, security_group_rules):",
            "        \"\"\"Check that all rules belong to the same security group and tenant",
            "        \"\"\"",
            "        sg_groups = set()",
            "        tenants = set()",
            "        for rule_dict in security_group_rules['security_group_rules']:",
            "            rule = rule_dict['security_group_rule']",
            "            sg_groups.add(rule['security_group_id'])",
            "            if len(sg_groups) > 1:",
            "                raise ext_sg.SecurityGroupNotSingleGroupRules()",
            "",
            "            tenants.add(rule['tenant_id'])",
            "            if len(tenants) > 1:",
            "                raise ext_sg.SecurityGroupRulesNotSingleTenant()",
            "        return sg_groups.pop()",
            "",
            "    def _make_canonical_ipv6_icmp_protocol(self, rule):",
            "        if rule.get('ethertype') == constants.IPv6:",
            "            if rule.get('protocol') in const.IPV6_ICMP_LEGACY_PROTO_LIST:",
            "                LOG.info('Project %(project)s added a security group rule '",
            "                         'with legacy IPv6 ICMP protocol name %(protocol)s, '",
            "                         '%(new_protocol)s should be used instead. It was '",
            "                         'automatically converted.',",
            "                         {'project': rule['tenant_id'],",
            "                          'protocol': rule['protocol'],",
            "                          'new_protocol': constants.PROTO_NAME_IPV6_ICMP})",
            "                rule['protocol'] = constants.PROTO_NAME_IPV6_ICMP",
            "            elif rule.get('protocol') == str(constants.PROTO_NUM_ICMP):",
            "                LOG.info('Project %(project)s added a security group rule '",
            "                         'with legacy IPv6 ICMP protocol number %(protocol)s, '",
            "                         '%(new_protocol)s should be used instead. It was '",
            "                         'automatically converted.',",
            "                         {'project': rule['tenant_id'],",
            "                          'protocol': rule['protocol'],",
            "                          'new_protocol': str(constants.PROTO_NUM_IPV6_ICMP)})",
            "                rule['protocol'] = str(constants.PROTO_NUM_IPV6_ICMP)",
            "",
            "    def _validate_security_group_rule(self, context, security_group_rule):",
            "        rule = security_group_rule['security_group_rule']",
            "        self._make_canonical_ipv6_icmp_protocol(rule)",
            "        self._make_canonical_port_range(rule)",
            "        self._validate_port_range(rule)",
            "        self._validate_ip_prefix(rule)",
            "        self._validate_ethertype_and_protocol(rule)",
            "",
            "        if rule['remote_ip_prefix'] and rule['remote_group_id']:",
            "            raise ext_sg.SecurityGroupRemoteGroupAndRemoteIpPrefix()",
            "",
            "        remote_group_id = rule['remote_group_id']",
            "        # Check that remote_group_id exists for tenant",
            "        if remote_group_id:",
            "            self._check_security_group(context, remote_group_id,",
            "                                       tenant_id=rule['tenant_id'])",
            "",
            "        security_group_id = rule['security_group_id']",
            "",
            "        # Confirm that the tenant has permission",
            "        # to add rules to this security group.",
            "        self._check_security_group(context, security_group_id,",
            "                                   tenant_id=rule['tenant_id'])",
            "        return security_group_id",
            "",
            "    @staticmethod",
            "    def _validate_sgs_for_port(security_groups):",
            "        if not security_groups:",
            "            return",
            "        if not len(set(sg.stateful for sg in security_groups)) == 1:",
            "            msg = (\"Cannot apply both stateful and stateless security \"",
            "                   \"groups on the same port at the same time\")",
            "            raise ext_sg.SecurityGroupConflict(reason=msg)",
            "",
            "    def _validate_security_group_rules(self, context, security_group_rules):",
            "        sg_id = self._validate_single_tenant_and_group(security_group_rules)",
            "        for rule in security_group_rules['security_group_rules']:",
            "            self._validate_security_group_rule(context, rule)",
            "        return sg_id",
            "",
            "    def _make_security_group_rule_dict(self, security_group_rule, fields=None):",
            "        res = {'id': security_group_rule['id'],",
            "               'tenant_id': security_group_rule['tenant_id'],",
            "               'security_group_id': security_group_rule['security_group_id'],",
            "               'ethertype': security_group_rule['ethertype'],",
            "               'direction': security_group_rule['direction'],",
            "               'protocol': security_group_rule['protocol'],",
            "               'port_range_min': security_group_rule['port_range_min'],",
            "               'port_range_max': security_group_rule['port_range_max'],",
            "               'remote_ip_prefix': security_group_rule['remote_ip_prefix'],",
            "               'remote_group_id': security_group_rule['remote_group_id']}",
            "",
            "        resource_extend.apply_funcs(ext_sg.SECURITYGROUPRULES, res,",
            "                                    security_group_rule)",
            "        return db_utils.resource_fields(res, fields)",
            "",
            "    def _rule_to_key(self, rule):",
            "        def _normalize_rule_value(key, value):",
            "            # This string is used as a placeholder for str(None), but shorter.",
            "            none_char = '+'",
            "",
            "            if key == 'remote_ip_prefix':",
            "                all_address = ['0.0.0.0/0', '::/0', None]",
            "                if value in all_address:",
            "                    return none_char",
            "            elif value is None:",
            "                return none_char",
            "            elif key == 'protocol':",
            "                return str(self._get_ip_proto_name_and_num(",
            "                               value, ethertype=rule.get('ethertype')))",
            "            return str(value)",
            "",
            "        comparison_keys = [",
            "            'direction',",
            "            'ethertype',",
            "            'port_range_max',",
            "            'port_range_min',",
            "            'protocol',",
            "            'remote_group_id',",
            "            'remote_ip_prefix',",
            "            'security_group_id'",
            "        ]",
            "        return '_'.join([_normalize_rule_value(x, rule.get(x))",
            "                         for x in comparison_keys])",
            "",
            "    def _check_for_duplicate_rules(self, context, security_group_id,",
            "                                   new_security_group_rules):",
            "        # First up, check for any duplicates in the new rules.",
            "        new_rules_set = set()",
            "        for i in new_security_group_rules:",
            "            rule_key = self._rule_to_key(i['security_group_rule'])",
            "            if rule_key in new_rules_set:",
            "                raise ext_sg.DuplicateSecurityGroupRuleInPost(rule=i)",
            "            new_rules_set.add(rule_key)",
            "",
            "        # Now, let's make sure none of the new rules conflict with",
            "        # existing rules; note that we do *not* store the db rules",
            "        # in the set, as we assume they were already checked,",
            "        # when added.",
            "        sg = self.get_security_group(context, security_group_id)",
            "        if sg:",
            "            for i in sg['security_group_rules']:",
            "                rule_key = self._rule_to_key(i)",
            "                if rule_key in new_rules_set:",
            "                    raise ext_sg.SecurityGroupRuleExists(rule_id=i.get('id'))",
            "",
            "    def _validate_ip_prefix(self, rule):",
            "        \"\"\"Check that a valid cidr was specified as remote_ip_prefix",
            "",
            "        No need to check that it is in fact an IP address as this is already",
            "        validated by attribute validators.",
            "        Check that rule ethertype is consistent with remote_ip_prefix ip type.",
            "        Add mask to ip_prefix if absent (192.168.1.10 -> 192.168.1.10/32).",
            "        \"\"\"",
            "        input_prefix = rule['remote_ip_prefix']",
            "        if input_prefix:",
            "            addr = netaddr.IPNetwork(input_prefix)",
            "            # set input_prefix to always include the netmask:",
            "            rule['remote_ip_prefix'] = str(addr)",
            "            # check consistency of ethertype with addr version",
            "            if rule['ethertype'] != \"IPv%d\" % (addr.version):",
            "                raise ext_sg.SecurityGroupRuleParameterConflict(",
            "                    ethertype=rule['ethertype'], cidr=input_prefix)",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def get_security_group_rules(self, context, filters=None, fields=None,",
            "                                 sorts=None, limit=None, marker=None,",
            "                                 page_reverse=False):",
            "        filters = filters or {}",
            "        pager = base_obj.Pager(",
            "            sorts=sorts, marker=marker, limit=limit, page_reverse=page_reverse)",
            "",
            "        project_id = filters.get('project_id') or filters.get('tenant_id')",
            "        if project_id:",
            "            project_id = project_id[0]",
            "        else:",
            "            project_id = context.project_id",
            "        if project_id:",
            "            self._ensure_default_security_group(context, project_id)",
            "",
            "        if not filters and context.project_id and not context.is_admin:",
            "            rule_ids = sg_obj.SecurityGroupRule.get_security_group_rule_ids(",
            "                context.project_id)",
            "            filters = {'id': rule_ids}",
            "",
            "        # NOTE(slaweq): use admin context here to be able to get all rules",
            "        # which fits filters' criteria. Later in policy engine rules will be",
            "        # filtered and only those which are allowed according to policy will",
            "        # be returned",
            "        rule_objs = sg_obj.SecurityGroupRule.get_objects(",
            "            context_lib.get_admin_context(), _pager=pager,",
            "            validate_filters=False, **filters",
            "        )",
            "        return [",
            "            self._make_security_group_rule_dict(obj.db_obj, fields)",
            "            for obj in rule_objs",
            "        ]",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def get_security_group_rule(self, context, id, fields=None):",
            "        # NOTE(slaweq): use admin context here to be able to get all rules",
            "        # which fits filters' criteria. Later in policy engine rules will be",
            "        # filtered and only those which are allowed according to policy will",
            "        # be returned",
            "        security_group_rule = self._get_security_group_rule(",
            "            context_lib.get_admin_context(), id)",
            "        return self._make_security_group_rule_dict(",
            "            security_group_rule.db_obj, fields)",
            "",
            "    def _get_security_group_rule(self, context, id):",
            "        sgr = sg_obj.SecurityGroupRule.get_object(context, id=id)",
            "        if sgr is None:",
            "            raise ext_sg.SecurityGroupRuleNotFound(id=id)",
            "        return sgr",
            "",
            "    @db_api.retry_if_session_inactive()",
            "    def delete_security_group_rule(self, context, id):",
            "        kwargs = {",
            "            'context': context,",
            "            'security_group_rule_id': id",
            "        }",
            "        self._registry_notify(resources.SECURITY_GROUP_RULE,",
            "                              events.BEFORE_DELETE, id=id,",
            "                              exc_cls=ext_sg.SecurityGroupRuleInUse, **kwargs)",
            "",
            "        with db_api.CONTEXT_WRITER.using(context):",
            "            sgr = self._get_security_group_rule(context, id)",
            "            kwargs['security_group_id'] = sgr['security_group_id']",
            "            self._registry_notify(resources.SECURITY_GROUP_RULE,",
            "                                  events.PRECOMMIT_DELETE,",
            "                                  exc_cls=ext_sg.SecurityGroupRuleInUse, id=id,",
            "                                  **kwargs)",
            "            sgr.delete()",
            "",
            "        registry.notify(",
            "            resources.SECURITY_GROUP_RULE, events.AFTER_DELETE, self,",
            "            **kwargs)",
            "",
            "    @staticmethod",
            "    @resource_extend.extends([port_def.COLLECTION_NAME])",
            "    def _extend_port_dict_security_group(port_res, port_db):",
            "        # Security group bindings will be retrieved from the SQLAlchemy",
            "        # model. As they're loaded eagerly with ports because of the",
            "        # joined load they will not cause an extra query.",
            "        if isinstance(port_db, port_obj.Port):",
            "            port_res[ext_sg.SECURITYGROUPS] = port_db.security_group_ids",
            "        else:",
            "            security_group_ids = [sec_group_mapping['security_group_id'] for",
            "                                  sec_group_mapping in port_db.security_groups]",
            "            port_res[ext_sg.SECURITYGROUPS] = security_group_ids",
            "        return port_res",
            "",
            "    def _process_port_create_security_group(self, context, port,",
            "                                            security_groups):",
            "        self._validate_sgs_for_port(security_groups)",
            "        if validators.is_attr_set(security_groups):",
            "            for sg in security_groups:",
            "                self._create_port_security_group_binding(context, port['id'],",
            "                                                         sg.id)",
            "        # Convert to list as a set might be passed here and",
            "        # this has to be serialized",
            "        port[ext_sg.SECURITYGROUPS] = ([sg.id for sg in security_groups] if",
            "                                       security_groups else [])",
            "",
            "    def _get_default_sg_id(self, context, tenant_id):",
            "        default_group = sg_obj.DefaultSecurityGroup.get_object(",
            "            context,",
            "            project_id=tenant_id,",
            "        )",
            "        if default_group:",
            "            return default_group.security_group_id",
            "",
            "    @registry.receives(resources.PORT, [events.BEFORE_CREATE,",
            "                                        events.BEFORE_UPDATE])",
            "    @registry.receives(resources.NETWORK, [events.BEFORE_CREATE])",
            "    def _ensure_default_security_group_handler(self, resource, event, trigger,",
            "                                               context, **kwargs):",
            "        if event == events.BEFORE_UPDATE:",
            "            tenant_id = kwargs['original_' + resource]['tenant_id']",
            "        else:",
            "            tenant_id = kwargs[resource]['tenant_id']",
            "        if tenant_id:",
            "            self._ensure_default_security_group(context, tenant_id)",
            "",
            "    def _ensure_default_security_group(self, context, tenant_id):",
            "        \"\"\"Create a default security group if one doesn't exist.",
            "",
            "        :returns: the default security group id for given tenant.",
            "        \"\"\"",
            "        # Do not allow a tenant to create a default SG for another one.",
            "        # See Bug 1987410.",
            "        if tenant_id != context.tenant_id and not context.is_admin:",
            "            return",
            "        if not extensions.is_extension_supported(self, 'security-group'):",
            "            return",
            "        default_group_id = self._get_default_sg_id(context, tenant_id)",
            "        if default_group_id:",
            "            return default_group_id",
            "",
            "        security_group = {",
            "            'security_group':",
            "                {'name': 'default',",
            "                 'tenant_id': tenant_id,",
            "                 'description': DEFAULT_SG_DESCRIPTION}",
            "        }",
            "        try:",
            "            return self.create_security_group(context, security_group,",
            "                                              default_sg=True)['id']",
            "        except obj_exc.NeutronDbObjectDuplicateEntry:",
            "            return self._get_default_sg_id(context, tenant_id)",
            "",
            "    def _get_security_groups_on_port(self, context, port):",
            "        \"\"\"Check that all security groups on port belong to tenant.",
            "",
            "        :returns: all security groups on port belonging to tenant)",
            "",
            "        \"\"\"",
            "        port = port['port']",
            "        if not validators.is_attr_set(port.get(ext_sg.SECURITYGROUPS)):",
            "            return",
            "        if port.get('device_owner') and net.is_port_trusted(port):",
            "            return",
            "",
            "        port_sg = port.get(ext_sg.SECURITYGROUPS, [])",
            "        tenant_id = port.get('tenant_id')",
            "",
            "        sg_objs = sg_obj.SecurityGroup.get_objects(context, id=port_sg)",
            "",
            "        valid_groups = set(",
            "            g.id for g in sg_objs",
            "            if (context.is_admin or not tenant_id or",
            "                g.tenant_id == tenant_id or",
            "                sg_obj.SecurityGroup.is_shared_with_tenant(",
            "                    context, g.id, tenant_id))",
            "        )",
            "",
            "        requested_groups = set(port_sg)",
            "        port_sg_missing = requested_groups - valid_groups",
            "        if port_sg_missing:",
            "            raise ext_sg.SecurityGroupNotFound(id=', '.join(port_sg_missing))",
            "",
            "        return sg_objs",
            "",
            "    def _ensure_default_security_group_on_port(self, context, port):",
            "        # we don't apply security groups for dhcp, router",
            "        port = port['port']",
            "        if port.get('device_owner') and net.is_port_trusted(port):",
            "            return",
            "        port_sg = port.get(ext_sg.SECURITYGROUPS)",
            "        if port_sg is None or not validators.is_attr_set(port_sg):",
            "            port_project = port.get('tenant_id')",
            "            default_sg = self._ensure_default_security_group(context,",
            "                                                             port_project)",
            "            if default_sg:",
            "                port[ext_sg.SECURITYGROUPS] = [default_sg]",
            "",
            "    def _check_update_deletes_security_groups(self, port):",
            "        \"\"\"Return True if port has as a security group and it's value",
            "        is either [] or not is_attr_set, otherwise return False",
            "        \"\"\"",
            "        if (ext_sg.SECURITYGROUPS in port['port'] and",
            "            not (validators.is_attr_set(",
            "                     port['port'][ext_sg.SECURITYGROUPS]) and",
            "                 port['port'][ext_sg.SECURITYGROUPS] != [])):",
            "            return True",
            "        return False",
            "",
            "    def _check_update_has_security_groups(self, port):",
            "        \"\"\"Return True if port has security_groups attribute set and",
            "        its not empty, or False otherwise.",
            "        This method is called both for port create and port update.",
            "        \"\"\"",
            "        if (ext_sg.SECURITYGROUPS in port['port'] and",
            "            (validators.is_attr_set(port['port'][ext_sg.SECURITYGROUPS]) and",
            "             port['port'][ext_sg.SECURITYGROUPS] != [])):",
            "            return True",
            "        return False",
            "",
            "    def update_security_group_on_port(self, context, id, port,",
            "                                      original_port, updated_port):",
            "        \"\"\"Update security groups on port.",
            "",
            "        This method returns a flag which indicates request notification",
            "        is required and does not perform notification itself.",
            "        It is because another changes for the port may require notification.",
            "        \"\"\"",
            "        need_notify = False",
            "        port_updates = port['port']",
            "        if (ext_sg.SECURITYGROUPS in port_updates and",
            "            not helpers.compare_elements(",
            "                original_port.get(ext_sg.SECURITYGROUPS),",
            "                port_updates[ext_sg.SECURITYGROUPS])):",
            "            # delete the port binding and read it with the new rules",
            "            sgs = self._get_security_groups_on_port(context, port)",
            "            port_updates[ext_sg.SECURITYGROUPS] = [sg.id for sg in sgs]",
            "            self._delete_port_security_group_bindings(context, id)",
            "            self._process_port_create_security_group(",
            "                context,",
            "                updated_port,",
            "                sgs)",
            "            need_notify = True",
            "        else:",
            "            updated_port[ext_sg.SECURITYGROUPS] = (",
            "                original_port[ext_sg.SECURITYGROUPS])",
            "        return need_notify"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "neutron.db.securitygroups_db.SecurityGroupDbMixin.get_security_group_rules",
            "neutron.db.securitygroups_db.SecurityGroupDbMixin.create_security_group",
            "neutron.db.securitygroups_db.SecurityGroupDbMixin._ensure_default_security_group.security_group",
            "neutron.db.securitygroups_db.SecurityGroupDbMixin.create_security_group.security_group",
            "neutron.db.securitygroups_db.SecurityGroupDbMixin.get_security_groups",
            "neutron.db.securitygroups_db.SecurityGroupDbMixin._ensure_default_security_group_handler",
            "neutron.db.securitygroups_db.SecurityGroupDbMixin._ensure_default_security_group_on_port",
            "nova.utils.ExceptionHelper"
        ]
    },
    "neutron/tests/unit/db/test_securitygroups_db.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 615,
                "afterPatchRowNumber": 615,
                "PatchRowcode": "             self.mixin._ensure_default_security_group(self.ctx, 'tenant_1')"
            },
            "1": {
                "beforePatchRowNumber": 616,
                "afterPatchRowNumber": 616,
                "PatchRowcode": "             create_sg.assert_not_called()"
            },
            "2": {
                "beforePatchRowNumber": 617,
                "afterPatchRowNumber": 617,
                "PatchRowcode": "             get_default_sg_id.assert_not_called()"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 618,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 619,
                "PatchRowcode": "+    def test__ensure_default_security_group_tenant_mismatch(self):"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 620,
                "PatchRowcode": "+        with mock.patch.object("
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 621,
                "PatchRowcode": "+                self.mixin, '_get_default_sg_id') as get_default_sg_id,\\"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 622,
                "PatchRowcode": "+                mock.patch.object("
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 623,
                "PatchRowcode": "+                        self.mixin, 'create_security_group') as create_sg:"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 624,
                "PatchRowcode": "+            context = mock.Mock()"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 625,
                "PatchRowcode": "+            context.tenant_id = 'tenant_0'"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 626,
                "PatchRowcode": "+            context.is_admin = False"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 627,
                "PatchRowcode": "+            self.mixin._ensure_default_security_group(context, 'tenant_1')"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 628,
                "PatchRowcode": "+            create_sg.assert_not_called()"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 629,
                "PatchRowcode": "+            get_default_sg_id.assert_not_called()"
            }
        },
        "frontPatchFile": [
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#    http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or",
            "# implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import copy",
            "from unittest import mock",
            "",
            "from neutron_lib.callbacks import events",
            "from neutron_lib.callbacks import exceptions",
            "from neutron_lib.callbacks import registry",
            "from neutron_lib.callbacks import resources",
            "from neutron_lib import constants",
            "from neutron_lib import context",
            "from neutron_lib.objects import exceptions as obj_exc",
            "import sqlalchemy",
            "import testtools",
            "",
            "from neutron.db import securitygroups_db",
            "from neutron.extensions import securitygroup",
            "from neutron import quota",
            "from neutron.services.revisions import revision_plugin",
            "from neutron.tests.unit import testlib_api",
            "",
            "",
            "FAKE_SECGROUP = {",
            "    'security_group': {",
            "        \"tenant_id\": 'fake',",
            "        'description': 'fake',",
            "        'name': 'fake'",
            "    }",
            "}",
            "",
            "FAKE_SECGROUP_RULE = {",
            "    'security_group_rule': {",
            "        \"tenant_id\": 'fake',",
            "        'description': 'fake',",
            "        'name': 'fake',",
            "        'port_range_min': '21',",
            "        'protocol': 'tcp',",
            "        'port_range_max': '23',",
            "        'remote_ip_prefix': '10.0.0.1',",
            "        'ethertype': 'IPv4',",
            "        'remote_group_id': None,",
            "        'security_group_id': 'None',",
            "        'direction': 'ingress'",
            "    }",
            "}",
            "",
            "DB_PLUGIN_KLASS = 'neutron.db.db_base_plugin_v2.NeutronDbPluginV2'",
            "",
            "",
            "def fake_callback(resource, event, *args, **kwargs):",
            "    raise KeyError('bar')",
            "",
            "",
            "class SecurityGroupDbMixinImpl(securitygroups_db.SecurityGroupDbMixin):",
            "    pass",
            "",
            "",
            "class SecurityGroupDbMixinTestCase(testlib_api.SqlTestCase):",
            "",
            "    def setUp(self):",
            "        super(SecurityGroupDbMixinTestCase, self).setUp()",
            "        self.setup_coreplugin(core_plugin=DB_PLUGIN_KLASS)",
            "        self.ctx = context.get_admin_context()",
            "        self.mixin = SecurityGroupDbMixinImpl()",
            "        make_res = mock.patch.object(quota.QuotaEngine, 'make_reservation')",
            "        self.mock_quota_make_res = make_res.start()",
            "        commit_res = mock.patch.object(quota.QuotaEngine, 'commit_reservation')",
            "        self.mock_quota_commit_res = commit_res.start()",
            "        is_ext_supported = mock.patch(",
            "            'neutron_lib.api.extensions.is_extension_supported')",
            "        self.is_ext_supported = is_ext_supported.start()",
            "        self.is_ext_supported.return_value = True",
            "",
            "    def test_create_security_group_conflict(self):",
            "        with mock.patch.object(registry, \"publish\") as mock_publish:",
            "            mock_publish.side_effect = exceptions.CallbackFailure(Exception())",
            "            secgroup = {'security_group': mock.ANY}",
            "            with testtools.ExpectedException(",
            "                    securitygroup.SecurityGroupConflict):",
            "                self.mixin.create_security_group(self.ctx, secgroup)",
            "",
            "    def test_delete_security_group_in_use(self):",
            "        with mock.patch.object(self.mixin,",
            "                               '_get_port_security_group_bindings'),\\",
            "                mock.patch.object(self.mixin, '_get_security_group'),\\",
            "                mock.patch.object(registry, \"notify\") as mock_notify:",
            "            mock_notify.side_effect = exceptions.CallbackFailure(Exception())",
            "            with testtools.ExpectedException(securitygroup.SecurityGroupInUse):",
            "                self.mixin.delete_security_group(self.ctx, mock.ANY)",
            "",
            "    def test_update_security_group_statefulness_binded_conflict(self):",
            "        FAKE_SECGROUP['security_group']['stateful'] = mock.ANY",
            "        sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "        FAKE_SECGROUP['security_group']['stateful'] = not sg_dict['stateful']",
            "        with mock.patch.object(self.mixin,",
            "                               '_get_port_security_group_bindings'), \\",
            "                mock.patch.object(registry, \"notify\") as mock_notify:",
            "            mock_notify.side_effect = exceptions.CallbackFailure(Exception())",
            "            with testtools.ExpectedException(securitygroup.SecurityGroupInUse):",
            "                self.mixin.update_security_group(self.ctx, sg_dict['id'],",
            "                                                 FAKE_SECGROUP)",
            "",
            "    def test_update_security_group_conflict(self):",
            "        with mock.patch.object(registry, \"notify\") as mock_notify:",
            "            mock_notify.side_effect = exceptions.CallbackFailure(Exception())",
            "            secgroup = {'security_group': FAKE_SECGROUP}",
            "            with testtools.ExpectedException(",
            "                    securitygroup.SecurityGroupConflict):",
            "                self.mixin.update_security_group(self.ctx, 'foo_id', secgroup)",
            "",
            "    def test_create_security_group_rule_conflict(self):",
            "        with mock.patch.object(self.mixin, '_validate_security_group_rule'),\\",
            "                mock.patch.object(self.mixin,",
            "                                  '_check_for_duplicate_rules'),\\",
            "                mock.patch.object(registry, \"notify\") as mock_notify:",
            "            mock_notify.side_effect = exceptions.CallbackFailure(Exception())",
            "            with testtools.ExpectedException(",
            "                    securitygroup.SecurityGroupConflict):",
            "                self.mixin.create_security_group_rule(",
            "                    self.ctx, mock.MagicMock())",
            "",
            "    def test__check_for_duplicate_rules_does_not_drop_protocol(self):",
            "        with mock.patch.object(self.mixin, 'get_security_group',",
            "                               return_value=None):",
            "            context = mock.Mock()",
            "            rule_dict = {",
            "                'security_group_rule': {'protocol': None,",
            "                                        'tenant_id': 'fake',",
            "                                        'security_group_id': 'fake',",
            "                                        'direction': 'fake'}",
            "            }",
            "            self.mixin._check_for_duplicate_rules(context, 'fake', [rule_dict])",
            "        self.assertIn('protocol', rule_dict['security_group_rule'])",
            "",
            "    def test__check_for_duplicate_rules_ignores_rule_id(self):",
            "        rules = [{'security_group_rule': {'protocol': 'tcp', 'id': 'fake1'}},",
            "                 {'security_group_rule': {'protocol': 'tcp', 'id': 'fake2'}}]",
            "",
            "        # NOTE(arosen): the name of this exception is a little misleading",
            "        # in this case as this test, tests that the id fields are dropped",
            "        # while being compared. This is in the case if a plugin specifies",
            "        # the rule ids themselves.",
            "        with mock.patch.object(self.mixin, 'get_security_group',",
            "                               return_value=None):",
            "            self.assertRaises(securitygroup.DuplicateSecurityGroupRuleInPost,",
            "                              self.mixin._check_for_duplicate_rules,",
            "                              context, 'fake', rules)",
            "",
            "    def test_check_for_duplicate_diff_rules_remote_ip_prefix_ipv4(self):",
            "        fake_secgroup = copy.deepcopy(FAKE_SECGROUP)",
            "        fake_secgroup['security_group_rules'] = \\",
            "            [{'id': 'fake', 'tenant_id': 'fake', 'ethertype': 'IPv4',",
            "              'direction': 'ingress', 'security_group_id': 'fake',",
            "              'remote_ip_prefix': None}]",
            "        with mock.patch.object(self.mixin, 'get_security_group',",
            "                               return_value=fake_secgroup):",
            "            context = mock.Mock()",
            "            rule_dict = {",
            "                'security_group_rule': {'id': 'fake2',",
            "                                        'tenant_id': 'fake',",
            "                                        'security_group_id': 'fake',",
            "                                        'ethertype': 'IPv4',",
            "                                        'direction': 'ingress',",
            "                                        'remote_ip_prefix': '0.0.0.0/0'}",
            "            }",
            "            self.assertRaises(securitygroup.SecurityGroupRuleExists,",
            "                self.mixin._check_for_duplicate_rules,",
            "                context, 'fake', [rule_dict])",
            "",
            "    def test_check_for_duplicate_diff_rules_remote_ip_prefix_ipv6(self):",
            "        fake_secgroup = copy.deepcopy(FAKE_SECGROUP)",
            "        fake_secgroup['security_group_rules'] = \\",
            "            [{'id': 'fake', 'tenant_id': 'fake', 'ethertype': 'IPv6',",
            "              'direction': 'ingress', 'security_group_id': 'fake',",
            "              'remote_ip_prefix': None}]",
            "        with mock.patch.object(self.mixin, 'get_security_group',",
            "                               return_value=fake_secgroup):",
            "            context = mock.Mock()",
            "            rule_dict = {",
            "                'security_group_rule': {'id': 'fake2',",
            "                                        'tenant_id': 'fake',",
            "                                        'security_group_id': 'fake',",
            "                                        'ethertype': 'IPv6',",
            "                                        'direction': 'ingress',",
            "                                        'remote_ip_prefix': '::/0'}",
            "            }",
            "            self.assertRaises(securitygroup.SecurityGroupRuleExists,",
            "                self.mixin._check_for_duplicate_rules,",
            "                context, 'fake', [rule_dict])",
            "",
            "    def test_delete_security_group_rule_in_use(self):",
            "        with mock.patch.object(registry, \"notify\") as mock_notify:",
            "            mock_notify.side_effect = exceptions.CallbackFailure(Exception())",
            "            with testtools.ExpectedException(",
            "                    securitygroup.SecurityGroupRuleInUse):",
            "                self.mixin.delete_security_group_rule(self.ctx, mock.ANY)",
            "",
            "    def test_delete_security_group_rule_raise_error_on_not_found(self):",
            "        with testtools.ExpectedException(",
            "                securitygroup.SecurityGroupRuleNotFound):",
            "            self.mixin.delete_security_group_rule(self.ctx, 'foo_rule')",
            "",
            "    def test_validate_ethertype_and_protocol(self):",
            "        fake_ipv4_rules = [{'protocol': constants.PROTO_NAME_IPV6_ICMP,",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': constants.PROTO_NAME_IPV6_ICMP_LEGACY,",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': constants.PROTO_NAME_IPV6_ENCAP,",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': constants.PROTO_NAME_IPV6_ROUTE,",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': constants.PROTO_NAME_IPV6_FRAG,",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': constants.PROTO_NAME_IPV6_NONXT,",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': constants.PROTO_NAME_IPV6_OPTS,",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': str(constants.PROTO_NUM_IPV6_ICMP),",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': str(constants.PROTO_NUM_IPV6_ENCAP),",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': str(constants.PROTO_NUM_IPV6_ROUTE),",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': str(constants.PROTO_NUM_IPV6_FRAG),",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': str(constants.PROTO_NUM_IPV6_NONXT),",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': str(constants.PROTO_NUM_IPV6_OPTS),",
            "                            'ethertype': constants.IPv4}]",
            "        # test wrong protocols",
            "        for rule in fake_ipv4_rules:",
            "            with testtools.ExpectedException(",
            "                    securitygroup.SecurityGroupEthertypeConflictWithProtocol):",
            "                self.mixin._validate_ethertype_and_protocol(rule)",
            "",
            "    def test_security_group_precommit_create_event_fail(self):",
            "        registry.subscribe(fake_callback, resources.SECURITY_GROUP,",
            "                           events.PRECOMMIT_CREATE)",
            "        with mock.patch.object(sqlalchemy.orm.session.SessionTransaction,",
            "                               'rollback') as mock_rollback:",
            "            self.assertRaises(securitygroup.SecurityGroupConflict,",
            "                              self.mixin.create_security_group,",
            "                              self.ctx, FAKE_SECGROUP)",
            "            self.assertTrue(mock_rollback.called)",
            "",
            "    def test_security_group_precommit_update_event_fail(self):",
            "        registry.subscribe(fake_callback, resources.SECURITY_GROUP,",
            "                           events.PRECOMMIT_UPDATE)",
            "        sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "        with mock.patch.object(sqlalchemy.orm.session.SessionTransaction,",
            "                              'rollback') as mock_rollback:",
            "            self.assertRaises(securitygroup.SecurityGroupConflict,",
            "                              self.mixin.update_security_group,",
            "                              self.ctx, sg_dict['id'], FAKE_SECGROUP)",
            "            self.assertTrue(mock_rollback.called)",
            "",
            "    def test_security_group_precommit_delete_event_fail(self):",
            "        registry.subscribe(fake_callback, resources.SECURITY_GROUP,",
            "                           events.PRECOMMIT_DELETE)",
            "        sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "        with mock.patch.object(sqlalchemy.orm.session.SessionTransaction,",
            "                              'rollback') as mock_rollback:",
            "            self.assertRaises(securitygroup.SecurityGroupInUse,",
            "                              self.mixin.delete_security_group,",
            "                              self.ctx, sg_dict['id'])",
            "            self.assertTrue(mock_rollback.called)",
            "",
            "    def _test_security_group_precommit_create_event(self,",
            "                                                    with_revisions=False):",
            "        DEFAULT_SECGROUP = {",
            "            'tenant_id': FAKE_SECGROUP['security_group']['tenant_id'],",
            "            'name': 'default',",
            "            'description': 'Default security group',",
            "        }",
            "        DEFAULT_SECGROUP_DICT = {",
            "            'id': mock.ANY,",
            "            'tenant_id': FAKE_SECGROUP['security_group']['tenant_id'],",
            "            'project_id': FAKE_SECGROUP['security_group']['tenant_id'],",
            "            'name': 'default',",
            "            'description': 'Default security group',",
            "            'stateful': mock.ANY,",
            "            'security_group_rules': [",
            "                # Four rules for egress/ingress and ipv4/ipv6",
            "                mock.ANY, mock.ANY, mock.ANY, mock.ANY,",
            "            ],",
            "        }",
            "        if with_revisions:",
            "            DEFAULT_SECGROUP_DICT.update({",
            "                'revision_number': mock.ANY,",
            "            })",
            "        with mock.patch.object(registry, 'publish') as publish, \\",
            "                mock.patch.object(registry, \"notify\") as mock_notify:",
            "            sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "            mock_notify.assert_has_calls([",
            "                mock.call('security_group', 'precommit_create', mock.ANY,",
            "                          context=mock.ANY, is_default=True,",
            "                          security_group=DEFAULT_SECGROUP_DICT),",
            "                mock.call('security_group', 'after_create', mock.ANY,",
            "                          context=mock.ANY, is_default=True,",
            "                          security_group=DEFAULT_SECGROUP_DICT),",
            "                mock.call('security_group', 'precommit_create', mock.ANY,",
            "                          context=mock.ANY, is_default=False,",
            "                          security_group=sg_dict),",
            "                mock.call('security_group', 'after_create', mock.ANY,",
            "                          context=mock.ANY, is_default=False,",
            "                          security_group=sg_dict)])",
            "",
            "            publish.assert_has_calls([",
            "                mock.call('security_group', 'before_create', mock.ANY,",
            "                          payload=mock.ANY),",
            "                mock.call('security_group', 'before_create', mock.ANY,",
            "                          payload=mock.ANY)])",
            "            payload = publish.mock_calls[0][2]['payload']",
            "            self.assertDictEqual(payload.desired_state,",
            "                                 FAKE_SECGROUP['security_group'])",
            "            payload = publish.mock_calls[1][2]['payload']",
            "            self.assertDictEqual(payload.desired_state, DEFAULT_SECGROUP)",
            "",
            "            # Ensure that the result of create is same as get.",
            "            # Especially we want to check the revision number here.",
            "            sg_dict_got = self.mixin.get_security_group(",
            "                self.ctx, sg_dict['id'])",
            "            self.assertEqual(sg_dict, sg_dict_got)",
            "",
            "    def test_security_group_precommit_create_event_with_revisions(self):",
            "        revision = revision_plugin.RevisionPlugin()",
            "        self._test_security_group_precommit_create_event(True)",
            "        del revision  # appease pep8",
            "",
            "    def test_security_group_precommit_create_event(self):",
            "        self._test_security_group_precommit_create_event()",
            "",
            "    def test_security_group_precommit_update_event(self):",
            "        FAKE_SECGROUP['security_group']['stateful'] = mock.ANY",
            "        original_sg_dict = self.mixin.create_security_group(self.ctx,",
            "                                                            FAKE_SECGROUP)",
            "        sg_id = original_sg_dict['id']",
            "        with mock.patch.object(self.mixin,",
            "                               '_get_port_security_group_bindings'), \\",
            "                mock.patch.object(registry, \"publish\") as mock_notify:",
            "            fake_secgroup = copy.deepcopy(FAKE_SECGROUP)",
            "            fake_secgroup['security_group']['name'] = 'updated_fake'",
            "            fake_secgroup['security_group']['stateful'] = mock.ANY",
            "            sg_dict = self.mixin.update_security_group(",
            "                    self.ctx, sg_id, fake_secgroup)",
            "",
            "            mock_notify.assert_has_calls(",
            "                [mock.call('security_group', 'precommit_update', mock.ANY,",
            "                           payload=mock.ANY)])",
            "            payload = mock_notify.call_args[1]['payload']",
            "            self.assertEqual(original_sg_dict, payload.states[0])",
            "            self.assertEqual(sg_id, payload.resource_id)",
            "            self.assertEqual(sg_dict, payload.desired_state)",
            "",
            "    def test_security_group_precommit_and_after_delete_event(self):",
            "        sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "        with mock.patch.object(registry, \"notify\") as mock_notify:",
            "            self.mixin.delete_security_group(self.ctx, sg_dict['id'])",
            "            sg_dict['security_group_rules'] = mock.ANY",
            "            mock_notify.assert_has_calls(",
            "                [mock.call('security_group', 'precommit_delete',",
            "                           mock.ANY, context=mock.ANY, security_group=sg_dict,",
            "                           security_group_id=sg_dict['id'],",
            "                           security_group_rule_ids=[mock.ANY, mock.ANY]),",
            "                 mock.call('security_group', 'after_delete',",
            "                           mock.ANY, context=mock.ANY,",
            "                           security_group_id=sg_dict['id'],",
            "                           security_group_rule_ids=[mock.ANY, mock.ANY],",
            "                           name=sg_dict['name'])])",
            "",
            "    def test_security_group_rule_precommit_create_event_fail(self):",
            "        registry.subscribe(fake_callback, resources.SECURITY_GROUP_RULE,",
            "                           events.PRECOMMIT_CREATE)",
            "        sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "        fake_rule = FAKE_SECGROUP_RULE",
            "        fake_rule['security_group_rule']['security_group_id'] = sg_dict['id']",
            "        with mock.patch.object(sqlalchemy.orm.session.SessionTransaction,",
            "                               'rollback') as mock_rollback,\\",
            "                mock.patch.object(self.mixin, '_get_security_group'):",
            "            self.assertRaises(securitygroup.SecurityGroupConflict,",
            "                              self.mixin.create_security_group_rule,",
            "                              self.ctx, fake_rule)",
            "            self.assertTrue(mock_rollback.called)",
            "",
            "    def test_security_group_rule_precommit_delete_event_fail(self):",
            "        registry.subscribe(fake_callback, resources.SECURITY_GROUP_RULE,",
            "                           events.PRECOMMIT_DELETE)",
            "        sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "        fake_rule = FAKE_SECGROUP_RULE",
            "        fake_rule['security_group_rule']['security_group_id'] = sg_dict['id']",
            "        with mock.patch.object(sqlalchemy.orm.session.SessionTransaction,",
            "                               'rollback') as mock_rollback,\\",
            "                mock.patch.object(self.mixin, '_get_security_group'):",
            "            sg_rule_dict = self.mixin.create_security_group_rule(self.ctx,",
            "                   fake_rule)",
            "            self.assertRaises(securitygroup.SecurityGroupRuleInUse,",
            "                              self.mixin.delete_security_group_rule, self.ctx,",
            "                              sg_rule_dict['id'])",
            "            self.assertTrue(mock_rollback.called)",
            "",
            "    def test_security_group_rule_precommit_create_event(self):",
            "        sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "        fake_rule = FAKE_SECGROUP_RULE",
            "        fake_rule['security_group_rule']['security_group_id'] = sg_dict['id']",
            "        with mock.patch.object(registry, \"notify\") as mock_notify, \\",
            "                mock.patch.object(self.mixin, '_get_security_group'):",
            "            mock_notify.assert_has_calls([mock.call('security_group_rule',",
            "                'precommit_create', mock.ANY, context=mock.ANY,",
            "                security_group_rule=self.mixin.create_security_group_rule(",
            "                    self.ctx, fake_rule))])",
            "",
            "    def test_sg_rule_before_precommit_and_after_delete_event(self):",
            "        sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "        fake_rule = FAKE_SECGROUP_RULE",
            "        fake_rule['security_group_rule']['security_group_id'] = sg_dict['id']",
            "        with mock.patch.object(registry, \"notify\") as mock_notify, \\",
            "                mock.patch.object(self.mixin, '_get_security_group'):",
            "            sg_rule_dict = self.mixin.create_security_group_rule(self.ctx,",
            "                   fake_rule)",
            "            self.mixin.delete_security_group_rule(self.ctx,",
            "                    sg_rule_dict['id'])",
            "            mock_notify.assert_has_calls([mock.call('security_group_rule',",
            "                'before_delete', mock.ANY, context=mock.ANY,",
            "                security_group_rule_id=sg_rule_dict['id'])])",
            "            mock_notify.assert_has_calls([mock.call('security_group_rule',",
            "                'precommit_delete', mock.ANY, context=mock.ANY,",
            "                security_group_id=sg_dict['id'],",
            "                security_group_rule_id=sg_rule_dict['id'])])",
            "            mock_notify.assert_has_calls([mock.call('security_group_rule',",
            "                'after_delete', mock.ANY, context=mock.ANY,",
            "                security_group_rule_id=sg_rule_dict['id'],",
            "                security_group_id=sg_dict['id'])])",
            "",
            "    def test_get_ip_proto_name_and_num(self):",
            "        protocols = [constants.PROTO_NAME_UDP, str(constants.PROTO_NUM_TCP),",
            "                     'blah', '111']",
            "        protocol_names_nums = (",
            "            [[constants.PROTO_NAME_UDP, str(constants.PROTO_NUM_UDP)],",
            "             [constants.PROTO_NAME_TCP, str(constants.PROTO_NUM_TCP)],",
            "             ['blah', 'blah'], ['111', '111']])",
            "",
            "        for i, protocol in enumerate(protocols):",
            "            self.assertEqual(protocol_names_nums[i],",
            "                             self.mixin._get_ip_proto_name_and_num(protocol))",
            "",
            "    def test__validate_port_range_for_icmp_exception(self):",
            "        states = [(1, 256, securitygroup.SecurityGroupInvalidIcmpValue),",
            "                  (None, 6, securitygroup.SecurityGroupMissingIcmpType),",
            "                  (300, 1, securitygroup.SecurityGroupInvalidIcmpValue)]",
            "        for protocol in (constants.PROTO_NAME_ICMP,",
            "                         constants.PROTO_NAME_IPV6_ICMP,",
            "                         constants.PROTO_NAME_IPV6_ICMP_LEGACY):",
            "            for pmin, pmax, exception in states:",
            "                self.assertRaises(exception,",
            "                    self.mixin._validate_port_range,",
            "                    {'port_range_min': pmin,",
            "                     'port_range_max': pmax,",
            "                     'protocol': protocol})",
            "",
            "    def test__validate_port_range_exception(self):",
            "        self.assertRaises(securitygroup.SecurityGroupInvalidPortValue,",
            "                          self.mixin._validate_port_range,",
            "                          {'port_range_min': 0,",
            "                           'port_range_max': None,",
            "                           'protocol': constants.PROTO_NAME_TCP})",
            "        self.assertRaises(securitygroup.SecurityGroupInvalidPortRange,",
            "                          self.mixin._validate_port_range,",
            "                          {'port_range_min': 1,",
            "                           'port_range_max': None,",
            "                           'protocol': constants.PROTO_NAME_SCTP})",
            "        self.assertRaises(securitygroup.SecurityGroupInvalidPortRange,",
            "                          self.mixin._validate_port_range,",
            "                          {'port_range_min': 1000,",
            "                           'port_range_max': 1,",
            "                           'protocol': constants.PROTO_NAME_UDPLITE})",
            "        self.assertRaises(",
            "            securitygroup.SecurityGroupInvalidProtocolForPort,",
            "            self.mixin._validate_port_range,",
            "            {'port_range_min': 100,",
            "             'port_range_max': 200,",
            "             'protocol': '111'})",
            "        self.assertRaises(",
            "            securitygroup.SecurityGroupInvalidProtocolForPort,",
            "            self.mixin._validate_port_range,",
            "            {'port_range_min': 100,",
            "             'port_range_max': None,",
            "             'protocol': constants.PROTO_NAME_VRRP})",
            "        self.assertRaises(",
            "            securitygroup.SecurityGroupInvalidProtocolForPort,",
            "            self.mixin._validate_port_range,",
            "            {'port_range_min': None,",
            "             'port_range_max': 200,",
            "             'protocol': constants.PROTO_NAME_VRRP})",
            "",
            "    def _create_environment(self):",
            "        self.sg = copy.deepcopy(FAKE_SECGROUP)",
            "        self.user_ctx = context.Context(user_id='user1', tenant_id='tenant_1',",
            "                                        is_admin=False, overwrite=False)",
            "        self.admin_ctx = context.Context(user_id='user2', tenant_id='tenant_2',",
            "                                         is_admin=True, overwrite=False)",
            "        self.sg_user = self.mixin.create_security_group(",
            "            self.user_ctx, {'security_group': {'name': 'name',",
            "                                               'tenant_id': 'tenant_1',",
            "                                               'description': 'fake'}})",
            "",
            "    def test_get_security_group_rules(self):",
            "        self._create_environment()",
            "        rules_before = self.mixin.get_security_group_rules(self.user_ctx)",
            "",
            "        rule = copy.deepcopy(FAKE_SECGROUP_RULE)",
            "        rule['security_group_rule']['security_group_id'] = self.sg_user['id']",
            "        rule['security_group_rule']['tenant_id'] = 'tenant_2'",
            "        self.mixin.create_security_group_rule(self.admin_ctx, rule)",
            "",
            "        rules_after = self.mixin.get_security_group_rules(self.user_ctx)",
            "        self.assertEqual(len(rules_before) + 1, len(rules_after))",
            "        for rule in (rule for rule in rules_after if rule not in rules_before):",
            "            self.assertEqual('tenant_2', rule['tenant_id'])",
            "",
            "    def test_get_security_group_rules_filters_passed(self):",
            "        self._create_environment()",
            "        filters = {'security_group_id': self.sg_user['id']}",
            "        rules_before = self.mixin.get_security_group_rules(self.user_ctx,",
            "                                                           filters=filters)",
            "",
            "        default_sg = self.mixin.get_security_groups(",
            "            self.user_ctx, filters={'name': 'default'})[0]",
            "        rule = copy.deepcopy(FAKE_SECGROUP_RULE)",
            "        rule['security_group_rule']['security_group_id'] = default_sg['id']",
            "        rule['security_group_rule']['tenant_id'] = 'tenant_1'",
            "        self.mixin.create_security_group_rule(self.user_ctx, rule)",
            "",
            "        rules_after = self.mixin.get_security_group_rules(self.user_ctx,",
            "                                                          filters=filters)",
            "        self.assertEqual(rules_before, rules_after)",
            "",
            "    def test_get_security_group_rules_admin_context(self):",
            "        self._create_environment()",
            "        rules_before = self.mixin.get_security_group_rules(self.ctx)",
            "",
            "        rule = copy.deepcopy(FAKE_SECGROUP_RULE)",
            "        rule['security_group_rule']['security_group_id'] = self.sg_user['id']",
            "        rule['security_group_rule']['tenant_id'] = 'tenant_1'",
            "        self.mixin.create_security_group_rule(self.user_ctx, rule)",
            "",
            "        rules_after = self.mixin.get_security_group_rules(self.ctx)",
            "        self.assertEqual(len(rules_before) + 1, len(rules_after))",
            "        for rule in (rule for rule in rules_after if rule not in rules_before):",
            "            self.assertEqual('tenant_1', rule['tenant_id'])",
            "            self.assertEqual(self.sg_user['id'], rule['security_group_id'])",
            "",
            "    def test__ensure_default_security_group(self):",
            "        with mock.patch.object(",
            "                self.mixin, '_get_default_sg_id') as get_default_sg_id,\\",
            "                mock.patch.object(",
            "                        self.mixin, 'create_security_group') as create_sg:",
            "            get_default_sg_id.return_value = None",
            "            self.mixin._ensure_default_security_group(self.ctx, 'tenant_1')",
            "            create_sg.assert_called_once_with(",
            "                self.ctx,",
            "                {'security_group': {",
            "                    'name': 'default',",
            "                    'tenant_id': 'tenant_1',",
            "                    'description': securitygroups_db.DEFAULT_SG_DESCRIPTION}},",
            "                default_sg=True)",
            "            get_default_sg_id.assert_called_once_with(self.ctx, 'tenant_1')",
            "",
            "    def test__ensure_default_security_group_already_exists(self):",
            "        with mock.patch.object(",
            "                self.mixin, '_get_default_sg_id') as get_default_sg_id,\\",
            "                mock.patch.object(",
            "                        self.mixin, 'create_security_group') as create_sg:",
            "            get_default_sg_id.return_value = 'default_sg_id'",
            "            self.mixin._ensure_default_security_group(self.ctx, 'tenant_1')",
            "            create_sg.assert_not_called()",
            "            get_default_sg_id.assert_called_once_with(self.ctx, 'tenant_1')",
            "",
            "    def test__ensure_default_security_group_created_in_parallel(self):",
            "        with mock.patch.object(",
            "                self.mixin, '_get_default_sg_id') as get_default_sg_id,\\",
            "                mock.patch.object(",
            "                        self.mixin, 'create_security_group') as create_sg:",
            "            get_default_sg_id.side_effect = [None, 'default_sg_id']",
            "            create_sg.side_effect = obj_exc.NeutronDbObjectDuplicateEntry(",
            "                mock.Mock(), mock.Mock())",
            "            self.mixin._ensure_default_security_group(self.ctx, 'tenant_1')",
            "            create_sg.assert_called_once_with(",
            "                self.ctx,",
            "                {'security_group': {",
            "                    'name': 'default',",
            "                    'tenant_id': 'tenant_1',",
            "                    'description': securitygroups_db.DEFAULT_SG_DESCRIPTION}},",
            "                default_sg=True)",
            "            get_default_sg_id.assert_has_calls([",
            "                mock.call(self.ctx, 'tenant_1'),",
            "                mock.call(self.ctx, 'tenant_1')])",
            "",
            "    def test__ensure_default_security_group_when_disabled(self):",
            "        with mock.patch.object(",
            "                    self.mixin, '_get_default_sg_id') as get_default_sg_id,\\",
            "                mock.patch.object(",
            "                        self.mixin, 'create_security_group') as create_sg:",
            "            self.is_ext_supported.return_value = False",
            "            self.mixin._ensure_default_security_group(self.ctx, 'tenant_1')",
            "            create_sg.assert_not_called()",
            "            get_default_sg_id.assert_not_called()"
        ],
        "afterPatchFile": [
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#    http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or",
            "# implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import copy",
            "from unittest import mock",
            "",
            "from neutron_lib.callbacks import events",
            "from neutron_lib.callbacks import exceptions",
            "from neutron_lib.callbacks import registry",
            "from neutron_lib.callbacks import resources",
            "from neutron_lib import constants",
            "from neutron_lib import context",
            "from neutron_lib.objects import exceptions as obj_exc",
            "import sqlalchemy",
            "import testtools",
            "",
            "from neutron.db import securitygroups_db",
            "from neutron.extensions import securitygroup",
            "from neutron import quota",
            "from neutron.services.revisions import revision_plugin",
            "from neutron.tests.unit import testlib_api",
            "",
            "",
            "FAKE_SECGROUP = {",
            "    'security_group': {",
            "        \"tenant_id\": 'fake',",
            "        'description': 'fake',",
            "        'name': 'fake'",
            "    }",
            "}",
            "",
            "FAKE_SECGROUP_RULE = {",
            "    'security_group_rule': {",
            "        \"tenant_id\": 'fake',",
            "        'description': 'fake',",
            "        'name': 'fake',",
            "        'port_range_min': '21',",
            "        'protocol': 'tcp',",
            "        'port_range_max': '23',",
            "        'remote_ip_prefix': '10.0.0.1',",
            "        'ethertype': 'IPv4',",
            "        'remote_group_id': None,",
            "        'security_group_id': 'None',",
            "        'direction': 'ingress'",
            "    }",
            "}",
            "",
            "DB_PLUGIN_KLASS = 'neutron.db.db_base_plugin_v2.NeutronDbPluginV2'",
            "",
            "",
            "def fake_callback(resource, event, *args, **kwargs):",
            "    raise KeyError('bar')",
            "",
            "",
            "class SecurityGroupDbMixinImpl(securitygroups_db.SecurityGroupDbMixin):",
            "    pass",
            "",
            "",
            "class SecurityGroupDbMixinTestCase(testlib_api.SqlTestCase):",
            "",
            "    def setUp(self):",
            "        super(SecurityGroupDbMixinTestCase, self).setUp()",
            "        self.setup_coreplugin(core_plugin=DB_PLUGIN_KLASS)",
            "        self.ctx = context.get_admin_context()",
            "        self.mixin = SecurityGroupDbMixinImpl()",
            "        make_res = mock.patch.object(quota.QuotaEngine, 'make_reservation')",
            "        self.mock_quota_make_res = make_res.start()",
            "        commit_res = mock.patch.object(quota.QuotaEngine, 'commit_reservation')",
            "        self.mock_quota_commit_res = commit_res.start()",
            "        is_ext_supported = mock.patch(",
            "            'neutron_lib.api.extensions.is_extension_supported')",
            "        self.is_ext_supported = is_ext_supported.start()",
            "        self.is_ext_supported.return_value = True",
            "",
            "    def test_create_security_group_conflict(self):",
            "        with mock.patch.object(registry, \"publish\") as mock_publish:",
            "            mock_publish.side_effect = exceptions.CallbackFailure(Exception())",
            "            secgroup = {'security_group': mock.ANY}",
            "            with testtools.ExpectedException(",
            "                    securitygroup.SecurityGroupConflict):",
            "                self.mixin.create_security_group(self.ctx, secgroup)",
            "",
            "    def test_delete_security_group_in_use(self):",
            "        with mock.patch.object(self.mixin,",
            "                               '_get_port_security_group_bindings'),\\",
            "                mock.patch.object(self.mixin, '_get_security_group'),\\",
            "                mock.patch.object(registry, \"notify\") as mock_notify:",
            "            mock_notify.side_effect = exceptions.CallbackFailure(Exception())",
            "            with testtools.ExpectedException(securitygroup.SecurityGroupInUse):",
            "                self.mixin.delete_security_group(self.ctx, mock.ANY)",
            "",
            "    def test_update_security_group_statefulness_binded_conflict(self):",
            "        FAKE_SECGROUP['security_group']['stateful'] = mock.ANY",
            "        sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "        FAKE_SECGROUP['security_group']['stateful'] = not sg_dict['stateful']",
            "        with mock.patch.object(self.mixin,",
            "                               '_get_port_security_group_bindings'), \\",
            "                mock.patch.object(registry, \"notify\") as mock_notify:",
            "            mock_notify.side_effect = exceptions.CallbackFailure(Exception())",
            "            with testtools.ExpectedException(securitygroup.SecurityGroupInUse):",
            "                self.mixin.update_security_group(self.ctx, sg_dict['id'],",
            "                                                 FAKE_SECGROUP)",
            "",
            "    def test_update_security_group_conflict(self):",
            "        with mock.patch.object(registry, \"notify\") as mock_notify:",
            "            mock_notify.side_effect = exceptions.CallbackFailure(Exception())",
            "            secgroup = {'security_group': FAKE_SECGROUP}",
            "            with testtools.ExpectedException(",
            "                    securitygroup.SecurityGroupConflict):",
            "                self.mixin.update_security_group(self.ctx, 'foo_id', secgroup)",
            "",
            "    def test_create_security_group_rule_conflict(self):",
            "        with mock.patch.object(self.mixin, '_validate_security_group_rule'),\\",
            "                mock.patch.object(self.mixin,",
            "                                  '_check_for_duplicate_rules'),\\",
            "                mock.patch.object(registry, \"notify\") as mock_notify:",
            "            mock_notify.side_effect = exceptions.CallbackFailure(Exception())",
            "            with testtools.ExpectedException(",
            "                    securitygroup.SecurityGroupConflict):",
            "                self.mixin.create_security_group_rule(",
            "                    self.ctx, mock.MagicMock())",
            "",
            "    def test__check_for_duplicate_rules_does_not_drop_protocol(self):",
            "        with mock.patch.object(self.mixin, 'get_security_group',",
            "                               return_value=None):",
            "            context = mock.Mock()",
            "            rule_dict = {",
            "                'security_group_rule': {'protocol': None,",
            "                                        'tenant_id': 'fake',",
            "                                        'security_group_id': 'fake',",
            "                                        'direction': 'fake'}",
            "            }",
            "            self.mixin._check_for_duplicate_rules(context, 'fake', [rule_dict])",
            "        self.assertIn('protocol', rule_dict['security_group_rule'])",
            "",
            "    def test__check_for_duplicate_rules_ignores_rule_id(self):",
            "        rules = [{'security_group_rule': {'protocol': 'tcp', 'id': 'fake1'}},",
            "                 {'security_group_rule': {'protocol': 'tcp', 'id': 'fake2'}}]",
            "",
            "        # NOTE(arosen): the name of this exception is a little misleading",
            "        # in this case as this test, tests that the id fields are dropped",
            "        # while being compared. This is in the case if a plugin specifies",
            "        # the rule ids themselves.",
            "        with mock.patch.object(self.mixin, 'get_security_group',",
            "                               return_value=None):",
            "            self.assertRaises(securitygroup.DuplicateSecurityGroupRuleInPost,",
            "                              self.mixin._check_for_duplicate_rules,",
            "                              context, 'fake', rules)",
            "",
            "    def test_check_for_duplicate_diff_rules_remote_ip_prefix_ipv4(self):",
            "        fake_secgroup = copy.deepcopy(FAKE_SECGROUP)",
            "        fake_secgroup['security_group_rules'] = \\",
            "            [{'id': 'fake', 'tenant_id': 'fake', 'ethertype': 'IPv4',",
            "              'direction': 'ingress', 'security_group_id': 'fake',",
            "              'remote_ip_prefix': None}]",
            "        with mock.patch.object(self.mixin, 'get_security_group',",
            "                               return_value=fake_secgroup):",
            "            context = mock.Mock()",
            "            rule_dict = {",
            "                'security_group_rule': {'id': 'fake2',",
            "                                        'tenant_id': 'fake',",
            "                                        'security_group_id': 'fake',",
            "                                        'ethertype': 'IPv4',",
            "                                        'direction': 'ingress',",
            "                                        'remote_ip_prefix': '0.0.0.0/0'}",
            "            }",
            "            self.assertRaises(securitygroup.SecurityGroupRuleExists,",
            "                self.mixin._check_for_duplicate_rules,",
            "                context, 'fake', [rule_dict])",
            "",
            "    def test_check_for_duplicate_diff_rules_remote_ip_prefix_ipv6(self):",
            "        fake_secgroup = copy.deepcopy(FAKE_SECGROUP)",
            "        fake_secgroup['security_group_rules'] = \\",
            "            [{'id': 'fake', 'tenant_id': 'fake', 'ethertype': 'IPv6',",
            "              'direction': 'ingress', 'security_group_id': 'fake',",
            "              'remote_ip_prefix': None}]",
            "        with mock.patch.object(self.mixin, 'get_security_group',",
            "                               return_value=fake_secgroup):",
            "            context = mock.Mock()",
            "            rule_dict = {",
            "                'security_group_rule': {'id': 'fake2',",
            "                                        'tenant_id': 'fake',",
            "                                        'security_group_id': 'fake',",
            "                                        'ethertype': 'IPv6',",
            "                                        'direction': 'ingress',",
            "                                        'remote_ip_prefix': '::/0'}",
            "            }",
            "            self.assertRaises(securitygroup.SecurityGroupRuleExists,",
            "                self.mixin._check_for_duplicate_rules,",
            "                context, 'fake', [rule_dict])",
            "",
            "    def test_delete_security_group_rule_in_use(self):",
            "        with mock.patch.object(registry, \"notify\") as mock_notify:",
            "            mock_notify.side_effect = exceptions.CallbackFailure(Exception())",
            "            with testtools.ExpectedException(",
            "                    securitygroup.SecurityGroupRuleInUse):",
            "                self.mixin.delete_security_group_rule(self.ctx, mock.ANY)",
            "",
            "    def test_delete_security_group_rule_raise_error_on_not_found(self):",
            "        with testtools.ExpectedException(",
            "                securitygroup.SecurityGroupRuleNotFound):",
            "            self.mixin.delete_security_group_rule(self.ctx, 'foo_rule')",
            "",
            "    def test_validate_ethertype_and_protocol(self):",
            "        fake_ipv4_rules = [{'protocol': constants.PROTO_NAME_IPV6_ICMP,",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': constants.PROTO_NAME_IPV6_ICMP_LEGACY,",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': constants.PROTO_NAME_IPV6_ENCAP,",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': constants.PROTO_NAME_IPV6_ROUTE,",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': constants.PROTO_NAME_IPV6_FRAG,",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': constants.PROTO_NAME_IPV6_NONXT,",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': constants.PROTO_NAME_IPV6_OPTS,",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': str(constants.PROTO_NUM_IPV6_ICMP),",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': str(constants.PROTO_NUM_IPV6_ENCAP),",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': str(constants.PROTO_NUM_IPV6_ROUTE),",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': str(constants.PROTO_NUM_IPV6_FRAG),",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': str(constants.PROTO_NUM_IPV6_NONXT),",
            "                            'ethertype': constants.IPv4},",
            "                           {'protocol': str(constants.PROTO_NUM_IPV6_OPTS),",
            "                            'ethertype': constants.IPv4}]",
            "        # test wrong protocols",
            "        for rule in fake_ipv4_rules:",
            "            with testtools.ExpectedException(",
            "                    securitygroup.SecurityGroupEthertypeConflictWithProtocol):",
            "                self.mixin._validate_ethertype_and_protocol(rule)",
            "",
            "    def test_security_group_precommit_create_event_fail(self):",
            "        registry.subscribe(fake_callback, resources.SECURITY_GROUP,",
            "                           events.PRECOMMIT_CREATE)",
            "        with mock.patch.object(sqlalchemy.orm.session.SessionTransaction,",
            "                               'rollback') as mock_rollback:",
            "            self.assertRaises(securitygroup.SecurityGroupConflict,",
            "                              self.mixin.create_security_group,",
            "                              self.ctx, FAKE_SECGROUP)",
            "            self.assertTrue(mock_rollback.called)",
            "",
            "    def test_security_group_precommit_update_event_fail(self):",
            "        registry.subscribe(fake_callback, resources.SECURITY_GROUP,",
            "                           events.PRECOMMIT_UPDATE)",
            "        sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "        with mock.patch.object(sqlalchemy.orm.session.SessionTransaction,",
            "                              'rollback') as mock_rollback:",
            "            self.assertRaises(securitygroup.SecurityGroupConflict,",
            "                              self.mixin.update_security_group,",
            "                              self.ctx, sg_dict['id'], FAKE_SECGROUP)",
            "            self.assertTrue(mock_rollback.called)",
            "",
            "    def test_security_group_precommit_delete_event_fail(self):",
            "        registry.subscribe(fake_callback, resources.SECURITY_GROUP,",
            "                           events.PRECOMMIT_DELETE)",
            "        sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "        with mock.patch.object(sqlalchemy.orm.session.SessionTransaction,",
            "                              'rollback') as mock_rollback:",
            "            self.assertRaises(securitygroup.SecurityGroupInUse,",
            "                              self.mixin.delete_security_group,",
            "                              self.ctx, sg_dict['id'])",
            "            self.assertTrue(mock_rollback.called)",
            "",
            "    def _test_security_group_precommit_create_event(self,",
            "                                                    with_revisions=False):",
            "        DEFAULT_SECGROUP = {",
            "            'tenant_id': FAKE_SECGROUP['security_group']['tenant_id'],",
            "            'name': 'default',",
            "            'description': 'Default security group',",
            "        }",
            "        DEFAULT_SECGROUP_DICT = {",
            "            'id': mock.ANY,",
            "            'tenant_id': FAKE_SECGROUP['security_group']['tenant_id'],",
            "            'project_id': FAKE_SECGROUP['security_group']['tenant_id'],",
            "            'name': 'default',",
            "            'description': 'Default security group',",
            "            'stateful': mock.ANY,",
            "            'security_group_rules': [",
            "                # Four rules for egress/ingress and ipv4/ipv6",
            "                mock.ANY, mock.ANY, mock.ANY, mock.ANY,",
            "            ],",
            "        }",
            "        if with_revisions:",
            "            DEFAULT_SECGROUP_DICT.update({",
            "                'revision_number': mock.ANY,",
            "            })",
            "        with mock.patch.object(registry, 'publish') as publish, \\",
            "                mock.patch.object(registry, \"notify\") as mock_notify:",
            "            sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "            mock_notify.assert_has_calls([",
            "                mock.call('security_group', 'precommit_create', mock.ANY,",
            "                          context=mock.ANY, is_default=True,",
            "                          security_group=DEFAULT_SECGROUP_DICT),",
            "                mock.call('security_group', 'after_create', mock.ANY,",
            "                          context=mock.ANY, is_default=True,",
            "                          security_group=DEFAULT_SECGROUP_DICT),",
            "                mock.call('security_group', 'precommit_create', mock.ANY,",
            "                          context=mock.ANY, is_default=False,",
            "                          security_group=sg_dict),",
            "                mock.call('security_group', 'after_create', mock.ANY,",
            "                          context=mock.ANY, is_default=False,",
            "                          security_group=sg_dict)])",
            "",
            "            publish.assert_has_calls([",
            "                mock.call('security_group', 'before_create', mock.ANY,",
            "                          payload=mock.ANY),",
            "                mock.call('security_group', 'before_create', mock.ANY,",
            "                          payload=mock.ANY)])",
            "            payload = publish.mock_calls[0][2]['payload']",
            "            self.assertDictEqual(payload.desired_state,",
            "                                 FAKE_SECGROUP['security_group'])",
            "            payload = publish.mock_calls[1][2]['payload']",
            "            self.assertDictEqual(payload.desired_state, DEFAULT_SECGROUP)",
            "",
            "            # Ensure that the result of create is same as get.",
            "            # Especially we want to check the revision number here.",
            "            sg_dict_got = self.mixin.get_security_group(",
            "                self.ctx, sg_dict['id'])",
            "            self.assertEqual(sg_dict, sg_dict_got)",
            "",
            "    def test_security_group_precommit_create_event_with_revisions(self):",
            "        revision = revision_plugin.RevisionPlugin()",
            "        self._test_security_group_precommit_create_event(True)",
            "        del revision  # appease pep8",
            "",
            "    def test_security_group_precommit_create_event(self):",
            "        self._test_security_group_precommit_create_event()",
            "",
            "    def test_security_group_precommit_update_event(self):",
            "        FAKE_SECGROUP['security_group']['stateful'] = mock.ANY",
            "        original_sg_dict = self.mixin.create_security_group(self.ctx,",
            "                                                            FAKE_SECGROUP)",
            "        sg_id = original_sg_dict['id']",
            "        with mock.patch.object(self.mixin,",
            "                               '_get_port_security_group_bindings'), \\",
            "                mock.patch.object(registry, \"publish\") as mock_notify:",
            "            fake_secgroup = copy.deepcopy(FAKE_SECGROUP)",
            "            fake_secgroup['security_group']['name'] = 'updated_fake'",
            "            fake_secgroup['security_group']['stateful'] = mock.ANY",
            "            sg_dict = self.mixin.update_security_group(",
            "                    self.ctx, sg_id, fake_secgroup)",
            "",
            "            mock_notify.assert_has_calls(",
            "                [mock.call('security_group', 'precommit_update', mock.ANY,",
            "                           payload=mock.ANY)])",
            "            payload = mock_notify.call_args[1]['payload']",
            "            self.assertEqual(original_sg_dict, payload.states[0])",
            "            self.assertEqual(sg_id, payload.resource_id)",
            "            self.assertEqual(sg_dict, payload.desired_state)",
            "",
            "    def test_security_group_precommit_and_after_delete_event(self):",
            "        sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "        with mock.patch.object(registry, \"notify\") as mock_notify:",
            "            self.mixin.delete_security_group(self.ctx, sg_dict['id'])",
            "            sg_dict['security_group_rules'] = mock.ANY",
            "            mock_notify.assert_has_calls(",
            "                [mock.call('security_group', 'precommit_delete',",
            "                           mock.ANY, context=mock.ANY, security_group=sg_dict,",
            "                           security_group_id=sg_dict['id'],",
            "                           security_group_rule_ids=[mock.ANY, mock.ANY]),",
            "                 mock.call('security_group', 'after_delete',",
            "                           mock.ANY, context=mock.ANY,",
            "                           security_group_id=sg_dict['id'],",
            "                           security_group_rule_ids=[mock.ANY, mock.ANY],",
            "                           name=sg_dict['name'])])",
            "",
            "    def test_security_group_rule_precommit_create_event_fail(self):",
            "        registry.subscribe(fake_callback, resources.SECURITY_GROUP_RULE,",
            "                           events.PRECOMMIT_CREATE)",
            "        sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "        fake_rule = FAKE_SECGROUP_RULE",
            "        fake_rule['security_group_rule']['security_group_id'] = sg_dict['id']",
            "        with mock.patch.object(sqlalchemy.orm.session.SessionTransaction,",
            "                               'rollback') as mock_rollback,\\",
            "                mock.patch.object(self.mixin, '_get_security_group'):",
            "            self.assertRaises(securitygroup.SecurityGroupConflict,",
            "                              self.mixin.create_security_group_rule,",
            "                              self.ctx, fake_rule)",
            "            self.assertTrue(mock_rollback.called)",
            "",
            "    def test_security_group_rule_precommit_delete_event_fail(self):",
            "        registry.subscribe(fake_callback, resources.SECURITY_GROUP_RULE,",
            "                           events.PRECOMMIT_DELETE)",
            "        sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "        fake_rule = FAKE_SECGROUP_RULE",
            "        fake_rule['security_group_rule']['security_group_id'] = sg_dict['id']",
            "        with mock.patch.object(sqlalchemy.orm.session.SessionTransaction,",
            "                               'rollback') as mock_rollback,\\",
            "                mock.patch.object(self.mixin, '_get_security_group'):",
            "            sg_rule_dict = self.mixin.create_security_group_rule(self.ctx,",
            "                   fake_rule)",
            "            self.assertRaises(securitygroup.SecurityGroupRuleInUse,",
            "                              self.mixin.delete_security_group_rule, self.ctx,",
            "                              sg_rule_dict['id'])",
            "            self.assertTrue(mock_rollback.called)",
            "",
            "    def test_security_group_rule_precommit_create_event(self):",
            "        sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "        fake_rule = FAKE_SECGROUP_RULE",
            "        fake_rule['security_group_rule']['security_group_id'] = sg_dict['id']",
            "        with mock.patch.object(registry, \"notify\") as mock_notify, \\",
            "                mock.patch.object(self.mixin, '_get_security_group'):",
            "            mock_notify.assert_has_calls([mock.call('security_group_rule',",
            "                'precommit_create', mock.ANY, context=mock.ANY,",
            "                security_group_rule=self.mixin.create_security_group_rule(",
            "                    self.ctx, fake_rule))])",
            "",
            "    def test_sg_rule_before_precommit_and_after_delete_event(self):",
            "        sg_dict = self.mixin.create_security_group(self.ctx, FAKE_SECGROUP)",
            "        fake_rule = FAKE_SECGROUP_RULE",
            "        fake_rule['security_group_rule']['security_group_id'] = sg_dict['id']",
            "        with mock.patch.object(registry, \"notify\") as mock_notify, \\",
            "                mock.patch.object(self.mixin, '_get_security_group'):",
            "            sg_rule_dict = self.mixin.create_security_group_rule(self.ctx,",
            "                   fake_rule)",
            "            self.mixin.delete_security_group_rule(self.ctx,",
            "                    sg_rule_dict['id'])",
            "            mock_notify.assert_has_calls([mock.call('security_group_rule',",
            "                'before_delete', mock.ANY, context=mock.ANY,",
            "                security_group_rule_id=sg_rule_dict['id'])])",
            "            mock_notify.assert_has_calls([mock.call('security_group_rule',",
            "                'precommit_delete', mock.ANY, context=mock.ANY,",
            "                security_group_id=sg_dict['id'],",
            "                security_group_rule_id=sg_rule_dict['id'])])",
            "            mock_notify.assert_has_calls([mock.call('security_group_rule',",
            "                'after_delete', mock.ANY, context=mock.ANY,",
            "                security_group_rule_id=sg_rule_dict['id'],",
            "                security_group_id=sg_dict['id'])])",
            "",
            "    def test_get_ip_proto_name_and_num(self):",
            "        protocols = [constants.PROTO_NAME_UDP, str(constants.PROTO_NUM_TCP),",
            "                     'blah', '111']",
            "        protocol_names_nums = (",
            "            [[constants.PROTO_NAME_UDP, str(constants.PROTO_NUM_UDP)],",
            "             [constants.PROTO_NAME_TCP, str(constants.PROTO_NUM_TCP)],",
            "             ['blah', 'blah'], ['111', '111']])",
            "",
            "        for i, protocol in enumerate(protocols):",
            "            self.assertEqual(protocol_names_nums[i],",
            "                             self.mixin._get_ip_proto_name_and_num(protocol))",
            "",
            "    def test__validate_port_range_for_icmp_exception(self):",
            "        states = [(1, 256, securitygroup.SecurityGroupInvalidIcmpValue),",
            "                  (None, 6, securitygroup.SecurityGroupMissingIcmpType),",
            "                  (300, 1, securitygroup.SecurityGroupInvalidIcmpValue)]",
            "        for protocol in (constants.PROTO_NAME_ICMP,",
            "                         constants.PROTO_NAME_IPV6_ICMP,",
            "                         constants.PROTO_NAME_IPV6_ICMP_LEGACY):",
            "            for pmin, pmax, exception in states:",
            "                self.assertRaises(exception,",
            "                    self.mixin._validate_port_range,",
            "                    {'port_range_min': pmin,",
            "                     'port_range_max': pmax,",
            "                     'protocol': protocol})",
            "",
            "    def test__validate_port_range_exception(self):",
            "        self.assertRaises(securitygroup.SecurityGroupInvalidPortValue,",
            "                          self.mixin._validate_port_range,",
            "                          {'port_range_min': 0,",
            "                           'port_range_max': None,",
            "                           'protocol': constants.PROTO_NAME_TCP})",
            "        self.assertRaises(securitygroup.SecurityGroupInvalidPortRange,",
            "                          self.mixin._validate_port_range,",
            "                          {'port_range_min': 1,",
            "                           'port_range_max': None,",
            "                           'protocol': constants.PROTO_NAME_SCTP})",
            "        self.assertRaises(securitygroup.SecurityGroupInvalidPortRange,",
            "                          self.mixin._validate_port_range,",
            "                          {'port_range_min': 1000,",
            "                           'port_range_max': 1,",
            "                           'protocol': constants.PROTO_NAME_UDPLITE})",
            "        self.assertRaises(",
            "            securitygroup.SecurityGroupInvalidProtocolForPort,",
            "            self.mixin._validate_port_range,",
            "            {'port_range_min': 100,",
            "             'port_range_max': 200,",
            "             'protocol': '111'})",
            "        self.assertRaises(",
            "            securitygroup.SecurityGroupInvalidProtocolForPort,",
            "            self.mixin._validate_port_range,",
            "            {'port_range_min': 100,",
            "             'port_range_max': None,",
            "             'protocol': constants.PROTO_NAME_VRRP})",
            "        self.assertRaises(",
            "            securitygroup.SecurityGroupInvalidProtocolForPort,",
            "            self.mixin._validate_port_range,",
            "            {'port_range_min': None,",
            "             'port_range_max': 200,",
            "             'protocol': constants.PROTO_NAME_VRRP})",
            "",
            "    def _create_environment(self):",
            "        self.sg = copy.deepcopy(FAKE_SECGROUP)",
            "        self.user_ctx = context.Context(user_id='user1', tenant_id='tenant_1',",
            "                                        is_admin=False, overwrite=False)",
            "        self.admin_ctx = context.Context(user_id='user2', tenant_id='tenant_2',",
            "                                         is_admin=True, overwrite=False)",
            "        self.sg_user = self.mixin.create_security_group(",
            "            self.user_ctx, {'security_group': {'name': 'name',",
            "                                               'tenant_id': 'tenant_1',",
            "                                               'description': 'fake'}})",
            "",
            "    def test_get_security_group_rules(self):",
            "        self._create_environment()",
            "        rules_before = self.mixin.get_security_group_rules(self.user_ctx)",
            "",
            "        rule = copy.deepcopy(FAKE_SECGROUP_RULE)",
            "        rule['security_group_rule']['security_group_id'] = self.sg_user['id']",
            "        rule['security_group_rule']['tenant_id'] = 'tenant_2'",
            "        self.mixin.create_security_group_rule(self.admin_ctx, rule)",
            "",
            "        rules_after = self.mixin.get_security_group_rules(self.user_ctx)",
            "        self.assertEqual(len(rules_before) + 1, len(rules_after))",
            "        for rule in (rule for rule in rules_after if rule not in rules_before):",
            "            self.assertEqual('tenant_2', rule['tenant_id'])",
            "",
            "    def test_get_security_group_rules_filters_passed(self):",
            "        self._create_environment()",
            "        filters = {'security_group_id': self.sg_user['id']}",
            "        rules_before = self.mixin.get_security_group_rules(self.user_ctx,",
            "                                                           filters=filters)",
            "",
            "        default_sg = self.mixin.get_security_groups(",
            "            self.user_ctx, filters={'name': 'default'})[0]",
            "        rule = copy.deepcopy(FAKE_SECGROUP_RULE)",
            "        rule['security_group_rule']['security_group_id'] = default_sg['id']",
            "        rule['security_group_rule']['tenant_id'] = 'tenant_1'",
            "        self.mixin.create_security_group_rule(self.user_ctx, rule)",
            "",
            "        rules_after = self.mixin.get_security_group_rules(self.user_ctx,",
            "                                                          filters=filters)",
            "        self.assertEqual(rules_before, rules_after)",
            "",
            "    def test_get_security_group_rules_admin_context(self):",
            "        self._create_environment()",
            "        rules_before = self.mixin.get_security_group_rules(self.ctx)",
            "",
            "        rule = copy.deepcopy(FAKE_SECGROUP_RULE)",
            "        rule['security_group_rule']['security_group_id'] = self.sg_user['id']",
            "        rule['security_group_rule']['tenant_id'] = 'tenant_1'",
            "        self.mixin.create_security_group_rule(self.user_ctx, rule)",
            "",
            "        rules_after = self.mixin.get_security_group_rules(self.ctx)",
            "        self.assertEqual(len(rules_before) + 1, len(rules_after))",
            "        for rule in (rule for rule in rules_after if rule not in rules_before):",
            "            self.assertEqual('tenant_1', rule['tenant_id'])",
            "            self.assertEqual(self.sg_user['id'], rule['security_group_id'])",
            "",
            "    def test__ensure_default_security_group(self):",
            "        with mock.patch.object(",
            "                self.mixin, '_get_default_sg_id') as get_default_sg_id,\\",
            "                mock.patch.object(",
            "                        self.mixin, 'create_security_group') as create_sg:",
            "            get_default_sg_id.return_value = None",
            "            self.mixin._ensure_default_security_group(self.ctx, 'tenant_1')",
            "            create_sg.assert_called_once_with(",
            "                self.ctx,",
            "                {'security_group': {",
            "                    'name': 'default',",
            "                    'tenant_id': 'tenant_1',",
            "                    'description': securitygroups_db.DEFAULT_SG_DESCRIPTION}},",
            "                default_sg=True)",
            "            get_default_sg_id.assert_called_once_with(self.ctx, 'tenant_1')",
            "",
            "    def test__ensure_default_security_group_already_exists(self):",
            "        with mock.patch.object(",
            "                self.mixin, '_get_default_sg_id') as get_default_sg_id,\\",
            "                mock.patch.object(",
            "                        self.mixin, 'create_security_group') as create_sg:",
            "            get_default_sg_id.return_value = 'default_sg_id'",
            "            self.mixin._ensure_default_security_group(self.ctx, 'tenant_1')",
            "            create_sg.assert_not_called()",
            "            get_default_sg_id.assert_called_once_with(self.ctx, 'tenant_1')",
            "",
            "    def test__ensure_default_security_group_created_in_parallel(self):",
            "        with mock.patch.object(",
            "                self.mixin, '_get_default_sg_id') as get_default_sg_id,\\",
            "                mock.patch.object(",
            "                        self.mixin, 'create_security_group') as create_sg:",
            "            get_default_sg_id.side_effect = [None, 'default_sg_id']",
            "            create_sg.side_effect = obj_exc.NeutronDbObjectDuplicateEntry(",
            "                mock.Mock(), mock.Mock())",
            "            self.mixin._ensure_default_security_group(self.ctx, 'tenant_1')",
            "            create_sg.assert_called_once_with(",
            "                self.ctx,",
            "                {'security_group': {",
            "                    'name': 'default',",
            "                    'tenant_id': 'tenant_1',",
            "                    'description': securitygroups_db.DEFAULT_SG_DESCRIPTION}},",
            "                default_sg=True)",
            "            get_default_sg_id.assert_has_calls([",
            "                mock.call(self.ctx, 'tenant_1'),",
            "                mock.call(self.ctx, 'tenant_1')])",
            "",
            "    def test__ensure_default_security_group_when_disabled(self):",
            "        with mock.patch.object(",
            "                    self.mixin, '_get_default_sg_id') as get_default_sg_id,\\",
            "                mock.patch.object(",
            "                        self.mixin, 'create_security_group') as create_sg:",
            "            self.is_ext_supported.return_value = False",
            "            self.mixin._ensure_default_security_group(self.ctx, 'tenant_1')",
            "            create_sg.assert_not_called()",
            "            get_default_sg_id.assert_not_called()",
            "",
            "    def test__ensure_default_security_group_tenant_mismatch(self):",
            "        with mock.patch.object(",
            "                self.mixin, '_get_default_sg_id') as get_default_sg_id,\\",
            "                mock.patch.object(",
            "                        self.mixin, 'create_security_group') as create_sg:",
            "            context = mock.Mock()",
            "            context.tenant_id = 'tenant_0'",
            "            context.is_admin = False",
            "            self.mixin._ensure_default_security_group(context, 'tenant_1')",
            "            create_sg.assert_not_called()",
            "            get_default_sg_id.assert_not_called()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    }
}