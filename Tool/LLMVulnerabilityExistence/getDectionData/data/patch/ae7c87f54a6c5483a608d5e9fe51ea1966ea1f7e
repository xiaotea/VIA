{
    "octavia/certificates/common/local.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 29,
                "PatchRowcode": "     'OS_OCTAVIA_TLS_CA_KEY', '/etc/ssl/private/ssl-cert-snakeoil.key'"
            },
            "1": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " )"
            },
            "2": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " TLS_PKP_DEFAULT = os.environ.get('OS_OCTAVIA_CA_KEY_PASS')"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+TLS_PASS_AMPS_DEFAULT = os.environ.get('TLS_PASS_AMPS_DEFAULT',"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+                                       'insecure-key-do-not-use-this-key')"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " TLS_DIGEST_DEFAULT = os.environ.get('OS_OCTAVIA_CA_SIGNING_DIGEST', 'sha256')"
            },
            "7": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " TLS_STORAGE_DEFAULT = os.environ.get("
            },
            "8": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 37,
                "PatchRowcode": "     'OS_OCTAVIA_TLS_STORAGE', '/var/lib/octavia/certificates/'"
            },
            "9": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 50,
                "PatchRowcode": "                default=TLS_PKP_DEFAULT,"
            },
            "10": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 51,
                "PatchRowcode": "                help='Passphrase for the Private Key. Defaults'"
            },
            "11": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 52,
                "PatchRowcode": "                     ' to env[OS_OCTAVIA_CA_KEY_PASS] or None.'),"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+    cfg.StrOpt('server_certs_key_passphrase',"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 54,
                "PatchRowcode": "+               default=TLS_PASS_AMPS_DEFAULT,"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+               help='Passphrase for encrypting Amphora Certificates and '"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 56,
                "PatchRowcode": "+                    'Private Keys. Defaults to env[TLS_PASS_AMPS_DEFAULT] or '"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 57,
                "PatchRowcode": "+                    'insecure-key-do-not-use-this-key',"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 58,
                "PatchRowcode": "+               required=True),"
            },
            "18": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 59,
                "PatchRowcode": "     cfg.StrOpt('signing_digest',"
            },
            "19": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 60,
                "PatchRowcode": "                default=TLS_DIGEST_DEFAULT,"
            },
            "20": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 61,
                "PatchRowcode": "                help='Certificate signing digest. Defaults'"
            },
            "21": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 69,
                "PatchRowcode": "                     'Defaults to env[OS_OCTAVIA_TLS_STORAGE].')"
            },
            "22": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 70,
                "PatchRowcode": " ]"
            },
            "23": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 71,
                "PatchRowcode": " "
            },
            "24": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-CONF = cfg.CONF"
            },
            "25": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-CONF.register_opts(certgen_opts, group='certificates')"
            },
            "26": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-CONF.register_opts(certmgr_opts, group='certificates')"
            },
            "27": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "28": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 72,
                "PatchRowcode": " "
            },
            "29": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 73,
                "PatchRowcode": " class LocalCert(cert.Cert):"
            },
            "30": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 74,
                "PatchRowcode": "     \"\"\"Representation of a Cert for local storage.\"\"\""
            }
        },
        "frontPatchFile": [
            "# Copyright (c) 2014 Rackspace US, Inc",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "\"\"\"",
            "Common classes for local filesystem certificate handling",
            "\"\"\"",
            "import os",
            "",
            "from oslo_config import cfg",
            "",
            "from octavia.certificates.common import cert",
            "",
            "TLS_CERT_DEFAULT = os.environ.get(",
            "    'OS_OCTAVIA_TLS_CA_CERT', '/etc/ssl/certs/ssl-cert-snakeoil.pem'",
            ")",
            "TLS_KEY_DEFAULT = os.environ.get(",
            "    'OS_OCTAVIA_TLS_CA_KEY', '/etc/ssl/private/ssl-cert-snakeoil.key'",
            ")",
            "TLS_PKP_DEFAULT = os.environ.get('OS_OCTAVIA_CA_KEY_PASS')",
            "TLS_DIGEST_DEFAULT = os.environ.get('OS_OCTAVIA_CA_SIGNING_DIGEST', 'sha256')",
            "TLS_STORAGE_DEFAULT = os.environ.get(",
            "    'OS_OCTAVIA_TLS_STORAGE', '/var/lib/octavia/certificates/'",
            ")",
            "",
            "certgen_opts = [",
            "    cfg.StrOpt('ca_certificate',",
            "               default=TLS_CERT_DEFAULT,",
            "               help='Absolute path to the CA Certificate for signing. Defaults'",
            "                    ' to env[OS_OCTAVIA_TLS_CA_CERT].'),",
            "    cfg.StrOpt('ca_private_key',",
            "               default=TLS_KEY_DEFAULT,",
            "               help='Absolute path to the Private Key for signing. Defaults'",
            "                    ' to env[OS_OCTAVIA_TLS_CA_KEY].'),",
            "    cfg.StrOpt('ca_private_key_passphrase',",
            "               default=TLS_PKP_DEFAULT,",
            "               help='Passphrase for the Private Key. Defaults'",
            "                    ' to env[OS_OCTAVIA_CA_KEY_PASS] or None.'),",
            "    cfg.StrOpt('signing_digest',",
            "               default=TLS_DIGEST_DEFAULT,",
            "               help='Certificate signing digest. Defaults'",
            "                    ' to env[OS_OCTAVIA_CA_SIGNING_DIGEST] or \"sha256\".')",
            "]",
            "",
            "certmgr_opts = [",
            "    cfg.StrOpt('storage_path',",
            "               default=TLS_STORAGE_DEFAULT,",
            "               help='Absolute path to the certificate storage directory. '",
            "                    'Defaults to env[OS_OCTAVIA_TLS_STORAGE].')",
            "]",
            "",
            "CONF = cfg.CONF",
            "CONF.register_opts(certgen_opts, group='certificates')",
            "CONF.register_opts(certmgr_opts, group='certificates')",
            "",
            "",
            "class LocalCert(cert.Cert):",
            "    \"\"\"Representation of a Cert for local storage.\"\"\"",
            "    def __init__(self, certificate, private_key, intermediates=None,",
            "                 private_key_passphrase=None):",
            "        self.certificate = certificate",
            "        self.intermediates = intermediates",
            "        self.private_key = private_key",
            "        self.private_key_passphrase = private_key_passphrase",
            "",
            "    def get_certificate(self):",
            "        return self.certificate",
            "",
            "    def get_intermediates(self):",
            "        return self.intermediates",
            "",
            "    def get_private_key(self):",
            "        return self.private_key",
            "",
            "    def get_private_key_passphrase(self):",
            "        return self.private_key_passphrase"
        ],
        "afterPatchFile": [
            "# Copyright (c) 2014 Rackspace US, Inc",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "\"\"\"",
            "Common classes for local filesystem certificate handling",
            "\"\"\"",
            "import os",
            "",
            "from oslo_config import cfg",
            "",
            "from octavia.certificates.common import cert",
            "",
            "TLS_CERT_DEFAULT = os.environ.get(",
            "    'OS_OCTAVIA_TLS_CA_CERT', '/etc/ssl/certs/ssl-cert-snakeoil.pem'",
            ")",
            "TLS_KEY_DEFAULT = os.environ.get(",
            "    'OS_OCTAVIA_TLS_CA_KEY', '/etc/ssl/private/ssl-cert-snakeoil.key'",
            ")",
            "TLS_PKP_DEFAULT = os.environ.get('OS_OCTAVIA_CA_KEY_PASS')",
            "TLS_PASS_AMPS_DEFAULT = os.environ.get('TLS_PASS_AMPS_DEFAULT',",
            "                                       'insecure-key-do-not-use-this-key')",
            "",
            "TLS_DIGEST_DEFAULT = os.environ.get('OS_OCTAVIA_CA_SIGNING_DIGEST', 'sha256')",
            "TLS_STORAGE_DEFAULT = os.environ.get(",
            "    'OS_OCTAVIA_TLS_STORAGE', '/var/lib/octavia/certificates/'",
            ")",
            "",
            "certgen_opts = [",
            "    cfg.StrOpt('ca_certificate',",
            "               default=TLS_CERT_DEFAULT,",
            "               help='Absolute path to the CA Certificate for signing. Defaults'",
            "                    ' to env[OS_OCTAVIA_TLS_CA_CERT].'),",
            "    cfg.StrOpt('ca_private_key',",
            "               default=TLS_KEY_DEFAULT,",
            "               help='Absolute path to the Private Key for signing. Defaults'",
            "                    ' to env[OS_OCTAVIA_TLS_CA_KEY].'),",
            "    cfg.StrOpt('ca_private_key_passphrase',",
            "               default=TLS_PKP_DEFAULT,",
            "               help='Passphrase for the Private Key. Defaults'",
            "                    ' to env[OS_OCTAVIA_CA_KEY_PASS] or None.'),",
            "    cfg.StrOpt('server_certs_key_passphrase',",
            "               default=TLS_PASS_AMPS_DEFAULT,",
            "               help='Passphrase for encrypting Amphora Certificates and '",
            "                    'Private Keys. Defaults to env[TLS_PASS_AMPS_DEFAULT] or '",
            "                    'insecure-key-do-not-use-this-key',",
            "               required=True),",
            "    cfg.StrOpt('signing_digest',",
            "               default=TLS_DIGEST_DEFAULT,",
            "               help='Certificate signing digest. Defaults'",
            "                    ' to env[OS_OCTAVIA_CA_SIGNING_DIGEST] or \"sha256\".')",
            "]",
            "",
            "certmgr_opts = [",
            "    cfg.StrOpt('storage_path',",
            "               default=TLS_STORAGE_DEFAULT,",
            "               help='Absolute path to the certificate storage directory. '",
            "                    'Defaults to env[OS_OCTAVIA_TLS_STORAGE].')",
            "]",
            "",
            "",
            "class LocalCert(cert.Cert):",
            "    \"\"\"Representation of a Cert for local storage.\"\"\"",
            "    def __init__(self, certificate, private_key, intermediates=None,",
            "                 private_key_passphrase=None):",
            "        self.certificate = certificate",
            "        self.intermediates = intermediates",
            "        self.private_key = private_key",
            "        self.private_key_passphrase = private_key_passphrase",
            "",
            "    def get_certificate(self):",
            "        return self.certificate",
            "",
            "    def get_intermediates(self):",
            "        return self.intermediates",
            "",
            "    def get_private_key(self):",
            "        return self.private_key",
            "",
            "    def get_private_key_passphrase(self):",
            "        return self.private_key_passphrase"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "63": [
                "CONF"
            ],
            "64": [],
            "65": [],
            "66": []
        },
        "addLocation": []
    },
    "octavia/common/config.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from oslo_log import log as logging"
            },
            "1": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " import oslo_messaging as messaging"
            },
            "2": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+from octavia.certificates.common import local"
            },
            "4": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " from octavia.common import constants"
            },
            "5": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " from octavia.common import utils"
            },
            "6": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " from octavia.i18n import _"
            },
            "7": {
                "beforePatchRowNumber": 621,
                "afterPatchRowNumber": 622,
                "PatchRowcode": " cfg.CONF.register_opts(quota_opts, group='quotas')"
            },
            "8": {
                "beforePatchRowNumber": 622,
                "afterPatchRowNumber": 623,
                "PatchRowcode": " cfg.CONF.register_opts(audit_opts, group='audit')"
            },
            "9": {
                "beforePatchRowNumber": 623,
                "afterPatchRowNumber": 624,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 625,
                "PatchRowcode": "+cfg.CONF.register_opts(local.certgen_opts, group='certificates')"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 626,
                "PatchRowcode": "+cfg.CONF.register_opts(local.certmgr_opts, group='certificates')"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 627,
                "PatchRowcode": "+"
            },
            "13": {
                "beforePatchRowNumber": 624,
                "afterPatchRowNumber": 628,
                "PatchRowcode": " # Ensure that the control exchange is set correctly"
            },
            "14": {
                "beforePatchRowNumber": 625,
                "afterPatchRowNumber": 629,
                "PatchRowcode": " messaging.set_transport_defaults(control_exchange='octavia')"
            },
            "15": {
                "beforePatchRowNumber": 626,
                "afterPatchRowNumber": 630,
                "PatchRowcode": " _SQL_CONNECTION_DEFAULT = 'sqlite://'"
            }
        },
        "frontPatchFile": [
            "# Copyright 2011 VMware, Inc., 2014 A10 Networks",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "\"\"\"",
            "Routines for configuring Octavia",
            "\"\"\"",
            "",
            "import os",
            "import sys",
            "",
            "from keystoneauth1 import loading as ks_loading",
            "from oslo_config import cfg",
            "from oslo_db import options as db_options",
            "from oslo_log import log as logging",
            "import oslo_messaging as messaging",
            "",
            "from octavia.common import constants",
            "from octavia.common import utils",
            "from octavia.i18n import _",
            "from octavia import version",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "core_opts = [",
            "    cfg.HostnameOpt('host', default=utils.get_hostname(),",
            "                    help=_(\"The hostname Octavia is running on\")),",
            "    cfg.StrOpt('octavia_plugins', default='hot_plug_plugin',",
            "               help=_(\"Name of the controller plugin to use\")),",
            "]",
            "",
            "api_opts = [",
            "    cfg.IPOpt('bind_host', default='127.0.0.1',",
            "              help=_(\"The host IP to bind to\")),",
            "    cfg.PortOpt('bind_port', default=9876,",
            "                help=_(\"The port to bind to\")),",
            "    cfg.StrOpt('auth_strategy', default=constants.KEYSTONE,",
            "               choices=[constants.NOAUTH,",
            "                        constants.KEYSTONE,",
            "                        constants.TESTING],",
            "               help=_(\"The auth strategy for API requests.\")),",
            "    cfg.StrOpt('api_handler', default='queue_producer',",
            "               help=_(\"The handler that the API communicates with\")),",
            "    cfg.BoolOpt('allow_pagination', default=True,",
            "                help=_(\"Allow the usage of pagination\")),",
            "    cfg.BoolOpt('allow_sorting', default=True,",
            "                help=_(\"Allow the usage of sorting\")),",
            "    cfg.BoolOpt('allow_filtering', default=True,",
            "                help=_(\"Allow the usage of filtering\")),",
            "    cfg.BoolOpt('allow_field_selection', default=True,",
            "                help=_(\"Allow the usage of field selection\")),",
            "    cfg.StrOpt('pagination_max_limit',",
            "               default=str(constants.DEFAULT_PAGE_SIZE),",
            "               help=_(\"The maximum number of items returned in a single \"",
            "                      \"response. The string 'infinite' or a negative \"",
            "                      \"integer value means 'no limit'\")),",
            "    cfg.StrOpt('api_base_uri',",
            "               help=_(\"Base URI for the API for use in pagination links. \"",
            "                      \"This will be autodetected from the request if not \"",
            "                      \"overridden here.\")),",
            "    cfg.BoolOpt('api_v1_enabled', default=True,",
            "                help=_(\"Expose the v1 API?\")),",
            "    cfg.BoolOpt('api_v2_enabled', default=True,",
            "                help=_(\"Expose the v2 API?\")),",
            "    cfg.BoolOpt('allow_tls_terminated_listeners', default=True,",
            "                help=_(\"Allow users to create TLS Terminated listeners?\")),",
            "    cfg.BoolOpt('allow_ping_health_monitors', default=True,",
            "                help=_(\"Allow users to create PING type Health Monitors?\")),",
            "    cfg.DictOpt('enabled_provider_drivers',",
            "                help=_('List of enabled provider drivers and description '",
            "                       'dictionaries. Must match the driver name in the '",
            "                       'octavia.api.drivers entrypoint. Example: '",
            "                       '{\\'amphora\\': \\'The Octavia Amphora driver.\\', '",
            "                       '\\'octavia\\': \\'Deprecated alias of the Octavia '",
            "                       'Amphora driver.\\'}'),",
            "                default={'amphora': 'The Octavia Amphora driver.',",
            "                         'octavia': 'Deprecated alias of the Octavia Amphora '",
            "                         'driver.'}),",
            "    cfg.StrOpt('default_provider_driver', default='amphora',",
            "               help=_('Default provider driver.')),",
            "    cfg.IntOpt('udp_connect_min_interval_health_monitor',",
            "               default=3,",
            "               help=_(\"The minimum health monitor delay interval for the \"",
            "                      \"UDP-CONNECT Health Monitor type. A negative integer \"",
            "                      \"value means 'no limit'.\")),",
            "]",
            "",
            "# Options only used by the amphora agent",
            "amphora_agent_opts = [",
            "    cfg.StrOpt('agent_server_ca', default='/etc/octavia/certs/client_ca.pem',",
            "               help=_(\"The ca which signed the client certificates\")),",
            "    cfg.StrOpt('agent_server_cert', default='/etc/octavia/certs/server.pem',",
            "               help=_(\"The server certificate for the agent.py server \"",
            "                      \"to use\")),",
            "    cfg.StrOpt('agent_server_network_dir',",
            "               help=_(\"The directory where new network interfaces \"",
            "                      \"are located\")),",
            "    cfg.StrOpt('agent_server_network_file',",
            "               help=_(\"The file where the network interfaces are located. \"",
            "                      \"Specifying this will override any value set for \"",
            "                      \"agent_server_network_dir.\")),",
            "    cfg.IntOpt('agent_request_read_timeout', default=120,",
            "               help=_(\"The time in seconds to allow a request from the \"",
            "                      \"controller to run before terminating the socket.\")),",
            "    # Do not specify in octavia.conf, loaded at runtime",
            "    cfg.StrOpt('amphora_id', help=_(\"The amphora ID.\")),",
            "    cfg.StrOpt('amphora_udp_driver',",
            "               default='keepalived_lvs',",
            "               help='The UDP API backend for amphora agent.'),",
            "]",
            "",
            "networking_opts = [",
            "    cfg.IntOpt('max_retries', default=15,",
            "               help=_('The maximum attempts to retry an action with the '",
            "                      'networking service.')),",
            "    cfg.IntOpt('retry_interval', default=1,",
            "               help=_('Seconds to wait before retrying an action with the '",
            "                      'networking service.')),",
            "    cfg.IntOpt('port_detach_timeout', default=300,",
            "               help=_('Seconds to wait for a port to detach from an '",
            "                      'amphora.')),",
            "    cfg.BoolOpt('allow_vip_network_id', default=True,",
            "                help=_('Can users supply a network_id for their VIP?')),",
            "    cfg.BoolOpt('allow_vip_subnet_id', default=True,",
            "                help=_('Can users supply a subnet_id for their VIP?')),",
            "    cfg.BoolOpt('allow_vip_port_id', default=True,",
            "                help=_('Can users supply a port_id for their VIP?')),",
            "    cfg.ListOpt('valid_vip_networks',",
            "                help=_('List of network_ids that are valid for VIP '",
            "                       'creation. If this field is empty, no validation '",
            "                       'is performed.')),",
            "    cfg.ListOpt('reserved_ips',",
            "                default=['169.254.169.254'],",
            "                item_type=cfg.types.IPAddress(),",
            "                help=_('List of IP addresses reserved from being used for '",
            "                       'member addresses. IPv6 addresses should be in '",
            "                       'expanded, uppercase form.')),",
            "]",
            "",
            "healthmanager_opts = [",
            "    cfg.IPOpt('bind_ip', default='127.0.0.1',",
            "              help=_('IP address the controller will listen on for '",
            "                     'heart beats')),",
            "    cfg.PortOpt('bind_port', default=5555,",
            "                help=_('Port number the controller will listen on '",
            "                       'for heart beats')),",
            "    cfg.IntOpt('failover_threads',",
            "               default=10,",
            "               help=_('Number of threads performing amphora failovers.')),",
            "    # TODO(tatsuma) Remove in or after \"T\" release",
            "    cfg.IntOpt('status_update_threads',",
            "               default=None,",
            "               help=_('Number of processes for amphora status update.'),",
            "               deprecated_for_removal=True,",
            "               deprecated_reason=_('This option is replaced as '",
            "                                   'health_update_threads and '",
            "                                   'stats_update_threads')),",
            "    cfg.IntOpt('health_update_threads',",
            "               default=None,",
            "               help=_('Number of processes for amphora health update.')),",
            "    cfg.IntOpt('stats_update_threads',",
            "               default=None,",
            "               help=_('Number of processes for amphora stats update.')),",
            "    cfg.StrOpt('heartbeat_key',",
            "               mutable=True,",
            "               help=_('key used to validate amphora sending '",
            "                      'the message'), secret=True),",
            "    cfg.IntOpt('heartbeat_timeout',",
            "               default=60,",
            "               help=_('Interval, in seconds, to wait before failing over an '",
            "                      'amphora.')),",
            "    cfg.IntOpt('health_check_interval',",
            "               default=3,",
            "               help=_('Sleep time between health checks in seconds.')),",
            "    cfg.IntOpt('sock_rlimit', default=0,",
            "               help=_(' sets the value of the heartbeat recv buffer')),",
            "",
            "    # Used by the health manager on the amphora",
            "    cfg.ListOpt('controller_ip_port_list',",
            "                help=_('List of controller ip and port pairs for the '",
            "                       'heartbeat receivers. Example 127.0.0.1:5555, '",
            "                       '192.168.0.1:5555'),",
            "                mutable=True,",
            "                default=[]),",
            "    cfg.IntOpt('heartbeat_interval',",
            "               default=10,",
            "               mutable=True,",
            "               help=_('Sleep time between sending heartbeats.')),",
            "",
            "    # Used for updating health and stats",
            "    cfg.StrOpt('health_update_driver', default='health_db',",
            "               help=_('Driver for updating amphora health system.')),",
            "    cfg.StrOpt('stats_update_driver', default='stats_db',",
            "               help=_('Driver for updating amphora statistics.')),",
            "",
            "    # Used for synchronizing neutron-lbaas and octavia",
            "    cfg.StrOpt('event_streamer_driver',",
            "               help=_('Specifies which driver to use for the event_streamer '",
            "                      'for syncing the octavia and neutron_lbaas dbs. If you '",
            "                      'don\\'t need to sync the database or are running '",
            "                      'octavia in stand alone mode use the '",
            "                      'noop_event_streamer'),",
            "               default='noop_event_streamer'),",
            "    cfg.BoolOpt('sync_provisioning_status', default=False,",
            "                help=_(\"Enable provisioning status sync with neutron db\"))]",
            "",
            "oslo_messaging_opts = [",
            "    cfg.StrOpt('topic'),",
            "    cfg.StrOpt('event_stream_topic',",
            "               default='neutron_lbaas_event',",
            "               help=_('topic name for communicating events through a queue')),",
            "    cfg.StrOpt('event_stream_transport_url', default=None,",
            "               help=_('Transport URL to use for the neutron-lbaas '",
            "                      'synchronization event stream when neutron and octavia '",
            "                      'have separate queues.')),",
            "]",
            "",
            "haproxy_amphora_opts = [",
            "    cfg.StrOpt('base_path',",
            "               default='/var/lib/octavia',",
            "               help=_('Base directory for amphora files.')),",
            "    cfg.StrOpt('base_cert_dir',",
            "               default='/var/lib/octavia/certs',",
            "               help=_('Base directory for cert storage.')),",
            "    cfg.StrOpt('haproxy_template', help=_('Custom haproxy template.')),",
            "    cfg.BoolOpt('connection_logging', default=True,",
            "                help=_('Set this to False to disable connection logging.')),",
            "    cfg.IntOpt('connection_max_retries',",
            "               default=120,",
            "               help=_('Retry threshold for connecting to amphorae.')),",
            "    cfg.IntOpt('connection_retry_interval',",
            "               default=5,",
            "               help=_('Retry timeout between connection attempts in '",
            "                      'seconds.')),",
            "    cfg.IntOpt('active_connection_max_retries',",
            "               default=15,",
            "               help=_('Retry threshold for connecting to active amphorae.')),",
            "    cfg.IntOpt('active_connection_rety_interval',",
            "               default=2,",
            "               help=_('Retry timeout between connection attempts in '",
            "                      'seconds for active amphora.')),",
            "    cfg.IntOpt('build_rate_limit',",
            "               default=-1,",
            "               help=_('Number of amphorae that could be built per controller '",
            "                      'worker, simultaneously.')),",
            "    cfg.IntOpt('build_active_retries',",
            "               default=120,",
            "               help=_('Retry threshold for waiting for a build slot for '",
            "                      'an amphorae.')),",
            "    cfg.IntOpt('build_retry_interval',",
            "               default=5,",
            "               help=_('Retry timeout between build attempts in '",
            "                      'seconds.')),",
            "    cfg.StrOpt('haproxy_stick_size', default='10k',",
            "               help=_('Size of the HAProxy stick table. Accepts k, m, g '",
            "                      'suffixes.  Example: 10k')),",
            "",
            "    # REST server",
            "    cfg.IPOpt('bind_host', default='::',  # nosec",
            "              help=_(\"The host IP to bind to\")),",
            "    cfg.PortOpt('bind_port', default=9443,",
            "                help=_(\"The port to bind to\")),",
            "    cfg.StrOpt('lb_network_interface',",
            "               default='o-hm0',",
            "               help=_('Network interface through which to reach amphora, only '",
            "                      'required if using IPv6 link local addresses.')),",
            "    cfg.StrOpt('haproxy_cmd', default='/usr/sbin/haproxy',",
            "               help=_(\"The full path to haproxy\")),",
            "    cfg.IntOpt('respawn_count', default=2,",
            "               help=_(\"The respawn count for haproxy's upstart script\")),",
            "    cfg.IntOpt('respawn_interval', default=2,",
            "               help=_(\"The respawn interval for haproxy's upstart script\")),",
            "    cfg.FloatOpt('rest_request_conn_timeout', default=10,",
            "                 help=_(\"The time in seconds to wait for a REST API \"",
            "                        \"to connect.\")),",
            "    cfg.FloatOpt('rest_request_read_timeout', default=60,",
            "                 help=_(\"The time in seconds to wait for a REST API \"",
            "                        \"response.\")),",
            "    cfg.IntOpt('timeout_client_data',",
            "               default=constants.DEFAULT_TIMEOUT_CLIENT_DATA,",
            "               help=_('Frontend client inactivity timeout.')),",
            "    cfg.IntOpt('timeout_member_connect',",
            "               default=constants.DEFAULT_TIMEOUT_MEMBER_CONNECT,",
            "               help=_('Backend member connection timeout.')),",
            "    cfg.IntOpt('timeout_member_data',",
            "               default=constants.DEFAULT_TIMEOUT_MEMBER_DATA,",
            "               help=_('Backend member inactivity timeout.')),",
            "    cfg.IntOpt('timeout_tcp_inspect',",
            "               default=constants.DEFAULT_TIMEOUT_TCP_INSPECT,",
            "               help=_('Time to wait for TCP packets for content inspection.')),",
            "    # REST client",
            "    cfg.StrOpt('client_cert', default='/etc/octavia/certs/client.pem',",
            "               help=_(\"The client certificate to talk to the agent\")),",
            "    cfg.StrOpt('server_ca', default='/etc/octavia/certs/server_ca.pem',",
            "               help=_(\"The ca which signed the server certificates\")),",
            "    cfg.BoolOpt('use_upstart', default=True,",
            "                deprecated_for_removal=True,",
            "                deprecated_reason='This is now automatically discovered '",
            "                                  ' and configured.',",
            "                help=_(\"If False, use sysvinit.\")),",
            "]",
            "",
            "controller_worker_opts = [",
            "    cfg.IntOpt('workers',",
            "               default=1, min=1,",
            "               help='Number of workers for the controller-worker service.'),",
            "    cfg.IntOpt('amp_active_retries',",
            "               default=30,",
            "               help=_('Retry attempts to wait for Amphora to become active')),",
            "    cfg.IntOpt('amp_active_wait_sec',",
            "               default=10,",
            "               help=_('Seconds to wait between checks on whether an Amphora '",
            "                      'has become active')),",
            "    cfg.StrOpt('amp_flavor_id',",
            "               default='',",
            "               help=_('Nova instance flavor id for the Amphora')),",
            "    cfg.StrOpt('amp_image_tag',",
            "               default='',",
            "               help=_('Glance image tag for the Amphora image to boot. '",
            "                      'Use this option to be able to update the image '",
            "                      'without reconfiguring Octavia. '",
            "                      'Ignored if amp_image_id is defined.')),",
            "    cfg.StrOpt('amp_image_id',",
            "               default='',",
            "               deprecated_for_removal=True,",
            "               deprecated_reason='Superseded by amp_image_tag option.',",
            "               help=_('Glance image id for the Amphora image to boot')),",
            "    cfg.StrOpt('amp_image_owner_id',",
            "               default='',",
            "               help=_('Restrict glance image selection to a specific '",
            "                      'owner ID.  This is a recommended security setting.')),",
            "    cfg.StrOpt('amp_ssh_key_name',",
            "               default='',",
            "               help=_('Optional SSH keypair name, in nova, that will be used '",
            "                      'for the authorized_keys inside the amphora.')),",
            "    cfg.BoolOpt('amp_ssh_access_allowed',",
            "                default=True,",
            "                deprecated_for_removal=True,",
            "                deprecated_reason='This option and amp_ssh_key_name overlap '",
            "                                  'in functionality, and only one is needed. '",
            "                                  'SSH access can be enabled/disabled simply '",
            "                                  'by setting amp_ssh_key_name, or not.',",
            "                help=_('Determines whether or not to allow access '",
            "                       'to the Amphorae')),",
            "    cfg.ListOpt('amp_boot_network_list',",
            "                default='',",
            "                help=_('List of networks to attach to the Amphorae. '",
            "                       'All networks defined in the list will '",
            "                       'be attached to each amphora.')),",
            "    cfg.ListOpt('amp_secgroup_list',",
            "                default='',",
            "                help=_('List of security groups to attach to the Amphora.')),",
            "    cfg.StrOpt('client_ca',",
            "               default='/etc/octavia/certs/ca_01.pem',",
            "               help=_('Client CA for the amphora agent to use')),",
            "    cfg.StrOpt('amphora_driver',",
            "               default='amphora_noop_driver',",
            "               help=_('Name of the amphora driver to use')),",
            "    cfg.StrOpt('compute_driver',",
            "               default='compute_noop_driver',",
            "               help=_('Name of the compute driver to use')),",
            "    cfg.StrOpt('network_driver',",
            "               default='network_noop_driver',",
            "               help=_('Name of the network driver to use')),",
            "    cfg.StrOpt('distributor_driver',",
            "               default='distributor_noop_driver',",
            "               help=_('Name of the distributor driver to use')),",
            "    cfg.StrOpt('loadbalancer_topology',",
            "               default=constants.TOPOLOGY_SINGLE,",
            "               choices=constants.SUPPORTED_LB_TOPOLOGIES,",
            "               mutable=True,",
            "               help=_('Load balancer topology configuration. '",
            "                      'SINGLE - One amphora per load balancer. '",
            "                      'ACTIVE_STANDBY - Two amphora per load balancer.')),",
            "    cfg.BoolOpt('user_data_config_drive', default=False,",
            "                help=_('If True, build cloud-init user-data that is passed '",
            "                       'to the config drive on Amphora boot instead of '",
            "                       'personality files. If False, utilize personality '",
            "                       'files.'))",
            "]",
            "",
            "task_flow_opts = [",
            "    cfg.StrOpt('engine',",
            "               default='serial',",
            "               help=_('TaskFlow engine to use')),",
            "    cfg.IntOpt('max_workers',",
            "               default=5,",
            "               help=_('The maximum number of workers')),",
            "    cfg.BoolOpt('disable_revert', default=False,",
            "                help=_('If True, disables the controller worker taskflow '",
            "                       'flows from reverting.  This will leave resources in '",
            "                       'an inconsistent state and should only be used for '",
            "                       'debugging purposes.'))",
            "]",
            "",
            "core_cli_opts = []",
            "",
            "certificate_opts = [",
            "    cfg.StrOpt('cert_manager',",
            "               default='barbican_cert_manager',",
            "               help='Name of the cert manager to use'),",
            "    cfg.StrOpt('cert_generator',",
            "               default='local_cert_generator',",
            "               help='Name of the cert generator to use'),",
            "    cfg.StrOpt('barbican_auth',",
            "               default='barbican_acl_auth',",
            "               help='Name of the Barbican authentication method to use'),",
            "    cfg.StrOpt('service_name',",
            "               help=_('The name of the certificate service in the keystone '",
            "                      'catalog')),",
            "    cfg.StrOpt('endpoint', help=_('A new endpoint to override the endpoint '",
            "                                  'in the keystone catalog.')),",
            "    cfg.StrOpt('region_name',",
            "               help='Region in Identity service catalog to use for '",
            "                    'communication with the barbican service.'),",
            "    cfg.StrOpt('endpoint_type',",
            "               default='publicURL',",
            "               help='The endpoint_type to be used for barbican service.'),",
            "    cfg.StrOpt('ca_certificates_file',",
            "               help=_('CA certificates file path')),",
            "    cfg.BoolOpt('insecure',",
            "                default=False,",
            "                help=_('Disable certificate validation on SSL connections ')),",
            "]",
            "",
            "house_keeping_opts = [",
            "    cfg.IntOpt('spare_check_interval',",
            "               default=30,",
            "               help=_('Spare check interval in seconds')),",
            "    cfg.IntOpt('spare_amphora_pool_size',",
            "               default=0,",
            "               help=_('Number of spare amphorae')),",
            "    cfg.IntOpt('cleanup_interval',",
            "               default=30,",
            "               help=_('DB cleanup interval in seconds')),",
            "    cfg.IntOpt('amphora_expiry_age',",
            "               default=604800,",
            "               help=_('Amphora expiry age in seconds')),",
            "    cfg.IntOpt('load_balancer_expiry_age',",
            "               default=604800,",
            "               help=_('Load balancer expiry age in seconds')),",
            "    cfg.IntOpt('cert_interval',",
            "               default=3600,",
            "               help=_('Certificate check interval in seconds')),",
            "    # 14 days for cert expiry buffer",
            "    cfg.IntOpt('cert_expiry_buffer',",
            "               default=1209600,",
            "               help=_('Seconds until certificate expiration')),",
            "    cfg.IntOpt('cert_rotate_threads',",
            "               default=10,",
            "               help=_('Number of threads performing amphora certificate'",
            "                      ' rotation'))",
            "]",
            "",
            "anchor_opts = [",
            "    cfg.StrOpt('url',",
            "               default='http://localhost:9999/v1/sign/default',",
            "               help=_('Anchor URL')),",
            "    cfg.StrOpt('username',",
            "               help=_('Anchor username')),",
            "    cfg.StrOpt('password',",
            "               help=_('Anchor password'),",
            "               secret=True)",
            "]",
            "",
            "keepalived_vrrp_opts = [",
            "    cfg.IntOpt('vrrp_advert_int',",
            "               default=1,",
            "               help=_('Amphora role and priority advertisement interval '",
            "                      'in seconds.')),",
            "    cfg.IntOpt('vrrp_check_interval',",
            "               default=5,",
            "               help=_('VRRP health check script run interval in seconds.')),",
            "    cfg.IntOpt('vrrp_fail_count',",
            "               default=2,",
            "               help=_('Number of successive failures before transition to a '",
            "                      'fail state.')),",
            "    cfg.IntOpt('vrrp_success_count',",
            "               default=2,",
            "               help=_('Number of consecutive successes before transition to a '",
            "                      'success state.')),",
            "    cfg.IntOpt('vrrp_garp_refresh_interval',",
            "               default=5,",
            "               help=_('Time in seconds between gratuitous ARP announcements '",
            "                      'from the MASTER.')),",
            "    cfg.IntOpt('vrrp_garp_refresh_count',",
            "               default=2,",
            "               help=_('Number of gratuitous ARP announcements to make on '",
            "                      'each refresh interval.'))",
            "",
            "]",
            "",
            "nova_opts = [",
            "    cfg.StrOpt('service_name',",
            "               help=_('The name of the nova service in the keystone catalog')),",
            "    cfg.StrOpt('endpoint', help=_('A new endpoint to override the endpoint '",
            "                                  'in the keystone catalog.')),",
            "    cfg.StrOpt('region_name',",
            "               help=_('Region in Identity service catalog to use for '",
            "                      'communication with the OpenStack services.')),",
            "    cfg.StrOpt('endpoint_type', default='publicURL',",
            "               help=_('Endpoint interface in identity service to use')),",
            "    cfg.StrOpt('ca_certificates_file',",
            "               help=_('CA certificates file path')),",
            "    cfg.BoolOpt('insecure',",
            "                default=False,",
            "                help=_('Disable certificate validation on SSL connections')),",
            "    cfg.BoolOpt('enable_anti_affinity', default=False,",
            "                help=_('Flag to indicate if nova anti-affinity feature is '",
            "                       'turned on.')),",
            "    cfg.StrOpt('anti_affinity_policy', default=constants.ANTI_AFFINITY,",
            "               choices=[constants.ANTI_AFFINITY, constants.SOFT_ANTI_AFFINITY],",
            "               help=_('Sets the anti-affinity policy for nova')),",
            "    cfg.IntOpt('random_amphora_name_length', default=0,",
            "               help=_('If non-zero, generate a random name of the length '",
            "                      'provided for each amphora, in the format \"a[A-Z0-9]*\". '",
            "                      'Otherwise, the default name format will be used: '",
            "                      '\"amphora-{UUID}\".')),",
            "    cfg.StrOpt('availability_zone', default=None,",
            "               help=_('Availability zone to use for creating Amphorae')),",
            "]",
            "neutron_opts = [",
            "    cfg.StrOpt('service_name',",
            "               help=_('The name of the neutron service in the '",
            "                      'keystone catalog')),",
            "    cfg.StrOpt('endpoint', help=_('A new endpoint to override the endpoint '",
            "                                  'in the keystone catalog.')),",
            "    cfg.StrOpt('region_name',",
            "               help=_('Region in Identity service catalog to use for '",
            "                      'communication with the OpenStack services.')),",
            "    cfg.StrOpt('endpoint_type', default='publicURL',",
            "               help=_('Endpoint interface in identity service to use')),",
            "    cfg.StrOpt('ca_certificates_file',",
            "               help=_('CA certificates file path')),",
            "    cfg.BoolOpt('insecure',",
            "                default=False,",
            "                help=_('Disable certificate validation on SSL connections ')),",
            "]",
            "",
            "glance_opts = [",
            "    cfg.StrOpt('service_name',",
            "               help=_('The name of the glance service in the '",
            "                      'keystone catalog')),",
            "    cfg.StrOpt('endpoint', help=_('A new endpoint to override the endpoint '",
            "                                  'in the keystone catalog.')),",
            "    cfg.StrOpt('region_name',",
            "               help=_('Region in Identity service catalog to use for '",
            "                      'communication with the OpenStack services.')),",
            "    cfg.StrOpt('endpoint_type', default='publicURL',",
            "               help=_('Endpoint interface in identity service to use')),",
            "    cfg.StrOpt('ca_certificates_file',",
            "               help=_('CA certificates file path')),",
            "    cfg.BoolOpt('insecure',",
            "                default=False,",
            "                help=_('Disable certificate validation on SSL connections ')),",
            "]",
            "",
            "quota_opts = [",
            "    cfg.IntOpt('default_load_balancer_quota',",
            "               default=constants.QUOTA_UNLIMITED,",
            "               help=_('Default per project load balancer quota.')),",
            "    cfg.IntOpt('default_listener_quota',",
            "               default=constants.QUOTA_UNLIMITED,",
            "               help=_('Default per project listener quota.')),",
            "    cfg.IntOpt('default_member_quota',",
            "               default=constants.QUOTA_UNLIMITED,",
            "               help=_('Default per project member quota.')),",
            "    cfg.IntOpt('default_pool_quota',",
            "               default=constants.QUOTA_UNLIMITED,",
            "               help=_('Default per project pool quota.')),",
            "    cfg.IntOpt('default_health_monitor_quota',",
            "               default=constants.QUOTA_UNLIMITED,",
            "               help=_('Default per project health monitor quota.')),",
            "]",
            "",
            "audit_opts = [",
            "    cfg.BoolOpt('enabled', default=False,",
            "                help=_('Enable auditing of API requests')),",
            "    cfg.StrOpt('audit_map_file',",
            "               default='/etc/octavia/octavia_api_audit_map.conf',",
            "               help=_('Path to audit map file for octavia-api service. '",
            "                      'Used only when API audit is enabled.')),",
            "    cfg.StrOpt('ignore_req_list', default='',",
            "               help=_('Comma separated list of REST API HTTP methods to be '",
            "                      'ignored during audit. For example: auditing will not '",
            "                      'be done on any GET or POST requests if this is set to '",
            "                      '\"GET,POST\". It is used only when API audit is '",
            "                      'enabled.')),",
            "]",
            "",
            "# Register the configuration options",
            "cfg.CONF.register_opts(core_opts)",
            "cfg.CONF.register_opts(api_opts, group='api_settings')",
            "cfg.CONF.register_opts(amphora_agent_opts, group='amphora_agent')",
            "cfg.CONF.register_opts(networking_opts, group='networking')",
            "cfg.CONF.register_opts(oslo_messaging_opts, group='oslo_messaging')",
            "cfg.CONF.register_opts(haproxy_amphora_opts, group='haproxy_amphora')",
            "cfg.CONF.register_opts(controller_worker_opts, group='controller_worker')",
            "cfg.CONF.register_opts(keepalived_vrrp_opts, group='keepalived_vrrp')",
            "cfg.CONF.register_opts(task_flow_opts, group='task_flow')",
            "cfg.CONF.register_opts(house_keeping_opts, group='house_keeping')",
            "cfg.CONF.register_opts(anchor_opts, group='anchor')",
            "cfg.CONF.register_cli_opts(core_cli_opts)",
            "cfg.CONF.register_opts(certificate_opts, group='certificates')",
            "cfg.CONF.register_cli_opts(healthmanager_opts, group='health_manager')",
            "cfg.CONF.register_opts(nova_opts, group='nova')",
            "cfg.CONF.register_opts(glance_opts, group='glance')",
            "cfg.CONF.register_opts(neutron_opts, group='neutron')",
            "cfg.CONF.register_opts(quota_opts, group='quotas')",
            "cfg.CONF.register_opts(audit_opts, group='audit')",
            "",
            "# Ensure that the control exchange is set correctly",
            "messaging.set_transport_defaults(control_exchange='octavia')",
            "_SQL_CONNECTION_DEFAULT = 'sqlite://'",
            "# Update the default QueuePool parameters. These can be tweaked by the",
            "# configuration variables - max_pool_size, max_overflow and pool_timeout",
            "db_options.set_defaults(cfg.CONF, connection=_SQL_CONNECTION_DEFAULT,",
            "                        max_pool_size=10, max_overflow=20, pool_timeout=10)",
            "",
            "logging.register_options(cfg.CONF)",
            "",
            "ks_loading.register_auth_conf_options(cfg.CONF, constants.SERVICE_AUTH)",
            "ks_loading.register_session_conf_options(cfg.CONF, constants.SERVICE_AUTH)",
            "",
            "",
            "def init(args, **kwargs):",
            "    cfg.CONF(args=args, project='octavia',",
            "             version='%%prog %s' % version.version_info.release_string(),",
            "             **kwargs)",
            "    handle_deprecation_compatibility()",
            "    setup_remote_pydev_debug()",
            "",
            "",
            "def setup_logging(conf):",
            "    \"\"\"Sets up the logging options for a log with supplied name.",
            "",
            "    :param conf: a cfg.ConfOpts object",
            "    \"\"\"",
            "    product_name = \"octavia\"",
            "    logging.setup(conf, product_name)",
            "    LOG.info(\"Logging enabled!\")",
            "    LOG.info(\"%(prog)s version %(version)s\",",
            "             {'prog': sys.argv[0],",
            "              'version': version.version_info.release_string()})",
            "    LOG.debug(\"command line: %s\", \" \".join(sys.argv))",
            "",
            "",
            "# Use cfg.CONF.set_default to override the new configuration setting",
            "# default value.  This allows a value set, at the new location, to override",
            "# a value set in the previous location while allowing settings that have",
            "# not yet been moved to be utilized.",
            "def handle_deprecation_compatibility():",
            "    # TODO(tatsuma) Remove in or after \"T\" release",
            "    if cfg.CONF.health_manager.status_update_threads is not None:",
            "        cfg.CONF.set_default('health_update_threads',",
            "                             cfg.CONF.health_manager.status_update_threads,",
            "                             group='health_manager')",
            "        cfg.CONF.set_default('stats_update_threads',",
            "                             cfg.CONF.health_manager.status_update_threads,",
            "                             group='health_manager')",
            "",
            "",
            "def setup_remote_pydev_debug():",
            "    \"\"\"Required setup for remote debugging.\"\"\"",
            "",
            "    pydev_debug_host = os.environ.get('PYDEV_DEBUG_HOST')",
            "    pydev_debug_port = os.environ.get('PYDEV_DEBUG_PORT')",
            "",
            "    if not pydev_debug_host or not pydev_debug_port:",
            "        return",
            "",
            "    try:",
            "        try:",
            "            from pydev import pydevd",
            "        except ImportError:",
            "            import pydevd",
            "",
            "        LOG.warning(\"Connecting to remote debugger. Once connected, resume \"",
            "                    \"the program on the debugger to continue with the \"",
            "                    \"initialization of the service.\")",
            "        pydevd.settrace(pydev_debug_host,",
            "                        port=int(pydev_debug_port),",
            "                        stdoutToServer=True,",
            "                        stderrToServer=True)",
            "    except Exception:",
            "        LOG.exception('Unable to join debugger, please make sure that the '",
            "                      'debugger processes is listening on debug-host '",
            "                      '\\'%(debug-host)s\\' debug-port \\'%(debug-port)s\\'.',",
            "                      {'debug-host': pydev_debug_host,",
            "                       'debug-port': pydev_debug_port})",
            "        raise"
        ],
        "afterPatchFile": [
            "# Copyright 2011 VMware, Inc., 2014 A10 Networks",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "\"\"\"",
            "Routines for configuring Octavia",
            "\"\"\"",
            "",
            "import os",
            "import sys",
            "",
            "from keystoneauth1 import loading as ks_loading",
            "from oslo_config import cfg",
            "from oslo_db import options as db_options",
            "from oslo_log import log as logging",
            "import oslo_messaging as messaging",
            "",
            "from octavia.certificates.common import local",
            "from octavia.common import constants",
            "from octavia.common import utils",
            "from octavia.i18n import _",
            "from octavia import version",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "core_opts = [",
            "    cfg.HostnameOpt('host', default=utils.get_hostname(),",
            "                    help=_(\"The hostname Octavia is running on\")),",
            "    cfg.StrOpt('octavia_plugins', default='hot_plug_plugin',",
            "               help=_(\"Name of the controller plugin to use\")),",
            "]",
            "",
            "api_opts = [",
            "    cfg.IPOpt('bind_host', default='127.0.0.1',",
            "              help=_(\"The host IP to bind to\")),",
            "    cfg.PortOpt('bind_port', default=9876,",
            "                help=_(\"The port to bind to\")),",
            "    cfg.StrOpt('auth_strategy', default=constants.KEYSTONE,",
            "               choices=[constants.NOAUTH,",
            "                        constants.KEYSTONE,",
            "                        constants.TESTING],",
            "               help=_(\"The auth strategy for API requests.\")),",
            "    cfg.StrOpt('api_handler', default='queue_producer',",
            "               help=_(\"The handler that the API communicates with\")),",
            "    cfg.BoolOpt('allow_pagination', default=True,",
            "                help=_(\"Allow the usage of pagination\")),",
            "    cfg.BoolOpt('allow_sorting', default=True,",
            "                help=_(\"Allow the usage of sorting\")),",
            "    cfg.BoolOpt('allow_filtering', default=True,",
            "                help=_(\"Allow the usage of filtering\")),",
            "    cfg.BoolOpt('allow_field_selection', default=True,",
            "                help=_(\"Allow the usage of field selection\")),",
            "    cfg.StrOpt('pagination_max_limit',",
            "               default=str(constants.DEFAULT_PAGE_SIZE),",
            "               help=_(\"The maximum number of items returned in a single \"",
            "                      \"response. The string 'infinite' or a negative \"",
            "                      \"integer value means 'no limit'\")),",
            "    cfg.StrOpt('api_base_uri',",
            "               help=_(\"Base URI for the API for use in pagination links. \"",
            "                      \"This will be autodetected from the request if not \"",
            "                      \"overridden here.\")),",
            "    cfg.BoolOpt('api_v1_enabled', default=True,",
            "                help=_(\"Expose the v1 API?\")),",
            "    cfg.BoolOpt('api_v2_enabled', default=True,",
            "                help=_(\"Expose the v2 API?\")),",
            "    cfg.BoolOpt('allow_tls_terminated_listeners', default=True,",
            "                help=_(\"Allow users to create TLS Terminated listeners?\")),",
            "    cfg.BoolOpt('allow_ping_health_monitors', default=True,",
            "                help=_(\"Allow users to create PING type Health Monitors?\")),",
            "    cfg.DictOpt('enabled_provider_drivers',",
            "                help=_('List of enabled provider drivers and description '",
            "                       'dictionaries. Must match the driver name in the '",
            "                       'octavia.api.drivers entrypoint. Example: '",
            "                       '{\\'amphora\\': \\'The Octavia Amphora driver.\\', '",
            "                       '\\'octavia\\': \\'Deprecated alias of the Octavia '",
            "                       'Amphora driver.\\'}'),",
            "                default={'amphora': 'The Octavia Amphora driver.',",
            "                         'octavia': 'Deprecated alias of the Octavia Amphora '",
            "                         'driver.'}),",
            "    cfg.StrOpt('default_provider_driver', default='amphora',",
            "               help=_('Default provider driver.')),",
            "    cfg.IntOpt('udp_connect_min_interval_health_monitor',",
            "               default=3,",
            "               help=_(\"The minimum health monitor delay interval for the \"",
            "                      \"UDP-CONNECT Health Monitor type. A negative integer \"",
            "                      \"value means 'no limit'.\")),",
            "]",
            "",
            "# Options only used by the amphora agent",
            "amphora_agent_opts = [",
            "    cfg.StrOpt('agent_server_ca', default='/etc/octavia/certs/client_ca.pem',",
            "               help=_(\"The ca which signed the client certificates\")),",
            "    cfg.StrOpt('agent_server_cert', default='/etc/octavia/certs/server.pem',",
            "               help=_(\"The server certificate for the agent.py server \"",
            "                      \"to use\")),",
            "    cfg.StrOpt('agent_server_network_dir',",
            "               help=_(\"The directory where new network interfaces \"",
            "                      \"are located\")),",
            "    cfg.StrOpt('agent_server_network_file',",
            "               help=_(\"The file where the network interfaces are located. \"",
            "                      \"Specifying this will override any value set for \"",
            "                      \"agent_server_network_dir.\")),",
            "    cfg.IntOpt('agent_request_read_timeout', default=120,",
            "               help=_(\"The time in seconds to allow a request from the \"",
            "                      \"controller to run before terminating the socket.\")),",
            "    # Do not specify in octavia.conf, loaded at runtime",
            "    cfg.StrOpt('amphora_id', help=_(\"The amphora ID.\")),",
            "    cfg.StrOpt('amphora_udp_driver',",
            "               default='keepalived_lvs',",
            "               help='The UDP API backend for amphora agent.'),",
            "]",
            "",
            "networking_opts = [",
            "    cfg.IntOpt('max_retries', default=15,",
            "               help=_('The maximum attempts to retry an action with the '",
            "                      'networking service.')),",
            "    cfg.IntOpt('retry_interval', default=1,",
            "               help=_('Seconds to wait before retrying an action with the '",
            "                      'networking service.')),",
            "    cfg.IntOpt('port_detach_timeout', default=300,",
            "               help=_('Seconds to wait for a port to detach from an '",
            "                      'amphora.')),",
            "    cfg.BoolOpt('allow_vip_network_id', default=True,",
            "                help=_('Can users supply a network_id for their VIP?')),",
            "    cfg.BoolOpt('allow_vip_subnet_id', default=True,",
            "                help=_('Can users supply a subnet_id for their VIP?')),",
            "    cfg.BoolOpt('allow_vip_port_id', default=True,",
            "                help=_('Can users supply a port_id for their VIP?')),",
            "    cfg.ListOpt('valid_vip_networks',",
            "                help=_('List of network_ids that are valid for VIP '",
            "                       'creation. If this field is empty, no validation '",
            "                       'is performed.')),",
            "    cfg.ListOpt('reserved_ips',",
            "                default=['169.254.169.254'],",
            "                item_type=cfg.types.IPAddress(),",
            "                help=_('List of IP addresses reserved from being used for '",
            "                       'member addresses. IPv6 addresses should be in '",
            "                       'expanded, uppercase form.')),",
            "]",
            "",
            "healthmanager_opts = [",
            "    cfg.IPOpt('bind_ip', default='127.0.0.1',",
            "              help=_('IP address the controller will listen on for '",
            "                     'heart beats')),",
            "    cfg.PortOpt('bind_port', default=5555,",
            "                help=_('Port number the controller will listen on '",
            "                       'for heart beats')),",
            "    cfg.IntOpt('failover_threads',",
            "               default=10,",
            "               help=_('Number of threads performing amphora failovers.')),",
            "    # TODO(tatsuma) Remove in or after \"T\" release",
            "    cfg.IntOpt('status_update_threads',",
            "               default=None,",
            "               help=_('Number of processes for amphora status update.'),",
            "               deprecated_for_removal=True,",
            "               deprecated_reason=_('This option is replaced as '",
            "                                   'health_update_threads and '",
            "                                   'stats_update_threads')),",
            "    cfg.IntOpt('health_update_threads',",
            "               default=None,",
            "               help=_('Number of processes for amphora health update.')),",
            "    cfg.IntOpt('stats_update_threads',",
            "               default=None,",
            "               help=_('Number of processes for amphora stats update.')),",
            "    cfg.StrOpt('heartbeat_key',",
            "               mutable=True,",
            "               help=_('key used to validate amphora sending '",
            "                      'the message'), secret=True),",
            "    cfg.IntOpt('heartbeat_timeout',",
            "               default=60,",
            "               help=_('Interval, in seconds, to wait before failing over an '",
            "                      'amphora.')),",
            "    cfg.IntOpt('health_check_interval',",
            "               default=3,",
            "               help=_('Sleep time between health checks in seconds.')),",
            "    cfg.IntOpt('sock_rlimit', default=0,",
            "               help=_(' sets the value of the heartbeat recv buffer')),",
            "",
            "    # Used by the health manager on the amphora",
            "    cfg.ListOpt('controller_ip_port_list',",
            "                help=_('List of controller ip and port pairs for the '",
            "                       'heartbeat receivers. Example 127.0.0.1:5555, '",
            "                       '192.168.0.1:5555'),",
            "                mutable=True,",
            "                default=[]),",
            "    cfg.IntOpt('heartbeat_interval',",
            "               default=10,",
            "               mutable=True,",
            "               help=_('Sleep time between sending heartbeats.')),",
            "",
            "    # Used for updating health and stats",
            "    cfg.StrOpt('health_update_driver', default='health_db',",
            "               help=_('Driver for updating amphora health system.')),",
            "    cfg.StrOpt('stats_update_driver', default='stats_db',",
            "               help=_('Driver for updating amphora statistics.')),",
            "",
            "    # Used for synchronizing neutron-lbaas and octavia",
            "    cfg.StrOpt('event_streamer_driver',",
            "               help=_('Specifies which driver to use for the event_streamer '",
            "                      'for syncing the octavia and neutron_lbaas dbs. If you '",
            "                      'don\\'t need to sync the database or are running '",
            "                      'octavia in stand alone mode use the '",
            "                      'noop_event_streamer'),",
            "               default='noop_event_streamer'),",
            "    cfg.BoolOpt('sync_provisioning_status', default=False,",
            "                help=_(\"Enable provisioning status sync with neutron db\"))]",
            "",
            "oslo_messaging_opts = [",
            "    cfg.StrOpt('topic'),",
            "    cfg.StrOpt('event_stream_topic',",
            "               default='neutron_lbaas_event',",
            "               help=_('topic name for communicating events through a queue')),",
            "    cfg.StrOpt('event_stream_transport_url', default=None,",
            "               help=_('Transport URL to use for the neutron-lbaas '",
            "                      'synchronization event stream when neutron and octavia '",
            "                      'have separate queues.')),",
            "]",
            "",
            "haproxy_amphora_opts = [",
            "    cfg.StrOpt('base_path',",
            "               default='/var/lib/octavia',",
            "               help=_('Base directory for amphora files.')),",
            "    cfg.StrOpt('base_cert_dir',",
            "               default='/var/lib/octavia/certs',",
            "               help=_('Base directory for cert storage.')),",
            "    cfg.StrOpt('haproxy_template', help=_('Custom haproxy template.')),",
            "    cfg.BoolOpt('connection_logging', default=True,",
            "                help=_('Set this to False to disable connection logging.')),",
            "    cfg.IntOpt('connection_max_retries',",
            "               default=120,",
            "               help=_('Retry threshold for connecting to amphorae.')),",
            "    cfg.IntOpt('connection_retry_interval',",
            "               default=5,",
            "               help=_('Retry timeout between connection attempts in '",
            "                      'seconds.')),",
            "    cfg.IntOpt('active_connection_max_retries',",
            "               default=15,",
            "               help=_('Retry threshold for connecting to active amphorae.')),",
            "    cfg.IntOpt('active_connection_rety_interval',",
            "               default=2,",
            "               help=_('Retry timeout between connection attempts in '",
            "                      'seconds for active amphora.')),",
            "    cfg.IntOpt('build_rate_limit',",
            "               default=-1,",
            "               help=_('Number of amphorae that could be built per controller '",
            "                      'worker, simultaneously.')),",
            "    cfg.IntOpt('build_active_retries',",
            "               default=120,",
            "               help=_('Retry threshold for waiting for a build slot for '",
            "                      'an amphorae.')),",
            "    cfg.IntOpt('build_retry_interval',",
            "               default=5,",
            "               help=_('Retry timeout between build attempts in '",
            "                      'seconds.')),",
            "    cfg.StrOpt('haproxy_stick_size', default='10k',",
            "               help=_('Size of the HAProxy stick table. Accepts k, m, g '",
            "                      'suffixes.  Example: 10k')),",
            "",
            "    # REST server",
            "    cfg.IPOpt('bind_host', default='::',  # nosec",
            "              help=_(\"The host IP to bind to\")),",
            "    cfg.PortOpt('bind_port', default=9443,",
            "                help=_(\"The port to bind to\")),",
            "    cfg.StrOpt('lb_network_interface',",
            "               default='o-hm0',",
            "               help=_('Network interface through which to reach amphora, only '",
            "                      'required if using IPv6 link local addresses.')),",
            "    cfg.StrOpt('haproxy_cmd', default='/usr/sbin/haproxy',",
            "               help=_(\"The full path to haproxy\")),",
            "    cfg.IntOpt('respawn_count', default=2,",
            "               help=_(\"The respawn count for haproxy's upstart script\")),",
            "    cfg.IntOpt('respawn_interval', default=2,",
            "               help=_(\"The respawn interval for haproxy's upstart script\")),",
            "    cfg.FloatOpt('rest_request_conn_timeout', default=10,",
            "                 help=_(\"The time in seconds to wait for a REST API \"",
            "                        \"to connect.\")),",
            "    cfg.FloatOpt('rest_request_read_timeout', default=60,",
            "                 help=_(\"The time in seconds to wait for a REST API \"",
            "                        \"response.\")),",
            "    cfg.IntOpt('timeout_client_data',",
            "               default=constants.DEFAULT_TIMEOUT_CLIENT_DATA,",
            "               help=_('Frontend client inactivity timeout.')),",
            "    cfg.IntOpt('timeout_member_connect',",
            "               default=constants.DEFAULT_TIMEOUT_MEMBER_CONNECT,",
            "               help=_('Backend member connection timeout.')),",
            "    cfg.IntOpt('timeout_member_data',",
            "               default=constants.DEFAULT_TIMEOUT_MEMBER_DATA,",
            "               help=_('Backend member inactivity timeout.')),",
            "    cfg.IntOpt('timeout_tcp_inspect',",
            "               default=constants.DEFAULT_TIMEOUT_TCP_INSPECT,",
            "               help=_('Time to wait for TCP packets for content inspection.')),",
            "    # REST client",
            "    cfg.StrOpt('client_cert', default='/etc/octavia/certs/client.pem',",
            "               help=_(\"The client certificate to talk to the agent\")),",
            "    cfg.StrOpt('server_ca', default='/etc/octavia/certs/server_ca.pem',",
            "               help=_(\"The ca which signed the server certificates\")),",
            "    cfg.BoolOpt('use_upstart', default=True,",
            "                deprecated_for_removal=True,",
            "                deprecated_reason='This is now automatically discovered '",
            "                                  ' and configured.',",
            "                help=_(\"If False, use sysvinit.\")),",
            "]",
            "",
            "controller_worker_opts = [",
            "    cfg.IntOpt('workers',",
            "               default=1, min=1,",
            "               help='Number of workers for the controller-worker service.'),",
            "    cfg.IntOpt('amp_active_retries',",
            "               default=30,",
            "               help=_('Retry attempts to wait for Amphora to become active')),",
            "    cfg.IntOpt('amp_active_wait_sec',",
            "               default=10,",
            "               help=_('Seconds to wait between checks on whether an Amphora '",
            "                      'has become active')),",
            "    cfg.StrOpt('amp_flavor_id',",
            "               default='',",
            "               help=_('Nova instance flavor id for the Amphora')),",
            "    cfg.StrOpt('amp_image_tag',",
            "               default='',",
            "               help=_('Glance image tag for the Amphora image to boot. '",
            "                      'Use this option to be able to update the image '",
            "                      'without reconfiguring Octavia. '",
            "                      'Ignored if amp_image_id is defined.')),",
            "    cfg.StrOpt('amp_image_id',",
            "               default='',",
            "               deprecated_for_removal=True,",
            "               deprecated_reason='Superseded by amp_image_tag option.',",
            "               help=_('Glance image id for the Amphora image to boot')),",
            "    cfg.StrOpt('amp_image_owner_id',",
            "               default='',",
            "               help=_('Restrict glance image selection to a specific '",
            "                      'owner ID.  This is a recommended security setting.')),",
            "    cfg.StrOpt('amp_ssh_key_name',",
            "               default='',",
            "               help=_('Optional SSH keypair name, in nova, that will be used '",
            "                      'for the authorized_keys inside the amphora.')),",
            "    cfg.BoolOpt('amp_ssh_access_allowed',",
            "                default=True,",
            "                deprecated_for_removal=True,",
            "                deprecated_reason='This option and amp_ssh_key_name overlap '",
            "                                  'in functionality, and only one is needed. '",
            "                                  'SSH access can be enabled/disabled simply '",
            "                                  'by setting amp_ssh_key_name, or not.',",
            "                help=_('Determines whether or not to allow access '",
            "                       'to the Amphorae')),",
            "    cfg.ListOpt('amp_boot_network_list',",
            "                default='',",
            "                help=_('List of networks to attach to the Amphorae. '",
            "                       'All networks defined in the list will '",
            "                       'be attached to each amphora.')),",
            "    cfg.ListOpt('amp_secgroup_list',",
            "                default='',",
            "                help=_('List of security groups to attach to the Amphora.')),",
            "    cfg.StrOpt('client_ca',",
            "               default='/etc/octavia/certs/ca_01.pem',",
            "               help=_('Client CA for the amphora agent to use')),",
            "    cfg.StrOpt('amphora_driver',",
            "               default='amphora_noop_driver',",
            "               help=_('Name of the amphora driver to use')),",
            "    cfg.StrOpt('compute_driver',",
            "               default='compute_noop_driver',",
            "               help=_('Name of the compute driver to use')),",
            "    cfg.StrOpt('network_driver',",
            "               default='network_noop_driver',",
            "               help=_('Name of the network driver to use')),",
            "    cfg.StrOpt('distributor_driver',",
            "               default='distributor_noop_driver',",
            "               help=_('Name of the distributor driver to use')),",
            "    cfg.StrOpt('loadbalancer_topology',",
            "               default=constants.TOPOLOGY_SINGLE,",
            "               choices=constants.SUPPORTED_LB_TOPOLOGIES,",
            "               mutable=True,",
            "               help=_('Load balancer topology configuration. '",
            "                      'SINGLE - One amphora per load balancer. '",
            "                      'ACTIVE_STANDBY - Two amphora per load balancer.')),",
            "    cfg.BoolOpt('user_data_config_drive', default=False,",
            "                help=_('If True, build cloud-init user-data that is passed '",
            "                       'to the config drive on Amphora boot instead of '",
            "                       'personality files. If False, utilize personality '",
            "                       'files.'))",
            "]",
            "",
            "task_flow_opts = [",
            "    cfg.StrOpt('engine',",
            "               default='serial',",
            "               help=_('TaskFlow engine to use')),",
            "    cfg.IntOpt('max_workers',",
            "               default=5,",
            "               help=_('The maximum number of workers')),",
            "    cfg.BoolOpt('disable_revert', default=False,",
            "                help=_('If True, disables the controller worker taskflow '",
            "                       'flows from reverting.  This will leave resources in '",
            "                       'an inconsistent state and should only be used for '",
            "                       'debugging purposes.'))",
            "]",
            "",
            "core_cli_opts = []",
            "",
            "certificate_opts = [",
            "    cfg.StrOpt('cert_manager',",
            "               default='barbican_cert_manager',",
            "               help='Name of the cert manager to use'),",
            "    cfg.StrOpt('cert_generator',",
            "               default='local_cert_generator',",
            "               help='Name of the cert generator to use'),",
            "    cfg.StrOpt('barbican_auth',",
            "               default='barbican_acl_auth',",
            "               help='Name of the Barbican authentication method to use'),",
            "    cfg.StrOpt('service_name',",
            "               help=_('The name of the certificate service in the keystone '",
            "                      'catalog')),",
            "    cfg.StrOpt('endpoint', help=_('A new endpoint to override the endpoint '",
            "                                  'in the keystone catalog.')),",
            "    cfg.StrOpt('region_name',",
            "               help='Region in Identity service catalog to use for '",
            "                    'communication with the barbican service.'),",
            "    cfg.StrOpt('endpoint_type',",
            "               default='publicURL',",
            "               help='The endpoint_type to be used for barbican service.'),",
            "    cfg.StrOpt('ca_certificates_file',",
            "               help=_('CA certificates file path')),",
            "    cfg.BoolOpt('insecure',",
            "                default=False,",
            "                help=_('Disable certificate validation on SSL connections ')),",
            "]",
            "",
            "house_keeping_opts = [",
            "    cfg.IntOpt('spare_check_interval',",
            "               default=30,",
            "               help=_('Spare check interval in seconds')),",
            "    cfg.IntOpt('spare_amphora_pool_size',",
            "               default=0,",
            "               help=_('Number of spare amphorae')),",
            "    cfg.IntOpt('cleanup_interval',",
            "               default=30,",
            "               help=_('DB cleanup interval in seconds')),",
            "    cfg.IntOpt('amphora_expiry_age',",
            "               default=604800,",
            "               help=_('Amphora expiry age in seconds')),",
            "    cfg.IntOpt('load_balancer_expiry_age',",
            "               default=604800,",
            "               help=_('Load balancer expiry age in seconds')),",
            "    cfg.IntOpt('cert_interval',",
            "               default=3600,",
            "               help=_('Certificate check interval in seconds')),",
            "    # 14 days for cert expiry buffer",
            "    cfg.IntOpt('cert_expiry_buffer',",
            "               default=1209600,",
            "               help=_('Seconds until certificate expiration')),",
            "    cfg.IntOpt('cert_rotate_threads',",
            "               default=10,",
            "               help=_('Number of threads performing amphora certificate'",
            "                      ' rotation'))",
            "]",
            "",
            "anchor_opts = [",
            "    cfg.StrOpt('url',",
            "               default='http://localhost:9999/v1/sign/default',",
            "               help=_('Anchor URL')),",
            "    cfg.StrOpt('username',",
            "               help=_('Anchor username')),",
            "    cfg.StrOpt('password',",
            "               help=_('Anchor password'),",
            "               secret=True)",
            "]",
            "",
            "keepalived_vrrp_opts = [",
            "    cfg.IntOpt('vrrp_advert_int',",
            "               default=1,",
            "               help=_('Amphora role and priority advertisement interval '",
            "                      'in seconds.')),",
            "    cfg.IntOpt('vrrp_check_interval',",
            "               default=5,",
            "               help=_('VRRP health check script run interval in seconds.')),",
            "    cfg.IntOpt('vrrp_fail_count',",
            "               default=2,",
            "               help=_('Number of successive failures before transition to a '",
            "                      'fail state.')),",
            "    cfg.IntOpt('vrrp_success_count',",
            "               default=2,",
            "               help=_('Number of consecutive successes before transition to a '",
            "                      'success state.')),",
            "    cfg.IntOpt('vrrp_garp_refresh_interval',",
            "               default=5,",
            "               help=_('Time in seconds between gratuitous ARP announcements '",
            "                      'from the MASTER.')),",
            "    cfg.IntOpt('vrrp_garp_refresh_count',",
            "               default=2,",
            "               help=_('Number of gratuitous ARP announcements to make on '",
            "                      'each refresh interval.'))",
            "",
            "]",
            "",
            "nova_opts = [",
            "    cfg.StrOpt('service_name',",
            "               help=_('The name of the nova service in the keystone catalog')),",
            "    cfg.StrOpt('endpoint', help=_('A new endpoint to override the endpoint '",
            "                                  'in the keystone catalog.')),",
            "    cfg.StrOpt('region_name',",
            "               help=_('Region in Identity service catalog to use for '",
            "                      'communication with the OpenStack services.')),",
            "    cfg.StrOpt('endpoint_type', default='publicURL',",
            "               help=_('Endpoint interface in identity service to use')),",
            "    cfg.StrOpt('ca_certificates_file',",
            "               help=_('CA certificates file path')),",
            "    cfg.BoolOpt('insecure',",
            "                default=False,",
            "                help=_('Disable certificate validation on SSL connections')),",
            "    cfg.BoolOpt('enable_anti_affinity', default=False,",
            "                help=_('Flag to indicate if nova anti-affinity feature is '",
            "                       'turned on.')),",
            "    cfg.StrOpt('anti_affinity_policy', default=constants.ANTI_AFFINITY,",
            "               choices=[constants.ANTI_AFFINITY, constants.SOFT_ANTI_AFFINITY],",
            "               help=_('Sets the anti-affinity policy for nova')),",
            "    cfg.IntOpt('random_amphora_name_length', default=0,",
            "               help=_('If non-zero, generate a random name of the length '",
            "                      'provided for each amphora, in the format \"a[A-Z0-9]*\". '",
            "                      'Otherwise, the default name format will be used: '",
            "                      '\"amphora-{UUID}\".')),",
            "    cfg.StrOpt('availability_zone', default=None,",
            "               help=_('Availability zone to use for creating Amphorae')),",
            "]",
            "neutron_opts = [",
            "    cfg.StrOpt('service_name',",
            "               help=_('The name of the neutron service in the '",
            "                      'keystone catalog')),",
            "    cfg.StrOpt('endpoint', help=_('A new endpoint to override the endpoint '",
            "                                  'in the keystone catalog.')),",
            "    cfg.StrOpt('region_name',",
            "               help=_('Region in Identity service catalog to use for '",
            "                      'communication with the OpenStack services.')),",
            "    cfg.StrOpt('endpoint_type', default='publicURL',",
            "               help=_('Endpoint interface in identity service to use')),",
            "    cfg.StrOpt('ca_certificates_file',",
            "               help=_('CA certificates file path')),",
            "    cfg.BoolOpt('insecure',",
            "                default=False,",
            "                help=_('Disable certificate validation on SSL connections ')),",
            "]",
            "",
            "glance_opts = [",
            "    cfg.StrOpt('service_name',",
            "               help=_('The name of the glance service in the '",
            "                      'keystone catalog')),",
            "    cfg.StrOpt('endpoint', help=_('A new endpoint to override the endpoint '",
            "                                  'in the keystone catalog.')),",
            "    cfg.StrOpt('region_name',",
            "               help=_('Region in Identity service catalog to use for '",
            "                      'communication with the OpenStack services.')),",
            "    cfg.StrOpt('endpoint_type', default='publicURL',",
            "               help=_('Endpoint interface in identity service to use')),",
            "    cfg.StrOpt('ca_certificates_file',",
            "               help=_('CA certificates file path')),",
            "    cfg.BoolOpt('insecure',",
            "                default=False,",
            "                help=_('Disable certificate validation on SSL connections ')),",
            "]",
            "",
            "quota_opts = [",
            "    cfg.IntOpt('default_load_balancer_quota',",
            "               default=constants.QUOTA_UNLIMITED,",
            "               help=_('Default per project load balancer quota.')),",
            "    cfg.IntOpt('default_listener_quota',",
            "               default=constants.QUOTA_UNLIMITED,",
            "               help=_('Default per project listener quota.')),",
            "    cfg.IntOpt('default_member_quota',",
            "               default=constants.QUOTA_UNLIMITED,",
            "               help=_('Default per project member quota.')),",
            "    cfg.IntOpt('default_pool_quota',",
            "               default=constants.QUOTA_UNLIMITED,",
            "               help=_('Default per project pool quota.')),",
            "    cfg.IntOpt('default_health_monitor_quota',",
            "               default=constants.QUOTA_UNLIMITED,",
            "               help=_('Default per project health monitor quota.')),",
            "]",
            "",
            "audit_opts = [",
            "    cfg.BoolOpt('enabled', default=False,",
            "                help=_('Enable auditing of API requests')),",
            "    cfg.StrOpt('audit_map_file',",
            "               default='/etc/octavia/octavia_api_audit_map.conf',",
            "               help=_('Path to audit map file for octavia-api service. '",
            "                      'Used only when API audit is enabled.')),",
            "    cfg.StrOpt('ignore_req_list', default='',",
            "               help=_('Comma separated list of REST API HTTP methods to be '",
            "                      'ignored during audit. For example: auditing will not '",
            "                      'be done on any GET or POST requests if this is set to '",
            "                      '\"GET,POST\". It is used only when API audit is '",
            "                      'enabled.')),",
            "]",
            "",
            "# Register the configuration options",
            "cfg.CONF.register_opts(core_opts)",
            "cfg.CONF.register_opts(api_opts, group='api_settings')",
            "cfg.CONF.register_opts(amphora_agent_opts, group='amphora_agent')",
            "cfg.CONF.register_opts(networking_opts, group='networking')",
            "cfg.CONF.register_opts(oslo_messaging_opts, group='oslo_messaging')",
            "cfg.CONF.register_opts(haproxy_amphora_opts, group='haproxy_amphora')",
            "cfg.CONF.register_opts(controller_worker_opts, group='controller_worker')",
            "cfg.CONF.register_opts(keepalived_vrrp_opts, group='keepalived_vrrp')",
            "cfg.CONF.register_opts(task_flow_opts, group='task_flow')",
            "cfg.CONF.register_opts(house_keeping_opts, group='house_keeping')",
            "cfg.CONF.register_opts(anchor_opts, group='anchor')",
            "cfg.CONF.register_cli_opts(core_cli_opts)",
            "cfg.CONF.register_opts(certificate_opts, group='certificates')",
            "cfg.CONF.register_cli_opts(healthmanager_opts, group='health_manager')",
            "cfg.CONF.register_opts(nova_opts, group='nova')",
            "cfg.CONF.register_opts(glance_opts, group='glance')",
            "cfg.CONF.register_opts(neutron_opts, group='neutron')",
            "cfg.CONF.register_opts(quota_opts, group='quotas')",
            "cfg.CONF.register_opts(audit_opts, group='audit')",
            "",
            "cfg.CONF.register_opts(local.certgen_opts, group='certificates')",
            "cfg.CONF.register_opts(local.certmgr_opts, group='certificates')",
            "",
            "# Ensure that the control exchange is set correctly",
            "messaging.set_transport_defaults(control_exchange='octavia')",
            "_SQL_CONNECTION_DEFAULT = 'sqlite://'",
            "# Update the default QueuePool parameters. These can be tweaked by the",
            "# configuration variables - max_pool_size, max_overflow and pool_timeout",
            "db_options.set_defaults(cfg.CONF, connection=_SQL_CONNECTION_DEFAULT,",
            "                        max_pool_size=10, max_overflow=20, pool_timeout=10)",
            "",
            "logging.register_options(cfg.CONF)",
            "",
            "ks_loading.register_auth_conf_options(cfg.CONF, constants.SERVICE_AUTH)",
            "ks_loading.register_session_conf_options(cfg.CONF, constants.SERVICE_AUTH)",
            "",
            "",
            "def init(args, **kwargs):",
            "    cfg.CONF(args=args, project='octavia',",
            "             version='%%prog %s' % version.version_info.release_string(),",
            "             **kwargs)",
            "    handle_deprecation_compatibility()",
            "    setup_remote_pydev_debug()",
            "",
            "",
            "def setup_logging(conf):",
            "    \"\"\"Sets up the logging options for a log with supplied name.",
            "",
            "    :param conf: a cfg.ConfOpts object",
            "    \"\"\"",
            "    product_name = \"octavia\"",
            "    logging.setup(conf, product_name)",
            "    LOG.info(\"Logging enabled!\")",
            "    LOG.info(\"%(prog)s version %(version)s\",",
            "             {'prog': sys.argv[0],",
            "              'version': version.version_info.release_string()})",
            "    LOG.debug(\"command line: %s\", \" \".join(sys.argv))",
            "",
            "",
            "# Use cfg.CONF.set_default to override the new configuration setting",
            "# default value.  This allows a value set, at the new location, to override",
            "# a value set in the previous location while allowing settings that have",
            "# not yet been moved to be utilized.",
            "def handle_deprecation_compatibility():",
            "    # TODO(tatsuma) Remove in or after \"T\" release",
            "    if cfg.CONF.health_manager.status_update_threads is not None:",
            "        cfg.CONF.set_default('health_update_threads',",
            "                             cfg.CONF.health_manager.status_update_threads,",
            "                             group='health_manager')",
            "        cfg.CONF.set_default('stats_update_threads',",
            "                             cfg.CONF.health_manager.status_update_threads,",
            "                             group='health_manager')",
            "",
            "",
            "def setup_remote_pydev_debug():",
            "    \"\"\"Required setup for remote debugging.\"\"\"",
            "",
            "    pydev_debug_host = os.environ.get('PYDEV_DEBUG_HOST')",
            "    pydev_debug_port = os.environ.get('PYDEV_DEBUG_PORT')",
            "",
            "    if not pydev_debug_host or not pydev_debug_port:",
            "        return",
            "",
            "    try:",
            "        try:",
            "            from pydev import pydevd",
            "        except ImportError:",
            "            import pydevd",
            "",
            "        LOG.warning(\"Connecting to remote debugger. Once connected, resume \"",
            "                    \"the program on the debugger to continue with the \"",
            "                    \"initialization of the service.\")",
            "        pydevd.settrace(pydev_debug_host,",
            "                        port=int(pydev_debug_port),",
            "                        stdoutToServer=True,",
            "                        stderrToServer=True)",
            "    except Exception:",
            "        LOG.exception('Unable to join debugger, please make sure that the '",
            "                      'debugger processes is listening on debug-host '",
            "                      '\\'%(debug-host)s\\' debug-port \\'%(debug-port)s\\'.',",
            "                      {'debug-host': pydev_debug_host,",
            "                       'debug-port': pydev_debug_port})",
            "        raise"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "octavia/common/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from oslo_config import cfg"
            },
            "1": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " from oslo_log import log as logging"
            },
            "2": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from oslo_utils import excutils"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+import six"
            },
            "4": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " from stevedore import driver as stevedore_driver"
            },
            "5": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " CONF = cfg.CONF"
            },
            "7": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 91,
                "PatchRowcode": "     return \"{ip}/{netmask}\".format(ip=net.network, netmask=net.prefixlen)"
            },
            "8": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 92,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 93,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+def get_six_compatible_value(value, six_type=six.string_types):"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+    if six.PY3 and isinstance(value, six_type):"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+        value = value.encode('utf-8')"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+    return value"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+def get_six_compatible_server_certs_key_passphrase():"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+    key = CONF.certificates.server_certs_key_passphrase"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+    if six.PY3 and isinstance(key, six.string_types):"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+        key = key.encode('utf-8')"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 104,
                "PatchRowcode": "+    return base64.urlsafe_b64encode(key)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 105,
                "PatchRowcode": "+"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+"
            },
            "23": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 107,
                "PatchRowcode": " class exception_logger(object):"
            },
            "24": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": 108,
                "PatchRowcode": "     \"\"\"Wrap a function and log raised exception"
            },
            "25": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 109,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# Copyright 2011, VMware, Inc., 2014 A10 Networks",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "#",
            "# Borrowed from nova code base, more utilities will be added/borrowed as and",
            "# when needed.",
            "",
            "\"\"\"Utilities and helper functions.\"\"\"",
            "",
            "import base64",
            "import hashlib",
            "import socket",
            "",
            "import netaddr",
            "from oslo_config import cfg",
            "from oslo_log import log as logging",
            "from oslo_utils import excutils",
            "from stevedore import driver as stevedore_driver",
            "",
            "CONF = cfg.CONF",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "def get_hostname():",
            "    return socket.gethostname()",
            "",
            "",
            "def base64_sha1_string(string_to_hash):",
            "    \"\"\"Get a b64-encoded sha1 hash of a string. Not intended to be secure!\"\"\"",
            "    # TODO(rm_work): applying nosec here because this is not intended to be",
            "    # secure, it's just a way to get a consistent ID. Changing this would",
            "    # break backwards compatibility with existing loadbalancers.",
            "    hash_str = hashlib.sha1(string_to_hash.encode('utf-8')).digest()  # nosec",
            "    b64_str = base64.b64encode(hash_str, str.encode('_-', 'ascii'))",
            "    return b64_str.decode('UTF-8')",
            "",
            "",
            "def get_network_driver():",
            "    CONF.import_group('controller_worker', 'octavia.common.config')",
            "    network_driver = stevedore_driver.DriverManager(",
            "        namespace='octavia.network.drivers',",
            "        name=CONF.controller_worker.network_driver,",
            "        invoke_on_load=True",
            "    ).driver",
            "    return network_driver",
            "",
            "",
            "def is_ipv6(ip_address):",
            "    \"\"\"Check if ip address is IPv6 address.\"\"\"",
            "    ip = netaddr.IPAddress(ip_address)",
            "    return ip.version == 6",
            "",
            "",
            "def is_ipv6_lla(ip_address):",
            "    \"\"\"Check if ip address is IPv6 link local address.\"\"\"",
            "    ip = netaddr.IPAddress(ip_address)",
            "    return ip.version == 6 and ip.is_link_local()",
            "",
            "",
            "def ip_port_str(ip_address, port):",
            "    \"\"\"Return IP port as string representation depending on address family.\"\"\"",
            "    ip = netaddr.IPAddress(ip_address)",
            "    if ip.version == 4:",
            "        return \"{ip}:{port}\".format(ip=ip, port=port)",
            "    return \"[{ip}]:{port}\".format(ip=ip, port=port)",
            "",
            "",
            "def netmask_to_prefix(netmask):",
            "    return netaddr.IPAddress(netmask).netmask_bits()",
            "",
            "",
            "def ip_netmask_to_cidr(ip, netmask):",
            "    net = netaddr.IPNetwork(\"0.0.0.0/0\")",
            "    if ip and netmask:",
            "        net = netaddr.IPNetwork(",
            "            \"{ip}/{netmask}\".format(ip=ip, netmask=netmask)",
            "        )",
            "    return \"{ip}/{netmask}\".format(ip=net.network, netmask=net.prefixlen)",
            "",
            "",
            "class exception_logger(object):",
            "    \"\"\"Wrap a function and log raised exception",
            "",
            "    :param logger: the logger to log the exception default is LOG.exception",
            "",
            "    :returns: origin value if no exception raised; re-raise the exception if",
            "              any occurred",
            "",
            "    \"\"\"",
            "    def __init__(self, logger=None):",
            "        self.logger = logger",
            "",
            "    def __call__(self, func):",
            "        if self.logger is None:",
            "            _LOG = logging.getLogger(func.__module__)",
            "            self.logger = _LOG.exception",
            "",
            "        def call(*args, **kwargs):",
            "            try:",
            "                return func(*args, **kwargs)",
            "            except Exception as e:",
            "                with excutils.save_and_reraise_exception():",
            "                    self.logger(e)",
            "        return call"
        ],
        "afterPatchFile": [
            "# Copyright 2011, VMware, Inc., 2014 A10 Networks",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "#",
            "# Borrowed from nova code base, more utilities will be added/borrowed as and",
            "# when needed.",
            "",
            "\"\"\"Utilities and helper functions.\"\"\"",
            "",
            "import base64",
            "import hashlib",
            "import socket",
            "",
            "import netaddr",
            "from oslo_config import cfg",
            "from oslo_log import log as logging",
            "from oslo_utils import excutils",
            "import six",
            "from stevedore import driver as stevedore_driver",
            "",
            "CONF = cfg.CONF",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "def get_hostname():",
            "    return socket.gethostname()",
            "",
            "",
            "def base64_sha1_string(string_to_hash):",
            "    \"\"\"Get a b64-encoded sha1 hash of a string. Not intended to be secure!\"\"\"",
            "    # TODO(rm_work): applying nosec here because this is not intended to be",
            "    # secure, it's just a way to get a consistent ID. Changing this would",
            "    # break backwards compatibility with existing loadbalancers.",
            "    hash_str = hashlib.sha1(string_to_hash.encode('utf-8')).digest()  # nosec",
            "    b64_str = base64.b64encode(hash_str, str.encode('_-', 'ascii'))",
            "    return b64_str.decode('UTF-8')",
            "",
            "",
            "def get_network_driver():",
            "    CONF.import_group('controller_worker', 'octavia.common.config')",
            "    network_driver = stevedore_driver.DriverManager(",
            "        namespace='octavia.network.drivers',",
            "        name=CONF.controller_worker.network_driver,",
            "        invoke_on_load=True",
            "    ).driver",
            "    return network_driver",
            "",
            "",
            "def is_ipv6(ip_address):",
            "    \"\"\"Check if ip address is IPv6 address.\"\"\"",
            "    ip = netaddr.IPAddress(ip_address)",
            "    return ip.version == 6",
            "",
            "",
            "def is_ipv6_lla(ip_address):",
            "    \"\"\"Check if ip address is IPv6 link local address.\"\"\"",
            "    ip = netaddr.IPAddress(ip_address)",
            "    return ip.version == 6 and ip.is_link_local()",
            "",
            "",
            "def ip_port_str(ip_address, port):",
            "    \"\"\"Return IP port as string representation depending on address family.\"\"\"",
            "    ip = netaddr.IPAddress(ip_address)",
            "    if ip.version == 4:",
            "        return \"{ip}:{port}\".format(ip=ip, port=port)",
            "    return \"[{ip}]:{port}\".format(ip=ip, port=port)",
            "",
            "",
            "def netmask_to_prefix(netmask):",
            "    return netaddr.IPAddress(netmask).netmask_bits()",
            "",
            "",
            "def ip_netmask_to_cidr(ip, netmask):",
            "    net = netaddr.IPNetwork(\"0.0.0.0/0\")",
            "    if ip and netmask:",
            "        net = netaddr.IPNetwork(",
            "            \"{ip}/{netmask}\".format(ip=ip, netmask=netmask)",
            "        )",
            "    return \"{ip}/{netmask}\".format(ip=net.network, netmask=net.prefixlen)",
            "",
            "",
            "def get_six_compatible_value(value, six_type=six.string_types):",
            "    if six.PY3 and isinstance(value, six_type):",
            "        value = value.encode('utf-8')",
            "    return value",
            "",
            "",
            "def get_six_compatible_server_certs_key_passphrase():",
            "    key = CONF.certificates.server_certs_key_passphrase",
            "    if six.PY3 and isinstance(key, six.string_types):",
            "        key = key.encode('utf-8')",
            "    return base64.urlsafe_b64encode(key)",
            "",
            "",
            "class exception_logger(object):",
            "    \"\"\"Wrap a function and log raised exception",
            "",
            "    :param logger: the logger to log the exception default is LOG.exception",
            "",
            "    :returns: origin value if no exception raised; re-raise the exception if",
            "              any occurred",
            "",
            "    \"\"\"",
            "    def __init__(self, logger=None):",
            "        self.logger = logger",
            "",
            "    def __call__(self, func):",
            "        if self.logger is None:",
            "            _LOG = logging.getLogger(func.__module__)",
            "            self.logger = _LOG.exception",
            "",
            "        def call(*args, **kwargs):",
            "            try:",
            "                return func(*args, **kwargs)",
            "            except Exception as e:",
            "                with excutils.save_and_reraise_exception():",
            "                    self.logger(e)",
            "        return call"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "octavia/controller/worker/controller_worker.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 71,
                "PatchRowcode": "         self._l7rule_repo = repo.L7RuleRepository()"
            },
            "1": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "         self._flavor_repo = repo.FlavorRepository()"
            },
            "2": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 73,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self._exclude_result_logging_tasks = ("
            },
            "4": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            constants.ROLE_STANDALONE + '-' +"
            },
            "5": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            constants.CREATE_AMP_FOR_LB_SUBFLOW + '-' +"
            },
            "6": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            constants.GENERATE_SERVER_PEM,"
            },
            "7": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            constants.ROLE_BACKUP + '-' +"
            },
            "8": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            constants.CREATE_AMP_FOR_LB_SUBFLOW + '-' +"
            },
            "9": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            constants.GENERATE_SERVER_PEM,"
            },
            "10": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            constants.ROLE_MASTER + '-' +"
            },
            "11": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            constants.CREATE_AMP_FOR_LB_SUBFLOW + '-' +"
            },
            "12": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            constants.GENERATE_SERVER_PEM,"
            },
            "13": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            constants.GENERATE_SERVER_PEM_TASK,"
            },
            "14": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            constants.FAILOVER_AMPHORA_FLOW + '-' +"
            },
            "15": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            constants.CREATE_AMP_FOR_LB_SUBFLOW + '-' +"
            },
            "16": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            constants.GENERATE_SERVER_PEM,"
            },
            "17": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            constants.CREATE_AMP_FOR_LB_SUBFLOW + '-' +"
            },
            "18": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            constants.UPDATE_CERT_EXPIRATION)"
            },
            "19": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "20": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 74,
                "PatchRowcode": "         super(ControllerWorker, self).__init__()"
            },
            "21": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 75,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 76,
                "PatchRowcode": "     @tenacity.retry("
            },
            "23": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 98,
                "PatchRowcode": "                        constants.LB_CREATE_SPARES_POOL_PRIORITY,"
            },
            "24": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 99,
                "PatchRowcode": "                        constants.FLAVOR: None}"
            },
            "25": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": 100,
                "PatchRowcode": "             )"
            },
            "26": {
                "beforePatchRowNumber": 118,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            with tf_logging.DynamicLoggingListener("
            },
            "27": {
                "beforePatchRowNumber": 119,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    create_amp_tf, log=LOG,"
            },
            "28": {
                "beforePatchRowNumber": 120,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    hide_inputs_outputs_of=self._exclude_result_logging_tasks):"
            },
            "29": {
                "beforePatchRowNumber": 121,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+            with tf_logging.DynamicLoggingListener(create_amp_tf, log=LOG):"
            },
            "31": {
                "beforePatchRowNumber": 122,
                "afterPatchRowNumber": 102,
                "PatchRowcode": "                 create_amp_tf.run()"
            },
            "32": {
                "beforePatchRowNumber": 123,
                "afterPatchRowNumber": 103,
                "PatchRowcode": " "
            },
            "33": {
                "beforePatchRowNumber": 124,
                "afterPatchRowNumber": 104,
                "PatchRowcode": "             return create_amp_tf.storage.fetch('amphora')"
            },
            "34": {
                "beforePatchRowNumber": 359,
                "afterPatchRowNumber": 339,
                "PatchRowcode": "             topology=topology, listeners=lb.listeners)"
            },
            "35": {
                "beforePatchRowNumber": 360,
                "afterPatchRowNumber": 340,
                "PatchRowcode": " "
            },
            "36": {
                "beforePatchRowNumber": 361,
                "afterPatchRowNumber": 341,
                "PatchRowcode": "         create_lb_tf = self._taskflow_load(create_lb_flow, store=store)"
            },
            "37": {
                "beforePatchRowNumber": 362,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        with tf_logging.DynamicLoggingListener("
            },
            "38": {
                "beforePatchRowNumber": 363,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                create_lb_tf, log=LOG,"
            },
            "39": {
                "beforePatchRowNumber": 364,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                hide_inputs_outputs_of=self._exclude_result_logging_tasks):"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 342,
                "PatchRowcode": "+        with tf_logging.DynamicLoggingListener(create_lb_tf, log=LOG):"
            },
            "41": {
                "beforePatchRowNumber": 365,
                "afterPatchRowNumber": 343,
                "PatchRowcode": "             create_lb_tf.run()"
            },
            "42": {
                "beforePatchRowNumber": 366,
                "afterPatchRowNumber": 344,
                "PatchRowcode": " "
            },
            "43": {
                "beforePatchRowNumber": 367,
                "afterPatchRowNumber": 345,
                "PatchRowcode": "     def delete_load_balancer(self, load_balancer_id, cascade=False):"
            },
            "44": {
                "beforePatchRowNumber": 857,
                "afterPatchRowNumber": 835,
                "PatchRowcode": "                 role=amp.role, load_balancer=lb),"
            },
            "45": {
                "beforePatchRowNumber": 858,
                "afterPatchRowNumber": 836,
                "PatchRowcode": "             store=stored_params)"
            },
            "46": {
                "beforePatchRowNumber": 859,
                "afterPatchRowNumber": 837,
                "PatchRowcode": " "
            },
            "47": {
                "beforePatchRowNumber": 860,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        with tf_logging.DynamicLoggingListener("
            },
            "48": {
                "beforePatchRowNumber": 861,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                failover_amphora_tf, log=LOG,"
            },
            "49": {
                "beforePatchRowNumber": 862,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                hide_inputs_outputs_of=self._exclude_result_logging_tasks):"
            },
            "50": {
                "beforePatchRowNumber": 863,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 838,
                "PatchRowcode": "+        with tf_logging.DynamicLoggingListener(failover_amphora_tf, log=LOG):"
            },
            "52": {
                "beforePatchRowNumber": 864,
                "afterPatchRowNumber": 839,
                "PatchRowcode": "             failover_amphora_tf.run()"
            },
            "53": {
                "beforePatchRowNumber": 865,
                "afterPatchRowNumber": 840,
                "PatchRowcode": " "
            },
            "54": {
                "beforePatchRowNumber": 866,
                "afterPatchRowNumber": 841,
                "PatchRowcode": "     def failover_amphora(self, amphora_id):"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015 Hewlett-Packard Development Company, L.P.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "# http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "",
            "",
            "from oslo_config import cfg",
            "from oslo_log import log as logging",
            "from oslo_utils import excutils",
            "from sqlalchemy.orm import exc as db_exceptions",
            "from taskflow.listeners import logging as tf_logging",
            "import tenacity",
            "",
            "from octavia.common import base_taskflow",
            "from octavia.common import constants",
            "from octavia.controller.worker.flows import amphora_flows",
            "from octavia.controller.worker.flows import health_monitor_flows",
            "from octavia.controller.worker.flows import l7policy_flows",
            "from octavia.controller.worker.flows import l7rule_flows",
            "from octavia.controller.worker.flows import listener_flows",
            "from octavia.controller.worker.flows import load_balancer_flows",
            "from octavia.controller.worker.flows import member_flows",
            "from octavia.controller.worker.flows import pool_flows",
            "from octavia.db import api as db_apis",
            "from octavia.db import repositories as repo",
            "",
            "CONF = cfg.CONF",
            "LOG = logging.getLogger(__name__)",
            "",
            "RETRY_ATTEMPTS = 15",
            "RETRY_INITIAL_DELAY = 1",
            "RETRY_BACKOFF = 1",
            "RETRY_MAX = 5",
            "",
            "",
            "def _is_provisioning_status_pending_update(lb_obj):",
            "    return not lb_obj.provisioning_status == constants.PENDING_UPDATE",
            "",
            "",
            "class ControllerWorker(base_taskflow.BaseTaskFlowEngine):",
            "",
            "    def __init__(self):",
            "",
            "        self._amphora_flows = amphora_flows.AmphoraFlows()",
            "        self._health_monitor_flows = health_monitor_flows.HealthMonitorFlows()",
            "        self._lb_flows = load_balancer_flows.LoadBalancerFlows()",
            "        self._listener_flows = listener_flows.ListenerFlows()",
            "        self._member_flows = member_flows.MemberFlows()",
            "        self._pool_flows = pool_flows.PoolFlows()",
            "        self._l7policy_flows = l7policy_flows.L7PolicyFlows()",
            "        self._l7rule_flows = l7rule_flows.L7RuleFlows()",
            "",
            "        self._amphora_repo = repo.AmphoraRepository()",
            "        self._amphora_health_repo = repo.AmphoraHealthRepository()",
            "        self._health_mon_repo = repo.HealthMonitorRepository()",
            "        self._lb_repo = repo.LoadBalancerRepository()",
            "        self._listener_repo = repo.ListenerRepository()",
            "        self._member_repo = repo.MemberRepository()",
            "        self._pool_repo = repo.PoolRepository()",
            "        self._l7policy_repo = repo.L7PolicyRepository()",
            "        self._l7rule_repo = repo.L7RuleRepository()",
            "        self._flavor_repo = repo.FlavorRepository()",
            "",
            "        self._exclude_result_logging_tasks = (",
            "            constants.ROLE_STANDALONE + '-' +",
            "            constants.CREATE_AMP_FOR_LB_SUBFLOW + '-' +",
            "            constants.GENERATE_SERVER_PEM,",
            "            constants.ROLE_BACKUP + '-' +",
            "            constants.CREATE_AMP_FOR_LB_SUBFLOW + '-' +",
            "            constants.GENERATE_SERVER_PEM,",
            "            constants.ROLE_MASTER + '-' +",
            "            constants.CREATE_AMP_FOR_LB_SUBFLOW + '-' +",
            "            constants.GENERATE_SERVER_PEM,",
            "            constants.GENERATE_SERVER_PEM_TASK,",
            "            constants.FAILOVER_AMPHORA_FLOW + '-' +",
            "            constants.CREATE_AMP_FOR_LB_SUBFLOW + '-' +",
            "            constants.GENERATE_SERVER_PEM,",
            "            constants.CREATE_AMP_FOR_LB_SUBFLOW + '-' +",
            "            constants.UPDATE_CERT_EXPIRATION)",
            "",
            "        super(ControllerWorker, self).__init__()",
            "",
            "    @tenacity.retry(",
            "        retry=(",
            "            tenacity.retry_if_result(_is_provisioning_status_pending_update) |",
            "            tenacity.retry_if_exception_type()),",
            "        wait=tenacity.wait_incrementing(",
            "            RETRY_INITIAL_DELAY, RETRY_BACKOFF, RETRY_MAX),",
            "        stop=tenacity.stop_after_attempt(RETRY_ATTEMPTS))",
            "    def _get_db_obj_until_pending_update(self, repo, id):",
            "",
            "        return repo.get(db_apis.get_session(), id=id)",
            "",
            "    def create_amphora(self):",
            "        \"\"\"Creates an Amphora.",
            "",
            "        This is used to create spare amphora.",
            "",
            "        :returns: amphora_id",
            "        \"\"\"",
            "        try:",
            "            create_amp_tf = self._taskflow_load(",
            "                self._amphora_flows.get_create_amphora_flow(),",
            "                store={constants.BUILD_TYPE_PRIORITY:",
            "                       constants.LB_CREATE_SPARES_POOL_PRIORITY,",
            "                       constants.FLAVOR: None}",
            "            )",
            "            with tf_logging.DynamicLoggingListener(",
            "                    create_amp_tf, log=LOG,",
            "                    hide_inputs_outputs_of=self._exclude_result_logging_tasks):",
            "",
            "                create_amp_tf.run()",
            "",
            "            return create_amp_tf.storage.fetch('amphora')",
            "        except Exception as e:",
            "            LOG.error('Failed to create an amphora due to: {}'.format(str(e)))",
            "",
            "    def delete_amphora(self, amphora_id):",
            "        \"\"\"Deletes an existing Amphora.",
            "",
            "        :param amphora_id: ID of the amphora to delete",
            "        :returns: None",
            "        :raises AmphoraNotFound: The referenced Amphora was not found",
            "        \"\"\"",
            "        amphora = self._amphora_repo.get(db_apis.get_session(),",
            "                                         id=amphora_id)",
            "        delete_amp_tf = self._taskflow_load(self._amphora_flows.",
            "                                            get_delete_amphora_flow(),",
            "                                            store={constants.AMPHORA: amphora})",
            "        with tf_logging.DynamicLoggingListener(delete_amp_tf,",
            "                                               log=LOG):",
            "            delete_amp_tf.run()",
            "",
            "    @tenacity.retry(",
            "        retry=tenacity.retry_if_exception_type(db_exceptions.NoResultFound),",
            "        wait=tenacity.wait_incrementing(",
            "            RETRY_INITIAL_DELAY, RETRY_BACKOFF, RETRY_MAX),",
            "        stop=tenacity.stop_after_attempt(RETRY_ATTEMPTS))",
            "    def create_health_monitor(self, health_monitor_id):",
            "        \"\"\"Creates a health monitor.",
            "",
            "        :param pool_id: ID of the pool to create a health monitor on",
            "        :returns: None",
            "        :raises NoResultFound: Unable to find the object",
            "        \"\"\"",
            "        health_mon = self._health_mon_repo.get(db_apis.get_session(),",
            "                                               id=health_monitor_id)",
            "        if not health_mon:",
            "            LOG.warning('Failed to fetch %s %s from DB. Retrying for up to '",
            "                        '60 seconds.', 'health_monitor', health_monitor_id)",
            "            raise db_exceptions.NoResultFound",
            "",
            "        pool = health_mon.pool",
            "        listeners = pool.listeners",
            "        pool.health_monitor = health_mon",
            "        load_balancer = pool.load_balancer",
            "",
            "        create_hm_tf = self._taskflow_load(",
            "            self._health_monitor_flows.get_create_health_monitor_flow(),",
            "            store={constants.HEALTH_MON: health_mon,",
            "                   constants.POOL: pool,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer})",
            "        with tf_logging.DynamicLoggingListener(create_hm_tf,",
            "                                               log=LOG):",
            "            create_hm_tf.run()",
            "",
            "    def delete_health_monitor(self, health_monitor_id):",
            "        \"\"\"Deletes a health monitor.",
            "",
            "        :param pool_id: ID of the pool to delete its health monitor",
            "        :returns: None",
            "        :raises HMNotFound: The referenced health monitor was not found",
            "        \"\"\"",
            "        health_mon = self._health_mon_repo.get(db_apis.get_session(),",
            "                                               id=health_monitor_id)",
            "",
            "        pool = health_mon.pool",
            "        listeners = pool.listeners",
            "        load_balancer = pool.load_balancer",
            "",
            "        delete_hm_tf = self._taskflow_load(",
            "            self._health_monitor_flows.get_delete_health_monitor_flow(),",
            "            store={constants.HEALTH_MON: health_mon,",
            "                   constants.POOL: pool,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer})",
            "        with tf_logging.DynamicLoggingListener(delete_hm_tf,",
            "                                               log=LOG):",
            "            delete_hm_tf.run()",
            "",
            "    def update_health_monitor(self, health_monitor_id, health_monitor_updates):",
            "        \"\"\"Updates a health monitor.",
            "",
            "        :param pool_id: ID of the pool to have it's health monitor updated",
            "        :param health_monitor_updates: Dict containing updated health monitor",
            "        :returns: None",
            "        :raises HMNotFound: The referenced health monitor was not found",
            "        \"\"\"",
            "        health_mon = None",
            "        try:",
            "            health_mon = self._get_db_obj_until_pending_update(",
            "                self._health_mon_repo, health_monitor_id)",
            "        except tenacity.RetryError as e:",
            "            LOG.warning('Health monitor did not go into %s in 60 seconds. '",
            "                        'This either due to an in-progress Octavia upgrade '",
            "                        'or an overloaded and failing database. Assuming '",
            "                        'an upgrade is in progress and continuing.',",
            "                        constants.PENDING_UPDATE)",
            "            health_mon = e.last_attempt.result()",
            "",
            "        pool = health_mon.pool",
            "        listeners = pool.listeners",
            "        pool.health_monitor = health_mon",
            "        load_balancer = pool.load_balancer",
            "",
            "        update_hm_tf = self._taskflow_load(",
            "            self._health_monitor_flows.get_update_health_monitor_flow(),",
            "            store={constants.HEALTH_MON: health_mon,",
            "                   constants.POOL: pool,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer,",
            "                   constants.UPDATE_DICT: health_monitor_updates})",
            "        with tf_logging.DynamicLoggingListener(update_hm_tf,",
            "                                               log=LOG):",
            "            update_hm_tf.run()",
            "",
            "    @tenacity.retry(",
            "        retry=tenacity.retry_if_exception_type(db_exceptions.NoResultFound),",
            "        wait=tenacity.wait_incrementing(",
            "            RETRY_INITIAL_DELAY, RETRY_BACKOFF, RETRY_MAX),",
            "        stop=tenacity.stop_after_attempt(RETRY_ATTEMPTS))",
            "    def create_listener(self, listener_id):",
            "        \"\"\"Creates a listener.",
            "",
            "        :param listener_id: ID of the listener to create",
            "        :returns: None",
            "        :raises NoResultFound: Unable to find the object",
            "        \"\"\"",
            "        listener = self._listener_repo.get(db_apis.get_session(),",
            "                                           id=listener_id)",
            "        if not listener:",
            "            LOG.warning('Failed to fetch %s %s from DB. Retrying for up to '",
            "                        '60 seconds.', 'listener', listener_id)",
            "            raise db_exceptions.NoResultFound",
            "",
            "        load_balancer = listener.load_balancer",
            "",
            "        create_listener_tf = self._taskflow_load(self._listener_flows.",
            "                                                 get_create_listener_flow(),",
            "                                                 store={constants.LOADBALANCER:",
            "                                                        load_balancer,",
            "                                                        constants.LISTENERS:",
            "                                                            [listener]})",
            "        with tf_logging.DynamicLoggingListener(create_listener_tf,",
            "                                               log=LOG):",
            "            create_listener_tf.run()",
            "",
            "    def delete_listener(self, listener_id):",
            "        \"\"\"Deletes a listener.",
            "",
            "        :param listener_id: ID of the listener to delete",
            "        :returns: None",
            "        :raises ListenerNotFound: The referenced listener was not found",
            "        \"\"\"",
            "        listener = self._listener_repo.get(db_apis.get_session(),",
            "                                           id=listener_id)",
            "        load_balancer = listener.load_balancer",
            "",
            "        delete_listener_tf = self._taskflow_load(",
            "            self._listener_flows.get_delete_listener_flow(),",
            "            store={constants.LOADBALANCER: load_balancer,",
            "                   constants.LISTENER: listener})",
            "        with tf_logging.DynamicLoggingListener(delete_listener_tf,",
            "                                               log=LOG):",
            "            delete_listener_tf.run()",
            "",
            "    def update_listener(self, listener_id, listener_updates):",
            "        \"\"\"Updates a listener.",
            "",
            "        :param listener_id: ID of the listener to update",
            "        :param listener_updates: Dict containing updated listener attributes",
            "        :returns: None",
            "        :raises ListenerNotFound: The referenced listener was not found",
            "        \"\"\"",
            "        listener = None",
            "        try:",
            "            listener = self._get_db_obj_until_pending_update(",
            "                self._listener_repo, listener_id)",
            "        except tenacity.RetryError as e:",
            "            LOG.warning('Listener did not go into %s in 60 seconds. '",
            "                        'This either due to an in-progress Octavia upgrade '",
            "                        'or an overloaded and failing database. Assuming '",
            "                        'an upgrade is in progress and continuing.',",
            "                        constants.PENDING_UPDATE)",
            "            listener = e.last_attempt.result()",
            "",
            "        load_balancer = listener.load_balancer",
            "",
            "        update_listener_tf = self._taskflow_load(self._listener_flows.",
            "                                                 get_update_listener_flow(),",
            "                                                 store={constants.LISTENER:",
            "                                                        listener,",
            "                                                        constants.LOADBALANCER:",
            "                                                            load_balancer,",
            "                                                        constants.UPDATE_DICT:",
            "                                                            listener_updates,",
            "                                                        constants.LISTENERS:",
            "                                                            [listener]})",
            "        with tf_logging.DynamicLoggingListener(update_listener_tf, log=LOG):",
            "            update_listener_tf.run()",
            "",
            "    @tenacity.retry(",
            "        retry=tenacity.retry_if_exception_type(db_exceptions.NoResultFound),",
            "        wait=tenacity.wait_incrementing(",
            "            RETRY_INITIAL_DELAY, RETRY_BACKOFF, RETRY_MAX),",
            "        stop=tenacity.stop_after_attempt(RETRY_ATTEMPTS))",
            "    def create_load_balancer(self, load_balancer_id, flavor=None):",
            "        \"\"\"Creates a load balancer by allocating Amphorae.",
            "",
            "        First tries to allocate an existing Amphora in READY state.",
            "        If none are available it will attempt to build one specifically",
            "        for this load balancer.",
            "",
            "        :param load_balancer_id: ID of the load balancer to create",
            "        :returns: None",
            "        :raises NoResultFound: Unable to find the object",
            "        \"\"\"",
            "        lb = self._lb_repo.get(db_apis.get_session(), id=load_balancer_id)",
            "        if not lb:",
            "            LOG.warning('Failed to fetch %s %s from DB. Retrying for up to '",
            "                        '60 seconds.', 'load_balancer', load_balancer_id)",
            "            raise db_exceptions.NoResultFound",
            "",
            "        # TODO(johnsom) convert this to octavia_lib constant flavor",
            "        # once octavia is transitioned to use octavia_lib",
            "        store = {constants.LOADBALANCER_ID: load_balancer_id,",
            "                 constants.BUILD_TYPE_PRIORITY:",
            "                 constants.LB_CREATE_NORMAL_PRIORITY,",
            "                 constants.FLAVOR: flavor}",
            "",
            "        topology = lb.topology",
            "",
            "        store[constants.UPDATE_DICT] = {",
            "            constants.TOPOLOGY: topology",
            "        }",
            "",
            "        create_lb_flow = self._lb_flows.get_create_load_balancer_flow(",
            "            topology=topology, listeners=lb.listeners)",
            "",
            "        create_lb_tf = self._taskflow_load(create_lb_flow, store=store)",
            "        with tf_logging.DynamicLoggingListener(",
            "                create_lb_tf, log=LOG,",
            "                hide_inputs_outputs_of=self._exclude_result_logging_tasks):",
            "            create_lb_tf.run()",
            "",
            "    def delete_load_balancer(self, load_balancer_id, cascade=False):",
            "        \"\"\"Deletes a load balancer by de-allocating Amphorae.",
            "",
            "        :param load_balancer_id: ID of the load balancer to delete",
            "        :returns: None",
            "        :raises LBNotFound: The referenced load balancer was not found",
            "        \"\"\"",
            "        lb = self._lb_repo.get(db_apis.get_session(),",
            "                               id=load_balancer_id)",
            "",
            "        if cascade:",
            "            (flow,",
            "             store) = self._lb_flows.get_cascade_delete_load_balancer_flow(lb)",
            "        else:",
            "            (flow, store) = self._lb_flows.get_delete_load_balancer_flow(lb)",
            "        store.update({constants.LOADBALANCER: lb,",
            "                      constants.SERVER_GROUP_ID: lb.server_group_id})",
            "        delete_lb_tf = self._taskflow_load(flow, store=store)",
            "",
            "        with tf_logging.DynamicLoggingListener(delete_lb_tf,",
            "                                               log=LOG):",
            "            delete_lb_tf.run()",
            "",
            "    def update_load_balancer(self, load_balancer_id, load_balancer_updates):",
            "        \"\"\"Updates a load balancer.",
            "",
            "        :param load_balancer_id: ID of the load balancer to update",
            "        :param load_balancer_updates: Dict containing updated load balancer",
            "        :returns: None",
            "        :raises LBNotFound: The referenced load balancer was not found",
            "        \"\"\"",
            "        lb = None",
            "        try:",
            "            lb = self._get_db_obj_until_pending_update(",
            "                self._lb_repo, load_balancer_id)",
            "        except tenacity.RetryError as e:",
            "            LOG.warning('Load balancer did not go into %s in 60 seconds. '",
            "                        'This either due to an in-progress Octavia upgrade '",
            "                        'or an overloaded and failing database. Assuming '",
            "                        'an upgrade is in progress and continuing.',",
            "                        constants.PENDING_UPDATE)",
            "            lb = e.last_attempt.result()",
            "",
            "        listeners, _ = self._listener_repo.get_all(",
            "            db_apis.get_session(),",
            "            load_balancer_id=load_balancer_id)",
            "",
            "        update_lb_tf = self._taskflow_load(",
            "            self._lb_flows.get_update_load_balancer_flow(),",
            "            store={constants.LOADBALANCER: lb,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.UPDATE_DICT: load_balancer_updates})",
            "",
            "        with tf_logging.DynamicLoggingListener(update_lb_tf,",
            "                                               log=LOG):",
            "            update_lb_tf.run()",
            "",
            "    @tenacity.retry(",
            "        retry=tenacity.retry_if_exception_type(db_exceptions.NoResultFound),",
            "        wait=tenacity.wait_incrementing(",
            "            RETRY_INITIAL_DELAY, RETRY_BACKOFF, RETRY_MAX),",
            "        stop=tenacity.stop_after_attempt(RETRY_ATTEMPTS))",
            "    def create_member(self, member_id):",
            "        \"\"\"Creates a pool member.",
            "",
            "        :param member_id: ID of the member to create",
            "        :returns: None",
            "        :raises NoSuitablePool: Unable to find the node pool",
            "        \"\"\"",
            "        member = self._member_repo.get(db_apis.get_session(),",
            "                                       id=member_id)",
            "        if not member:",
            "            LOG.warning('Failed to fetch %s %s from DB. Retrying for up to '",
            "                        '60 seconds.', 'member', member_id)",
            "            raise db_exceptions.NoResultFound",
            "",
            "        pool = member.pool",
            "        listeners = pool.listeners",
            "        load_balancer = pool.load_balancer",
            "",
            "        create_member_tf = self._taskflow_load(self._member_flows.",
            "                                               get_create_member_flow(),",
            "                                               store={constants.MEMBER: member,",
            "                                                      constants.LISTENERS:",
            "                                                          listeners,",
            "                                                      constants.LOADBALANCER:",
            "                                                          load_balancer,",
            "                                                      constants.POOL: pool})",
            "        with tf_logging.DynamicLoggingListener(create_member_tf,",
            "                                               log=LOG):",
            "            create_member_tf.run()",
            "",
            "    def delete_member(self, member_id):",
            "        \"\"\"Deletes a pool member.",
            "",
            "        :param member_id: ID of the member to delete",
            "        :returns: None",
            "        :raises MemberNotFound: The referenced member was not found",
            "        \"\"\"",
            "        member = self._member_repo.get(db_apis.get_session(),",
            "                                       id=member_id)",
            "        pool = member.pool",
            "        listeners = pool.listeners",
            "        load_balancer = pool.load_balancer",
            "",
            "        delete_member_tf = self._taskflow_load(",
            "            self._member_flows.get_delete_member_flow(),",
            "            store={constants.MEMBER: member, constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer, constants.POOL: pool}",
            "        )",
            "        with tf_logging.DynamicLoggingListener(delete_member_tf,",
            "                                               log=LOG):",
            "            delete_member_tf.run()",
            "",
            "    def batch_update_members(self, old_member_ids, new_member_ids,",
            "                             updated_members):",
            "        old_members = [self._member_repo.get(db_apis.get_session(), id=mid)",
            "                       for mid in old_member_ids]",
            "        new_members = [self._member_repo.get(db_apis.get_session(), id=mid)",
            "                       for mid in new_member_ids]",
            "        updated_members = [",
            "            (self._member_repo.get(db_apis.get_session(), id=m.get('id')), m)",
            "            for m in updated_members]",
            "        if old_members:",
            "            pool = old_members[0].pool",
            "        elif new_members:",
            "            pool = new_members[0].pool",
            "        else:",
            "            pool = updated_members[0][0].pool",
            "        listeners = pool.listeners",
            "        load_balancer = pool.load_balancer",
            "",
            "        batch_update_members_tf = self._taskflow_load(",
            "            self._member_flows.get_batch_update_members_flow(",
            "                old_members, new_members, updated_members),",
            "            store={constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer,",
            "                   constants.POOL: pool})",
            "        with tf_logging.DynamicLoggingListener(batch_update_members_tf,",
            "                                               log=LOG):",
            "            batch_update_members_tf.run()",
            "",
            "    def update_member(self, member_id, member_updates):",
            "        \"\"\"Updates a pool member.",
            "",
            "        :param member_id: ID of the member to update",
            "        :param member_updates: Dict containing updated member attributes",
            "        :returns: None",
            "        :raises MemberNotFound: The referenced member was not found",
            "        \"\"\"",
            "        member = None",
            "        try:",
            "            member = self._get_db_obj_until_pending_update(",
            "                self._member_repo, member_id)",
            "        except tenacity.RetryError as e:",
            "            LOG.warning('Member did not go into %s in 60 seconds. '",
            "                        'This either due to an in-progress Octavia upgrade '",
            "                        'or an overloaded and failing database. Assuming '",
            "                        'an upgrade is in progress and continuing.',",
            "                        constants.PENDING_UPDATE)",
            "            member = e.last_attempt.result()",
            "",
            "        pool = member.pool",
            "        listeners = pool.listeners",
            "        load_balancer = pool.load_balancer",
            "",
            "        update_member_tf = self._taskflow_load(self._member_flows.",
            "                                               get_update_member_flow(),",
            "                                               store={constants.MEMBER: member,",
            "                                                      constants.LISTENERS:",
            "                                                          listeners,",
            "                                                      constants.LOADBALANCER:",
            "                                                          load_balancer,",
            "                                                      constants.POOL:",
            "                                                          pool,",
            "                                                      constants.UPDATE_DICT:",
            "                                                          member_updates})",
            "        with tf_logging.DynamicLoggingListener(update_member_tf,",
            "                                               log=LOG):",
            "            update_member_tf.run()",
            "",
            "    @tenacity.retry(",
            "        retry=tenacity.retry_if_exception_type(db_exceptions.NoResultFound),",
            "        wait=tenacity.wait_incrementing(",
            "            RETRY_INITIAL_DELAY, RETRY_BACKOFF, RETRY_MAX),",
            "        stop=tenacity.stop_after_attempt(RETRY_ATTEMPTS))",
            "    def create_pool(self, pool_id):",
            "        \"\"\"Creates a node pool.",
            "",
            "        :param pool_id: ID of the pool to create",
            "        :returns: None",
            "        :raises NoResultFound: Unable to find the object",
            "        \"\"\"",
            "        pool = self._pool_repo.get(db_apis.get_session(),",
            "                                   id=pool_id)",
            "        if not pool:",
            "            LOG.warning('Failed to fetch %s %s from DB. Retrying for up to '",
            "                        '60 seconds.', 'pool', pool_id)",
            "            raise db_exceptions.NoResultFound",
            "",
            "        listeners = pool.listeners",
            "        load_balancer = pool.load_balancer",
            "",
            "        create_pool_tf = self._taskflow_load(self._pool_flows.",
            "                                             get_create_pool_flow(),",
            "                                             store={constants.POOL: pool,",
            "                                                    constants.LISTENERS:",
            "                                                        listeners,",
            "                                                    constants.LOADBALANCER:",
            "                                                        load_balancer})",
            "        with tf_logging.DynamicLoggingListener(create_pool_tf,",
            "                                               log=LOG):",
            "            create_pool_tf.run()",
            "",
            "    def delete_pool(self, pool_id):",
            "        \"\"\"Deletes a node pool.",
            "",
            "        :param pool_id: ID of the pool to delete",
            "        :returns: None",
            "        :raises PoolNotFound: The referenced pool was not found",
            "        \"\"\"",
            "        pool = self._pool_repo.get(db_apis.get_session(),",
            "                                   id=pool_id)",
            "",
            "        load_balancer = pool.load_balancer",
            "        listeners = pool.listeners",
            "",
            "        delete_pool_tf = self._taskflow_load(",
            "            self._pool_flows.get_delete_pool_flow(),",
            "            store={constants.POOL: pool, constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer})",
            "        with tf_logging.DynamicLoggingListener(delete_pool_tf,",
            "                                               log=LOG):",
            "            delete_pool_tf.run()",
            "",
            "    def update_pool(self, pool_id, pool_updates):",
            "        \"\"\"Updates a node pool.",
            "",
            "        :param pool_id: ID of the pool to update",
            "        :param pool_updates: Dict containing updated pool attributes",
            "        :returns: None",
            "        :raises PoolNotFound: The referenced pool was not found",
            "        \"\"\"",
            "        pool = None",
            "        try:",
            "            pool = self._get_db_obj_until_pending_update(",
            "                self._pool_repo, pool_id)",
            "        except tenacity.RetryError as e:",
            "            LOG.warning('Pool did not go into %s in 60 seconds. '",
            "                        'This either due to an in-progress Octavia upgrade '",
            "                        'or an overloaded and failing database. Assuming '",
            "                        'an upgrade is in progress and continuing.',",
            "                        constants.PENDING_UPDATE)",
            "            pool = e.last_attempt.result()",
            "",
            "        listeners = pool.listeners",
            "        load_balancer = pool.load_balancer",
            "",
            "        update_pool_tf = self._taskflow_load(self._pool_flows.",
            "                                             get_update_pool_flow(),",
            "                                             store={constants.POOL: pool,",
            "                                                    constants.LISTENERS:",
            "                                                        listeners,",
            "                                                    constants.LOADBALANCER:",
            "                                                        load_balancer,",
            "                                                    constants.UPDATE_DICT:",
            "                                                        pool_updates})",
            "        with tf_logging.DynamicLoggingListener(update_pool_tf,",
            "                                               log=LOG):",
            "            update_pool_tf.run()",
            "",
            "    @tenacity.retry(",
            "        retry=tenacity.retry_if_exception_type(db_exceptions.NoResultFound),",
            "        wait=tenacity.wait_incrementing(",
            "            RETRY_INITIAL_DELAY, RETRY_BACKOFF, RETRY_MAX),",
            "        stop=tenacity.stop_after_attempt(RETRY_ATTEMPTS))",
            "    def create_l7policy(self, l7policy_id):",
            "        \"\"\"Creates an L7 Policy.",
            "",
            "        :param l7policy_id: ID of the l7policy to create",
            "        :returns: None",
            "        :raises NoResultFound: Unable to find the object",
            "        \"\"\"",
            "        l7policy = self._l7policy_repo.get(db_apis.get_session(),",
            "                                           id=l7policy_id)",
            "        if not l7policy:",
            "            LOG.warning('Failed to fetch %s %s from DB. Retrying for up to '",
            "                        '60 seconds.', 'l7policy', l7policy_id)",
            "            raise db_exceptions.NoResultFound",
            "",
            "        listeners = [l7policy.listener]",
            "        load_balancer = l7policy.listener.load_balancer",
            "",
            "        create_l7policy_tf = self._taskflow_load(",
            "            self._l7policy_flows.get_create_l7policy_flow(),",
            "            store={constants.L7POLICY: l7policy,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer})",
            "        with tf_logging.DynamicLoggingListener(create_l7policy_tf,",
            "                                               log=LOG):",
            "            create_l7policy_tf.run()",
            "",
            "    def delete_l7policy(self, l7policy_id):",
            "        \"\"\"Deletes an L7 policy.",
            "",
            "        :param l7policy_id: ID of the l7policy to delete",
            "        :returns: None",
            "        :raises L7PolicyNotFound: The referenced l7policy was not found",
            "        \"\"\"",
            "        l7policy = self._l7policy_repo.get(db_apis.get_session(),",
            "                                           id=l7policy_id)",
            "",
            "        load_balancer = l7policy.listener.load_balancer",
            "        listeners = [l7policy.listener]",
            "",
            "        delete_l7policy_tf = self._taskflow_load(",
            "            self._l7policy_flows.get_delete_l7policy_flow(),",
            "            store={constants.L7POLICY: l7policy,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer})",
            "        with tf_logging.DynamicLoggingListener(delete_l7policy_tf,",
            "                                               log=LOG):",
            "            delete_l7policy_tf.run()",
            "",
            "    def update_l7policy(self, l7policy_id, l7policy_updates):",
            "        \"\"\"Updates an L7 policy.",
            "",
            "        :param l7policy_id: ID of the l7policy to update",
            "        :param l7policy_updates: Dict containing updated l7policy attributes",
            "        :returns: None",
            "        :raises L7PolicyNotFound: The referenced l7policy was not found",
            "        \"\"\"",
            "        l7policy = None",
            "        try:",
            "            l7policy = self._get_db_obj_until_pending_update(",
            "                self._l7policy_repo, l7policy_id)",
            "        except tenacity.RetryError as e:",
            "            LOG.warning('L7 policy did not go into %s in 60 seconds. '",
            "                        'This either due to an in-progress Octavia upgrade '",
            "                        'or an overloaded and failing database. Assuming '",
            "                        'an upgrade is in progress and continuing.',",
            "                        constants.PENDING_UPDATE)",
            "            l7policy = e.last_attempt.result()",
            "",
            "        listeners = [l7policy.listener]",
            "        load_balancer = l7policy.listener.load_balancer",
            "",
            "        update_l7policy_tf = self._taskflow_load(",
            "            self._l7policy_flows.get_update_l7policy_flow(),",
            "            store={constants.L7POLICY: l7policy,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer,",
            "                   constants.UPDATE_DICT: l7policy_updates})",
            "        with tf_logging.DynamicLoggingListener(update_l7policy_tf,",
            "                                               log=LOG):",
            "            update_l7policy_tf.run()",
            "",
            "    @tenacity.retry(",
            "        retry=tenacity.retry_if_exception_type(db_exceptions.NoResultFound),",
            "        wait=tenacity.wait_incrementing(",
            "            RETRY_INITIAL_DELAY, RETRY_BACKOFF, RETRY_MAX),",
            "        stop=tenacity.stop_after_attempt(RETRY_ATTEMPTS))",
            "    def create_l7rule(self, l7rule_id):",
            "        \"\"\"Creates an L7 Rule.",
            "",
            "        :param l7rule_id: ID of the l7rule to create",
            "        :returns: None",
            "        :raises NoResultFound: Unable to find the object",
            "        \"\"\"",
            "        l7rule = self._l7rule_repo.get(db_apis.get_session(),",
            "                                       id=l7rule_id)",
            "        if not l7rule:",
            "            LOG.warning('Failed to fetch %s %s from DB. Retrying for up to '",
            "                        '60 seconds.', 'l7rule', l7rule_id)",
            "            raise db_exceptions.NoResultFound",
            "",
            "        l7policy = l7rule.l7policy",
            "        listeners = [l7policy.listener]",
            "        load_balancer = l7policy.listener.load_balancer",
            "",
            "        create_l7rule_tf = self._taskflow_load(",
            "            self._l7rule_flows.get_create_l7rule_flow(),",
            "            store={constants.L7RULE: l7rule,",
            "                   constants.L7POLICY: l7policy,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer})",
            "        with tf_logging.DynamicLoggingListener(create_l7rule_tf,",
            "                                               log=LOG):",
            "            create_l7rule_tf.run()",
            "",
            "    def delete_l7rule(self, l7rule_id):",
            "        \"\"\"Deletes an L7 rule.",
            "",
            "        :param l7rule_id: ID of the l7rule to delete",
            "        :returns: None",
            "        :raises L7RuleNotFound: The referenced l7rule was not found",
            "        \"\"\"",
            "        l7rule = self._l7rule_repo.get(db_apis.get_session(),",
            "                                       id=l7rule_id)",
            "        l7policy = l7rule.l7policy",
            "        load_balancer = l7policy.listener.load_balancer",
            "        listeners = [l7policy.listener]",
            "",
            "        delete_l7rule_tf = self._taskflow_load(",
            "            self._l7rule_flows.get_delete_l7rule_flow(),",
            "            store={constants.L7RULE: l7rule,",
            "                   constants.L7POLICY: l7policy,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer})",
            "        with tf_logging.DynamicLoggingListener(delete_l7rule_tf,",
            "                                               log=LOG):",
            "            delete_l7rule_tf.run()",
            "",
            "    def update_l7rule(self, l7rule_id, l7rule_updates):",
            "        \"\"\"Updates an L7 rule.",
            "",
            "        :param l7rule_id: ID of the l7rule to update",
            "        :param l7rule_updates: Dict containing updated l7rule attributes",
            "        :returns: None",
            "        :raises L7RuleNotFound: The referenced l7rule was not found",
            "        \"\"\"",
            "        l7rule = None",
            "        try:",
            "            l7rule = self._get_db_obj_until_pending_update(",
            "                self._l7rule_repo, l7rule_id)",
            "        except tenacity.RetryError as e:",
            "            LOG.warning('L7 rule did not go into %s in 60 seconds. '",
            "                        'This either due to an in-progress Octavia upgrade '",
            "                        'or an overloaded and failing database. Assuming '",
            "                        'an upgrade is in progress and continuing.',",
            "                        constants.PENDING_UPDATE)",
            "            l7rule = e.last_attempt.result()",
            "",
            "        l7policy = l7rule.l7policy",
            "        listeners = [l7policy.listener]",
            "        load_balancer = l7policy.listener.load_balancer",
            "",
            "        update_l7rule_tf = self._taskflow_load(",
            "            self._l7rule_flows.get_update_l7rule_flow(),",
            "            store={constants.L7RULE: l7rule,",
            "                   constants.L7POLICY: l7policy,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer,",
            "                   constants.UPDATE_DICT: l7rule_updates})",
            "        with tf_logging.DynamicLoggingListener(update_l7rule_tf,",
            "                                               log=LOG):",
            "            update_l7rule_tf.run()",
            "",
            "    def _perform_amphora_failover(self, amp, priority):",
            "        \"\"\"Internal method to perform failover operations for an amphora.",
            "",
            "        :param amp: The amphora to failover",
            "        :param priority: The create priority",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        stored_params = {constants.FAILED_AMPHORA: amp,",
            "                         constants.LOADBALANCER_ID: amp.load_balancer_id,",
            "                         constants.BUILD_TYPE_PRIORITY: priority, }",
            "",
            "        if amp.status == constants.DELETED:",
            "            LOG.warning('Amphora %s is marked DELETED in the database but '",
            "                        'was submitted for failover. Deleting it from the '",
            "                        'amphora health table to exclude it from health '",
            "                        'checks and skipping the failover.', amp.id)",
            "            self._amphora_health_repo.delete(db_apis.get_session(),",
            "                                             amphora_id=amp.id)",
            "            return",
            "",
            "        if (CONF.house_keeping.spare_amphora_pool_size == 0) and (",
            "                CONF.nova.enable_anti_affinity is False):",
            "            LOG.warning(\"Failing over amphora with no spares pool may \"",
            "                        \"cause delays in failover times while a new \"",
            "                        \"amphora instance boots.\")",
            "",
            "        # if we run with anti-affinity we need to set the server group",
            "        # as well",
            "        lb = self._amphora_repo.get_lb_for_amphora(",
            "            db_apis.get_session(), amp.id)",
            "        if CONF.nova.enable_anti_affinity and lb:",
            "            stored_params[constants.SERVER_GROUP_ID] = lb.server_group_id",
            "        if lb.flavor_id:",
            "            stored_params[constants.FLAVOR] = (",
            "                self._flavor_repo.get_flavor_metadata_dict(",
            "                    db_apis.get_session(), lb.flavor_id))",
            "        else:",
            "            stored_params[constants.FLAVOR] = {}",
            "",
            "        failover_amphora_tf = self._taskflow_load(",
            "            self._amphora_flows.get_failover_flow(",
            "                role=amp.role, load_balancer=lb),",
            "            store=stored_params)",
            "",
            "        with tf_logging.DynamicLoggingListener(",
            "                failover_amphora_tf, log=LOG,",
            "                hide_inputs_outputs_of=self._exclude_result_logging_tasks):",
            "",
            "            failover_amphora_tf.run()",
            "",
            "    def failover_amphora(self, amphora_id):",
            "        \"\"\"Perform failover operations for an amphora.",
            "",
            "        :param amphora_id: ID for amphora to failover",
            "        :returns: None",
            "        :raises AmphoraNotFound: The referenced amphora was not found",
            "        \"\"\"",
            "        try:",
            "            amp = self._amphora_repo.get(db_apis.get_session(),",
            "                                         id=amphora_id)",
            "            if not amp:",
            "                LOG.warning(\"Could not fetch Amphora %s from DB, ignoring \"",
            "                            \"failover request.\", amphora_id)",
            "                return",
            "            self._perform_amphora_failover(",
            "                amp, constants.LB_CREATE_FAILOVER_PRIORITY)",
            "            if amp.load_balancer_id:",
            "                LOG.info(\"Mark ACTIVE in DB for load balancer id: %s\",",
            "                         amp.load_balancer_id)",
            "                self._lb_repo.update(",
            "                    db_apis.get_session(), amp.load_balancer_id,",
            "                    provisioning_status=constants.ACTIVE)",
            "        except Exception as e:",
            "            try:",
            "                self._lb_repo.update(",
            "                    db_apis.get_session(), amp.load_balancer_id,",
            "                    provisioning_status=constants.ERROR)",
            "            except Exception:",
            "                LOG.error(\"Unable to revert LB status to ERROR.\")",
            "            with excutils.save_and_reraise_exception():",
            "                LOG.error(\"Failover exception: %s\", e)",
            "",
            "    def failover_loadbalancer(self, load_balancer_id):",
            "        \"\"\"Perform failover operations for a load balancer.",
            "",
            "        :param load_balancer_id: ID for load balancer to failover",
            "        :returns: None",
            "        :raises LBNotFound: The referenced load balancer was not found",
            "        \"\"\"",
            "",
            "        # Note: This expects that the load balancer is already in",
            "        #       provisioning_status=PENDING_UPDATE state",
            "        try:",
            "            lb = self._lb_repo.get(db_apis.get_session(),",
            "                                   id=load_balancer_id)",
            "",
            "            # Exclude amphora already deleted",
            "            amps = [a for a in lb.amphorae if a.status != constants.DELETED]",
            "            for amp in amps:",
            "                # failover amphora in backup role",
            "                # Note: this amp may not currently be the backup",
            "                # TODO(johnsom) Change this to query the amp state",
            "                #               once the amp API supports it.",
            "                if amp.role == constants.ROLE_BACKUP:",
            "                    self._perform_amphora_failover(",
            "                        amp, constants.LB_CREATE_ADMIN_FAILOVER_PRIORITY)",
            "",
            "            for amp in amps:",
            "                # failover everyhting else",
            "                if amp.role != constants.ROLE_BACKUP:",
            "                    self._perform_amphora_failover(",
            "                        amp, constants.LB_CREATE_ADMIN_FAILOVER_PRIORITY)",
            "",
            "            self._lb_repo.update(",
            "                db_apis.get_session(), load_balancer_id,",
            "                provisioning_status=constants.ACTIVE)",
            "",
            "        except Exception as e:",
            "            with excutils.save_and_reraise_exception():",
            "                LOG.error(\"LB %(lbid)s failover exception: %(exc)s\",",
            "                          {'lbid': load_balancer_id, 'exc': e})",
            "                self._lb_repo.update(",
            "                    db_apis.get_session(), load_balancer_id,",
            "                    provisioning_status=constants.ERROR)",
            "",
            "    def amphora_cert_rotation(self, amphora_id):",
            "        \"\"\"Perform cert rotation for an amphora.",
            "",
            "        :param amphora_id: ID for amphora to rotate",
            "        :returns: None",
            "        :raises AmphoraNotFound: The referenced amphora was not found",
            "        \"\"\"",
            "",
            "        amp = self._amphora_repo.get(db_apis.get_session(),",
            "                                     id=amphora_id)",
            "        LOG.info(\"Start amphora cert rotation, amphora's id is: %s\", amp.id)",
            "",
            "        certrotation_amphora_tf = self._taskflow_load(",
            "            self._amphora_flows.cert_rotate_amphora_flow(),",
            "            store={constants.AMPHORA: amp,",
            "                   constants.AMPHORA_ID: amp.id})",
            "",
            "        with tf_logging.DynamicLoggingListener(certrotation_amphora_tf,",
            "                                               log=LOG):",
            "            certrotation_amphora_tf.run()",
            "",
            "    def update_amphora_agent_config(self, amphora_id):",
            "        \"\"\"Update the amphora agent configuration.",
            "",
            "        Note: This will update the amphora agent configuration file and",
            "              update the running configuration for mutatable configuration",
            "              items.",
            "",
            "        :param amphora_id: ID of the amphora to update.",
            "        :returns: None",
            "        \"\"\"",
            "        LOG.info(\"Start amphora agent configuration update, amphora's id \"",
            "                 \"is: %s\", amphora_id)",
            "        amp = self._amphora_repo.get(db_apis.get_session(), id=amphora_id)",
            "        lb = self._amphora_repo.get_lb_for_amphora(db_apis.get_session(),",
            "                                                   amphora_id)",
            "        flavor = {}",
            "        if lb.flavor_id:",
            "            flavor = self._flavor_repo.get_flavor_metadata_dict(",
            "                db_apis.get_session(), lb.flavor_id)",
            "",
            "        update_amphora_tf = self._taskflow_load(",
            "            self._amphora_flows.update_amphora_config_flow(),",
            "            store={constants.AMPHORA: amp,",
            "                   constants.FLAVOR: flavor})",
            "",
            "        with tf_logging.DynamicLoggingListener(update_amphora_tf,",
            "                                               log=LOG):",
            "            update_amphora_tf.run()"
        ],
        "afterPatchFile": [
            "# Copyright 2015 Hewlett-Packard Development Company, L.P.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "# http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "",
            "",
            "from oslo_config import cfg",
            "from oslo_log import log as logging",
            "from oslo_utils import excutils",
            "from sqlalchemy.orm import exc as db_exceptions",
            "from taskflow.listeners import logging as tf_logging",
            "import tenacity",
            "",
            "from octavia.common import base_taskflow",
            "from octavia.common import constants",
            "from octavia.controller.worker.flows import amphora_flows",
            "from octavia.controller.worker.flows import health_monitor_flows",
            "from octavia.controller.worker.flows import l7policy_flows",
            "from octavia.controller.worker.flows import l7rule_flows",
            "from octavia.controller.worker.flows import listener_flows",
            "from octavia.controller.worker.flows import load_balancer_flows",
            "from octavia.controller.worker.flows import member_flows",
            "from octavia.controller.worker.flows import pool_flows",
            "from octavia.db import api as db_apis",
            "from octavia.db import repositories as repo",
            "",
            "CONF = cfg.CONF",
            "LOG = logging.getLogger(__name__)",
            "",
            "RETRY_ATTEMPTS = 15",
            "RETRY_INITIAL_DELAY = 1",
            "RETRY_BACKOFF = 1",
            "RETRY_MAX = 5",
            "",
            "",
            "def _is_provisioning_status_pending_update(lb_obj):",
            "    return not lb_obj.provisioning_status == constants.PENDING_UPDATE",
            "",
            "",
            "class ControllerWorker(base_taskflow.BaseTaskFlowEngine):",
            "",
            "    def __init__(self):",
            "",
            "        self._amphora_flows = amphora_flows.AmphoraFlows()",
            "        self._health_monitor_flows = health_monitor_flows.HealthMonitorFlows()",
            "        self._lb_flows = load_balancer_flows.LoadBalancerFlows()",
            "        self._listener_flows = listener_flows.ListenerFlows()",
            "        self._member_flows = member_flows.MemberFlows()",
            "        self._pool_flows = pool_flows.PoolFlows()",
            "        self._l7policy_flows = l7policy_flows.L7PolicyFlows()",
            "        self._l7rule_flows = l7rule_flows.L7RuleFlows()",
            "",
            "        self._amphora_repo = repo.AmphoraRepository()",
            "        self._amphora_health_repo = repo.AmphoraHealthRepository()",
            "        self._health_mon_repo = repo.HealthMonitorRepository()",
            "        self._lb_repo = repo.LoadBalancerRepository()",
            "        self._listener_repo = repo.ListenerRepository()",
            "        self._member_repo = repo.MemberRepository()",
            "        self._pool_repo = repo.PoolRepository()",
            "        self._l7policy_repo = repo.L7PolicyRepository()",
            "        self._l7rule_repo = repo.L7RuleRepository()",
            "        self._flavor_repo = repo.FlavorRepository()",
            "",
            "        super(ControllerWorker, self).__init__()",
            "",
            "    @tenacity.retry(",
            "        retry=(",
            "            tenacity.retry_if_result(_is_provisioning_status_pending_update) |",
            "            tenacity.retry_if_exception_type()),",
            "        wait=tenacity.wait_incrementing(",
            "            RETRY_INITIAL_DELAY, RETRY_BACKOFF, RETRY_MAX),",
            "        stop=tenacity.stop_after_attempt(RETRY_ATTEMPTS))",
            "    def _get_db_obj_until_pending_update(self, repo, id):",
            "",
            "        return repo.get(db_apis.get_session(), id=id)",
            "",
            "    def create_amphora(self):",
            "        \"\"\"Creates an Amphora.",
            "",
            "        This is used to create spare amphora.",
            "",
            "        :returns: amphora_id",
            "        \"\"\"",
            "        try:",
            "            create_amp_tf = self._taskflow_load(",
            "                self._amphora_flows.get_create_amphora_flow(),",
            "                store={constants.BUILD_TYPE_PRIORITY:",
            "                       constants.LB_CREATE_SPARES_POOL_PRIORITY,",
            "                       constants.FLAVOR: None}",
            "            )",
            "            with tf_logging.DynamicLoggingListener(create_amp_tf, log=LOG):",
            "                create_amp_tf.run()",
            "",
            "            return create_amp_tf.storage.fetch('amphora')",
            "        except Exception as e:",
            "            LOG.error('Failed to create an amphora due to: {}'.format(str(e)))",
            "",
            "    def delete_amphora(self, amphora_id):",
            "        \"\"\"Deletes an existing Amphora.",
            "",
            "        :param amphora_id: ID of the amphora to delete",
            "        :returns: None",
            "        :raises AmphoraNotFound: The referenced Amphora was not found",
            "        \"\"\"",
            "        amphora = self._amphora_repo.get(db_apis.get_session(),",
            "                                         id=amphora_id)",
            "        delete_amp_tf = self._taskflow_load(self._amphora_flows.",
            "                                            get_delete_amphora_flow(),",
            "                                            store={constants.AMPHORA: amphora})",
            "        with tf_logging.DynamicLoggingListener(delete_amp_tf,",
            "                                               log=LOG):",
            "            delete_amp_tf.run()",
            "",
            "    @tenacity.retry(",
            "        retry=tenacity.retry_if_exception_type(db_exceptions.NoResultFound),",
            "        wait=tenacity.wait_incrementing(",
            "            RETRY_INITIAL_DELAY, RETRY_BACKOFF, RETRY_MAX),",
            "        stop=tenacity.stop_after_attempt(RETRY_ATTEMPTS))",
            "    def create_health_monitor(self, health_monitor_id):",
            "        \"\"\"Creates a health monitor.",
            "",
            "        :param pool_id: ID of the pool to create a health monitor on",
            "        :returns: None",
            "        :raises NoResultFound: Unable to find the object",
            "        \"\"\"",
            "        health_mon = self._health_mon_repo.get(db_apis.get_session(),",
            "                                               id=health_monitor_id)",
            "        if not health_mon:",
            "            LOG.warning('Failed to fetch %s %s from DB. Retrying for up to '",
            "                        '60 seconds.', 'health_monitor', health_monitor_id)",
            "            raise db_exceptions.NoResultFound",
            "",
            "        pool = health_mon.pool",
            "        listeners = pool.listeners",
            "        pool.health_monitor = health_mon",
            "        load_balancer = pool.load_balancer",
            "",
            "        create_hm_tf = self._taskflow_load(",
            "            self._health_monitor_flows.get_create_health_monitor_flow(),",
            "            store={constants.HEALTH_MON: health_mon,",
            "                   constants.POOL: pool,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer})",
            "        with tf_logging.DynamicLoggingListener(create_hm_tf,",
            "                                               log=LOG):",
            "            create_hm_tf.run()",
            "",
            "    def delete_health_monitor(self, health_monitor_id):",
            "        \"\"\"Deletes a health monitor.",
            "",
            "        :param pool_id: ID of the pool to delete its health monitor",
            "        :returns: None",
            "        :raises HMNotFound: The referenced health monitor was not found",
            "        \"\"\"",
            "        health_mon = self._health_mon_repo.get(db_apis.get_session(),",
            "                                               id=health_monitor_id)",
            "",
            "        pool = health_mon.pool",
            "        listeners = pool.listeners",
            "        load_balancer = pool.load_balancer",
            "",
            "        delete_hm_tf = self._taskflow_load(",
            "            self._health_monitor_flows.get_delete_health_monitor_flow(),",
            "            store={constants.HEALTH_MON: health_mon,",
            "                   constants.POOL: pool,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer})",
            "        with tf_logging.DynamicLoggingListener(delete_hm_tf,",
            "                                               log=LOG):",
            "            delete_hm_tf.run()",
            "",
            "    def update_health_monitor(self, health_monitor_id, health_monitor_updates):",
            "        \"\"\"Updates a health monitor.",
            "",
            "        :param pool_id: ID of the pool to have it's health monitor updated",
            "        :param health_monitor_updates: Dict containing updated health monitor",
            "        :returns: None",
            "        :raises HMNotFound: The referenced health monitor was not found",
            "        \"\"\"",
            "        health_mon = None",
            "        try:",
            "            health_mon = self._get_db_obj_until_pending_update(",
            "                self._health_mon_repo, health_monitor_id)",
            "        except tenacity.RetryError as e:",
            "            LOG.warning('Health monitor did not go into %s in 60 seconds. '",
            "                        'This either due to an in-progress Octavia upgrade '",
            "                        'or an overloaded and failing database. Assuming '",
            "                        'an upgrade is in progress and continuing.',",
            "                        constants.PENDING_UPDATE)",
            "            health_mon = e.last_attempt.result()",
            "",
            "        pool = health_mon.pool",
            "        listeners = pool.listeners",
            "        pool.health_monitor = health_mon",
            "        load_balancer = pool.load_balancer",
            "",
            "        update_hm_tf = self._taskflow_load(",
            "            self._health_monitor_flows.get_update_health_monitor_flow(),",
            "            store={constants.HEALTH_MON: health_mon,",
            "                   constants.POOL: pool,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer,",
            "                   constants.UPDATE_DICT: health_monitor_updates})",
            "        with tf_logging.DynamicLoggingListener(update_hm_tf,",
            "                                               log=LOG):",
            "            update_hm_tf.run()",
            "",
            "    @tenacity.retry(",
            "        retry=tenacity.retry_if_exception_type(db_exceptions.NoResultFound),",
            "        wait=tenacity.wait_incrementing(",
            "            RETRY_INITIAL_DELAY, RETRY_BACKOFF, RETRY_MAX),",
            "        stop=tenacity.stop_after_attempt(RETRY_ATTEMPTS))",
            "    def create_listener(self, listener_id):",
            "        \"\"\"Creates a listener.",
            "",
            "        :param listener_id: ID of the listener to create",
            "        :returns: None",
            "        :raises NoResultFound: Unable to find the object",
            "        \"\"\"",
            "        listener = self._listener_repo.get(db_apis.get_session(),",
            "                                           id=listener_id)",
            "        if not listener:",
            "            LOG.warning('Failed to fetch %s %s from DB. Retrying for up to '",
            "                        '60 seconds.', 'listener', listener_id)",
            "            raise db_exceptions.NoResultFound",
            "",
            "        load_balancer = listener.load_balancer",
            "",
            "        create_listener_tf = self._taskflow_load(self._listener_flows.",
            "                                                 get_create_listener_flow(),",
            "                                                 store={constants.LOADBALANCER:",
            "                                                        load_balancer,",
            "                                                        constants.LISTENERS:",
            "                                                            [listener]})",
            "        with tf_logging.DynamicLoggingListener(create_listener_tf,",
            "                                               log=LOG):",
            "            create_listener_tf.run()",
            "",
            "    def delete_listener(self, listener_id):",
            "        \"\"\"Deletes a listener.",
            "",
            "        :param listener_id: ID of the listener to delete",
            "        :returns: None",
            "        :raises ListenerNotFound: The referenced listener was not found",
            "        \"\"\"",
            "        listener = self._listener_repo.get(db_apis.get_session(),",
            "                                           id=listener_id)",
            "        load_balancer = listener.load_balancer",
            "",
            "        delete_listener_tf = self._taskflow_load(",
            "            self._listener_flows.get_delete_listener_flow(),",
            "            store={constants.LOADBALANCER: load_balancer,",
            "                   constants.LISTENER: listener})",
            "        with tf_logging.DynamicLoggingListener(delete_listener_tf,",
            "                                               log=LOG):",
            "            delete_listener_tf.run()",
            "",
            "    def update_listener(self, listener_id, listener_updates):",
            "        \"\"\"Updates a listener.",
            "",
            "        :param listener_id: ID of the listener to update",
            "        :param listener_updates: Dict containing updated listener attributes",
            "        :returns: None",
            "        :raises ListenerNotFound: The referenced listener was not found",
            "        \"\"\"",
            "        listener = None",
            "        try:",
            "            listener = self._get_db_obj_until_pending_update(",
            "                self._listener_repo, listener_id)",
            "        except tenacity.RetryError as e:",
            "            LOG.warning('Listener did not go into %s in 60 seconds. '",
            "                        'This either due to an in-progress Octavia upgrade '",
            "                        'or an overloaded and failing database. Assuming '",
            "                        'an upgrade is in progress and continuing.',",
            "                        constants.PENDING_UPDATE)",
            "            listener = e.last_attempt.result()",
            "",
            "        load_balancer = listener.load_balancer",
            "",
            "        update_listener_tf = self._taskflow_load(self._listener_flows.",
            "                                                 get_update_listener_flow(),",
            "                                                 store={constants.LISTENER:",
            "                                                        listener,",
            "                                                        constants.LOADBALANCER:",
            "                                                            load_balancer,",
            "                                                        constants.UPDATE_DICT:",
            "                                                            listener_updates,",
            "                                                        constants.LISTENERS:",
            "                                                            [listener]})",
            "        with tf_logging.DynamicLoggingListener(update_listener_tf, log=LOG):",
            "            update_listener_tf.run()",
            "",
            "    @tenacity.retry(",
            "        retry=tenacity.retry_if_exception_type(db_exceptions.NoResultFound),",
            "        wait=tenacity.wait_incrementing(",
            "            RETRY_INITIAL_DELAY, RETRY_BACKOFF, RETRY_MAX),",
            "        stop=tenacity.stop_after_attempt(RETRY_ATTEMPTS))",
            "    def create_load_balancer(self, load_balancer_id, flavor=None):",
            "        \"\"\"Creates a load balancer by allocating Amphorae.",
            "",
            "        First tries to allocate an existing Amphora in READY state.",
            "        If none are available it will attempt to build one specifically",
            "        for this load balancer.",
            "",
            "        :param load_balancer_id: ID of the load balancer to create",
            "        :returns: None",
            "        :raises NoResultFound: Unable to find the object",
            "        \"\"\"",
            "        lb = self._lb_repo.get(db_apis.get_session(), id=load_balancer_id)",
            "        if not lb:",
            "            LOG.warning('Failed to fetch %s %s from DB. Retrying for up to '",
            "                        '60 seconds.', 'load_balancer', load_balancer_id)",
            "            raise db_exceptions.NoResultFound",
            "",
            "        # TODO(johnsom) convert this to octavia_lib constant flavor",
            "        # once octavia is transitioned to use octavia_lib",
            "        store = {constants.LOADBALANCER_ID: load_balancer_id,",
            "                 constants.BUILD_TYPE_PRIORITY:",
            "                 constants.LB_CREATE_NORMAL_PRIORITY,",
            "                 constants.FLAVOR: flavor}",
            "",
            "        topology = lb.topology",
            "",
            "        store[constants.UPDATE_DICT] = {",
            "            constants.TOPOLOGY: topology",
            "        }",
            "",
            "        create_lb_flow = self._lb_flows.get_create_load_balancer_flow(",
            "            topology=topology, listeners=lb.listeners)",
            "",
            "        create_lb_tf = self._taskflow_load(create_lb_flow, store=store)",
            "        with tf_logging.DynamicLoggingListener(create_lb_tf, log=LOG):",
            "            create_lb_tf.run()",
            "",
            "    def delete_load_balancer(self, load_balancer_id, cascade=False):",
            "        \"\"\"Deletes a load balancer by de-allocating Amphorae.",
            "",
            "        :param load_balancer_id: ID of the load balancer to delete",
            "        :returns: None",
            "        :raises LBNotFound: The referenced load balancer was not found",
            "        \"\"\"",
            "        lb = self._lb_repo.get(db_apis.get_session(),",
            "                               id=load_balancer_id)",
            "",
            "        if cascade:",
            "            (flow,",
            "             store) = self._lb_flows.get_cascade_delete_load_balancer_flow(lb)",
            "        else:",
            "            (flow, store) = self._lb_flows.get_delete_load_balancer_flow(lb)",
            "        store.update({constants.LOADBALANCER: lb,",
            "                      constants.SERVER_GROUP_ID: lb.server_group_id})",
            "        delete_lb_tf = self._taskflow_load(flow, store=store)",
            "",
            "        with tf_logging.DynamicLoggingListener(delete_lb_tf,",
            "                                               log=LOG):",
            "            delete_lb_tf.run()",
            "",
            "    def update_load_balancer(self, load_balancer_id, load_balancer_updates):",
            "        \"\"\"Updates a load balancer.",
            "",
            "        :param load_balancer_id: ID of the load balancer to update",
            "        :param load_balancer_updates: Dict containing updated load balancer",
            "        :returns: None",
            "        :raises LBNotFound: The referenced load balancer was not found",
            "        \"\"\"",
            "        lb = None",
            "        try:",
            "            lb = self._get_db_obj_until_pending_update(",
            "                self._lb_repo, load_balancer_id)",
            "        except tenacity.RetryError as e:",
            "            LOG.warning('Load balancer did not go into %s in 60 seconds. '",
            "                        'This either due to an in-progress Octavia upgrade '",
            "                        'or an overloaded and failing database. Assuming '",
            "                        'an upgrade is in progress and continuing.',",
            "                        constants.PENDING_UPDATE)",
            "            lb = e.last_attempt.result()",
            "",
            "        listeners, _ = self._listener_repo.get_all(",
            "            db_apis.get_session(),",
            "            load_balancer_id=load_balancer_id)",
            "",
            "        update_lb_tf = self._taskflow_load(",
            "            self._lb_flows.get_update_load_balancer_flow(),",
            "            store={constants.LOADBALANCER: lb,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.UPDATE_DICT: load_balancer_updates})",
            "",
            "        with tf_logging.DynamicLoggingListener(update_lb_tf,",
            "                                               log=LOG):",
            "            update_lb_tf.run()",
            "",
            "    @tenacity.retry(",
            "        retry=tenacity.retry_if_exception_type(db_exceptions.NoResultFound),",
            "        wait=tenacity.wait_incrementing(",
            "            RETRY_INITIAL_DELAY, RETRY_BACKOFF, RETRY_MAX),",
            "        stop=tenacity.stop_after_attempt(RETRY_ATTEMPTS))",
            "    def create_member(self, member_id):",
            "        \"\"\"Creates a pool member.",
            "",
            "        :param member_id: ID of the member to create",
            "        :returns: None",
            "        :raises NoSuitablePool: Unable to find the node pool",
            "        \"\"\"",
            "        member = self._member_repo.get(db_apis.get_session(),",
            "                                       id=member_id)",
            "        if not member:",
            "            LOG.warning('Failed to fetch %s %s from DB. Retrying for up to '",
            "                        '60 seconds.', 'member', member_id)",
            "            raise db_exceptions.NoResultFound",
            "",
            "        pool = member.pool",
            "        listeners = pool.listeners",
            "        load_balancer = pool.load_balancer",
            "",
            "        create_member_tf = self._taskflow_load(self._member_flows.",
            "                                               get_create_member_flow(),",
            "                                               store={constants.MEMBER: member,",
            "                                                      constants.LISTENERS:",
            "                                                          listeners,",
            "                                                      constants.LOADBALANCER:",
            "                                                          load_balancer,",
            "                                                      constants.POOL: pool})",
            "        with tf_logging.DynamicLoggingListener(create_member_tf,",
            "                                               log=LOG):",
            "            create_member_tf.run()",
            "",
            "    def delete_member(self, member_id):",
            "        \"\"\"Deletes a pool member.",
            "",
            "        :param member_id: ID of the member to delete",
            "        :returns: None",
            "        :raises MemberNotFound: The referenced member was not found",
            "        \"\"\"",
            "        member = self._member_repo.get(db_apis.get_session(),",
            "                                       id=member_id)",
            "        pool = member.pool",
            "        listeners = pool.listeners",
            "        load_balancer = pool.load_balancer",
            "",
            "        delete_member_tf = self._taskflow_load(",
            "            self._member_flows.get_delete_member_flow(),",
            "            store={constants.MEMBER: member, constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer, constants.POOL: pool}",
            "        )",
            "        with tf_logging.DynamicLoggingListener(delete_member_tf,",
            "                                               log=LOG):",
            "            delete_member_tf.run()",
            "",
            "    def batch_update_members(self, old_member_ids, new_member_ids,",
            "                             updated_members):",
            "        old_members = [self._member_repo.get(db_apis.get_session(), id=mid)",
            "                       for mid in old_member_ids]",
            "        new_members = [self._member_repo.get(db_apis.get_session(), id=mid)",
            "                       for mid in new_member_ids]",
            "        updated_members = [",
            "            (self._member_repo.get(db_apis.get_session(), id=m.get('id')), m)",
            "            for m in updated_members]",
            "        if old_members:",
            "            pool = old_members[0].pool",
            "        elif new_members:",
            "            pool = new_members[0].pool",
            "        else:",
            "            pool = updated_members[0][0].pool",
            "        listeners = pool.listeners",
            "        load_balancer = pool.load_balancer",
            "",
            "        batch_update_members_tf = self._taskflow_load(",
            "            self._member_flows.get_batch_update_members_flow(",
            "                old_members, new_members, updated_members),",
            "            store={constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer,",
            "                   constants.POOL: pool})",
            "        with tf_logging.DynamicLoggingListener(batch_update_members_tf,",
            "                                               log=LOG):",
            "            batch_update_members_tf.run()",
            "",
            "    def update_member(self, member_id, member_updates):",
            "        \"\"\"Updates a pool member.",
            "",
            "        :param member_id: ID of the member to update",
            "        :param member_updates: Dict containing updated member attributes",
            "        :returns: None",
            "        :raises MemberNotFound: The referenced member was not found",
            "        \"\"\"",
            "        member = None",
            "        try:",
            "            member = self._get_db_obj_until_pending_update(",
            "                self._member_repo, member_id)",
            "        except tenacity.RetryError as e:",
            "            LOG.warning('Member did not go into %s in 60 seconds. '",
            "                        'This either due to an in-progress Octavia upgrade '",
            "                        'or an overloaded and failing database. Assuming '",
            "                        'an upgrade is in progress and continuing.',",
            "                        constants.PENDING_UPDATE)",
            "            member = e.last_attempt.result()",
            "",
            "        pool = member.pool",
            "        listeners = pool.listeners",
            "        load_balancer = pool.load_balancer",
            "",
            "        update_member_tf = self._taskflow_load(self._member_flows.",
            "                                               get_update_member_flow(),",
            "                                               store={constants.MEMBER: member,",
            "                                                      constants.LISTENERS:",
            "                                                          listeners,",
            "                                                      constants.LOADBALANCER:",
            "                                                          load_balancer,",
            "                                                      constants.POOL:",
            "                                                          pool,",
            "                                                      constants.UPDATE_DICT:",
            "                                                          member_updates})",
            "        with tf_logging.DynamicLoggingListener(update_member_tf,",
            "                                               log=LOG):",
            "            update_member_tf.run()",
            "",
            "    @tenacity.retry(",
            "        retry=tenacity.retry_if_exception_type(db_exceptions.NoResultFound),",
            "        wait=tenacity.wait_incrementing(",
            "            RETRY_INITIAL_DELAY, RETRY_BACKOFF, RETRY_MAX),",
            "        stop=tenacity.stop_after_attempt(RETRY_ATTEMPTS))",
            "    def create_pool(self, pool_id):",
            "        \"\"\"Creates a node pool.",
            "",
            "        :param pool_id: ID of the pool to create",
            "        :returns: None",
            "        :raises NoResultFound: Unable to find the object",
            "        \"\"\"",
            "        pool = self._pool_repo.get(db_apis.get_session(),",
            "                                   id=pool_id)",
            "        if not pool:",
            "            LOG.warning('Failed to fetch %s %s from DB. Retrying for up to '",
            "                        '60 seconds.', 'pool', pool_id)",
            "            raise db_exceptions.NoResultFound",
            "",
            "        listeners = pool.listeners",
            "        load_balancer = pool.load_balancer",
            "",
            "        create_pool_tf = self._taskflow_load(self._pool_flows.",
            "                                             get_create_pool_flow(),",
            "                                             store={constants.POOL: pool,",
            "                                                    constants.LISTENERS:",
            "                                                        listeners,",
            "                                                    constants.LOADBALANCER:",
            "                                                        load_balancer})",
            "        with tf_logging.DynamicLoggingListener(create_pool_tf,",
            "                                               log=LOG):",
            "            create_pool_tf.run()",
            "",
            "    def delete_pool(self, pool_id):",
            "        \"\"\"Deletes a node pool.",
            "",
            "        :param pool_id: ID of the pool to delete",
            "        :returns: None",
            "        :raises PoolNotFound: The referenced pool was not found",
            "        \"\"\"",
            "        pool = self._pool_repo.get(db_apis.get_session(),",
            "                                   id=pool_id)",
            "",
            "        load_balancer = pool.load_balancer",
            "        listeners = pool.listeners",
            "",
            "        delete_pool_tf = self._taskflow_load(",
            "            self._pool_flows.get_delete_pool_flow(),",
            "            store={constants.POOL: pool, constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer})",
            "        with tf_logging.DynamicLoggingListener(delete_pool_tf,",
            "                                               log=LOG):",
            "            delete_pool_tf.run()",
            "",
            "    def update_pool(self, pool_id, pool_updates):",
            "        \"\"\"Updates a node pool.",
            "",
            "        :param pool_id: ID of the pool to update",
            "        :param pool_updates: Dict containing updated pool attributes",
            "        :returns: None",
            "        :raises PoolNotFound: The referenced pool was not found",
            "        \"\"\"",
            "        pool = None",
            "        try:",
            "            pool = self._get_db_obj_until_pending_update(",
            "                self._pool_repo, pool_id)",
            "        except tenacity.RetryError as e:",
            "            LOG.warning('Pool did not go into %s in 60 seconds. '",
            "                        'This either due to an in-progress Octavia upgrade '",
            "                        'or an overloaded and failing database. Assuming '",
            "                        'an upgrade is in progress and continuing.',",
            "                        constants.PENDING_UPDATE)",
            "            pool = e.last_attempt.result()",
            "",
            "        listeners = pool.listeners",
            "        load_balancer = pool.load_balancer",
            "",
            "        update_pool_tf = self._taskflow_load(self._pool_flows.",
            "                                             get_update_pool_flow(),",
            "                                             store={constants.POOL: pool,",
            "                                                    constants.LISTENERS:",
            "                                                        listeners,",
            "                                                    constants.LOADBALANCER:",
            "                                                        load_balancer,",
            "                                                    constants.UPDATE_DICT:",
            "                                                        pool_updates})",
            "        with tf_logging.DynamicLoggingListener(update_pool_tf,",
            "                                               log=LOG):",
            "            update_pool_tf.run()",
            "",
            "    @tenacity.retry(",
            "        retry=tenacity.retry_if_exception_type(db_exceptions.NoResultFound),",
            "        wait=tenacity.wait_incrementing(",
            "            RETRY_INITIAL_DELAY, RETRY_BACKOFF, RETRY_MAX),",
            "        stop=tenacity.stop_after_attempt(RETRY_ATTEMPTS))",
            "    def create_l7policy(self, l7policy_id):",
            "        \"\"\"Creates an L7 Policy.",
            "",
            "        :param l7policy_id: ID of the l7policy to create",
            "        :returns: None",
            "        :raises NoResultFound: Unable to find the object",
            "        \"\"\"",
            "        l7policy = self._l7policy_repo.get(db_apis.get_session(),",
            "                                           id=l7policy_id)",
            "        if not l7policy:",
            "            LOG.warning('Failed to fetch %s %s from DB. Retrying for up to '",
            "                        '60 seconds.', 'l7policy', l7policy_id)",
            "            raise db_exceptions.NoResultFound",
            "",
            "        listeners = [l7policy.listener]",
            "        load_balancer = l7policy.listener.load_balancer",
            "",
            "        create_l7policy_tf = self._taskflow_load(",
            "            self._l7policy_flows.get_create_l7policy_flow(),",
            "            store={constants.L7POLICY: l7policy,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer})",
            "        with tf_logging.DynamicLoggingListener(create_l7policy_tf,",
            "                                               log=LOG):",
            "            create_l7policy_tf.run()",
            "",
            "    def delete_l7policy(self, l7policy_id):",
            "        \"\"\"Deletes an L7 policy.",
            "",
            "        :param l7policy_id: ID of the l7policy to delete",
            "        :returns: None",
            "        :raises L7PolicyNotFound: The referenced l7policy was not found",
            "        \"\"\"",
            "        l7policy = self._l7policy_repo.get(db_apis.get_session(),",
            "                                           id=l7policy_id)",
            "",
            "        load_balancer = l7policy.listener.load_balancer",
            "        listeners = [l7policy.listener]",
            "",
            "        delete_l7policy_tf = self._taskflow_load(",
            "            self._l7policy_flows.get_delete_l7policy_flow(),",
            "            store={constants.L7POLICY: l7policy,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer})",
            "        with tf_logging.DynamicLoggingListener(delete_l7policy_tf,",
            "                                               log=LOG):",
            "            delete_l7policy_tf.run()",
            "",
            "    def update_l7policy(self, l7policy_id, l7policy_updates):",
            "        \"\"\"Updates an L7 policy.",
            "",
            "        :param l7policy_id: ID of the l7policy to update",
            "        :param l7policy_updates: Dict containing updated l7policy attributes",
            "        :returns: None",
            "        :raises L7PolicyNotFound: The referenced l7policy was not found",
            "        \"\"\"",
            "        l7policy = None",
            "        try:",
            "            l7policy = self._get_db_obj_until_pending_update(",
            "                self._l7policy_repo, l7policy_id)",
            "        except tenacity.RetryError as e:",
            "            LOG.warning('L7 policy did not go into %s in 60 seconds. '",
            "                        'This either due to an in-progress Octavia upgrade '",
            "                        'or an overloaded and failing database. Assuming '",
            "                        'an upgrade is in progress and continuing.',",
            "                        constants.PENDING_UPDATE)",
            "            l7policy = e.last_attempt.result()",
            "",
            "        listeners = [l7policy.listener]",
            "        load_balancer = l7policy.listener.load_balancer",
            "",
            "        update_l7policy_tf = self._taskflow_load(",
            "            self._l7policy_flows.get_update_l7policy_flow(),",
            "            store={constants.L7POLICY: l7policy,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer,",
            "                   constants.UPDATE_DICT: l7policy_updates})",
            "        with tf_logging.DynamicLoggingListener(update_l7policy_tf,",
            "                                               log=LOG):",
            "            update_l7policy_tf.run()",
            "",
            "    @tenacity.retry(",
            "        retry=tenacity.retry_if_exception_type(db_exceptions.NoResultFound),",
            "        wait=tenacity.wait_incrementing(",
            "            RETRY_INITIAL_DELAY, RETRY_BACKOFF, RETRY_MAX),",
            "        stop=tenacity.stop_after_attempt(RETRY_ATTEMPTS))",
            "    def create_l7rule(self, l7rule_id):",
            "        \"\"\"Creates an L7 Rule.",
            "",
            "        :param l7rule_id: ID of the l7rule to create",
            "        :returns: None",
            "        :raises NoResultFound: Unable to find the object",
            "        \"\"\"",
            "        l7rule = self._l7rule_repo.get(db_apis.get_session(),",
            "                                       id=l7rule_id)",
            "        if not l7rule:",
            "            LOG.warning('Failed to fetch %s %s from DB. Retrying for up to '",
            "                        '60 seconds.', 'l7rule', l7rule_id)",
            "            raise db_exceptions.NoResultFound",
            "",
            "        l7policy = l7rule.l7policy",
            "        listeners = [l7policy.listener]",
            "        load_balancer = l7policy.listener.load_balancer",
            "",
            "        create_l7rule_tf = self._taskflow_load(",
            "            self._l7rule_flows.get_create_l7rule_flow(),",
            "            store={constants.L7RULE: l7rule,",
            "                   constants.L7POLICY: l7policy,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer})",
            "        with tf_logging.DynamicLoggingListener(create_l7rule_tf,",
            "                                               log=LOG):",
            "            create_l7rule_tf.run()",
            "",
            "    def delete_l7rule(self, l7rule_id):",
            "        \"\"\"Deletes an L7 rule.",
            "",
            "        :param l7rule_id: ID of the l7rule to delete",
            "        :returns: None",
            "        :raises L7RuleNotFound: The referenced l7rule was not found",
            "        \"\"\"",
            "        l7rule = self._l7rule_repo.get(db_apis.get_session(),",
            "                                       id=l7rule_id)",
            "        l7policy = l7rule.l7policy",
            "        load_balancer = l7policy.listener.load_balancer",
            "        listeners = [l7policy.listener]",
            "",
            "        delete_l7rule_tf = self._taskflow_load(",
            "            self._l7rule_flows.get_delete_l7rule_flow(),",
            "            store={constants.L7RULE: l7rule,",
            "                   constants.L7POLICY: l7policy,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer})",
            "        with tf_logging.DynamicLoggingListener(delete_l7rule_tf,",
            "                                               log=LOG):",
            "            delete_l7rule_tf.run()",
            "",
            "    def update_l7rule(self, l7rule_id, l7rule_updates):",
            "        \"\"\"Updates an L7 rule.",
            "",
            "        :param l7rule_id: ID of the l7rule to update",
            "        :param l7rule_updates: Dict containing updated l7rule attributes",
            "        :returns: None",
            "        :raises L7RuleNotFound: The referenced l7rule was not found",
            "        \"\"\"",
            "        l7rule = None",
            "        try:",
            "            l7rule = self._get_db_obj_until_pending_update(",
            "                self._l7rule_repo, l7rule_id)",
            "        except tenacity.RetryError as e:",
            "            LOG.warning('L7 rule did not go into %s in 60 seconds. '",
            "                        'This either due to an in-progress Octavia upgrade '",
            "                        'or an overloaded and failing database. Assuming '",
            "                        'an upgrade is in progress and continuing.',",
            "                        constants.PENDING_UPDATE)",
            "            l7rule = e.last_attempt.result()",
            "",
            "        l7policy = l7rule.l7policy",
            "        listeners = [l7policy.listener]",
            "        load_balancer = l7policy.listener.load_balancer",
            "",
            "        update_l7rule_tf = self._taskflow_load(",
            "            self._l7rule_flows.get_update_l7rule_flow(),",
            "            store={constants.L7RULE: l7rule,",
            "                   constants.L7POLICY: l7policy,",
            "                   constants.LISTENERS: listeners,",
            "                   constants.LOADBALANCER: load_balancer,",
            "                   constants.UPDATE_DICT: l7rule_updates})",
            "        with tf_logging.DynamicLoggingListener(update_l7rule_tf,",
            "                                               log=LOG):",
            "            update_l7rule_tf.run()",
            "",
            "    def _perform_amphora_failover(self, amp, priority):",
            "        \"\"\"Internal method to perform failover operations for an amphora.",
            "",
            "        :param amp: The amphora to failover",
            "        :param priority: The create priority",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        stored_params = {constants.FAILED_AMPHORA: amp,",
            "                         constants.LOADBALANCER_ID: amp.load_balancer_id,",
            "                         constants.BUILD_TYPE_PRIORITY: priority, }",
            "",
            "        if amp.status == constants.DELETED:",
            "            LOG.warning('Amphora %s is marked DELETED in the database but '",
            "                        'was submitted for failover. Deleting it from the '",
            "                        'amphora health table to exclude it from health '",
            "                        'checks and skipping the failover.', amp.id)",
            "            self._amphora_health_repo.delete(db_apis.get_session(),",
            "                                             amphora_id=amp.id)",
            "            return",
            "",
            "        if (CONF.house_keeping.spare_amphora_pool_size == 0) and (",
            "                CONF.nova.enable_anti_affinity is False):",
            "            LOG.warning(\"Failing over amphora with no spares pool may \"",
            "                        \"cause delays in failover times while a new \"",
            "                        \"amphora instance boots.\")",
            "",
            "        # if we run with anti-affinity we need to set the server group",
            "        # as well",
            "        lb = self._amphora_repo.get_lb_for_amphora(",
            "            db_apis.get_session(), amp.id)",
            "        if CONF.nova.enable_anti_affinity and lb:",
            "            stored_params[constants.SERVER_GROUP_ID] = lb.server_group_id",
            "        if lb.flavor_id:",
            "            stored_params[constants.FLAVOR] = (",
            "                self._flavor_repo.get_flavor_metadata_dict(",
            "                    db_apis.get_session(), lb.flavor_id))",
            "        else:",
            "            stored_params[constants.FLAVOR] = {}",
            "",
            "        failover_amphora_tf = self._taskflow_load(",
            "            self._amphora_flows.get_failover_flow(",
            "                role=amp.role, load_balancer=lb),",
            "            store=stored_params)",
            "",
            "        with tf_logging.DynamicLoggingListener(failover_amphora_tf, log=LOG):",
            "            failover_amphora_tf.run()",
            "",
            "    def failover_amphora(self, amphora_id):",
            "        \"\"\"Perform failover operations for an amphora.",
            "",
            "        :param amphora_id: ID for amphora to failover",
            "        :returns: None",
            "        :raises AmphoraNotFound: The referenced amphora was not found",
            "        \"\"\"",
            "        try:",
            "            amp = self._amphora_repo.get(db_apis.get_session(),",
            "                                         id=amphora_id)",
            "            if not amp:",
            "                LOG.warning(\"Could not fetch Amphora %s from DB, ignoring \"",
            "                            \"failover request.\", amphora_id)",
            "                return",
            "            self._perform_amphora_failover(",
            "                amp, constants.LB_CREATE_FAILOVER_PRIORITY)",
            "            if amp.load_balancer_id:",
            "                LOG.info(\"Mark ACTIVE in DB for load balancer id: %s\",",
            "                         amp.load_balancer_id)",
            "                self._lb_repo.update(",
            "                    db_apis.get_session(), amp.load_balancer_id,",
            "                    provisioning_status=constants.ACTIVE)",
            "        except Exception as e:",
            "            try:",
            "                self._lb_repo.update(",
            "                    db_apis.get_session(), amp.load_balancer_id,",
            "                    provisioning_status=constants.ERROR)",
            "            except Exception:",
            "                LOG.error(\"Unable to revert LB status to ERROR.\")",
            "            with excutils.save_and_reraise_exception():",
            "                LOG.error(\"Failover exception: %s\", e)",
            "",
            "    def failover_loadbalancer(self, load_balancer_id):",
            "        \"\"\"Perform failover operations for a load balancer.",
            "",
            "        :param load_balancer_id: ID for load balancer to failover",
            "        :returns: None",
            "        :raises LBNotFound: The referenced load balancer was not found",
            "        \"\"\"",
            "",
            "        # Note: This expects that the load balancer is already in",
            "        #       provisioning_status=PENDING_UPDATE state",
            "        try:",
            "            lb = self._lb_repo.get(db_apis.get_session(),",
            "                                   id=load_balancer_id)",
            "",
            "            # Exclude amphora already deleted",
            "            amps = [a for a in lb.amphorae if a.status != constants.DELETED]",
            "            for amp in amps:",
            "                # failover amphora in backup role",
            "                # Note: this amp may not currently be the backup",
            "                # TODO(johnsom) Change this to query the amp state",
            "                #               once the amp API supports it.",
            "                if amp.role == constants.ROLE_BACKUP:",
            "                    self._perform_amphora_failover(",
            "                        amp, constants.LB_CREATE_ADMIN_FAILOVER_PRIORITY)",
            "",
            "            for amp in amps:",
            "                # failover everyhting else",
            "                if amp.role != constants.ROLE_BACKUP:",
            "                    self._perform_amphora_failover(",
            "                        amp, constants.LB_CREATE_ADMIN_FAILOVER_PRIORITY)",
            "",
            "            self._lb_repo.update(",
            "                db_apis.get_session(), load_balancer_id,",
            "                provisioning_status=constants.ACTIVE)",
            "",
            "        except Exception as e:",
            "            with excutils.save_and_reraise_exception():",
            "                LOG.error(\"LB %(lbid)s failover exception: %(exc)s\",",
            "                          {'lbid': load_balancer_id, 'exc': e})",
            "                self._lb_repo.update(",
            "                    db_apis.get_session(), load_balancer_id,",
            "                    provisioning_status=constants.ERROR)",
            "",
            "    def amphora_cert_rotation(self, amphora_id):",
            "        \"\"\"Perform cert rotation for an amphora.",
            "",
            "        :param amphora_id: ID for amphora to rotate",
            "        :returns: None",
            "        :raises AmphoraNotFound: The referenced amphora was not found",
            "        \"\"\"",
            "",
            "        amp = self._amphora_repo.get(db_apis.get_session(),",
            "                                     id=amphora_id)",
            "        LOG.info(\"Start amphora cert rotation, amphora's id is: %s\", amp.id)",
            "",
            "        certrotation_amphora_tf = self._taskflow_load(",
            "            self._amphora_flows.cert_rotate_amphora_flow(),",
            "            store={constants.AMPHORA: amp,",
            "                   constants.AMPHORA_ID: amp.id})",
            "",
            "        with tf_logging.DynamicLoggingListener(certrotation_amphora_tf,",
            "                                               log=LOG):",
            "            certrotation_amphora_tf.run()",
            "",
            "    def update_amphora_agent_config(self, amphora_id):",
            "        \"\"\"Update the amphora agent configuration.",
            "",
            "        Note: This will update the amphora agent configuration file and",
            "              update the running configuration for mutatable configuration",
            "              items.",
            "",
            "        :param amphora_id: ID of the amphora to update.",
            "        :returns: None",
            "        \"\"\"",
            "        LOG.info(\"Start amphora agent configuration update, amphora's id \"",
            "                 \"is: %s\", amphora_id)",
            "        amp = self._amphora_repo.get(db_apis.get_session(), id=amphora_id)",
            "        lb = self._amphora_repo.get_lb_for_amphora(db_apis.get_session(),",
            "                                                   amphora_id)",
            "        flavor = {}",
            "        if lb.flavor_id:",
            "            flavor = self._flavor_repo.get_flavor_metadata_dict(",
            "                db_apis.get_session(), lb.flavor_id)",
            "",
            "        update_amphora_tf = self._taskflow_load(",
            "            self._amphora_flows.update_amphora_config_flow(),",
            "            store={constants.AMPHORA: amp,",
            "                   constants.FLAVOR: flavor})",
            "",
            "        with tf_logging.DynamicLoggingListener(update_amphora_tf,",
            "                                               log=LOG):",
            "            update_amphora_tf.run()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "74": [
                "ControllerWorker",
                "__init__"
            ],
            "75": [
                "ControllerWorker",
                "__init__"
            ],
            "76": [
                "ControllerWorker",
                "__init__"
            ],
            "77": [
                "ControllerWorker",
                "__init__"
            ],
            "78": [
                "ControllerWorker",
                "__init__"
            ],
            "79": [
                "ControllerWorker",
                "__init__"
            ],
            "80": [
                "ControllerWorker",
                "__init__"
            ],
            "81": [
                "ControllerWorker",
                "__init__"
            ],
            "82": [
                "ControllerWorker",
                "__init__"
            ],
            "83": [
                "ControllerWorker",
                "__init__"
            ],
            "84": [
                "ControllerWorker",
                "__init__"
            ],
            "85": [
                "ControllerWorker",
                "__init__"
            ],
            "86": [
                "ControllerWorker",
                "__init__"
            ],
            "87": [
                "ControllerWorker",
                "__init__"
            ],
            "88": [
                "ControllerWorker",
                "__init__"
            ],
            "89": [
                "ControllerWorker",
                "__init__"
            ],
            "90": [
                "ControllerWorker",
                "__init__"
            ],
            "118": [
                "ControllerWorker",
                "create_amphora"
            ],
            "119": [
                "ControllerWorker",
                "create_amphora"
            ],
            "120": [
                "ControllerWorker",
                "create_amphora"
            ],
            "121": [
                "ControllerWorker",
                "create_amphora"
            ],
            "362": [
                "ControllerWorker",
                "create_load_balancer"
            ],
            "363": [
                "ControllerWorker",
                "create_load_balancer"
            ],
            "364": [
                "ControllerWorker",
                "create_load_balancer"
            ],
            "860": [
                "ControllerWorker",
                "_perform_amphora_failover"
            ],
            "861": [
                "ControllerWorker",
                "_perform_amphora_failover"
            ],
            "862": [
                "ControllerWorker",
                "_perform_amphora_failover"
            ],
            "863": [
                "ControllerWorker",
                "_perform_amphora_failover"
            ]
        },
        "addLocation": []
    },
    "octavia/controller/worker/tasks/amphora_driver_tasks.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " # under the License."
            },
            "1": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " #"
            },
            "2": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+from cryptography import fernet"
            },
            "4": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " from oslo_config import cfg"
            },
            "5": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from oslo_log import log as logging"
            },
            "6": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " import six"
            },
            "7": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " from octavia.amphorae.backends.agent import agent_jinja_cfg"
            },
            "8": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " from octavia.amphorae.driver_exceptions import exceptions as driver_except"
            },
            "9": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from octavia.common import constants"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 27,
                "PatchRowcode": "+from octavia.common import utils"
            },
            "11": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from octavia.controller.worker import task_utils as task_utilities"
            },
            "12": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " from octavia.db import api as db_apis"
            },
            "13": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " from octavia.db import repositories as repo"
            },
            "14": {
                "beforePatchRowNumber": 273,
                "afterPatchRowNumber": 275,
                "PatchRowcode": "     def execute(self, amphora, server_pem):"
            },
            "15": {
                "beforePatchRowNumber": 274,
                "afterPatchRowNumber": 276,
                "PatchRowcode": "         \"\"\"Execute cert_update_amphora routine.\"\"\""
            },
            "16": {
                "beforePatchRowNumber": 275,
                "afterPatchRowNumber": 277,
                "PatchRowcode": "         LOG.debug(\"Upload cert in amphora REST driver\")"
            },
            "17": {
                "beforePatchRowNumber": 276,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.amphora_driver.upload_cert_amp(amphora, server_pem)"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 278,
                "PatchRowcode": "+        key = utils.get_six_compatible_server_certs_key_passphrase()"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 279,
                "PatchRowcode": "+        fer = fernet.Fernet(key)"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 280,
                "PatchRowcode": "+        self.amphora_driver.upload_cert_amp(amphora, fer.decrypt(server_pem))"
            },
            "21": {
                "beforePatchRowNumber": 277,
                "afterPatchRowNumber": 281,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 278,
                "afterPatchRowNumber": 282,
                "PatchRowcode": " "
            },
            "23": {
                "beforePatchRowNumber": 279,
                "afterPatchRowNumber": 283,
                "PatchRowcode": " class AmphoraUpdateVRRPInterface(BaseAmphoraTask):"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015 Hewlett-Packard Development Company, L.P.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "# http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "",
            "from oslo_config import cfg",
            "from oslo_log import log as logging",
            "import six",
            "from stevedore import driver as stevedore_driver",
            "from taskflow import task",
            "from taskflow.types import failure",
            "",
            "from octavia.amphorae.backends.agent import agent_jinja_cfg",
            "from octavia.amphorae.driver_exceptions import exceptions as driver_except",
            "from octavia.common import constants",
            "from octavia.controller.worker import task_utils as task_utilities",
            "from octavia.db import api as db_apis",
            "from octavia.db import repositories as repo",
            "",
            "CONF = cfg.CONF",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "class BaseAmphoraTask(task.Task):",
            "    \"\"\"Base task to load drivers common to the tasks.\"\"\"",
            "",
            "    def __init__(self, **kwargs):",
            "        super(BaseAmphoraTask, self).__init__(**kwargs)",
            "        self.amphora_driver = stevedore_driver.DriverManager(",
            "            namespace='octavia.amphora.drivers',",
            "            name=CONF.controller_worker.amphora_driver,",
            "            invoke_on_load=True",
            "        ).driver",
            "        self.amphora_repo = repo.AmphoraRepository()",
            "        self.listener_repo = repo.ListenerRepository()",
            "        self.loadbalancer_repo = repo.LoadBalancerRepository()",
            "        self.task_utils = task_utilities.TaskUtils()",
            "",
            "",
            "class AmpListenersUpdate(BaseAmphoraTask):",
            "    \"\"\"Task to update the listeners on one amphora.\"\"\"",
            "",
            "    def execute(self, listeners, amphora_index, amphorae, timeout_dict=()):",
            "        # Note, we don't want this to cause a revert as it may be used",
            "        # in a failover flow with both amps failing. Skip it and let",
            "        # health manager fix it.",
            "        try:",
            "            self.amphora_driver.update_amphora_listeners(",
            "                listeners, amphora_index, amphorae, timeout_dict)",
            "        except Exception as e:",
            "            amphora_id = amphorae[amphora_index].id",
            "            LOG.error('Failed to update listeners on amphora %s. Skipping '",
            "                      'this amphora as it is failing to update due to: %s',",
            "                      amphora_id, str(e))",
            "            self.amphora_repo.update(db_apis.get_session(), amphora_id,",
            "                                     status=constants.ERROR)",
            "",
            "",
            "class ListenersUpdate(BaseAmphoraTask):",
            "    \"\"\"Task to update amphora with all specified listeners' configurations.\"\"\"",
            "",
            "    def execute(self, loadbalancer, listeners):",
            "        \"\"\"Execute updates per listener for an amphora.\"\"\"",
            "        for listener in listeners:",
            "            listener.load_balancer = loadbalancer",
            "            self.amphora_driver.update(listener, loadbalancer.vip)",
            "",
            "    def revert(self, loadbalancer, *args, **kwargs):",
            "        \"\"\"Handle failed listeners updates.\"\"\"",
            "",
            "        LOG.warning(\"Reverting listeners updates.\")",
            "",
            "        for listener in loadbalancer.listeners:",
            "            self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "        return None",
            "",
            "",
            "class ListenerStop(BaseAmphoraTask):",
            "    \"\"\"Task to stop the listener on the vip.\"\"\"",
            "",
            "    def execute(self, loadbalancer, listener):",
            "        \"\"\"Execute listener stop routines for an amphora.\"\"\"",
            "        self.amphora_driver.stop(listener, loadbalancer.vip)",
            "        LOG.debug(\"Stopped the listener on the vip\")",
            "",
            "    def revert(self, listener, *args, **kwargs):",
            "        \"\"\"Handle a failed listener stop.\"\"\"",
            "",
            "        LOG.warning(\"Reverting listener stop.\")",
            "",
            "        self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "        return None",
            "",
            "",
            "class ListenerStart(BaseAmphoraTask):",
            "    \"\"\"Task to start the listener on the vip.\"\"\"",
            "",
            "    def execute(self, loadbalancer, listener):",
            "        \"\"\"Execute listener start routines for an amphora.\"\"\"",
            "        self.amphora_driver.start(listener, loadbalancer.vip)",
            "        LOG.debug(\"Started the listener on the vip\")",
            "",
            "    def revert(self, listener, *args, **kwargs):",
            "        \"\"\"Handle a failed listener start.\"\"\"",
            "",
            "        LOG.warning(\"Reverting listener start.\")",
            "",
            "        self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "        return None",
            "",
            "",
            "class ListenersStart(BaseAmphoraTask):",
            "    \"\"\"Task to start all listeners on the vip.\"\"\"",
            "",
            "    def execute(self, loadbalancer, listeners, amphora=None):",
            "        \"\"\"Execute listener start routines for listeners on an amphora.\"\"\"",
            "        for listener in listeners:",
            "            self.amphora_driver.start(listener, loadbalancer.vip, amphora)",
            "        LOG.debug(\"Started the listeners on the vip\")",
            "",
            "    def revert(self, listeners, *args, **kwargs):",
            "        \"\"\"Handle failed listeners starts.\"\"\"",
            "",
            "        LOG.warning(\"Reverting listeners starts.\")",
            "        for listener in listeners:",
            "            self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "        return None",
            "",
            "",
            "class ListenerDelete(BaseAmphoraTask):",
            "    \"\"\"Task to delete the listener on the vip.\"\"\"",
            "",
            "    def execute(self, loadbalancer, listener):",
            "        \"\"\"Execute listener delete routines for an amphora.\"\"\"",
            "        self.amphora_driver.delete(listener, loadbalancer.vip)",
            "        LOG.debug(\"Deleted the listener on the vip\")",
            "",
            "    def revert(self, listener, *args, **kwargs):",
            "        \"\"\"Handle a failed listener delete.\"\"\"",
            "",
            "        LOG.warning(\"Reverting listener delete.\")",
            "",
            "        self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "",
            "class AmphoraGetInfo(BaseAmphoraTask):",
            "    \"\"\"Task to get information on an amphora.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Execute get_info routine for an amphora.\"\"\"",
            "        self.amphora_driver.get_info(amphora)",
            "",
            "",
            "class AmphoraGetDiagnostics(BaseAmphoraTask):",
            "    \"\"\"Task to get diagnostics on the amphora and the loadbalancers.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Execute get_diagnostic routine for an amphora.\"\"\"",
            "        self.amphora_driver.get_diagnostics(amphora)",
            "",
            "",
            "class AmphoraFinalize(BaseAmphoraTask):",
            "    \"\"\"Task to finalize the amphora before any listeners are configured.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Execute finalize_amphora routine.\"\"\"",
            "        self.amphora_driver.finalize_amphora(amphora)",
            "        LOG.debug(\"Finalized the amphora.\")",
            "",
            "    def revert(self, result, amphora, *args, **kwargs):",
            "        \"\"\"Handle a failed amphora finalize.\"\"\"",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "        LOG.warning(\"Reverting amphora finalize.\")",
            "        self.task_utils.mark_amphora_status_error(amphora.id)",
            "",
            "",
            "class AmphoraPostNetworkPlug(BaseAmphoraTask):",
            "    \"\"\"Task to notify the amphora post network plug.\"\"\"",
            "",
            "    def execute(self, amphora, ports):",
            "        \"\"\"Execute post_network_plug routine.\"\"\"",
            "        for port in ports:",
            "            self.amphora_driver.post_network_plug(amphora, port)",
            "            LOG.debug(\"post_network_plug called on compute instance \"",
            "                      \"%(compute_id)s for port %(port_id)s\",",
            "                      {\"compute_id\": amphora.compute_id, \"port_id\": port.id})",
            "",
            "    def revert(self, result, amphora, *args, **kwargs):",
            "        \"\"\"Handle a failed post network plug.\"\"\"",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "        LOG.warning(\"Reverting post network plug.\")",
            "        self.task_utils.mark_amphora_status_error(amphora.id)",
            "",
            "",
            "class AmphoraePostNetworkPlug(BaseAmphoraTask):",
            "    \"\"\"Task to notify the amphorae post network plug.\"\"\"",
            "",
            "    def execute(self, loadbalancer, added_ports):",
            "        \"\"\"Execute post_network_plug routine.\"\"\"",
            "        amp_post_plug = AmphoraPostNetworkPlug()",
            "        for amphora in loadbalancer.amphorae:",
            "            if amphora.id in added_ports:",
            "                amp_post_plug.execute(amphora, added_ports[amphora.id])",
            "",
            "    def revert(self, result, loadbalancer, added_ports, *args, **kwargs):",
            "        \"\"\"Handle a failed post network plug.\"\"\"",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "        LOG.warning(\"Reverting post network plug.\")",
            "        for amphora in six.moves.filter(",
            "            lambda amp: amp.status == constants.AMPHORA_ALLOCATED,",
            "                loadbalancer.amphorae):",
            "",
            "            self.task_utils.mark_amphora_status_error(amphora.id)",
            "",
            "",
            "class AmphoraPostVIPPlug(BaseAmphoraTask):",
            "    \"\"\"Task to notify the amphora post VIP plug.\"\"\"",
            "",
            "    def execute(self, amphora, loadbalancer, amphorae_network_config):",
            "        \"\"\"Execute post_vip_routine.\"\"\"",
            "        self.amphora_driver.post_vip_plug(",
            "            amphora, loadbalancer, amphorae_network_config)",
            "        LOG.debug(\"Notified amphora of vip plug\")",
            "",
            "    def revert(self, result, amphora, loadbalancer, *args, **kwargs):",
            "        \"\"\"Handle a failed amphora vip plug notification.\"\"\"",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "        LOG.warning(\"Reverting post vip plug.\")",
            "        self.task_utils.mark_amphora_status_error(amphora.id)",
            "        self.task_utils.mark_loadbalancer_prov_status_error(loadbalancer.id)",
            "",
            "",
            "class AmphoraePostVIPPlug(BaseAmphoraTask):",
            "    \"\"\"Task to notify the amphorae post VIP plug.\"\"\"",
            "",
            "    def execute(self, loadbalancer, amphorae_network_config):",
            "        \"\"\"Execute post_vip_plug across the amphorae.\"\"\"",
            "        amp_post_vip_plug = AmphoraPostVIPPlug()",
            "        for amphora in loadbalancer.amphorae:",
            "            amp_post_vip_plug.execute(amphora,",
            "                                      loadbalancer,",
            "                                      amphorae_network_config)",
            "",
            "    def revert(self, result, loadbalancer, *args, **kwargs):",
            "        \"\"\"Handle a failed amphora vip plug notification.\"\"\"",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "        LOG.warning(\"Reverting amphorae post vip plug.\")",
            "        self.task_utils.mark_loadbalancer_prov_status_error(loadbalancer.id)",
            "",
            "",
            "class AmphoraCertUpload(BaseAmphoraTask):",
            "    \"\"\"Upload a certificate to the amphora.\"\"\"",
            "",
            "    def execute(self, amphora, server_pem):",
            "        \"\"\"Execute cert_update_amphora routine.\"\"\"",
            "        LOG.debug(\"Upload cert in amphora REST driver\")",
            "        self.amphora_driver.upload_cert_amp(amphora, server_pem)",
            "",
            "",
            "class AmphoraUpdateVRRPInterface(BaseAmphoraTask):",
            "    \"\"\"Task to get and update the VRRP interface device name from amphora.\"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Execute post_vip_routine.\"\"\"",
            "        amps = []",
            "        timeout_dict = {",
            "            constants.CONN_MAX_RETRIES:",
            "                CONF.haproxy_amphora.active_connection_max_retries,",
            "            constants.CONN_RETRY_INTERVAL:",
            "                CONF.haproxy_amphora.active_connection_rety_interval}",
            "        for amp in six.moves.filter(",
            "            lambda amp: amp.status == constants.AMPHORA_ALLOCATED,",
            "                loadbalancer.amphorae):",
            "",
            "            try:",
            "                interface = self.amphora_driver.get_vrrp_interface(",
            "                    amp, timeout_dict=timeout_dict)",
            "            except Exception as e:",
            "                # This can occur when an active/standby LB has no listener",
            "                LOG.error('Failed to get amphora VRRP interface on amphora '",
            "                          '%s. Skipping this amphora as it is failing due to: '",
            "                          '%s', amp.id, str(e))",
            "                self.amphora_repo.update(db_apis.get_session(), amp.id,",
            "                                         status=constants.ERROR)",
            "                continue",
            "",
            "            self.amphora_repo.update(db_apis.get_session(), amp.id,",
            "                                     vrrp_interface=interface)",
            "            amps.append(self.amphora_repo.get(db_apis.get_session(),",
            "                                              id=amp.id))",
            "        loadbalancer.amphorae = amps",
            "        return loadbalancer",
            "",
            "    def revert(self, result, loadbalancer, *args, **kwargs):",
            "        \"\"\"Handle a failed amphora vip plug notification.\"\"\"",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "        LOG.warning(\"Reverting Get Amphora VRRP Interface.\")",
            "        for amp in six.moves.filter(",
            "            lambda amp: amp.status == constants.AMPHORA_ALLOCATED,",
            "                loadbalancer.amphorae):",
            "",
            "            try:",
            "                self.amphora_repo.update(db_apis.get_session(), amp.id,",
            "                                         vrrp_interface=None)",
            "            except Exception as e:",
            "                LOG.error(\"Failed to update amphora %(amp)s \"",
            "                          \"VRRP interface to None due to: %(except)s\",",
            "                          {'amp': amp.id, 'except': e})",
            "",
            "",
            "class AmphoraVRRPUpdate(BaseAmphoraTask):",
            "    \"\"\"Task to update the VRRP configuration of the loadbalancer amphorae.\"\"\"",
            "",
            "    def execute(self, loadbalancer, amphorae_network_config):",
            "        \"\"\"Execute update_vrrp_conf.\"\"\"",
            "        self.amphora_driver.update_vrrp_conf(loadbalancer,",
            "                                             amphorae_network_config)",
            "        LOG.debug(\"Uploaded VRRP configuration of loadbalancer %s amphorae\",",
            "                  loadbalancer.id)",
            "",
            "",
            "class AmphoraVRRPStop(BaseAmphoraTask):",
            "    \"\"\"Task to stop keepalived of all amphorae of a LB.\"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        self.amphora_driver.stop_vrrp_service(loadbalancer)",
            "        LOG.debug(\"Stopped VRRP of loadbalancer %s amphorae\",",
            "                  loadbalancer.id)",
            "",
            "",
            "class AmphoraVRRPStart(BaseAmphoraTask):",
            "    \"\"\"Task to start keepalived of all amphorae of a LB.\"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        self.amphora_driver.start_vrrp_service(loadbalancer)",
            "        LOG.debug(\"Started VRRP of loadbalancer %s amphorae\",",
            "                  loadbalancer.id)",
            "",
            "",
            "class AmphoraComputeConnectivityWait(BaseAmphoraTask):",
            "    \"\"\"Task to wait for the compute instance to be up.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Execute get_info routine for an amphora until it responds.\"\"\"",
            "        try:",
            "            amp_info = self.amphora_driver.get_info(amphora)",
            "            LOG.debug('Successfuly connected to amphora %s: %s',",
            "                      amphora.id, amp_info)",
            "        except driver_except.TimeOutException:",
            "            LOG.error(\"Amphora compute instance failed to become reachable. \"",
            "                      \"This either means the compute driver failed to fully \"",
            "                      \"boot the instance inside the timeout interval or the \"",
            "                      \"instance is not reachable via the lb-mgmt-net.\")",
            "            self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                     status=constants.ERROR)",
            "            raise",
            "",
            "",
            "class AmphoraConfigUpdate(BaseAmphoraTask):",
            "    \"\"\"Task to push a new amphora agent configuration to the amphroa.\"\"\"",
            "",
            "    def execute(self, amphora, flavor):",
            "        # Extract any flavor based settings",
            "        if flavor:",
            "            topology = flavor.get(constants.LOADBALANCER_TOPOLOGY,",
            "                                  CONF.controller_worker.loadbalancer_topology)",
            "        else:",
            "            topology = CONF.controller_worker.loadbalancer_topology",
            "",
            "        # Build the amphora agent config",
            "        agent_cfg_tmpl = agent_jinja_cfg.AgentJinjaTemplater()",
            "        agent_config = agent_cfg_tmpl.build_agent_config(amphora.id, topology)",
            "",
            "        # Push the new configuration to the amphroa",
            "        try:",
            "            self.amphora_driver.update_amphora_agent_config(amphora,",
            "                                                            agent_config)",
            "        except driver_except.AmpDriverNotImplementedError:",
            "            LOG.error('Amphora {} does not support agent configuration '",
            "                      'update. Please update the amphora image for this '",
            "                      'amphora. Skipping.'.format(amphora.id))"
        ],
        "afterPatchFile": [
            "# Copyright 2015 Hewlett-Packard Development Company, L.P.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "# http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "",
            "from cryptography import fernet",
            "from oslo_config import cfg",
            "from oslo_log import log as logging",
            "import six",
            "from stevedore import driver as stevedore_driver",
            "from taskflow import task",
            "from taskflow.types import failure",
            "",
            "from octavia.amphorae.backends.agent import agent_jinja_cfg",
            "from octavia.amphorae.driver_exceptions import exceptions as driver_except",
            "from octavia.common import constants",
            "from octavia.common import utils",
            "from octavia.controller.worker import task_utils as task_utilities",
            "from octavia.db import api as db_apis",
            "from octavia.db import repositories as repo",
            "",
            "CONF = cfg.CONF",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "class BaseAmphoraTask(task.Task):",
            "    \"\"\"Base task to load drivers common to the tasks.\"\"\"",
            "",
            "    def __init__(self, **kwargs):",
            "        super(BaseAmphoraTask, self).__init__(**kwargs)",
            "        self.amphora_driver = stevedore_driver.DriverManager(",
            "            namespace='octavia.amphora.drivers',",
            "            name=CONF.controller_worker.amphora_driver,",
            "            invoke_on_load=True",
            "        ).driver",
            "        self.amphora_repo = repo.AmphoraRepository()",
            "        self.listener_repo = repo.ListenerRepository()",
            "        self.loadbalancer_repo = repo.LoadBalancerRepository()",
            "        self.task_utils = task_utilities.TaskUtils()",
            "",
            "",
            "class AmpListenersUpdate(BaseAmphoraTask):",
            "    \"\"\"Task to update the listeners on one amphora.\"\"\"",
            "",
            "    def execute(self, listeners, amphora_index, amphorae, timeout_dict=()):",
            "        # Note, we don't want this to cause a revert as it may be used",
            "        # in a failover flow with both amps failing. Skip it and let",
            "        # health manager fix it.",
            "        try:",
            "            self.amphora_driver.update_amphora_listeners(",
            "                listeners, amphora_index, amphorae, timeout_dict)",
            "        except Exception as e:",
            "            amphora_id = amphorae[amphora_index].id",
            "            LOG.error('Failed to update listeners on amphora %s. Skipping '",
            "                      'this amphora as it is failing to update due to: %s',",
            "                      amphora_id, str(e))",
            "            self.amphora_repo.update(db_apis.get_session(), amphora_id,",
            "                                     status=constants.ERROR)",
            "",
            "",
            "class ListenersUpdate(BaseAmphoraTask):",
            "    \"\"\"Task to update amphora with all specified listeners' configurations.\"\"\"",
            "",
            "    def execute(self, loadbalancer, listeners):",
            "        \"\"\"Execute updates per listener for an amphora.\"\"\"",
            "        for listener in listeners:",
            "            listener.load_balancer = loadbalancer",
            "            self.amphora_driver.update(listener, loadbalancer.vip)",
            "",
            "    def revert(self, loadbalancer, *args, **kwargs):",
            "        \"\"\"Handle failed listeners updates.\"\"\"",
            "",
            "        LOG.warning(\"Reverting listeners updates.\")",
            "",
            "        for listener in loadbalancer.listeners:",
            "            self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "        return None",
            "",
            "",
            "class ListenerStop(BaseAmphoraTask):",
            "    \"\"\"Task to stop the listener on the vip.\"\"\"",
            "",
            "    def execute(self, loadbalancer, listener):",
            "        \"\"\"Execute listener stop routines for an amphora.\"\"\"",
            "        self.amphora_driver.stop(listener, loadbalancer.vip)",
            "        LOG.debug(\"Stopped the listener on the vip\")",
            "",
            "    def revert(self, listener, *args, **kwargs):",
            "        \"\"\"Handle a failed listener stop.\"\"\"",
            "",
            "        LOG.warning(\"Reverting listener stop.\")",
            "",
            "        self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "        return None",
            "",
            "",
            "class ListenerStart(BaseAmphoraTask):",
            "    \"\"\"Task to start the listener on the vip.\"\"\"",
            "",
            "    def execute(self, loadbalancer, listener):",
            "        \"\"\"Execute listener start routines for an amphora.\"\"\"",
            "        self.amphora_driver.start(listener, loadbalancer.vip)",
            "        LOG.debug(\"Started the listener on the vip\")",
            "",
            "    def revert(self, listener, *args, **kwargs):",
            "        \"\"\"Handle a failed listener start.\"\"\"",
            "",
            "        LOG.warning(\"Reverting listener start.\")",
            "",
            "        self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "        return None",
            "",
            "",
            "class ListenersStart(BaseAmphoraTask):",
            "    \"\"\"Task to start all listeners on the vip.\"\"\"",
            "",
            "    def execute(self, loadbalancer, listeners, amphora=None):",
            "        \"\"\"Execute listener start routines for listeners on an amphora.\"\"\"",
            "        for listener in listeners:",
            "            self.amphora_driver.start(listener, loadbalancer.vip, amphora)",
            "        LOG.debug(\"Started the listeners on the vip\")",
            "",
            "    def revert(self, listeners, *args, **kwargs):",
            "        \"\"\"Handle failed listeners starts.\"\"\"",
            "",
            "        LOG.warning(\"Reverting listeners starts.\")",
            "        for listener in listeners:",
            "            self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "        return None",
            "",
            "",
            "class ListenerDelete(BaseAmphoraTask):",
            "    \"\"\"Task to delete the listener on the vip.\"\"\"",
            "",
            "    def execute(self, loadbalancer, listener):",
            "        \"\"\"Execute listener delete routines for an amphora.\"\"\"",
            "        self.amphora_driver.delete(listener, loadbalancer.vip)",
            "        LOG.debug(\"Deleted the listener on the vip\")",
            "",
            "    def revert(self, listener, *args, **kwargs):",
            "        \"\"\"Handle a failed listener delete.\"\"\"",
            "",
            "        LOG.warning(\"Reverting listener delete.\")",
            "",
            "        self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "",
            "class AmphoraGetInfo(BaseAmphoraTask):",
            "    \"\"\"Task to get information on an amphora.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Execute get_info routine for an amphora.\"\"\"",
            "        self.amphora_driver.get_info(amphora)",
            "",
            "",
            "class AmphoraGetDiagnostics(BaseAmphoraTask):",
            "    \"\"\"Task to get diagnostics on the amphora and the loadbalancers.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Execute get_diagnostic routine for an amphora.\"\"\"",
            "        self.amphora_driver.get_diagnostics(amphora)",
            "",
            "",
            "class AmphoraFinalize(BaseAmphoraTask):",
            "    \"\"\"Task to finalize the amphora before any listeners are configured.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Execute finalize_amphora routine.\"\"\"",
            "        self.amphora_driver.finalize_amphora(amphora)",
            "        LOG.debug(\"Finalized the amphora.\")",
            "",
            "    def revert(self, result, amphora, *args, **kwargs):",
            "        \"\"\"Handle a failed amphora finalize.\"\"\"",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "        LOG.warning(\"Reverting amphora finalize.\")",
            "        self.task_utils.mark_amphora_status_error(amphora.id)",
            "",
            "",
            "class AmphoraPostNetworkPlug(BaseAmphoraTask):",
            "    \"\"\"Task to notify the amphora post network plug.\"\"\"",
            "",
            "    def execute(self, amphora, ports):",
            "        \"\"\"Execute post_network_plug routine.\"\"\"",
            "        for port in ports:",
            "            self.amphora_driver.post_network_plug(amphora, port)",
            "            LOG.debug(\"post_network_plug called on compute instance \"",
            "                      \"%(compute_id)s for port %(port_id)s\",",
            "                      {\"compute_id\": amphora.compute_id, \"port_id\": port.id})",
            "",
            "    def revert(self, result, amphora, *args, **kwargs):",
            "        \"\"\"Handle a failed post network plug.\"\"\"",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "        LOG.warning(\"Reverting post network plug.\")",
            "        self.task_utils.mark_amphora_status_error(amphora.id)",
            "",
            "",
            "class AmphoraePostNetworkPlug(BaseAmphoraTask):",
            "    \"\"\"Task to notify the amphorae post network plug.\"\"\"",
            "",
            "    def execute(self, loadbalancer, added_ports):",
            "        \"\"\"Execute post_network_plug routine.\"\"\"",
            "        amp_post_plug = AmphoraPostNetworkPlug()",
            "        for amphora in loadbalancer.amphorae:",
            "            if amphora.id in added_ports:",
            "                amp_post_plug.execute(amphora, added_ports[amphora.id])",
            "",
            "    def revert(self, result, loadbalancer, added_ports, *args, **kwargs):",
            "        \"\"\"Handle a failed post network plug.\"\"\"",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "        LOG.warning(\"Reverting post network plug.\")",
            "        for amphora in six.moves.filter(",
            "            lambda amp: amp.status == constants.AMPHORA_ALLOCATED,",
            "                loadbalancer.amphorae):",
            "",
            "            self.task_utils.mark_amphora_status_error(amphora.id)",
            "",
            "",
            "class AmphoraPostVIPPlug(BaseAmphoraTask):",
            "    \"\"\"Task to notify the amphora post VIP plug.\"\"\"",
            "",
            "    def execute(self, amphora, loadbalancer, amphorae_network_config):",
            "        \"\"\"Execute post_vip_routine.\"\"\"",
            "        self.amphora_driver.post_vip_plug(",
            "            amphora, loadbalancer, amphorae_network_config)",
            "        LOG.debug(\"Notified amphora of vip plug\")",
            "",
            "    def revert(self, result, amphora, loadbalancer, *args, **kwargs):",
            "        \"\"\"Handle a failed amphora vip plug notification.\"\"\"",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "        LOG.warning(\"Reverting post vip plug.\")",
            "        self.task_utils.mark_amphora_status_error(amphora.id)",
            "        self.task_utils.mark_loadbalancer_prov_status_error(loadbalancer.id)",
            "",
            "",
            "class AmphoraePostVIPPlug(BaseAmphoraTask):",
            "    \"\"\"Task to notify the amphorae post VIP plug.\"\"\"",
            "",
            "    def execute(self, loadbalancer, amphorae_network_config):",
            "        \"\"\"Execute post_vip_plug across the amphorae.\"\"\"",
            "        amp_post_vip_plug = AmphoraPostVIPPlug()",
            "        for amphora in loadbalancer.amphorae:",
            "            amp_post_vip_plug.execute(amphora,",
            "                                      loadbalancer,",
            "                                      amphorae_network_config)",
            "",
            "    def revert(self, result, loadbalancer, *args, **kwargs):",
            "        \"\"\"Handle a failed amphora vip plug notification.\"\"\"",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "        LOG.warning(\"Reverting amphorae post vip plug.\")",
            "        self.task_utils.mark_loadbalancer_prov_status_error(loadbalancer.id)",
            "",
            "",
            "class AmphoraCertUpload(BaseAmphoraTask):",
            "    \"\"\"Upload a certificate to the amphora.\"\"\"",
            "",
            "    def execute(self, amphora, server_pem):",
            "        \"\"\"Execute cert_update_amphora routine.\"\"\"",
            "        LOG.debug(\"Upload cert in amphora REST driver\")",
            "        key = utils.get_six_compatible_server_certs_key_passphrase()",
            "        fer = fernet.Fernet(key)",
            "        self.amphora_driver.upload_cert_amp(amphora, fer.decrypt(server_pem))",
            "",
            "",
            "class AmphoraUpdateVRRPInterface(BaseAmphoraTask):",
            "    \"\"\"Task to get and update the VRRP interface device name from amphora.\"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Execute post_vip_routine.\"\"\"",
            "        amps = []",
            "        timeout_dict = {",
            "            constants.CONN_MAX_RETRIES:",
            "                CONF.haproxy_amphora.active_connection_max_retries,",
            "            constants.CONN_RETRY_INTERVAL:",
            "                CONF.haproxy_amphora.active_connection_rety_interval}",
            "        for amp in six.moves.filter(",
            "            lambda amp: amp.status == constants.AMPHORA_ALLOCATED,",
            "                loadbalancer.amphorae):",
            "",
            "            try:",
            "                interface = self.amphora_driver.get_vrrp_interface(",
            "                    amp, timeout_dict=timeout_dict)",
            "            except Exception as e:",
            "                # This can occur when an active/standby LB has no listener",
            "                LOG.error('Failed to get amphora VRRP interface on amphora '",
            "                          '%s. Skipping this amphora as it is failing due to: '",
            "                          '%s', amp.id, str(e))",
            "                self.amphora_repo.update(db_apis.get_session(), amp.id,",
            "                                         status=constants.ERROR)",
            "                continue",
            "",
            "            self.amphora_repo.update(db_apis.get_session(), amp.id,",
            "                                     vrrp_interface=interface)",
            "            amps.append(self.amphora_repo.get(db_apis.get_session(),",
            "                                              id=amp.id))",
            "        loadbalancer.amphorae = amps",
            "        return loadbalancer",
            "",
            "    def revert(self, result, loadbalancer, *args, **kwargs):",
            "        \"\"\"Handle a failed amphora vip plug notification.\"\"\"",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "        LOG.warning(\"Reverting Get Amphora VRRP Interface.\")",
            "        for amp in six.moves.filter(",
            "            lambda amp: amp.status == constants.AMPHORA_ALLOCATED,",
            "                loadbalancer.amphorae):",
            "",
            "            try:",
            "                self.amphora_repo.update(db_apis.get_session(), amp.id,",
            "                                         vrrp_interface=None)",
            "            except Exception as e:",
            "                LOG.error(\"Failed to update amphora %(amp)s \"",
            "                          \"VRRP interface to None due to: %(except)s\",",
            "                          {'amp': amp.id, 'except': e})",
            "",
            "",
            "class AmphoraVRRPUpdate(BaseAmphoraTask):",
            "    \"\"\"Task to update the VRRP configuration of the loadbalancer amphorae.\"\"\"",
            "",
            "    def execute(self, loadbalancer, amphorae_network_config):",
            "        \"\"\"Execute update_vrrp_conf.\"\"\"",
            "        self.amphora_driver.update_vrrp_conf(loadbalancer,",
            "                                             amphorae_network_config)",
            "        LOG.debug(\"Uploaded VRRP configuration of loadbalancer %s amphorae\",",
            "                  loadbalancer.id)",
            "",
            "",
            "class AmphoraVRRPStop(BaseAmphoraTask):",
            "    \"\"\"Task to stop keepalived of all amphorae of a LB.\"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        self.amphora_driver.stop_vrrp_service(loadbalancer)",
            "        LOG.debug(\"Stopped VRRP of loadbalancer %s amphorae\",",
            "                  loadbalancer.id)",
            "",
            "",
            "class AmphoraVRRPStart(BaseAmphoraTask):",
            "    \"\"\"Task to start keepalived of all amphorae of a LB.\"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        self.amphora_driver.start_vrrp_service(loadbalancer)",
            "        LOG.debug(\"Started VRRP of loadbalancer %s amphorae\",",
            "                  loadbalancer.id)",
            "",
            "",
            "class AmphoraComputeConnectivityWait(BaseAmphoraTask):",
            "    \"\"\"Task to wait for the compute instance to be up.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Execute get_info routine for an amphora until it responds.\"\"\"",
            "        try:",
            "            amp_info = self.amphora_driver.get_info(amphora)",
            "            LOG.debug('Successfuly connected to amphora %s: %s',",
            "                      amphora.id, amp_info)",
            "        except driver_except.TimeOutException:",
            "            LOG.error(\"Amphora compute instance failed to become reachable. \"",
            "                      \"This either means the compute driver failed to fully \"",
            "                      \"boot the instance inside the timeout interval or the \"",
            "                      \"instance is not reachable via the lb-mgmt-net.\")",
            "            self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                     status=constants.ERROR)",
            "            raise",
            "",
            "",
            "class AmphoraConfigUpdate(BaseAmphoraTask):",
            "    \"\"\"Task to push a new amphora agent configuration to the amphroa.\"\"\"",
            "",
            "    def execute(self, amphora, flavor):",
            "        # Extract any flavor based settings",
            "        if flavor:",
            "            topology = flavor.get(constants.LOADBALANCER_TOPOLOGY,",
            "                                  CONF.controller_worker.loadbalancer_topology)",
            "        else:",
            "            topology = CONF.controller_worker.loadbalancer_topology",
            "",
            "        # Build the amphora agent config",
            "        agent_cfg_tmpl = agent_jinja_cfg.AgentJinjaTemplater()",
            "        agent_config = agent_cfg_tmpl.build_agent_config(amphora.id, topology)",
            "",
            "        # Push the new configuration to the amphroa",
            "        try:",
            "            self.amphora_driver.update_amphora_agent_config(amphora,",
            "                                                            agent_config)",
            "        except driver_except.AmpDriverNotImplementedError:",
            "            LOG.error('Amphora {} does not support agent configuration '",
            "                      'update. Please update the amphora image for this '",
            "                      'amphora. Skipping.'.format(amphora.id))"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "276": [
                "AmphoraCertUpload",
                "execute"
            ]
        },
        "addLocation": []
    },
    "octavia/controller/worker/tasks/cert_task.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " # under the License."
            },
            "1": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " #"
            },
            "2": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+from cryptography import fernet"
            },
            "4": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " from oslo_config import cfg"
            },
            "5": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from stevedore import driver as stevedore_driver"
            },
            "6": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " from taskflow import task"
            },
            "7": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 21,
                "PatchRowcode": "+from octavia.common import utils"
            },
            "9": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " CONF = cfg.CONF"
            },
            "11": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " CERT_VALIDITY = 2 * 365 * 24 * 60 * 60"
            },
            "12": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 46,
                "PatchRowcode": "         cert = self.cert_generator.generate_cert_key_pair("
            },
            "13": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": 47,
                "PatchRowcode": "             cn=amphora_id,"
            },
            "14": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 48,
                "PatchRowcode": "             validity=CERT_VALIDITY)"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+        key = utils.get_six_compatible_server_certs_key_passphrase()"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+        fer = fernet.Fernet(key)"
            },
            "17": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 51,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return cert.certificate + cert.private_key"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+        return fer.encrypt(cert.certificate + cert.private_key)"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015 Hewlett-Packard Development Company, L.P.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "# http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "",
            "from oslo_config import cfg",
            "from stevedore import driver as stevedore_driver",
            "from taskflow import task",
            "",
            "",
            "CONF = cfg.CONF",
            "CERT_VALIDITY = 2 * 365 * 24 * 60 * 60",
            "",
            "",
            "class BaseCertTask(task.Task):",
            "    \"\"\"Base task to load drivers common to the tasks.\"\"\"",
            "",
            "    def __init__(self, **kwargs):",
            "        super(BaseCertTask, self).__init__(**kwargs)",
            "        self.cert_generator = stevedore_driver.DriverManager(",
            "            namespace='octavia.cert_generator',",
            "            name=CONF.certificates.cert_generator,",
            "            invoke_on_load=True,",
            "        ).driver",
            "",
            "",
            "class GenerateServerPEMTask(BaseCertTask):",
            "    \"\"\"Create the server certs for the agent comm",
            "",
            "    Use the amphora_id for the CN",
            "    \"\"\"",
            "",
            "    def execute(self, amphora_id):",
            "        cert = self.cert_generator.generate_cert_key_pair(",
            "            cn=amphora_id,",
            "            validity=CERT_VALIDITY)",
            "",
            "        return cert.certificate + cert.private_key"
        ],
        "afterPatchFile": [
            "# Copyright 2015 Hewlett-Packard Development Company, L.P.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "# http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "",
            "from cryptography import fernet",
            "from oslo_config import cfg",
            "from stevedore import driver as stevedore_driver",
            "from taskflow import task",
            "",
            "from octavia.common import utils",
            "",
            "CONF = cfg.CONF",
            "CERT_VALIDITY = 2 * 365 * 24 * 60 * 60",
            "",
            "",
            "class BaseCertTask(task.Task):",
            "    \"\"\"Base task to load drivers common to the tasks.\"\"\"",
            "",
            "    def __init__(self, **kwargs):",
            "        super(BaseCertTask, self).__init__(**kwargs)",
            "        self.cert_generator = stevedore_driver.DriverManager(",
            "            namespace='octavia.cert_generator',",
            "            name=CONF.certificates.cert_generator,",
            "            invoke_on_load=True,",
            "        ).driver",
            "",
            "",
            "class GenerateServerPEMTask(BaseCertTask):",
            "    \"\"\"Create the server certs for the agent comm",
            "",
            "    Use the amphora_id for the CN",
            "    \"\"\"",
            "",
            "    def execute(self, amphora_id):",
            "        cert = self.cert_generator.generate_cert_key_pair(",
            "            cn=amphora_id,",
            "            validity=CERT_VALIDITY)",
            "        key = utils.get_six_compatible_server_certs_key_passphrase()",
            "        fer = fernet.Fernet(key)",
            "",
            "        return fer.encrypt(cert.certificate + cert.private_key)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "48": [
                "GenerateServerPEMTask",
                "execute"
            ]
        },
        "addLocation": []
    },
    "octavia/controller/worker/tasks/compute_tasks.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " import time"
            },
            "2": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 18,
                "PatchRowcode": "+from cryptography import fernet"
            },
            "4": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " from oslo_config import cfg"
            },
            "5": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " from oslo_log import log as logging"
            },
            "6": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from stevedore import driver as stevedore_driver"
            },
            "7": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from octavia.common import constants"
            },
            "8": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " from octavia.common import exceptions"
            },
            "9": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from octavia.common.jinja import user_data_jinja_cfg"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+from octavia.common import utils"
            },
            "11": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " from octavia.controller.worker import amphora_rate_limit"
            },
            "12": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " CONF = cfg.CONF"
            },
            "14": {
                "beforePatchRowNumber": 144,
                "afterPatchRowNumber": 146,
                "PatchRowcode": "         # load client certificate"
            },
            "15": {
                "beforePatchRowNumber": 145,
                "afterPatchRowNumber": 147,
                "PatchRowcode": "         with open(CONF.controller_worker.client_ca, 'r') as client_ca:"
            },
            "16": {
                "beforePatchRowNumber": 146,
                "afterPatchRowNumber": 148,
                "PatchRowcode": "             ca = client_ca.read()"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 149,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 150,
                "PatchRowcode": "+        key = utils.get_six_compatible_server_certs_key_passphrase()"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 151,
                "PatchRowcode": "+        fer = fernet.Fernet(key)"
            },
            "20": {
                "beforePatchRowNumber": 147,
                "afterPatchRowNumber": 152,
                "PatchRowcode": "         config_drive_files = {"
            },
            "21": {
                "beforePatchRowNumber": 148,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            '/etc/octavia/certs/server.pem': server_pem,"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 153,
                "PatchRowcode": "+            '/etc/octavia/certs/server.pem': fer.decrypt(server_pem),"
            },
            "23": {
                "beforePatchRowNumber": 149,
                "afterPatchRowNumber": 154,
                "PatchRowcode": "             '/etc/octavia/certs/client_ca.pem': ca}"
            },
            "24": {
                "beforePatchRowNumber": 150,
                "afterPatchRowNumber": 155,
                "PatchRowcode": "         return super(CertComputeCreate, self).execute("
            },
            "25": {
                "beforePatchRowNumber": 151,
                "afterPatchRowNumber": 156,
                "PatchRowcode": "             amphora_id, config_drive_files=config_drive_files,"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015 Hewlett-Packard Development Company, L.P.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "# http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "",
            "import time",
            "",
            "from oslo_config import cfg",
            "from oslo_log import log as logging",
            "from stevedore import driver as stevedore_driver",
            "from taskflow import task",
            "from taskflow.types import failure",
            "",
            "from octavia.amphorae.backends.agent import agent_jinja_cfg",
            "from octavia.common import constants",
            "from octavia.common import exceptions",
            "from octavia.common.jinja import user_data_jinja_cfg",
            "from octavia.controller.worker import amphora_rate_limit",
            "",
            "CONF = cfg.CONF",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "class BaseComputeTask(task.Task):",
            "    \"\"\"Base task to load drivers common to the tasks.\"\"\"",
            "",
            "    def __init__(self, **kwargs):",
            "        super(BaseComputeTask, self).__init__(**kwargs)",
            "        self.compute = stevedore_driver.DriverManager(",
            "            namespace='octavia.compute.drivers',",
            "            name=CONF.controller_worker.compute_driver,",
            "            invoke_on_load=True",
            "        ).driver",
            "        self.rate_limit = amphora_rate_limit.AmphoraBuildRateLimit()",
            "",
            "",
            "class ComputeCreate(BaseComputeTask):",
            "    \"\"\"Create the compute instance for a new amphora.\"\"\"",
            "",
            "    def execute(self, amphora_id, config_drive_files=None,",
            "                build_type_priority=constants.LB_CREATE_NORMAL_PRIORITY,",
            "                server_group_id=None, ports=None, flavor=None):",
            "        \"\"\"Create an amphora",
            "",
            "        :returns: an amphora",
            "        \"\"\"",
            "        ports = ports or []",
            "        network_ids = CONF.controller_worker.amp_boot_network_list[:]",
            "        config_drive_files = config_drive_files or {}",
            "        user_data = None",
            "        LOG.debug(\"Compute create execute for amphora with id %s\", amphora_id)",
            "",
            "        user_data_config_drive = CONF.controller_worker.user_data_config_drive",
            "",
            "        key_name = CONF.controller_worker.amp_ssh_key_name",
            "        # TODO(rm_work): amp_ssh_access_allowed is deprecated in Pike.",
            "        # Remove the following two lines in the S release.",
            "        ssh_access = CONF.controller_worker.amp_ssh_access_allowed",
            "        key_name = None if not ssh_access else key_name",
            "",
            "        # Apply an Octavia flavor customizations",
            "        if flavor:",
            "            topology = flavor.get(constants.LOADBALANCER_TOPOLOGY,",
            "                                  CONF.controller_worker.loadbalancer_topology)",
            "            amp_compute_flavor = flavor.get(",
            "                constants.COMPUTE_FLAVOR, CONF.controller_worker.amp_flavor_id)",
            "        else:",
            "            topology = CONF.controller_worker.loadbalancer_topology",
            "            amp_compute_flavor = CONF.controller_worker.amp_flavor_id",
            "",
            "        try:",
            "            if CONF.haproxy_amphora.build_rate_limit != -1:",
            "                self.rate_limit.add_to_build_request_queue(",
            "                    amphora_id, build_type_priority)",
            "",
            "            agent_cfg = agent_jinja_cfg.AgentJinjaTemplater()",
            "            config_drive_files['/etc/octavia/amphora-agent.conf'] = (",
            "                agent_cfg.build_agent_config(amphora_id, topology))",
            "            if user_data_config_drive:",
            "                udtemplater = user_data_jinja_cfg.UserDataJinjaCfg()",
            "                user_data = udtemplater.build_user_data_config(",
            "                    config_drive_files)",
            "                config_drive_files = None",
            "",
            "            compute_id = self.compute.build(",
            "                name=\"amphora-\" + amphora_id,",
            "                amphora_flavor=amp_compute_flavor,",
            "                image_id=CONF.controller_worker.amp_image_id,",
            "                image_tag=CONF.controller_worker.amp_image_tag,",
            "                image_owner=CONF.controller_worker.amp_image_owner_id,",
            "                key_name=key_name,",
            "                sec_groups=CONF.controller_worker.amp_secgroup_list,",
            "                network_ids=network_ids,",
            "                port_ids=[port.id for port in ports],",
            "                config_drive_files=config_drive_files,",
            "                user_data=user_data,",
            "                server_group_id=server_group_id)",
            "",
            "            LOG.debug(\"Server created with id: %s for amphora id: %s\",",
            "                      compute_id, amphora_id)",
            "            return compute_id",
            "",
            "        except Exception:",
            "            LOG.exception(\"Compute create for amphora id: %s failed\",",
            "                          amphora_id)",
            "            raise",
            "",
            "    def revert(self, result, amphora_id, *args, **kwargs):",
            "        \"\"\"This method will revert the creation of the",
            "",
            "        amphora. So it will just delete it in this flow",
            "        \"\"\"",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "        compute_id = result",
            "        LOG.warning(\"Reverting compute create for amphora with id \"",
            "                    \"%(amp)s and compute id: %(comp)s\",",
            "                    {'amp': amphora_id, 'comp': compute_id})",
            "        try:",
            "            self.compute.delete(compute_id)",
            "        except Exception:",
            "            LOG.exception(\"Reverting compute create failed\")",
            "",
            "",
            "class CertComputeCreate(ComputeCreate):",
            "    def execute(self, amphora_id, server_pem,",
            "                build_type_priority=constants.LB_CREATE_NORMAL_PRIORITY,",
            "                server_group_id=None, ports=None, flavor=None):",
            "        \"\"\"Create an amphora",
            "",
            "        :returns: an amphora",
            "        \"\"\"",
            "",
            "        # load client certificate",
            "        with open(CONF.controller_worker.client_ca, 'r') as client_ca:",
            "            ca = client_ca.read()",
            "        config_drive_files = {",
            "            '/etc/octavia/certs/server.pem': server_pem,",
            "            '/etc/octavia/certs/client_ca.pem': ca}",
            "        return super(CertComputeCreate, self).execute(",
            "            amphora_id, config_drive_files=config_drive_files,",
            "            build_type_priority=build_type_priority,",
            "            server_group_id=server_group_id, ports=ports, flavor=flavor)",
            "",
            "",
            "class DeleteAmphoraeOnLoadBalancer(BaseComputeTask):",
            "    \"\"\"Delete the amphorae on a load balancer.",
            "",
            "    Iterate through amphorae, deleting them",
            "    \"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        for amp in loadbalancer.amphorae:",
            "            # The compute driver will already handle NotFound",
            "            try:",
            "                self.compute.delete(amp.compute_id)",
            "            except Exception:",
            "                LOG.exception(\"Compute delete for amphora id: %s failed\",",
            "                              amp.id)",
            "                raise",
            "",
            "",
            "class ComputeDelete(BaseComputeTask):",
            "    def execute(self, amphora):",
            "        LOG.debug(\"Compute Delete execute for amphora with id %s\", amphora.id)",
            "",
            "        try:",
            "            self.compute.delete(amphora.compute_id)",
            "        except Exception:",
            "            LOG.exception(\"Compute delete for amphora id: %s failed\",",
            "                          amphora.id)",
            "            raise",
            "",
            "",
            "class ComputeActiveWait(BaseComputeTask):",
            "    \"\"\"Wait for the compute driver to mark the amphora active.\"\"\"",
            "",
            "    def execute(self, compute_id, amphora_id):",
            "        \"\"\"Wait for the compute driver to mark the amphora active",
            "",
            "        :raises: Generic exception if the amphora is not active",
            "        :returns: An amphora object",
            "        \"\"\"",
            "        for i in range(CONF.controller_worker.amp_active_retries):",
            "            amp, fault = self.compute.get_amphora(compute_id)",
            "            if amp.status == constants.ACTIVE:",
            "                if CONF.haproxy_amphora.build_rate_limit != -1:",
            "                    self.rate_limit.remove_from_build_req_queue(amphora_id)",
            "                return amp",
            "            elif amp.status == constants.ERROR:",
            "                raise exceptions.ComputeBuildException(fault=fault)",
            "            time.sleep(CONF.controller_worker.amp_active_wait_sec)",
            "",
            "        raise exceptions.ComputeWaitTimeoutException(id=compute_id)",
            "",
            "",
            "class NovaServerGroupCreate(BaseComputeTask):",
            "    def execute(self, loadbalancer_id):",
            "        \"\"\"Create a server group by nova client api",
            "",
            "        :param loadbalancer_id: will be used for server group's name",
            "        :param policy: will used for server group's policy",
            "        :raises: Generic exception if the server group is not created",
            "        :returns: server group's id",
            "        \"\"\"",
            "",
            "        name = 'octavia-lb-' + loadbalancer_id",
            "        server_group = self.compute.create_server_group(",
            "            name, CONF.nova.anti_affinity_policy)",
            "        LOG.debug(\"Server Group created with id: %s for load balancer id: \"",
            "                  \"%s\", server_group.id, loadbalancer_id)",
            "        return server_group.id",
            "",
            "    def revert(self, result, *args, **kwargs):",
            "        \"\"\"This method will revert the creation of the",
            "",
            "        :param result: here it refers to server group id",
            "        \"\"\"",
            "        server_group_id = result",
            "        LOG.warning(\"Reverting server group create with id:%s\",",
            "                    server_group_id)",
            "        try:",
            "            self.compute.delete_server_group(server_group_id)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to delete server group.  Resources may \"",
            "                      \"still be in use for server group: %(sg)s due to \"",
            "                      \"error: %(except)s\",",
            "                      {'sg': server_group_id, 'except': e})",
            "",
            "",
            "class NovaServerGroupDelete(BaseComputeTask):",
            "    def execute(self, server_group_id):",
            "        if server_group_id is not None:",
            "            self.compute.delete_server_group(server_group_id)",
            "        else:",
            "            return"
        ],
        "afterPatchFile": [
            "# Copyright 2015 Hewlett-Packard Development Company, L.P.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "# http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "",
            "import time",
            "",
            "from cryptography import fernet",
            "from oslo_config import cfg",
            "from oslo_log import log as logging",
            "from stevedore import driver as stevedore_driver",
            "from taskflow import task",
            "from taskflow.types import failure",
            "",
            "from octavia.amphorae.backends.agent import agent_jinja_cfg",
            "from octavia.common import constants",
            "from octavia.common import exceptions",
            "from octavia.common.jinja import user_data_jinja_cfg",
            "from octavia.common import utils",
            "from octavia.controller.worker import amphora_rate_limit",
            "",
            "CONF = cfg.CONF",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "class BaseComputeTask(task.Task):",
            "    \"\"\"Base task to load drivers common to the tasks.\"\"\"",
            "",
            "    def __init__(self, **kwargs):",
            "        super(BaseComputeTask, self).__init__(**kwargs)",
            "        self.compute = stevedore_driver.DriverManager(",
            "            namespace='octavia.compute.drivers',",
            "            name=CONF.controller_worker.compute_driver,",
            "            invoke_on_load=True",
            "        ).driver",
            "        self.rate_limit = amphora_rate_limit.AmphoraBuildRateLimit()",
            "",
            "",
            "class ComputeCreate(BaseComputeTask):",
            "    \"\"\"Create the compute instance for a new amphora.\"\"\"",
            "",
            "    def execute(self, amphora_id, config_drive_files=None,",
            "                build_type_priority=constants.LB_CREATE_NORMAL_PRIORITY,",
            "                server_group_id=None, ports=None, flavor=None):",
            "        \"\"\"Create an amphora",
            "",
            "        :returns: an amphora",
            "        \"\"\"",
            "        ports = ports or []",
            "        network_ids = CONF.controller_worker.amp_boot_network_list[:]",
            "        config_drive_files = config_drive_files or {}",
            "        user_data = None",
            "        LOG.debug(\"Compute create execute for amphora with id %s\", amphora_id)",
            "",
            "        user_data_config_drive = CONF.controller_worker.user_data_config_drive",
            "",
            "        key_name = CONF.controller_worker.amp_ssh_key_name",
            "        # TODO(rm_work): amp_ssh_access_allowed is deprecated in Pike.",
            "        # Remove the following two lines in the S release.",
            "        ssh_access = CONF.controller_worker.amp_ssh_access_allowed",
            "        key_name = None if not ssh_access else key_name",
            "",
            "        # Apply an Octavia flavor customizations",
            "        if flavor:",
            "            topology = flavor.get(constants.LOADBALANCER_TOPOLOGY,",
            "                                  CONF.controller_worker.loadbalancer_topology)",
            "            amp_compute_flavor = flavor.get(",
            "                constants.COMPUTE_FLAVOR, CONF.controller_worker.amp_flavor_id)",
            "        else:",
            "            topology = CONF.controller_worker.loadbalancer_topology",
            "            amp_compute_flavor = CONF.controller_worker.amp_flavor_id",
            "",
            "        try:",
            "            if CONF.haproxy_amphora.build_rate_limit != -1:",
            "                self.rate_limit.add_to_build_request_queue(",
            "                    amphora_id, build_type_priority)",
            "",
            "            agent_cfg = agent_jinja_cfg.AgentJinjaTemplater()",
            "            config_drive_files['/etc/octavia/amphora-agent.conf'] = (",
            "                agent_cfg.build_agent_config(amphora_id, topology))",
            "            if user_data_config_drive:",
            "                udtemplater = user_data_jinja_cfg.UserDataJinjaCfg()",
            "                user_data = udtemplater.build_user_data_config(",
            "                    config_drive_files)",
            "                config_drive_files = None",
            "",
            "            compute_id = self.compute.build(",
            "                name=\"amphora-\" + amphora_id,",
            "                amphora_flavor=amp_compute_flavor,",
            "                image_id=CONF.controller_worker.amp_image_id,",
            "                image_tag=CONF.controller_worker.amp_image_tag,",
            "                image_owner=CONF.controller_worker.amp_image_owner_id,",
            "                key_name=key_name,",
            "                sec_groups=CONF.controller_worker.amp_secgroup_list,",
            "                network_ids=network_ids,",
            "                port_ids=[port.id for port in ports],",
            "                config_drive_files=config_drive_files,",
            "                user_data=user_data,",
            "                server_group_id=server_group_id)",
            "",
            "            LOG.debug(\"Server created with id: %s for amphora id: %s\",",
            "                      compute_id, amphora_id)",
            "            return compute_id",
            "",
            "        except Exception:",
            "            LOG.exception(\"Compute create for amphora id: %s failed\",",
            "                          amphora_id)",
            "            raise",
            "",
            "    def revert(self, result, amphora_id, *args, **kwargs):",
            "        \"\"\"This method will revert the creation of the",
            "",
            "        amphora. So it will just delete it in this flow",
            "        \"\"\"",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "        compute_id = result",
            "        LOG.warning(\"Reverting compute create for amphora with id \"",
            "                    \"%(amp)s and compute id: %(comp)s\",",
            "                    {'amp': amphora_id, 'comp': compute_id})",
            "        try:",
            "            self.compute.delete(compute_id)",
            "        except Exception:",
            "            LOG.exception(\"Reverting compute create failed\")",
            "",
            "",
            "class CertComputeCreate(ComputeCreate):",
            "    def execute(self, amphora_id, server_pem,",
            "                build_type_priority=constants.LB_CREATE_NORMAL_PRIORITY,",
            "                server_group_id=None, ports=None, flavor=None):",
            "        \"\"\"Create an amphora",
            "",
            "        :returns: an amphora",
            "        \"\"\"",
            "",
            "        # load client certificate",
            "        with open(CONF.controller_worker.client_ca, 'r') as client_ca:",
            "            ca = client_ca.read()",
            "",
            "        key = utils.get_six_compatible_server_certs_key_passphrase()",
            "        fer = fernet.Fernet(key)",
            "        config_drive_files = {",
            "            '/etc/octavia/certs/server.pem': fer.decrypt(server_pem),",
            "            '/etc/octavia/certs/client_ca.pem': ca}",
            "        return super(CertComputeCreate, self).execute(",
            "            amphora_id, config_drive_files=config_drive_files,",
            "            build_type_priority=build_type_priority,",
            "            server_group_id=server_group_id, ports=ports, flavor=flavor)",
            "",
            "",
            "class DeleteAmphoraeOnLoadBalancer(BaseComputeTask):",
            "    \"\"\"Delete the amphorae on a load balancer.",
            "",
            "    Iterate through amphorae, deleting them",
            "    \"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        for amp in loadbalancer.amphorae:",
            "            # The compute driver will already handle NotFound",
            "            try:",
            "                self.compute.delete(amp.compute_id)",
            "            except Exception:",
            "                LOG.exception(\"Compute delete for amphora id: %s failed\",",
            "                              amp.id)",
            "                raise",
            "",
            "",
            "class ComputeDelete(BaseComputeTask):",
            "    def execute(self, amphora):",
            "        LOG.debug(\"Compute Delete execute for amphora with id %s\", amphora.id)",
            "",
            "        try:",
            "            self.compute.delete(amphora.compute_id)",
            "        except Exception:",
            "            LOG.exception(\"Compute delete for amphora id: %s failed\",",
            "                          amphora.id)",
            "            raise",
            "",
            "",
            "class ComputeActiveWait(BaseComputeTask):",
            "    \"\"\"Wait for the compute driver to mark the amphora active.\"\"\"",
            "",
            "    def execute(self, compute_id, amphora_id):",
            "        \"\"\"Wait for the compute driver to mark the amphora active",
            "",
            "        :raises: Generic exception if the amphora is not active",
            "        :returns: An amphora object",
            "        \"\"\"",
            "        for i in range(CONF.controller_worker.amp_active_retries):",
            "            amp, fault = self.compute.get_amphora(compute_id)",
            "            if amp.status == constants.ACTIVE:",
            "                if CONF.haproxy_amphora.build_rate_limit != -1:",
            "                    self.rate_limit.remove_from_build_req_queue(amphora_id)",
            "                return amp",
            "            elif amp.status == constants.ERROR:",
            "                raise exceptions.ComputeBuildException(fault=fault)",
            "            time.sleep(CONF.controller_worker.amp_active_wait_sec)",
            "",
            "        raise exceptions.ComputeWaitTimeoutException(id=compute_id)",
            "",
            "",
            "class NovaServerGroupCreate(BaseComputeTask):",
            "    def execute(self, loadbalancer_id):",
            "        \"\"\"Create a server group by nova client api",
            "",
            "        :param loadbalancer_id: will be used for server group's name",
            "        :param policy: will used for server group's policy",
            "        :raises: Generic exception if the server group is not created",
            "        :returns: server group's id",
            "        \"\"\"",
            "",
            "        name = 'octavia-lb-' + loadbalancer_id",
            "        server_group = self.compute.create_server_group(",
            "            name, CONF.nova.anti_affinity_policy)",
            "        LOG.debug(\"Server Group created with id: %s for load balancer id: \"",
            "                  \"%s\", server_group.id, loadbalancer_id)",
            "        return server_group.id",
            "",
            "    def revert(self, result, *args, **kwargs):",
            "        \"\"\"This method will revert the creation of the",
            "",
            "        :param result: here it refers to server group id",
            "        \"\"\"",
            "        server_group_id = result",
            "        LOG.warning(\"Reverting server group create with id:%s\",",
            "                    server_group_id)",
            "        try:",
            "            self.compute.delete_server_group(server_group_id)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to delete server group.  Resources may \"",
            "                      \"still be in use for server group: %(sg)s due to \"",
            "                      \"error: %(except)s\",",
            "                      {'sg': server_group_id, 'except': e})",
            "",
            "",
            "class NovaServerGroupDelete(BaseComputeTask):",
            "    def execute(self, server_group_id):",
            "        if server_group_id is not None:",
            "            self.compute.delete_server_group(server_group_id)",
            "        else:",
            "            return"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "148": [
                "CertComputeCreate",
                "execute"
            ]
        },
        "addLocation": [
            "octavia.controller.worker.tasks.compute_tasks.CertComputeCreate.execute.config_drive_files"
        ]
    },
    "octavia/controller/worker/tasks/database_tasks.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " # under the License."
            },
            "1": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " #"
            },
            "2": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+from cryptography import fernet"
            },
            "4": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " from oslo_config import cfg"
            },
            "5": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from oslo_db import exception as odb_exceptions"
            },
            "6": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " from oslo_log import log as logging"
            },
            "7": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from octavia.common import constants"
            },
            "8": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " from octavia.common import data_models"
            },
            "9": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " import octavia.common.tls_utils.cert_parser as cert_parser"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+from octavia.common import utils"
            },
            "11": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " from octavia.common import validate"
            },
            "12": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " from octavia.controller.worker import task_utils as task_utilities"
            },
            "13": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " from octavia.db import api as db_apis"
            },
            "14": {
                "beforePatchRowNumber": 918,
                "afterPatchRowNumber": 920,
                "PatchRowcode": "         \"\"\""
            },
            "15": {
                "beforePatchRowNumber": 919,
                "afterPatchRowNumber": 921,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 920,
                "afterPatchRowNumber": 922,
                "PatchRowcode": "         LOG.debug(\"Update DB cert expiry date of amphora id: %s\", amphora_id)"
            },
            "17": {
                "beforePatchRowNumber": 921,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        cert_expiration = cert_parser.get_cert_expiration(server_pem)"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 923,
                "PatchRowcode": "+"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 924,
                "PatchRowcode": "+        key = utils.get_six_compatible_server_certs_key_passphrase()"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 925,
                "PatchRowcode": "+        fer = fernet.Fernet(key)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 926,
                "PatchRowcode": "+        cert_expiration = cert_parser.get_cert_expiration("
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 927,
                "PatchRowcode": "+            fer.decrypt(server_pem))"
            },
            "23": {
                "beforePatchRowNumber": 922,
                "afterPatchRowNumber": 928,
                "PatchRowcode": "         LOG.debug(\"Certificate expiration date is %s \", cert_expiration)"
            },
            "24": {
                "beforePatchRowNumber": 923,
                "afterPatchRowNumber": 929,
                "PatchRowcode": "         self.amphora_repo.update(db_apis.get_session(), amphora_id,"
            },
            "25": {
                "beforePatchRowNumber": 924,
                "afterPatchRowNumber": 930,
                "PatchRowcode": "                                  cert_expiration=cert_expiration)"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015 Hewlett-Packard Development Company, L.P.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "# http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "",
            "from oslo_config import cfg",
            "from oslo_db import exception as odb_exceptions",
            "from oslo_log import log as logging",
            "from oslo_utils import excutils",
            "from oslo_utils import uuidutils",
            "import six",
            "import sqlalchemy",
            "from sqlalchemy.orm import exc",
            "from taskflow import task",
            "from taskflow.types import failure",
            "",
            "from octavia.common import constants",
            "from octavia.common import data_models",
            "import octavia.common.tls_utils.cert_parser as cert_parser",
            "from octavia.common import validate",
            "from octavia.controller.worker import task_utils as task_utilities",
            "from octavia.db import api as db_apis",
            "from octavia.db import repositories as repo",
            "",
            "CONF = cfg.CONF",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "class BaseDatabaseTask(task.Task):",
            "    \"\"\"Base task to load drivers common to the tasks.\"\"\"",
            "",
            "    def __init__(self, **kwargs):",
            "        self.repos = repo.Repositories()",
            "        self.amphora_repo = repo.AmphoraRepository()",
            "        self.health_mon_repo = repo.HealthMonitorRepository()",
            "        self.listener_repo = repo.ListenerRepository()",
            "        self.loadbalancer_repo = repo.LoadBalancerRepository()",
            "        self.vip_repo = repo.VipRepository()",
            "        self.member_repo = repo.MemberRepository()",
            "        self.pool_repo = repo.PoolRepository()",
            "        self.amp_health_repo = repo.AmphoraHealthRepository()",
            "        self.l7policy_repo = repo.L7PolicyRepository()",
            "        self.l7rule_repo = repo.L7RuleRepository()",
            "        self.task_utils = task_utilities.TaskUtils()",
            "        super(BaseDatabaseTask, self).__init__(**kwargs)",
            "",
            "    def _delete_from_amp_health(self, amphora_id):",
            "        \"\"\"Delete the amphora_health record for an amphora.",
            "",
            "        :param amphora_id: The amphora id to delete",
            "        \"\"\"",
            "        LOG.debug('Disabling health monitoring on amphora: %s', amphora_id)",
            "        try:",
            "            self.amp_health_repo.delete(db_apis.get_session(),",
            "                                        amphora_id=amphora_id)",
            "        except (sqlalchemy.orm.exc.NoResultFound,",
            "                sqlalchemy.orm.exc.UnmappedInstanceError):",
            "            LOG.debug('No existing amphora health record to delete '",
            "                      'for amphora: %s, skipping.', amphora_id)",
            "",
            "    def _mark_amp_health_busy(self, amphora_id):",
            "        \"\"\"Mark the amphora_health record busy for an amphora.",
            "",
            "        :param amphora_id: The amphora id to mark busy",
            "        \"\"\"",
            "        LOG.debug('Marking health monitoring busy on amphora: %s', amphora_id)",
            "        try:",
            "            self.amp_health_repo.update(db_apis.get_session(),",
            "                                        amphora_id=amphora_id,",
            "                                        busy=True)",
            "        except (sqlalchemy.orm.exc.NoResultFound,",
            "                sqlalchemy.orm.exc.UnmappedInstanceError):",
            "            LOG.debug('No existing amphora health record to mark busy '",
            "                      'for amphora: %s, skipping.', amphora_id)",
            "",
            "",
            "class CreateAmphoraInDB(BaseDatabaseTask):",
            "    \"\"\"Task to create an initial amphora in the Database.\"\"\"",
            "",
            "    def execute(self, *args, **kwargs):",
            "        \"\"\"Creates an pending create amphora record in the database.",
            "",
            "        :returns: The created amphora object",
            "        \"\"\"",
            "",
            "        amphora = self.amphora_repo.create(db_apis.get_session(),",
            "                                           id=uuidutils.generate_uuid(),",
            "                                           status=constants.PENDING_CREATE,",
            "                                           cert_busy=False)",
            "",
            "        LOG.info(\"Created Amphora in DB with id %s\", amphora.id)",
            "        return amphora.id",
            "",
            "    def revert(self, result, *args, **kwargs):",
            "        \"\"\"Revert by storing the amphora in error state in the DB",
            "",
            "        In a future version we might change the status to DELETED",
            "        if deleting the amphora was successful",
            "",
            "        :param result: Id of created amphora.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        if isinstance(result, failure.Failure):",
            "            # This task's execute failed, so nothing needed to be done to",
            "            # revert",
            "            return",
            "",
            "        # At this point the revert is being called because another task",
            "        # executed after this failed so we will need to do something and",
            "        # result is the amphora's id",
            "",
            "        LOG.warning(\"Reverting create amphora in DB for amp id %s \", result)",
            "",
            "        # Delete the amphora for now. May want to just update status later",
            "        try:",
            "            self.amphora_repo.delete(db_apis.get_session(), id=result)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to delete amphora %(amp)s \"",
            "                      \"in the database due to: \"",
            "                      \"%(except)s\", {'amp': result, 'except': e})",
            "",
            "",
            "class MarkLBAmphoraeDeletedInDB(BaseDatabaseTask):",
            "    \"\"\"Task to mark a list of amphora deleted in the Database.\"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Update load balancer's amphorae statuses to DELETED in the database.",
            "",
            "        :param loadbalancer: The load balancer which amphorae should be",
            "               marked DELETED.",
            "        :returns: None",
            "        \"\"\"",
            "        for amp in loadbalancer.amphorae:",
            "            LOG.debug(\"Marking amphora %s DELETED \", amp.id)",
            "            self.amphora_repo.update(db_apis.get_session(),",
            "                                     id=amp.id, status=constants.DELETED)",
            "",
            "",
            "class DeleteHealthMonitorInDB(BaseDatabaseTask):",
            "    \"\"\"Delete the health monitor in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, health_mon):",
            "        \"\"\"Delete the health monitor in DB",
            "",
            "        :param health_mon: The health monitor which should be deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"DB delete health monitor: %s \", health_mon.id)",
            "        try:",
            "            self.health_mon_repo.delete(db_apis.get_session(),",
            "                                        id=health_mon.id)",
            "        except exc.NoResultFound:",
            "            # ignore if the HealthMonitor was not found",
            "            pass",
            "",
            "    def revert(self, health_mon, *args, **kwargs):",
            "        \"\"\"Mark the health monitor ERROR since the mark active couldn't happen",
            "",
            "        :param health_mon: The health monitor which couldn't be deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark health monitor delete in DB \"",
            "                    \"for health monitor with id %s\", health_mon.id)",
            "        self.health_mon_repo.update(db_apis.get_session(), id=health_mon.id,",
            "                                    provisioning_status=constants.ERROR)",
            "",
            "",
            "class DeleteHealthMonitorInDBByPool(DeleteHealthMonitorInDB):",
            "    \"\"\"Delete the health monitor in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool):",
            "        \"\"\"Delete the health monitor in the DB.",
            "",
            "        :param pool: A pool which health monitor should be deleted.",
            "        :returns: None",
            "        \"\"\"",
            "        super(DeleteHealthMonitorInDBByPool, self).execute(",
            "            pool.health_monitor)",
            "",
            "    def revert(self, pool, *args, **kwargs):",
            "        \"\"\"Mark the health monitor ERROR since the mark active couldn't happen",
            "",
            "        :param pool: A pool which health monitor couldn't be deleted",
            "        :returns: None",
            "        \"\"\"",
            "        super(DeleteHealthMonitorInDBByPool, self).revert(",
            "            pool.health_monitor, *args, **kwargs)",
            "",
            "",
            "class DeleteMemberInDB(BaseDatabaseTask):",
            "    \"\"\"Delete the member in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, member):",
            "        \"\"\"Delete the member in the DB",
            "",
            "        :param member: The member to be deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"DB delete member for id: %s \", member.id)",
            "        self.member_repo.delete(db_apis.get_session(), id=member.id)",
            "",
            "    def revert(self, member, *args, **kwargs):",
            "        \"\"\"Mark the member ERROR since the delete couldn't happen",
            "",
            "        :param member: Member that failed to get deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting delete in DB for member id %s\", member.id)",
            "        try:",
            "            self.member_repo.update(db_apis.get_session(), member.id,",
            "                                    provisioning_status=constants.ERROR)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update member %(mem)s \"",
            "                      \"provisioning_status to ERROR due to: %(except)s\",",
            "                      {'mem': member.id, 'except': e})",
            "",
            "",
            "class DeleteListenerInDB(BaseDatabaseTask):",
            "    \"\"\"Delete the listener in the DB.\"\"\"",
            "",
            "    def execute(self, listener):",
            "        \"\"\"Delete the listener in DB",
            "",
            "        :param listener: The listener to delete",
            "        :returns: None",
            "        \"\"\"",
            "        LOG.debug(\"Delete in DB for listener id: %s\", listener.id)",
            "        self.listener_repo.delete(db_apis.get_session(), id=listener.id)",
            "",
            "    def revert(self, listener, *args, **kwargs):",
            "        \"\"\"Mark the listener ERROR since the listener didn't delete",
            "",
            "        :param listener: Listener that failed to get deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark listener delete in DB for listener id %s\",",
            "                    listener.id)",
            "",
            "",
            "class DeletePoolInDB(BaseDatabaseTask):",
            "    \"\"\"Delete the pool in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool):",
            "        \"\"\"Delete the pool in DB",
            "",
            "        :param pool: The pool to be deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Delete in DB for pool id: %s \", pool.id)",
            "        self.pool_repo.delete(db_apis.get_session(), id=pool.id)",
            "",
            "    def revert(self, pool, *args, **kwargs):",
            "        \"\"\"Mark the pool ERROR since the delete couldn't happen",
            "",
            "        :param pool: Pool that failed to get deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting delete in DB for pool id %s\", pool.id)",
            "        try:",
            "            self.pool_repo.update(db_apis.get_session(), pool.id,",
            "                                  provisioning_status=constants.ERROR)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update pool %(pool)s \"",
            "                      \"provisioning_status to ERROR due to: %(except)s\",",
            "                      {'pool': pool.id, 'except': e})",
            "",
            "",
            "class DeleteL7PolicyInDB(BaseDatabaseTask):",
            "    \"\"\"Delete the L7 policy in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7policy):",
            "        \"\"\"Delete the l7policy in DB",
            "",
            "        :param l7policy: The l7policy to be deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Delete in DB for l7policy id: %s \", l7policy.id)",
            "        self.l7policy_repo.delete(db_apis.get_session(), id=l7policy.id)",
            "",
            "    def revert(self, l7policy, *args, **kwargs):",
            "        \"\"\"Mark the l7policy ERROR since the delete couldn't happen",
            "",
            "        :param l7policy: L7 policy that failed to get deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting delete in DB for l7policy id %s\", l7policy.id)",
            "        try:",
            "            self.l7policy_repo.update(db_apis.get_session(), l7policy.id,",
            "                                      provisioning_status=constants.ERROR)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update l7policy %(l7policy)s \"",
            "                      \"provisioning_status to ERROR due to: %(except)s\",",
            "                      {'l7policy': l7policy.id, 'except': e})",
            "",
            "",
            "class DeleteL7RuleInDB(BaseDatabaseTask):",
            "    \"\"\"Delete the L7 rule in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7rule):",
            "        \"\"\"Delete the l7rule in DB",
            "",
            "        :param l7rule: The l7rule to be deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Delete in DB for l7rule id: %s \", l7rule.id)",
            "        self.l7rule_repo.delete(db_apis.get_session(), id=l7rule.id)",
            "",
            "    def revert(self, l7rule, *args, **kwargs):",
            "        \"\"\"Mark the l7rule ERROR since the delete couldn't happen",
            "",
            "        :param l7rule: L7 rule that failed to get deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting delete in DB for l7rule id %s\", l7rule.id)",
            "        try:",
            "            self.l7rule_repo.update(db_apis.get_session(), l7rule.id,",
            "                                    provisioning_status=constants.ERROR)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update l7rule %(l7rule)s \"",
            "                      \"provisioning_status to ERROR due to: %(except)s\",",
            "                      {'l7rule': l7rule.id, 'except': e})",
            "",
            "",
            "class ReloadAmphora(BaseDatabaseTask):",
            "    \"\"\"Get an amphora object from the database.\"\"\"",
            "",
            "    def execute(self, amphora_id):",
            "        \"\"\"Get an amphora object from the database.",
            "",
            "        :param amphora_id: The amphora ID to lookup",
            "        :returns: The amphora object",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Get amphora from DB for amphora id: %s \", amphora_id)",
            "        return self.amphora_repo.get(db_apis.get_session(), id=amphora_id)",
            "",
            "",
            "class ReloadLoadBalancer(BaseDatabaseTask):",
            "    \"\"\"Get an load balancer object from the database.\"\"\"",
            "",
            "    def execute(self, loadbalancer_id, *args, **kwargs):",
            "        \"\"\"Get an load balancer object from the database.",
            "",
            "        :param loadbalancer_id: The load balancer ID to lookup",
            "        :returns: The load balancer object",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Get load balancer from DB for load balancer id: %s \",",
            "                  loadbalancer_id)",
            "        return self.loadbalancer_repo.get(db_apis.get_session(),",
            "                                          id=loadbalancer_id)",
            "",
            "",
            "class UpdateVIPAfterAllocation(BaseDatabaseTask):",
            "    \"\"\"Update a VIP associated with a given load balancer.\"\"\"",
            "",
            "    def execute(self, loadbalancer_id, vip):",
            "        \"\"\"Update a VIP associated with a given load balancer.",
            "",
            "        :param loadbalancer_id: Id of a load balancer which VIP should be",
            "               updated.",
            "        :param vip: data_models.Vip object with update data.",
            "        :returns: The load balancer object.",
            "        \"\"\"",
            "        self.repos.vip.update(db_apis.get_session(), loadbalancer_id,",
            "                              port_id=vip.port_id, subnet_id=vip.subnet_id,",
            "                              ip_address=vip.ip_address)",
            "        return self.repos.load_balancer.get(db_apis.get_session(),",
            "                                            id=loadbalancer_id)",
            "",
            "",
            "class UpdateAmphoraeVIPData(BaseDatabaseTask):",
            "    \"\"\"Update amphorae VIP data.\"\"\"",
            "",
            "    def execute(self, amps_data):",
            "        \"\"\"Update amphorae VIP data.",
            "",
            "        :param amps_data: Amphorae update dicts.",
            "        :returns: None",
            "        \"\"\"",
            "        for amp_data in amps_data:",
            "            self.repos.amphora.update(db_apis.get_session(), amp_data.id,",
            "                                      vrrp_ip=amp_data.vrrp_ip,",
            "                                      ha_ip=amp_data.ha_ip,",
            "                                      vrrp_port_id=amp_data.vrrp_port_id,",
            "                                      ha_port_id=amp_data.ha_port_id,",
            "                                      vrrp_id=1)",
            "",
            "",
            "class UpdateAmphoraVIPData(BaseDatabaseTask):",
            "    \"\"\"Update amphorae VIP data.\"\"\"",
            "",
            "    def execute(self, amp_data):",
            "        \"\"\"Update amphorae VIP data.",
            "",
            "        :param amps_data: Amphorae update dicts.",
            "        :returns: None",
            "        \"\"\"",
            "        self.repos.amphora.update(db_apis.get_session(), amp_data.id,",
            "                                  vrrp_ip=amp_data.vrrp_ip,",
            "                                  ha_ip=amp_data.ha_ip,",
            "                                  vrrp_port_id=amp_data.vrrp_port_id,",
            "                                  ha_port_id=amp_data.ha_port_id,",
            "                                  vrrp_id=1)",
            "",
            "",
            "class UpdateAmpFailoverDetails(BaseDatabaseTask):",
            "    \"\"\"Update amphora failover details in the database.\"\"\"",
            "",
            "    def execute(self, amphora, amp_data):",
            "        \"\"\"Update amphora failover details in the database.",
            "",
            "        :param amphora: The amphora to update",
            "        :param amp_data: data_models.Amphora object with update data",
            "        :returns: None",
            "        \"\"\"",
            "        # role and vrrp_priority will be updated later.",
            "        self.repos.amphora.update(db_apis.get_session(), amphora.id,",
            "                                  vrrp_ip=amp_data.vrrp_ip,",
            "                                  ha_ip=amp_data.ha_ip,",
            "                                  vrrp_port_id=amp_data.vrrp_port_id,",
            "                                  ha_port_id=amp_data.ha_port_id,",
            "                                  vrrp_id=amp_data.vrrp_id)",
            "",
            "",
            "class AssociateFailoverAmphoraWithLBID(BaseDatabaseTask):",
            "    \"\"\"Associate failover amphora with loadbalancer in the database.\"\"\"",
            "",
            "    def execute(self, amphora_id, loadbalancer_id):",
            "        \"\"\"Associate failover amphora with loadbalancer in the database.",
            "",
            "        :param amphora_id: Id of an amphora to update",
            "        :param loadbalancer_id: Id of a load balancer to be associated with",
            "               a given amphora.",
            "        :returns: None",
            "        \"\"\"",
            "        self.repos.amphora.associate(db_apis.get_session(),",
            "                                     load_balancer_id=loadbalancer_id,",
            "                                     amphora_id=amphora_id)",
            "",
            "    def revert(self, amphora_id, *args, **kwargs):",
            "        \"\"\"Remove amphora-load balancer association.",
            "",
            "        :param amphora_id: Id of an amphora that couldn't be associated",
            "               with a load balancer.",
            "        :returns: None",
            "        \"\"\"",
            "        try:",
            "            self.repos.amphora.update(db_apis.get_session(), amphora_id,",
            "                                      loadbalancer_id=None)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update amphora %(amp)s \"",
            "                      \"load balancer id to None due to: \"",
            "                      \"%(except)s\", {'amp': amphora_id, 'except': e})",
            "",
            "",
            "class MapLoadbalancerToAmphora(BaseDatabaseTask):",
            "    \"\"\"Maps and assigns a load balancer to an amphora in the database.\"\"\"",
            "",
            "    def execute(self, loadbalancer_id, server_group_id=None, flavor=None):",
            "        \"\"\"Allocates an Amphora for the load balancer in the database.",
            "",
            "        :param loadbalancer_id: The load balancer id to map to an amphora",
            "        :returns: Amphora ID if one was allocated, None if it was",
            "                  unable to allocate an Amphora",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Allocating an Amphora for load balancer with id %s\",",
            "                  loadbalancer_id)",
            "",
            "        if server_group_id is not None:",
            "            LOG.debug(\"Load balancer is using anti-affinity. Skipping spares \"",
            "                      \"pool allocation.\")",
            "            return None",
            "",
            "        # Validate the flavor is spares compatible",
            "        if not validate.is_flavor_spares_compatible(flavor):",
            "            LOG.debug(\"Load balancer has a flavor that is not compatible with \"",
            "                      \"using spares pool amphora. Skipping spares pool \"",
            "                      \"allocation.\")",
            "            return None",
            "",
            "        amp = self.amphora_repo.allocate_and_associate(",
            "            db_apis.get_session(),",
            "            loadbalancer_id)",
            "        if amp is None:",
            "            LOG.debug(\"No Amphora available for load balancer with id %s\",",
            "                      loadbalancer_id)",
            "            return None",
            "",
            "        LOG.debug(\"Allocated Amphora with id %(amp)s for load balancer \"",
            "                  \"with id %(lb)s\", {'amp': amp.id, 'lb': loadbalancer_id})",
            "",
            "        return amp.id",
            "",
            "    def revert(self, result, loadbalancer_id, *args, **kwargs):",
            "        LOG.warning(\"Reverting Amphora allocation for the load \"",
            "                    \"balancer %s in the database.\", loadbalancer_id)",
            "        self.task_utils.mark_loadbalancer_prov_status_error(loadbalancer_id)",
            "",
            "",
            "class _MarkAmphoraRoleAndPriorityInDB(BaseDatabaseTask):",
            "    \"\"\"Alter the amphora role and priority in DB.\"\"\"",
            "",
            "    def _execute(self, amphora, amp_role, vrrp_priority):",
            "        \"\"\"Alter the amphora role and priority in DB.",
            "",
            "        :param amphora: Amphora to update.",
            "        :param amp_role: Amphora role to be set.",
            "        :param vrrp_priority: VRRP priority to set.",
            "        :returns: None",
            "        \"\"\"",
            "        LOG.debug(\"Mark %(role)s in DB for amphora: %(amp)s\",",
            "                  {'role': amp_role, 'amp': amphora.id})",
            "        self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                 role=amp_role,",
            "                                 vrrp_priority=vrrp_priority)",
            "",
            "    def _revert(self, result, amphora, *args, **kwargs):",
            "        \"\"\"Removes role and vrrp_priority association.",
            "",
            "        :param result: Result of the association.",
            "        :param amphora: Amphora which role/vrrp_priority association",
            "               failed.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "",
            "        LOG.warning(\"Reverting amphora role in DB for amp id %(amp)s\",",
            "                    {'amp': amphora.id})",
            "        try:",
            "            self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                     role=None,",
            "                                     vrrp_priority=None)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update amphora %(amp)s \"",
            "                      \"role and vrrp_priority to None due to: \"",
            "                      \"%(except)s\", {'amp': amphora.id, 'except': e})",
            "",
            "",
            "class MarkAmphoraMasterInDB(_MarkAmphoraRoleAndPriorityInDB):",
            "    \"\"\"Alter the amphora role to: MASTER.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Mark amphora as MASTER in db.",
            "",
            "        :param amphora: Amphora to update role.",
            "        :returns: None",
            "        \"\"\"",
            "        amp_role = constants.ROLE_MASTER",
            "        self._execute(amphora, amp_role, constants.ROLE_MASTER_PRIORITY)",
            "",
            "    def revert(self, result, amphora, *args, **kwargs):",
            "        \"\"\"Removes amphora role association.",
            "",
            "        :param amphora: Amphora to update role.",
            "        :returns: None",
            "        \"\"\"",
            "        self._revert(result, amphora, *args, **kwargs)",
            "",
            "",
            "class MarkAmphoraBackupInDB(_MarkAmphoraRoleAndPriorityInDB):",
            "    \"\"\"Alter the amphora role to: Backup.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Mark amphora as BACKUP in db.",
            "",
            "        :param amphora: Amphora to update role.",
            "        :returns: None",
            "        \"\"\"",
            "        amp_role = constants.ROLE_BACKUP",
            "        self._execute(amphora, amp_role, constants.ROLE_BACKUP_PRIORITY)",
            "",
            "    def revert(self, result, amphora, *args, **kwargs):",
            "        \"\"\"Removes amphora role association.",
            "",
            "        :param amphora: Amphora to update role.",
            "        :returns: None",
            "        \"\"\"",
            "        self._revert(result, amphora, *args, **kwargs)",
            "",
            "",
            "class MarkAmphoraStandAloneInDB(_MarkAmphoraRoleAndPriorityInDB):",
            "    \"\"\"Alter the amphora role to: Standalone.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Mark amphora as STANDALONE in db.",
            "",
            "        :param amphora: Amphora to update role.",
            "        :returns: None",
            "        \"\"\"",
            "        amp_role = constants.ROLE_STANDALONE",
            "        self._execute(amphora, amp_role, None)",
            "",
            "    def revert(self, result, amphora, *args, **kwargs):",
            "        \"\"\"Removes amphora role association.",
            "",
            "        :param amphora: Amphora to update role.",
            "        :returns: None",
            "        \"\"\"",
            "        self._revert(result, amphora, *args, **kwargs)",
            "",
            "",
            "class MarkAmphoraAllocatedInDB(BaseDatabaseTask):",
            "    \"\"\"Will mark an amphora as allocated to a load balancer in the database.",
            "",
            "    Assume sqlalchemy made sure the DB got",
            "    retried sufficiently - so just abort",
            "    \"\"\"",
            "",
            "    def execute(self, amphora, loadbalancer_id):",
            "        \"\"\"Mark amphora as allocated to a load balancer in DB.",
            "",
            "        :param amphora: Amphora to be updated.",
            "        :param loadbalancer_id: Id of a load balancer to which an amphora",
            "               should be allocated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.info('Mark ALLOCATED in DB for amphora: %(amp)s with '",
            "                 'compute id %(comp)s for load balancer: %(lb)s',",
            "                 {",
            "                     'amp': amphora.id,",
            "                     'comp': amphora.compute_id,",
            "                     'lb': loadbalancer_id",
            "                 })",
            "        self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                 status=constants.AMPHORA_ALLOCATED,",
            "                                 compute_id=amphora.compute_id,",
            "                                 lb_network_ip=amphora.lb_network_ip,",
            "                                 load_balancer_id=loadbalancer_id)",
            "",
            "    def revert(self, result, amphora, loadbalancer_id, *args, **kwargs):",
            "        \"\"\"Mark the amphora as broken and ready to be cleaned up.",
            "",
            "        :param result: Execute task result",
            "        :param amphora: Amphora that was updated.",
            "        :param loadbalancer_id: Id of a load balancer to which an amphora",
            "               failed to be allocated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "",
            "        LOG.warning(\"Reverting mark amphora ready in DB for amp \"",
            "                    \"id %(amp)s and compute id %(comp)s\",",
            "                    {'amp': amphora.id, 'comp': amphora.compute_id})",
            "        self.task_utils.mark_amphora_status_error(amphora.id)",
            "",
            "",
            "class MarkAmphoraBootingInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the amphora as booting in the database.\"\"\"",
            "",
            "    def execute(self, amphora_id, compute_id):",
            "        \"\"\"Mark amphora booting in DB.",
            "",
            "        :param amphora_id: Id of the amphora to update",
            "        :param compute_id: Id of a compute on which an amphora resides",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark BOOTING in DB for amphora: %(amp)s with \"",
            "                  \"compute id %(id)s\", {'amp': amphora_id, 'id': compute_id})",
            "        self.amphora_repo.update(db_apis.get_session(), amphora_id,",
            "                                 status=constants.AMPHORA_BOOTING,",
            "                                 compute_id=compute_id)",
            "",
            "    def revert(self, result, amphora_id, compute_id, *args, **kwargs):",
            "        \"\"\"Mark the amphora as broken and ready to be cleaned up.",
            "",
            "        :param result: Execute task result",
            "        :param amphora_id: Id of the amphora that failed to update",
            "        :param compute_id: Id of a compute on which an amphora resides",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "",
            "        LOG.warning(\"Reverting mark amphora booting in DB for amp \"",
            "                    \"id %(amp)s and compute id %(comp)s\",",
            "                    {'amp': amphora_id, 'comp': compute_id})",
            "        try:",
            "            self.amphora_repo.update(db_apis.get_session(), amphora_id,",
            "                                     status=constants.ERROR,",
            "                                     compute_id=compute_id)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update amphora %(amp)s \"",
            "                      \"status to ERROR due to: \"",
            "                      \"%(except)s\", {'amp': amphora_id, 'except': e})",
            "",
            "",
            "class MarkAmphoraDeletedInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the amphora deleted in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Mark the amphora as deleted in DB.",
            "",
            "        :param amphora: Amphora to be updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark DELETED in DB for amphora: %(amp)s with \"",
            "                  \"compute id %(comp)s\",",
            "                  {'amp': amphora.id, 'comp': amphora.compute_id})",
            "        self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                 status=constants.DELETED)",
            "",
            "    def revert(self, amphora, *args, **kwargs):",
            "        \"\"\"Mark the amphora as broken and ready to be cleaned up.",
            "",
            "        :param amphora: Amphora that was updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark amphora deleted in DB \"",
            "                    \"for amp id %(amp)s and compute id %(comp)s\",",
            "                    {'amp': amphora.id, 'comp': amphora.compute_id})",
            "        self.task_utils.mark_amphora_status_error(amphora.id)",
            "",
            "",
            "class MarkAmphoraPendingDeleteInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the amphora pending delete in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Mark the amphora as pending delete in DB.",
            "",
            "        :param amphora: Amphora to be updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING DELETE in DB for amphora: %(amp)s \"",
            "                  \"with compute id %(id)s\",",
            "                  {'amp': amphora.id, 'id': amphora.compute_id})",
            "        self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                 status=constants.PENDING_DELETE)",
            "",
            "    def revert(self, amphora, *args, **kwargs):",
            "        \"\"\"Mark the amphora as broken and ready to be cleaned up.",
            "",
            "        :param amphora: Amphora that was updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark amphora pending delete in DB \"",
            "                    \"for amp id %(amp)s and compute id %(comp)s\",",
            "                    {'amp': amphora.id, 'comp': amphora.compute_id})",
            "        self.task_utils.mark_amphora_status_error(amphora.id)",
            "",
            "",
            "class MarkAmphoraPendingUpdateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the amphora pending update in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Mark the amphora as pending update in DB.",
            "",
            "        :param amphora: Amphora to be updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING UPDATE in DB for amphora: %(amp)s \"",
            "                  \"with compute id %(id)s\",",
            "                  {'amp': amphora.id, 'id': amphora.compute_id})",
            "        self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                 status=constants.PENDING_UPDATE)",
            "",
            "    def revert(self, amphora, *args, **kwargs):",
            "        \"\"\"Mark the amphora as broken and ready to be cleaned up.",
            "",
            "        :param amphora: Amphora that was updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark amphora pending update in DB \"",
            "                    \"for amp id %(amp)s and compute id %(comp)s\",",
            "                    {'amp': amphora.id, 'comp': amphora.compute_id})",
            "        self.task_utils.mark_amphora_status_error(amphora.id)",
            "",
            "",
            "class MarkAmphoraReadyInDB(BaseDatabaseTask):",
            "    \"\"\"This task will mark an amphora as ready in the database.",
            "",
            "    Assume sqlalchemy made sure the DB got",
            "    retried sufficiently - so just abort",
            "    \"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Mark amphora as ready in DB.",
            "",
            "        :param amphora: Amphora to be updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.info(\"Mark READY in DB for amphora: %(amp)s with compute \"",
            "                 \"id %(comp)s\",",
            "                 {\"amp\": amphora.id, \"comp\": amphora.compute_id})",
            "        self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                 status=constants.AMPHORA_READY,",
            "                                 compute_id=amphora.compute_id,",
            "                                 lb_network_ip=amphora.lb_network_ip)",
            "",
            "    def revert(self, amphora, *args, **kwargs):",
            "        \"\"\"Mark the amphora as broken and ready to be cleaned up.",
            "",
            "        :param amphora: Amphora that was updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark amphora ready in DB for amp \"",
            "                    \"id %(amp)s and compute id %(comp)s\",",
            "                    {'amp': amphora.id, 'comp': amphora.compute_id})",
            "        try:",
            "            self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                     status=constants.ERROR,",
            "                                     compute_id=amphora.compute_id,",
            "                                     lb_network_ip=amphora.lb_network_ip)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update amphora %(amp)s \"",
            "                      \"status to ERROR due to: \"",
            "                      \"%(except)s\", {'amp': amphora.id, 'except': e})",
            "",
            "",
            "class UpdateAmphoraComputeId(BaseDatabaseTask):",
            "    \"\"\"Associate amphora with a compute in DB.\"\"\"",
            "",
            "    def execute(self, amphora_id, compute_id):",
            "        \"\"\"Associate amphora with a compute in DB.",
            "",
            "        :param amphora_id: Id of the amphora to update",
            "        :param compute_id: Id of a compute on which an amphora resides",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        self.amphora_repo.update(db_apis.get_session(), amphora_id,",
            "                                 compute_id=compute_id)",
            "",
            "",
            "class UpdateAmphoraInfo(BaseDatabaseTask):",
            "    \"\"\"Update amphora with compute instance details.\"\"\"",
            "",
            "    def execute(self, amphora_id, compute_obj):",
            "        \"\"\"Update amphora with compute instance details.",
            "",
            "        :param amphora_id: Id of the amphora to update",
            "        :param compute_obj: Compute on which an amphora resides",
            "        :returns: Updated amphora object",
            "        \"\"\"",
            "        self.amphora_repo.update(",
            "            db_apis.get_session(), amphora_id,",
            "            lb_network_ip=compute_obj.lb_network_ip,",
            "            cached_zone=compute_obj.cached_zone,",
            "            image_id=compute_obj.image_id,",
            "            compute_flavor=compute_obj.compute_flavor)",
            "        return self.amphora_repo.get(db_apis.get_session(), id=amphora_id)",
            "",
            "",
            "class UpdateAmphoraDBCertExpiration(BaseDatabaseTask):",
            "    \"\"\"Update the amphora expiration date with new cert file date.\"\"\"",
            "",
            "    def execute(self, amphora_id, server_pem):",
            "        \"\"\"Update the amphora expiration date with new cert file date.",
            "",
            "        :param amphora_id: Id of the amphora to update",
            "        :param server_pem: Certificate in PEM format",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update DB cert expiry date of amphora id: %s\", amphora_id)",
            "        cert_expiration = cert_parser.get_cert_expiration(server_pem)",
            "        LOG.debug(\"Certificate expiration date is %s \", cert_expiration)",
            "        self.amphora_repo.update(db_apis.get_session(), amphora_id,",
            "                                 cert_expiration=cert_expiration)",
            "",
            "",
            "class UpdateAmphoraCertBusyToFalse(BaseDatabaseTask):",
            "    \"\"\"Update the amphora cert_busy flag to be false.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Update the amphora cert_busy flag to be false.",
            "",
            "        :param amphora: Amphora to be updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update cert_busy flag of amphora id %s to False\",",
            "                  amphora.id)",
            "        self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                 cert_busy=False)",
            "",
            "",
            "class MarkLBActiveInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the load balancer active in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def __init__(self, mark_subobjects=False, **kwargs):",
            "        super(MarkLBActiveInDB, self).__init__(**kwargs)",
            "        self.mark_subobjects = mark_subobjects",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Mark the load balancer as active in DB.",
            "",
            "        This also marks ACTIVE all sub-objects of the load balancer if",
            "        self.mark_subobjects is True.",
            "",
            "        :param loadbalancer: Load balancer object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        if self.mark_subobjects:",
            "            LOG.debug(\"Marking all listeners of loadbalancer %s ACTIVE\",",
            "                      loadbalancer.id)",
            "            for listener in loadbalancer.listeners:",
            "                self._mark_listener_status(listener, constants.ACTIVE)",
            "",
            "        LOG.info(\"Mark ACTIVE in DB for load balancer id: %s\",",
            "                 loadbalancer.id)",
            "        self.loadbalancer_repo.update(db_apis.get_session(),",
            "                                      loadbalancer.id,",
            "                                      provisioning_status=constants.ACTIVE)",
            "",
            "    def _mark_listener_status(self, listener, status):",
            "        self.listener_repo.update(db_apis.get_session(),",
            "                                  listener.id,",
            "                                  provisioning_status=status)",
            "        LOG.debug(\"Marking all l7policies of listener %s %s\",",
            "                  listener.id, status)",
            "        for l7policy in listener.l7policies:",
            "            self._mark_l7policy_status(l7policy, status)",
            "",
            "        if listener.default_pool:",
            "            LOG.debug(\"Marking default pool of listener %s %s\",",
            "                      listener.id, status)",
            "            self._mark_pool_status(listener.default_pool, status)",
            "",
            "    def _mark_l7policy_status(self, l7policy, status):",
            "        self.l7policy_repo.update(",
            "            db_apis.get_session(), l7policy.id,",
            "            provisioning_status=status)",
            "",
            "        LOG.debug(\"Marking all l7rules of l7policy %s %s\",",
            "                  l7policy.id, status)",
            "        for l7rule in l7policy.l7rules:",
            "            self._mark_l7rule_status(l7rule, status)",
            "",
            "        if l7policy.redirect_pool:",
            "            LOG.debug(\"Marking redirect pool of l7policy %s %s\",",
            "                      l7policy.id, status)",
            "            self._mark_pool_status(l7policy.redirect_pool, status)",
            "",
            "    def _mark_l7rule_status(self, l7rule, status):",
            "        self.l7rule_repo.update(",
            "            db_apis.get_session(), l7rule.id,",
            "            provisioning_status=status)",
            "",
            "    def _mark_pool_status(self, pool, status):",
            "        self.pool_repo.update(",
            "            db_apis.get_session(), pool.id,",
            "            provisioning_status=status)",
            "        if pool.health_monitor:",
            "            LOG.debug(\"Marking health monitor of pool %s %s\", pool.id, status)",
            "            self._mark_hm_status(pool.health_monitor, status)",
            "",
            "        LOG.debug(\"Marking all members of pool %s %s\", pool.id, status)",
            "        for member in pool.members:",
            "            self._mark_member_status(member, status)",
            "",
            "    def _mark_hm_status(self, hm, status):",
            "        self.health_mon_repo.update(",
            "            db_apis.get_session(), hm.id,",
            "            provisioning_status=status)",
            "",
            "    def _mark_member_status(self, member, status):",
            "        self.member_repo.update(",
            "            db_apis.get_session(), member.id,",
            "            provisioning_status=status)",
            "",
            "    def revert(self, loadbalancer, *args, **kwargs):",
            "        \"\"\"Mark the load balancer as broken and ready to be cleaned up.",
            "",
            "        This also puts all sub-objects of the load balancer to ERROR state if",
            "        self.mark_subobjects is True",
            "",
            "        :param loadbalancer: Load balancer object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        if self.mark_subobjects:",
            "            LOG.debug(\"Marking all listeners of loadbalancer %s ERROR\",",
            "                      loadbalancer.id)",
            "            for listener in loadbalancer.listeners:",
            "                try:",
            "                    self._mark_listener_status(listener, constants.ERROR)",
            "                except Exception:",
            "                    LOG.warning(\"Error updating listener %s provisioning \"",
            "                                \"status\", listener.id)",
            "",
            "        LOG.warning(\"Reverting mark load balancer deleted in DB \"",
            "                    \"for load balancer id %s\", loadbalancer.id)",
            "        self.task_utils.mark_loadbalancer_prov_status_error(loadbalancer.id)",
            "",
            "",
            "class UpdateLBServerGroupInDB(BaseDatabaseTask):",
            "    \"\"\"Update the server group id info for load balancer in DB.\"\"\"",
            "",
            "    def execute(self, loadbalancer_id, server_group_id):",
            "        \"\"\"Update the server group id info for load balancer in DB.",
            "",
            "        :param loadbalancer_id: Id of a load balancer to update",
            "        :param server_group_id: Id of a server group to associate with",
            "               the load balancer",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Server Group updated with id: %s for load balancer id: %s:\",",
            "                  server_group_id, loadbalancer_id)",
            "        self.loadbalancer_repo.update(db_apis.get_session(),",
            "                                      id=loadbalancer_id,",
            "                                      server_group_id=server_group_id)",
            "",
            "    def revert(self, loadbalancer_id, server_group_id, *args, **kwargs):",
            "        \"\"\"Remove server group information from a load balancer in DB.",
            "",
            "        :param loadbalancer_id: Id of a load balancer that failed to update",
            "        :param server_group_id: Id of a server group that couldn't be",
            "               associated with the load balancer",
            "        :returns: None",
            "        \"\"\"",
            "        LOG.warning('Reverting Server Group updated with id: %(s1)s for '",
            "                    'load balancer id: %(s2)s ',",
            "                    {'s1': server_group_id, 's2': loadbalancer_id})",
            "        try:",
            "            self.loadbalancer_repo.update(db_apis.get_session(),",
            "                                          id=loadbalancer_id,",
            "                                          server_group_id=None)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update load balancer %(lb)s \"",
            "                      \"server_group_id to None due to: \"",
            "                      \"%(except)s\", {'lb': loadbalancer_id, 'except': e})",
            "",
            "",
            "class MarkLBDeletedInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the load balancer deleted in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Mark the load balancer as deleted in DB.",
            "",
            "        :param loadbalancer: Load balancer object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark DELETED in DB for load balancer id: %s\",",
            "                  loadbalancer.id)",
            "        self.loadbalancer_repo.update(db_apis.get_session(),",
            "                                      loadbalancer.id,",
            "                                      provisioning_status=constants.DELETED)",
            "",
            "    def revert(self, loadbalancer, *args, **kwargs):",
            "        \"\"\"Mark the load balancer as broken and ready to be cleaned up.",
            "",
            "        :param loadbalancer: Load balancer object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark load balancer deleted in DB \"",
            "                    \"for load balancer id %s\", loadbalancer.id)",
            "        self.task_utils.mark_loadbalancer_prov_status_error(loadbalancer.id)",
            "",
            "",
            "class MarkLBPendingDeleteInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the load balancer pending delete in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Mark the load balancer as pending delete in DB.",
            "",
            "        :param loadbalancer: Load balancer object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING DELETE in DB for load balancer id: %s\",",
            "                  loadbalancer.id)",
            "        self.loadbalancer_repo.update(db_apis.get_session(),",
            "                                      loadbalancer.id,",
            "                                      provisioning_status=(constants.",
            "                                                           PENDING_DELETE))",
            "",
            "    def revert(self, loadbalancer, *args, **kwargs):",
            "        \"\"\"Mark the load balancer as broken and ready to be cleaned up.",
            "",
            "        :param loadbalancer: Load balancer object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark load balancer pending delete in DB \"",
            "                    \"for load balancer id %s\", loadbalancer.id)",
            "        self.task_utils.mark_loadbalancer_prov_status_error(loadbalancer.id)",
            "",
            "",
            "class MarkLBAndListenersActiveInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the load balancer and specified listeners active in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, loadbalancer, listeners):",
            "        \"\"\"Mark the load balancer and listeners as active in DB.",
            "",
            "        :param loadbalancer: Load balancer object to be updated",
            "        :param listeners: Listener objects to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark ACTIVE in DB for load balancer id: %s \"",
            "                  \"and listener ids: %s\", loadbalancer.id,",
            "                  ', '.join([l.id for l in listeners]))",
            "        self.loadbalancer_repo.update(db_apis.get_session(),",
            "                                      loadbalancer.id,",
            "                                      provisioning_status=constants.ACTIVE)",
            "        for listener in listeners:",
            "            self.listener_repo.update(db_apis.get_session(), listener.id,",
            "                                      provisioning_status=constants.ACTIVE)",
            "",
            "    def revert(self, loadbalancer, listeners, *args, **kwargs):",
            "        \"\"\"Mark the load balancer and listeners as broken.",
            "",
            "        :param loadbalancer: Load balancer object that failed to update",
            "        :param listeners: Listener objects that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark load balancer and listeners active in DB \"",
            "                    \"for load balancer id %(LB)s and listener ids: %(list)s\",",
            "                    {'LB': loadbalancer.id,",
            "                     'list': ', '.join([l.id for l in listeners])})",
            "        self.task_utils.mark_loadbalancer_prov_status_error(loadbalancer.id)",
            "        for listener in listeners:",
            "            self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "",
            "class MarkListenerActiveInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the listener active in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, listener):",
            "        \"\"\"Mark the listener as active in DB",
            "",
            "        :param listener: The listener to be marked active",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark ACTIVE in DB for listener id: %s \", listener.id)",
            "        self.listener_repo.update(db_apis.get_session(), listener.id,",
            "                                  provisioning_status=constants.ACTIVE)",
            "",
            "    def revert(self, listener, *args, **kwargs):",
            "        \"\"\"Mark the listener ERROR since the delete couldn't happen",
            "",
            "        :param listener: The listener that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark listener active in DB \"",
            "                    \"for listener id %s\", listener.id)",
            "        self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "",
            "class MarkListenerDeletedInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the listener deleted in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, listener):",
            "        \"\"\"Mark the listener as deleted in DB",
            "",
            "        :param listener: The listener to be marked deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark DELETED in DB for listener id: %s \", listener.id)",
            "        self.listener_repo.update(db_apis.get_session(), listener.id,",
            "                                  provisioning_status=constants.DELETED)",
            "",
            "    def revert(self, listener, *args, **kwargs):",
            "        \"\"\"Mark the listener ERROR since the delete couldn't happen",
            "",
            "        :param listener: The listener that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark listener deleted in DB \"",
            "                    \"for listener id %s\", listener.id)",
            "        self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "",
            "class MarkListenerPendingDeleteInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the listener pending delete in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, listener):",
            "        \"\"\"Mark the listener as pending delete in DB.",
            "",
            "        :param listener: The listener to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING DELETE in DB for listener id: %s\",",
            "                  listener.id)",
            "        self.listener_repo.update(db_apis.get_session(), listener.id,",
            "                                  provisioning_status=constants.PENDING_DELETE)",
            "",
            "    def revert(self, listener, *args, **kwargs):",
            "        \"\"\"Mark the listener as broken and ready to be cleaned up.",
            "",
            "        :param listener: The listener that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark listener pending delete in DB \"",
            "                    \"for listener id %s\", listener.id)",
            "        self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "",
            "class UpdateLoadbalancerInDB(BaseDatabaseTask):",
            "    \"\"\"Update the loadbalancer in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, loadbalancer, update_dict):",
            "        \"\"\"Update the loadbalancer in the DB",
            "",
            "        :param loadbalancer: The load balancer to be updated",
            "        :param update_dict: The dictionary of updates to apply",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update DB for loadbalancer id: %s \", loadbalancer.id)",
            "        if update_dict.get('vip'):",
            "            vip_dict = update_dict.pop('vip')",
            "            self.vip_repo.update(db_apis.get_session(),",
            "                                 loadbalancer.vip.load_balancer_id,",
            "                                 **vip_dict)",
            "        self.loadbalancer_repo.update(db_apis.get_session(), loadbalancer.id,",
            "                                      **update_dict)",
            "",
            "    def revert(self, loadbalancer, *args, **kwargs):",
            "        \"\"\"Mark the loadbalancer ERROR since the update couldn't happen",
            "",
            "        :param loadbalancer: The load balancer that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting update loadbalancer in DB \"",
            "                    \"for loadbalancer id %s\", loadbalancer.id)",
            "",
            "        self.task_utils.mark_loadbalancer_prov_status_error(loadbalancer.id)",
            "",
            "",
            "class UpdateHealthMonInDB(BaseDatabaseTask):",
            "    \"\"\"Update the health monitor in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, health_mon, update_dict):",
            "        \"\"\"Update the health monitor in the DB",
            "",
            "        :param health_mon: The health monitor to be updated",
            "        :param update_dict: The dictionary of updates to apply",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update DB for health monitor id: %s \", health_mon.id)",
            "        self.health_mon_repo.update(db_apis.get_session(), health_mon.id,",
            "                                    **update_dict)",
            "",
            "    def revert(self, health_mon, *args, **kwargs):",
            "        \"\"\"Mark the health monitor ERROR since the update couldn't happen",
            "",
            "        :param health_mon: The health monitor that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting update health monitor in DB \"",
            "                    \"for health monitor id %s\", health_mon.id)",
            "        try:",
            "            self.health_mon_repo.update(db_apis.get_session(),",
            "                                        health_mon.id,",
            "                                        provisioning_status=constants.ERROR)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update health monitor %(hm)s \"",
            "                      \"provisioning_status to ERROR due to: %(except)s\",",
            "                      {'hm': health_mon.id, 'except': e})",
            "",
            "",
            "class UpdateListenerInDB(BaseDatabaseTask):",
            "    \"\"\"Update the listener in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, listener, update_dict):",
            "        \"\"\"Update the listener in the DB",
            "",
            "        :param listener: The listener to be updated",
            "        :param update_dict: The dictionary of updates to apply",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update DB for listener id: %s \", listener.id)",
            "        self.listener_repo.update(db_apis.get_session(), listener.id,",
            "                                  **update_dict)",
            "",
            "    def revert(self, listener, *args, **kwargs):",
            "        \"\"\"Mark the listener ERROR since the update couldn't happen",
            "",
            "        :param listener: The listener that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting update listener in DB \"",
            "                    \"for listener id %s\", listener.id)",
            "        self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "",
            "class UpdateMemberInDB(BaseDatabaseTask):",
            "    \"\"\"Update the member in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, member, update_dict):",
            "        \"\"\"Update the member in the DB",
            "",
            "        :param member: The member to be updated",
            "        :param update_dict: The dictionary of updates to apply",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update DB for member id: %s \", member.id)",
            "        self.member_repo.update(db_apis.get_session(), member.id,",
            "                                **update_dict)",
            "",
            "    def revert(self, member, *args, **kwargs):",
            "        \"\"\"Mark the member ERROR since the update couldn't happen",
            "",
            "        :param member: The member that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting update member in DB \"",
            "                    \"for member id %s\", member.id)",
            "        try:",
            "            self.member_repo.update(db_apis.get_session(), member.id,",
            "                                    provisioning_status=constants.ERROR)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update member %(member)s provisioning_status \"",
            "                      \"to ERROR due to: %(except)s\", {'member': member.id,",
            "                                                      'except': e})",
            "",
            "",
            "class UpdatePoolInDB(BaseDatabaseTask):",
            "    \"\"\"Update the pool in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool, update_dict):",
            "        \"\"\"Update the pool in the DB",
            "",
            "        :param pool: The pool to be updated",
            "        :param update_dict: The dictionary of updates to apply",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update DB for pool id: %s \", pool.id)",
            "        self.repos.update_pool_and_sp(db_apis.get_session(), pool.id,",
            "                                      update_dict)",
            "",
            "    def revert(self, pool, *args, **kwargs):",
            "        \"\"\"Mark the pool ERROR since the update couldn't happen",
            "",
            "        :param pool: The pool that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting update pool in DB for pool id %s\", pool.id)",
            "        try:",
            "            self.repos.update_pool_and_sp(",
            "                db_apis.get_session(), pool.id,",
            "                dict(provisioning_status=constants.ERROR))",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update pool %(pool)s provisioning_status to \"",
            "                      \"ERROR due to: %(except)s\", {'pool': pool.id,",
            "                                                   'except': e})",
            "",
            "",
            "class UpdateL7PolicyInDB(BaseDatabaseTask):",
            "    \"\"\"Update the L7 policy in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7policy, update_dict):",
            "        \"\"\"Update the L7 policy in the DB",
            "",
            "        :param l7policy: The L7 policy to be updated",
            "        :param update_dict: The dictionary of updates to apply",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update DB for l7policy id: %s \", l7policy.id)",
            "        self.l7policy_repo.update(db_apis.get_session(), l7policy.id,",
            "                                  **update_dict)",
            "",
            "    def revert(self, l7policy, *args, **kwargs):",
            "        \"\"\"Mark the l7policy ERROR since the update couldn't happen",
            "",
            "        :param l7policy: L7 policy that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting update l7policy in DB \"",
            "                    \"for l7policy id %s\", l7policy.id)",
            "        try:",
            "            self.l7policy_repo.update(db_apis.get_session(), l7policy.id,",
            "                                      provisioning_status=constants.ERROR)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update l7policy %(l7p)s provisioning_status \"",
            "                      \"to ERROR due to: %(except)s\", {'l7p': l7policy.id,",
            "                                                      'except': e})",
            "",
            "",
            "class UpdateL7RuleInDB(BaseDatabaseTask):",
            "    \"\"\"Update the L7 rule in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7rule, update_dict):",
            "        \"\"\"Update the L7 rule in the DB",
            "",
            "        :param l7rule: The L7 rule to be updated",
            "        :param update_dict: The dictionary of updates to apply",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update DB for l7rule id: %s \", l7rule.id)",
            "        self.l7rule_repo.update(db_apis.get_session(), l7rule.id,",
            "                                **update_dict)",
            "",
            "    def revert(self, l7rule, *args, **kwargs):",
            "        \"\"\"Mark the L7 rule ERROR since the update couldn't happen",
            "",
            "        :param l7rule: L7 rule that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting update l7rule in DB \"",
            "                    \"for l7rule id %s\", l7rule.id)",
            "        try:",
            "            self.l7policy_repo.update(db_apis.get_session(),",
            "                                      l7rule.l7policy.id,",
            "                                      provisioning_status=constants.ERROR)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update L7rule %(l7r)s provisioning_status to \"",
            "                      \"ERROR due to: %(except)s\", {'l7r': l7rule.l7policy.id,",
            "                                                   'except': e})",
            "",
            "",
            "class GetAmphoraDetails(BaseDatabaseTask):",
            "    \"\"\"Task to retrieve amphora network details.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Retrieve amphora network details.",
            "",
            "        :param amphora: Amphora which network details are required",
            "        :returns: data_models.Amphora object",
            "        \"\"\"",
            "        return data_models.Amphora(id=amphora.id,",
            "                                   vrrp_ip=amphora.vrrp_ip,",
            "                                   ha_ip=amphora.ha_ip,",
            "                                   vrrp_port_id=amphora.vrrp_port_id,",
            "                                   ha_port_id=amphora.ha_port_id,",
            "                                   role=amphora.role,",
            "                                   vrrp_id=amphora.vrrp_id,",
            "                                   vrrp_priority=amphora.vrrp_priority)",
            "",
            "",
            "class GetAmphoraeFromLoadbalancer(BaseDatabaseTask):",
            "    \"\"\"Task to pull the listeners from a loadbalancer.\"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Pull the amphorae from a loadbalancer.",
            "",
            "        :param loadbalancer: Load balancer which listeners are required",
            "        :returns: A list of Listener objects",
            "        \"\"\"",
            "        amphorae = []",
            "        for amp in loadbalancer.amphorae:",
            "            a = self.amphora_repo.get(db_apis.get_session(), id=amp.id,",
            "                                      show_deleted=False)",
            "            if a is None:",
            "                continue",
            "            amphorae.append(a)",
            "        return amphorae",
            "",
            "",
            "class GetListenersFromLoadbalancer(BaseDatabaseTask):",
            "    \"\"\"Task to pull the listeners from a loadbalancer.\"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Pull the listeners from a loadbalancer.",
            "",
            "        :param loadbalancer: Load balancer which listeners are required",
            "        :returns: A list of Listener objects",
            "        \"\"\"",
            "        listeners = []",
            "        for listener in loadbalancer.listeners:",
            "            l = self.listener_repo.get(db_apis.get_session(), id=listener.id)",
            "            l.load_balancer = loadbalancer",
            "            listeners.append(l)",
            "        return listeners",
            "",
            "",
            "class GetVipFromLoadbalancer(BaseDatabaseTask):",
            "    \"\"\"Task to pull the vip from a loadbalancer.\"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Pull the vip from a loadbalancer.",
            "",
            "        :param loadbalancer: Load balancer which VIP is required",
            "        :returns: VIP associated with a given load balancer",
            "        \"\"\"",
            "        return loadbalancer.vip",
            "",
            "",
            "class CreateVRRPGroupForLB(BaseDatabaseTask):",
            "    \"\"\"Create a VRRP group for a load balancer.\"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Create a VRRP group for a load balancer.",
            "",
            "        :param loadbalancer: Load balancer for which a VRRP group",
            "               should be created",
            "        :returns: Updated load balancer",
            "        \"\"\"",
            "        try:",
            "            loadbalancer.vrrp_group = self.repos.vrrpgroup.create(",
            "                db_apis.get_session(),",
            "                load_balancer_id=loadbalancer.id,",
            "                vrrp_group_name=str(loadbalancer.id).replace('-', ''),",
            "                vrrp_auth_type=constants.VRRP_AUTH_DEFAULT,",
            "                vrrp_auth_pass=uuidutils.generate_uuid().replace('-', '')[0:7],",
            "                advert_int=CONF.keepalived_vrrp.vrrp_advert_int)",
            "        except odb_exceptions.DBDuplicateEntry:",
            "            LOG.debug('VRRP_GROUP entry already exists for load balancer, '",
            "                      'skipping create.')",
            "        return loadbalancer",
            "",
            "",
            "class DisableAmphoraHealthMonitoring(BaseDatabaseTask):",
            "    \"\"\"Disable amphora health monitoring.",
            "",
            "    This disables amphora health monitoring by removing it from",
            "    the amphora_health table.",
            "    \"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Disable health monitoring for an amphora",
            "",
            "        :param amphora: The amphora to disable health monitoring for",
            "        :returns: None",
            "        \"\"\"",
            "        self._delete_from_amp_health(amphora.id)",
            "",
            "",
            "class DisableLBAmphoraeHealthMonitoring(BaseDatabaseTask):",
            "    \"\"\"Disable health monitoring on the LB amphorae.",
            "",
            "    This disables amphora health monitoring by removing it from",
            "    the amphora_health table for each amphora on a load balancer.",
            "    \"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Disable health monitoring for amphora on a load balancer",
            "",
            "        :param loadbalancer: The load balancer to disable health monitoring on",
            "        :returns: None",
            "        \"\"\"",
            "        for amphora in loadbalancer.amphorae:",
            "            self._delete_from_amp_health(amphora.id)",
            "",
            "",
            "class MarkAmphoraHealthBusy(BaseDatabaseTask):",
            "    \"\"\"Mark amphora health monitoring busy.",
            "",
            "    This prevents amphora failover by marking the amphora busy in",
            "    the amphora_health table.",
            "    \"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Mark amphora health monitoring busy",
            "",
            "        :param amphora: The amphora to mark amphora health busy",
            "        :returns: None",
            "        \"\"\"",
            "        self._mark_amp_health_busy(amphora.id)",
            "",
            "",
            "class MarkLBAmphoraeHealthBusy(BaseDatabaseTask):",
            "    \"\"\"Mark amphorae health monitoring busy for the LB.",
            "",
            "    This prevents amphorae failover by marking each amphora of a given",
            "    load balancer busy in the amphora_health table.",
            "    \"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Marks amphorae health busy for each amphora on a load balancer",
            "",
            "        :param loadbalancer: The load balancer to mark amphorae health busy",
            "        :returns: None",
            "        \"\"\"",
            "        for amphora in loadbalancer.amphorae:",
            "            self._mark_amp_health_busy(amphora.id)",
            "",
            "",
            "class MarkHealthMonitorActiveInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the health monitor ACTIVE in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, health_mon):",
            "        \"\"\"Mark the health monitor ACTIVE in DB.",
            "",
            "        :param health_mon: Health Monitor object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark ACTIVE in DB for health monitor id: %s\",",
            "                  health_mon.id)",
            "",
            "        op_status = (constants.ONLINE if health_mon.enabled",
            "                     else constants.OFFLINE)",
            "        self.health_mon_repo.update(db_apis.get_session(),",
            "                                    health_mon.id,",
            "                                    provisioning_status=constants.ACTIVE,",
            "                                    operating_status=op_status)",
            "",
            "    def revert(self, health_mon, *args, **kwargs):",
            "        \"\"\"Mark the health monitor as broken",
            "",
            "        :param health_mon: Health Monitor object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark health montor ACTIVE in DB \"",
            "                    \"for health monitor id %s\", health_mon.id)",
            "        self.task_utils.mark_health_mon_prov_status_error(health_mon.id)",
            "",
            "",
            "class MarkHealthMonitorPendingCreateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the health monitor pending create in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, health_mon):",
            "        \"\"\"Mark the health monitor as pending create in DB.",
            "",
            "        :param health_mon: Health Monitor object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING CREATE in DB for health monitor id: %s\",",
            "                  health_mon.id)",
            "        self.health_mon_repo.update(db_apis.get_session(),",
            "                                    health_mon.id,",
            "                                    provisioning_status=(constants.",
            "                                                         PENDING_CREATE))",
            "",
            "    def revert(self, health_mon, *args, **kwargs):",
            "        \"\"\"Mark the health monitor as broken",
            "",
            "        :param health_mon: Health Monitor object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark health monitor pending create in DB \"",
            "                    \"for health monitor id %s\", health_mon.id)",
            "        self.task_utils.mark_health_mon_prov_status_error(health_mon.id)",
            "",
            "",
            "class MarkHealthMonitorPendingDeleteInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the health monitor pending delete in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, health_mon):",
            "        \"\"\"Mark the health monitor as pending delete in DB.",
            "",
            "        :param health_mon: Health Monitor object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING DELETE in DB for health monitor id: %s\",",
            "                  health_mon.id)",
            "        self.health_mon_repo.update(db_apis.get_session(),",
            "                                    health_mon.id,",
            "                                    provisioning_status=(constants.",
            "                                                         PENDING_DELETE))",
            "",
            "    def revert(self, health_mon, *args, **kwargs):",
            "        \"\"\"Mark the health monitor as broken",
            "",
            "        :param health_mon: Health Monitor object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark health monitor pending delete in DB \"",
            "                    \"for health monitor id %s\", health_mon.id)",
            "        self.task_utils.mark_health_mon_prov_status_error(health_mon.id)",
            "",
            "",
            "class MarkHealthMonitorPendingUpdateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the health monitor pending update in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, health_mon):",
            "        \"\"\"Mark the health monitor as pending update in DB.",
            "",
            "        :param health_mon: Health Monitor object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING UPDATE in DB for health monitor id: %s\",",
            "                  health_mon.id)",
            "        self.health_mon_repo.update(db_apis.get_session(),",
            "                                    health_mon.id,",
            "                                    provisioning_status=(constants.",
            "                                                         PENDING_UPDATE))",
            "",
            "    def revert(self, health_mon, *args, **kwargs):",
            "        \"\"\"Mark the health monitor as broken",
            "",
            "        :param health_mon: Health Monitor object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark health monitor pending update in DB \"",
            "                    \"for health monitor id %s\", health_mon.id)",
            "        self.task_utils.mark_health_mon_prov_status_error(health_mon.id)",
            "",
            "",
            "class MarkL7PolicyActiveInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the l7policy ACTIVE in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7policy):",
            "        \"\"\"Mark the l7policy ACTIVE in DB.",
            "",
            "        :param l7policy: L7Policy object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark ACTIVE in DB for l7policy id: %s\",",
            "                  l7policy.id)",
            "",
            "        op_status = constants.ONLINE if l7policy.enabled else constants.OFFLINE",
            "        self.l7policy_repo.update(db_apis.get_session(),",
            "                                  l7policy.id,",
            "                                  provisioning_status=constants.ACTIVE,",
            "                                  operating_status=op_status)",
            "",
            "    def revert(self, l7policy, *args, **kwargs):",
            "        \"\"\"Mark the l7policy as broken",
            "",
            "        :param l7policy: L7Policy object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark l7policy ACTIVE in DB \"",
            "                    \"for l7policy id %s\", l7policy.id)",
            "        self.task_utils.mark_l7policy_prov_status_error(l7policy.id)",
            "",
            "",
            "class MarkL7PolicyPendingCreateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the l7policy pending create in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7policy):",
            "        \"\"\"Mark the l7policy as pending create in DB.",
            "",
            "        :param l7policy: L7Policy object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING CREATE in DB for l7policy id: %s\",",
            "                  l7policy.id)",
            "        self.l7policy_repo.update(db_apis.get_session(),",
            "                                  l7policy.id,",
            "                                  provisioning_status=constants.PENDING_CREATE)",
            "",
            "    def revert(self, l7policy, *args, **kwargs):",
            "        \"\"\"Mark the l7policy as broken",
            "",
            "        :param l7policy: L7Policy object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark l7policy pending create in DB \"",
            "                    \"for l7policy id %s\", l7policy.id)",
            "        self.task_utils.mark_l7policy_prov_status_error(l7policy.id)",
            "",
            "",
            "class MarkL7PolicyPendingDeleteInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the l7policy pending delete in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7policy):",
            "        \"\"\"Mark the l7policy as pending delete in DB.",
            "",
            "        :param l7policy: L7Policy object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING DELETE in DB for l7policy id: %s\",",
            "                  l7policy.id)",
            "        self.l7policy_repo.update(db_apis.get_session(),",
            "                                  l7policy.id,",
            "                                  provisioning_status=constants.PENDING_DELETE)",
            "",
            "    def revert(self, l7policy, *args, **kwargs):",
            "        \"\"\"Mark the l7policy as broken",
            "",
            "        :param l7policy: L7Policy object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark l7policy pending delete in DB \"",
            "                    \"for l7policy id %s\", l7policy.id)",
            "        self.task_utils.mark_l7policy_prov_status_error(l7policy.id)",
            "",
            "",
            "class MarkL7PolicyPendingUpdateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the l7policy pending update in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7policy):",
            "        \"\"\"Mark the l7policy as pending update in DB.",
            "",
            "        :param l7policy: L7Policy object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING UPDATE in DB for l7policy id: %s\",",
            "                  l7policy.id)",
            "        self.l7policy_repo.update(db_apis.get_session(),",
            "                                  l7policy.id,",
            "                                  provisioning_status=(constants.",
            "                                                       PENDING_UPDATE))",
            "",
            "    def revert(self, l7policy, *args, **kwargs):",
            "        \"\"\"Mark the l7policy as broken",
            "",
            "        :param l7policy: L7Policy object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark l7policy pending update in DB \"",
            "                    \"for l7policy id %s\", l7policy.id)",
            "        self.task_utils.mark_l7policy_prov_status_error(l7policy.id)",
            "",
            "",
            "class MarkL7RuleActiveInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the l7rule ACTIVE in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7rule):",
            "        \"\"\"Mark the l7rule ACTIVE in DB.",
            "",
            "        :param l7rule: L7Rule object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark ACTIVE in DB for l7rule id: %s\",",
            "                  l7rule.id)",
            "        op_status = constants.ONLINE if l7rule.enabled else constants.OFFLINE",
            "        self.l7rule_repo.update(db_apis.get_session(),",
            "                                l7rule.id,",
            "                                provisioning_status=constants.ACTIVE,",
            "                                operating_status=op_status)",
            "",
            "    def revert(self, l7rule, *args, **kwargs):",
            "        \"\"\"Mark the l7rule as broken",
            "",
            "        :param l7rule: L7Rule object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark l7rule ACTIVE in DB \"",
            "                    \"for l7rule id %s\", l7rule.id)",
            "        self.task_utils.mark_l7rule_prov_status_error(l7rule.id)",
            "",
            "",
            "class MarkL7RulePendingCreateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the l7rule pending create in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7rule):",
            "        \"\"\"Mark the l7rule as pending create in DB.",
            "",
            "        :param l7rule: L7Rule object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING CREATE in DB for l7rule id: %s\",",
            "                  l7rule.id)",
            "        self.l7rule_repo.update(db_apis.get_session(),",
            "                                l7rule.id,",
            "                                provisioning_status=constants.PENDING_CREATE)",
            "",
            "    def revert(self, l7rule, *args, **kwargs):",
            "        \"\"\"Mark the l7rule as broken",
            "",
            "        :param l7rule: L7Rule object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark l7rule pending create in DB \"",
            "                    \"for l7rule id %s\", l7rule.id)",
            "        self.task_utils.mark_l7rule_prov_status_error(l7rule.id)",
            "",
            "",
            "class MarkL7RulePendingDeleteInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the l7rule pending delete in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7rule):",
            "        \"\"\"Mark the l7rule as pending delete in DB.",
            "",
            "        :param l7rule: L7Rule object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING DELETE in DB for l7rule id: %s\",",
            "                  l7rule.id)",
            "        self.l7rule_repo.update(db_apis.get_session(),",
            "                                l7rule.id,",
            "                                provisioning_status=constants.PENDING_DELETE)",
            "",
            "    def revert(self, l7rule, *args, **kwargs):",
            "        \"\"\"Mark the l7rule as broken",
            "",
            "        :param l7rule: L7Rule object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark l7rule pending delete in DB \"",
            "                    \"for l7rule id %s\", l7rule.id)",
            "        self.task_utils.mark_l7rule_prov_status_error(l7rule.id)",
            "",
            "",
            "class MarkL7RulePendingUpdateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the l7rule pending update in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7rule):",
            "        \"\"\"Mark the l7rule as pending update in DB.",
            "",
            "        :param l7rule: L7Rule object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING UPDATE in DB for l7rule id: %s\",",
            "                  l7rule.id)",
            "        self.l7rule_repo.update(db_apis.get_session(),",
            "                                l7rule.id,",
            "                                provisioning_status=constants.PENDING_UPDATE)",
            "",
            "    def revert(self, l7rule, *args, **kwargs):",
            "        \"\"\"Mark the l7rule as broken",
            "",
            "        :param l7rule: L7Rule object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark l7rule pending update in DB \"",
            "                    \"for l7rule id %s\", l7rule.id)",
            "        self.task_utils.mark_l7rule_prov_status_error(l7rule.id)",
            "",
            "",
            "class MarkMemberActiveInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the member ACTIVE in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, member):",
            "        \"\"\"Mark the member ACTIVE in DB.",
            "",
            "        :param member: Member object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark ACTIVE in DB for member id: %s\", member.id)",
            "        self.member_repo.update(db_apis.get_session(),",
            "                                member.id,",
            "                                provisioning_status=constants.ACTIVE)",
            "",
            "    def revert(self, member, *args, **kwargs):",
            "        \"\"\"Mark the member as broken",
            "",
            "        :param member: Member object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark member ACTIVE in DB \"",
            "                    \"for member id %s\", member.id)",
            "        self.task_utils.mark_member_prov_status_error(member.id)",
            "",
            "",
            "class MarkMemberPendingCreateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the member pending create in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, member):",
            "        \"\"\"Mark the member as pending create in DB.",
            "",
            "        :param member: Member object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING CREATE in DB for member id: %s\", member.id)",
            "        self.member_repo.update(db_apis.get_session(),",
            "                                member.id,",
            "                                provisioning_status=constants.PENDING_CREATE)",
            "",
            "    def revert(self, member, *args, **kwargs):",
            "        \"\"\"Mark the member as broken",
            "",
            "        :param member: Member object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark member pending create in DB \"",
            "                    \"for member id %s\", member.id)",
            "        self.task_utils.mark_member_prov_status_error(member.id)",
            "",
            "",
            "class MarkMemberPendingDeleteInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the member pending delete in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, member):",
            "        \"\"\"Mark the member as pending delete in DB.",
            "",
            "        :param member: Member object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING DELETE in DB for member id: %s\", member.id)",
            "        self.member_repo.update(db_apis.get_session(),",
            "                                member.id,",
            "                                provisioning_status=constants.PENDING_DELETE)",
            "",
            "    def revert(self, member, *args, **kwargs):",
            "        \"\"\"Mark the member as broken",
            "",
            "        :param member: Member object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark member pending delete in DB \"",
            "                    \"for member id %s\", member.id)",
            "        self.task_utils.mark_member_prov_status_error(member.id)",
            "",
            "",
            "class MarkMemberPendingUpdateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the member pending update in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, member):",
            "        \"\"\"Mark the member as pending update in DB.",
            "",
            "        :param member: Member object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING UPDATE in DB for member id: %s\",",
            "                  member.id)",
            "        self.member_repo.update(db_apis.get_session(),",
            "                                member.id,",
            "                                provisioning_status=constants.PENDING_UPDATE)",
            "",
            "    def revert(self, member, *args, **kwargs):",
            "        \"\"\"Mark the member as broken",
            "",
            "        :param member: Member object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark member pending update in DB \"",
            "                    \"for member id %s\", member.id)",
            "        self.task_utils.mark_member_prov_status_error(member.id)",
            "",
            "",
            "class MarkPoolActiveInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the pool ACTIVE in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool):",
            "        \"\"\"Mark the pool ACTIVE in DB.",
            "",
            "        :param pool: Pool object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark ACTIVE in DB for pool id: %s\",",
            "                  pool.id)",
            "        self.pool_repo.update(db_apis.get_session(),",
            "                              pool.id,",
            "                              provisioning_status=constants.ACTIVE)",
            "",
            "    def revert(self, pool, *args, **kwargs):",
            "        \"\"\"Mark the pool as broken",
            "",
            "        :param pool: Pool object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark pool ACTIVE in DB for pool id %s\", pool.id)",
            "        self.task_utils.mark_pool_prov_status_error(pool.id)",
            "",
            "",
            "class MarkPoolPendingCreateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the pool pending create in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool):",
            "        \"\"\"Mark the pool as pending create in DB.",
            "",
            "        :param pool: Pool object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING CREATE in DB for pool id: %s\",",
            "                  pool.id)",
            "        self.pool_repo.update(db_apis.get_session(),",
            "                              pool.id,",
            "                              provisioning_status=constants.PENDING_CREATE)",
            "",
            "    def revert(self, pool, *args, **kwargs):",
            "        \"\"\"Mark the pool as broken",
            "",
            "        :param pool: Pool object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark pool pending create in DB \"",
            "                    \"for pool id %s\", pool.id)",
            "        self.task_utils.mark_pool_prov_status_error(pool.id)",
            "",
            "",
            "class MarkPoolPendingDeleteInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the pool pending delete in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool):",
            "        \"\"\"Mark the pool as pending delete in DB.",
            "",
            "        :param pool: Pool object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING DELETE in DB for pool id: %s\",",
            "                  pool.id)",
            "        self.pool_repo.update(db_apis.get_session(),",
            "                              pool.id,",
            "                              provisioning_status=constants.PENDING_DELETE)",
            "",
            "    def revert(self, pool, *args, **kwargs):",
            "        \"\"\"Mark the pool as broken",
            "",
            "        :param pool: Pool object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark pool pending delete in DB \"",
            "                    \"for pool id %s\", pool.id)",
            "        self.task_utils.mark_pool_prov_status_error(pool.id)",
            "",
            "",
            "class MarkPoolPendingUpdateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the pool pending update in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool):",
            "        \"\"\"Mark the pool as pending update in DB.",
            "",
            "        :param pool: Pool object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING UPDATE in DB for pool id: %s\",",
            "                  pool.id)",
            "        self.pool_repo.update(db_apis.get_session(),",
            "                              pool.id,",
            "                              provisioning_status=constants.PENDING_UPDATE)",
            "",
            "    def revert(self, pool, *args, **kwargs):",
            "        \"\"\"Mark the pool as broken",
            "",
            "        :param pool: Pool object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark pool pending update in DB \"",
            "                    \"for pool id %s\", pool.id)",
            "        self.task_utils.mark_pool_prov_status_error(pool.id)",
            "",
            "",
            "class DecrementHealthMonitorQuota(BaseDatabaseTask):",
            "    \"\"\"Decrements the health monitor quota for a project.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, health_mon):",
            "        \"\"\"Decrements the health monitor quota.",
            "",
            "        :param health_mon: The health monitor to decrement the quota on.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Decrementing health monitor quota for \"",
            "                  \"project: %s \", health_mon.project_id)",
            "",
            "        lock_session = db_apis.get_session(autocommit=False)",
            "        try:",
            "            self.repos.decrement_quota(lock_session,",
            "                                       data_models.HealthMonitor,",
            "                                       health_mon.project_id)",
            "            lock_session.commit()",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                LOG.error('Failed to decrement health monitor quota for '",
            "                          'project: %(proj)s the project may have excess '",
            "                          'quota in use.', {'proj': health_mon.project_id})",
            "                lock_session.rollback()",
            "",
            "    def revert(self, health_mon, result, *args, **kwargs):",
            "        \"\"\"Re-apply the quota",
            "",
            "        :param health_mon: The health monitor to decrement the quota on.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning('Reverting decrement quota for health monitor on project'",
            "                    ' %(proj)s Project quota counts may be incorrect.',",
            "                    {'proj': health_mon.project_id})",
            "",
            "        # Increment the quota back if this task wasn't the failure",
            "        if not isinstance(result, failure.Failure):",
            "",
            "            try:",
            "                session = db_apis.get_session()",
            "                lock_session = db_apis.get_session(autocommit=False)",
            "                try:",
            "                    self.repos.check_quota_met(session,",
            "                                               lock_session,",
            "                                               data_models.HealthMonitor,",
            "                                               health_mon.project_id)",
            "                    lock_session.commit()",
            "                except Exception:",
            "                    lock_session.rollback()",
            "            except Exception:",
            "                # Don't fail the revert flow",
            "                pass",
            "",
            "",
            "class DecrementListenerQuota(BaseDatabaseTask):",
            "    \"\"\"Decrements the listener quota for a project.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, listener):",
            "        \"\"\"Decrements the listener quota.",
            "",
            "        :param listener: The listener to decrement the quota on.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Decrementing listener quota for \"",
            "                  \"project: %s \", listener.project_id)",
            "",
            "        lock_session = db_apis.get_session(autocommit=False)",
            "        try:",
            "            self.repos.decrement_quota(lock_session,",
            "                                       data_models.Listener,",
            "                                       listener.project_id)",
            "            lock_session.commit()",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                LOG.error('Failed to decrement listener quota for project: '",
            "                          '%(proj)s the project may have excess quota in use.',",
            "                          {'proj': listener.project_id})",
            "                lock_session.rollback()",
            "",
            "    def revert(self, listener, result, *args, **kwargs):",
            "        \"\"\"Re-apply the quota",
            "",
            "        :param listener: The listener to decrement the quota on.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning('Reverting decrement quota for listener on project '",
            "                    '%(proj)s Project quota counts may be incorrect.',",
            "                    {'proj': listener.project_id})",
            "",
            "        # Increment the quota back if this task wasn't the failure",
            "        if not isinstance(result, failure.Failure):",
            "",
            "            try:",
            "                session = db_apis.get_session()",
            "                lock_session = db_apis.get_session(autocommit=False)",
            "                try:",
            "                    self.repos.check_quota_met(session,",
            "                                               lock_session,",
            "                                               data_models.Listener,",
            "                                               listener.project_id)",
            "                    lock_session.commit()",
            "                except Exception:",
            "                    lock_session.rollback()",
            "            except Exception:",
            "                # Don't fail the revert flow",
            "                pass",
            "",
            "",
            "class DecrementLoadBalancerQuota(BaseDatabaseTask):",
            "    \"\"\"Decrements the load balancer quota for a project.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Decrements the load balancer quota.",
            "",
            "        :param loadbalancer: The load balancer to decrement the quota on.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Decrementing load balancer quota for \"",
            "                  \"project: %s \", loadbalancer.project_id)",
            "",
            "        lock_session = db_apis.get_session(autocommit=False)",
            "        try:",
            "            self.repos.decrement_quota(lock_session,",
            "                                       data_models.LoadBalancer,",
            "                                       loadbalancer.project_id)",
            "            lock_session.commit()",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                LOG.error('Failed to decrement load balancer quota for '",
            "                          'project: %(proj)s the project may have excess '",
            "                          'quota in use.', {'proj': loadbalancer.project_id})",
            "                lock_session.rollback()",
            "",
            "    def revert(self, loadbalancer, result, *args, **kwargs):",
            "        \"\"\"Re-apply the quota",
            "",
            "        :param loadbalancer: The load balancer to decrement the quota on.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning('Reverting decrement quota for load balancer on project '",
            "                    '%(proj)s Project quota counts may be incorrect.',",
            "                    {'proj': loadbalancer.project_id})",
            "",
            "        # Increment the quota back if this task wasn't the failure",
            "        if not isinstance(result, failure.Failure):",
            "",
            "            try:",
            "                session = db_apis.get_session()",
            "                lock_session = db_apis.get_session(autocommit=False)",
            "                try:",
            "                    self.repos.check_quota_met(session,",
            "                                               lock_session,",
            "                                               data_models.LoadBalancer,",
            "                                               loadbalancer.project_id)",
            "                    lock_session.commit()",
            "                except Exception:",
            "                    lock_session.rollback()",
            "            except Exception:",
            "                # Don't fail the revert flow",
            "                pass",
            "",
            "",
            "class DecrementMemberQuota(BaseDatabaseTask):",
            "    \"\"\"Decrements the member quota for a project.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, member):",
            "        \"\"\"Decrements the member quota.",
            "",
            "        :param member: The member to decrement the quota on.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Decrementing member quota for \"",
            "                  \"project: %s \", member.project_id)",
            "",
            "        lock_session = db_apis.get_session(autocommit=False)",
            "        try:",
            "            self.repos.decrement_quota(lock_session,",
            "                                       data_models.Member,",
            "                                       member.project_id)",
            "            lock_session.commit()",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                LOG.error('Failed to decrement member quota for project: '",
            "                          '%(proj)s the project may have excess quota in use.',",
            "                          {'proj': member.project_id})",
            "                lock_session.rollback()",
            "",
            "    def revert(self, member, result, *args, **kwargs):",
            "        \"\"\"Re-apply the quota",
            "",
            "        :param member: The member to decrement the quota on.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning('Reverting decrement quota for member on project %(proj)s '",
            "                    'Project quota counts may be incorrect.',",
            "                    {'proj': member.project_id})",
            "",
            "        # Increment the quota back if this task wasn't the failure",
            "        if not isinstance(result, failure.Failure):",
            "",
            "            try:",
            "                session = db_apis.get_session()",
            "                lock_session = db_apis.get_session(autocommit=False)",
            "                try:",
            "                    self.repos.check_quota_met(session,",
            "                                               lock_session,",
            "                                               data_models.Member,",
            "                                               member.project_id)",
            "                    lock_session.commit()",
            "                except Exception:",
            "                    lock_session.rollback()",
            "            except Exception:",
            "                # Don't fail the revert flow",
            "                pass",
            "",
            "",
            "class DecrementPoolQuota(BaseDatabaseTask):",
            "    \"\"\"Decrements the pool quota for a project.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool, pool_child_count):",
            "        \"\"\"Decrements the pool quota.",
            "",
            "        :param pool: The pool to decrement the quota on",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Decrementing pool quota for \"",
            "                  \"project: %s \", pool.project_id)",
            "",
            "        lock_session = db_apis.get_session(autocommit=False)",
            "        try:",
            "            self.repos.decrement_quota(lock_session,",
            "                                       data_models.Pool,",
            "                                       pool.project_id)",
            "",
            "            # Pools cascade delete members and health monitors",
            "            # update the quota for those items as well.",
            "            if pool_child_count['HM'] > 0:",
            "                self.repos.decrement_quota(lock_session,",
            "                                           data_models.HealthMonitor,",
            "                                           pool.project_id)",
            "            if pool_child_count['member'] > 0:",
            "                self.repos.decrement_quota(",
            "                    lock_session, data_models.Member,",
            "                    pool.project_id, quantity=pool_child_count['member'])",
            "",
            "            lock_session.commit()",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                LOG.error('Failed to decrement pool quota for project: '",
            "                          '%(proj)s the project may have excess quota in use.',",
            "                          {'proj': pool.project_id})",
            "                lock_session.rollback()",
            "",
            "    def revert(self, pool, pool_child_count, result, *args, **kwargs):",
            "        \"\"\"Re-apply the quota",
            "",
            "        :param project_id: The id of project to decrement the quota on",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning('Reverting decrement quota for pool on project %(proj)s '",
            "                    'Project quota counts may be incorrect.',",
            "                    {'proj': pool.project_id})",
            "",
            "        # Increment the quota back if this task wasn't the failure",
            "        if not isinstance(result, failure.Failure):",
            "",
            "            # These are all independent to maximize the correction",
            "            # in case other quota actions have occurred",
            "            try:",
            "                session = db_apis.get_session()",
            "                lock_session = db_apis.get_session(autocommit=False)",
            "                try:",
            "                    self.repos.check_quota_met(session,",
            "                                               lock_session,",
            "                                               data_models.Pool,",
            "                                               pool.project_id)",
            "                    lock_session.commit()",
            "                except Exception:",
            "                    lock_session.rollback()",
            "",
            "                # Attempt to increment back the health monitor quota",
            "                if pool_child_count['HM'] > 0:",
            "                    lock_session = db_apis.get_session(autocommit=False)",
            "                    try:",
            "                        self.repos.check_quota_met(session,",
            "                                                   lock_session,",
            "                                                   data_models.HealthMonitor,",
            "                                                   pool.project_id)",
            "                        lock_session.commit()",
            "                    except Exception:",
            "                        lock_session.rollback()",
            "",
            "                # Attempt to increment back the member quota",
            "                # This is separate calls to maximize the correction",
            "                # should other factors have increased the in use quota",
            "                # before this point in the revert flow",
            "                for i in six.moves.range(pool_child_count['member']):",
            "                    lock_session = db_apis.get_session(autocommit=False)",
            "                    try:",
            "                        self.repos.check_quota_met(session,",
            "                                                   lock_session,",
            "                                                   data_models.Member,",
            "                                                   pool.project_id)",
            "                        lock_session.commit()",
            "                    except Exception:",
            "                        lock_session.rollback()",
            "            except Exception:",
            "                # Don't fail the revert flow",
            "                pass",
            "",
            "",
            "class CountPoolChildrenForQuota(BaseDatabaseTask):",
            "    \"\"\"Counts the pool child resources for quota management.",
            "",
            "    Since the children of pools are cleaned up by the sqlalchemy",
            "    cascade delete settings, we need to collect the quota counts",
            "    for the child objects early.",
            "",
            "    \"\"\"",
            "",
            "    def execute(self, pool):",
            "        \"\"\"Count the pool child resources for quota management",
            "",
            "        :param pool: The pool to count children on",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Counting pool children for \"",
            "                  \"project: %s \", pool.project_id)",
            "",
            "        health_mon_count = 1 if pool.health_monitor else 0",
            "        member_count = len(pool.members)",
            "",
            "        return {'HM': health_mon_count, 'member': member_count}",
            "",
            "",
            "class UpdatePoolMembersOperatingStatusInDB(BaseDatabaseTask):",
            "    \"\"\"Updates the members of a pool operating status.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool, operating_status):",
            "        \"\"\"Update the members of a pool operating status in DB.",
            "",
            "        :param pool: Pool object to be updated",
            "        :param operating_status: Operating status to set",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Updating member operating status to %(status)s in DB for \"",
            "                  \"pool id: %(pool)s\", {'status': operating_status,",
            "                                        'pool': pool.id})",
            "        self.member_repo.update_pool_members(db_apis.get_session(),",
            "                                             pool.id,",
            "                                             operating_status=operating_status)"
        ],
        "afterPatchFile": [
            "# Copyright 2015 Hewlett-Packard Development Company, L.P.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "# http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "",
            "from cryptography import fernet",
            "from oslo_config import cfg",
            "from oslo_db import exception as odb_exceptions",
            "from oslo_log import log as logging",
            "from oslo_utils import excutils",
            "from oslo_utils import uuidutils",
            "import six",
            "import sqlalchemy",
            "from sqlalchemy.orm import exc",
            "from taskflow import task",
            "from taskflow.types import failure",
            "",
            "from octavia.common import constants",
            "from octavia.common import data_models",
            "import octavia.common.tls_utils.cert_parser as cert_parser",
            "from octavia.common import utils",
            "from octavia.common import validate",
            "from octavia.controller.worker import task_utils as task_utilities",
            "from octavia.db import api as db_apis",
            "from octavia.db import repositories as repo",
            "",
            "CONF = cfg.CONF",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "class BaseDatabaseTask(task.Task):",
            "    \"\"\"Base task to load drivers common to the tasks.\"\"\"",
            "",
            "    def __init__(self, **kwargs):",
            "        self.repos = repo.Repositories()",
            "        self.amphora_repo = repo.AmphoraRepository()",
            "        self.health_mon_repo = repo.HealthMonitorRepository()",
            "        self.listener_repo = repo.ListenerRepository()",
            "        self.loadbalancer_repo = repo.LoadBalancerRepository()",
            "        self.vip_repo = repo.VipRepository()",
            "        self.member_repo = repo.MemberRepository()",
            "        self.pool_repo = repo.PoolRepository()",
            "        self.amp_health_repo = repo.AmphoraHealthRepository()",
            "        self.l7policy_repo = repo.L7PolicyRepository()",
            "        self.l7rule_repo = repo.L7RuleRepository()",
            "        self.task_utils = task_utilities.TaskUtils()",
            "        super(BaseDatabaseTask, self).__init__(**kwargs)",
            "",
            "    def _delete_from_amp_health(self, amphora_id):",
            "        \"\"\"Delete the amphora_health record for an amphora.",
            "",
            "        :param amphora_id: The amphora id to delete",
            "        \"\"\"",
            "        LOG.debug('Disabling health monitoring on amphora: %s', amphora_id)",
            "        try:",
            "            self.amp_health_repo.delete(db_apis.get_session(),",
            "                                        amphora_id=amphora_id)",
            "        except (sqlalchemy.orm.exc.NoResultFound,",
            "                sqlalchemy.orm.exc.UnmappedInstanceError):",
            "            LOG.debug('No existing amphora health record to delete '",
            "                      'for amphora: %s, skipping.', amphora_id)",
            "",
            "    def _mark_amp_health_busy(self, amphora_id):",
            "        \"\"\"Mark the amphora_health record busy for an amphora.",
            "",
            "        :param amphora_id: The amphora id to mark busy",
            "        \"\"\"",
            "        LOG.debug('Marking health monitoring busy on amphora: %s', amphora_id)",
            "        try:",
            "            self.amp_health_repo.update(db_apis.get_session(),",
            "                                        amphora_id=amphora_id,",
            "                                        busy=True)",
            "        except (sqlalchemy.orm.exc.NoResultFound,",
            "                sqlalchemy.orm.exc.UnmappedInstanceError):",
            "            LOG.debug('No existing amphora health record to mark busy '",
            "                      'for amphora: %s, skipping.', amphora_id)",
            "",
            "",
            "class CreateAmphoraInDB(BaseDatabaseTask):",
            "    \"\"\"Task to create an initial amphora in the Database.\"\"\"",
            "",
            "    def execute(self, *args, **kwargs):",
            "        \"\"\"Creates an pending create amphora record in the database.",
            "",
            "        :returns: The created amphora object",
            "        \"\"\"",
            "",
            "        amphora = self.amphora_repo.create(db_apis.get_session(),",
            "                                           id=uuidutils.generate_uuid(),",
            "                                           status=constants.PENDING_CREATE,",
            "                                           cert_busy=False)",
            "",
            "        LOG.info(\"Created Amphora in DB with id %s\", amphora.id)",
            "        return amphora.id",
            "",
            "    def revert(self, result, *args, **kwargs):",
            "        \"\"\"Revert by storing the amphora in error state in the DB",
            "",
            "        In a future version we might change the status to DELETED",
            "        if deleting the amphora was successful",
            "",
            "        :param result: Id of created amphora.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        if isinstance(result, failure.Failure):",
            "            # This task's execute failed, so nothing needed to be done to",
            "            # revert",
            "            return",
            "",
            "        # At this point the revert is being called because another task",
            "        # executed after this failed so we will need to do something and",
            "        # result is the amphora's id",
            "",
            "        LOG.warning(\"Reverting create amphora in DB for amp id %s \", result)",
            "",
            "        # Delete the amphora for now. May want to just update status later",
            "        try:",
            "            self.amphora_repo.delete(db_apis.get_session(), id=result)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to delete amphora %(amp)s \"",
            "                      \"in the database due to: \"",
            "                      \"%(except)s\", {'amp': result, 'except': e})",
            "",
            "",
            "class MarkLBAmphoraeDeletedInDB(BaseDatabaseTask):",
            "    \"\"\"Task to mark a list of amphora deleted in the Database.\"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Update load balancer's amphorae statuses to DELETED in the database.",
            "",
            "        :param loadbalancer: The load balancer which amphorae should be",
            "               marked DELETED.",
            "        :returns: None",
            "        \"\"\"",
            "        for amp in loadbalancer.amphorae:",
            "            LOG.debug(\"Marking amphora %s DELETED \", amp.id)",
            "            self.amphora_repo.update(db_apis.get_session(),",
            "                                     id=amp.id, status=constants.DELETED)",
            "",
            "",
            "class DeleteHealthMonitorInDB(BaseDatabaseTask):",
            "    \"\"\"Delete the health monitor in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, health_mon):",
            "        \"\"\"Delete the health monitor in DB",
            "",
            "        :param health_mon: The health monitor which should be deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"DB delete health monitor: %s \", health_mon.id)",
            "        try:",
            "            self.health_mon_repo.delete(db_apis.get_session(),",
            "                                        id=health_mon.id)",
            "        except exc.NoResultFound:",
            "            # ignore if the HealthMonitor was not found",
            "            pass",
            "",
            "    def revert(self, health_mon, *args, **kwargs):",
            "        \"\"\"Mark the health monitor ERROR since the mark active couldn't happen",
            "",
            "        :param health_mon: The health monitor which couldn't be deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark health monitor delete in DB \"",
            "                    \"for health monitor with id %s\", health_mon.id)",
            "        self.health_mon_repo.update(db_apis.get_session(), id=health_mon.id,",
            "                                    provisioning_status=constants.ERROR)",
            "",
            "",
            "class DeleteHealthMonitorInDBByPool(DeleteHealthMonitorInDB):",
            "    \"\"\"Delete the health monitor in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool):",
            "        \"\"\"Delete the health monitor in the DB.",
            "",
            "        :param pool: A pool which health monitor should be deleted.",
            "        :returns: None",
            "        \"\"\"",
            "        super(DeleteHealthMonitorInDBByPool, self).execute(",
            "            pool.health_monitor)",
            "",
            "    def revert(self, pool, *args, **kwargs):",
            "        \"\"\"Mark the health monitor ERROR since the mark active couldn't happen",
            "",
            "        :param pool: A pool which health monitor couldn't be deleted",
            "        :returns: None",
            "        \"\"\"",
            "        super(DeleteHealthMonitorInDBByPool, self).revert(",
            "            pool.health_monitor, *args, **kwargs)",
            "",
            "",
            "class DeleteMemberInDB(BaseDatabaseTask):",
            "    \"\"\"Delete the member in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, member):",
            "        \"\"\"Delete the member in the DB",
            "",
            "        :param member: The member to be deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"DB delete member for id: %s \", member.id)",
            "        self.member_repo.delete(db_apis.get_session(), id=member.id)",
            "",
            "    def revert(self, member, *args, **kwargs):",
            "        \"\"\"Mark the member ERROR since the delete couldn't happen",
            "",
            "        :param member: Member that failed to get deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting delete in DB for member id %s\", member.id)",
            "        try:",
            "            self.member_repo.update(db_apis.get_session(), member.id,",
            "                                    provisioning_status=constants.ERROR)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update member %(mem)s \"",
            "                      \"provisioning_status to ERROR due to: %(except)s\",",
            "                      {'mem': member.id, 'except': e})",
            "",
            "",
            "class DeleteListenerInDB(BaseDatabaseTask):",
            "    \"\"\"Delete the listener in the DB.\"\"\"",
            "",
            "    def execute(self, listener):",
            "        \"\"\"Delete the listener in DB",
            "",
            "        :param listener: The listener to delete",
            "        :returns: None",
            "        \"\"\"",
            "        LOG.debug(\"Delete in DB for listener id: %s\", listener.id)",
            "        self.listener_repo.delete(db_apis.get_session(), id=listener.id)",
            "",
            "    def revert(self, listener, *args, **kwargs):",
            "        \"\"\"Mark the listener ERROR since the listener didn't delete",
            "",
            "        :param listener: Listener that failed to get deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark listener delete in DB for listener id %s\",",
            "                    listener.id)",
            "",
            "",
            "class DeletePoolInDB(BaseDatabaseTask):",
            "    \"\"\"Delete the pool in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool):",
            "        \"\"\"Delete the pool in DB",
            "",
            "        :param pool: The pool to be deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Delete in DB for pool id: %s \", pool.id)",
            "        self.pool_repo.delete(db_apis.get_session(), id=pool.id)",
            "",
            "    def revert(self, pool, *args, **kwargs):",
            "        \"\"\"Mark the pool ERROR since the delete couldn't happen",
            "",
            "        :param pool: Pool that failed to get deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting delete in DB for pool id %s\", pool.id)",
            "        try:",
            "            self.pool_repo.update(db_apis.get_session(), pool.id,",
            "                                  provisioning_status=constants.ERROR)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update pool %(pool)s \"",
            "                      \"provisioning_status to ERROR due to: %(except)s\",",
            "                      {'pool': pool.id, 'except': e})",
            "",
            "",
            "class DeleteL7PolicyInDB(BaseDatabaseTask):",
            "    \"\"\"Delete the L7 policy in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7policy):",
            "        \"\"\"Delete the l7policy in DB",
            "",
            "        :param l7policy: The l7policy to be deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Delete in DB for l7policy id: %s \", l7policy.id)",
            "        self.l7policy_repo.delete(db_apis.get_session(), id=l7policy.id)",
            "",
            "    def revert(self, l7policy, *args, **kwargs):",
            "        \"\"\"Mark the l7policy ERROR since the delete couldn't happen",
            "",
            "        :param l7policy: L7 policy that failed to get deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting delete in DB for l7policy id %s\", l7policy.id)",
            "        try:",
            "            self.l7policy_repo.update(db_apis.get_session(), l7policy.id,",
            "                                      provisioning_status=constants.ERROR)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update l7policy %(l7policy)s \"",
            "                      \"provisioning_status to ERROR due to: %(except)s\",",
            "                      {'l7policy': l7policy.id, 'except': e})",
            "",
            "",
            "class DeleteL7RuleInDB(BaseDatabaseTask):",
            "    \"\"\"Delete the L7 rule in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7rule):",
            "        \"\"\"Delete the l7rule in DB",
            "",
            "        :param l7rule: The l7rule to be deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Delete in DB for l7rule id: %s \", l7rule.id)",
            "        self.l7rule_repo.delete(db_apis.get_session(), id=l7rule.id)",
            "",
            "    def revert(self, l7rule, *args, **kwargs):",
            "        \"\"\"Mark the l7rule ERROR since the delete couldn't happen",
            "",
            "        :param l7rule: L7 rule that failed to get deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting delete in DB for l7rule id %s\", l7rule.id)",
            "        try:",
            "            self.l7rule_repo.update(db_apis.get_session(), l7rule.id,",
            "                                    provisioning_status=constants.ERROR)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update l7rule %(l7rule)s \"",
            "                      \"provisioning_status to ERROR due to: %(except)s\",",
            "                      {'l7rule': l7rule.id, 'except': e})",
            "",
            "",
            "class ReloadAmphora(BaseDatabaseTask):",
            "    \"\"\"Get an amphora object from the database.\"\"\"",
            "",
            "    def execute(self, amphora_id):",
            "        \"\"\"Get an amphora object from the database.",
            "",
            "        :param amphora_id: The amphora ID to lookup",
            "        :returns: The amphora object",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Get amphora from DB for amphora id: %s \", amphora_id)",
            "        return self.amphora_repo.get(db_apis.get_session(), id=amphora_id)",
            "",
            "",
            "class ReloadLoadBalancer(BaseDatabaseTask):",
            "    \"\"\"Get an load balancer object from the database.\"\"\"",
            "",
            "    def execute(self, loadbalancer_id, *args, **kwargs):",
            "        \"\"\"Get an load balancer object from the database.",
            "",
            "        :param loadbalancer_id: The load balancer ID to lookup",
            "        :returns: The load balancer object",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Get load balancer from DB for load balancer id: %s \",",
            "                  loadbalancer_id)",
            "        return self.loadbalancer_repo.get(db_apis.get_session(),",
            "                                          id=loadbalancer_id)",
            "",
            "",
            "class UpdateVIPAfterAllocation(BaseDatabaseTask):",
            "    \"\"\"Update a VIP associated with a given load balancer.\"\"\"",
            "",
            "    def execute(self, loadbalancer_id, vip):",
            "        \"\"\"Update a VIP associated with a given load balancer.",
            "",
            "        :param loadbalancer_id: Id of a load balancer which VIP should be",
            "               updated.",
            "        :param vip: data_models.Vip object with update data.",
            "        :returns: The load balancer object.",
            "        \"\"\"",
            "        self.repos.vip.update(db_apis.get_session(), loadbalancer_id,",
            "                              port_id=vip.port_id, subnet_id=vip.subnet_id,",
            "                              ip_address=vip.ip_address)",
            "        return self.repos.load_balancer.get(db_apis.get_session(),",
            "                                            id=loadbalancer_id)",
            "",
            "",
            "class UpdateAmphoraeVIPData(BaseDatabaseTask):",
            "    \"\"\"Update amphorae VIP data.\"\"\"",
            "",
            "    def execute(self, amps_data):",
            "        \"\"\"Update amphorae VIP data.",
            "",
            "        :param amps_data: Amphorae update dicts.",
            "        :returns: None",
            "        \"\"\"",
            "        for amp_data in amps_data:",
            "            self.repos.amphora.update(db_apis.get_session(), amp_data.id,",
            "                                      vrrp_ip=amp_data.vrrp_ip,",
            "                                      ha_ip=amp_data.ha_ip,",
            "                                      vrrp_port_id=amp_data.vrrp_port_id,",
            "                                      ha_port_id=amp_data.ha_port_id,",
            "                                      vrrp_id=1)",
            "",
            "",
            "class UpdateAmphoraVIPData(BaseDatabaseTask):",
            "    \"\"\"Update amphorae VIP data.\"\"\"",
            "",
            "    def execute(self, amp_data):",
            "        \"\"\"Update amphorae VIP data.",
            "",
            "        :param amps_data: Amphorae update dicts.",
            "        :returns: None",
            "        \"\"\"",
            "        self.repos.amphora.update(db_apis.get_session(), amp_data.id,",
            "                                  vrrp_ip=amp_data.vrrp_ip,",
            "                                  ha_ip=amp_data.ha_ip,",
            "                                  vrrp_port_id=amp_data.vrrp_port_id,",
            "                                  ha_port_id=amp_data.ha_port_id,",
            "                                  vrrp_id=1)",
            "",
            "",
            "class UpdateAmpFailoverDetails(BaseDatabaseTask):",
            "    \"\"\"Update amphora failover details in the database.\"\"\"",
            "",
            "    def execute(self, amphora, amp_data):",
            "        \"\"\"Update amphora failover details in the database.",
            "",
            "        :param amphora: The amphora to update",
            "        :param amp_data: data_models.Amphora object with update data",
            "        :returns: None",
            "        \"\"\"",
            "        # role and vrrp_priority will be updated later.",
            "        self.repos.amphora.update(db_apis.get_session(), amphora.id,",
            "                                  vrrp_ip=amp_data.vrrp_ip,",
            "                                  ha_ip=amp_data.ha_ip,",
            "                                  vrrp_port_id=amp_data.vrrp_port_id,",
            "                                  ha_port_id=amp_data.ha_port_id,",
            "                                  vrrp_id=amp_data.vrrp_id)",
            "",
            "",
            "class AssociateFailoverAmphoraWithLBID(BaseDatabaseTask):",
            "    \"\"\"Associate failover amphora with loadbalancer in the database.\"\"\"",
            "",
            "    def execute(self, amphora_id, loadbalancer_id):",
            "        \"\"\"Associate failover amphora with loadbalancer in the database.",
            "",
            "        :param amphora_id: Id of an amphora to update",
            "        :param loadbalancer_id: Id of a load balancer to be associated with",
            "               a given amphora.",
            "        :returns: None",
            "        \"\"\"",
            "        self.repos.amphora.associate(db_apis.get_session(),",
            "                                     load_balancer_id=loadbalancer_id,",
            "                                     amphora_id=amphora_id)",
            "",
            "    def revert(self, amphora_id, *args, **kwargs):",
            "        \"\"\"Remove amphora-load balancer association.",
            "",
            "        :param amphora_id: Id of an amphora that couldn't be associated",
            "               with a load balancer.",
            "        :returns: None",
            "        \"\"\"",
            "        try:",
            "            self.repos.amphora.update(db_apis.get_session(), amphora_id,",
            "                                      loadbalancer_id=None)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update amphora %(amp)s \"",
            "                      \"load balancer id to None due to: \"",
            "                      \"%(except)s\", {'amp': amphora_id, 'except': e})",
            "",
            "",
            "class MapLoadbalancerToAmphora(BaseDatabaseTask):",
            "    \"\"\"Maps and assigns a load balancer to an amphora in the database.\"\"\"",
            "",
            "    def execute(self, loadbalancer_id, server_group_id=None, flavor=None):",
            "        \"\"\"Allocates an Amphora for the load balancer in the database.",
            "",
            "        :param loadbalancer_id: The load balancer id to map to an amphora",
            "        :returns: Amphora ID if one was allocated, None if it was",
            "                  unable to allocate an Amphora",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Allocating an Amphora for load balancer with id %s\",",
            "                  loadbalancer_id)",
            "",
            "        if server_group_id is not None:",
            "            LOG.debug(\"Load balancer is using anti-affinity. Skipping spares \"",
            "                      \"pool allocation.\")",
            "            return None",
            "",
            "        # Validate the flavor is spares compatible",
            "        if not validate.is_flavor_spares_compatible(flavor):",
            "            LOG.debug(\"Load balancer has a flavor that is not compatible with \"",
            "                      \"using spares pool amphora. Skipping spares pool \"",
            "                      \"allocation.\")",
            "            return None",
            "",
            "        amp = self.amphora_repo.allocate_and_associate(",
            "            db_apis.get_session(),",
            "            loadbalancer_id)",
            "        if amp is None:",
            "            LOG.debug(\"No Amphora available for load balancer with id %s\",",
            "                      loadbalancer_id)",
            "            return None",
            "",
            "        LOG.debug(\"Allocated Amphora with id %(amp)s for load balancer \"",
            "                  \"with id %(lb)s\", {'amp': amp.id, 'lb': loadbalancer_id})",
            "",
            "        return amp.id",
            "",
            "    def revert(self, result, loadbalancer_id, *args, **kwargs):",
            "        LOG.warning(\"Reverting Amphora allocation for the load \"",
            "                    \"balancer %s in the database.\", loadbalancer_id)",
            "        self.task_utils.mark_loadbalancer_prov_status_error(loadbalancer_id)",
            "",
            "",
            "class _MarkAmphoraRoleAndPriorityInDB(BaseDatabaseTask):",
            "    \"\"\"Alter the amphora role and priority in DB.\"\"\"",
            "",
            "    def _execute(self, amphora, amp_role, vrrp_priority):",
            "        \"\"\"Alter the amphora role and priority in DB.",
            "",
            "        :param amphora: Amphora to update.",
            "        :param amp_role: Amphora role to be set.",
            "        :param vrrp_priority: VRRP priority to set.",
            "        :returns: None",
            "        \"\"\"",
            "        LOG.debug(\"Mark %(role)s in DB for amphora: %(amp)s\",",
            "                  {'role': amp_role, 'amp': amphora.id})",
            "        self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                 role=amp_role,",
            "                                 vrrp_priority=vrrp_priority)",
            "",
            "    def _revert(self, result, amphora, *args, **kwargs):",
            "        \"\"\"Removes role and vrrp_priority association.",
            "",
            "        :param result: Result of the association.",
            "        :param amphora: Amphora which role/vrrp_priority association",
            "               failed.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "",
            "        LOG.warning(\"Reverting amphora role in DB for amp id %(amp)s\",",
            "                    {'amp': amphora.id})",
            "        try:",
            "            self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                     role=None,",
            "                                     vrrp_priority=None)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update amphora %(amp)s \"",
            "                      \"role and vrrp_priority to None due to: \"",
            "                      \"%(except)s\", {'amp': amphora.id, 'except': e})",
            "",
            "",
            "class MarkAmphoraMasterInDB(_MarkAmphoraRoleAndPriorityInDB):",
            "    \"\"\"Alter the amphora role to: MASTER.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Mark amphora as MASTER in db.",
            "",
            "        :param amphora: Amphora to update role.",
            "        :returns: None",
            "        \"\"\"",
            "        amp_role = constants.ROLE_MASTER",
            "        self._execute(amphora, amp_role, constants.ROLE_MASTER_PRIORITY)",
            "",
            "    def revert(self, result, amphora, *args, **kwargs):",
            "        \"\"\"Removes amphora role association.",
            "",
            "        :param amphora: Amphora to update role.",
            "        :returns: None",
            "        \"\"\"",
            "        self._revert(result, amphora, *args, **kwargs)",
            "",
            "",
            "class MarkAmphoraBackupInDB(_MarkAmphoraRoleAndPriorityInDB):",
            "    \"\"\"Alter the amphora role to: Backup.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Mark amphora as BACKUP in db.",
            "",
            "        :param amphora: Amphora to update role.",
            "        :returns: None",
            "        \"\"\"",
            "        amp_role = constants.ROLE_BACKUP",
            "        self._execute(amphora, amp_role, constants.ROLE_BACKUP_PRIORITY)",
            "",
            "    def revert(self, result, amphora, *args, **kwargs):",
            "        \"\"\"Removes amphora role association.",
            "",
            "        :param amphora: Amphora to update role.",
            "        :returns: None",
            "        \"\"\"",
            "        self._revert(result, amphora, *args, **kwargs)",
            "",
            "",
            "class MarkAmphoraStandAloneInDB(_MarkAmphoraRoleAndPriorityInDB):",
            "    \"\"\"Alter the amphora role to: Standalone.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Mark amphora as STANDALONE in db.",
            "",
            "        :param amphora: Amphora to update role.",
            "        :returns: None",
            "        \"\"\"",
            "        amp_role = constants.ROLE_STANDALONE",
            "        self._execute(amphora, amp_role, None)",
            "",
            "    def revert(self, result, amphora, *args, **kwargs):",
            "        \"\"\"Removes amphora role association.",
            "",
            "        :param amphora: Amphora to update role.",
            "        :returns: None",
            "        \"\"\"",
            "        self._revert(result, amphora, *args, **kwargs)",
            "",
            "",
            "class MarkAmphoraAllocatedInDB(BaseDatabaseTask):",
            "    \"\"\"Will mark an amphora as allocated to a load balancer in the database.",
            "",
            "    Assume sqlalchemy made sure the DB got",
            "    retried sufficiently - so just abort",
            "    \"\"\"",
            "",
            "    def execute(self, amphora, loadbalancer_id):",
            "        \"\"\"Mark amphora as allocated to a load balancer in DB.",
            "",
            "        :param amphora: Amphora to be updated.",
            "        :param loadbalancer_id: Id of a load balancer to which an amphora",
            "               should be allocated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.info('Mark ALLOCATED in DB for amphora: %(amp)s with '",
            "                 'compute id %(comp)s for load balancer: %(lb)s',",
            "                 {",
            "                     'amp': amphora.id,",
            "                     'comp': amphora.compute_id,",
            "                     'lb': loadbalancer_id",
            "                 })",
            "        self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                 status=constants.AMPHORA_ALLOCATED,",
            "                                 compute_id=amphora.compute_id,",
            "                                 lb_network_ip=amphora.lb_network_ip,",
            "                                 load_balancer_id=loadbalancer_id)",
            "",
            "    def revert(self, result, amphora, loadbalancer_id, *args, **kwargs):",
            "        \"\"\"Mark the amphora as broken and ready to be cleaned up.",
            "",
            "        :param result: Execute task result",
            "        :param amphora: Amphora that was updated.",
            "        :param loadbalancer_id: Id of a load balancer to which an amphora",
            "               failed to be allocated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "",
            "        LOG.warning(\"Reverting mark amphora ready in DB for amp \"",
            "                    \"id %(amp)s and compute id %(comp)s\",",
            "                    {'amp': amphora.id, 'comp': amphora.compute_id})",
            "        self.task_utils.mark_amphora_status_error(amphora.id)",
            "",
            "",
            "class MarkAmphoraBootingInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the amphora as booting in the database.\"\"\"",
            "",
            "    def execute(self, amphora_id, compute_id):",
            "        \"\"\"Mark amphora booting in DB.",
            "",
            "        :param amphora_id: Id of the amphora to update",
            "        :param compute_id: Id of a compute on which an amphora resides",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark BOOTING in DB for amphora: %(amp)s with \"",
            "                  \"compute id %(id)s\", {'amp': amphora_id, 'id': compute_id})",
            "        self.amphora_repo.update(db_apis.get_session(), amphora_id,",
            "                                 status=constants.AMPHORA_BOOTING,",
            "                                 compute_id=compute_id)",
            "",
            "    def revert(self, result, amphora_id, compute_id, *args, **kwargs):",
            "        \"\"\"Mark the amphora as broken and ready to be cleaned up.",
            "",
            "        :param result: Execute task result",
            "        :param amphora_id: Id of the amphora that failed to update",
            "        :param compute_id: Id of a compute on which an amphora resides",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        if isinstance(result, failure.Failure):",
            "            return",
            "",
            "        LOG.warning(\"Reverting mark amphora booting in DB for amp \"",
            "                    \"id %(amp)s and compute id %(comp)s\",",
            "                    {'amp': amphora_id, 'comp': compute_id})",
            "        try:",
            "            self.amphora_repo.update(db_apis.get_session(), amphora_id,",
            "                                     status=constants.ERROR,",
            "                                     compute_id=compute_id)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update amphora %(amp)s \"",
            "                      \"status to ERROR due to: \"",
            "                      \"%(except)s\", {'amp': amphora_id, 'except': e})",
            "",
            "",
            "class MarkAmphoraDeletedInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the amphora deleted in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Mark the amphora as deleted in DB.",
            "",
            "        :param amphora: Amphora to be updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark DELETED in DB for amphora: %(amp)s with \"",
            "                  \"compute id %(comp)s\",",
            "                  {'amp': amphora.id, 'comp': amphora.compute_id})",
            "        self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                 status=constants.DELETED)",
            "",
            "    def revert(self, amphora, *args, **kwargs):",
            "        \"\"\"Mark the amphora as broken and ready to be cleaned up.",
            "",
            "        :param amphora: Amphora that was updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark amphora deleted in DB \"",
            "                    \"for amp id %(amp)s and compute id %(comp)s\",",
            "                    {'amp': amphora.id, 'comp': amphora.compute_id})",
            "        self.task_utils.mark_amphora_status_error(amphora.id)",
            "",
            "",
            "class MarkAmphoraPendingDeleteInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the amphora pending delete in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Mark the amphora as pending delete in DB.",
            "",
            "        :param amphora: Amphora to be updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING DELETE in DB for amphora: %(amp)s \"",
            "                  \"with compute id %(id)s\",",
            "                  {'amp': amphora.id, 'id': amphora.compute_id})",
            "        self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                 status=constants.PENDING_DELETE)",
            "",
            "    def revert(self, amphora, *args, **kwargs):",
            "        \"\"\"Mark the amphora as broken and ready to be cleaned up.",
            "",
            "        :param amphora: Amphora that was updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark amphora pending delete in DB \"",
            "                    \"for amp id %(amp)s and compute id %(comp)s\",",
            "                    {'amp': amphora.id, 'comp': amphora.compute_id})",
            "        self.task_utils.mark_amphora_status_error(amphora.id)",
            "",
            "",
            "class MarkAmphoraPendingUpdateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the amphora pending update in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Mark the amphora as pending update in DB.",
            "",
            "        :param amphora: Amphora to be updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING UPDATE in DB for amphora: %(amp)s \"",
            "                  \"with compute id %(id)s\",",
            "                  {'amp': amphora.id, 'id': amphora.compute_id})",
            "        self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                 status=constants.PENDING_UPDATE)",
            "",
            "    def revert(self, amphora, *args, **kwargs):",
            "        \"\"\"Mark the amphora as broken and ready to be cleaned up.",
            "",
            "        :param amphora: Amphora that was updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark amphora pending update in DB \"",
            "                    \"for amp id %(amp)s and compute id %(comp)s\",",
            "                    {'amp': amphora.id, 'comp': amphora.compute_id})",
            "        self.task_utils.mark_amphora_status_error(amphora.id)",
            "",
            "",
            "class MarkAmphoraReadyInDB(BaseDatabaseTask):",
            "    \"\"\"This task will mark an amphora as ready in the database.",
            "",
            "    Assume sqlalchemy made sure the DB got",
            "    retried sufficiently - so just abort",
            "    \"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Mark amphora as ready in DB.",
            "",
            "        :param amphora: Amphora to be updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.info(\"Mark READY in DB for amphora: %(amp)s with compute \"",
            "                 \"id %(comp)s\",",
            "                 {\"amp\": amphora.id, \"comp\": amphora.compute_id})",
            "        self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                 status=constants.AMPHORA_READY,",
            "                                 compute_id=amphora.compute_id,",
            "                                 lb_network_ip=amphora.lb_network_ip)",
            "",
            "    def revert(self, amphora, *args, **kwargs):",
            "        \"\"\"Mark the amphora as broken and ready to be cleaned up.",
            "",
            "        :param amphora: Amphora that was updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark amphora ready in DB for amp \"",
            "                    \"id %(amp)s and compute id %(comp)s\",",
            "                    {'amp': amphora.id, 'comp': amphora.compute_id})",
            "        try:",
            "            self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                     status=constants.ERROR,",
            "                                     compute_id=amphora.compute_id,",
            "                                     lb_network_ip=amphora.lb_network_ip)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update amphora %(amp)s \"",
            "                      \"status to ERROR due to: \"",
            "                      \"%(except)s\", {'amp': amphora.id, 'except': e})",
            "",
            "",
            "class UpdateAmphoraComputeId(BaseDatabaseTask):",
            "    \"\"\"Associate amphora with a compute in DB.\"\"\"",
            "",
            "    def execute(self, amphora_id, compute_id):",
            "        \"\"\"Associate amphora with a compute in DB.",
            "",
            "        :param amphora_id: Id of the amphora to update",
            "        :param compute_id: Id of a compute on which an amphora resides",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        self.amphora_repo.update(db_apis.get_session(), amphora_id,",
            "                                 compute_id=compute_id)",
            "",
            "",
            "class UpdateAmphoraInfo(BaseDatabaseTask):",
            "    \"\"\"Update amphora with compute instance details.\"\"\"",
            "",
            "    def execute(self, amphora_id, compute_obj):",
            "        \"\"\"Update amphora with compute instance details.",
            "",
            "        :param amphora_id: Id of the amphora to update",
            "        :param compute_obj: Compute on which an amphora resides",
            "        :returns: Updated amphora object",
            "        \"\"\"",
            "        self.amphora_repo.update(",
            "            db_apis.get_session(), amphora_id,",
            "            lb_network_ip=compute_obj.lb_network_ip,",
            "            cached_zone=compute_obj.cached_zone,",
            "            image_id=compute_obj.image_id,",
            "            compute_flavor=compute_obj.compute_flavor)",
            "        return self.amphora_repo.get(db_apis.get_session(), id=amphora_id)",
            "",
            "",
            "class UpdateAmphoraDBCertExpiration(BaseDatabaseTask):",
            "    \"\"\"Update the amphora expiration date with new cert file date.\"\"\"",
            "",
            "    def execute(self, amphora_id, server_pem):",
            "        \"\"\"Update the amphora expiration date with new cert file date.",
            "",
            "        :param amphora_id: Id of the amphora to update",
            "        :param server_pem: Certificate in PEM format",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update DB cert expiry date of amphora id: %s\", amphora_id)",
            "",
            "        key = utils.get_six_compatible_server_certs_key_passphrase()",
            "        fer = fernet.Fernet(key)",
            "        cert_expiration = cert_parser.get_cert_expiration(",
            "            fer.decrypt(server_pem))",
            "        LOG.debug(\"Certificate expiration date is %s \", cert_expiration)",
            "        self.amphora_repo.update(db_apis.get_session(), amphora_id,",
            "                                 cert_expiration=cert_expiration)",
            "",
            "",
            "class UpdateAmphoraCertBusyToFalse(BaseDatabaseTask):",
            "    \"\"\"Update the amphora cert_busy flag to be false.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Update the amphora cert_busy flag to be false.",
            "",
            "        :param amphora: Amphora to be updated.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update cert_busy flag of amphora id %s to False\",",
            "                  amphora.id)",
            "        self.amphora_repo.update(db_apis.get_session(), amphora.id,",
            "                                 cert_busy=False)",
            "",
            "",
            "class MarkLBActiveInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the load balancer active in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def __init__(self, mark_subobjects=False, **kwargs):",
            "        super(MarkLBActiveInDB, self).__init__(**kwargs)",
            "        self.mark_subobjects = mark_subobjects",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Mark the load balancer as active in DB.",
            "",
            "        This also marks ACTIVE all sub-objects of the load balancer if",
            "        self.mark_subobjects is True.",
            "",
            "        :param loadbalancer: Load balancer object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        if self.mark_subobjects:",
            "            LOG.debug(\"Marking all listeners of loadbalancer %s ACTIVE\",",
            "                      loadbalancer.id)",
            "            for listener in loadbalancer.listeners:",
            "                self._mark_listener_status(listener, constants.ACTIVE)",
            "",
            "        LOG.info(\"Mark ACTIVE in DB for load balancer id: %s\",",
            "                 loadbalancer.id)",
            "        self.loadbalancer_repo.update(db_apis.get_session(),",
            "                                      loadbalancer.id,",
            "                                      provisioning_status=constants.ACTIVE)",
            "",
            "    def _mark_listener_status(self, listener, status):",
            "        self.listener_repo.update(db_apis.get_session(),",
            "                                  listener.id,",
            "                                  provisioning_status=status)",
            "        LOG.debug(\"Marking all l7policies of listener %s %s\",",
            "                  listener.id, status)",
            "        for l7policy in listener.l7policies:",
            "            self._mark_l7policy_status(l7policy, status)",
            "",
            "        if listener.default_pool:",
            "            LOG.debug(\"Marking default pool of listener %s %s\",",
            "                      listener.id, status)",
            "            self._mark_pool_status(listener.default_pool, status)",
            "",
            "    def _mark_l7policy_status(self, l7policy, status):",
            "        self.l7policy_repo.update(",
            "            db_apis.get_session(), l7policy.id,",
            "            provisioning_status=status)",
            "",
            "        LOG.debug(\"Marking all l7rules of l7policy %s %s\",",
            "                  l7policy.id, status)",
            "        for l7rule in l7policy.l7rules:",
            "            self._mark_l7rule_status(l7rule, status)",
            "",
            "        if l7policy.redirect_pool:",
            "            LOG.debug(\"Marking redirect pool of l7policy %s %s\",",
            "                      l7policy.id, status)",
            "            self._mark_pool_status(l7policy.redirect_pool, status)",
            "",
            "    def _mark_l7rule_status(self, l7rule, status):",
            "        self.l7rule_repo.update(",
            "            db_apis.get_session(), l7rule.id,",
            "            provisioning_status=status)",
            "",
            "    def _mark_pool_status(self, pool, status):",
            "        self.pool_repo.update(",
            "            db_apis.get_session(), pool.id,",
            "            provisioning_status=status)",
            "        if pool.health_monitor:",
            "            LOG.debug(\"Marking health monitor of pool %s %s\", pool.id, status)",
            "            self._mark_hm_status(pool.health_monitor, status)",
            "",
            "        LOG.debug(\"Marking all members of pool %s %s\", pool.id, status)",
            "        for member in pool.members:",
            "            self._mark_member_status(member, status)",
            "",
            "    def _mark_hm_status(self, hm, status):",
            "        self.health_mon_repo.update(",
            "            db_apis.get_session(), hm.id,",
            "            provisioning_status=status)",
            "",
            "    def _mark_member_status(self, member, status):",
            "        self.member_repo.update(",
            "            db_apis.get_session(), member.id,",
            "            provisioning_status=status)",
            "",
            "    def revert(self, loadbalancer, *args, **kwargs):",
            "        \"\"\"Mark the load balancer as broken and ready to be cleaned up.",
            "",
            "        This also puts all sub-objects of the load balancer to ERROR state if",
            "        self.mark_subobjects is True",
            "",
            "        :param loadbalancer: Load balancer object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        if self.mark_subobjects:",
            "            LOG.debug(\"Marking all listeners of loadbalancer %s ERROR\",",
            "                      loadbalancer.id)",
            "            for listener in loadbalancer.listeners:",
            "                try:",
            "                    self._mark_listener_status(listener, constants.ERROR)",
            "                except Exception:",
            "                    LOG.warning(\"Error updating listener %s provisioning \"",
            "                                \"status\", listener.id)",
            "",
            "        LOG.warning(\"Reverting mark load balancer deleted in DB \"",
            "                    \"for load balancer id %s\", loadbalancer.id)",
            "        self.task_utils.mark_loadbalancer_prov_status_error(loadbalancer.id)",
            "",
            "",
            "class UpdateLBServerGroupInDB(BaseDatabaseTask):",
            "    \"\"\"Update the server group id info for load balancer in DB.\"\"\"",
            "",
            "    def execute(self, loadbalancer_id, server_group_id):",
            "        \"\"\"Update the server group id info for load balancer in DB.",
            "",
            "        :param loadbalancer_id: Id of a load balancer to update",
            "        :param server_group_id: Id of a server group to associate with",
            "               the load balancer",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Server Group updated with id: %s for load balancer id: %s:\",",
            "                  server_group_id, loadbalancer_id)",
            "        self.loadbalancer_repo.update(db_apis.get_session(),",
            "                                      id=loadbalancer_id,",
            "                                      server_group_id=server_group_id)",
            "",
            "    def revert(self, loadbalancer_id, server_group_id, *args, **kwargs):",
            "        \"\"\"Remove server group information from a load balancer in DB.",
            "",
            "        :param loadbalancer_id: Id of a load balancer that failed to update",
            "        :param server_group_id: Id of a server group that couldn't be",
            "               associated with the load balancer",
            "        :returns: None",
            "        \"\"\"",
            "        LOG.warning('Reverting Server Group updated with id: %(s1)s for '",
            "                    'load balancer id: %(s2)s ',",
            "                    {'s1': server_group_id, 's2': loadbalancer_id})",
            "        try:",
            "            self.loadbalancer_repo.update(db_apis.get_session(),",
            "                                          id=loadbalancer_id,",
            "                                          server_group_id=None)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update load balancer %(lb)s \"",
            "                      \"server_group_id to None due to: \"",
            "                      \"%(except)s\", {'lb': loadbalancer_id, 'except': e})",
            "",
            "",
            "class MarkLBDeletedInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the load balancer deleted in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Mark the load balancer as deleted in DB.",
            "",
            "        :param loadbalancer: Load balancer object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark DELETED in DB for load balancer id: %s\",",
            "                  loadbalancer.id)",
            "        self.loadbalancer_repo.update(db_apis.get_session(),",
            "                                      loadbalancer.id,",
            "                                      provisioning_status=constants.DELETED)",
            "",
            "    def revert(self, loadbalancer, *args, **kwargs):",
            "        \"\"\"Mark the load balancer as broken and ready to be cleaned up.",
            "",
            "        :param loadbalancer: Load balancer object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark load balancer deleted in DB \"",
            "                    \"for load balancer id %s\", loadbalancer.id)",
            "        self.task_utils.mark_loadbalancer_prov_status_error(loadbalancer.id)",
            "",
            "",
            "class MarkLBPendingDeleteInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the load balancer pending delete in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Mark the load balancer as pending delete in DB.",
            "",
            "        :param loadbalancer: Load balancer object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING DELETE in DB for load balancer id: %s\",",
            "                  loadbalancer.id)",
            "        self.loadbalancer_repo.update(db_apis.get_session(),",
            "                                      loadbalancer.id,",
            "                                      provisioning_status=(constants.",
            "                                                           PENDING_DELETE))",
            "",
            "    def revert(self, loadbalancer, *args, **kwargs):",
            "        \"\"\"Mark the load balancer as broken and ready to be cleaned up.",
            "",
            "        :param loadbalancer: Load balancer object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark load balancer pending delete in DB \"",
            "                    \"for load balancer id %s\", loadbalancer.id)",
            "        self.task_utils.mark_loadbalancer_prov_status_error(loadbalancer.id)",
            "",
            "",
            "class MarkLBAndListenersActiveInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the load balancer and specified listeners active in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, loadbalancer, listeners):",
            "        \"\"\"Mark the load balancer and listeners as active in DB.",
            "",
            "        :param loadbalancer: Load balancer object to be updated",
            "        :param listeners: Listener objects to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark ACTIVE in DB for load balancer id: %s \"",
            "                  \"and listener ids: %s\", loadbalancer.id,",
            "                  ', '.join([l.id for l in listeners]))",
            "        self.loadbalancer_repo.update(db_apis.get_session(),",
            "                                      loadbalancer.id,",
            "                                      provisioning_status=constants.ACTIVE)",
            "        for listener in listeners:",
            "            self.listener_repo.update(db_apis.get_session(), listener.id,",
            "                                      provisioning_status=constants.ACTIVE)",
            "",
            "    def revert(self, loadbalancer, listeners, *args, **kwargs):",
            "        \"\"\"Mark the load balancer and listeners as broken.",
            "",
            "        :param loadbalancer: Load balancer object that failed to update",
            "        :param listeners: Listener objects that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark load balancer and listeners active in DB \"",
            "                    \"for load balancer id %(LB)s and listener ids: %(list)s\",",
            "                    {'LB': loadbalancer.id,",
            "                     'list': ', '.join([l.id for l in listeners])})",
            "        self.task_utils.mark_loadbalancer_prov_status_error(loadbalancer.id)",
            "        for listener in listeners:",
            "            self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "",
            "class MarkListenerActiveInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the listener active in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, listener):",
            "        \"\"\"Mark the listener as active in DB",
            "",
            "        :param listener: The listener to be marked active",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark ACTIVE in DB for listener id: %s \", listener.id)",
            "        self.listener_repo.update(db_apis.get_session(), listener.id,",
            "                                  provisioning_status=constants.ACTIVE)",
            "",
            "    def revert(self, listener, *args, **kwargs):",
            "        \"\"\"Mark the listener ERROR since the delete couldn't happen",
            "",
            "        :param listener: The listener that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark listener active in DB \"",
            "                    \"for listener id %s\", listener.id)",
            "        self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "",
            "class MarkListenerDeletedInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the listener deleted in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, listener):",
            "        \"\"\"Mark the listener as deleted in DB",
            "",
            "        :param listener: The listener to be marked deleted",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark DELETED in DB for listener id: %s \", listener.id)",
            "        self.listener_repo.update(db_apis.get_session(), listener.id,",
            "                                  provisioning_status=constants.DELETED)",
            "",
            "    def revert(self, listener, *args, **kwargs):",
            "        \"\"\"Mark the listener ERROR since the delete couldn't happen",
            "",
            "        :param listener: The listener that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark listener deleted in DB \"",
            "                    \"for listener id %s\", listener.id)",
            "        self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "",
            "class MarkListenerPendingDeleteInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the listener pending delete in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, listener):",
            "        \"\"\"Mark the listener as pending delete in DB.",
            "",
            "        :param listener: The listener to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING DELETE in DB for listener id: %s\",",
            "                  listener.id)",
            "        self.listener_repo.update(db_apis.get_session(), listener.id,",
            "                                  provisioning_status=constants.PENDING_DELETE)",
            "",
            "    def revert(self, listener, *args, **kwargs):",
            "        \"\"\"Mark the listener as broken and ready to be cleaned up.",
            "",
            "        :param listener: The listener that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark listener pending delete in DB \"",
            "                    \"for listener id %s\", listener.id)",
            "        self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "",
            "class UpdateLoadbalancerInDB(BaseDatabaseTask):",
            "    \"\"\"Update the loadbalancer in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, loadbalancer, update_dict):",
            "        \"\"\"Update the loadbalancer in the DB",
            "",
            "        :param loadbalancer: The load balancer to be updated",
            "        :param update_dict: The dictionary of updates to apply",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update DB for loadbalancer id: %s \", loadbalancer.id)",
            "        if update_dict.get('vip'):",
            "            vip_dict = update_dict.pop('vip')",
            "            self.vip_repo.update(db_apis.get_session(),",
            "                                 loadbalancer.vip.load_balancer_id,",
            "                                 **vip_dict)",
            "        self.loadbalancer_repo.update(db_apis.get_session(), loadbalancer.id,",
            "                                      **update_dict)",
            "",
            "    def revert(self, loadbalancer, *args, **kwargs):",
            "        \"\"\"Mark the loadbalancer ERROR since the update couldn't happen",
            "",
            "        :param loadbalancer: The load balancer that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting update loadbalancer in DB \"",
            "                    \"for loadbalancer id %s\", loadbalancer.id)",
            "",
            "        self.task_utils.mark_loadbalancer_prov_status_error(loadbalancer.id)",
            "",
            "",
            "class UpdateHealthMonInDB(BaseDatabaseTask):",
            "    \"\"\"Update the health monitor in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, health_mon, update_dict):",
            "        \"\"\"Update the health monitor in the DB",
            "",
            "        :param health_mon: The health monitor to be updated",
            "        :param update_dict: The dictionary of updates to apply",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update DB for health monitor id: %s \", health_mon.id)",
            "        self.health_mon_repo.update(db_apis.get_session(), health_mon.id,",
            "                                    **update_dict)",
            "",
            "    def revert(self, health_mon, *args, **kwargs):",
            "        \"\"\"Mark the health monitor ERROR since the update couldn't happen",
            "",
            "        :param health_mon: The health monitor that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting update health monitor in DB \"",
            "                    \"for health monitor id %s\", health_mon.id)",
            "        try:",
            "            self.health_mon_repo.update(db_apis.get_session(),",
            "                                        health_mon.id,",
            "                                        provisioning_status=constants.ERROR)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update health monitor %(hm)s \"",
            "                      \"provisioning_status to ERROR due to: %(except)s\",",
            "                      {'hm': health_mon.id, 'except': e})",
            "",
            "",
            "class UpdateListenerInDB(BaseDatabaseTask):",
            "    \"\"\"Update the listener in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, listener, update_dict):",
            "        \"\"\"Update the listener in the DB",
            "",
            "        :param listener: The listener to be updated",
            "        :param update_dict: The dictionary of updates to apply",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update DB for listener id: %s \", listener.id)",
            "        self.listener_repo.update(db_apis.get_session(), listener.id,",
            "                                  **update_dict)",
            "",
            "    def revert(self, listener, *args, **kwargs):",
            "        \"\"\"Mark the listener ERROR since the update couldn't happen",
            "",
            "        :param listener: The listener that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting update listener in DB \"",
            "                    \"for listener id %s\", listener.id)",
            "        self.task_utils.mark_listener_prov_status_error(listener.id)",
            "",
            "",
            "class UpdateMemberInDB(BaseDatabaseTask):",
            "    \"\"\"Update the member in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, member, update_dict):",
            "        \"\"\"Update the member in the DB",
            "",
            "        :param member: The member to be updated",
            "        :param update_dict: The dictionary of updates to apply",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update DB for member id: %s \", member.id)",
            "        self.member_repo.update(db_apis.get_session(), member.id,",
            "                                **update_dict)",
            "",
            "    def revert(self, member, *args, **kwargs):",
            "        \"\"\"Mark the member ERROR since the update couldn't happen",
            "",
            "        :param member: The member that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting update member in DB \"",
            "                    \"for member id %s\", member.id)",
            "        try:",
            "            self.member_repo.update(db_apis.get_session(), member.id,",
            "                                    provisioning_status=constants.ERROR)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update member %(member)s provisioning_status \"",
            "                      \"to ERROR due to: %(except)s\", {'member': member.id,",
            "                                                      'except': e})",
            "",
            "",
            "class UpdatePoolInDB(BaseDatabaseTask):",
            "    \"\"\"Update the pool in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool, update_dict):",
            "        \"\"\"Update the pool in the DB",
            "",
            "        :param pool: The pool to be updated",
            "        :param update_dict: The dictionary of updates to apply",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update DB for pool id: %s \", pool.id)",
            "        self.repos.update_pool_and_sp(db_apis.get_session(), pool.id,",
            "                                      update_dict)",
            "",
            "    def revert(self, pool, *args, **kwargs):",
            "        \"\"\"Mark the pool ERROR since the update couldn't happen",
            "",
            "        :param pool: The pool that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting update pool in DB for pool id %s\", pool.id)",
            "        try:",
            "            self.repos.update_pool_and_sp(",
            "                db_apis.get_session(), pool.id,",
            "                dict(provisioning_status=constants.ERROR))",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update pool %(pool)s provisioning_status to \"",
            "                      \"ERROR due to: %(except)s\", {'pool': pool.id,",
            "                                                   'except': e})",
            "",
            "",
            "class UpdateL7PolicyInDB(BaseDatabaseTask):",
            "    \"\"\"Update the L7 policy in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7policy, update_dict):",
            "        \"\"\"Update the L7 policy in the DB",
            "",
            "        :param l7policy: The L7 policy to be updated",
            "        :param update_dict: The dictionary of updates to apply",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update DB for l7policy id: %s \", l7policy.id)",
            "        self.l7policy_repo.update(db_apis.get_session(), l7policy.id,",
            "                                  **update_dict)",
            "",
            "    def revert(self, l7policy, *args, **kwargs):",
            "        \"\"\"Mark the l7policy ERROR since the update couldn't happen",
            "",
            "        :param l7policy: L7 policy that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting update l7policy in DB \"",
            "                    \"for l7policy id %s\", l7policy.id)",
            "        try:",
            "            self.l7policy_repo.update(db_apis.get_session(), l7policy.id,",
            "                                      provisioning_status=constants.ERROR)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update l7policy %(l7p)s provisioning_status \"",
            "                      \"to ERROR due to: %(except)s\", {'l7p': l7policy.id,",
            "                                                      'except': e})",
            "",
            "",
            "class UpdateL7RuleInDB(BaseDatabaseTask):",
            "    \"\"\"Update the L7 rule in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7rule, update_dict):",
            "        \"\"\"Update the L7 rule in the DB",
            "",
            "        :param l7rule: The L7 rule to be updated",
            "        :param update_dict: The dictionary of updates to apply",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Update DB for l7rule id: %s \", l7rule.id)",
            "        self.l7rule_repo.update(db_apis.get_session(), l7rule.id,",
            "                                **update_dict)",
            "",
            "    def revert(self, l7rule, *args, **kwargs):",
            "        \"\"\"Mark the L7 rule ERROR since the update couldn't happen",
            "",
            "        :param l7rule: L7 rule that couldn't be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting update l7rule in DB \"",
            "                    \"for l7rule id %s\", l7rule.id)",
            "        try:",
            "            self.l7policy_repo.update(db_apis.get_session(),",
            "                                      l7rule.l7policy.id,",
            "                                      provisioning_status=constants.ERROR)",
            "        except Exception as e:",
            "            LOG.error(\"Failed to update L7rule %(l7r)s provisioning_status to \"",
            "                      \"ERROR due to: %(except)s\", {'l7r': l7rule.l7policy.id,",
            "                                                   'except': e})",
            "",
            "",
            "class GetAmphoraDetails(BaseDatabaseTask):",
            "    \"\"\"Task to retrieve amphora network details.\"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Retrieve amphora network details.",
            "",
            "        :param amphora: Amphora which network details are required",
            "        :returns: data_models.Amphora object",
            "        \"\"\"",
            "        return data_models.Amphora(id=amphora.id,",
            "                                   vrrp_ip=amphora.vrrp_ip,",
            "                                   ha_ip=amphora.ha_ip,",
            "                                   vrrp_port_id=amphora.vrrp_port_id,",
            "                                   ha_port_id=amphora.ha_port_id,",
            "                                   role=amphora.role,",
            "                                   vrrp_id=amphora.vrrp_id,",
            "                                   vrrp_priority=amphora.vrrp_priority)",
            "",
            "",
            "class GetAmphoraeFromLoadbalancer(BaseDatabaseTask):",
            "    \"\"\"Task to pull the listeners from a loadbalancer.\"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Pull the amphorae from a loadbalancer.",
            "",
            "        :param loadbalancer: Load balancer which listeners are required",
            "        :returns: A list of Listener objects",
            "        \"\"\"",
            "        amphorae = []",
            "        for amp in loadbalancer.amphorae:",
            "            a = self.amphora_repo.get(db_apis.get_session(), id=amp.id,",
            "                                      show_deleted=False)",
            "            if a is None:",
            "                continue",
            "            amphorae.append(a)",
            "        return amphorae",
            "",
            "",
            "class GetListenersFromLoadbalancer(BaseDatabaseTask):",
            "    \"\"\"Task to pull the listeners from a loadbalancer.\"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Pull the listeners from a loadbalancer.",
            "",
            "        :param loadbalancer: Load balancer which listeners are required",
            "        :returns: A list of Listener objects",
            "        \"\"\"",
            "        listeners = []",
            "        for listener in loadbalancer.listeners:",
            "            l = self.listener_repo.get(db_apis.get_session(), id=listener.id)",
            "            l.load_balancer = loadbalancer",
            "            listeners.append(l)",
            "        return listeners",
            "",
            "",
            "class GetVipFromLoadbalancer(BaseDatabaseTask):",
            "    \"\"\"Task to pull the vip from a loadbalancer.\"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Pull the vip from a loadbalancer.",
            "",
            "        :param loadbalancer: Load balancer which VIP is required",
            "        :returns: VIP associated with a given load balancer",
            "        \"\"\"",
            "        return loadbalancer.vip",
            "",
            "",
            "class CreateVRRPGroupForLB(BaseDatabaseTask):",
            "    \"\"\"Create a VRRP group for a load balancer.\"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Create a VRRP group for a load balancer.",
            "",
            "        :param loadbalancer: Load balancer for which a VRRP group",
            "               should be created",
            "        :returns: Updated load balancer",
            "        \"\"\"",
            "        try:",
            "            loadbalancer.vrrp_group = self.repos.vrrpgroup.create(",
            "                db_apis.get_session(),",
            "                load_balancer_id=loadbalancer.id,",
            "                vrrp_group_name=str(loadbalancer.id).replace('-', ''),",
            "                vrrp_auth_type=constants.VRRP_AUTH_DEFAULT,",
            "                vrrp_auth_pass=uuidutils.generate_uuid().replace('-', '')[0:7],",
            "                advert_int=CONF.keepalived_vrrp.vrrp_advert_int)",
            "        except odb_exceptions.DBDuplicateEntry:",
            "            LOG.debug('VRRP_GROUP entry already exists for load balancer, '",
            "                      'skipping create.')",
            "        return loadbalancer",
            "",
            "",
            "class DisableAmphoraHealthMonitoring(BaseDatabaseTask):",
            "    \"\"\"Disable amphora health monitoring.",
            "",
            "    This disables amphora health monitoring by removing it from",
            "    the amphora_health table.",
            "    \"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Disable health monitoring for an amphora",
            "",
            "        :param amphora: The amphora to disable health monitoring for",
            "        :returns: None",
            "        \"\"\"",
            "        self._delete_from_amp_health(amphora.id)",
            "",
            "",
            "class DisableLBAmphoraeHealthMonitoring(BaseDatabaseTask):",
            "    \"\"\"Disable health monitoring on the LB amphorae.",
            "",
            "    This disables amphora health monitoring by removing it from",
            "    the amphora_health table for each amphora on a load balancer.",
            "    \"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Disable health monitoring for amphora on a load balancer",
            "",
            "        :param loadbalancer: The load balancer to disable health monitoring on",
            "        :returns: None",
            "        \"\"\"",
            "        for amphora in loadbalancer.amphorae:",
            "            self._delete_from_amp_health(amphora.id)",
            "",
            "",
            "class MarkAmphoraHealthBusy(BaseDatabaseTask):",
            "    \"\"\"Mark amphora health monitoring busy.",
            "",
            "    This prevents amphora failover by marking the amphora busy in",
            "    the amphora_health table.",
            "    \"\"\"",
            "",
            "    def execute(self, amphora):",
            "        \"\"\"Mark amphora health monitoring busy",
            "",
            "        :param amphora: The amphora to mark amphora health busy",
            "        :returns: None",
            "        \"\"\"",
            "        self._mark_amp_health_busy(amphora.id)",
            "",
            "",
            "class MarkLBAmphoraeHealthBusy(BaseDatabaseTask):",
            "    \"\"\"Mark amphorae health monitoring busy for the LB.",
            "",
            "    This prevents amphorae failover by marking each amphora of a given",
            "    load balancer busy in the amphora_health table.",
            "    \"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Marks amphorae health busy for each amphora on a load balancer",
            "",
            "        :param loadbalancer: The load balancer to mark amphorae health busy",
            "        :returns: None",
            "        \"\"\"",
            "        for amphora in loadbalancer.amphorae:",
            "            self._mark_amp_health_busy(amphora.id)",
            "",
            "",
            "class MarkHealthMonitorActiveInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the health monitor ACTIVE in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, health_mon):",
            "        \"\"\"Mark the health monitor ACTIVE in DB.",
            "",
            "        :param health_mon: Health Monitor object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark ACTIVE in DB for health monitor id: %s\",",
            "                  health_mon.id)",
            "",
            "        op_status = (constants.ONLINE if health_mon.enabled",
            "                     else constants.OFFLINE)",
            "        self.health_mon_repo.update(db_apis.get_session(),",
            "                                    health_mon.id,",
            "                                    provisioning_status=constants.ACTIVE,",
            "                                    operating_status=op_status)",
            "",
            "    def revert(self, health_mon, *args, **kwargs):",
            "        \"\"\"Mark the health monitor as broken",
            "",
            "        :param health_mon: Health Monitor object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark health montor ACTIVE in DB \"",
            "                    \"for health monitor id %s\", health_mon.id)",
            "        self.task_utils.mark_health_mon_prov_status_error(health_mon.id)",
            "",
            "",
            "class MarkHealthMonitorPendingCreateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the health monitor pending create in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, health_mon):",
            "        \"\"\"Mark the health monitor as pending create in DB.",
            "",
            "        :param health_mon: Health Monitor object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING CREATE in DB for health monitor id: %s\",",
            "                  health_mon.id)",
            "        self.health_mon_repo.update(db_apis.get_session(),",
            "                                    health_mon.id,",
            "                                    provisioning_status=(constants.",
            "                                                         PENDING_CREATE))",
            "",
            "    def revert(self, health_mon, *args, **kwargs):",
            "        \"\"\"Mark the health monitor as broken",
            "",
            "        :param health_mon: Health Monitor object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark health monitor pending create in DB \"",
            "                    \"for health monitor id %s\", health_mon.id)",
            "        self.task_utils.mark_health_mon_prov_status_error(health_mon.id)",
            "",
            "",
            "class MarkHealthMonitorPendingDeleteInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the health monitor pending delete in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, health_mon):",
            "        \"\"\"Mark the health monitor as pending delete in DB.",
            "",
            "        :param health_mon: Health Monitor object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING DELETE in DB for health monitor id: %s\",",
            "                  health_mon.id)",
            "        self.health_mon_repo.update(db_apis.get_session(),",
            "                                    health_mon.id,",
            "                                    provisioning_status=(constants.",
            "                                                         PENDING_DELETE))",
            "",
            "    def revert(self, health_mon, *args, **kwargs):",
            "        \"\"\"Mark the health monitor as broken",
            "",
            "        :param health_mon: Health Monitor object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark health monitor pending delete in DB \"",
            "                    \"for health monitor id %s\", health_mon.id)",
            "        self.task_utils.mark_health_mon_prov_status_error(health_mon.id)",
            "",
            "",
            "class MarkHealthMonitorPendingUpdateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the health monitor pending update in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, health_mon):",
            "        \"\"\"Mark the health monitor as pending update in DB.",
            "",
            "        :param health_mon: Health Monitor object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING UPDATE in DB for health monitor id: %s\",",
            "                  health_mon.id)",
            "        self.health_mon_repo.update(db_apis.get_session(),",
            "                                    health_mon.id,",
            "                                    provisioning_status=(constants.",
            "                                                         PENDING_UPDATE))",
            "",
            "    def revert(self, health_mon, *args, **kwargs):",
            "        \"\"\"Mark the health monitor as broken",
            "",
            "        :param health_mon: Health Monitor object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark health monitor pending update in DB \"",
            "                    \"for health monitor id %s\", health_mon.id)",
            "        self.task_utils.mark_health_mon_prov_status_error(health_mon.id)",
            "",
            "",
            "class MarkL7PolicyActiveInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the l7policy ACTIVE in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7policy):",
            "        \"\"\"Mark the l7policy ACTIVE in DB.",
            "",
            "        :param l7policy: L7Policy object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark ACTIVE in DB for l7policy id: %s\",",
            "                  l7policy.id)",
            "",
            "        op_status = constants.ONLINE if l7policy.enabled else constants.OFFLINE",
            "        self.l7policy_repo.update(db_apis.get_session(),",
            "                                  l7policy.id,",
            "                                  provisioning_status=constants.ACTIVE,",
            "                                  operating_status=op_status)",
            "",
            "    def revert(self, l7policy, *args, **kwargs):",
            "        \"\"\"Mark the l7policy as broken",
            "",
            "        :param l7policy: L7Policy object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark l7policy ACTIVE in DB \"",
            "                    \"for l7policy id %s\", l7policy.id)",
            "        self.task_utils.mark_l7policy_prov_status_error(l7policy.id)",
            "",
            "",
            "class MarkL7PolicyPendingCreateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the l7policy pending create in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7policy):",
            "        \"\"\"Mark the l7policy as pending create in DB.",
            "",
            "        :param l7policy: L7Policy object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING CREATE in DB for l7policy id: %s\",",
            "                  l7policy.id)",
            "        self.l7policy_repo.update(db_apis.get_session(),",
            "                                  l7policy.id,",
            "                                  provisioning_status=constants.PENDING_CREATE)",
            "",
            "    def revert(self, l7policy, *args, **kwargs):",
            "        \"\"\"Mark the l7policy as broken",
            "",
            "        :param l7policy: L7Policy object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark l7policy pending create in DB \"",
            "                    \"for l7policy id %s\", l7policy.id)",
            "        self.task_utils.mark_l7policy_prov_status_error(l7policy.id)",
            "",
            "",
            "class MarkL7PolicyPendingDeleteInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the l7policy pending delete in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7policy):",
            "        \"\"\"Mark the l7policy as pending delete in DB.",
            "",
            "        :param l7policy: L7Policy object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING DELETE in DB for l7policy id: %s\",",
            "                  l7policy.id)",
            "        self.l7policy_repo.update(db_apis.get_session(),",
            "                                  l7policy.id,",
            "                                  provisioning_status=constants.PENDING_DELETE)",
            "",
            "    def revert(self, l7policy, *args, **kwargs):",
            "        \"\"\"Mark the l7policy as broken",
            "",
            "        :param l7policy: L7Policy object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark l7policy pending delete in DB \"",
            "                    \"for l7policy id %s\", l7policy.id)",
            "        self.task_utils.mark_l7policy_prov_status_error(l7policy.id)",
            "",
            "",
            "class MarkL7PolicyPendingUpdateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the l7policy pending update in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7policy):",
            "        \"\"\"Mark the l7policy as pending update in DB.",
            "",
            "        :param l7policy: L7Policy object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING UPDATE in DB for l7policy id: %s\",",
            "                  l7policy.id)",
            "        self.l7policy_repo.update(db_apis.get_session(),",
            "                                  l7policy.id,",
            "                                  provisioning_status=(constants.",
            "                                                       PENDING_UPDATE))",
            "",
            "    def revert(self, l7policy, *args, **kwargs):",
            "        \"\"\"Mark the l7policy as broken",
            "",
            "        :param l7policy: L7Policy object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark l7policy pending update in DB \"",
            "                    \"for l7policy id %s\", l7policy.id)",
            "        self.task_utils.mark_l7policy_prov_status_error(l7policy.id)",
            "",
            "",
            "class MarkL7RuleActiveInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the l7rule ACTIVE in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7rule):",
            "        \"\"\"Mark the l7rule ACTIVE in DB.",
            "",
            "        :param l7rule: L7Rule object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark ACTIVE in DB for l7rule id: %s\",",
            "                  l7rule.id)",
            "        op_status = constants.ONLINE if l7rule.enabled else constants.OFFLINE",
            "        self.l7rule_repo.update(db_apis.get_session(),",
            "                                l7rule.id,",
            "                                provisioning_status=constants.ACTIVE,",
            "                                operating_status=op_status)",
            "",
            "    def revert(self, l7rule, *args, **kwargs):",
            "        \"\"\"Mark the l7rule as broken",
            "",
            "        :param l7rule: L7Rule object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark l7rule ACTIVE in DB \"",
            "                    \"for l7rule id %s\", l7rule.id)",
            "        self.task_utils.mark_l7rule_prov_status_error(l7rule.id)",
            "",
            "",
            "class MarkL7RulePendingCreateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the l7rule pending create in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7rule):",
            "        \"\"\"Mark the l7rule as pending create in DB.",
            "",
            "        :param l7rule: L7Rule object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING CREATE in DB for l7rule id: %s\",",
            "                  l7rule.id)",
            "        self.l7rule_repo.update(db_apis.get_session(),",
            "                                l7rule.id,",
            "                                provisioning_status=constants.PENDING_CREATE)",
            "",
            "    def revert(self, l7rule, *args, **kwargs):",
            "        \"\"\"Mark the l7rule as broken",
            "",
            "        :param l7rule: L7Rule object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark l7rule pending create in DB \"",
            "                    \"for l7rule id %s\", l7rule.id)",
            "        self.task_utils.mark_l7rule_prov_status_error(l7rule.id)",
            "",
            "",
            "class MarkL7RulePendingDeleteInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the l7rule pending delete in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7rule):",
            "        \"\"\"Mark the l7rule as pending delete in DB.",
            "",
            "        :param l7rule: L7Rule object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING DELETE in DB for l7rule id: %s\",",
            "                  l7rule.id)",
            "        self.l7rule_repo.update(db_apis.get_session(),",
            "                                l7rule.id,",
            "                                provisioning_status=constants.PENDING_DELETE)",
            "",
            "    def revert(self, l7rule, *args, **kwargs):",
            "        \"\"\"Mark the l7rule as broken",
            "",
            "        :param l7rule: L7Rule object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark l7rule pending delete in DB \"",
            "                    \"for l7rule id %s\", l7rule.id)",
            "        self.task_utils.mark_l7rule_prov_status_error(l7rule.id)",
            "",
            "",
            "class MarkL7RulePendingUpdateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the l7rule pending update in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, l7rule):",
            "        \"\"\"Mark the l7rule as pending update in DB.",
            "",
            "        :param l7rule: L7Rule object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING UPDATE in DB for l7rule id: %s\",",
            "                  l7rule.id)",
            "        self.l7rule_repo.update(db_apis.get_session(),",
            "                                l7rule.id,",
            "                                provisioning_status=constants.PENDING_UPDATE)",
            "",
            "    def revert(self, l7rule, *args, **kwargs):",
            "        \"\"\"Mark the l7rule as broken",
            "",
            "        :param l7rule: L7Rule object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark l7rule pending update in DB \"",
            "                    \"for l7rule id %s\", l7rule.id)",
            "        self.task_utils.mark_l7rule_prov_status_error(l7rule.id)",
            "",
            "",
            "class MarkMemberActiveInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the member ACTIVE in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, member):",
            "        \"\"\"Mark the member ACTIVE in DB.",
            "",
            "        :param member: Member object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark ACTIVE in DB for member id: %s\", member.id)",
            "        self.member_repo.update(db_apis.get_session(),",
            "                                member.id,",
            "                                provisioning_status=constants.ACTIVE)",
            "",
            "    def revert(self, member, *args, **kwargs):",
            "        \"\"\"Mark the member as broken",
            "",
            "        :param member: Member object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark member ACTIVE in DB \"",
            "                    \"for member id %s\", member.id)",
            "        self.task_utils.mark_member_prov_status_error(member.id)",
            "",
            "",
            "class MarkMemberPendingCreateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the member pending create in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, member):",
            "        \"\"\"Mark the member as pending create in DB.",
            "",
            "        :param member: Member object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING CREATE in DB for member id: %s\", member.id)",
            "        self.member_repo.update(db_apis.get_session(),",
            "                                member.id,",
            "                                provisioning_status=constants.PENDING_CREATE)",
            "",
            "    def revert(self, member, *args, **kwargs):",
            "        \"\"\"Mark the member as broken",
            "",
            "        :param member: Member object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark member pending create in DB \"",
            "                    \"for member id %s\", member.id)",
            "        self.task_utils.mark_member_prov_status_error(member.id)",
            "",
            "",
            "class MarkMemberPendingDeleteInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the member pending delete in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, member):",
            "        \"\"\"Mark the member as pending delete in DB.",
            "",
            "        :param member: Member object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING DELETE in DB for member id: %s\", member.id)",
            "        self.member_repo.update(db_apis.get_session(),",
            "                                member.id,",
            "                                provisioning_status=constants.PENDING_DELETE)",
            "",
            "    def revert(self, member, *args, **kwargs):",
            "        \"\"\"Mark the member as broken",
            "",
            "        :param member: Member object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark member pending delete in DB \"",
            "                    \"for member id %s\", member.id)",
            "        self.task_utils.mark_member_prov_status_error(member.id)",
            "",
            "",
            "class MarkMemberPendingUpdateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the member pending update in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, member):",
            "        \"\"\"Mark the member as pending update in DB.",
            "",
            "        :param member: Member object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING UPDATE in DB for member id: %s\",",
            "                  member.id)",
            "        self.member_repo.update(db_apis.get_session(),",
            "                                member.id,",
            "                                provisioning_status=constants.PENDING_UPDATE)",
            "",
            "    def revert(self, member, *args, **kwargs):",
            "        \"\"\"Mark the member as broken",
            "",
            "        :param member: Member object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark member pending update in DB \"",
            "                    \"for member id %s\", member.id)",
            "        self.task_utils.mark_member_prov_status_error(member.id)",
            "",
            "",
            "class MarkPoolActiveInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the pool ACTIVE in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool):",
            "        \"\"\"Mark the pool ACTIVE in DB.",
            "",
            "        :param pool: Pool object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark ACTIVE in DB for pool id: %s\",",
            "                  pool.id)",
            "        self.pool_repo.update(db_apis.get_session(),",
            "                              pool.id,",
            "                              provisioning_status=constants.ACTIVE)",
            "",
            "    def revert(self, pool, *args, **kwargs):",
            "        \"\"\"Mark the pool as broken",
            "",
            "        :param pool: Pool object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark pool ACTIVE in DB for pool id %s\", pool.id)",
            "        self.task_utils.mark_pool_prov_status_error(pool.id)",
            "",
            "",
            "class MarkPoolPendingCreateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the pool pending create in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool):",
            "        \"\"\"Mark the pool as pending create in DB.",
            "",
            "        :param pool: Pool object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING CREATE in DB for pool id: %s\",",
            "                  pool.id)",
            "        self.pool_repo.update(db_apis.get_session(),",
            "                              pool.id,",
            "                              provisioning_status=constants.PENDING_CREATE)",
            "",
            "    def revert(self, pool, *args, **kwargs):",
            "        \"\"\"Mark the pool as broken",
            "",
            "        :param pool: Pool object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark pool pending create in DB \"",
            "                    \"for pool id %s\", pool.id)",
            "        self.task_utils.mark_pool_prov_status_error(pool.id)",
            "",
            "",
            "class MarkPoolPendingDeleteInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the pool pending delete in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool):",
            "        \"\"\"Mark the pool as pending delete in DB.",
            "",
            "        :param pool: Pool object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING DELETE in DB for pool id: %s\",",
            "                  pool.id)",
            "        self.pool_repo.update(db_apis.get_session(),",
            "                              pool.id,",
            "                              provisioning_status=constants.PENDING_DELETE)",
            "",
            "    def revert(self, pool, *args, **kwargs):",
            "        \"\"\"Mark the pool as broken",
            "",
            "        :param pool: Pool object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark pool pending delete in DB \"",
            "                    \"for pool id %s\", pool.id)",
            "        self.task_utils.mark_pool_prov_status_error(pool.id)",
            "",
            "",
            "class MarkPoolPendingUpdateInDB(BaseDatabaseTask):",
            "    \"\"\"Mark the pool pending update in the DB.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool):",
            "        \"\"\"Mark the pool as pending update in DB.",
            "",
            "        :param pool: Pool object to be updated",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Mark PENDING UPDATE in DB for pool id: %s\",",
            "                  pool.id)",
            "        self.pool_repo.update(db_apis.get_session(),",
            "                              pool.id,",
            "                              provisioning_status=constants.PENDING_UPDATE)",
            "",
            "    def revert(self, pool, *args, **kwargs):",
            "        \"\"\"Mark the pool as broken",
            "",
            "        :param pool: Pool object that failed to update",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning(\"Reverting mark pool pending update in DB \"",
            "                    \"for pool id %s\", pool.id)",
            "        self.task_utils.mark_pool_prov_status_error(pool.id)",
            "",
            "",
            "class DecrementHealthMonitorQuota(BaseDatabaseTask):",
            "    \"\"\"Decrements the health monitor quota for a project.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, health_mon):",
            "        \"\"\"Decrements the health monitor quota.",
            "",
            "        :param health_mon: The health monitor to decrement the quota on.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Decrementing health monitor quota for \"",
            "                  \"project: %s \", health_mon.project_id)",
            "",
            "        lock_session = db_apis.get_session(autocommit=False)",
            "        try:",
            "            self.repos.decrement_quota(lock_session,",
            "                                       data_models.HealthMonitor,",
            "                                       health_mon.project_id)",
            "            lock_session.commit()",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                LOG.error('Failed to decrement health monitor quota for '",
            "                          'project: %(proj)s the project may have excess '",
            "                          'quota in use.', {'proj': health_mon.project_id})",
            "                lock_session.rollback()",
            "",
            "    def revert(self, health_mon, result, *args, **kwargs):",
            "        \"\"\"Re-apply the quota",
            "",
            "        :param health_mon: The health monitor to decrement the quota on.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning('Reverting decrement quota for health monitor on project'",
            "                    ' %(proj)s Project quota counts may be incorrect.',",
            "                    {'proj': health_mon.project_id})",
            "",
            "        # Increment the quota back if this task wasn't the failure",
            "        if not isinstance(result, failure.Failure):",
            "",
            "            try:",
            "                session = db_apis.get_session()",
            "                lock_session = db_apis.get_session(autocommit=False)",
            "                try:",
            "                    self.repos.check_quota_met(session,",
            "                                               lock_session,",
            "                                               data_models.HealthMonitor,",
            "                                               health_mon.project_id)",
            "                    lock_session.commit()",
            "                except Exception:",
            "                    lock_session.rollback()",
            "            except Exception:",
            "                # Don't fail the revert flow",
            "                pass",
            "",
            "",
            "class DecrementListenerQuota(BaseDatabaseTask):",
            "    \"\"\"Decrements the listener quota for a project.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, listener):",
            "        \"\"\"Decrements the listener quota.",
            "",
            "        :param listener: The listener to decrement the quota on.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Decrementing listener quota for \"",
            "                  \"project: %s \", listener.project_id)",
            "",
            "        lock_session = db_apis.get_session(autocommit=False)",
            "        try:",
            "            self.repos.decrement_quota(lock_session,",
            "                                       data_models.Listener,",
            "                                       listener.project_id)",
            "            lock_session.commit()",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                LOG.error('Failed to decrement listener quota for project: '",
            "                          '%(proj)s the project may have excess quota in use.',",
            "                          {'proj': listener.project_id})",
            "                lock_session.rollback()",
            "",
            "    def revert(self, listener, result, *args, **kwargs):",
            "        \"\"\"Re-apply the quota",
            "",
            "        :param listener: The listener to decrement the quota on.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning('Reverting decrement quota for listener on project '",
            "                    '%(proj)s Project quota counts may be incorrect.',",
            "                    {'proj': listener.project_id})",
            "",
            "        # Increment the quota back if this task wasn't the failure",
            "        if not isinstance(result, failure.Failure):",
            "",
            "            try:",
            "                session = db_apis.get_session()",
            "                lock_session = db_apis.get_session(autocommit=False)",
            "                try:",
            "                    self.repos.check_quota_met(session,",
            "                                               lock_session,",
            "                                               data_models.Listener,",
            "                                               listener.project_id)",
            "                    lock_session.commit()",
            "                except Exception:",
            "                    lock_session.rollback()",
            "            except Exception:",
            "                # Don't fail the revert flow",
            "                pass",
            "",
            "",
            "class DecrementLoadBalancerQuota(BaseDatabaseTask):",
            "    \"\"\"Decrements the load balancer quota for a project.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, loadbalancer):",
            "        \"\"\"Decrements the load balancer quota.",
            "",
            "        :param loadbalancer: The load balancer to decrement the quota on.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Decrementing load balancer quota for \"",
            "                  \"project: %s \", loadbalancer.project_id)",
            "",
            "        lock_session = db_apis.get_session(autocommit=False)",
            "        try:",
            "            self.repos.decrement_quota(lock_session,",
            "                                       data_models.LoadBalancer,",
            "                                       loadbalancer.project_id)",
            "            lock_session.commit()",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                LOG.error('Failed to decrement load balancer quota for '",
            "                          'project: %(proj)s the project may have excess '",
            "                          'quota in use.', {'proj': loadbalancer.project_id})",
            "                lock_session.rollback()",
            "",
            "    def revert(self, loadbalancer, result, *args, **kwargs):",
            "        \"\"\"Re-apply the quota",
            "",
            "        :param loadbalancer: The load balancer to decrement the quota on.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning('Reverting decrement quota for load balancer on project '",
            "                    '%(proj)s Project quota counts may be incorrect.',",
            "                    {'proj': loadbalancer.project_id})",
            "",
            "        # Increment the quota back if this task wasn't the failure",
            "        if not isinstance(result, failure.Failure):",
            "",
            "            try:",
            "                session = db_apis.get_session()",
            "                lock_session = db_apis.get_session(autocommit=False)",
            "                try:",
            "                    self.repos.check_quota_met(session,",
            "                                               lock_session,",
            "                                               data_models.LoadBalancer,",
            "                                               loadbalancer.project_id)",
            "                    lock_session.commit()",
            "                except Exception:",
            "                    lock_session.rollback()",
            "            except Exception:",
            "                # Don't fail the revert flow",
            "                pass",
            "",
            "",
            "class DecrementMemberQuota(BaseDatabaseTask):",
            "    \"\"\"Decrements the member quota for a project.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, member):",
            "        \"\"\"Decrements the member quota.",
            "",
            "        :param member: The member to decrement the quota on.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Decrementing member quota for \"",
            "                  \"project: %s \", member.project_id)",
            "",
            "        lock_session = db_apis.get_session(autocommit=False)",
            "        try:",
            "            self.repos.decrement_quota(lock_session,",
            "                                       data_models.Member,",
            "                                       member.project_id)",
            "            lock_session.commit()",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                LOG.error('Failed to decrement member quota for project: '",
            "                          '%(proj)s the project may have excess quota in use.',",
            "                          {'proj': member.project_id})",
            "                lock_session.rollback()",
            "",
            "    def revert(self, member, result, *args, **kwargs):",
            "        \"\"\"Re-apply the quota",
            "",
            "        :param member: The member to decrement the quota on.",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning('Reverting decrement quota for member on project %(proj)s '",
            "                    'Project quota counts may be incorrect.',",
            "                    {'proj': member.project_id})",
            "",
            "        # Increment the quota back if this task wasn't the failure",
            "        if not isinstance(result, failure.Failure):",
            "",
            "            try:",
            "                session = db_apis.get_session()",
            "                lock_session = db_apis.get_session(autocommit=False)",
            "                try:",
            "                    self.repos.check_quota_met(session,",
            "                                               lock_session,",
            "                                               data_models.Member,",
            "                                               member.project_id)",
            "                    lock_session.commit()",
            "                except Exception:",
            "                    lock_session.rollback()",
            "            except Exception:",
            "                # Don't fail the revert flow",
            "                pass",
            "",
            "",
            "class DecrementPoolQuota(BaseDatabaseTask):",
            "    \"\"\"Decrements the pool quota for a project.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool, pool_child_count):",
            "        \"\"\"Decrements the pool quota.",
            "",
            "        :param pool: The pool to decrement the quota on",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Decrementing pool quota for \"",
            "                  \"project: %s \", pool.project_id)",
            "",
            "        lock_session = db_apis.get_session(autocommit=False)",
            "        try:",
            "            self.repos.decrement_quota(lock_session,",
            "                                       data_models.Pool,",
            "                                       pool.project_id)",
            "",
            "            # Pools cascade delete members and health monitors",
            "            # update the quota for those items as well.",
            "            if pool_child_count['HM'] > 0:",
            "                self.repos.decrement_quota(lock_session,",
            "                                           data_models.HealthMonitor,",
            "                                           pool.project_id)",
            "            if pool_child_count['member'] > 0:",
            "                self.repos.decrement_quota(",
            "                    lock_session, data_models.Member,",
            "                    pool.project_id, quantity=pool_child_count['member'])",
            "",
            "            lock_session.commit()",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                LOG.error('Failed to decrement pool quota for project: '",
            "                          '%(proj)s the project may have excess quota in use.',",
            "                          {'proj': pool.project_id})",
            "                lock_session.rollback()",
            "",
            "    def revert(self, pool, pool_child_count, result, *args, **kwargs):",
            "        \"\"\"Re-apply the quota",
            "",
            "        :param project_id: The id of project to decrement the quota on",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.warning('Reverting decrement quota for pool on project %(proj)s '",
            "                    'Project quota counts may be incorrect.',",
            "                    {'proj': pool.project_id})",
            "",
            "        # Increment the quota back if this task wasn't the failure",
            "        if not isinstance(result, failure.Failure):",
            "",
            "            # These are all independent to maximize the correction",
            "            # in case other quota actions have occurred",
            "            try:",
            "                session = db_apis.get_session()",
            "                lock_session = db_apis.get_session(autocommit=False)",
            "                try:",
            "                    self.repos.check_quota_met(session,",
            "                                               lock_session,",
            "                                               data_models.Pool,",
            "                                               pool.project_id)",
            "                    lock_session.commit()",
            "                except Exception:",
            "                    lock_session.rollback()",
            "",
            "                # Attempt to increment back the health monitor quota",
            "                if pool_child_count['HM'] > 0:",
            "                    lock_session = db_apis.get_session(autocommit=False)",
            "                    try:",
            "                        self.repos.check_quota_met(session,",
            "                                                   lock_session,",
            "                                                   data_models.HealthMonitor,",
            "                                                   pool.project_id)",
            "                        lock_session.commit()",
            "                    except Exception:",
            "                        lock_session.rollback()",
            "",
            "                # Attempt to increment back the member quota",
            "                # This is separate calls to maximize the correction",
            "                # should other factors have increased the in use quota",
            "                # before this point in the revert flow",
            "                for i in six.moves.range(pool_child_count['member']):",
            "                    lock_session = db_apis.get_session(autocommit=False)",
            "                    try:",
            "                        self.repos.check_quota_met(session,",
            "                                                   lock_session,",
            "                                                   data_models.Member,",
            "                                                   pool.project_id)",
            "                        lock_session.commit()",
            "                    except Exception:",
            "                        lock_session.rollback()",
            "            except Exception:",
            "                # Don't fail the revert flow",
            "                pass",
            "",
            "",
            "class CountPoolChildrenForQuota(BaseDatabaseTask):",
            "    \"\"\"Counts the pool child resources for quota management.",
            "",
            "    Since the children of pools are cleaned up by the sqlalchemy",
            "    cascade delete settings, we need to collect the quota counts",
            "    for the child objects early.",
            "",
            "    \"\"\"",
            "",
            "    def execute(self, pool):",
            "        \"\"\"Count the pool child resources for quota management",
            "",
            "        :param pool: The pool to count children on",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Counting pool children for \"",
            "                  \"project: %s \", pool.project_id)",
            "",
            "        health_mon_count = 1 if pool.health_monitor else 0",
            "        member_count = len(pool.members)",
            "",
            "        return {'HM': health_mon_count, 'member': member_count}",
            "",
            "",
            "class UpdatePoolMembersOperatingStatusInDB(BaseDatabaseTask):",
            "    \"\"\"Updates the members of a pool operating status.",
            "",
            "    Since sqlalchemy will likely retry by itself always revert if it fails",
            "    \"\"\"",
            "",
            "    def execute(self, pool, operating_status):",
            "        \"\"\"Update the members of a pool operating status in DB.",
            "",
            "        :param pool: Pool object to be updated",
            "        :param operating_status: Operating status to set",
            "        :returns: None",
            "        \"\"\"",
            "",
            "        LOG.debug(\"Updating member operating status to %(status)s in DB for \"",
            "                  \"pool id: %(pool)s\", {'status': operating_status,",
            "                                        'pool': pool.id})",
            "        self.member_repo.update_pool_members(db_apis.get_session(),",
            "                                             pool.id,",
            "                                             operating_status=operating_status)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "921": [
                "UpdateAmphoraDBCertExpiration",
                "execute"
            ]
        },
        "addLocation": []
    },
    "octavia/tests/unit/controller/worker/tasks/test_amphora_driver_tasks.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " # under the License."
            },
            "1": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " #"
            },
            "2": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+from cryptography import fernet"
            },
            "4": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " import mock"
            },
            "5": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from oslo_config import cfg"
            },
            "6": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " from oslo_config import fixture as oslo_fixture"
            },
            "7": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " from octavia.amphorae.driver_exceptions import exceptions as driver_except"
            },
            "8": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " from octavia.common import constants"
            },
            "9": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " from octavia.common import data_models"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+from octavia.common import utils"
            },
            "11": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " from octavia.controller.worker.tasks import amphora_driver_tasks"
            },
            "12": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from octavia.db import repositories as repo"
            },
            "13": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " import octavia.tests.unit.base as base"
            },
            "14": {
                "beforePatchRowNumber": 506,
                "afterPatchRowNumber": 508,
                "PatchRowcode": "                                  mock_listener_repo_get,"
            },
            "15": {
                "beforePatchRowNumber": 507,
                "afterPatchRowNumber": 509,
                "PatchRowcode": "                                  mock_listener_repo_update,"
            },
            "16": {
                "beforePatchRowNumber": 508,
                "afterPatchRowNumber": 510,
                "PatchRowcode": "                                  mock_amphora_repo_update):"
            },
            "17": {
                "beforePatchRowNumber": 509,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        pem_file_mock = 'test-perm-file'"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 511,
                "PatchRowcode": "+        key = utils.get_six_compatible_server_certs_key_passphrase()"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 512,
                "PatchRowcode": "+        fer = fernet.Fernet(key)"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 513,
                "PatchRowcode": "+        pem_file_mock = fer.encrypt("
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 514,
                "PatchRowcode": "+            utils.get_six_compatible_value('test-pem-file'))"
            },
            "22": {
                "beforePatchRowNumber": 510,
                "afterPatchRowNumber": 515,
                "PatchRowcode": "         amphora_cert_upload_mock = amphora_driver_tasks.AmphoraCertUpload()"
            },
            "23": {
                "beforePatchRowNumber": 511,
                "afterPatchRowNumber": 516,
                "PatchRowcode": "         amphora_cert_upload_mock.execute(_amphora_mock, pem_file_mock)"
            },
            "24": {
                "beforePatchRowNumber": 512,
                "afterPatchRowNumber": 517,
                "PatchRowcode": " "
            },
            "25": {
                "beforePatchRowNumber": 513,
                "afterPatchRowNumber": 518,
                "PatchRowcode": "         mock_driver.upload_cert_amp.assert_called_once_with("
            },
            "26": {
                "beforePatchRowNumber": 514,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            _amphora_mock, pem_file_mock)"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 519,
                "PatchRowcode": "+            _amphora_mock, fer.decrypt(pem_file_mock))"
            },
            "28": {
                "beforePatchRowNumber": 515,
                "afterPatchRowNumber": 520,
                "PatchRowcode": " "
            },
            "29": {
                "beforePatchRowNumber": 516,
                "afterPatchRowNumber": 521,
                "PatchRowcode": "     def test_amphora_update_vrrp_interface(self,"
            },
            "30": {
                "beforePatchRowNumber": 517,
                "afterPatchRowNumber": 522,
                "PatchRowcode": "                                            mock_driver,"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015 Hewlett-Packard Development Company, L.P.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "# http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "",
            "import mock",
            "from oslo_config import cfg",
            "from oslo_config import fixture as oslo_fixture",
            "from oslo_utils import uuidutils",
            "from taskflow.types import failure",
            "",
            "from octavia.amphorae.driver_exceptions import exceptions as driver_except",
            "from octavia.common import constants",
            "from octavia.common import data_models",
            "from octavia.controller.worker.tasks import amphora_driver_tasks",
            "from octavia.db import repositories as repo",
            "import octavia.tests.unit.base as base",
            "",
            "",
            "AMP_ID = uuidutils.generate_uuid()",
            "COMPUTE_ID = uuidutils.generate_uuid()",
            "LISTENER_ID = uuidutils.generate_uuid()",
            "LB_ID = uuidutils.generate_uuid()",
            "CONN_MAX_RETRIES = 10",
            "CONN_RETRY_INTERVAL = 6",
            "FAKE_CONFIG_FILE = 'fake config file'",
            "",
            "_amphora_mock = mock.MagicMock()",
            "_amphora_mock.id = AMP_ID",
            "_amphora_mock.status = constants.AMPHORA_ALLOCATED",
            "_load_balancer_mock = mock.MagicMock()",
            "_load_balancer_mock.id = LB_ID",
            "_listener_mock = mock.MagicMock()",
            "_listener_mock.id = LISTENER_ID",
            "_load_balancer_mock.listeners = [_listener_mock]",
            "_vip_mock = mock.MagicMock()",
            "_load_balancer_mock.vip = _vip_mock",
            "_LB_mock = mock.MagicMock()",
            "_amphorae_mock = [_amphora_mock]",
            "_network_mock = mock.MagicMock()",
            "_port_mock = mock.MagicMock()",
            "_ports_mock = [_port_mock]",
            "_session_mock = mock.MagicMock()",
            "",
            "",
            "@mock.patch('octavia.db.repositories.AmphoraRepository.update')",
            "@mock.patch('octavia.db.repositories.ListenerRepository.update')",
            "@mock.patch('octavia.db.repositories.ListenerRepository.get',",
            "            return_value=_listener_mock)",
            "@mock.patch('octavia.db.api.get_session', return_value=_session_mock)",
            "@mock.patch('octavia.controller.worker.tasks.amphora_driver_tasks.LOG')",
            "@mock.patch('oslo_utils.uuidutils.generate_uuid', return_value=AMP_ID)",
            "@mock.patch('stevedore.driver.DriverManager.driver')",
            "class TestAmphoraDriverTasks(base.TestCase):",
            "",
            "    def setUp(self):",
            "",
            "        _LB_mock.amphorae = [_amphora_mock]",
            "        _LB_mock.id = LB_ID",
            "        conf = oslo_fixture.Config(cfg.CONF)",
            "        conf.config(group=\"haproxy_amphora\",",
            "                    active_connection_max_retries=CONN_MAX_RETRIES)",
            "        conf.config(group=\"haproxy_amphora\",",
            "                    active_connection_rety_interval=CONN_RETRY_INTERVAL)",
            "        conf.config(group=\"controller_worker\",",
            "                    loadbalancer_topology=constants.TOPOLOGY_SINGLE)",
            "        super(TestAmphoraDriverTasks, self).setUp()",
            "",
            "    def test_amp_listener_update(self,",
            "                                 mock_driver,",
            "                                 mock_generate_uuid,",
            "                                 mock_log,",
            "                                 mock_get_session,",
            "                                 mock_listener_repo_get,",
            "                                 mock_listener_repo_update,",
            "                                 mock_amphora_repo_update):",
            "",
            "        timeout_dict = {constants.REQ_CONN_TIMEOUT: 1,",
            "                        constants.REQ_READ_TIMEOUT: 2,",
            "                        constants.CONN_MAX_RETRIES: 3,",
            "                        constants.CONN_RETRY_INTERVAL: 4}",
            "",
            "        amp_list_update_obj = amphora_driver_tasks.AmpListenersUpdate()",
            "        amp_list_update_obj.execute([_listener_mock], 0,",
            "                                    [_amphora_mock], timeout_dict)",
            "",
            "        mock_driver.update_amphora_listeners.assert_called_once_with(",
            "            [_listener_mock], 0, [_amphora_mock], timeout_dict)",
            "",
            "        mock_driver.update_amphora_listeners.side_effect = Exception('boom')",
            "",
            "        amp_list_update_obj.execute([_listener_mock], 0,",
            "                                    [_amphora_mock], timeout_dict)",
            "",
            "        mock_amphora_repo_update.assert_called_once_with(",
            "            _session_mock, AMP_ID, status=constants.ERROR)",
            "",
            "    def test_listener_update(self,",
            "                             mock_driver,",
            "                             mock_generate_uuid,",
            "                             mock_log,",
            "                             mock_get_session,",
            "                             mock_listener_repo_get,",
            "                             mock_listener_repo_update,",
            "                             mock_amphora_repo_update):",
            "",
            "        listener_update_obj = amphora_driver_tasks.ListenersUpdate()",
            "        listener_update_obj.execute(_load_balancer_mock, [_listener_mock])",
            "",
            "        mock_driver.update.assert_called_once_with(_listener_mock, _vip_mock)",
            "",
            "        # Test the revert",
            "        amp = listener_update_obj.revert(_load_balancer_mock)",
            "        repo.ListenerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LISTENER_ID,",
            "            provisioning_status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "        # Test the revert with exception",
            "        repo.ListenerRepository.update.reset_mock()",
            "        mock_listener_repo_update.side_effect = Exception('fail')",
            "        amp = listener_update_obj.revert(_load_balancer_mock)",
            "        repo.ListenerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LISTENER_ID,",
            "            provisioning_status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "    def test_listeners_update(self,",
            "                              mock_driver,",
            "                              mock_generate_uuid,",
            "                              mock_log,",
            "                              mock_get_session,",
            "                              mock_listener_repo_get,",
            "                              mock_listener_repo_update,",
            "                              mock_amphora_repo_update):",
            "        listeners_update_obj = amphora_driver_tasks.ListenersUpdate()",
            "        listeners = [data_models.Listener(id='listener1'),",
            "                     data_models.Listener(id='listener2')]",
            "        vip = data_models.Vip(ip_address='10.0.0.1')",
            "        lb = data_models.LoadBalancer(id='lb1', listeners=listeners, vip=vip)",
            "        listeners_update_obj.execute(lb, listeners)",
            "        mock_driver.update.assert_has_calls([mock.call(listeners[0], vip),",
            "                                             mock.call(listeners[1], vip)])",
            "        self.assertEqual(2, mock_driver.update.call_count)",
            "        self.assertIsNotNone(listeners[0].load_balancer)",
            "        self.assertIsNotNone(listeners[1].load_balancer)",
            "",
            "        # Test the revert",
            "        amp = listeners_update_obj.revert(lb)",
            "        expected_db_calls = [mock.call(_session_mock,",
            "                                       id=listeners[0].id,",
            "                                       provisioning_status=constants.ERROR),",
            "                             mock.call(_session_mock,",
            "                                       id=listeners[1].id,",
            "                                       provisioning_status=constants.ERROR)]",
            "        repo.ListenerRepository.update.has_calls(expected_db_calls)",
            "        self.assertEqual(2, repo.ListenerRepository.update.call_count)",
            "        self.assertIsNone(amp)",
            "",
            "    def test_listener_stop(self,",
            "                           mock_driver,",
            "                           mock_generate_uuid,",
            "                           mock_log,",
            "                           mock_get_session,",
            "                           mock_listener_repo_get,",
            "                           mock_listener_repo_update,",
            "                           mock_amphora_repo_update):",
            "",
            "        listener_stop_obj = amphora_driver_tasks.ListenerStop()",
            "        listener_stop_obj.execute(_load_balancer_mock, _listener_mock)",
            "",
            "        mock_driver.stop.assert_called_once_with(_listener_mock, _vip_mock)",
            "",
            "        # Test the revert",
            "        amp = listener_stop_obj.revert(_listener_mock)",
            "        repo.ListenerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LISTENER_ID,",
            "            provisioning_status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "        # Test the revert with exception",
            "        repo.ListenerRepository.update.reset_mock()",
            "        mock_listener_repo_update.side_effect = Exception('fail')",
            "        amp = listener_stop_obj.revert(_listener_mock)",
            "        repo.ListenerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LISTENER_ID,",
            "            provisioning_status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "    def test_listener_start(self,",
            "                            mock_driver,",
            "                            mock_generate_uuid,",
            "                            mock_log,",
            "                            mock_get_session,",
            "                            mock_listener_repo_get,",
            "                            mock_listener_repo_update,",
            "                            mock_amphora_repo_update):",
            "",
            "        listener_start_obj = amphora_driver_tasks.ListenerStart()",
            "        listener_start_obj.execute(_load_balancer_mock, _listener_mock)",
            "",
            "        mock_driver.start.assert_called_once_with(_listener_mock, _vip_mock)",
            "",
            "        # Test the revert",
            "        amp = listener_start_obj.revert(_listener_mock)",
            "        repo.ListenerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LISTENER_ID,",
            "            provisioning_status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "        # Test the revert with exception",
            "        repo.ListenerRepository.update.reset_mock()",
            "        mock_listener_repo_update.side_effect = Exception('fail')",
            "        amp = listener_start_obj.revert(_listener_mock)",
            "        repo.ListenerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LISTENER_ID,",
            "            provisioning_status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "    def test_listener_delete(self,",
            "                             mock_driver,",
            "                             mock_generate_uuid,",
            "                             mock_log,",
            "                             mock_get_session,",
            "                             mock_listener_repo_get,",
            "                             mock_listener_repo_update,",
            "                             mock_amphora_repo_update):",
            "",
            "        listener_delete_obj = amphora_driver_tasks.ListenerDelete()",
            "        listener_delete_obj.execute(_load_balancer_mock, _listener_mock)",
            "",
            "        mock_driver.delete.assert_called_once_with(_listener_mock, _vip_mock)",
            "",
            "        # Test the revert",
            "        amp = listener_delete_obj.revert(_listener_mock)",
            "        repo.ListenerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LISTENER_ID,",
            "            provisioning_status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "        # Test the revert with exception",
            "        repo.ListenerRepository.update.reset_mock()",
            "        mock_listener_repo_update.side_effect = Exception('fail')",
            "        amp = listener_delete_obj.revert(_listener_mock)",
            "        repo.ListenerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LISTENER_ID,",
            "            provisioning_status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "    def test_amphora_get_info(self,",
            "                              mock_driver,",
            "                              mock_generate_uuid,",
            "                              mock_log,",
            "                              mock_get_session,",
            "                              mock_listener_repo_get,",
            "                              mock_listener_repo_update,",
            "                              mock_amphora_repo_update):",
            "",
            "        amphora_get_info_obj = amphora_driver_tasks.AmphoraGetInfo()",
            "        amphora_get_info_obj.execute(_amphora_mock)",
            "",
            "        mock_driver.get_info.assert_called_once_with(",
            "            _amphora_mock)",
            "",
            "    def test_amphora_get_diagnostics(self,",
            "                                     mock_driver,",
            "                                     mock_generate_uuid,",
            "                                     mock_log,",
            "                                     mock_get_session,",
            "                                     mock_listener_repo_get,",
            "                                     mock_listener_repo_update,",
            "                                     mock_amphora_repo_update):",
            "",
            "        amphora_get_diagnostics_obj = (amphora_driver_tasks.",
            "                                       AmphoraGetDiagnostics())",
            "        amphora_get_diagnostics_obj.execute(_amphora_mock)",
            "",
            "        mock_driver.get_diagnostics.assert_called_once_with(",
            "            _amphora_mock)",
            "",
            "    def test_amphora_finalize(self,",
            "                              mock_driver,",
            "                              mock_generate_uuid,",
            "                              mock_log,",
            "                              mock_get_session,",
            "                              mock_listener_repo_get,",
            "                              mock_listener_repo_update,",
            "                              mock_amphora_repo_update):",
            "",
            "        amphora_finalize_obj = amphora_driver_tasks.AmphoraFinalize()",
            "        amphora_finalize_obj.execute(_amphora_mock)",
            "",
            "        mock_driver.finalize_amphora.assert_called_once_with(",
            "            _amphora_mock)",
            "",
            "        # Test revert",
            "        amp = amphora_finalize_obj.revert(None, _amphora_mock)",
            "        repo.AmphoraRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=AMP_ID,",
            "            status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "        # Test revert with exception",
            "        repo.AmphoraRepository.update.reset_mock()",
            "        mock_amphora_repo_update.side_effect = Exception('fail')",
            "        amp = amphora_finalize_obj.revert(None, _amphora_mock)",
            "        repo.AmphoraRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=AMP_ID,",
            "            status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "    def test_amphora_post_network_plug(self,",
            "                                       mock_driver,",
            "                                       mock_generate_uuid,",
            "                                       mock_log,",
            "                                       mock_get_session,",
            "                                       mock_listener_repo_get,",
            "                                       mock_listener_repo_update,",
            "                                       mock_amphora_repo_update):",
            "",
            "        amphora_post_network_plug_obj = (amphora_driver_tasks.",
            "                                         AmphoraPostNetworkPlug())",
            "        amphora_post_network_plug_obj.execute(_amphora_mock, _ports_mock)",
            "",
            "        (mock_driver.post_network_plug.",
            "            assert_called_once_with)(_amphora_mock, _port_mock)",
            "",
            "        # Test revert",
            "        amp = amphora_post_network_plug_obj.revert(None, _amphora_mock)",
            "        repo.AmphoraRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=AMP_ID,",
            "            status=constants.ERROR)",
            "",
            "        self.assertIsNone(amp)",
            "",
            "        # Test revert with exception",
            "        repo.AmphoraRepository.update.reset_mock()",
            "        mock_amphora_repo_update.side_effect = Exception('fail')",
            "        amp = amphora_post_network_plug_obj.revert(None, _amphora_mock)",
            "        repo.AmphoraRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=AMP_ID,",
            "            status=constants.ERROR)",
            "",
            "        self.assertIsNone(amp)",
            "",
            "    def test_amphorae_post_network_plug(self, mock_driver,",
            "                                        mock_generate_uuid,",
            "                                        mock_log,",
            "                                        mock_get_session,",
            "                                        mock_listener_repo_get,",
            "                                        mock_listener_repo_update,",
            "                                        mock_amphora_repo_update):",
            "        mock_driver.get_network.return_value = _network_mock",
            "        _amphora_mock.id = AMP_ID",
            "        _amphora_mock.compute_id = COMPUTE_ID",
            "        _LB_mock.amphorae = [_amphora_mock]",
            "        amphora_post_network_plug_obj = (amphora_driver_tasks.",
            "                                         AmphoraePostNetworkPlug())",
            "",
            "        port_mock = mock.Mock()",
            "        _deltas_mock = {_amphora_mock.id: [port_mock]}",
            "",
            "        amphora_post_network_plug_obj.execute(_LB_mock, _deltas_mock)",
            "",
            "        (mock_driver.post_network_plug.",
            "            assert_called_once_with(_amphora_mock, port_mock))",
            "",
            "        # Test revert",
            "        amp = amphora_post_network_plug_obj.revert(None, _LB_mock,",
            "                                                   _deltas_mock)",
            "        repo.AmphoraRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=AMP_ID,",
            "            status=constants.ERROR)",
            "",
            "        self.assertIsNone(amp)",
            "",
            "        # Test revert with exception",
            "        repo.AmphoraRepository.update.reset_mock()",
            "        mock_amphora_repo_update.side_effect = Exception('fail')",
            "        amp = amphora_post_network_plug_obj.revert(None, _LB_mock,",
            "                                                   _deltas_mock)",
            "        repo.AmphoraRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=AMP_ID,",
            "            status=constants.ERROR)",
            "",
            "        self.assertIsNone(amp)",
            "",
            "    @mock.patch('octavia.db.repositories.LoadBalancerRepository.update')",
            "    def test_amphora_post_vip_plug(self,",
            "                                   mock_loadbalancer_repo_update,",
            "                                   mock_driver,",
            "                                   mock_generate_uuid,",
            "                                   mock_log,",
            "                                   mock_get_session,",
            "                                   mock_listener_repo_get,",
            "                                   mock_listener_repo_update,",
            "                                   mock_amphora_repo_update):",
            "",
            "        amphorae_net_config_mock = mock.Mock()",
            "        amphora_post_vip_plug_obj = amphora_driver_tasks.AmphoraPostVIPPlug()",
            "        amphora_post_vip_plug_obj.execute(_amphora_mock,",
            "                                          _LB_mock,",
            "                                          amphorae_net_config_mock)",
            "",
            "        mock_driver.post_vip_plug.assert_called_once_with(",
            "            _amphora_mock, _LB_mock, amphorae_net_config_mock)",
            "",
            "        # Test revert",
            "        amp = amphora_post_vip_plug_obj.revert(None, _amphora_mock, _LB_mock)",
            "        repo.AmphoraRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=AMP_ID,",
            "            status=constants.ERROR)",
            "        repo.LoadBalancerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LB_ID,",
            "            provisioning_status=constants.ERROR)",
            "",
            "        self.assertIsNone(amp)",
            "",
            "        # Test revert with repo exceptions",
            "        repo.AmphoraRepository.update.reset_mock()",
            "        repo.LoadBalancerRepository.update.reset_mock()",
            "        mock_amphora_repo_update.side_effect = Exception('fail')",
            "        mock_loadbalancer_repo_update.side_effect = Exception('fail')",
            "        amp = amphora_post_vip_plug_obj.revert(None, _amphora_mock, _LB_mock)",
            "        repo.AmphoraRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=AMP_ID,",
            "            status=constants.ERROR)",
            "        repo.LoadBalancerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LB_ID,",
            "            provisioning_status=constants.ERROR)",
            "",
            "        self.assertIsNone(amp)",
            "",
            "    @mock.patch('octavia.db.repositories.LoadBalancerRepository.update')",
            "    def test_amphorae_post_vip_plug(self,",
            "                                    mock_loadbalancer_repo_update,",
            "                                    mock_driver,",
            "                                    mock_generate_uuid,",
            "                                    mock_log,",
            "                                    mock_get_session,",
            "                                    mock_listener_repo_get,",
            "                                    mock_listener_repo_update,",
            "                                    mock_amphora_repo_update):",
            "",
            "        amphorae_net_config_mock = mock.Mock()",
            "        amphora_post_vip_plug_obj = amphora_driver_tasks.AmphoraePostVIPPlug()",
            "        amphora_post_vip_plug_obj.execute(_LB_mock,",
            "                                          amphorae_net_config_mock)",
            "",
            "        mock_driver.post_vip_plug.assert_called_once_with(",
            "            _amphora_mock, _LB_mock, amphorae_net_config_mock)",
            "",
            "        # Test revert",
            "        amp = amphora_post_vip_plug_obj.revert(None, _LB_mock)",
            "        repo.LoadBalancerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LB_ID,",
            "            provisioning_status=constants.ERROR)",
            "",
            "        self.assertIsNone(amp)",
            "",
            "        # Test revert with exception",
            "        repo.LoadBalancerRepository.update.reset_mock()",
            "        mock_loadbalancer_repo_update.side_effect = Exception('fail')",
            "        amp = amphora_post_vip_plug_obj.revert(None, _LB_mock)",
            "        repo.LoadBalancerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LB_ID,",
            "            provisioning_status=constants.ERROR)",
            "",
            "        self.assertIsNone(amp)",
            "",
            "    def test_amphora_cert_upload(self,",
            "                                 mock_driver,",
            "                                 mock_generate_uuid,",
            "                                 mock_log,",
            "                                 mock_get_session,",
            "                                 mock_listener_repo_get,",
            "                                 mock_listener_repo_update,",
            "                                 mock_amphora_repo_update):",
            "        pem_file_mock = 'test-perm-file'",
            "        amphora_cert_upload_mock = amphora_driver_tasks.AmphoraCertUpload()",
            "        amphora_cert_upload_mock.execute(_amphora_mock, pem_file_mock)",
            "",
            "        mock_driver.upload_cert_amp.assert_called_once_with(",
            "            _amphora_mock, pem_file_mock)",
            "",
            "    def test_amphora_update_vrrp_interface(self,",
            "                                           mock_driver,",
            "                                           mock_generate_uuid,",
            "                                           mock_log,",
            "                                           mock_get_session,",
            "                                           mock_listener_repo_get,",
            "                                           mock_listener_repo_update,",
            "                                           mock_amphora_repo_update):",
            "        _LB_mock.amphorae = _amphorae_mock",
            "",
            "        timeout_dict = {constants.CONN_MAX_RETRIES: CONN_MAX_RETRIES,",
            "                        constants.CONN_RETRY_INTERVAL: CONN_RETRY_INTERVAL}",
            "",
            "        amphora_update_vrrp_interface_obj = (",
            "            amphora_driver_tasks.AmphoraUpdateVRRPInterface())",
            "        amphora_update_vrrp_interface_obj.execute(_LB_mock)",
            "        mock_driver.get_vrrp_interface.assert_called_once_with(",
            "            _amphora_mock, timeout_dict=timeout_dict)",
            "",
            "        # Test revert",
            "        mock_driver.reset_mock()",
            "",
            "        _LB_mock.amphorae = _amphorae_mock",
            "        amphora_update_vrrp_interface_obj.revert(\"BADRESULT\", _LB_mock)",
            "        mock_amphora_repo_update.assert_called_with(_session_mock,",
            "                                                    _amphora_mock.id,",
            "                                                    vrrp_interface=None)",
            "",
            "        mock_driver.reset_mock()",
            "        mock_amphora_repo_update.reset_mock()",
            "",
            "        failure_obj = failure.Failure.from_exception(Exception(\"TESTEXCEPT\"))",
            "        amphora_update_vrrp_interface_obj.revert(failure_obj, _LB_mock)",
            "        self.assertFalse(mock_amphora_repo_update.called)",
            "",
            "        # Test revert with exception",
            "        mock_driver.reset_mock()",
            "        mock_amphora_repo_update.reset_mock()",
            "        mock_amphora_repo_update.side_effect = Exception('fail')",
            "",
            "        _LB_mock.amphorae = _amphorae_mock",
            "        amphora_update_vrrp_interface_obj.revert(\"BADRESULT\", _LB_mock)",
            "        mock_amphora_repo_update.assert_called_with(_session_mock,",
            "                                                    _amphora_mock.id,",
            "                                                    vrrp_interface=None)",
            "",
            "    def test_amphora_vrrp_update(self,",
            "                                 mock_driver,",
            "                                 mock_generate_uuid,",
            "                                 mock_log,",
            "                                 mock_get_session,",
            "                                 mock_listener_repo_get,",
            "                                 mock_listener_repo_update,",
            "                                 mock_amphora_repo_update):",
            "        amphorae_network_config = mock.MagicMock()",
            "        amphora_vrrp_update_obj = (",
            "            amphora_driver_tasks.AmphoraVRRPUpdate())",
            "        amphora_vrrp_update_obj.execute(_LB_mock, amphorae_network_config)",
            "        mock_driver.update_vrrp_conf.assert_called_once_with(",
            "            _LB_mock, amphorae_network_config)",
            "",
            "    def test_amphora_vrrp_stop(self,",
            "                               mock_driver,",
            "                               mock_generate_uuid,",
            "                               mock_log,",
            "                               mock_get_session,",
            "                               mock_listener_repo_get,",
            "                               mock_listener_repo_update,",
            "                               mock_amphora_repo_update):",
            "        amphora_vrrp_stop_obj = (",
            "            amphora_driver_tasks.AmphoraVRRPStop())",
            "        amphora_vrrp_stop_obj.execute(_LB_mock)",
            "        mock_driver.stop_vrrp_service.assert_called_once_with(_LB_mock)",
            "",
            "    def test_amphora_vrrp_start(self,",
            "                                mock_driver,",
            "                                mock_generate_uuid,",
            "                                mock_log,",
            "                                mock_get_session,",
            "                                mock_listener_repo_get,",
            "                                mock_listener_repo_update,",
            "                                mock_amphora_repo_update):",
            "        amphora_vrrp_start_obj = (",
            "            amphora_driver_tasks.AmphoraVRRPStart())",
            "        amphora_vrrp_start_obj.execute(_LB_mock)",
            "        mock_driver.start_vrrp_service.assert_called_once_with(_LB_mock)",
            "",
            "    def test_amphora_compute_connectivity_wait(self,",
            "                                               mock_driver,",
            "                                               mock_generate_uuid,",
            "                                               mock_log,",
            "                                               mock_get_session,",
            "                                               mock_listener_repo_get,",
            "                                               mock_listener_repo_update,",
            "                                               mock_amphora_repo_update):",
            "        amp_compute_conn_wait_obj = (",
            "            amphora_driver_tasks.AmphoraComputeConnectivityWait())",
            "        amp_compute_conn_wait_obj.execute(_amphora_mock)",
            "        mock_driver.get_info.assert_called_once_with(_amphora_mock)",
            "",
            "        mock_driver.get_info.side_effect = driver_except.TimeOutException()",
            "        self.assertRaises(driver_except.TimeOutException,",
            "                          amp_compute_conn_wait_obj.execute, _amphora_mock)",
            "        mock_amphora_repo_update.assert_called_once_with(",
            "            _session_mock, AMP_ID, status=constants.ERROR)",
            "",
            "    @mock.patch('octavia.amphorae.backends.agent.agent_jinja_cfg.'",
            "                'AgentJinjaTemplater.build_agent_config')",
            "    def test_amphora_config_update(self,",
            "                                   mock_build_config,",
            "                                   mock_driver,",
            "                                   mock_generate_uuid,",
            "                                   mock_log,",
            "                                   mock_get_session,",
            "                                   mock_listener_repo_get,",
            "                                   mock_listener_repo_update,",
            "                                   mock_amphora_repo_update):",
            "        mock_build_config.return_value = FAKE_CONFIG_FILE",
            "        amp_config_update_obj = amphora_driver_tasks.AmphoraConfigUpdate()",
            "        mock_driver.update_amphora_agent_config.side_effect = [",
            "            None, None, driver_except.AmpDriverNotImplementedError,",
            "            driver_except.TimeOutException]",
            "        # With Flavor",
            "        flavor = {constants.LOADBALANCER_TOPOLOGY:",
            "                  constants.TOPOLOGY_ACTIVE_STANDBY}",
            "        amp_config_update_obj.execute(_amphora_mock, flavor)",
            "        mock_build_config.assert_called_once_with(",
            "            _amphora_mock.id, constants.TOPOLOGY_ACTIVE_STANDBY)",
            "        mock_driver.update_amphora_agent_config.assert_called_once_with(",
            "            _amphora_mock, FAKE_CONFIG_FILE)",
            "        # With no Flavor",
            "        mock_driver.reset_mock()",
            "        mock_build_config.reset_mock()",
            "        amp_config_update_obj.execute(_amphora_mock, None)",
            "        mock_build_config.assert_called_once_with(",
            "            _amphora_mock.id, constants.TOPOLOGY_SINGLE)",
            "        mock_driver.update_amphora_agent_config.assert_called_once_with(",
            "            _amphora_mock, FAKE_CONFIG_FILE)",
            "        # With amphora that does not support config update",
            "        mock_driver.reset_mock()",
            "        mock_build_config.reset_mock()",
            "        amp_config_update_obj.execute(_amphora_mock, flavor)",
            "        mock_build_config.assert_called_once_with(",
            "            _amphora_mock.id, constants.TOPOLOGY_ACTIVE_STANDBY)",
            "        mock_driver.update_amphora_agent_config.assert_called_once_with(",
            "            _amphora_mock, FAKE_CONFIG_FILE)",
            "        # With an unknown exception",
            "        mock_driver.reset_mock()",
            "        mock_build_config.reset_mock()",
            "        self.assertRaises(driver_except.TimeOutException,",
            "                          amp_config_update_obj.execute,",
            "                          _amphora_mock, flavor)"
        ],
        "afterPatchFile": [
            "# Copyright 2015 Hewlett-Packard Development Company, L.P.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "# http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "",
            "from cryptography import fernet",
            "import mock",
            "from oslo_config import cfg",
            "from oslo_config import fixture as oslo_fixture",
            "from oslo_utils import uuidutils",
            "from taskflow.types import failure",
            "",
            "from octavia.amphorae.driver_exceptions import exceptions as driver_except",
            "from octavia.common import constants",
            "from octavia.common import data_models",
            "from octavia.common import utils",
            "from octavia.controller.worker.tasks import amphora_driver_tasks",
            "from octavia.db import repositories as repo",
            "import octavia.tests.unit.base as base",
            "",
            "",
            "AMP_ID = uuidutils.generate_uuid()",
            "COMPUTE_ID = uuidutils.generate_uuid()",
            "LISTENER_ID = uuidutils.generate_uuid()",
            "LB_ID = uuidutils.generate_uuid()",
            "CONN_MAX_RETRIES = 10",
            "CONN_RETRY_INTERVAL = 6",
            "FAKE_CONFIG_FILE = 'fake config file'",
            "",
            "_amphora_mock = mock.MagicMock()",
            "_amphora_mock.id = AMP_ID",
            "_amphora_mock.status = constants.AMPHORA_ALLOCATED",
            "_load_balancer_mock = mock.MagicMock()",
            "_load_balancer_mock.id = LB_ID",
            "_listener_mock = mock.MagicMock()",
            "_listener_mock.id = LISTENER_ID",
            "_load_balancer_mock.listeners = [_listener_mock]",
            "_vip_mock = mock.MagicMock()",
            "_load_balancer_mock.vip = _vip_mock",
            "_LB_mock = mock.MagicMock()",
            "_amphorae_mock = [_amphora_mock]",
            "_network_mock = mock.MagicMock()",
            "_port_mock = mock.MagicMock()",
            "_ports_mock = [_port_mock]",
            "_session_mock = mock.MagicMock()",
            "",
            "",
            "@mock.patch('octavia.db.repositories.AmphoraRepository.update')",
            "@mock.patch('octavia.db.repositories.ListenerRepository.update')",
            "@mock.patch('octavia.db.repositories.ListenerRepository.get',",
            "            return_value=_listener_mock)",
            "@mock.patch('octavia.db.api.get_session', return_value=_session_mock)",
            "@mock.patch('octavia.controller.worker.tasks.amphora_driver_tasks.LOG')",
            "@mock.patch('oslo_utils.uuidutils.generate_uuid', return_value=AMP_ID)",
            "@mock.patch('stevedore.driver.DriverManager.driver')",
            "class TestAmphoraDriverTasks(base.TestCase):",
            "",
            "    def setUp(self):",
            "",
            "        _LB_mock.amphorae = [_amphora_mock]",
            "        _LB_mock.id = LB_ID",
            "        conf = oslo_fixture.Config(cfg.CONF)",
            "        conf.config(group=\"haproxy_amphora\",",
            "                    active_connection_max_retries=CONN_MAX_RETRIES)",
            "        conf.config(group=\"haproxy_amphora\",",
            "                    active_connection_rety_interval=CONN_RETRY_INTERVAL)",
            "        conf.config(group=\"controller_worker\",",
            "                    loadbalancer_topology=constants.TOPOLOGY_SINGLE)",
            "        super(TestAmphoraDriverTasks, self).setUp()",
            "",
            "    def test_amp_listener_update(self,",
            "                                 mock_driver,",
            "                                 mock_generate_uuid,",
            "                                 mock_log,",
            "                                 mock_get_session,",
            "                                 mock_listener_repo_get,",
            "                                 mock_listener_repo_update,",
            "                                 mock_amphora_repo_update):",
            "",
            "        timeout_dict = {constants.REQ_CONN_TIMEOUT: 1,",
            "                        constants.REQ_READ_TIMEOUT: 2,",
            "                        constants.CONN_MAX_RETRIES: 3,",
            "                        constants.CONN_RETRY_INTERVAL: 4}",
            "",
            "        amp_list_update_obj = amphora_driver_tasks.AmpListenersUpdate()",
            "        amp_list_update_obj.execute([_listener_mock], 0,",
            "                                    [_amphora_mock], timeout_dict)",
            "",
            "        mock_driver.update_amphora_listeners.assert_called_once_with(",
            "            [_listener_mock], 0, [_amphora_mock], timeout_dict)",
            "",
            "        mock_driver.update_amphora_listeners.side_effect = Exception('boom')",
            "",
            "        amp_list_update_obj.execute([_listener_mock], 0,",
            "                                    [_amphora_mock], timeout_dict)",
            "",
            "        mock_amphora_repo_update.assert_called_once_with(",
            "            _session_mock, AMP_ID, status=constants.ERROR)",
            "",
            "    def test_listener_update(self,",
            "                             mock_driver,",
            "                             mock_generate_uuid,",
            "                             mock_log,",
            "                             mock_get_session,",
            "                             mock_listener_repo_get,",
            "                             mock_listener_repo_update,",
            "                             mock_amphora_repo_update):",
            "",
            "        listener_update_obj = amphora_driver_tasks.ListenersUpdate()",
            "        listener_update_obj.execute(_load_balancer_mock, [_listener_mock])",
            "",
            "        mock_driver.update.assert_called_once_with(_listener_mock, _vip_mock)",
            "",
            "        # Test the revert",
            "        amp = listener_update_obj.revert(_load_balancer_mock)",
            "        repo.ListenerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LISTENER_ID,",
            "            provisioning_status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "        # Test the revert with exception",
            "        repo.ListenerRepository.update.reset_mock()",
            "        mock_listener_repo_update.side_effect = Exception('fail')",
            "        amp = listener_update_obj.revert(_load_balancer_mock)",
            "        repo.ListenerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LISTENER_ID,",
            "            provisioning_status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "    def test_listeners_update(self,",
            "                              mock_driver,",
            "                              mock_generate_uuid,",
            "                              mock_log,",
            "                              mock_get_session,",
            "                              mock_listener_repo_get,",
            "                              mock_listener_repo_update,",
            "                              mock_amphora_repo_update):",
            "        listeners_update_obj = amphora_driver_tasks.ListenersUpdate()",
            "        listeners = [data_models.Listener(id='listener1'),",
            "                     data_models.Listener(id='listener2')]",
            "        vip = data_models.Vip(ip_address='10.0.0.1')",
            "        lb = data_models.LoadBalancer(id='lb1', listeners=listeners, vip=vip)",
            "        listeners_update_obj.execute(lb, listeners)",
            "        mock_driver.update.assert_has_calls([mock.call(listeners[0], vip),",
            "                                             mock.call(listeners[1], vip)])",
            "        self.assertEqual(2, mock_driver.update.call_count)",
            "        self.assertIsNotNone(listeners[0].load_balancer)",
            "        self.assertIsNotNone(listeners[1].load_balancer)",
            "",
            "        # Test the revert",
            "        amp = listeners_update_obj.revert(lb)",
            "        expected_db_calls = [mock.call(_session_mock,",
            "                                       id=listeners[0].id,",
            "                                       provisioning_status=constants.ERROR),",
            "                             mock.call(_session_mock,",
            "                                       id=listeners[1].id,",
            "                                       provisioning_status=constants.ERROR)]",
            "        repo.ListenerRepository.update.has_calls(expected_db_calls)",
            "        self.assertEqual(2, repo.ListenerRepository.update.call_count)",
            "        self.assertIsNone(amp)",
            "",
            "    def test_listener_stop(self,",
            "                           mock_driver,",
            "                           mock_generate_uuid,",
            "                           mock_log,",
            "                           mock_get_session,",
            "                           mock_listener_repo_get,",
            "                           mock_listener_repo_update,",
            "                           mock_amphora_repo_update):",
            "",
            "        listener_stop_obj = amphora_driver_tasks.ListenerStop()",
            "        listener_stop_obj.execute(_load_balancer_mock, _listener_mock)",
            "",
            "        mock_driver.stop.assert_called_once_with(_listener_mock, _vip_mock)",
            "",
            "        # Test the revert",
            "        amp = listener_stop_obj.revert(_listener_mock)",
            "        repo.ListenerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LISTENER_ID,",
            "            provisioning_status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "        # Test the revert with exception",
            "        repo.ListenerRepository.update.reset_mock()",
            "        mock_listener_repo_update.side_effect = Exception('fail')",
            "        amp = listener_stop_obj.revert(_listener_mock)",
            "        repo.ListenerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LISTENER_ID,",
            "            provisioning_status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "    def test_listener_start(self,",
            "                            mock_driver,",
            "                            mock_generate_uuid,",
            "                            mock_log,",
            "                            mock_get_session,",
            "                            mock_listener_repo_get,",
            "                            mock_listener_repo_update,",
            "                            mock_amphora_repo_update):",
            "",
            "        listener_start_obj = amphora_driver_tasks.ListenerStart()",
            "        listener_start_obj.execute(_load_balancer_mock, _listener_mock)",
            "",
            "        mock_driver.start.assert_called_once_with(_listener_mock, _vip_mock)",
            "",
            "        # Test the revert",
            "        amp = listener_start_obj.revert(_listener_mock)",
            "        repo.ListenerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LISTENER_ID,",
            "            provisioning_status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "        # Test the revert with exception",
            "        repo.ListenerRepository.update.reset_mock()",
            "        mock_listener_repo_update.side_effect = Exception('fail')",
            "        amp = listener_start_obj.revert(_listener_mock)",
            "        repo.ListenerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LISTENER_ID,",
            "            provisioning_status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "    def test_listener_delete(self,",
            "                             mock_driver,",
            "                             mock_generate_uuid,",
            "                             mock_log,",
            "                             mock_get_session,",
            "                             mock_listener_repo_get,",
            "                             mock_listener_repo_update,",
            "                             mock_amphora_repo_update):",
            "",
            "        listener_delete_obj = amphora_driver_tasks.ListenerDelete()",
            "        listener_delete_obj.execute(_load_balancer_mock, _listener_mock)",
            "",
            "        mock_driver.delete.assert_called_once_with(_listener_mock, _vip_mock)",
            "",
            "        # Test the revert",
            "        amp = listener_delete_obj.revert(_listener_mock)",
            "        repo.ListenerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LISTENER_ID,",
            "            provisioning_status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "        # Test the revert with exception",
            "        repo.ListenerRepository.update.reset_mock()",
            "        mock_listener_repo_update.side_effect = Exception('fail')",
            "        amp = listener_delete_obj.revert(_listener_mock)",
            "        repo.ListenerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LISTENER_ID,",
            "            provisioning_status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "    def test_amphora_get_info(self,",
            "                              mock_driver,",
            "                              mock_generate_uuid,",
            "                              mock_log,",
            "                              mock_get_session,",
            "                              mock_listener_repo_get,",
            "                              mock_listener_repo_update,",
            "                              mock_amphora_repo_update):",
            "",
            "        amphora_get_info_obj = amphora_driver_tasks.AmphoraGetInfo()",
            "        amphora_get_info_obj.execute(_amphora_mock)",
            "",
            "        mock_driver.get_info.assert_called_once_with(",
            "            _amphora_mock)",
            "",
            "    def test_amphora_get_diagnostics(self,",
            "                                     mock_driver,",
            "                                     mock_generate_uuid,",
            "                                     mock_log,",
            "                                     mock_get_session,",
            "                                     mock_listener_repo_get,",
            "                                     mock_listener_repo_update,",
            "                                     mock_amphora_repo_update):",
            "",
            "        amphora_get_diagnostics_obj = (amphora_driver_tasks.",
            "                                       AmphoraGetDiagnostics())",
            "        amphora_get_diagnostics_obj.execute(_amphora_mock)",
            "",
            "        mock_driver.get_diagnostics.assert_called_once_with(",
            "            _amphora_mock)",
            "",
            "    def test_amphora_finalize(self,",
            "                              mock_driver,",
            "                              mock_generate_uuid,",
            "                              mock_log,",
            "                              mock_get_session,",
            "                              mock_listener_repo_get,",
            "                              mock_listener_repo_update,",
            "                              mock_amphora_repo_update):",
            "",
            "        amphora_finalize_obj = amphora_driver_tasks.AmphoraFinalize()",
            "        amphora_finalize_obj.execute(_amphora_mock)",
            "",
            "        mock_driver.finalize_amphora.assert_called_once_with(",
            "            _amphora_mock)",
            "",
            "        # Test revert",
            "        amp = amphora_finalize_obj.revert(None, _amphora_mock)",
            "        repo.AmphoraRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=AMP_ID,",
            "            status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "        # Test revert with exception",
            "        repo.AmphoraRepository.update.reset_mock()",
            "        mock_amphora_repo_update.side_effect = Exception('fail')",
            "        amp = amphora_finalize_obj.revert(None, _amphora_mock)",
            "        repo.AmphoraRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=AMP_ID,",
            "            status=constants.ERROR)",
            "        self.assertIsNone(amp)",
            "",
            "    def test_amphora_post_network_plug(self,",
            "                                       mock_driver,",
            "                                       mock_generate_uuid,",
            "                                       mock_log,",
            "                                       mock_get_session,",
            "                                       mock_listener_repo_get,",
            "                                       mock_listener_repo_update,",
            "                                       mock_amphora_repo_update):",
            "",
            "        amphora_post_network_plug_obj = (amphora_driver_tasks.",
            "                                         AmphoraPostNetworkPlug())",
            "        amphora_post_network_plug_obj.execute(_amphora_mock, _ports_mock)",
            "",
            "        (mock_driver.post_network_plug.",
            "            assert_called_once_with)(_amphora_mock, _port_mock)",
            "",
            "        # Test revert",
            "        amp = amphora_post_network_plug_obj.revert(None, _amphora_mock)",
            "        repo.AmphoraRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=AMP_ID,",
            "            status=constants.ERROR)",
            "",
            "        self.assertIsNone(amp)",
            "",
            "        # Test revert with exception",
            "        repo.AmphoraRepository.update.reset_mock()",
            "        mock_amphora_repo_update.side_effect = Exception('fail')",
            "        amp = amphora_post_network_plug_obj.revert(None, _amphora_mock)",
            "        repo.AmphoraRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=AMP_ID,",
            "            status=constants.ERROR)",
            "",
            "        self.assertIsNone(amp)",
            "",
            "    def test_amphorae_post_network_plug(self, mock_driver,",
            "                                        mock_generate_uuid,",
            "                                        mock_log,",
            "                                        mock_get_session,",
            "                                        mock_listener_repo_get,",
            "                                        mock_listener_repo_update,",
            "                                        mock_amphora_repo_update):",
            "        mock_driver.get_network.return_value = _network_mock",
            "        _amphora_mock.id = AMP_ID",
            "        _amphora_mock.compute_id = COMPUTE_ID",
            "        _LB_mock.amphorae = [_amphora_mock]",
            "        amphora_post_network_plug_obj = (amphora_driver_tasks.",
            "                                         AmphoraePostNetworkPlug())",
            "",
            "        port_mock = mock.Mock()",
            "        _deltas_mock = {_amphora_mock.id: [port_mock]}",
            "",
            "        amphora_post_network_plug_obj.execute(_LB_mock, _deltas_mock)",
            "",
            "        (mock_driver.post_network_plug.",
            "            assert_called_once_with(_amphora_mock, port_mock))",
            "",
            "        # Test revert",
            "        amp = amphora_post_network_plug_obj.revert(None, _LB_mock,",
            "                                                   _deltas_mock)",
            "        repo.AmphoraRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=AMP_ID,",
            "            status=constants.ERROR)",
            "",
            "        self.assertIsNone(amp)",
            "",
            "        # Test revert with exception",
            "        repo.AmphoraRepository.update.reset_mock()",
            "        mock_amphora_repo_update.side_effect = Exception('fail')",
            "        amp = amphora_post_network_plug_obj.revert(None, _LB_mock,",
            "                                                   _deltas_mock)",
            "        repo.AmphoraRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=AMP_ID,",
            "            status=constants.ERROR)",
            "",
            "        self.assertIsNone(amp)",
            "",
            "    @mock.patch('octavia.db.repositories.LoadBalancerRepository.update')",
            "    def test_amphora_post_vip_plug(self,",
            "                                   mock_loadbalancer_repo_update,",
            "                                   mock_driver,",
            "                                   mock_generate_uuid,",
            "                                   mock_log,",
            "                                   mock_get_session,",
            "                                   mock_listener_repo_get,",
            "                                   mock_listener_repo_update,",
            "                                   mock_amphora_repo_update):",
            "",
            "        amphorae_net_config_mock = mock.Mock()",
            "        amphora_post_vip_plug_obj = amphora_driver_tasks.AmphoraPostVIPPlug()",
            "        amphora_post_vip_plug_obj.execute(_amphora_mock,",
            "                                          _LB_mock,",
            "                                          amphorae_net_config_mock)",
            "",
            "        mock_driver.post_vip_plug.assert_called_once_with(",
            "            _amphora_mock, _LB_mock, amphorae_net_config_mock)",
            "",
            "        # Test revert",
            "        amp = amphora_post_vip_plug_obj.revert(None, _amphora_mock, _LB_mock)",
            "        repo.AmphoraRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=AMP_ID,",
            "            status=constants.ERROR)",
            "        repo.LoadBalancerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LB_ID,",
            "            provisioning_status=constants.ERROR)",
            "",
            "        self.assertIsNone(amp)",
            "",
            "        # Test revert with repo exceptions",
            "        repo.AmphoraRepository.update.reset_mock()",
            "        repo.LoadBalancerRepository.update.reset_mock()",
            "        mock_amphora_repo_update.side_effect = Exception('fail')",
            "        mock_loadbalancer_repo_update.side_effect = Exception('fail')",
            "        amp = amphora_post_vip_plug_obj.revert(None, _amphora_mock, _LB_mock)",
            "        repo.AmphoraRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=AMP_ID,",
            "            status=constants.ERROR)",
            "        repo.LoadBalancerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LB_ID,",
            "            provisioning_status=constants.ERROR)",
            "",
            "        self.assertIsNone(amp)",
            "",
            "    @mock.patch('octavia.db.repositories.LoadBalancerRepository.update')",
            "    def test_amphorae_post_vip_plug(self,",
            "                                    mock_loadbalancer_repo_update,",
            "                                    mock_driver,",
            "                                    mock_generate_uuid,",
            "                                    mock_log,",
            "                                    mock_get_session,",
            "                                    mock_listener_repo_get,",
            "                                    mock_listener_repo_update,",
            "                                    mock_amphora_repo_update):",
            "",
            "        amphorae_net_config_mock = mock.Mock()",
            "        amphora_post_vip_plug_obj = amphora_driver_tasks.AmphoraePostVIPPlug()",
            "        amphora_post_vip_plug_obj.execute(_LB_mock,",
            "                                          amphorae_net_config_mock)",
            "",
            "        mock_driver.post_vip_plug.assert_called_once_with(",
            "            _amphora_mock, _LB_mock, amphorae_net_config_mock)",
            "",
            "        # Test revert",
            "        amp = amphora_post_vip_plug_obj.revert(None, _LB_mock)",
            "        repo.LoadBalancerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LB_ID,",
            "            provisioning_status=constants.ERROR)",
            "",
            "        self.assertIsNone(amp)",
            "",
            "        # Test revert with exception",
            "        repo.LoadBalancerRepository.update.reset_mock()",
            "        mock_loadbalancer_repo_update.side_effect = Exception('fail')",
            "        amp = amphora_post_vip_plug_obj.revert(None, _LB_mock)",
            "        repo.LoadBalancerRepository.update.assert_called_once_with(",
            "            _session_mock,",
            "            id=LB_ID,",
            "            provisioning_status=constants.ERROR)",
            "",
            "        self.assertIsNone(amp)",
            "",
            "    def test_amphora_cert_upload(self,",
            "                                 mock_driver,",
            "                                 mock_generate_uuid,",
            "                                 mock_log,",
            "                                 mock_get_session,",
            "                                 mock_listener_repo_get,",
            "                                 mock_listener_repo_update,",
            "                                 mock_amphora_repo_update):",
            "        key = utils.get_six_compatible_server_certs_key_passphrase()",
            "        fer = fernet.Fernet(key)",
            "        pem_file_mock = fer.encrypt(",
            "            utils.get_six_compatible_value('test-pem-file'))",
            "        amphora_cert_upload_mock = amphora_driver_tasks.AmphoraCertUpload()",
            "        amphora_cert_upload_mock.execute(_amphora_mock, pem_file_mock)",
            "",
            "        mock_driver.upload_cert_amp.assert_called_once_with(",
            "            _amphora_mock, fer.decrypt(pem_file_mock))",
            "",
            "    def test_amphora_update_vrrp_interface(self,",
            "                                           mock_driver,",
            "                                           mock_generate_uuid,",
            "                                           mock_log,",
            "                                           mock_get_session,",
            "                                           mock_listener_repo_get,",
            "                                           mock_listener_repo_update,",
            "                                           mock_amphora_repo_update):",
            "        _LB_mock.amphorae = _amphorae_mock",
            "",
            "        timeout_dict = {constants.CONN_MAX_RETRIES: CONN_MAX_RETRIES,",
            "                        constants.CONN_RETRY_INTERVAL: CONN_RETRY_INTERVAL}",
            "",
            "        amphora_update_vrrp_interface_obj = (",
            "            amphora_driver_tasks.AmphoraUpdateVRRPInterface())",
            "        amphora_update_vrrp_interface_obj.execute(_LB_mock)",
            "        mock_driver.get_vrrp_interface.assert_called_once_with(",
            "            _amphora_mock, timeout_dict=timeout_dict)",
            "",
            "        # Test revert",
            "        mock_driver.reset_mock()",
            "",
            "        _LB_mock.amphorae = _amphorae_mock",
            "        amphora_update_vrrp_interface_obj.revert(\"BADRESULT\", _LB_mock)",
            "        mock_amphora_repo_update.assert_called_with(_session_mock,",
            "                                                    _amphora_mock.id,",
            "                                                    vrrp_interface=None)",
            "",
            "        mock_driver.reset_mock()",
            "        mock_amphora_repo_update.reset_mock()",
            "",
            "        failure_obj = failure.Failure.from_exception(Exception(\"TESTEXCEPT\"))",
            "        amphora_update_vrrp_interface_obj.revert(failure_obj, _LB_mock)",
            "        self.assertFalse(mock_amphora_repo_update.called)",
            "",
            "        # Test revert with exception",
            "        mock_driver.reset_mock()",
            "        mock_amphora_repo_update.reset_mock()",
            "        mock_amphora_repo_update.side_effect = Exception('fail')",
            "",
            "        _LB_mock.amphorae = _amphorae_mock",
            "        amphora_update_vrrp_interface_obj.revert(\"BADRESULT\", _LB_mock)",
            "        mock_amphora_repo_update.assert_called_with(_session_mock,",
            "                                                    _amphora_mock.id,",
            "                                                    vrrp_interface=None)",
            "",
            "    def test_amphora_vrrp_update(self,",
            "                                 mock_driver,",
            "                                 mock_generate_uuid,",
            "                                 mock_log,",
            "                                 mock_get_session,",
            "                                 mock_listener_repo_get,",
            "                                 mock_listener_repo_update,",
            "                                 mock_amphora_repo_update):",
            "        amphorae_network_config = mock.MagicMock()",
            "        amphora_vrrp_update_obj = (",
            "            amphora_driver_tasks.AmphoraVRRPUpdate())",
            "        amphora_vrrp_update_obj.execute(_LB_mock, amphorae_network_config)",
            "        mock_driver.update_vrrp_conf.assert_called_once_with(",
            "            _LB_mock, amphorae_network_config)",
            "",
            "    def test_amphora_vrrp_stop(self,",
            "                               mock_driver,",
            "                               mock_generate_uuid,",
            "                               mock_log,",
            "                               mock_get_session,",
            "                               mock_listener_repo_get,",
            "                               mock_listener_repo_update,",
            "                               mock_amphora_repo_update):",
            "        amphora_vrrp_stop_obj = (",
            "            amphora_driver_tasks.AmphoraVRRPStop())",
            "        amphora_vrrp_stop_obj.execute(_LB_mock)",
            "        mock_driver.stop_vrrp_service.assert_called_once_with(_LB_mock)",
            "",
            "    def test_amphora_vrrp_start(self,",
            "                                mock_driver,",
            "                                mock_generate_uuid,",
            "                                mock_log,",
            "                                mock_get_session,",
            "                                mock_listener_repo_get,",
            "                                mock_listener_repo_update,",
            "                                mock_amphora_repo_update):",
            "        amphora_vrrp_start_obj = (",
            "            amphora_driver_tasks.AmphoraVRRPStart())",
            "        amphora_vrrp_start_obj.execute(_LB_mock)",
            "        mock_driver.start_vrrp_service.assert_called_once_with(_LB_mock)",
            "",
            "    def test_amphora_compute_connectivity_wait(self,",
            "                                               mock_driver,",
            "                                               mock_generate_uuid,",
            "                                               mock_log,",
            "                                               mock_get_session,",
            "                                               mock_listener_repo_get,",
            "                                               mock_listener_repo_update,",
            "                                               mock_amphora_repo_update):",
            "        amp_compute_conn_wait_obj = (",
            "            amphora_driver_tasks.AmphoraComputeConnectivityWait())",
            "        amp_compute_conn_wait_obj.execute(_amphora_mock)",
            "        mock_driver.get_info.assert_called_once_with(_amphora_mock)",
            "",
            "        mock_driver.get_info.side_effect = driver_except.TimeOutException()",
            "        self.assertRaises(driver_except.TimeOutException,",
            "                          amp_compute_conn_wait_obj.execute, _amphora_mock)",
            "        mock_amphora_repo_update.assert_called_once_with(",
            "            _session_mock, AMP_ID, status=constants.ERROR)",
            "",
            "    @mock.patch('octavia.amphorae.backends.agent.agent_jinja_cfg.'",
            "                'AgentJinjaTemplater.build_agent_config')",
            "    def test_amphora_config_update(self,",
            "                                   mock_build_config,",
            "                                   mock_driver,",
            "                                   mock_generate_uuid,",
            "                                   mock_log,",
            "                                   mock_get_session,",
            "                                   mock_listener_repo_get,",
            "                                   mock_listener_repo_update,",
            "                                   mock_amphora_repo_update):",
            "        mock_build_config.return_value = FAKE_CONFIG_FILE",
            "        amp_config_update_obj = amphora_driver_tasks.AmphoraConfigUpdate()",
            "        mock_driver.update_amphora_agent_config.side_effect = [",
            "            None, None, driver_except.AmpDriverNotImplementedError,",
            "            driver_except.TimeOutException]",
            "        # With Flavor",
            "        flavor = {constants.LOADBALANCER_TOPOLOGY:",
            "                  constants.TOPOLOGY_ACTIVE_STANDBY}",
            "        amp_config_update_obj.execute(_amphora_mock, flavor)",
            "        mock_build_config.assert_called_once_with(",
            "            _amphora_mock.id, constants.TOPOLOGY_ACTIVE_STANDBY)",
            "        mock_driver.update_amphora_agent_config.assert_called_once_with(",
            "            _amphora_mock, FAKE_CONFIG_FILE)",
            "        # With no Flavor",
            "        mock_driver.reset_mock()",
            "        mock_build_config.reset_mock()",
            "        amp_config_update_obj.execute(_amphora_mock, None)",
            "        mock_build_config.assert_called_once_with(",
            "            _amphora_mock.id, constants.TOPOLOGY_SINGLE)",
            "        mock_driver.update_amphora_agent_config.assert_called_once_with(",
            "            _amphora_mock, FAKE_CONFIG_FILE)",
            "        # With amphora that does not support config update",
            "        mock_driver.reset_mock()",
            "        mock_build_config.reset_mock()",
            "        amp_config_update_obj.execute(_amphora_mock, flavor)",
            "        mock_build_config.assert_called_once_with(",
            "            _amphora_mock.id, constants.TOPOLOGY_ACTIVE_STANDBY)",
            "        mock_driver.update_amphora_agent_config.assert_called_once_with(",
            "            _amphora_mock, FAKE_CONFIG_FILE)",
            "        # With an unknown exception",
            "        mock_driver.reset_mock()",
            "        mock_build_config.reset_mock()",
            "        self.assertRaises(driver_except.TimeOutException,",
            "                          amp_config_update_obj.execute,",
            "                          _amphora_mock, flavor)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "509": [
                "TestAmphoraDriverTasks",
                "test_amphora_cert_upload"
            ],
            "514": [
                "TestAmphoraDriverTasks",
                "test_amphora_cert_upload"
            ]
        },
        "addLocation": []
    },
    "octavia/tests/unit/controller/worker/tasks/test_cert_task.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " # under the License."
            },
            "1": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " #"
            },
            "2": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+from cryptography import fernet"
            },
            "4": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " import mock"
            },
            "5": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " from octavia.certificates.common import local"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 20,
                "PatchRowcode": "+from octavia.common import utils"
            },
            "8": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from octavia.controller.worker.tasks import cert_task"
            },
            "9": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " import octavia.tests.unit.base as base"
            },
            "10": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " class TestCertTasks(base.TestCase):"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+"
            },
            "14": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 27,
                "PatchRowcode": "     @mock.patch('stevedore.driver.DriverManager.driver')"
            },
            "15": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 28,
                "PatchRowcode": "     def test_execute(self, mock_driver):"
            },
            "16": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        dummy_cert = local.LocalCert('test_cert', 'test_key')"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+        key = utils.get_six_compatible_server_certs_key_passphrase()"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+        fer = fernet.Fernet(key)"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+        dummy_cert = local.LocalCert("
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+            utils.get_six_compatible_value('test_cert'),"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+            utils.get_six_compatible_value('test_key'))"
            },
            "22": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 34,
                "PatchRowcode": "         mock_driver.generate_cert_key_pair.side_effect = [dummy_cert]"
            },
            "23": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 35,
                "PatchRowcode": "         c = cert_task.GenerateServerPEMTask()"
            },
            "24": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 36,
                "PatchRowcode": "         pem = c.execute('123')"
            },
            "25": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 37,
                "PatchRowcode": "         self.assertEqual("
            },
            "26": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            pem, dummy_cert.get_certificate() + dummy_cert.get_private_key())"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 38,
                "PatchRowcode": "+            fer.decrypt(pem),"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 39,
                "PatchRowcode": "+            dummy_cert.get_certificate() +"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 40,
                "PatchRowcode": "+            dummy_cert.get_private_key()"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 41,
                "PatchRowcode": "+        )"
            },
            "31": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 42,
                "PatchRowcode": "         mock_driver.generate_cert_key_pair.assert_called_once_with("
            },
            "32": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 43,
                "PatchRowcode": "             cn='123', validity=cert_task.CERT_VALIDITY)"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015 Hewlett-Packard Development Company, L.P.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "# http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "",
            "import mock",
            "",
            "from octavia.certificates.common import local",
            "from octavia.controller.worker.tasks import cert_task",
            "import octavia.tests.unit.base as base",
            "",
            "",
            "class TestCertTasks(base.TestCase):",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_execute(self, mock_driver):",
            "        dummy_cert = local.LocalCert('test_cert', 'test_key')",
            "        mock_driver.generate_cert_key_pair.side_effect = [dummy_cert]",
            "        c = cert_task.GenerateServerPEMTask()",
            "        pem = c.execute('123')",
            "        self.assertEqual(",
            "            pem, dummy_cert.get_certificate() + dummy_cert.get_private_key())",
            "        mock_driver.generate_cert_key_pair.assert_called_once_with(",
            "            cn='123', validity=cert_task.CERT_VALIDITY)"
        ],
        "afterPatchFile": [
            "# Copyright 2015 Hewlett-Packard Development Company, L.P.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "# http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "",
            "from cryptography import fernet",
            "import mock",
            "",
            "from octavia.certificates.common import local",
            "from octavia.common import utils",
            "from octavia.controller.worker.tasks import cert_task",
            "import octavia.tests.unit.base as base",
            "",
            "",
            "class TestCertTasks(base.TestCase):",
            "",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_execute(self, mock_driver):",
            "        key = utils.get_six_compatible_server_certs_key_passphrase()",
            "        fer = fernet.Fernet(key)",
            "        dummy_cert = local.LocalCert(",
            "            utils.get_six_compatible_value('test_cert'),",
            "            utils.get_six_compatible_value('test_key'))",
            "        mock_driver.generate_cert_key_pair.side_effect = [dummy_cert]",
            "        c = cert_task.GenerateServerPEMTask()",
            "        pem = c.execute('123')",
            "        self.assertEqual(",
            "            fer.decrypt(pem),",
            "            dummy_cert.get_certificate() +",
            "            dummy_cert.get_private_key()",
            "        )",
            "        mock_driver.generate_cert_key_pair.assert_called_once_with(",
            "            cn='123', validity=cert_task.CERT_VALIDITY)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "26": [
                "TestCertTasks",
                "test_execute"
            ],
            "31": [
                "TestCertTasks",
                "test_execute"
            ]
        },
        "addLocation": [
            "octavia.tests.unit.controller.worker.tasks.test_cert_task.TestCertTasks.self"
        ]
    },
    "octavia/tests/unit/controller/worker/tasks/test_compute_tasks.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " # under the License."
            },
            "1": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " #"
            },
            "2": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+from cryptography import fernet"
            },
            "4": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " import mock"
            },
            "5": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from oslo_config import cfg"
            },
            "6": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " from oslo_config import fixture as oslo_fixture"
            },
            "7": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " from oslo_utils import uuidutils"
            },
            "8": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " from octavia.common import constants"
            },
            "10": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " from octavia.common import exceptions"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 24,
                "PatchRowcode": "+from octavia.common import utils"
            },
            "12": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " from octavia.controller.worker.tasks import compute_tasks"
            },
            "13": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from octavia.tests.common import utils as test_utils"
            },
            "14": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " import octavia.tests.unit.base as base"
            },
            "15": {
                "beforePatchRowNumber": 270,
                "afterPatchRowNumber": 272,
                "PatchRowcode": "     @mock.patch('stevedore.driver.DriverManager.driver')"
            },
            "16": {
                "beforePatchRowNumber": 271,
                "afterPatchRowNumber": 273,
                "PatchRowcode": "     def test_compute_create_cert(self, mock_driver, mock_conf, mock_jinja):"
            },
            "17": {
                "beforePatchRowNumber": 272,
                "afterPatchRowNumber": 274,
                "PatchRowcode": "         createcompute = compute_tasks.CertComputeCreate()"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 275,
                "PatchRowcode": "+        key = utils.get_six_compatible_server_certs_key_passphrase()"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 276,
                "PatchRowcode": "+        fer = fernet.Fernet(key)"
            },
            "20": {
                "beforePatchRowNumber": 273,
                "afterPatchRowNumber": 277,
                "PatchRowcode": " "
            },
            "21": {
                "beforePatchRowNumber": 274,
                "afterPatchRowNumber": 278,
                "PatchRowcode": "         mock_driver.build.return_value = COMPUTE_ID"
            },
            "22": {
                "beforePatchRowNumber": 275,
                "afterPatchRowNumber": 279,
                "PatchRowcode": "         path = '/etc/octavia/certs/ca_01.pem'"
            },
            "23": {
                "beforePatchRowNumber": 276,
                "afterPatchRowNumber": 280,
                "PatchRowcode": "         self.useFixture(test_utils.OpenFixture(path, 'test'))"
            },
            "24": {
                "beforePatchRowNumber": 277,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "25": {
                "beforePatchRowNumber": 278,
                "afterPatchRowNumber": 281,
                "PatchRowcode": "         # Test execute()"
            },
            "26": {
                "beforePatchRowNumber": 279,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        compute_id = createcompute.execute(_amphora_mock.id, 'test_cert',"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 282,
                "PatchRowcode": "+        test_cert = fer.encrypt("
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 283,
                "PatchRowcode": "+            utils.get_six_compatible_value('test_cert')"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 284,
                "PatchRowcode": "+        )"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 285,
                "PatchRowcode": "+        compute_id = createcompute.execute(_amphora_mock.id, test_cert,"
            },
            "31": {
                "beforePatchRowNumber": 280,
                "afterPatchRowNumber": 286,
                "PatchRowcode": "                                            server_group_id=SERVER_GRPOUP_ID"
            },
            "32": {
                "beforePatchRowNumber": 281,
                "afterPatchRowNumber": 287,
                "PatchRowcode": "                                            )"
            },
            "33": {
                "beforePatchRowNumber": 282,
                "afterPatchRowNumber": 288,
                "PatchRowcode": " "
            },
            "34": {
                "beforePatchRowNumber": 293,
                "afterPatchRowNumber": 299,
                "PatchRowcode": "             port_ids=[],"
            },
            "35": {
                "beforePatchRowNumber": 294,
                "afterPatchRowNumber": 300,
                "PatchRowcode": "             user_data=None,"
            },
            "36": {
                "beforePatchRowNumber": 295,
                "afterPatchRowNumber": 301,
                "PatchRowcode": "             config_drive_files={"
            },
            "37": {
                "beforePatchRowNumber": 296,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                '/etc/octavia/certs/server.pem': 'test_cert',"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 302,
                "PatchRowcode": "+                '/etc/octavia/certs/server.pem': fer.decrypt(test_cert),"
            },
            "39": {
                "beforePatchRowNumber": 297,
                "afterPatchRowNumber": 303,
                "PatchRowcode": "                 '/etc/octavia/certs/client_ca.pem': 'test',"
            },
            "40": {
                "beforePatchRowNumber": 298,
                "afterPatchRowNumber": 304,
                "PatchRowcode": "                 '/etc/octavia/amphora-agent.conf': 'test_conf'},"
            },
            "41": {
                "beforePatchRowNumber": 299,
                "afterPatchRowNumber": 305,
                "PatchRowcode": "             server_group_id=SERVER_GRPOUP_ID)"
            },
            "42": {
                "beforePatchRowNumber": 307,
                "afterPatchRowNumber": 313,
                "PatchRowcode": "         self.assertRaises(TypeError,"
            },
            "43": {
                "beforePatchRowNumber": 308,
                "afterPatchRowNumber": 314,
                "PatchRowcode": "                           createcompute.execute,"
            },
            "44": {
                "beforePatchRowNumber": 309,
                "afterPatchRowNumber": 315,
                "PatchRowcode": "                           _amphora_mock,"
            },
            "45": {
                "beforePatchRowNumber": 310,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                          config_drive_files='test_cert')"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 316,
                "PatchRowcode": "+                          config_drive_files=test_cert)"
            },
            "47": {
                "beforePatchRowNumber": 311,
                "afterPatchRowNumber": 317,
                "PatchRowcode": " "
            },
            "48": {
                "beforePatchRowNumber": 312,
                "afterPatchRowNumber": 318,
                "PatchRowcode": "         # Test revert()"
            },
            "49": {
                "beforePatchRowNumber": 313,
                "afterPatchRowNumber": 319,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# Copyright 2015 Hewlett-Packard Development Company, L.P.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "# http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "",
            "import mock",
            "from oslo_config import cfg",
            "from oslo_config import fixture as oslo_fixture",
            "from oslo_utils import uuidutils",
            "",
            "from octavia.common import constants",
            "from octavia.common import exceptions",
            "from octavia.controller.worker.tasks import compute_tasks",
            "from octavia.tests.common import utils as test_utils",
            "import octavia.tests.unit.base as base",
            "",
            "",
            "AMP_FLAVOR_ID = '10'",
            "AMP_IMAGE_ID = '11'",
            "AMP_IMAGE_TAG = 'glance_tag'",
            "AMP_SSH_KEY_NAME = None",
            "AMP_NET = [uuidutils.generate_uuid()]",
            "AMP_SEC_GROUPS = []",
            "AMP_WAIT = 12",
            "AMPHORA_ID = uuidutils.generate_uuid()",
            "COMPUTE_ID = uuidutils.generate_uuid()",
            "LB_NET_IP = '192.0.2.1'",
            "PORT_ID = uuidutils.generate_uuid()",
            "SERVER_GRPOUP_ID = uuidutils.generate_uuid()",
            "",
            "",
            "class TestException(Exception):",
            "",
            "    def __init__(self, value):",
            "        self.value = value",
            "",
            "    def __str__(self):",
            "        return repr(self.value)",
            "",
            "_amphora_mock = mock.MagicMock()",
            "_amphora_mock.id = AMPHORA_ID",
            "_amphora_mock.compute_id = COMPUTE_ID",
            "_load_balancer_mock = mock.MagicMock()",
            "_load_balancer_mock.amphorae = [_amphora_mock]",
            "_port = mock.MagicMock()",
            "_port.id = PORT_ID",
            "",
            "",
            "class TestComputeTasks(base.TestCase):",
            "",
            "    def setUp(self):",
            "        self.conf = self.useFixture(oslo_fixture.Config(cfg.CONF))",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_flavor_id=AMP_FLAVOR_ID)",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_image_id=AMP_IMAGE_ID)",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_image_tag=AMP_IMAGE_TAG)",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_ssh_key_name=AMP_SSH_KEY_NAME)",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_boot_network_list=AMP_NET)",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_active_wait_sec=AMP_WAIT)",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_secgroup_list=AMP_SEC_GROUPS)",
            "        self.conf.config(group=\"controller_worker\", amp_image_owner_id='')",
            "",
            "        _amphora_mock.id = AMPHORA_ID",
            "        _amphora_mock.status = constants.AMPHORA_ALLOCATED",
            "",
            "        logging_mock = mock.MagicMock()",
            "        compute_tasks.LOG = logging_mock",
            "",
            "        super(TestComputeTasks, self).setUp()",
            "",
            "    @mock.patch('jinja2.Environment.get_template')",
            "    @mock.patch('octavia.amphorae.backends.agent.'",
            "                'agent_jinja_cfg.AgentJinjaTemplater.'",
            "                'build_agent_config', return_value='test_conf')",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_compute_create(self, mock_driver, mock_conf, mock_jinja):",
            "",
            "        image_owner_id = uuidutils.generate_uuid()",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_image_owner_id=image_owner_id)",
            "",
            "        createcompute = compute_tasks.ComputeCreate()",
            "",
            "        mock_driver.build.return_value = COMPUTE_ID",
            "        # Test execute()",
            "        compute_id = createcompute.execute(_amphora_mock.id, ports=[_port],",
            "                                           server_group_id=SERVER_GRPOUP_ID)",
            "",
            "        # Validate that the build method was called properly",
            "        mock_driver.build.assert_called_once_with(",
            "            name=\"amphora-\" + _amphora_mock.id,",
            "            amphora_flavor=AMP_FLAVOR_ID,",
            "            image_id=AMP_IMAGE_ID,",
            "            image_tag=AMP_IMAGE_TAG,",
            "            image_owner=image_owner_id,",
            "            key_name=AMP_SSH_KEY_NAME,",
            "            sec_groups=AMP_SEC_GROUPS,",
            "            network_ids=AMP_NET,",
            "            port_ids=[PORT_ID],",
            "            config_drive_files={'/etc/octavia/'",
            "                                'amphora-agent.conf': 'test_conf'},",
            "            user_data=None,",
            "            server_group_id=SERVER_GRPOUP_ID)",
            "",
            "        # Make sure it returns the expected compute_id",
            "        self.assertEqual(COMPUTE_ID, compute_id)",
            "",
            "        # Test that a build exception is raised",
            "        createcompute = compute_tasks.ComputeCreate()",
            "",
            "        self.assertRaises(TypeError,",
            "                          createcompute.execute,",
            "                          _amphora_mock, config_drive_files='test_cert')",
            "",
            "        # Test revert()",
            "",
            "        _amphora_mock.compute_id = COMPUTE_ID",
            "",
            "        createcompute = compute_tasks.ComputeCreate()",
            "        createcompute.revert(compute_id, _amphora_mock.id)",
            "",
            "        # Validate that the delete method was called properly",
            "        mock_driver.delete.assert_called_once_with(",
            "            COMPUTE_ID)",
            "",
            "        # Test that a delete exception is not raised",
            "",
            "        createcompute.revert(COMPUTE_ID, _amphora_mock.id)",
            "",
            "    @mock.patch('jinja2.Environment.get_template')",
            "    @mock.patch('octavia.amphorae.backends.agent.'",
            "                'agent_jinja_cfg.AgentJinjaTemplater.'",
            "                'build_agent_config', return_value='test_conf')",
            "    @mock.patch('octavia.common.jinja.'",
            "                'user_data_jinja_cfg.UserDataJinjaCfg.'",
            "                'build_user_data_config', return_value='test_conf')",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_compute_create_user_data(self, mock_driver,",
            "                                      mock_ud_conf, mock_conf, mock_jinja):",
            "",
            "        self.conf.config(",
            "            group=\"controller_worker\", user_data_config_drive=True)",
            "        mock_ud_conf.return_value = 'test_ud_conf'",
            "        createcompute = compute_tasks.ComputeCreate()",
            "",
            "        mock_driver.build.return_value = COMPUTE_ID",
            "        # Test execute()",
            "        compute_id = createcompute.execute(_amphora_mock.id, ports=[_port])",
            "",
            "        # Validate that the build method was called properly",
            "        mock_driver.build.assert_called_once_with(",
            "            name=\"amphora-\" + _amphora_mock.id,",
            "            amphora_flavor=AMP_FLAVOR_ID,",
            "            image_id=AMP_IMAGE_ID,",
            "            image_tag=AMP_IMAGE_TAG,",
            "            image_owner='',",
            "            key_name=AMP_SSH_KEY_NAME,",
            "            sec_groups=AMP_SEC_GROUPS,",
            "            network_ids=AMP_NET,",
            "            port_ids=[PORT_ID],",
            "            config_drive_files=None,",
            "            user_data='test_ud_conf',",
            "            server_group_id=None)",
            "",
            "        # Make sure it returns the expected compute_id",
            "        self.assertEqual(COMPUTE_ID, compute_id)",
            "",
            "        # Test that a build exception is raised",
            "        createcompute = compute_tasks.ComputeCreate()",
            "",
            "        self.assertRaises(TypeError,",
            "                          createcompute.execute,",
            "                          _amphora_mock, config_drive_files='test_cert')",
            "",
            "        # Test revert()",
            "",
            "        _amphora_mock.compute_id = COMPUTE_ID",
            "",
            "        createcompute = compute_tasks.ComputeCreate()",
            "        createcompute.revert(compute_id, _amphora_mock.id)",
            "",
            "        # Validate that the delete method was called properly",
            "        mock_driver.delete.assert_called_once_with(",
            "            COMPUTE_ID)",
            "",
            "        # Test that a delete exception is not raised",
            "",
            "        createcompute.revert(COMPUTE_ID, _amphora_mock.id)",
            "",
            "    @mock.patch('jinja2.Environment.get_template')",
            "    @mock.patch('octavia.amphorae.backends.agent.'",
            "                'agent_jinja_cfg.AgentJinjaTemplater.'",
            "                'build_agent_config', return_value='test_conf')",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_compute_create_without_ssh_access(self, mock_driver,",
            "                                               mock_conf, mock_jinja):",
            "",
            "        createcompute = compute_tasks.ComputeCreate()",
            "",
            "        mock_driver.build.return_value = COMPUTE_ID",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_ssh_access_allowed=False)",
            "        self.conf.config(",
            "            group=\"controller_worker\", user_data_config_drive=False)",
            "",
            "        # Test execute()",
            "        compute_id = createcompute.execute(_amphora_mock.id, ports=[_port],",
            "                                           server_group_id=SERVER_GRPOUP_ID)",
            "",
            "        # Validate that the build method was called properly",
            "        mock_driver.build.assert_called_once_with(",
            "            name=\"amphora-\" + _amphora_mock.id,",
            "            amphora_flavor=AMP_FLAVOR_ID,",
            "            image_id=AMP_IMAGE_ID,",
            "            image_tag=AMP_IMAGE_TAG,",
            "            image_owner='',",
            "            key_name=None,",
            "            sec_groups=AMP_SEC_GROUPS,",
            "            network_ids=AMP_NET,",
            "            port_ids=[PORT_ID],",
            "            config_drive_files={'/etc/octavia/'",
            "                                'amphora-agent.conf': 'test_conf'},",
            "            user_data=None,",
            "            server_group_id=SERVER_GRPOUP_ID)",
            "",
            "        self.assertEqual(COMPUTE_ID, compute_id)",
            "",
            "        # Test that a build exception is raised",
            "        createcompute = compute_tasks.ComputeCreate()",
            "",
            "        self.assertRaises(TypeError,",
            "                          createcompute.execute,",
            "                          _amphora_mock, config_drive_files='test_cert')",
            "",
            "        # Test revert()",
            "",
            "        _amphora_mock.compute_id = COMPUTE_ID",
            "",
            "        createcompute = compute_tasks.ComputeCreate()",
            "        createcompute.revert(compute_id, _amphora_mock.id)",
            "",
            "        # Validate that the delete method was called properly",
            "        mock_driver.delete.assert_called_once_with(",
            "            COMPUTE_ID)",
            "",
            "        # Test that a delete exception is not raised",
            "",
            "        createcompute.revert(COMPUTE_ID, _amphora_mock.id)",
            "",
            "    @mock.patch('jinja2.Environment.get_template')",
            "    @mock.patch('octavia.amphorae.backends.agent.'",
            "                'agent_jinja_cfg.AgentJinjaTemplater.'",
            "                'build_agent_config', return_value='test_conf')",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_compute_create_cert(self, mock_driver, mock_conf, mock_jinja):",
            "        createcompute = compute_tasks.CertComputeCreate()",
            "",
            "        mock_driver.build.return_value = COMPUTE_ID",
            "        path = '/etc/octavia/certs/ca_01.pem'",
            "        self.useFixture(test_utils.OpenFixture(path, 'test'))",
            "",
            "        # Test execute()",
            "        compute_id = createcompute.execute(_amphora_mock.id, 'test_cert',",
            "                                           server_group_id=SERVER_GRPOUP_ID",
            "                                           )",
            "",
            "        # Validate that the build method was called properly",
            "        mock_driver.build.assert_called_once_with(",
            "            name=\"amphora-\" + _amphora_mock.id,",
            "            amphora_flavor=AMP_FLAVOR_ID,",
            "            image_id=AMP_IMAGE_ID,",
            "            image_tag=AMP_IMAGE_TAG,",
            "            image_owner='',",
            "            key_name=AMP_SSH_KEY_NAME,",
            "            sec_groups=AMP_SEC_GROUPS,",
            "            network_ids=AMP_NET,",
            "            port_ids=[],",
            "            user_data=None,",
            "            config_drive_files={",
            "                '/etc/octavia/certs/server.pem': 'test_cert',",
            "                '/etc/octavia/certs/client_ca.pem': 'test',",
            "                '/etc/octavia/amphora-agent.conf': 'test_conf'},",
            "            server_group_id=SERVER_GRPOUP_ID)",
            "",
            "        self.assertEqual(COMPUTE_ID, compute_id)",
            "",
            "        # Test that a build exception is raised",
            "        self.useFixture(test_utils.OpenFixture(path, 'test'))",
            "",
            "        createcompute = compute_tasks.ComputeCreate()",
            "        self.assertRaises(TypeError,",
            "                          createcompute.execute,",
            "                          _amphora_mock,",
            "                          config_drive_files='test_cert')",
            "",
            "        # Test revert()",
            "",
            "        _amphora_mock.compute_id = COMPUTE_ID",
            "",
            "        createcompute = compute_tasks.ComputeCreate()",
            "        createcompute.revert(compute_id, _amphora_mock.id)",
            "",
            "        # Validate that the delete method was called properly",
            "        mock_driver.delete.assert_called_once_with(COMPUTE_ID)",
            "",
            "        # Test that a delete exception is not raised",
            "",
            "        createcompute.revert(COMPUTE_ID, _amphora_mock.id)",
            "",
            "    @mock.patch('octavia.controller.worker.amphora_rate_limit'",
            "                '.AmphoraBuildRateLimit.remove_from_build_req_queue')",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    @mock.patch('time.sleep')",
            "    def test_compute_wait(self,",
            "                          mock_time_sleep,",
            "                          mock_driver,",
            "                          mock_remove_from_build_queue):",
            "",
            "        self.conf.config(group='haproxy_amphora', build_rate_limit=5)",
            "        _amphora_mock.compute_id = COMPUTE_ID",
            "        _amphora_mock.status = constants.ACTIVE",
            "        _amphora_mock.lb_network_ip = LB_NET_IP",
            "",
            "        mock_driver.get_amphora.return_value = _amphora_mock, None",
            "",
            "        computewait = compute_tasks.ComputeActiveWait()",
            "        computewait.execute(COMPUTE_ID, AMPHORA_ID)",
            "",
            "        mock_driver.get_amphora.assert_called_once_with(COMPUTE_ID)",
            "",
            "        _amphora_mock.status = constants.DELETED",
            "",
            "        self.assertRaises(exceptions.ComputeWaitTimeoutException,",
            "                          computewait.execute,",
            "                          _amphora_mock, AMPHORA_ID)",
            "",
            "    @mock.patch('octavia.controller.worker.amphora_rate_limit'",
            "                '.AmphoraBuildRateLimit.remove_from_build_req_queue')",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    @mock.patch('time.sleep')",
            "    def test_compute_wait_error_status(self,",
            "                                       mock_time_sleep,",
            "                                       mock_driver,",
            "                                       mock_remove_from_build_queue):",
            "",
            "        self.conf.config(group='haproxy_amphora', build_rate_limit=5)",
            "        _amphora_mock.compute_id = COMPUTE_ID",
            "        _amphora_mock.status = constants.ACTIVE",
            "        _amphora_mock.lb_network_ip = LB_NET_IP",
            "",
            "        mock_driver.get_amphora.return_value = _amphora_mock, None",
            "",
            "        computewait = compute_tasks.ComputeActiveWait()",
            "        computewait.execute(COMPUTE_ID, AMPHORA_ID)",
            "",
            "        mock_driver.get_amphora.assert_called_once_with(COMPUTE_ID)",
            "",
            "        _amphora_mock.status = constants.ERROR",
            "",
            "        self.assertRaises(exceptions.ComputeBuildException,",
            "                          computewait.execute,",
            "                          _amphora_mock, AMPHORA_ID)",
            "",
            "    @mock.patch('octavia.controller.worker.amphora_rate_limit'",
            "                '.AmphoraBuildRateLimit.remove_from_build_req_queue')",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    @mock.patch('time.sleep')",
            "    def test_compute_wait_skipped(self,",
            "                                  mock_time_sleep,",
            "                                  mock_driver,",
            "                                  mock_remove_from_build_queue):",
            "        _amphora_mock.compute_id = COMPUTE_ID",
            "        _amphora_mock.status = constants.ACTIVE",
            "        _amphora_mock.lb_network_ip = LB_NET_IP",
            "",
            "        mock_driver.get_amphora.return_value = _amphora_mock, None",
            "",
            "        computewait = compute_tasks.ComputeActiveWait()",
            "        computewait.execute(COMPUTE_ID, AMPHORA_ID)",
            "",
            "        mock_driver.get_amphora.assert_called_once_with(COMPUTE_ID)",
            "        mock_remove_from_build_queue.assert_not_called()",
            "",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_delete_amphorae_on_load_balancer(self, mock_driver):",
            "",
            "        delete_amps = compute_tasks.DeleteAmphoraeOnLoadBalancer()",
            "        delete_amps.execute(_load_balancer_mock)",
            "",
            "        mock_driver.delete.assert_called_once_with(COMPUTE_ID)",
            "",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_compute_delete(self, mock_driver):",
            "",
            "        delete_compute = compute_tasks.ComputeDelete()",
            "        delete_compute.execute(_amphora_mock)",
            "",
            "        mock_driver.delete.assert_called_once_with(COMPUTE_ID)",
            "",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_nova_server_group_create(self, mock_driver):",
            "        nova_sever_group_obj = compute_tasks.NovaServerGroupCreate()",
            "",
            "        server_group_test_id = '6789'",
            "        fake_server_group = mock.MagicMock()",
            "        fake_server_group.id = server_group_test_id",
            "        fake_server_group.policy = 'anti-affinity'",
            "        mock_driver.create_server_group.return_value = fake_server_group",
            "",
            "        # Test execute()",
            "        sg_id = nova_sever_group_obj.execute('123')",
            "",
            "        # Validate that the build method was called properly",
            "        mock_driver.create_server_group.assert_called_once_with(",
            "            'octavia-lb-123', 'anti-affinity')",
            "",
            "        # Make sure it returns the expected server group_id",
            "        self.assertEqual(server_group_test_id, sg_id)",
            "",
            "        # Test revert()",
            "        nova_sever_group_obj.revert(sg_id)",
            "",
            "        # Validate that the delete_server_group method was called properly",
            "        mock_driver.delete_server_group.assert_called_once_with(sg_id)",
            "",
            "        # Test revert with exception",
            "        mock_driver.reset_mock()",
            "        mock_driver.delete_server_group.side_effect = Exception('DelSGExcept')",
            "        nova_sever_group_obj.revert(sg_id)",
            "        mock_driver.delete_server_group.assert_called_once_with(sg_id)",
            "",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_nova_server_group_delete_with_sever_group_id(self, mock_driver):",
            "        nova_sever_group_obj = compute_tasks.NovaServerGroupDelete()",
            "        sg_id = '6789'",
            "        nova_sever_group_obj.execute(sg_id)",
            "        mock_driver.delete_server_group.assert_called_once_with(sg_id)",
            "",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_nova_server_group_delete_with_None(self, mock_driver):",
            "        nova_sever_group_obj = compute_tasks.NovaServerGroupDelete()",
            "        sg_id = None",
            "        nova_sever_group_obj.execute(sg_id)",
            "        self.assertFalse(mock_driver.delete_server_group.called, sg_id)"
        ],
        "afterPatchFile": [
            "# Copyright 2015 Hewlett-Packard Development Company, L.P.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "# http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "",
            "from cryptography import fernet",
            "import mock",
            "from oslo_config import cfg",
            "from oslo_config import fixture as oslo_fixture",
            "from oslo_utils import uuidutils",
            "",
            "from octavia.common import constants",
            "from octavia.common import exceptions",
            "from octavia.common import utils",
            "from octavia.controller.worker.tasks import compute_tasks",
            "from octavia.tests.common import utils as test_utils",
            "import octavia.tests.unit.base as base",
            "",
            "",
            "AMP_FLAVOR_ID = '10'",
            "AMP_IMAGE_ID = '11'",
            "AMP_IMAGE_TAG = 'glance_tag'",
            "AMP_SSH_KEY_NAME = None",
            "AMP_NET = [uuidutils.generate_uuid()]",
            "AMP_SEC_GROUPS = []",
            "AMP_WAIT = 12",
            "AMPHORA_ID = uuidutils.generate_uuid()",
            "COMPUTE_ID = uuidutils.generate_uuid()",
            "LB_NET_IP = '192.0.2.1'",
            "PORT_ID = uuidutils.generate_uuid()",
            "SERVER_GRPOUP_ID = uuidutils.generate_uuid()",
            "",
            "",
            "class TestException(Exception):",
            "",
            "    def __init__(self, value):",
            "        self.value = value",
            "",
            "    def __str__(self):",
            "        return repr(self.value)",
            "",
            "_amphora_mock = mock.MagicMock()",
            "_amphora_mock.id = AMPHORA_ID",
            "_amphora_mock.compute_id = COMPUTE_ID",
            "_load_balancer_mock = mock.MagicMock()",
            "_load_balancer_mock.amphorae = [_amphora_mock]",
            "_port = mock.MagicMock()",
            "_port.id = PORT_ID",
            "",
            "",
            "class TestComputeTasks(base.TestCase):",
            "",
            "    def setUp(self):",
            "        self.conf = self.useFixture(oslo_fixture.Config(cfg.CONF))",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_flavor_id=AMP_FLAVOR_ID)",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_image_id=AMP_IMAGE_ID)",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_image_tag=AMP_IMAGE_TAG)",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_ssh_key_name=AMP_SSH_KEY_NAME)",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_boot_network_list=AMP_NET)",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_active_wait_sec=AMP_WAIT)",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_secgroup_list=AMP_SEC_GROUPS)",
            "        self.conf.config(group=\"controller_worker\", amp_image_owner_id='')",
            "",
            "        _amphora_mock.id = AMPHORA_ID",
            "        _amphora_mock.status = constants.AMPHORA_ALLOCATED",
            "",
            "        logging_mock = mock.MagicMock()",
            "        compute_tasks.LOG = logging_mock",
            "",
            "        super(TestComputeTasks, self).setUp()",
            "",
            "    @mock.patch('jinja2.Environment.get_template')",
            "    @mock.patch('octavia.amphorae.backends.agent.'",
            "                'agent_jinja_cfg.AgentJinjaTemplater.'",
            "                'build_agent_config', return_value='test_conf')",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_compute_create(self, mock_driver, mock_conf, mock_jinja):",
            "",
            "        image_owner_id = uuidutils.generate_uuid()",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_image_owner_id=image_owner_id)",
            "",
            "        createcompute = compute_tasks.ComputeCreate()",
            "",
            "        mock_driver.build.return_value = COMPUTE_ID",
            "        # Test execute()",
            "        compute_id = createcompute.execute(_amphora_mock.id, ports=[_port],",
            "                                           server_group_id=SERVER_GRPOUP_ID)",
            "",
            "        # Validate that the build method was called properly",
            "        mock_driver.build.assert_called_once_with(",
            "            name=\"amphora-\" + _amphora_mock.id,",
            "            amphora_flavor=AMP_FLAVOR_ID,",
            "            image_id=AMP_IMAGE_ID,",
            "            image_tag=AMP_IMAGE_TAG,",
            "            image_owner=image_owner_id,",
            "            key_name=AMP_SSH_KEY_NAME,",
            "            sec_groups=AMP_SEC_GROUPS,",
            "            network_ids=AMP_NET,",
            "            port_ids=[PORT_ID],",
            "            config_drive_files={'/etc/octavia/'",
            "                                'amphora-agent.conf': 'test_conf'},",
            "            user_data=None,",
            "            server_group_id=SERVER_GRPOUP_ID)",
            "",
            "        # Make sure it returns the expected compute_id",
            "        self.assertEqual(COMPUTE_ID, compute_id)",
            "",
            "        # Test that a build exception is raised",
            "        createcompute = compute_tasks.ComputeCreate()",
            "",
            "        self.assertRaises(TypeError,",
            "                          createcompute.execute,",
            "                          _amphora_mock, config_drive_files='test_cert')",
            "",
            "        # Test revert()",
            "",
            "        _amphora_mock.compute_id = COMPUTE_ID",
            "",
            "        createcompute = compute_tasks.ComputeCreate()",
            "        createcompute.revert(compute_id, _amphora_mock.id)",
            "",
            "        # Validate that the delete method was called properly",
            "        mock_driver.delete.assert_called_once_with(",
            "            COMPUTE_ID)",
            "",
            "        # Test that a delete exception is not raised",
            "",
            "        createcompute.revert(COMPUTE_ID, _amphora_mock.id)",
            "",
            "    @mock.patch('jinja2.Environment.get_template')",
            "    @mock.patch('octavia.amphorae.backends.agent.'",
            "                'agent_jinja_cfg.AgentJinjaTemplater.'",
            "                'build_agent_config', return_value='test_conf')",
            "    @mock.patch('octavia.common.jinja.'",
            "                'user_data_jinja_cfg.UserDataJinjaCfg.'",
            "                'build_user_data_config', return_value='test_conf')",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_compute_create_user_data(self, mock_driver,",
            "                                      mock_ud_conf, mock_conf, mock_jinja):",
            "",
            "        self.conf.config(",
            "            group=\"controller_worker\", user_data_config_drive=True)",
            "        mock_ud_conf.return_value = 'test_ud_conf'",
            "        createcompute = compute_tasks.ComputeCreate()",
            "",
            "        mock_driver.build.return_value = COMPUTE_ID",
            "        # Test execute()",
            "        compute_id = createcompute.execute(_amphora_mock.id, ports=[_port])",
            "",
            "        # Validate that the build method was called properly",
            "        mock_driver.build.assert_called_once_with(",
            "            name=\"amphora-\" + _amphora_mock.id,",
            "            amphora_flavor=AMP_FLAVOR_ID,",
            "            image_id=AMP_IMAGE_ID,",
            "            image_tag=AMP_IMAGE_TAG,",
            "            image_owner='',",
            "            key_name=AMP_SSH_KEY_NAME,",
            "            sec_groups=AMP_SEC_GROUPS,",
            "            network_ids=AMP_NET,",
            "            port_ids=[PORT_ID],",
            "            config_drive_files=None,",
            "            user_data='test_ud_conf',",
            "            server_group_id=None)",
            "",
            "        # Make sure it returns the expected compute_id",
            "        self.assertEqual(COMPUTE_ID, compute_id)",
            "",
            "        # Test that a build exception is raised",
            "        createcompute = compute_tasks.ComputeCreate()",
            "",
            "        self.assertRaises(TypeError,",
            "                          createcompute.execute,",
            "                          _amphora_mock, config_drive_files='test_cert')",
            "",
            "        # Test revert()",
            "",
            "        _amphora_mock.compute_id = COMPUTE_ID",
            "",
            "        createcompute = compute_tasks.ComputeCreate()",
            "        createcompute.revert(compute_id, _amphora_mock.id)",
            "",
            "        # Validate that the delete method was called properly",
            "        mock_driver.delete.assert_called_once_with(",
            "            COMPUTE_ID)",
            "",
            "        # Test that a delete exception is not raised",
            "",
            "        createcompute.revert(COMPUTE_ID, _amphora_mock.id)",
            "",
            "    @mock.patch('jinja2.Environment.get_template')",
            "    @mock.patch('octavia.amphorae.backends.agent.'",
            "                'agent_jinja_cfg.AgentJinjaTemplater.'",
            "                'build_agent_config', return_value='test_conf')",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_compute_create_without_ssh_access(self, mock_driver,",
            "                                               mock_conf, mock_jinja):",
            "",
            "        createcompute = compute_tasks.ComputeCreate()",
            "",
            "        mock_driver.build.return_value = COMPUTE_ID",
            "        self.conf.config(",
            "            group=\"controller_worker\", amp_ssh_access_allowed=False)",
            "        self.conf.config(",
            "            group=\"controller_worker\", user_data_config_drive=False)",
            "",
            "        # Test execute()",
            "        compute_id = createcompute.execute(_amphora_mock.id, ports=[_port],",
            "                                           server_group_id=SERVER_GRPOUP_ID)",
            "",
            "        # Validate that the build method was called properly",
            "        mock_driver.build.assert_called_once_with(",
            "            name=\"amphora-\" + _amphora_mock.id,",
            "            amphora_flavor=AMP_FLAVOR_ID,",
            "            image_id=AMP_IMAGE_ID,",
            "            image_tag=AMP_IMAGE_TAG,",
            "            image_owner='',",
            "            key_name=None,",
            "            sec_groups=AMP_SEC_GROUPS,",
            "            network_ids=AMP_NET,",
            "            port_ids=[PORT_ID],",
            "            config_drive_files={'/etc/octavia/'",
            "                                'amphora-agent.conf': 'test_conf'},",
            "            user_data=None,",
            "            server_group_id=SERVER_GRPOUP_ID)",
            "",
            "        self.assertEqual(COMPUTE_ID, compute_id)",
            "",
            "        # Test that a build exception is raised",
            "        createcompute = compute_tasks.ComputeCreate()",
            "",
            "        self.assertRaises(TypeError,",
            "                          createcompute.execute,",
            "                          _amphora_mock, config_drive_files='test_cert')",
            "",
            "        # Test revert()",
            "",
            "        _amphora_mock.compute_id = COMPUTE_ID",
            "",
            "        createcompute = compute_tasks.ComputeCreate()",
            "        createcompute.revert(compute_id, _amphora_mock.id)",
            "",
            "        # Validate that the delete method was called properly",
            "        mock_driver.delete.assert_called_once_with(",
            "            COMPUTE_ID)",
            "",
            "        # Test that a delete exception is not raised",
            "",
            "        createcompute.revert(COMPUTE_ID, _amphora_mock.id)",
            "",
            "    @mock.patch('jinja2.Environment.get_template')",
            "    @mock.patch('octavia.amphorae.backends.agent.'",
            "                'agent_jinja_cfg.AgentJinjaTemplater.'",
            "                'build_agent_config', return_value='test_conf')",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_compute_create_cert(self, mock_driver, mock_conf, mock_jinja):",
            "        createcompute = compute_tasks.CertComputeCreate()",
            "        key = utils.get_six_compatible_server_certs_key_passphrase()",
            "        fer = fernet.Fernet(key)",
            "",
            "        mock_driver.build.return_value = COMPUTE_ID",
            "        path = '/etc/octavia/certs/ca_01.pem'",
            "        self.useFixture(test_utils.OpenFixture(path, 'test'))",
            "        # Test execute()",
            "        test_cert = fer.encrypt(",
            "            utils.get_six_compatible_value('test_cert')",
            "        )",
            "        compute_id = createcompute.execute(_amphora_mock.id, test_cert,",
            "                                           server_group_id=SERVER_GRPOUP_ID",
            "                                           )",
            "",
            "        # Validate that the build method was called properly",
            "        mock_driver.build.assert_called_once_with(",
            "            name=\"amphora-\" + _amphora_mock.id,",
            "            amphora_flavor=AMP_FLAVOR_ID,",
            "            image_id=AMP_IMAGE_ID,",
            "            image_tag=AMP_IMAGE_TAG,",
            "            image_owner='',",
            "            key_name=AMP_SSH_KEY_NAME,",
            "            sec_groups=AMP_SEC_GROUPS,",
            "            network_ids=AMP_NET,",
            "            port_ids=[],",
            "            user_data=None,",
            "            config_drive_files={",
            "                '/etc/octavia/certs/server.pem': fer.decrypt(test_cert),",
            "                '/etc/octavia/certs/client_ca.pem': 'test',",
            "                '/etc/octavia/amphora-agent.conf': 'test_conf'},",
            "            server_group_id=SERVER_GRPOUP_ID)",
            "",
            "        self.assertEqual(COMPUTE_ID, compute_id)",
            "",
            "        # Test that a build exception is raised",
            "        self.useFixture(test_utils.OpenFixture(path, 'test'))",
            "",
            "        createcompute = compute_tasks.ComputeCreate()",
            "        self.assertRaises(TypeError,",
            "                          createcompute.execute,",
            "                          _amphora_mock,",
            "                          config_drive_files=test_cert)",
            "",
            "        # Test revert()",
            "",
            "        _amphora_mock.compute_id = COMPUTE_ID",
            "",
            "        createcompute = compute_tasks.ComputeCreate()",
            "        createcompute.revert(compute_id, _amphora_mock.id)",
            "",
            "        # Validate that the delete method was called properly",
            "        mock_driver.delete.assert_called_once_with(COMPUTE_ID)",
            "",
            "        # Test that a delete exception is not raised",
            "",
            "        createcompute.revert(COMPUTE_ID, _amphora_mock.id)",
            "",
            "    @mock.patch('octavia.controller.worker.amphora_rate_limit'",
            "                '.AmphoraBuildRateLimit.remove_from_build_req_queue')",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    @mock.patch('time.sleep')",
            "    def test_compute_wait(self,",
            "                          mock_time_sleep,",
            "                          mock_driver,",
            "                          mock_remove_from_build_queue):",
            "",
            "        self.conf.config(group='haproxy_amphora', build_rate_limit=5)",
            "        _amphora_mock.compute_id = COMPUTE_ID",
            "        _amphora_mock.status = constants.ACTIVE",
            "        _amphora_mock.lb_network_ip = LB_NET_IP",
            "",
            "        mock_driver.get_amphora.return_value = _amphora_mock, None",
            "",
            "        computewait = compute_tasks.ComputeActiveWait()",
            "        computewait.execute(COMPUTE_ID, AMPHORA_ID)",
            "",
            "        mock_driver.get_amphora.assert_called_once_with(COMPUTE_ID)",
            "",
            "        _amphora_mock.status = constants.DELETED",
            "",
            "        self.assertRaises(exceptions.ComputeWaitTimeoutException,",
            "                          computewait.execute,",
            "                          _amphora_mock, AMPHORA_ID)",
            "",
            "    @mock.patch('octavia.controller.worker.amphora_rate_limit'",
            "                '.AmphoraBuildRateLimit.remove_from_build_req_queue')",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    @mock.patch('time.sleep')",
            "    def test_compute_wait_error_status(self,",
            "                                       mock_time_sleep,",
            "                                       mock_driver,",
            "                                       mock_remove_from_build_queue):",
            "",
            "        self.conf.config(group='haproxy_amphora', build_rate_limit=5)",
            "        _amphora_mock.compute_id = COMPUTE_ID",
            "        _amphora_mock.status = constants.ACTIVE",
            "        _amphora_mock.lb_network_ip = LB_NET_IP",
            "",
            "        mock_driver.get_amphora.return_value = _amphora_mock, None",
            "",
            "        computewait = compute_tasks.ComputeActiveWait()",
            "        computewait.execute(COMPUTE_ID, AMPHORA_ID)",
            "",
            "        mock_driver.get_amphora.assert_called_once_with(COMPUTE_ID)",
            "",
            "        _amphora_mock.status = constants.ERROR",
            "",
            "        self.assertRaises(exceptions.ComputeBuildException,",
            "                          computewait.execute,",
            "                          _amphora_mock, AMPHORA_ID)",
            "",
            "    @mock.patch('octavia.controller.worker.amphora_rate_limit'",
            "                '.AmphoraBuildRateLimit.remove_from_build_req_queue')",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    @mock.patch('time.sleep')",
            "    def test_compute_wait_skipped(self,",
            "                                  mock_time_sleep,",
            "                                  mock_driver,",
            "                                  mock_remove_from_build_queue):",
            "        _amphora_mock.compute_id = COMPUTE_ID",
            "        _amphora_mock.status = constants.ACTIVE",
            "        _amphora_mock.lb_network_ip = LB_NET_IP",
            "",
            "        mock_driver.get_amphora.return_value = _amphora_mock, None",
            "",
            "        computewait = compute_tasks.ComputeActiveWait()",
            "        computewait.execute(COMPUTE_ID, AMPHORA_ID)",
            "",
            "        mock_driver.get_amphora.assert_called_once_with(COMPUTE_ID)",
            "        mock_remove_from_build_queue.assert_not_called()",
            "",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_delete_amphorae_on_load_balancer(self, mock_driver):",
            "",
            "        delete_amps = compute_tasks.DeleteAmphoraeOnLoadBalancer()",
            "        delete_amps.execute(_load_balancer_mock)",
            "",
            "        mock_driver.delete.assert_called_once_with(COMPUTE_ID)",
            "",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_compute_delete(self, mock_driver):",
            "",
            "        delete_compute = compute_tasks.ComputeDelete()",
            "        delete_compute.execute(_amphora_mock)",
            "",
            "        mock_driver.delete.assert_called_once_with(COMPUTE_ID)",
            "",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_nova_server_group_create(self, mock_driver):",
            "        nova_sever_group_obj = compute_tasks.NovaServerGroupCreate()",
            "",
            "        server_group_test_id = '6789'",
            "        fake_server_group = mock.MagicMock()",
            "        fake_server_group.id = server_group_test_id",
            "        fake_server_group.policy = 'anti-affinity'",
            "        mock_driver.create_server_group.return_value = fake_server_group",
            "",
            "        # Test execute()",
            "        sg_id = nova_sever_group_obj.execute('123')",
            "",
            "        # Validate that the build method was called properly",
            "        mock_driver.create_server_group.assert_called_once_with(",
            "            'octavia-lb-123', 'anti-affinity')",
            "",
            "        # Make sure it returns the expected server group_id",
            "        self.assertEqual(server_group_test_id, sg_id)",
            "",
            "        # Test revert()",
            "        nova_sever_group_obj.revert(sg_id)",
            "",
            "        # Validate that the delete_server_group method was called properly",
            "        mock_driver.delete_server_group.assert_called_once_with(sg_id)",
            "",
            "        # Test revert with exception",
            "        mock_driver.reset_mock()",
            "        mock_driver.delete_server_group.side_effect = Exception('DelSGExcept')",
            "        nova_sever_group_obj.revert(sg_id)",
            "        mock_driver.delete_server_group.assert_called_once_with(sg_id)",
            "",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_nova_server_group_delete_with_sever_group_id(self, mock_driver):",
            "        nova_sever_group_obj = compute_tasks.NovaServerGroupDelete()",
            "        sg_id = '6789'",
            "        nova_sever_group_obj.execute(sg_id)",
            "        mock_driver.delete_server_group.assert_called_once_with(sg_id)",
            "",
            "    @mock.patch('stevedore.driver.DriverManager.driver')",
            "    def test_nova_server_group_delete_with_None(self, mock_driver):",
            "        nova_sever_group_obj = compute_tasks.NovaServerGroupDelete()",
            "        sg_id = None",
            "        nova_sever_group_obj.execute(sg_id)",
            "        self.assertFalse(mock_driver.delete_server_group.called, sg_id)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "277": [
                "TestComputeTasks",
                "test_compute_create_cert"
            ],
            "279": [
                "TestComputeTasks",
                "test_compute_create_cert"
            ],
            "296": [
                "TestComputeTasks",
                "test_compute_create_cert"
            ],
            "310": [
                "TestComputeTasks",
                "test_compute_create_cert"
            ]
        },
        "addLocation": []
    }
}