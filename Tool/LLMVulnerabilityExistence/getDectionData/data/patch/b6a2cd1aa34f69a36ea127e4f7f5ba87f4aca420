{
    "airflow/cli/cli_parser.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 667,
                "afterPatchRowNumber": 667,
                "PatchRowcode": " ARG_UMASK = Arg("
            },
            "1": {
                "beforePatchRowNumber": 668,
                "afterPatchRowNumber": 668,
                "PatchRowcode": "     (\"-u\", \"--umask\"),"
            },
            "2": {
                "beforePatchRowNumber": 669,
                "afterPatchRowNumber": 669,
                "PatchRowcode": "     help=\"Set the umask of celery worker in daemon mode\","
            },
            "3": {
                "beforePatchRowNumber": 670,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    default=conf.get('celery', 'worker_umask'),"
            },
            "4": {
                "beforePatchRowNumber": 671,
                "afterPatchRowNumber": 670,
                "PatchRowcode": " )"
            },
            "5": {
                "beforePatchRowNumber": 672,
                "afterPatchRowNumber": 671,
                "PatchRowcode": " ARG_WITHOUT_MINGLE = Arg("
            },
            "6": {
                "beforePatchRowNumber": 673,
                "afterPatchRowNumber": 672,
                "PatchRowcode": "     (\"--without-mingle\",),"
            }
        },
        "frontPatchFile": [
            "#!/usr/bin/env python",
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"Command-line interface\"\"\"",
            "",
            "import argparse",
            "import json",
            "import os",
            "import textwrap",
            "from argparse import Action, ArgumentError, RawTextHelpFormatter",
            "from functools import lru_cache",
            "from typing import Callable, Dict, Iterable, List, NamedTuple, Optional, Union",
            "",
            "import lazy_object_proxy",
            "",
            "from airflow import settings",
            "from airflow.cli.commands.legacy_commands import check_legacy_command",
            "from airflow.configuration import conf",
            "from airflow.exceptions import AirflowException",
            "from airflow.executors.executor_constants import CELERY_EXECUTOR, CELERY_KUBERNETES_EXECUTOR",
            "from airflow.executors.executor_loader import ExecutorLoader",
            "from airflow.utils.cli import ColorMode",
            "from airflow.utils.helpers import partition",
            "from airflow.utils.module_loading import import_string",
            "from airflow.utils.timezone import parse as parsedate",
            "",
            "BUILD_DOCS = \"BUILDING_AIRFLOW_DOCS\" in os.environ",
            "",
            "",
            "def lazy_load_command(import_path: str) -> Callable:",
            "    \"\"\"Create a lazy loader for command\"\"\"",
            "    _, _, name = import_path.rpartition('.')",
            "",
            "    def command(*args, **kwargs):",
            "        func = import_string(import_path)",
            "        return func(*args, **kwargs)",
            "",
            "    command.__name__ = name",
            "",
            "    return command",
            "",
            "",
            "class DefaultHelpParser(argparse.ArgumentParser):",
            "    \"\"\"CustomParser to display help message\"\"\"",
            "",
            "    def _check_value(self, action, value):",
            "        \"\"\"Override _check_value and check conditionally added command\"\"\"",
            "        if action.dest == 'subcommand' and value == 'celery':",
            "            executor = conf.get('core', 'EXECUTOR')",
            "            if executor not in (CELERY_EXECUTOR, CELERY_KUBERNETES_EXECUTOR):",
            "                executor_cls, _ = ExecutorLoader.import_executor_cls(executor)",
            "                classes = ()",
            "                try:",
            "                    from airflow.executors.celery_executor import CeleryExecutor",
            "",
            "                    classes += (CeleryExecutor,)",
            "                except ImportError:",
            "                    message = (",
            "                        \"The celery subcommand requires that you pip install the celery module. \"",
            "                        \"To do it, run: pip install 'apache-airflow[celery]'\"",
            "                    )",
            "                    raise ArgumentError(action, message)",
            "                try:",
            "                    from airflow.executors.celery_kubernetes_executor import CeleryKubernetesExecutor",
            "",
            "                    classes += (CeleryKubernetesExecutor,)",
            "                except ImportError:",
            "                    pass",
            "                if not issubclass(executor_cls, classes):",
            "                    message = (",
            "                        f'celery subcommand works only with CeleryExecutor, CeleryKubernetesExecutor and '",
            "                        f'executors derived from them, your current executor: {executor}, subclassed from: '",
            "                        f'{\", \".join([base_cls.__qualname__ for base_cls in executor_cls.__bases__])}'",
            "                    )",
            "                    raise ArgumentError(action, message)",
            "        if action.dest == 'subcommand' and value == 'kubernetes':",
            "            try:",
            "                import kubernetes.client  # noqa: F401",
            "            except ImportError:",
            "                message = (",
            "                    \"The kubernetes subcommand requires that you pip install the kubernetes python client. \"",
            "                    \"To do it, run: pip install 'apache-airflow[cncf.kubernetes]'\"",
            "                )",
            "                raise ArgumentError(action, message)",
            "",
            "        if action.choices is not None and value not in action.choices:",
            "            check_legacy_command(action, value)",
            "",
            "        super()._check_value(action, value)",
            "",
            "    def error(self, message):",
            "        \"\"\"Override error and use print_instead of print_usage\"\"\"",
            "        self.print_help()",
            "        self.exit(2, f'\\n{self.prog} command error: {message}, see help above.\\n')",
            "",
            "",
            "# Used in Arg to enable `None' as a distinct value from \"not passed\"",
            "_UNSET = object()",
            "",
            "",
            "class Arg:",
            "    \"\"\"Class to keep information about command line argument\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        flags=_UNSET,",
            "        help=_UNSET,",
            "        action=_UNSET,",
            "        default=_UNSET,",
            "        nargs=_UNSET,",
            "        type=_UNSET,",
            "        choices=_UNSET,",
            "        required=_UNSET,",
            "        metavar=_UNSET,",
            "        dest=_UNSET,",
            "    ):",
            "        self.flags = flags",
            "        self.kwargs = {}",
            "        for k, v in locals().items():",
            "            if v is _UNSET:",
            "                continue",
            "            if k in (\"self\", \"flags\"):",
            "                continue",
            "",
            "            self.kwargs[k] = v",
            "",
            "    def add_to_parser(self, parser: argparse.ArgumentParser):",
            "        \"\"\"Add this argument to an ArgumentParser\"\"\"",
            "        parser.add_argument(*self.flags, **self.kwargs)",
            "",
            "",
            "def positive_int(*, allow_zero):",
            "    \"\"\"Define a positive int type for an argument.\"\"\"",
            "",
            "    def _check(value):",
            "        try:",
            "            value = int(value)",
            "            if allow_zero and value == 0:",
            "                return value",
            "            if value > 0:",
            "                return value",
            "        except ValueError:",
            "            pass",
            "        raise argparse.ArgumentTypeError(f\"invalid positive int value: '{value}'\")",
            "",
            "    return _check",
            "",
            "",
            "def string_list_type(val):",
            "    \"\"\"Parses comma-separated list and returns list of string (strips whitespace)\"\"\"",
            "    return [x.strip() for x in val.split(',')]",
            "",
            "",
            "def string_lower_type(val):",
            "    \"\"\"Lowers arg\"\"\"",
            "    if not val:",
            "        return",
            "    return val.strip().lower()",
            "",
            "",
            "# Shared",
            "ARG_DAG_ID = Arg((\"dag_id\",), help=\"The id of the dag\")",
            "ARG_TASK_ID = Arg((\"task_id\",), help=\"The id of the task\")",
            "ARG_EXECUTION_DATE = Arg((\"execution_date\",), help=\"The execution date of the DAG\", type=parsedate)",
            "ARG_EXECUTION_DATE_OR_RUN_ID = Arg(",
            "    ('execution_date_or_run_id',), help=\"The execution_date of the DAG or run_id of the DAGRun\"",
            ")",
            "ARG_TASK_REGEX = Arg(",
            "    (\"-t\", \"--task-regex\"), help=\"The regex to filter specific task_ids to backfill (optional)\"",
            ")",
            "ARG_SUBDIR = Arg(",
            "    (\"-S\", \"--subdir\"),",
            "    help=(",
            "        \"File location or directory from which to look for the dag. \"",
            "        \"Defaults to '[AIRFLOW_HOME]/dags' where [AIRFLOW_HOME] is the \"",
            "        \"value you set for 'AIRFLOW_HOME' config you set in 'airflow.cfg' \"",
            "    ),",
            "    default='[AIRFLOW_HOME]/dags' if BUILD_DOCS else settings.DAGS_FOLDER,",
            ")",
            "ARG_START_DATE = Arg((\"-s\", \"--start-date\"), help=\"Override start_date YYYY-MM-DD\", type=parsedate)",
            "ARG_END_DATE = Arg((\"-e\", \"--end-date\"), help=\"Override end_date YYYY-MM-DD\", type=parsedate)",
            "ARG_OUTPUT_PATH = Arg(",
            "    (",
            "        \"-o\",",
            "        \"--output-path\",",
            "    ),",
            "    help=\"The output for generated yaml files\",",
            "    type=str,",
            "    default=\"[CWD]\" if BUILD_DOCS else os.getcwd(),",
            ")",
            "ARG_DRY_RUN = Arg(",
            "    (\"-n\", \"--dry-run\"),",
            "    help=\"Perform a dry run for each task. Only renders Template Fields for each task, nothing else\",",
            "    action=\"store_true\",",
            ")",
            "ARG_PID = Arg((\"--pid\",), help=\"PID file location\", nargs='?')",
            "ARG_DAEMON = Arg(",
            "    (\"-D\", \"--daemon\"), help=\"Daemonize instead of running in the foreground\", action=\"store_true\"",
            ")",
            "ARG_STDERR = Arg((\"--stderr\",), help=\"Redirect stderr to this file\")",
            "ARG_STDOUT = Arg((\"--stdout\",), help=\"Redirect stdout to this file\")",
            "ARG_LOG_FILE = Arg((\"-l\", \"--log-file\"), help=\"Location of the log file\")",
            "ARG_YES = Arg(",
            "    (\"-y\", \"--yes\"),",
            "    help=\"Do not prompt to confirm. Use with care!\",",
            "    action=\"store_true\",",
            "    default=False,",
            ")",
            "ARG_OUTPUT = Arg(",
            "    (",
            "        \"-o\",",
            "        \"--output\",",
            "    ),",
            "    help=\"Output format. Allowed values: json, yaml, plain, table (default: table)\",",
            "    metavar=\"(table, json, yaml, plain)\",",
            "    choices=(\"table\", \"json\", \"yaml\", \"plain\"),",
            "    default=\"table\",",
            ")",
            "ARG_COLOR = Arg(",
            "    ('--color',),",
            "    help=\"Do emit colored output (default: auto)\",",
            "    choices={ColorMode.ON, ColorMode.OFF, ColorMode.AUTO},",
            "    default=ColorMode.AUTO,",
            ")",
            "",
            "# DB args",
            "ARG_VERSION_RANGE = Arg(",
            "    (\"-r\", \"--range\"),",
            "    help=\"Version range(start:end) for offline sql generation. Example: '2.0.2:2.2.3'\",",
            "    default=None,",
            ")",
            "ARG_REVISION_RANGE = Arg(",
            "    ('--revision-range',),",
            "    help=(",
            "        \"Migration revision range(start:end) to use for offline sql generation. \"",
            "        \"Example: ``a13f7613ad25:7b2661a43ba3``\"",
            "    ),",
            "    default=None,",
            ")",
            "",
            "# list_dag_runs",
            "ARG_DAG_ID_OPT = Arg((\"-d\", \"--dag-id\"), help=\"The id of the dag\")",
            "ARG_NO_BACKFILL = Arg(",
            "    (\"--no-backfill\",), help=\"filter all the backfill dagruns given the dag id\", action=\"store_true\"",
            ")",
            "ARG_STATE = Arg((\"--state\",), help=\"Only list the dag runs corresponding to the state\")",
            "",
            "# list_jobs",
            "ARG_LIMIT = Arg((\"--limit\",), help=\"Return a limited number of records\")",
            "",
            "# next_execution",
            "ARG_NUM_EXECUTIONS = Arg(",
            "    (\"-n\", \"--num-executions\"),",
            "    default=1,",
            "    type=positive_int(allow_zero=False),",
            "    help=\"The number of next execution datetimes to show\",",
            ")",
            "",
            "# backfill",
            "ARG_MARK_SUCCESS = Arg(",
            "    (\"-m\", \"--mark-success\"), help=\"Mark jobs as succeeded without running them\", action=\"store_true\"",
            ")",
            "ARG_VERBOSE = Arg((\"-v\", \"--verbose\"), help=\"Make logging output more verbose\", action=\"store_true\")",
            "ARG_LOCAL = Arg((\"-l\", \"--local\"), help=\"Run the task using the LocalExecutor\", action=\"store_true\")",
            "ARG_DONOT_PICKLE = Arg(",
            "    (\"-x\", \"--donot-pickle\"),",
            "    help=(",
            "        \"Do not attempt to pickle the DAG object to send over \"",
            "        \"to the workers, just tell the workers to run their version \"",
            "        \"of the code\"",
            "    ),",
            "    action=\"store_true\",",
            ")",
            "ARG_BF_IGNORE_DEPENDENCIES = Arg(",
            "    (\"-i\", \"--ignore-dependencies\"),",
            "    help=(",
            "        \"Skip upstream tasks, run only the tasks \"",
            "        \"matching the regexp. Only works in conjunction \"",
            "        \"with task_regex\"",
            "    ),",
            "    action=\"store_true\",",
            ")",
            "ARG_BF_IGNORE_FIRST_DEPENDS_ON_PAST = Arg(",
            "    (\"-I\", \"--ignore-first-depends-on-past\"),",
            "    help=(",
            "        \"Ignores depends_on_past dependencies for the first \"",
            "        \"set of tasks only (subsequent executions in the backfill \"",
            "        \"DO respect depends_on_past)\"",
            "    ),",
            "    action=\"store_true\",",
            ")",
            "ARG_POOL = Arg((\"--pool\",), \"Resource pool to use\")",
            "ARG_DELAY_ON_LIMIT = Arg(",
            "    (\"--delay-on-limit\",),",
            "    help=(",
            "        \"Amount of time in seconds to wait when the limit \"",
            "        \"on maximum active dag runs (max_active_runs) has \"",
            "        \"been reached before trying to execute a dag run \"",
            "        \"again\"",
            "    ),",
            "    type=float,",
            "    default=1.0,",
            ")",
            "ARG_RESET_DAG_RUN = Arg(",
            "    (\"--reset-dagruns\",),",
            "    help=(",
            "        \"if set, the backfill will delete existing \"",
            "        \"backfill-related DAG runs and start \"",
            "        \"anew with fresh, running DAG runs\"",
            "    ),",
            "    action=\"store_true\",",
            ")",
            "ARG_RERUN_FAILED_TASKS = Arg(",
            "    (\"--rerun-failed-tasks\",),",
            "    help=(",
            "        \"if set, the backfill will auto-rerun \"",
            "        \"all the failed tasks for the backfill date range \"",
            "        \"instead of throwing exceptions\"",
            "    ),",
            "    action=\"store_true\",",
            ")",
            "ARG_CONTINUE_ON_FAILURES = Arg(",
            "    (\"--continue-on-failures\",),",
            "    help=(\"if set, the backfill will keep going even if some of the tasks failed\"),",
            "    action=\"store_true\",",
            ")",
            "ARG_RUN_BACKWARDS = Arg(",
            "    (",
            "        \"-B\",",
            "        \"--run-backwards\",",
            "    ),",
            "    help=(",
            "        \"if set, the backfill will run tasks from the most \"",
            "        \"recent day first.  if there are tasks that depend_on_past \"",
            "        \"this option will throw an exception\"",
            "    ),",
            "    action=\"store_true\",",
            ")",
            "# test_dag",
            "ARG_SHOW_DAGRUN = Arg(",
            "    (\"--show-dagrun\",),",
            "    help=(",
            "        \"After completing the backfill, shows the diagram for current DAG Run.\\n\"",
            "        \"\\n\"",
            "        \"The diagram is in DOT language\\n\"",
            "    ),",
            "    action='store_true',",
            ")",
            "ARG_IMGCAT_DAGRUN = Arg(",
            "    (\"--imgcat-dagrun\",),",
            "    help=(",
            "        \"After completing the dag run, prints a diagram on the screen for the \"",
            "        \"current DAG Run using the imgcat tool.\\n\"",
            "    ),",
            "    action='store_true',",
            ")",
            "ARG_SAVE_DAGRUN = Arg(",
            "    (\"--save-dagrun\",),",
            "    help=\"After completing the backfill, saves the diagram for current DAG Run to the indicated file.\\n\\n\",",
            ")",
            "",
            "# list_tasks",
            "ARG_TREE = Arg((\"-t\", \"--tree\"), help=\"Tree view\", action=\"store_true\")",
            "",
            "# tasks_run",
            "# This is a hidden option -- not meant for users to set or know about",
            "ARG_SHUT_DOWN_LOGGING = Arg(",
            "    (\"--no-shut-down-logging\",),",
            "    help=argparse.SUPPRESS,",
            "    dest=\"shut_down_logging\",",
            "    action=\"store_false\",",
            "    default=True,",
            ")",
            "",
            "# clear",
            "ARG_UPSTREAM = Arg((\"-u\", \"--upstream\"), help=\"Include upstream tasks\", action=\"store_true\")",
            "ARG_ONLY_FAILED = Arg((\"-f\", \"--only-failed\"), help=\"Only failed jobs\", action=\"store_true\")",
            "ARG_ONLY_RUNNING = Arg((\"-r\", \"--only-running\"), help=\"Only running jobs\", action=\"store_true\")",
            "ARG_DOWNSTREAM = Arg((\"-d\", \"--downstream\"), help=\"Include downstream tasks\", action=\"store_true\")",
            "ARG_EXCLUDE_SUBDAGS = Arg((\"-x\", \"--exclude-subdags\"), help=\"Exclude subdags\", action=\"store_true\")",
            "ARG_EXCLUDE_PARENTDAG = Arg(",
            "    (\"-X\", \"--exclude-parentdag\"),",
            "    help=\"Exclude ParentDAGS if the task cleared is a part of a SubDAG\",",
            "    action=\"store_true\",",
            ")",
            "ARG_DAG_REGEX = Arg(",
            "    (\"-R\", \"--dag-regex\"), help=\"Search dag_id as regex instead of exact string\", action=\"store_true\"",
            ")",
            "",
            "# show_dag",
            "ARG_SAVE = Arg((\"-s\", \"--save\"), help=\"Saves the result to the indicated file.\")",
            "",
            "ARG_IMGCAT = Arg((\"--imgcat\",), help=\"Displays graph using the imgcat tool.\", action='store_true')",
            "",
            "# trigger_dag",
            "ARG_RUN_ID = Arg((\"-r\", \"--run-id\"), help=\"Helps to identify this run\")",
            "ARG_CONF = Arg(('-c', '--conf'), help=\"JSON string that gets pickled into the DagRun's conf attribute\")",
            "ARG_EXEC_DATE = Arg((\"-e\", \"--exec-date\"), help=\"The execution date of the DAG\", type=parsedate)",
            "",
            "# db",
            "ARG_DB_TABLES = Arg(",
            "    (\"-t\", \"--tables\"),",
            "    help=lazy_object_proxy.Proxy(",
            "        lambda: f\"Table names to perform maintenance on (use comma-separated list).\\n\"",
            "        f\"Options: {import_string('airflow.cli.commands.db_command.all_tables')}\"",
            "    ),",
            "    type=string_list_type,",
            ")",
            "ARG_DB_CLEANUP_TIMESTAMP = Arg(",
            "    (\"--clean-before-timestamp\",),",
            "    help=\"The date or timestamp before which data should be purged.\\n\"",
            "    \"If no timezone info is supplied then dates are assumed to be in airflow default timezone.\\n\"",
            "    \"Example: '2022-01-01 00:00:00+01:00'\",",
            "    type=parsedate,",
            "    required=True,",
            ")",
            "ARG_DB_DRY_RUN = Arg(",
            "    (\"--dry-run\",),",
            "    help=\"Perform a dry run\",",
            "    action=\"store_true\",",
            ")",
            "ARG_DB_SKIP_ARCHIVE = Arg(",
            "    (\"--skip-archive\",),",
            "    help=\"Don't preserve purged records in an archive table.\",",
            "    action=\"store_true\",",
            ")",
            "",
            "",
            "# pool",
            "ARG_POOL_NAME = Arg((\"pool\",), metavar='NAME', help=\"Pool name\")",
            "ARG_POOL_SLOTS = Arg((\"slots\",), type=int, help=\"Pool slots\")",
            "ARG_POOL_DESCRIPTION = Arg((\"description\",), help=\"Pool description\")",
            "ARG_POOL_IMPORT = Arg(",
            "    (\"file\",),",
            "    metavar=\"FILEPATH\",",
            "    help=\"Import pools from JSON file. Example format::\\n\"",
            "    + textwrap.indent(",
            "        textwrap.dedent(",
            "            '''",
            "            {",
            "                \"pool_1\": {\"slots\": 5, \"description\": \"\"},",
            "                \"pool_2\": {\"slots\": 10, \"description\": \"test\"}",
            "            }'''",
            "        ),",
            "        \" \" * 4,",
            "    ),",
            ")",
            "",
            "ARG_POOL_EXPORT = Arg((\"file\",), metavar=\"FILEPATH\", help=\"Export all pools to JSON file\")",
            "",
            "# variables",
            "ARG_VAR = Arg((\"key\",), help=\"Variable key\")",
            "ARG_VAR_VALUE = Arg((\"value\",), metavar='VALUE', help=\"Variable value\")",
            "ARG_DEFAULT = Arg(",
            "    (\"-d\", \"--default\"), metavar=\"VAL\", default=None, help=\"Default value returned if variable does not exist\"",
            ")",
            "ARG_JSON = Arg((\"-j\", \"--json\"), help=\"Deserialize JSON variable\", action=\"store_true\")",
            "ARG_VAR_IMPORT = Arg((\"file\",), help=\"Import variables from JSON file\")",
            "ARG_VAR_EXPORT = Arg((\"file\",), help=\"Export all variables to JSON file\")",
            "",
            "# kerberos",
            "ARG_PRINCIPAL = Arg((\"principal\",), help=\"kerberos principal\", nargs='?')",
            "ARG_KEYTAB = Arg((\"-k\", \"--keytab\"), help=\"keytab\", nargs='?', default=conf.get('kerberos', 'keytab'))",
            "# run",
            "ARG_INTERACTIVE = Arg(",
            "    ('-N', '--interactive'),",
            "    help='Do not capture standard output and error streams (useful for interactive debugging)',",
            "    action='store_true',",
            ")",
            "# TODO(aoen): \"force\" is a poor choice of name here since it implies it overrides",
            "# all dependencies (not just past success), e.g. the ignore_depends_on_past",
            "# dependency. This flag should be deprecated and renamed to 'ignore_ti_state' and",
            "# the \"ignore_all_dependencies\" command should be called the\"force\" command",
            "# instead.",
            "ARG_FORCE = Arg(",
            "    (\"-f\", \"--force\"),",
            "    help=\"Ignore previous task instance state, rerun regardless if task already succeeded/failed\",",
            "    action=\"store_true\",",
            ")",
            "ARG_RAW = Arg((\"-r\", \"--raw\"), argparse.SUPPRESS, \"store_true\")",
            "ARG_IGNORE_ALL_DEPENDENCIES = Arg(",
            "    (\"-A\", \"--ignore-all-dependencies\"),",
            "    help=\"Ignores all non-critical dependencies, including ignore_ti_state and ignore_task_deps\",",
            "    action=\"store_true\",",
            ")",
            "# TODO(aoen): ignore_dependencies is a poor choice of name here because it is too",
            "# vague (e.g. a task being in the appropriate state to be run is also a dependency",
            "# but is not ignored by this flag), the name 'ignore_task_dependencies' is",
            "# slightly better (as it ignores all dependencies that are specific to the task),",
            "# so deprecate the old command name and use this instead.",
            "ARG_IGNORE_DEPENDENCIES = Arg(",
            "    (\"-i\", \"--ignore-dependencies\"),",
            "    help=\"Ignore task-specific dependencies, e.g. upstream, depends_on_past, and retry delay dependencies\",",
            "    action=\"store_true\",",
            ")",
            "ARG_IGNORE_DEPENDS_ON_PAST = Arg(",
            "    (\"-I\", \"--ignore-depends-on-past\"),",
            "    help=\"Ignore depends_on_past dependencies (but respect upstream dependencies)\",",
            "    action=\"store_true\",",
            ")",
            "ARG_SHIP_DAG = Arg(",
            "    (\"--ship-dag\",), help=\"Pickles (serializes) the DAG and ships it to the worker\", action=\"store_true\"",
            ")",
            "ARG_PICKLE = Arg((\"-p\", \"--pickle\"), help=\"Serialized pickle object of the entire dag (used internally)\")",
            "ARG_ERROR_FILE = Arg((\"--error-file\",), help=\"File to store task failure error\")",
            "ARG_JOB_ID = Arg((\"-j\", \"--job-id\"), help=argparse.SUPPRESS)",
            "ARG_CFG_PATH = Arg((\"--cfg-path\",), help=\"Path to config file to use instead of airflow.cfg\")",
            "ARG_MAP_INDEX = Arg(('--map-index',), type=int, default=-1, help=\"Mapped task index\")",
            "",
            "",
            "# database",
            "ARG_MIGRATION_TIMEOUT = Arg(",
            "    (\"-t\", \"--migration-wait-timeout\"),",
            "    help=\"timeout to wait for db to migrate \",",
            "    type=int,",
            "    default=60,",
            ")",
            "ARG_DB_VERSION__UPGRADE = Arg(",
            "    (\"-n\", \"--to-version\"),",
            "    help=(",
            "        \"(Optional) The airflow version to upgrade to. Note: must provide either \"",
            "        \"`--to-revision` or `--to-version`.\"",
            "    ),",
            ")",
            "ARG_DB_REVISION__UPGRADE = Arg(",
            "    (\"-r\", \"--to-revision\"),",
            "    help=\"(Optional) If provided, only run migrations up to and including this Alembic revision.\",",
            ")",
            "ARG_DB_VERSION__DOWNGRADE = Arg(",
            "    (\"-n\", \"--to-version\"),",
            "    help=\"(Optional) If provided, only run migrations up to this version.\",",
            ")",
            "ARG_DB_FROM_VERSION = Arg(",
            "    (\"--from-version\",),",
            "    help=\"(Optional) If generating sql, may supply a *from* version\",",
            ")",
            "ARG_DB_REVISION__DOWNGRADE = Arg(",
            "    (\"-r\", \"--to-revision\"),",
            "    help=\"The Alembic revision to downgrade to. Note: must provide either `--to-revision` or `--to-version`.\",",
            ")",
            "ARG_DB_FROM_REVISION = Arg(",
            "    (\"--from-revision\",),",
            "    help=\"(Optional) If generating sql, may supply a *from* Alembic revision\",",
            ")",
            "ARG_DB_SQL_ONLY = Arg(",
            "    (\"-s\", \"--show-sql-only\"),",
            "    help=\"Don't actually run migrations; just print out sql scripts for offline migration. \"",
            "    \"Required if using either `--from-version` or `--from-version`.\",",
            "    action=\"store_true\",",
            "    default=False,",
            ")",
            "ARG_DB_SKIP_INIT = Arg(",
            "    (\"-s\", \"--skip-init\"),",
            "    help=\"Only remove tables; do not perform db init.\",",
            "    action=\"store_true\",",
            "    default=False,",
            ")",
            "",
            "# webserver",
            "ARG_PORT = Arg(",
            "    (\"-p\", \"--port\"),",
            "    default=conf.get('webserver', 'WEB_SERVER_PORT'),",
            "    type=int,",
            "    help=\"The port on which to run the server\",",
            ")",
            "ARG_SSL_CERT = Arg(",
            "    (\"--ssl-cert\",),",
            "    default=conf.get('webserver', 'WEB_SERVER_SSL_CERT'),",
            "    help=\"Path to the SSL certificate for the webserver\",",
            ")",
            "ARG_SSL_KEY = Arg(",
            "    (\"--ssl-key\",),",
            "    default=conf.get('webserver', 'WEB_SERVER_SSL_KEY'),",
            "    help=\"Path to the key to use with the SSL certificate\",",
            ")",
            "ARG_WORKERS = Arg(",
            "    (\"-w\", \"--workers\"),",
            "    default=conf.get('webserver', 'WORKERS'),",
            "    type=int,",
            "    help=\"Number of workers to run the webserver on\",",
            ")",
            "ARG_WORKERCLASS = Arg(",
            "    (\"-k\", \"--workerclass\"),",
            "    default=conf.get('webserver', 'WORKER_CLASS'),",
            "    choices=['sync', 'eventlet', 'gevent', 'tornado'],",
            "    help=\"The worker class to use for Gunicorn\",",
            ")",
            "ARG_WORKER_TIMEOUT = Arg(",
            "    (\"-t\", \"--worker-timeout\"),",
            "    default=conf.get('webserver', 'WEB_SERVER_WORKER_TIMEOUT'),",
            "    type=int,",
            "    help=\"The timeout for waiting on webserver workers\",",
            ")",
            "ARG_HOSTNAME = Arg(",
            "    (\"-H\", \"--hostname\"),",
            "    default=conf.get('webserver', 'WEB_SERVER_HOST'),",
            "    help=\"Set the hostname on which to run the web server\",",
            ")",
            "ARG_DEBUG = Arg(",
            "    (\"-d\", \"--debug\"), help=\"Use the server that ships with Flask in debug mode\", action=\"store_true\"",
            ")",
            "ARG_ACCESS_LOGFILE = Arg(",
            "    (\"-A\", \"--access-logfile\"),",
            "    default=conf.get('webserver', 'ACCESS_LOGFILE'),",
            "    help=\"The logfile to store the webserver access log. Use '-' to print to stderr\",",
            ")",
            "ARG_ERROR_LOGFILE = Arg(",
            "    (\"-E\", \"--error-logfile\"),",
            "    default=conf.get('webserver', 'ERROR_LOGFILE'),",
            "    help=\"The logfile to store the webserver error log. Use '-' to print to stderr\",",
            ")",
            "ARG_ACCESS_LOGFORMAT = Arg(",
            "    (\"-L\", \"--access-logformat\"),",
            "    default=conf.get('webserver', 'ACCESS_LOGFORMAT'),",
            "    help=\"The access log format for gunicorn logs\",",
            ")",
            "",
            "# scheduler",
            "ARG_NUM_RUNS = Arg(",
            "    (\"-n\", \"--num-runs\"),",
            "    default=conf.getint('scheduler', 'num_runs'),",
            "    type=int,",
            "    help=\"Set the number of runs to execute before exiting\",",
            ")",
            "ARG_DO_PICKLE = Arg(",
            "    (\"-p\", \"--do-pickle\"),",
            "    default=False,",
            "    help=(",
            "        \"Attempt to pickle the DAG object to send over \"",
            "        \"to the workers, instead of letting workers run their version \"",
            "        \"of the code\"",
            "    ),",
            "    action=\"store_true\",",
            ")",
            "",
            "# worker",
            "ARG_QUEUES = Arg(",
            "    (\"-q\", \"--queues\"),",
            "    help=\"Comma delimited list of queues to serve\",",
            "    default=conf.get('operators', 'DEFAULT_QUEUE'),",
            ")",
            "ARG_CONCURRENCY = Arg(",
            "    (\"-c\", \"--concurrency\"),",
            "    type=int,",
            "    help=\"The number of worker processes\",",
            "    default=conf.get('celery', 'worker_concurrency'),",
            ")",
            "ARG_CELERY_HOSTNAME = Arg(",
            "    (\"-H\", \"--celery-hostname\"),",
            "    help=\"Set the hostname of celery worker if you have multiple workers on a single machine\",",
            ")",
            "ARG_UMASK = Arg(",
            "    (\"-u\", \"--umask\"),",
            "    help=\"Set the umask of celery worker in daemon mode\",",
            "    default=conf.get('celery', 'worker_umask'),",
            ")",
            "ARG_WITHOUT_MINGLE = Arg(",
            "    (\"--without-mingle\",),",
            "    default=False,",
            "    help=\"Don\u2019t synchronize with other workers at start-up\",",
            "    action=\"store_true\",",
            ")",
            "ARG_WITHOUT_GOSSIP = Arg(",
            "    (\"--without-gossip\",),",
            "    default=False,",
            "    help=\"Don\u2019t subscribe to other workers events\",",
            "    action=\"store_true\",",
            ")",
            "",
            "# flower",
            "ARG_BROKER_API = Arg((\"-a\", \"--broker-api\"), help=\"Broker API\")",
            "ARG_FLOWER_HOSTNAME = Arg(",
            "    (\"-H\", \"--hostname\"),",
            "    default=conf.get('celery', 'FLOWER_HOST'),",
            "    help=\"Set the hostname on which to run the server\",",
            ")",
            "ARG_FLOWER_PORT = Arg(",
            "    (\"-p\", \"--port\"),",
            "    default=conf.get('celery', 'FLOWER_PORT'),",
            "    type=int,",
            "    help=\"The port on which to run the server\",",
            ")",
            "ARG_FLOWER_CONF = Arg((\"-c\", \"--flower-conf\"), help=\"Configuration file for flower\")",
            "ARG_FLOWER_URL_PREFIX = Arg(",
            "    (\"-u\", \"--url-prefix\"), default=conf.get('celery', 'FLOWER_URL_PREFIX'), help=\"URL prefix for Flower\"",
            ")",
            "ARG_FLOWER_BASIC_AUTH = Arg(",
            "    (\"-A\", \"--basic-auth\"),",
            "    default=conf.get('celery', 'FLOWER_BASIC_AUTH'),",
            "    help=(",
            "        \"Securing Flower with Basic Authentication. \"",
            "        \"Accepts user:password pairs separated by a comma. \"",
            "        \"Example: flower_basic_auth = user1:password1,user2:password2\"",
            "    ),",
            ")",
            "ARG_TASK_PARAMS = Arg((\"-t\", \"--task-params\"), help=\"Sends a JSON params dict to the task\")",
            "ARG_POST_MORTEM = Arg(",
            "    (\"-m\", \"--post-mortem\"), action=\"store_true\", help=\"Open debugger on uncaught exception\"",
            ")",
            "ARG_ENV_VARS = Arg(",
            "    (\"--env-vars\",),",
            "    help=\"Set env var in both parsing time and runtime for each of entry supplied in a JSON dict\",",
            "    type=json.loads,",
            ")",
            "",
            "# connections",
            "ARG_CONN_ID = Arg(('conn_id',), help='Connection id, required to get/add/delete a connection', type=str)",
            "ARG_CONN_ID_FILTER = Arg(",
            "    ('--conn-id',), help='If passed, only items with the specified connection ID will be displayed', type=str",
            ")",
            "ARG_CONN_URI = Arg(",
            "    ('--conn-uri',), help='Connection URI, required to add a connection without conn_type', type=str",
            ")",
            "ARG_CONN_JSON = Arg(",
            "    ('--conn-json',), help='Connection JSON, required to add a connection using JSON representation', type=str",
            ")",
            "ARG_CONN_TYPE = Arg(",
            "    ('--conn-type',), help='Connection type, required to add a connection without conn_uri', type=str",
            ")",
            "ARG_CONN_DESCRIPTION = Arg(",
            "    ('--conn-description',), help='Connection description, optional when adding a connection', type=str",
            ")",
            "ARG_CONN_HOST = Arg(('--conn-host',), help='Connection host, optional when adding a connection', type=str)",
            "ARG_CONN_LOGIN = Arg(('--conn-login',), help='Connection login, optional when adding a connection', type=str)",
            "ARG_CONN_PASSWORD = Arg(",
            "    ('--conn-password',), help='Connection password, optional when adding a connection', type=str",
            ")",
            "ARG_CONN_SCHEMA = Arg(",
            "    ('--conn-schema',), help='Connection schema, optional when adding a connection', type=str",
            ")",
            "ARG_CONN_PORT = Arg(('--conn-port',), help='Connection port, optional when adding a connection', type=str)",
            "ARG_CONN_EXTRA = Arg(",
            "    ('--conn-extra',), help='Connection `Extra` field, optional when adding a connection', type=str",
            ")",
            "ARG_CONN_EXPORT = Arg(",
            "    ('file',),",
            "    help='Output file path for exporting the connections',",
            "    type=argparse.FileType('w', encoding='UTF-8'),",
            ")",
            "ARG_CONN_EXPORT_FORMAT = Arg(",
            "    ('--format',),",
            "    help='Deprecated -- use `--file-format` instead. File format to use for the export.',",
            "    type=str,",
            "    choices=['json', 'yaml', 'env'],",
            ")",
            "ARG_CONN_EXPORT_FILE_FORMAT = Arg(",
            "    ('--file-format',), help='File format for the export', type=str, choices=['json', 'yaml', 'env']",
            ")",
            "ARG_CONN_SERIALIZATION_FORMAT = Arg(",
            "    ('--serialization-format',),",
            "    help='When exporting as `.env` format, defines how connections should be serialized. Default is `uri`.',",
            "    type=string_lower_type,",
            "    choices=['json', 'uri'],",
            ")",
            "ARG_CONN_IMPORT = Arg((\"file\",), help=\"Import connections from a file\")",
            "",
            "# providers",
            "ARG_PROVIDER_NAME = Arg(",
            "    ('provider_name',), help='Provider name, required to get provider information', type=str",
            ")",
            "ARG_FULL = Arg(",
            "    ('-f', '--full'),",
            "    help='Full information about the provider, including documentation information.',",
            "    required=False,",
            "    action=\"store_true\",",
            ")",
            "",
            "# users",
            "ARG_USERNAME = Arg(('-u', '--username'), help='Username of the user', required=True, type=str)",
            "ARG_USERNAME_OPTIONAL = Arg(('-u', '--username'), help='Username of the user', type=str)",
            "ARG_FIRSTNAME = Arg(('-f', '--firstname'), help='First name of the user', required=True, type=str)",
            "ARG_LASTNAME = Arg(('-l', '--lastname'), help='Last name of the user', required=True, type=str)",
            "ARG_ROLE = Arg(",
            "    ('-r', '--role'),",
            "    help='Role of the user. Existing roles include Admin, User, Op, Viewer, and Public',",
            "    required=True,",
            "    type=str,",
            ")",
            "ARG_EMAIL = Arg(('-e', '--email'), help='Email of the user', required=True, type=str)",
            "ARG_EMAIL_OPTIONAL = Arg(('-e', '--email'), help='Email of the user', type=str)",
            "ARG_PASSWORD = Arg(",
            "    ('-p', '--password'),",
            "    help='Password of the user, required to create a user without --use-random-password',",
            "    type=str,",
            ")",
            "ARG_USE_RANDOM_PASSWORD = Arg(",
            "    ('--use-random-password',),",
            "    help='Do not prompt for password. Use random string instead.'",
            "    ' Required to create a user without --password ',",
            "    default=False,",
            "    action='store_true',",
            ")",
            "ARG_USER_IMPORT = Arg(",
            "    (\"import\",),",
            "    metavar=\"FILEPATH\",",
            "    help=\"Import users from JSON file. Example format::\\n\"",
            "    + textwrap.indent(",
            "        textwrap.dedent(",
            "            '''",
            "            [",
            "                {",
            "                    \"email\": \"foo@bar.org\",",
            "                    \"firstname\": \"Jon\",",
            "                    \"lastname\": \"Doe\",",
            "                    \"roles\": [\"Public\"],",
            "                    \"username\": \"jondoe\"",
            "                }",
            "            ]'''",
            "        ),",
            "        \" \" * 4,",
            "    ),",
            ")",
            "ARG_USER_EXPORT = Arg((\"export\",), metavar=\"FILEPATH\", help=\"Export all users to JSON file\")",
            "",
            "# roles",
            "ARG_CREATE_ROLE = Arg(('-c', '--create'), help='Create a new role', action='store_true')",
            "ARG_LIST_ROLES = Arg(('-l', '--list'), help='List roles', action='store_true')",
            "ARG_ROLES = Arg(('role',), help='The name of a role', nargs='*')",
            "ARG_AUTOSCALE = Arg(('-a', '--autoscale'), help=\"Minimum and Maximum number of worker to autoscale\")",
            "ARG_SKIP_SERVE_LOGS = Arg(",
            "    (\"-s\", \"--skip-serve-logs\"),",
            "    default=False,",
            "    help=\"Don't start the serve logs process along with the workers\",",
            "    action=\"store_true\",",
            ")",
            "ARG_ROLE_IMPORT = Arg((\"file\",), help=\"Import roles from JSON file\", nargs=None)",
            "ARG_ROLE_EXPORT = Arg((\"file\",), help=\"Export all roles to JSON file\", nargs=None)",
            "ARG_ROLE_EXPORT_FMT = Arg(",
            "    ('-p', '--pretty'),",
            "    help='Format output JSON file by sorting role names and indenting by 4 spaces',",
            "    action='store_true',",
            ")",
            "",
            "# info",
            "ARG_ANONYMIZE = Arg(",
            "    ('--anonymize',),",
            "    help='Minimize any personal identifiable information. Use it when sharing output with others.',",
            "    action='store_true',",
            ")",
            "ARG_FILE_IO = Arg(",
            "    ('--file-io',), help='Send output to file.io service and returns link.', action='store_true'",
            ")",
            "",
            "# config",
            "ARG_SECTION = Arg(",
            "    (\"section\",),",
            "    help=\"The section name\",",
            ")",
            "ARG_OPTION = Arg(",
            "    (\"option\",),",
            "    help=\"The option name\",",
            ")",
            "",
            "# kubernetes cleanup-pods",
            "ARG_NAMESPACE = Arg(",
            "    (\"--namespace\",),",
            "    default=conf.get('kubernetes', 'namespace'),",
            "    help=\"Kubernetes Namespace. Default value is `[kubernetes] namespace` in configuration.\",",
            ")",
            "",
            "ARG_MIN_PENDING_MINUTES = Arg(",
            "    (\"--min-pending-minutes\",),",
            "    default=30,",
            "    type=positive_int(allow_zero=False),",
            "    help=(",
            "        \"Pending pods created before the time interval are to be cleaned up, \"",
            "        \"measured in minutes. Default value is 30(m). The minimum value is 5(m).\"",
            "    ),",
            ")",
            "",
            "# jobs check",
            "ARG_JOB_TYPE_FILTER = Arg(",
            "    ('--job-type',),",
            "    choices=('BackfillJob', 'LocalTaskJob', 'SchedulerJob', 'TriggererJob'),",
            "    action='store',",
            "    help='The type of job(s) that will be checked.',",
            ")",
            "",
            "ARG_JOB_HOSTNAME_FILTER = Arg(",
            "    (\"--hostname\",),",
            "    default=None,",
            "    type=str,",
            "    help=\"The hostname of job(s) that will be checked.\",",
            ")",
            "",
            "ARG_JOB_LIMIT = Arg(",
            "    (\"--limit\",),",
            "    default=1,",
            "    type=positive_int(allow_zero=True),",
            "    help=\"The number of recent jobs that will be checked. To disable limit, set 0. \",",
            ")",
            "",
            "ARG_ALLOW_MULTIPLE = Arg(",
            "    (\"--allow-multiple\",),",
            "    action='store_true',",
            "    help=\"If passed, this command will be successful even if multiple matching alive jobs are found.\",",
            ")",
            "",
            "# sync-perm",
            "ARG_INCLUDE_DAGS = Arg(",
            "    (\"--include-dags\",), help=\"If passed, DAG specific permissions will also be synced.\", action=\"store_true\"",
            ")",
            "",
            "# triggerer",
            "ARG_CAPACITY = Arg(",
            "    (\"--capacity\",),",
            "    type=positive_int(allow_zero=False),",
            "    help=\"The maximum number of triggers that a Triggerer will run at one time.\",",
            ")",
            "",
            "# reserialize",
            "ARG_CLEAR_ONLY = Arg(",
            "    (\"--clear-only\",),",
            "    action=\"store_true\",",
            "    help=\"If passed, serialized DAGs will be cleared but not reserialized.\",",
            ")",
            "",
            "ALTERNATIVE_CONN_SPECS_ARGS = [",
            "    ARG_CONN_TYPE,",
            "    ARG_CONN_DESCRIPTION,",
            "    ARG_CONN_HOST,",
            "    ARG_CONN_LOGIN,",
            "    ARG_CONN_PASSWORD,",
            "    ARG_CONN_SCHEMA,",
            "    ARG_CONN_PORT,",
            "]",
            "",
            "",
            "class ActionCommand(NamedTuple):",
            "    \"\"\"Single CLI command\"\"\"",
            "",
            "    name: str",
            "    help: str",
            "    func: Callable",
            "    args: Iterable[Arg]",
            "    description: Optional[str] = None",
            "    epilog: Optional[str] = None",
            "",
            "",
            "class GroupCommand(NamedTuple):",
            "    \"\"\"ClI command with subcommands\"\"\"",
            "",
            "    name: str",
            "    help: str",
            "    subcommands: Iterable",
            "    description: Optional[str] = None",
            "    epilog: Optional[str] = None",
            "",
            "",
            "CLICommand = Union[ActionCommand, GroupCommand]",
            "",
            "DAGS_COMMANDS = (",
            "    ActionCommand(",
            "        name='list',",
            "        help=\"List all the DAGs\",",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_list_dags'),",
            "        args=(ARG_SUBDIR, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='list-import-errors',",
            "        help=\"List all the DAGs that have import errors\",",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_list_import_errors'),",
            "        args=(ARG_SUBDIR, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='report',",
            "        help='Show DagBag loading report',",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_report'),",
            "        args=(ARG_SUBDIR, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='list-runs',",
            "        help=\"List DAG runs given a DAG id\",",
            "        description=(",
            "            \"List DAG runs given a DAG id. If state option is given, it will only search for all the \"",
            "            \"dagruns with the given state. If no_backfill option is given, it will filter out all \"",
            "            \"backfill dagruns for given dag id. If start_date is given, it will filter out all the \"",
            "            \"dagruns that were executed before this date. If end_date is given, it will filter out \"",
            "            \"all the dagruns that were executed after this date. \"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_list_dag_runs'),",
            "        args=(",
            "            ARG_DAG_ID_OPT,",
            "            ARG_NO_BACKFILL,",
            "            ARG_STATE,",
            "            ARG_OUTPUT,",
            "            ARG_VERBOSE,",
            "            ARG_START_DATE,",
            "            ARG_END_DATE,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='list-jobs',",
            "        help=\"List the jobs\",",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_list_jobs'),",
            "        args=(ARG_DAG_ID_OPT, ARG_STATE, ARG_LIMIT, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='state',",
            "        help=\"Get the status of a dag run\",",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_state'),",
            "        args=(ARG_DAG_ID, ARG_EXECUTION_DATE, ARG_SUBDIR),",
            "    ),",
            "    ActionCommand(",
            "        name='next-execution',",
            "        help=\"Get the next execution datetimes of a DAG\",",
            "        description=(",
            "            \"Get the next execution datetimes of a DAG. It returns one execution unless the \"",
            "            \"num-executions option is given\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_next_execution'),",
            "        args=(ARG_DAG_ID, ARG_SUBDIR, ARG_NUM_EXECUTIONS),",
            "    ),",
            "    ActionCommand(",
            "        name='pause',",
            "        help='Pause a DAG',",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_pause'),",
            "        args=(ARG_DAG_ID, ARG_SUBDIR),",
            "    ),",
            "    ActionCommand(",
            "        name='unpause',",
            "        help='Resume a paused DAG',",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_unpause'),",
            "        args=(ARG_DAG_ID, ARG_SUBDIR),",
            "    ),",
            "    ActionCommand(",
            "        name='trigger',",
            "        help='Trigger a DAG run',",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_trigger'),",
            "        args=(ARG_DAG_ID, ARG_SUBDIR, ARG_RUN_ID, ARG_CONF, ARG_EXEC_DATE),",
            "    ),",
            "    ActionCommand(",
            "        name='delete',",
            "        help=\"Delete all DB records related to the specified DAG\",",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_delete'),",
            "        args=(ARG_DAG_ID, ARG_YES),",
            "    ),",
            "    ActionCommand(",
            "        name='show',",
            "        help=\"Displays DAG's tasks with their dependencies\",",
            "        description=(",
            "            \"The --imgcat option only works in iTerm.\\n\"",
            "            \"\\n\"",
            "            \"For more information, see: https://www.iterm2.com/documentation-images.html\\n\"",
            "            \"\\n\"",
            "            \"The --save option saves the result to the indicated file.\\n\"",
            "            \"\\n\"",
            "            \"The file format is determined by the file extension. \"",
            "            \"For more information about supported \"",
            "            \"format, see: https://www.graphviz.org/doc/info/output.html\\n\"",
            "            \"\\n\"",
            "            \"If you want to create a PNG file then you should execute the following command:\\n\"",
            "            \"airflow dags show <DAG_ID> --save output.png\\n\"",
            "            \"\\n\"",
            "            \"If you want to create a DOT file then you should execute the following command:\\n\"",
            "            \"airflow dags show <DAG_ID> --save output.dot\\n\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_show'),",
            "        args=(",
            "            ARG_DAG_ID,",
            "            ARG_SUBDIR,",
            "            ARG_SAVE,",
            "            ARG_IMGCAT,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='show-dependencies',",
            "        help=\"Displays DAGs with their dependencies\",",
            "        description=(",
            "            \"The --imgcat option only works in iTerm.\\n\"",
            "            \"\\n\"",
            "            \"For more information, see: https://www.iterm2.com/documentation-images.html\\n\"",
            "            \"\\n\"",
            "            \"The --save option saves the result to the indicated file.\\n\"",
            "            \"\\n\"",
            "            \"The file format is determined by the file extension. \"",
            "            \"For more information about supported \"",
            "            \"format, see: https://www.graphviz.org/doc/info/output.html\\n\"",
            "            \"\\n\"",
            "            \"If you want to create a PNG file then you should execute the following command:\\n\"",
            "            \"airflow dags show-dependencies --save output.png\\n\"",
            "            \"\\n\"",
            "            \"If you want to create a DOT file then you should execute the following command:\\n\"",
            "            \"airflow dags show-dependencies --save output.dot\\n\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_dependencies_show'),",
            "        args=(",
            "            ARG_SUBDIR,",
            "            ARG_SAVE,",
            "            ARG_IMGCAT,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='backfill',",
            "        help=\"Run subsections of a DAG for a specified date range\",",
            "        description=(",
            "            \"Run subsections of a DAG for a specified date range. If reset_dag_run option is used, \"",
            "            \"backfill will first prompt users whether airflow should clear all the previous dag_run and \"",
            "            \"task_instances within the backfill date range. If rerun_failed_tasks is used, backfill \"",
            "            \"will auto re-run the previous failed task instances  within the backfill date range\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_backfill'),",
            "        args=(",
            "            ARG_DAG_ID,",
            "            ARG_TASK_REGEX,",
            "            ARG_START_DATE,",
            "            ARG_END_DATE,",
            "            ARG_MARK_SUCCESS,",
            "            ARG_LOCAL,",
            "            ARG_DONOT_PICKLE,",
            "            ARG_YES,",
            "            ARG_CONTINUE_ON_FAILURES,",
            "            ARG_BF_IGNORE_DEPENDENCIES,",
            "            ARG_BF_IGNORE_FIRST_DEPENDS_ON_PAST,",
            "            ARG_SUBDIR,",
            "            ARG_POOL,",
            "            ARG_DELAY_ON_LIMIT,",
            "            ARG_DRY_RUN,",
            "            ARG_VERBOSE,",
            "            ARG_CONF,",
            "            ARG_RESET_DAG_RUN,",
            "            ARG_RERUN_FAILED_TASKS,",
            "            ARG_RUN_BACKWARDS,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='test',",
            "        help=\"Execute one single DagRun\",",
            "        description=(",
            "            \"Execute one single DagRun for a given DAG and execution date, \"",
            "            \"using the DebugExecutor.\\n\"",
            "            \"\\n\"",
            "            \"The --imgcat-dagrun option only works in iTerm.\\n\"",
            "            \"\\n\"",
            "            \"For more information, see: https://www.iterm2.com/documentation-images.html\\n\"",
            "            \"\\n\"",
            "            \"If --save-dagrun is used, then, after completing the backfill, saves the diagram \"",
            "            \"for current DAG Run to the indicated file.\\n\"",
            "            \"The file format is determined by the file extension. \"",
            "            \"For more information about supported format, \"",
            "            \"see: https://www.graphviz.org/doc/info/output.html\\n\"",
            "            \"\\n\"",
            "            \"If you want to create a PNG file then you should execute the following command:\\n\"",
            "            \"airflow dags test <DAG_ID> <EXECUTION_DATE> --save-dagrun output.png\\n\"",
            "            \"\\n\"",
            "            \"If you want to create a DOT file then you should execute the following command:\\n\"",
            "            \"airflow dags test <DAG_ID> <EXECUTION_DATE> --save-dagrun output.dot\\n\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_test'),",
            "        args=(",
            "            ARG_DAG_ID,",
            "            ARG_EXECUTION_DATE,",
            "            ARG_SUBDIR,",
            "            ARG_SHOW_DAGRUN,",
            "            ARG_IMGCAT_DAGRUN,",
            "            ARG_SAVE_DAGRUN,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='reserialize',",
            "        help=\"Reserialize all DAGs by parsing the DagBag files\",",
            "        description=(",
            "            \"Drop all serialized dags from the metadata DB. This will cause all DAGs to be reserialized \"",
            "            \"from the DagBag folder. This can be helpful if your serialized DAGs get out of sync with the \"",
            "            \"version of Airflow that you are running.\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_reserialize'),",
            "        args=(ARG_CLEAR_ONLY,),",
            "    ),",
            ")",
            "TASKS_COMMANDS = (",
            "    ActionCommand(",
            "        name='list',",
            "        help=\"List the tasks within a DAG\",",
            "        func=lazy_load_command('airflow.cli.commands.task_command.task_list'),",
            "        args=(ARG_DAG_ID, ARG_TREE, ARG_SUBDIR, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='clear',",
            "        help=\"Clear a set of task instance, as if they never ran\",",
            "        func=lazy_load_command('airflow.cli.commands.task_command.task_clear'),",
            "        args=(",
            "            ARG_DAG_ID,",
            "            ARG_TASK_REGEX,",
            "            ARG_START_DATE,",
            "            ARG_END_DATE,",
            "            ARG_SUBDIR,",
            "            ARG_UPSTREAM,",
            "            ARG_DOWNSTREAM,",
            "            ARG_YES,",
            "            ARG_ONLY_FAILED,",
            "            ARG_ONLY_RUNNING,",
            "            ARG_EXCLUDE_SUBDAGS,",
            "            ARG_EXCLUDE_PARENTDAG,",
            "            ARG_DAG_REGEX,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='state',",
            "        help=\"Get the status of a task instance\",",
            "        func=lazy_load_command('airflow.cli.commands.task_command.task_state'),",
            "        args=(",
            "            ARG_DAG_ID,",
            "            ARG_TASK_ID,",
            "            ARG_EXECUTION_DATE_OR_RUN_ID,",
            "            ARG_SUBDIR,",
            "            ARG_VERBOSE,",
            "            ARG_MAP_INDEX,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='failed-deps',",
            "        help=\"Returns the unmet dependencies for a task instance\",",
            "        description=(",
            "            \"Returns the unmet dependencies for a task instance from the perspective of the scheduler. \"",
            "            \"In other words, why a task instance doesn't get scheduled and then queued by the scheduler, \"",
            "            \"and then run by an executor.\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.task_command.task_failed_deps'),",
            "        args=(ARG_DAG_ID, ARG_TASK_ID, ARG_EXECUTION_DATE_OR_RUN_ID, ARG_SUBDIR, ARG_MAP_INDEX),",
            "    ),",
            "    ActionCommand(",
            "        name='render',",
            "        help=\"Render a task instance's template(s)\",",
            "        func=lazy_load_command('airflow.cli.commands.task_command.task_render'),",
            "        args=(",
            "            ARG_DAG_ID,",
            "            ARG_TASK_ID,",
            "            ARG_EXECUTION_DATE_OR_RUN_ID,",
            "            ARG_SUBDIR,",
            "            ARG_VERBOSE,",
            "            ARG_MAP_INDEX,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='run',",
            "        help=\"Run a single task instance\",",
            "        func=lazy_load_command('airflow.cli.commands.task_command.task_run'),",
            "        args=(",
            "            ARG_DAG_ID,",
            "            ARG_TASK_ID,",
            "            ARG_EXECUTION_DATE_OR_RUN_ID,",
            "            ARG_SUBDIR,",
            "            ARG_MARK_SUCCESS,",
            "            ARG_FORCE,",
            "            ARG_POOL,",
            "            ARG_CFG_PATH,",
            "            ARG_LOCAL,",
            "            ARG_RAW,",
            "            ARG_IGNORE_ALL_DEPENDENCIES,",
            "            ARG_IGNORE_DEPENDENCIES,",
            "            ARG_IGNORE_DEPENDS_ON_PAST,",
            "            ARG_SHIP_DAG,",
            "            ARG_PICKLE,",
            "            ARG_JOB_ID,",
            "            ARG_INTERACTIVE,",
            "            ARG_ERROR_FILE,",
            "            ARG_SHUT_DOWN_LOGGING,",
            "            ARG_MAP_INDEX,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='test',",
            "        help=\"Test a task instance\",",
            "        description=(",
            "            \"Test a task instance. This will run a task without checking for dependencies or recording \"",
            "            \"its state in the database\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.task_command.task_test'),",
            "        args=(",
            "            ARG_DAG_ID,",
            "            ARG_TASK_ID,",
            "            ARG_EXECUTION_DATE_OR_RUN_ID,",
            "            ARG_SUBDIR,",
            "            ARG_DRY_RUN,",
            "            ARG_TASK_PARAMS,",
            "            ARG_POST_MORTEM,",
            "            ARG_ENV_VARS,",
            "            ARG_MAP_INDEX,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='states-for-dag-run',",
            "        help=\"Get the status of all task instances in a dag run\",",
            "        func=lazy_load_command('airflow.cli.commands.task_command.task_states_for_dag_run'),",
            "        args=(ARG_DAG_ID, ARG_EXECUTION_DATE_OR_RUN_ID, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            ")",
            "POOLS_COMMANDS = (",
            "    ActionCommand(",
            "        name='list',",
            "        help='List pools',",
            "        func=lazy_load_command('airflow.cli.commands.pool_command.pool_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='get',",
            "        help='Get pool size',",
            "        func=lazy_load_command('airflow.cli.commands.pool_command.pool_get'),",
            "        args=(ARG_POOL_NAME, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='set',",
            "        help='Configure pool',",
            "        func=lazy_load_command('airflow.cli.commands.pool_command.pool_set'),",
            "        args=(ARG_POOL_NAME, ARG_POOL_SLOTS, ARG_POOL_DESCRIPTION, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='delete',",
            "        help='Delete pool',",
            "        func=lazy_load_command('airflow.cli.commands.pool_command.pool_delete'),",
            "        args=(ARG_POOL_NAME, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='import',",
            "        help='Import pools',",
            "        func=lazy_load_command('airflow.cli.commands.pool_command.pool_import'),",
            "        args=(ARG_POOL_IMPORT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='export',",
            "        help='Export all pools',",
            "        func=lazy_load_command('airflow.cli.commands.pool_command.pool_export'),",
            "        args=(ARG_POOL_EXPORT,),",
            "    ),",
            ")",
            "VARIABLES_COMMANDS = (",
            "    ActionCommand(",
            "        name='list',",
            "        help='List variables',",
            "        func=lazy_load_command('airflow.cli.commands.variable_command.variables_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='get',",
            "        help='Get variable',",
            "        func=lazy_load_command('airflow.cli.commands.variable_command.variables_get'),",
            "        args=(ARG_VAR, ARG_JSON, ARG_DEFAULT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='set',",
            "        help='Set variable',",
            "        func=lazy_load_command('airflow.cli.commands.variable_command.variables_set'),",
            "        args=(ARG_VAR, ARG_VAR_VALUE, ARG_JSON),",
            "    ),",
            "    ActionCommand(",
            "        name='delete',",
            "        help='Delete variable',",
            "        func=lazy_load_command('airflow.cli.commands.variable_command.variables_delete'),",
            "        args=(ARG_VAR,),",
            "    ),",
            "    ActionCommand(",
            "        name='import',",
            "        help='Import variables',",
            "        func=lazy_load_command('airflow.cli.commands.variable_command.variables_import'),",
            "        args=(ARG_VAR_IMPORT,),",
            "    ),",
            "    ActionCommand(",
            "        name='export',",
            "        help='Export all variables',",
            "        func=lazy_load_command('airflow.cli.commands.variable_command.variables_export'),",
            "        args=(ARG_VAR_EXPORT,),",
            "    ),",
            ")",
            "DB_COMMANDS = (",
            "    ActionCommand(",
            "        name='init',",
            "        help=\"Initialize the metadata database\",",
            "        func=lazy_load_command('airflow.cli.commands.db_command.initdb'),",
            "        args=(),",
            "    ),",
            "    ActionCommand(",
            "        name=\"check-migrations\",",
            "        help=\"Check if migration have finished\",",
            "        description=\"Check if migration have finished (or continually check until timeout)\",",
            "        func=lazy_load_command('airflow.cli.commands.db_command.check_migrations'),",
            "        args=(ARG_MIGRATION_TIMEOUT,),",
            "    ),",
            "    ActionCommand(",
            "        name='reset',",
            "        help=\"Burn down and rebuild the metadata database\",",
            "        func=lazy_load_command('airflow.cli.commands.db_command.resetdb'),",
            "        args=(ARG_YES, ARG_DB_SKIP_INIT),",
            "    ),",
            "    ActionCommand(",
            "        name='upgrade',",
            "        help=\"Upgrade the metadata database to latest version\",",
            "        description=(",
            "            \"Upgrade the schema of the metadata database. \"",
            "            \"To print but not execute commands, use option ``--show-sql-only``. \"",
            "            \"If using options ``--from-revision`` or ``--from-version``, you must also use \"",
            "            \"``--show-sql-only``, because if actually *running* migrations, we should only \"",
            "            \"migrate from the *current* Alembic revision.\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.db_command.upgradedb'),",
            "        args=(",
            "            ARG_DB_REVISION__UPGRADE,",
            "            ARG_DB_VERSION__UPGRADE,",
            "            ARG_DB_SQL_ONLY,",
            "            ARG_DB_FROM_REVISION,",
            "            ARG_DB_FROM_VERSION,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='downgrade',",
            "        help=\"Downgrade the schema of the metadata database.\",",
            "        description=(",
            "            \"Downgrade the schema of the metadata database. \"",
            "            \"You must provide either `--to-revision` or `--to-version`. \"",
            "            \"To print but not execute commands, use option `--show-sql-only`. \"",
            "            \"If using options `--from-revision` or `--from-version`, you must also use `--show-sql-only`, \"",
            "            \"because if actually *running* migrations, we should only migrate from the *current* Alembic \"",
            "            \"revision.\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.db_command.downgrade'),",
            "        args=(",
            "            ARG_DB_REVISION__DOWNGRADE,",
            "            ARG_DB_VERSION__DOWNGRADE,",
            "            ARG_DB_SQL_ONLY,",
            "            ARG_YES,",
            "            ARG_DB_FROM_REVISION,",
            "            ARG_DB_FROM_VERSION,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='shell',",
            "        help=\"Runs a shell to access the database\",",
            "        func=lazy_load_command('airflow.cli.commands.db_command.shell'),",
            "        args=(),",
            "    ),",
            "    ActionCommand(",
            "        name='check',",
            "        help=\"Check if the database can be reached\",",
            "        func=lazy_load_command('airflow.cli.commands.db_command.check'),",
            "        args=(),",
            "    ),",
            "    ActionCommand(",
            "        name='clean',",
            "        help=\"Purge old records in metastore tables\",",
            "        func=lazy_load_command('airflow.cli.commands.db_command.cleanup_tables'),",
            "        args=(",
            "            ARG_DB_TABLES,",
            "            ARG_DB_DRY_RUN,",
            "            ARG_DB_CLEANUP_TIMESTAMP,",
            "            ARG_VERBOSE,",
            "            ARG_YES,",
            "            ARG_DB_SKIP_ARCHIVE,",
            "        ),",
            "    ),",
            ")",
            "CONNECTIONS_COMMANDS = (",
            "    ActionCommand(",
            "        name='get',",
            "        help='Get a connection',",
            "        func=lazy_load_command('airflow.cli.commands.connection_command.connections_get'),",
            "        args=(ARG_CONN_ID, ARG_COLOR, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='list',",
            "        help='List connections',",
            "        func=lazy_load_command('airflow.cli.commands.connection_command.connections_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE, ARG_CONN_ID_FILTER),",
            "    ),",
            "    ActionCommand(",
            "        name='add',",
            "        help='Add a connection',",
            "        func=lazy_load_command('airflow.cli.commands.connection_command.connections_add'),",
            "        args=(ARG_CONN_ID, ARG_CONN_URI, ARG_CONN_JSON, ARG_CONN_EXTRA) + tuple(ALTERNATIVE_CONN_SPECS_ARGS),",
            "    ),",
            "    ActionCommand(",
            "        name='delete',",
            "        help='Delete a connection',",
            "        func=lazy_load_command('airflow.cli.commands.connection_command.connections_delete'),",
            "        args=(ARG_CONN_ID, ARG_COLOR),",
            "    ),",
            "    ActionCommand(",
            "        name='export',",
            "        help='Export all connections',",
            "        description=(",
            "            \"All connections can be exported in STDOUT using the following command:\\n\"",
            "            \"airflow connections export -\\n\"",
            "            \"The file format can be determined by the provided file extension. E.g., The following \"",
            "            \"command will export the connections in JSON format:\\n\"",
            "            \"airflow connections export /tmp/connections.json\\n\"",
            "            \"The --file-format parameter can be used to control the file format. E.g., \"",
            "            \"the default format is JSON in STDOUT mode, which can be overridden using: \\n\"",
            "            \"airflow connections export - --file-format yaml\\n\"",
            "            \"The --file-format parameter can also be used for the files, for example:\\n\"",
            "            \"airflow connections export /tmp/connections --file-format json.\\n\"",
            "            \"When exporting in `env` file format, you control whether URI format or JSON format \"",
            "            \"is used to serialize the connection by passing `uri` or `json` with option \"",
            "            \"`--serialization-format`.\\n\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.connection_command.connections_export'),",
            "        args=(",
            "            ARG_CONN_EXPORT,",
            "            ARG_CONN_EXPORT_FORMAT,",
            "            ARG_CONN_EXPORT_FILE_FORMAT,",
            "            ARG_CONN_SERIALIZATION_FORMAT,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='import',",
            "        help='Import connections from a file',",
            "        description=(",
            "            \"Connections can be imported from the output of the export command.\\n\"",
            "            \"The filetype must by json, yaml or env and will be automatically inferred.\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.connection_command.connections_import'),",
            "        args=(ARG_CONN_IMPORT,),",
            "    ),",
            ")",
            "PROVIDERS_COMMANDS = (",
            "    ActionCommand(",
            "        name='list',",
            "        help='List installed providers',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.providers_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='get',",
            "        help='Get detailed information about a provider',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.provider_get'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE, ARG_FULL, ARG_COLOR, ARG_PROVIDER_NAME),",
            "    ),",
            "    ActionCommand(",
            "        name='links',",
            "        help='List extra links registered by the providers',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.extra_links_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='widgets',",
            "        help='Get information about registered connection form widgets',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.connection_form_widget_list'),",
            "        args=(",
            "            ARG_OUTPUT,",
            "            ARG_VERBOSE,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='hooks',",
            "        help='List registered provider hooks',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.hooks_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='behaviours',",
            "        help='Get information about registered connection types with custom behaviours',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.connection_field_behaviours'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='logging',",
            "        help='Get information about task logging handlers provided',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.logging_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='secrets',",
            "        help='Get information about secrets backends provided',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.secrets_backends_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='auth',",
            "        help='Get information about API auth backends provided',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.auth_backend_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            ")",
            "",
            "USERS_COMMANDS = (",
            "    ActionCommand(",
            "        name='list',",
            "        help='List users',",
            "        func=lazy_load_command('airflow.cli.commands.user_command.users_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='create',",
            "        help='Create a user',",
            "        func=lazy_load_command('airflow.cli.commands.user_command.users_create'),",
            "        args=(",
            "            ARG_ROLE,",
            "            ARG_USERNAME,",
            "            ARG_EMAIL,",
            "            ARG_FIRSTNAME,",
            "            ARG_LASTNAME,",
            "            ARG_PASSWORD,",
            "            ARG_USE_RANDOM_PASSWORD,",
            "        ),",
            "        epilog=(",
            "            'examples:\\n'",
            "            'To create an user with \"Admin\" role and username equals to \"admin\", run:\\n'",
            "            '\\n'",
            "            '    $ airflow users create \\\\\\n'",
            "            '          --username admin \\\\\\n'",
            "            '          --firstname FIRST_NAME \\\\\\n'",
            "            '          --lastname LAST_NAME \\\\\\n'",
            "            '          --role Admin \\\\\\n'",
            "            '          --email admin@example.org'",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='delete',",
            "        help='Delete a user',",
            "        func=lazy_load_command('airflow.cli.commands.user_command.users_delete'),",
            "        args=(ARG_USERNAME_OPTIONAL, ARG_EMAIL_OPTIONAL),",
            "    ),",
            "    ActionCommand(",
            "        name='add-role',",
            "        help='Add role to a user',",
            "        func=lazy_load_command('airflow.cli.commands.user_command.add_role'),",
            "        args=(ARG_USERNAME_OPTIONAL, ARG_EMAIL_OPTIONAL, ARG_ROLE),",
            "    ),",
            "    ActionCommand(",
            "        name='remove-role',",
            "        help='Remove role from a user',",
            "        func=lazy_load_command('airflow.cli.commands.user_command.remove_role'),",
            "        args=(ARG_USERNAME_OPTIONAL, ARG_EMAIL_OPTIONAL, ARG_ROLE),",
            "    ),",
            "    ActionCommand(",
            "        name='import',",
            "        help='Import users',",
            "        func=lazy_load_command('airflow.cli.commands.user_command.users_import'),",
            "        args=(ARG_USER_IMPORT,),",
            "    ),",
            "    ActionCommand(",
            "        name='export',",
            "        help='Export all users',",
            "        func=lazy_load_command('airflow.cli.commands.user_command.users_export'),",
            "        args=(ARG_USER_EXPORT,),",
            "    ),",
            ")",
            "ROLES_COMMANDS = (",
            "    ActionCommand(",
            "        name='list',",
            "        help='List roles',",
            "        func=lazy_load_command('airflow.cli.commands.role_command.roles_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='create',",
            "        help='Create role',",
            "        func=lazy_load_command('airflow.cli.commands.role_command.roles_create'),",
            "        args=(ARG_ROLES, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='export',",
            "        help='Export roles (without permissions) from db to JSON file',",
            "        func=lazy_load_command('airflow.cli.commands.role_command.roles_export'),",
            "        args=(ARG_ROLE_EXPORT, ARG_ROLE_EXPORT_FMT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='import',",
            "        help='Import roles (without permissions) from JSON file to db',",
            "        func=lazy_load_command('airflow.cli.commands.role_command.roles_import'),",
            "        args=(ARG_ROLE_IMPORT, ARG_VERBOSE),",
            "    ),",
            ")",
            "",
            "CELERY_COMMANDS = (",
            "    ActionCommand(",
            "        name='worker',",
            "        help=\"Start a Celery worker node\",",
            "        func=lazy_load_command('airflow.cli.commands.celery_command.worker'),",
            "        args=(",
            "            ARG_QUEUES,",
            "            ARG_CONCURRENCY,",
            "            ARG_CELERY_HOSTNAME,",
            "            ARG_PID,",
            "            ARG_DAEMON,",
            "            ARG_UMASK,",
            "            ARG_STDOUT,",
            "            ARG_STDERR,",
            "            ARG_LOG_FILE,",
            "            ARG_AUTOSCALE,",
            "            ARG_SKIP_SERVE_LOGS,",
            "            ARG_WITHOUT_MINGLE,",
            "            ARG_WITHOUT_GOSSIP,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='flower',",
            "        help=\"Start a Celery Flower\",",
            "        func=lazy_load_command('airflow.cli.commands.celery_command.flower'),",
            "        args=(",
            "            ARG_FLOWER_HOSTNAME,",
            "            ARG_FLOWER_PORT,",
            "            ARG_FLOWER_CONF,",
            "            ARG_FLOWER_URL_PREFIX,",
            "            ARG_FLOWER_BASIC_AUTH,",
            "            ARG_BROKER_API,",
            "            ARG_PID,",
            "            ARG_DAEMON,",
            "            ARG_STDOUT,",
            "            ARG_STDERR,",
            "            ARG_LOG_FILE,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='stop',",
            "        help=\"Stop the Celery worker gracefully\",",
            "        func=lazy_load_command('airflow.cli.commands.celery_command.stop_worker'),",
            "        args=(ARG_PID,),",
            "    ),",
            ")",
            "",
            "CONFIG_COMMANDS = (",
            "    ActionCommand(",
            "        name='get-value',",
            "        help='Print the value of the configuration',",
            "        func=lazy_load_command('airflow.cli.commands.config_command.get_value'),",
            "        args=(",
            "            ARG_SECTION,",
            "            ARG_OPTION,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='list',",
            "        help='List options for the configuration',",
            "        func=lazy_load_command('airflow.cli.commands.config_command.show_config'),",
            "        args=(ARG_COLOR,),",
            "    ),",
            ")",
            "",
            "KUBERNETES_COMMANDS = (",
            "    ActionCommand(",
            "        name='cleanup-pods',",
            "        help=(",
            "            \"Clean up Kubernetes pods \"",
            "            \"(created by KubernetesExecutor/KubernetesPodOperator) \"",
            "            \"in evicted/failed/succeeded/pending states\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.kubernetes_command.cleanup_pods'),",
            "        args=(ARG_NAMESPACE, ARG_MIN_PENDING_MINUTES),",
            "    ),",
            "    ActionCommand(",
            "        name='generate-dag-yaml',",
            "        help=\"Generate YAML files for all tasks in DAG. Useful for debugging tasks without \"",
            "        \"launching into a cluster\",",
            "        func=lazy_load_command('airflow.cli.commands.kubernetes_command.generate_pod_yaml'),",
            "        args=(ARG_DAG_ID, ARG_EXECUTION_DATE, ARG_SUBDIR, ARG_OUTPUT_PATH),",
            "    ),",
            ")",
            "",
            "JOBS_COMMANDS = (",
            "    ActionCommand(",
            "        name='check',",
            "        help=\"Checks if job(s) are still alive\",",
            "        func=lazy_load_command('airflow.cli.commands.jobs_command.check'),",
            "        args=(ARG_JOB_TYPE_FILTER, ARG_JOB_HOSTNAME_FILTER, ARG_JOB_LIMIT, ARG_ALLOW_MULTIPLE),",
            "        epilog=(",
            "            'examples:\\n'",
            "            'To check if the local scheduler is still working properly, run:\\n'",
            "            '\\n'",
            "            '    $ airflow jobs check --job-type SchedulerJob --hostname \"$(hostname)\"\\n'",
            "            '\\n'",
            "            'To check if any scheduler is running when you are using high availability, run:\\n'",
            "            '\\n'",
            "            '    $ airflow jobs check --job-type SchedulerJob --allow-multiple --limit 100'",
            "        ),",
            "    ),",
            ")",
            "",
            "airflow_commands: List[CLICommand] = [",
            "    GroupCommand(",
            "        name='dags',",
            "        help='Manage DAGs',",
            "        subcommands=DAGS_COMMANDS,",
            "    ),",
            "    GroupCommand(",
            "        name=\"kubernetes\", help='Tools to help run the KubernetesExecutor', subcommands=KUBERNETES_COMMANDS",
            "    ),",
            "    GroupCommand(",
            "        name='tasks',",
            "        help='Manage tasks',",
            "        subcommands=TASKS_COMMANDS,",
            "    ),",
            "    GroupCommand(",
            "        name='pools',",
            "        help=\"Manage pools\",",
            "        subcommands=POOLS_COMMANDS,",
            "    ),",
            "    GroupCommand(",
            "        name='variables',",
            "        help=\"Manage variables\",",
            "        subcommands=VARIABLES_COMMANDS,",
            "    ),",
            "    GroupCommand(",
            "        name='jobs',",
            "        help=\"Manage jobs\",",
            "        subcommands=JOBS_COMMANDS,",
            "    ),",
            "    GroupCommand(",
            "        name='db',",
            "        help=\"Database operations\",",
            "        subcommands=DB_COMMANDS,",
            "    ),",
            "    ActionCommand(",
            "        name='kerberos',",
            "        help=\"Start a kerberos ticket renewer\",",
            "        func=lazy_load_command('airflow.cli.commands.kerberos_command.kerberos'),",
            "        args=(ARG_PRINCIPAL, ARG_KEYTAB, ARG_PID, ARG_DAEMON, ARG_STDOUT, ARG_STDERR, ARG_LOG_FILE),",
            "    ),",
            "    ActionCommand(",
            "        name='webserver',",
            "        help=\"Start a Airflow webserver instance\",",
            "        func=lazy_load_command('airflow.cli.commands.webserver_command.webserver'),",
            "        args=(",
            "            ARG_PORT,",
            "            ARG_WORKERS,",
            "            ARG_WORKERCLASS,",
            "            ARG_WORKER_TIMEOUT,",
            "            ARG_HOSTNAME,",
            "            ARG_PID,",
            "            ARG_DAEMON,",
            "            ARG_STDOUT,",
            "            ARG_STDERR,",
            "            ARG_ACCESS_LOGFILE,",
            "            ARG_ERROR_LOGFILE,",
            "            ARG_ACCESS_LOGFORMAT,",
            "            ARG_LOG_FILE,",
            "            ARG_SSL_CERT,",
            "            ARG_SSL_KEY,",
            "            ARG_DEBUG,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='scheduler',",
            "        help=\"Start a scheduler instance\",",
            "        func=lazy_load_command('airflow.cli.commands.scheduler_command.scheduler'),",
            "        args=(",
            "            ARG_SUBDIR,",
            "            ARG_NUM_RUNS,",
            "            ARG_DO_PICKLE,",
            "            ARG_PID,",
            "            ARG_DAEMON,",
            "            ARG_STDOUT,",
            "            ARG_STDERR,",
            "            ARG_LOG_FILE,",
            "            ARG_SKIP_SERVE_LOGS,",
            "        ),",
            "        epilog=(",
            "            'Signals:\\n'",
            "            '\\n'",
            "            '  - SIGUSR2: Dump a snapshot of task state being tracked by the executor.\\n'",
            "            '\\n'",
            "            '    Example:\\n'",
            "            '        pkill -f -USR2 \"airflow scheduler\"'",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='triggerer',",
            "        help=\"Start a triggerer instance\",",
            "        func=lazy_load_command('airflow.cli.commands.triggerer_command.triggerer'),",
            "        args=(",
            "            ARG_PID,",
            "            ARG_DAEMON,",
            "            ARG_STDOUT,",
            "            ARG_STDERR,",
            "            ARG_LOG_FILE,",
            "            ARG_CAPACITY,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='dag-processor',",
            "        help=\"Start a standalone Dag Processor instance\",",
            "        func=lazy_load_command('airflow.cli.commands.dag_processor_command.dag_processor'),",
            "        args=(",
            "            ARG_PID,",
            "            ARG_DAEMON,",
            "            ARG_SUBDIR,",
            "            ARG_NUM_RUNS,",
            "            ARG_DO_PICKLE,",
            "            ARG_STDOUT,",
            "            ARG_STDERR,",
            "            ARG_LOG_FILE,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='version',",
            "        help=\"Show the version\",",
            "        func=lazy_load_command('airflow.cli.commands.version_command.version'),",
            "        args=(),",
            "    ),",
            "    ActionCommand(",
            "        name='cheat-sheet',",
            "        help=\"Display cheat sheet\",",
            "        func=lazy_load_command('airflow.cli.commands.cheat_sheet_command.cheat_sheet'),",
            "        args=(ARG_VERBOSE,),",
            "    ),",
            "    GroupCommand(",
            "        name='connections',",
            "        help=\"Manage connections\",",
            "        subcommands=CONNECTIONS_COMMANDS,",
            "    ),",
            "    GroupCommand(",
            "        name='providers',",
            "        help=\"Display providers\",",
            "        subcommands=PROVIDERS_COMMANDS,",
            "    ),",
            "    GroupCommand(",
            "        name='users',",
            "        help=\"Manage users\",",
            "        subcommands=USERS_COMMANDS,",
            "    ),",
            "    GroupCommand(",
            "        name='roles',",
            "        help='Manage roles',",
            "        subcommands=ROLES_COMMANDS,",
            "    ),",
            "    ActionCommand(",
            "        name='sync-perm',",
            "        help=\"Update permissions for existing roles and optionally DAGs\",",
            "        func=lazy_load_command('airflow.cli.commands.sync_perm_command.sync_perm'),",
            "        args=(ARG_INCLUDE_DAGS,),",
            "    ),",
            "    ActionCommand(",
            "        name='rotate-fernet-key',",
            "        func=lazy_load_command('airflow.cli.commands.rotate_fernet_key_command.rotate_fernet_key'),",
            "        help='Rotate encrypted connection credentials and variables',",
            "        description=(",
            "            'Rotate all encrypted connection credentials and variables; see '",
            "            'https://airflow.apache.org/docs/apache-airflow/stable/howto/secure-connections.html'",
            "            '#rotating-encryption-keys'",
            "        ),",
            "        args=(),",
            "    ),",
            "    GroupCommand(name=\"config\", help='View configuration', subcommands=CONFIG_COMMANDS),",
            "    ActionCommand(",
            "        name='info',",
            "        help='Show information about current Airflow and environment',",
            "        func=lazy_load_command('airflow.cli.commands.info_command.show_info'),",
            "        args=(",
            "            ARG_ANONYMIZE,",
            "            ARG_FILE_IO,",
            "            ARG_VERBOSE,",
            "            ARG_OUTPUT,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='plugins',",
            "        help='Dump information about loaded plugins',",
            "        func=lazy_load_command('airflow.cli.commands.plugins_command.dump_plugins'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    GroupCommand(",
            "        name=\"celery\",",
            "        help='Celery components',",
            "        description=(",
            "            'Start celery components. Works only when using CeleryExecutor. For more information, see '",
            "            'https://airflow.apache.org/docs/apache-airflow/stable/executor/celery.html'",
            "        ),",
            "        subcommands=CELERY_COMMANDS,",
            "    ),",
            "    ActionCommand(",
            "        name='standalone',",
            "        help='Run an all-in-one copy of Airflow',",
            "        func=lazy_load_command('airflow.cli.commands.standalone_command.standalone'),",
            "        args=tuple(),",
            "    ),",
            "]",
            "ALL_COMMANDS_DICT: Dict[str, CLICommand] = {sp.name: sp for sp in airflow_commands}",
            "",
            "",
            "def _remove_dag_id_opt(command: ActionCommand):",
            "    cmd = command._asdict()",
            "    cmd['args'] = (arg for arg in command.args if arg is not ARG_DAG_ID)",
            "    return ActionCommand(**cmd)",
            "",
            "",
            "dag_cli_commands: List[CLICommand] = [",
            "    GroupCommand(",
            "        name='dags',",
            "        help='Manage DAGs',",
            "        subcommands=[",
            "            _remove_dag_id_opt(sp)",
            "            for sp in DAGS_COMMANDS",
            "            if sp.name in ['backfill', 'list-runs', 'pause', 'unpause']",
            "        ],",
            "    ),",
            "    GroupCommand(",
            "        name='tasks',",
            "        help='Manage tasks',",
            "        subcommands=[_remove_dag_id_opt(sp) for sp in TASKS_COMMANDS if sp.name in ['list', 'test', 'run']],",
            "    ),",
            "]",
            "DAG_CLI_DICT: Dict[str, CLICommand] = {sp.name: sp for sp in dag_cli_commands}",
            "",
            "",
            "class AirflowHelpFormatter(argparse.HelpFormatter):",
            "    \"\"\"",
            "    Custom help formatter to display help message.",
            "",
            "    It displays simple commands and groups of commands in separate sections.",
            "    \"\"\"",
            "",
            "    def _format_action(self, action: Action):",
            "        if isinstance(action, argparse._SubParsersAction):",
            "",
            "            parts = []",
            "            action_header = self._format_action_invocation(action)",
            "            action_header = '%*s%s\\n' % (self._current_indent, '', action_header)",
            "            parts.append(action_header)",
            "",
            "            self._indent()",
            "            subactions = action._get_subactions()",
            "            action_subcommands, group_subcommands = partition(",
            "                lambda d: isinstance(ALL_COMMANDS_DICT[d.dest], GroupCommand), subactions",
            "            )",
            "            parts.append(\"\\n\")",
            "            parts.append('%*s%s:\\n' % (self._current_indent, '', \"Groups\"))",
            "            self._indent()",
            "            for subaction in group_subcommands:",
            "                parts.append(self._format_action(subaction))",
            "            self._dedent()",
            "",
            "            parts.append(\"\\n\")",
            "            parts.append('%*s%s:\\n' % (self._current_indent, '', \"Commands\"))",
            "            self._indent()",
            "",
            "            for subaction in action_subcommands:",
            "                parts.append(self._format_action(subaction))",
            "            self._dedent()",
            "            self._dedent()",
            "",
            "            # return a single string",
            "            return self._join_parts(parts)",
            "",
            "        return super()._format_action(action)",
            "",
            "",
            "@lru_cache(maxsize=None)",
            "def get_parser(dag_parser: bool = False) -> argparse.ArgumentParser:",
            "    \"\"\"Creates and returns command line argument parser\"\"\"",
            "    parser = DefaultHelpParser(prog=\"airflow\", formatter_class=AirflowHelpFormatter)",
            "    subparsers = parser.add_subparsers(dest='subcommand', metavar=\"GROUP_OR_COMMAND\")",
            "    subparsers.required = True",
            "",
            "    command_dict = DAG_CLI_DICT if dag_parser else ALL_COMMANDS_DICT",
            "    subparser_list = command_dict.keys()",
            "    sub_name: str",
            "    for sub_name in sorted(subparser_list):",
            "        sub: CLICommand = command_dict[sub_name]",
            "        _add_command(subparsers, sub)",
            "    return parser",
            "",
            "",
            "def _sort_args(args: Iterable[Arg]) -> Iterable[Arg]:",
            "    \"\"\"Sort subcommand optional args, keep positional args\"\"\"",
            "",
            "    def get_long_option(arg: Arg):",
            "        \"\"\"Get long option from Arg.flags\"\"\"",
            "        return arg.flags[0] if len(arg.flags) == 1 else arg.flags[1]",
            "",
            "    positional, optional = partition(lambda x: x.flags[0].startswith(\"-\"), args)",
            "    yield from positional",
            "    yield from sorted(optional, key=lambda x: get_long_option(x).lower())",
            "",
            "",
            "def _add_command(subparsers: argparse._SubParsersAction, sub: CLICommand) -> None:",
            "    sub_proc = subparsers.add_parser(",
            "        sub.name, help=sub.help, description=sub.description or sub.help, epilog=sub.epilog",
            "    )",
            "    sub_proc.formatter_class = RawTextHelpFormatter",
            "",
            "    if isinstance(sub, GroupCommand):",
            "        _add_group_command(sub, sub_proc)",
            "    elif isinstance(sub, ActionCommand):",
            "        _add_action_command(sub, sub_proc)",
            "    else:",
            "        raise AirflowException(\"Invalid command definition.\")",
            "",
            "",
            "def _add_action_command(sub: ActionCommand, sub_proc: argparse.ArgumentParser) -> None:",
            "    for arg in _sort_args(sub.args):",
            "        arg.add_to_parser(sub_proc)",
            "    sub_proc.set_defaults(func=sub.func)",
            "",
            "",
            "def _add_group_command(sub: GroupCommand, sub_proc: argparse.ArgumentParser) -> None:",
            "    subcommands = sub.subcommands",
            "    sub_subparsers = sub_proc.add_subparsers(dest=\"subcommand\", metavar=\"COMMAND\")",
            "    sub_subparsers.required = True",
            "",
            "    for command in sorted(subcommands, key=lambda x: x.name):",
            "        _add_command(sub_subparsers, command)"
        ],
        "afterPatchFile": [
            "#!/usr/bin/env python",
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"Command-line interface\"\"\"",
            "",
            "import argparse",
            "import json",
            "import os",
            "import textwrap",
            "from argparse import Action, ArgumentError, RawTextHelpFormatter",
            "from functools import lru_cache",
            "from typing import Callable, Dict, Iterable, List, NamedTuple, Optional, Union",
            "",
            "import lazy_object_proxy",
            "",
            "from airflow import settings",
            "from airflow.cli.commands.legacy_commands import check_legacy_command",
            "from airflow.configuration import conf",
            "from airflow.exceptions import AirflowException",
            "from airflow.executors.executor_constants import CELERY_EXECUTOR, CELERY_KUBERNETES_EXECUTOR",
            "from airflow.executors.executor_loader import ExecutorLoader",
            "from airflow.utils.cli import ColorMode",
            "from airflow.utils.helpers import partition",
            "from airflow.utils.module_loading import import_string",
            "from airflow.utils.timezone import parse as parsedate",
            "",
            "BUILD_DOCS = \"BUILDING_AIRFLOW_DOCS\" in os.environ",
            "",
            "",
            "def lazy_load_command(import_path: str) -> Callable:",
            "    \"\"\"Create a lazy loader for command\"\"\"",
            "    _, _, name = import_path.rpartition('.')",
            "",
            "    def command(*args, **kwargs):",
            "        func = import_string(import_path)",
            "        return func(*args, **kwargs)",
            "",
            "    command.__name__ = name",
            "",
            "    return command",
            "",
            "",
            "class DefaultHelpParser(argparse.ArgumentParser):",
            "    \"\"\"CustomParser to display help message\"\"\"",
            "",
            "    def _check_value(self, action, value):",
            "        \"\"\"Override _check_value and check conditionally added command\"\"\"",
            "        if action.dest == 'subcommand' and value == 'celery':",
            "            executor = conf.get('core', 'EXECUTOR')",
            "            if executor not in (CELERY_EXECUTOR, CELERY_KUBERNETES_EXECUTOR):",
            "                executor_cls, _ = ExecutorLoader.import_executor_cls(executor)",
            "                classes = ()",
            "                try:",
            "                    from airflow.executors.celery_executor import CeleryExecutor",
            "",
            "                    classes += (CeleryExecutor,)",
            "                except ImportError:",
            "                    message = (",
            "                        \"The celery subcommand requires that you pip install the celery module. \"",
            "                        \"To do it, run: pip install 'apache-airflow[celery]'\"",
            "                    )",
            "                    raise ArgumentError(action, message)",
            "                try:",
            "                    from airflow.executors.celery_kubernetes_executor import CeleryKubernetesExecutor",
            "",
            "                    classes += (CeleryKubernetesExecutor,)",
            "                except ImportError:",
            "                    pass",
            "                if not issubclass(executor_cls, classes):",
            "                    message = (",
            "                        f'celery subcommand works only with CeleryExecutor, CeleryKubernetesExecutor and '",
            "                        f'executors derived from them, your current executor: {executor}, subclassed from: '",
            "                        f'{\", \".join([base_cls.__qualname__ for base_cls in executor_cls.__bases__])}'",
            "                    )",
            "                    raise ArgumentError(action, message)",
            "        if action.dest == 'subcommand' and value == 'kubernetes':",
            "            try:",
            "                import kubernetes.client  # noqa: F401",
            "            except ImportError:",
            "                message = (",
            "                    \"The kubernetes subcommand requires that you pip install the kubernetes python client. \"",
            "                    \"To do it, run: pip install 'apache-airflow[cncf.kubernetes]'\"",
            "                )",
            "                raise ArgumentError(action, message)",
            "",
            "        if action.choices is not None and value not in action.choices:",
            "            check_legacy_command(action, value)",
            "",
            "        super()._check_value(action, value)",
            "",
            "    def error(self, message):",
            "        \"\"\"Override error and use print_instead of print_usage\"\"\"",
            "        self.print_help()",
            "        self.exit(2, f'\\n{self.prog} command error: {message}, see help above.\\n')",
            "",
            "",
            "# Used in Arg to enable `None' as a distinct value from \"not passed\"",
            "_UNSET = object()",
            "",
            "",
            "class Arg:",
            "    \"\"\"Class to keep information about command line argument\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        flags=_UNSET,",
            "        help=_UNSET,",
            "        action=_UNSET,",
            "        default=_UNSET,",
            "        nargs=_UNSET,",
            "        type=_UNSET,",
            "        choices=_UNSET,",
            "        required=_UNSET,",
            "        metavar=_UNSET,",
            "        dest=_UNSET,",
            "    ):",
            "        self.flags = flags",
            "        self.kwargs = {}",
            "        for k, v in locals().items():",
            "            if v is _UNSET:",
            "                continue",
            "            if k in (\"self\", \"flags\"):",
            "                continue",
            "",
            "            self.kwargs[k] = v",
            "",
            "    def add_to_parser(self, parser: argparse.ArgumentParser):",
            "        \"\"\"Add this argument to an ArgumentParser\"\"\"",
            "        parser.add_argument(*self.flags, **self.kwargs)",
            "",
            "",
            "def positive_int(*, allow_zero):",
            "    \"\"\"Define a positive int type for an argument.\"\"\"",
            "",
            "    def _check(value):",
            "        try:",
            "            value = int(value)",
            "            if allow_zero and value == 0:",
            "                return value",
            "            if value > 0:",
            "                return value",
            "        except ValueError:",
            "            pass",
            "        raise argparse.ArgumentTypeError(f\"invalid positive int value: '{value}'\")",
            "",
            "    return _check",
            "",
            "",
            "def string_list_type(val):",
            "    \"\"\"Parses comma-separated list and returns list of string (strips whitespace)\"\"\"",
            "    return [x.strip() for x in val.split(',')]",
            "",
            "",
            "def string_lower_type(val):",
            "    \"\"\"Lowers arg\"\"\"",
            "    if not val:",
            "        return",
            "    return val.strip().lower()",
            "",
            "",
            "# Shared",
            "ARG_DAG_ID = Arg((\"dag_id\",), help=\"The id of the dag\")",
            "ARG_TASK_ID = Arg((\"task_id\",), help=\"The id of the task\")",
            "ARG_EXECUTION_DATE = Arg((\"execution_date\",), help=\"The execution date of the DAG\", type=parsedate)",
            "ARG_EXECUTION_DATE_OR_RUN_ID = Arg(",
            "    ('execution_date_or_run_id',), help=\"The execution_date of the DAG or run_id of the DAGRun\"",
            ")",
            "ARG_TASK_REGEX = Arg(",
            "    (\"-t\", \"--task-regex\"), help=\"The regex to filter specific task_ids to backfill (optional)\"",
            ")",
            "ARG_SUBDIR = Arg(",
            "    (\"-S\", \"--subdir\"),",
            "    help=(",
            "        \"File location or directory from which to look for the dag. \"",
            "        \"Defaults to '[AIRFLOW_HOME]/dags' where [AIRFLOW_HOME] is the \"",
            "        \"value you set for 'AIRFLOW_HOME' config you set in 'airflow.cfg' \"",
            "    ),",
            "    default='[AIRFLOW_HOME]/dags' if BUILD_DOCS else settings.DAGS_FOLDER,",
            ")",
            "ARG_START_DATE = Arg((\"-s\", \"--start-date\"), help=\"Override start_date YYYY-MM-DD\", type=parsedate)",
            "ARG_END_DATE = Arg((\"-e\", \"--end-date\"), help=\"Override end_date YYYY-MM-DD\", type=parsedate)",
            "ARG_OUTPUT_PATH = Arg(",
            "    (",
            "        \"-o\",",
            "        \"--output-path\",",
            "    ),",
            "    help=\"The output for generated yaml files\",",
            "    type=str,",
            "    default=\"[CWD]\" if BUILD_DOCS else os.getcwd(),",
            ")",
            "ARG_DRY_RUN = Arg(",
            "    (\"-n\", \"--dry-run\"),",
            "    help=\"Perform a dry run for each task. Only renders Template Fields for each task, nothing else\",",
            "    action=\"store_true\",",
            ")",
            "ARG_PID = Arg((\"--pid\",), help=\"PID file location\", nargs='?')",
            "ARG_DAEMON = Arg(",
            "    (\"-D\", \"--daemon\"), help=\"Daemonize instead of running in the foreground\", action=\"store_true\"",
            ")",
            "ARG_STDERR = Arg((\"--stderr\",), help=\"Redirect stderr to this file\")",
            "ARG_STDOUT = Arg((\"--stdout\",), help=\"Redirect stdout to this file\")",
            "ARG_LOG_FILE = Arg((\"-l\", \"--log-file\"), help=\"Location of the log file\")",
            "ARG_YES = Arg(",
            "    (\"-y\", \"--yes\"),",
            "    help=\"Do not prompt to confirm. Use with care!\",",
            "    action=\"store_true\",",
            "    default=False,",
            ")",
            "ARG_OUTPUT = Arg(",
            "    (",
            "        \"-o\",",
            "        \"--output\",",
            "    ),",
            "    help=\"Output format. Allowed values: json, yaml, plain, table (default: table)\",",
            "    metavar=\"(table, json, yaml, plain)\",",
            "    choices=(\"table\", \"json\", \"yaml\", \"plain\"),",
            "    default=\"table\",",
            ")",
            "ARG_COLOR = Arg(",
            "    ('--color',),",
            "    help=\"Do emit colored output (default: auto)\",",
            "    choices={ColorMode.ON, ColorMode.OFF, ColorMode.AUTO},",
            "    default=ColorMode.AUTO,",
            ")",
            "",
            "# DB args",
            "ARG_VERSION_RANGE = Arg(",
            "    (\"-r\", \"--range\"),",
            "    help=\"Version range(start:end) for offline sql generation. Example: '2.0.2:2.2.3'\",",
            "    default=None,",
            ")",
            "ARG_REVISION_RANGE = Arg(",
            "    ('--revision-range',),",
            "    help=(",
            "        \"Migration revision range(start:end) to use for offline sql generation. \"",
            "        \"Example: ``a13f7613ad25:7b2661a43ba3``\"",
            "    ),",
            "    default=None,",
            ")",
            "",
            "# list_dag_runs",
            "ARG_DAG_ID_OPT = Arg((\"-d\", \"--dag-id\"), help=\"The id of the dag\")",
            "ARG_NO_BACKFILL = Arg(",
            "    (\"--no-backfill\",), help=\"filter all the backfill dagruns given the dag id\", action=\"store_true\"",
            ")",
            "ARG_STATE = Arg((\"--state\",), help=\"Only list the dag runs corresponding to the state\")",
            "",
            "# list_jobs",
            "ARG_LIMIT = Arg((\"--limit\",), help=\"Return a limited number of records\")",
            "",
            "# next_execution",
            "ARG_NUM_EXECUTIONS = Arg(",
            "    (\"-n\", \"--num-executions\"),",
            "    default=1,",
            "    type=positive_int(allow_zero=False),",
            "    help=\"The number of next execution datetimes to show\",",
            ")",
            "",
            "# backfill",
            "ARG_MARK_SUCCESS = Arg(",
            "    (\"-m\", \"--mark-success\"), help=\"Mark jobs as succeeded without running them\", action=\"store_true\"",
            ")",
            "ARG_VERBOSE = Arg((\"-v\", \"--verbose\"), help=\"Make logging output more verbose\", action=\"store_true\")",
            "ARG_LOCAL = Arg((\"-l\", \"--local\"), help=\"Run the task using the LocalExecutor\", action=\"store_true\")",
            "ARG_DONOT_PICKLE = Arg(",
            "    (\"-x\", \"--donot-pickle\"),",
            "    help=(",
            "        \"Do not attempt to pickle the DAG object to send over \"",
            "        \"to the workers, just tell the workers to run their version \"",
            "        \"of the code\"",
            "    ),",
            "    action=\"store_true\",",
            ")",
            "ARG_BF_IGNORE_DEPENDENCIES = Arg(",
            "    (\"-i\", \"--ignore-dependencies\"),",
            "    help=(",
            "        \"Skip upstream tasks, run only the tasks \"",
            "        \"matching the regexp. Only works in conjunction \"",
            "        \"with task_regex\"",
            "    ),",
            "    action=\"store_true\",",
            ")",
            "ARG_BF_IGNORE_FIRST_DEPENDS_ON_PAST = Arg(",
            "    (\"-I\", \"--ignore-first-depends-on-past\"),",
            "    help=(",
            "        \"Ignores depends_on_past dependencies for the first \"",
            "        \"set of tasks only (subsequent executions in the backfill \"",
            "        \"DO respect depends_on_past)\"",
            "    ),",
            "    action=\"store_true\",",
            ")",
            "ARG_POOL = Arg((\"--pool\",), \"Resource pool to use\")",
            "ARG_DELAY_ON_LIMIT = Arg(",
            "    (\"--delay-on-limit\",),",
            "    help=(",
            "        \"Amount of time in seconds to wait when the limit \"",
            "        \"on maximum active dag runs (max_active_runs) has \"",
            "        \"been reached before trying to execute a dag run \"",
            "        \"again\"",
            "    ),",
            "    type=float,",
            "    default=1.0,",
            ")",
            "ARG_RESET_DAG_RUN = Arg(",
            "    (\"--reset-dagruns\",),",
            "    help=(",
            "        \"if set, the backfill will delete existing \"",
            "        \"backfill-related DAG runs and start \"",
            "        \"anew with fresh, running DAG runs\"",
            "    ),",
            "    action=\"store_true\",",
            ")",
            "ARG_RERUN_FAILED_TASKS = Arg(",
            "    (\"--rerun-failed-tasks\",),",
            "    help=(",
            "        \"if set, the backfill will auto-rerun \"",
            "        \"all the failed tasks for the backfill date range \"",
            "        \"instead of throwing exceptions\"",
            "    ),",
            "    action=\"store_true\",",
            ")",
            "ARG_CONTINUE_ON_FAILURES = Arg(",
            "    (\"--continue-on-failures\",),",
            "    help=(\"if set, the backfill will keep going even if some of the tasks failed\"),",
            "    action=\"store_true\",",
            ")",
            "ARG_RUN_BACKWARDS = Arg(",
            "    (",
            "        \"-B\",",
            "        \"--run-backwards\",",
            "    ),",
            "    help=(",
            "        \"if set, the backfill will run tasks from the most \"",
            "        \"recent day first.  if there are tasks that depend_on_past \"",
            "        \"this option will throw an exception\"",
            "    ),",
            "    action=\"store_true\",",
            ")",
            "# test_dag",
            "ARG_SHOW_DAGRUN = Arg(",
            "    (\"--show-dagrun\",),",
            "    help=(",
            "        \"After completing the backfill, shows the diagram for current DAG Run.\\n\"",
            "        \"\\n\"",
            "        \"The diagram is in DOT language\\n\"",
            "    ),",
            "    action='store_true',",
            ")",
            "ARG_IMGCAT_DAGRUN = Arg(",
            "    (\"--imgcat-dagrun\",),",
            "    help=(",
            "        \"After completing the dag run, prints a diagram on the screen for the \"",
            "        \"current DAG Run using the imgcat tool.\\n\"",
            "    ),",
            "    action='store_true',",
            ")",
            "ARG_SAVE_DAGRUN = Arg(",
            "    (\"--save-dagrun\",),",
            "    help=\"After completing the backfill, saves the diagram for current DAG Run to the indicated file.\\n\\n\",",
            ")",
            "",
            "# list_tasks",
            "ARG_TREE = Arg((\"-t\", \"--tree\"), help=\"Tree view\", action=\"store_true\")",
            "",
            "# tasks_run",
            "# This is a hidden option -- not meant for users to set or know about",
            "ARG_SHUT_DOWN_LOGGING = Arg(",
            "    (\"--no-shut-down-logging\",),",
            "    help=argparse.SUPPRESS,",
            "    dest=\"shut_down_logging\",",
            "    action=\"store_false\",",
            "    default=True,",
            ")",
            "",
            "# clear",
            "ARG_UPSTREAM = Arg((\"-u\", \"--upstream\"), help=\"Include upstream tasks\", action=\"store_true\")",
            "ARG_ONLY_FAILED = Arg((\"-f\", \"--only-failed\"), help=\"Only failed jobs\", action=\"store_true\")",
            "ARG_ONLY_RUNNING = Arg((\"-r\", \"--only-running\"), help=\"Only running jobs\", action=\"store_true\")",
            "ARG_DOWNSTREAM = Arg((\"-d\", \"--downstream\"), help=\"Include downstream tasks\", action=\"store_true\")",
            "ARG_EXCLUDE_SUBDAGS = Arg((\"-x\", \"--exclude-subdags\"), help=\"Exclude subdags\", action=\"store_true\")",
            "ARG_EXCLUDE_PARENTDAG = Arg(",
            "    (\"-X\", \"--exclude-parentdag\"),",
            "    help=\"Exclude ParentDAGS if the task cleared is a part of a SubDAG\",",
            "    action=\"store_true\",",
            ")",
            "ARG_DAG_REGEX = Arg(",
            "    (\"-R\", \"--dag-regex\"), help=\"Search dag_id as regex instead of exact string\", action=\"store_true\"",
            ")",
            "",
            "# show_dag",
            "ARG_SAVE = Arg((\"-s\", \"--save\"), help=\"Saves the result to the indicated file.\")",
            "",
            "ARG_IMGCAT = Arg((\"--imgcat\",), help=\"Displays graph using the imgcat tool.\", action='store_true')",
            "",
            "# trigger_dag",
            "ARG_RUN_ID = Arg((\"-r\", \"--run-id\"), help=\"Helps to identify this run\")",
            "ARG_CONF = Arg(('-c', '--conf'), help=\"JSON string that gets pickled into the DagRun's conf attribute\")",
            "ARG_EXEC_DATE = Arg((\"-e\", \"--exec-date\"), help=\"The execution date of the DAG\", type=parsedate)",
            "",
            "# db",
            "ARG_DB_TABLES = Arg(",
            "    (\"-t\", \"--tables\"),",
            "    help=lazy_object_proxy.Proxy(",
            "        lambda: f\"Table names to perform maintenance on (use comma-separated list).\\n\"",
            "        f\"Options: {import_string('airflow.cli.commands.db_command.all_tables')}\"",
            "    ),",
            "    type=string_list_type,",
            ")",
            "ARG_DB_CLEANUP_TIMESTAMP = Arg(",
            "    (\"--clean-before-timestamp\",),",
            "    help=\"The date or timestamp before which data should be purged.\\n\"",
            "    \"If no timezone info is supplied then dates are assumed to be in airflow default timezone.\\n\"",
            "    \"Example: '2022-01-01 00:00:00+01:00'\",",
            "    type=parsedate,",
            "    required=True,",
            ")",
            "ARG_DB_DRY_RUN = Arg(",
            "    (\"--dry-run\",),",
            "    help=\"Perform a dry run\",",
            "    action=\"store_true\",",
            ")",
            "ARG_DB_SKIP_ARCHIVE = Arg(",
            "    (\"--skip-archive\",),",
            "    help=\"Don't preserve purged records in an archive table.\",",
            "    action=\"store_true\",",
            ")",
            "",
            "",
            "# pool",
            "ARG_POOL_NAME = Arg((\"pool\",), metavar='NAME', help=\"Pool name\")",
            "ARG_POOL_SLOTS = Arg((\"slots\",), type=int, help=\"Pool slots\")",
            "ARG_POOL_DESCRIPTION = Arg((\"description\",), help=\"Pool description\")",
            "ARG_POOL_IMPORT = Arg(",
            "    (\"file\",),",
            "    metavar=\"FILEPATH\",",
            "    help=\"Import pools from JSON file. Example format::\\n\"",
            "    + textwrap.indent(",
            "        textwrap.dedent(",
            "            '''",
            "            {",
            "                \"pool_1\": {\"slots\": 5, \"description\": \"\"},",
            "                \"pool_2\": {\"slots\": 10, \"description\": \"test\"}",
            "            }'''",
            "        ),",
            "        \" \" * 4,",
            "    ),",
            ")",
            "",
            "ARG_POOL_EXPORT = Arg((\"file\",), metavar=\"FILEPATH\", help=\"Export all pools to JSON file\")",
            "",
            "# variables",
            "ARG_VAR = Arg((\"key\",), help=\"Variable key\")",
            "ARG_VAR_VALUE = Arg((\"value\",), metavar='VALUE', help=\"Variable value\")",
            "ARG_DEFAULT = Arg(",
            "    (\"-d\", \"--default\"), metavar=\"VAL\", default=None, help=\"Default value returned if variable does not exist\"",
            ")",
            "ARG_JSON = Arg((\"-j\", \"--json\"), help=\"Deserialize JSON variable\", action=\"store_true\")",
            "ARG_VAR_IMPORT = Arg((\"file\",), help=\"Import variables from JSON file\")",
            "ARG_VAR_EXPORT = Arg((\"file\",), help=\"Export all variables to JSON file\")",
            "",
            "# kerberos",
            "ARG_PRINCIPAL = Arg((\"principal\",), help=\"kerberos principal\", nargs='?')",
            "ARG_KEYTAB = Arg((\"-k\", \"--keytab\"), help=\"keytab\", nargs='?', default=conf.get('kerberos', 'keytab'))",
            "# run",
            "ARG_INTERACTIVE = Arg(",
            "    ('-N', '--interactive'),",
            "    help='Do not capture standard output and error streams (useful for interactive debugging)',",
            "    action='store_true',",
            ")",
            "# TODO(aoen): \"force\" is a poor choice of name here since it implies it overrides",
            "# all dependencies (not just past success), e.g. the ignore_depends_on_past",
            "# dependency. This flag should be deprecated and renamed to 'ignore_ti_state' and",
            "# the \"ignore_all_dependencies\" command should be called the\"force\" command",
            "# instead.",
            "ARG_FORCE = Arg(",
            "    (\"-f\", \"--force\"),",
            "    help=\"Ignore previous task instance state, rerun regardless if task already succeeded/failed\",",
            "    action=\"store_true\",",
            ")",
            "ARG_RAW = Arg((\"-r\", \"--raw\"), argparse.SUPPRESS, \"store_true\")",
            "ARG_IGNORE_ALL_DEPENDENCIES = Arg(",
            "    (\"-A\", \"--ignore-all-dependencies\"),",
            "    help=\"Ignores all non-critical dependencies, including ignore_ti_state and ignore_task_deps\",",
            "    action=\"store_true\",",
            ")",
            "# TODO(aoen): ignore_dependencies is a poor choice of name here because it is too",
            "# vague (e.g. a task being in the appropriate state to be run is also a dependency",
            "# but is not ignored by this flag), the name 'ignore_task_dependencies' is",
            "# slightly better (as it ignores all dependencies that are specific to the task),",
            "# so deprecate the old command name and use this instead.",
            "ARG_IGNORE_DEPENDENCIES = Arg(",
            "    (\"-i\", \"--ignore-dependencies\"),",
            "    help=\"Ignore task-specific dependencies, e.g. upstream, depends_on_past, and retry delay dependencies\",",
            "    action=\"store_true\",",
            ")",
            "ARG_IGNORE_DEPENDS_ON_PAST = Arg(",
            "    (\"-I\", \"--ignore-depends-on-past\"),",
            "    help=\"Ignore depends_on_past dependencies (but respect upstream dependencies)\",",
            "    action=\"store_true\",",
            ")",
            "ARG_SHIP_DAG = Arg(",
            "    (\"--ship-dag\",), help=\"Pickles (serializes) the DAG and ships it to the worker\", action=\"store_true\"",
            ")",
            "ARG_PICKLE = Arg((\"-p\", \"--pickle\"), help=\"Serialized pickle object of the entire dag (used internally)\")",
            "ARG_ERROR_FILE = Arg((\"--error-file\",), help=\"File to store task failure error\")",
            "ARG_JOB_ID = Arg((\"-j\", \"--job-id\"), help=argparse.SUPPRESS)",
            "ARG_CFG_PATH = Arg((\"--cfg-path\",), help=\"Path to config file to use instead of airflow.cfg\")",
            "ARG_MAP_INDEX = Arg(('--map-index',), type=int, default=-1, help=\"Mapped task index\")",
            "",
            "",
            "# database",
            "ARG_MIGRATION_TIMEOUT = Arg(",
            "    (\"-t\", \"--migration-wait-timeout\"),",
            "    help=\"timeout to wait for db to migrate \",",
            "    type=int,",
            "    default=60,",
            ")",
            "ARG_DB_VERSION__UPGRADE = Arg(",
            "    (\"-n\", \"--to-version\"),",
            "    help=(",
            "        \"(Optional) The airflow version to upgrade to. Note: must provide either \"",
            "        \"`--to-revision` or `--to-version`.\"",
            "    ),",
            ")",
            "ARG_DB_REVISION__UPGRADE = Arg(",
            "    (\"-r\", \"--to-revision\"),",
            "    help=\"(Optional) If provided, only run migrations up to and including this Alembic revision.\",",
            ")",
            "ARG_DB_VERSION__DOWNGRADE = Arg(",
            "    (\"-n\", \"--to-version\"),",
            "    help=\"(Optional) If provided, only run migrations up to this version.\",",
            ")",
            "ARG_DB_FROM_VERSION = Arg(",
            "    (\"--from-version\",),",
            "    help=\"(Optional) If generating sql, may supply a *from* version\",",
            ")",
            "ARG_DB_REVISION__DOWNGRADE = Arg(",
            "    (\"-r\", \"--to-revision\"),",
            "    help=\"The Alembic revision to downgrade to. Note: must provide either `--to-revision` or `--to-version`.\",",
            ")",
            "ARG_DB_FROM_REVISION = Arg(",
            "    (\"--from-revision\",),",
            "    help=\"(Optional) If generating sql, may supply a *from* Alembic revision\",",
            ")",
            "ARG_DB_SQL_ONLY = Arg(",
            "    (\"-s\", \"--show-sql-only\"),",
            "    help=\"Don't actually run migrations; just print out sql scripts for offline migration. \"",
            "    \"Required if using either `--from-version` or `--from-version`.\",",
            "    action=\"store_true\",",
            "    default=False,",
            ")",
            "ARG_DB_SKIP_INIT = Arg(",
            "    (\"-s\", \"--skip-init\"),",
            "    help=\"Only remove tables; do not perform db init.\",",
            "    action=\"store_true\",",
            "    default=False,",
            ")",
            "",
            "# webserver",
            "ARG_PORT = Arg(",
            "    (\"-p\", \"--port\"),",
            "    default=conf.get('webserver', 'WEB_SERVER_PORT'),",
            "    type=int,",
            "    help=\"The port on which to run the server\",",
            ")",
            "ARG_SSL_CERT = Arg(",
            "    (\"--ssl-cert\",),",
            "    default=conf.get('webserver', 'WEB_SERVER_SSL_CERT'),",
            "    help=\"Path to the SSL certificate for the webserver\",",
            ")",
            "ARG_SSL_KEY = Arg(",
            "    (\"--ssl-key\",),",
            "    default=conf.get('webserver', 'WEB_SERVER_SSL_KEY'),",
            "    help=\"Path to the key to use with the SSL certificate\",",
            ")",
            "ARG_WORKERS = Arg(",
            "    (\"-w\", \"--workers\"),",
            "    default=conf.get('webserver', 'WORKERS'),",
            "    type=int,",
            "    help=\"Number of workers to run the webserver on\",",
            ")",
            "ARG_WORKERCLASS = Arg(",
            "    (\"-k\", \"--workerclass\"),",
            "    default=conf.get('webserver', 'WORKER_CLASS'),",
            "    choices=['sync', 'eventlet', 'gevent', 'tornado'],",
            "    help=\"The worker class to use for Gunicorn\",",
            ")",
            "ARG_WORKER_TIMEOUT = Arg(",
            "    (\"-t\", \"--worker-timeout\"),",
            "    default=conf.get('webserver', 'WEB_SERVER_WORKER_TIMEOUT'),",
            "    type=int,",
            "    help=\"The timeout for waiting on webserver workers\",",
            ")",
            "ARG_HOSTNAME = Arg(",
            "    (\"-H\", \"--hostname\"),",
            "    default=conf.get('webserver', 'WEB_SERVER_HOST'),",
            "    help=\"Set the hostname on which to run the web server\",",
            ")",
            "ARG_DEBUG = Arg(",
            "    (\"-d\", \"--debug\"), help=\"Use the server that ships with Flask in debug mode\", action=\"store_true\"",
            ")",
            "ARG_ACCESS_LOGFILE = Arg(",
            "    (\"-A\", \"--access-logfile\"),",
            "    default=conf.get('webserver', 'ACCESS_LOGFILE'),",
            "    help=\"The logfile to store the webserver access log. Use '-' to print to stderr\",",
            ")",
            "ARG_ERROR_LOGFILE = Arg(",
            "    (\"-E\", \"--error-logfile\"),",
            "    default=conf.get('webserver', 'ERROR_LOGFILE'),",
            "    help=\"The logfile to store the webserver error log. Use '-' to print to stderr\",",
            ")",
            "ARG_ACCESS_LOGFORMAT = Arg(",
            "    (\"-L\", \"--access-logformat\"),",
            "    default=conf.get('webserver', 'ACCESS_LOGFORMAT'),",
            "    help=\"The access log format for gunicorn logs\",",
            ")",
            "",
            "# scheduler",
            "ARG_NUM_RUNS = Arg(",
            "    (\"-n\", \"--num-runs\"),",
            "    default=conf.getint('scheduler', 'num_runs'),",
            "    type=int,",
            "    help=\"Set the number of runs to execute before exiting\",",
            ")",
            "ARG_DO_PICKLE = Arg(",
            "    (\"-p\", \"--do-pickle\"),",
            "    default=False,",
            "    help=(",
            "        \"Attempt to pickle the DAG object to send over \"",
            "        \"to the workers, instead of letting workers run their version \"",
            "        \"of the code\"",
            "    ),",
            "    action=\"store_true\",",
            ")",
            "",
            "# worker",
            "ARG_QUEUES = Arg(",
            "    (\"-q\", \"--queues\"),",
            "    help=\"Comma delimited list of queues to serve\",",
            "    default=conf.get('operators', 'DEFAULT_QUEUE'),",
            ")",
            "ARG_CONCURRENCY = Arg(",
            "    (\"-c\", \"--concurrency\"),",
            "    type=int,",
            "    help=\"The number of worker processes\",",
            "    default=conf.get('celery', 'worker_concurrency'),",
            ")",
            "ARG_CELERY_HOSTNAME = Arg(",
            "    (\"-H\", \"--celery-hostname\"),",
            "    help=\"Set the hostname of celery worker if you have multiple workers on a single machine\",",
            ")",
            "ARG_UMASK = Arg(",
            "    (\"-u\", \"--umask\"),",
            "    help=\"Set the umask of celery worker in daemon mode\",",
            ")",
            "ARG_WITHOUT_MINGLE = Arg(",
            "    (\"--without-mingle\",),",
            "    default=False,",
            "    help=\"Don\u2019t synchronize with other workers at start-up\",",
            "    action=\"store_true\",",
            ")",
            "ARG_WITHOUT_GOSSIP = Arg(",
            "    (\"--without-gossip\",),",
            "    default=False,",
            "    help=\"Don\u2019t subscribe to other workers events\",",
            "    action=\"store_true\",",
            ")",
            "",
            "# flower",
            "ARG_BROKER_API = Arg((\"-a\", \"--broker-api\"), help=\"Broker API\")",
            "ARG_FLOWER_HOSTNAME = Arg(",
            "    (\"-H\", \"--hostname\"),",
            "    default=conf.get('celery', 'FLOWER_HOST'),",
            "    help=\"Set the hostname on which to run the server\",",
            ")",
            "ARG_FLOWER_PORT = Arg(",
            "    (\"-p\", \"--port\"),",
            "    default=conf.get('celery', 'FLOWER_PORT'),",
            "    type=int,",
            "    help=\"The port on which to run the server\",",
            ")",
            "ARG_FLOWER_CONF = Arg((\"-c\", \"--flower-conf\"), help=\"Configuration file for flower\")",
            "ARG_FLOWER_URL_PREFIX = Arg(",
            "    (\"-u\", \"--url-prefix\"), default=conf.get('celery', 'FLOWER_URL_PREFIX'), help=\"URL prefix for Flower\"",
            ")",
            "ARG_FLOWER_BASIC_AUTH = Arg(",
            "    (\"-A\", \"--basic-auth\"),",
            "    default=conf.get('celery', 'FLOWER_BASIC_AUTH'),",
            "    help=(",
            "        \"Securing Flower with Basic Authentication. \"",
            "        \"Accepts user:password pairs separated by a comma. \"",
            "        \"Example: flower_basic_auth = user1:password1,user2:password2\"",
            "    ),",
            ")",
            "ARG_TASK_PARAMS = Arg((\"-t\", \"--task-params\"), help=\"Sends a JSON params dict to the task\")",
            "ARG_POST_MORTEM = Arg(",
            "    (\"-m\", \"--post-mortem\"), action=\"store_true\", help=\"Open debugger on uncaught exception\"",
            ")",
            "ARG_ENV_VARS = Arg(",
            "    (\"--env-vars\",),",
            "    help=\"Set env var in both parsing time and runtime for each of entry supplied in a JSON dict\",",
            "    type=json.loads,",
            ")",
            "",
            "# connections",
            "ARG_CONN_ID = Arg(('conn_id',), help='Connection id, required to get/add/delete a connection', type=str)",
            "ARG_CONN_ID_FILTER = Arg(",
            "    ('--conn-id',), help='If passed, only items with the specified connection ID will be displayed', type=str",
            ")",
            "ARG_CONN_URI = Arg(",
            "    ('--conn-uri',), help='Connection URI, required to add a connection without conn_type', type=str",
            ")",
            "ARG_CONN_JSON = Arg(",
            "    ('--conn-json',), help='Connection JSON, required to add a connection using JSON representation', type=str",
            ")",
            "ARG_CONN_TYPE = Arg(",
            "    ('--conn-type',), help='Connection type, required to add a connection without conn_uri', type=str",
            ")",
            "ARG_CONN_DESCRIPTION = Arg(",
            "    ('--conn-description',), help='Connection description, optional when adding a connection', type=str",
            ")",
            "ARG_CONN_HOST = Arg(('--conn-host',), help='Connection host, optional when adding a connection', type=str)",
            "ARG_CONN_LOGIN = Arg(('--conn-login',), help='Connection login, optional when adding a connection', type=str)",
            "ARG_CONN_PASSWORD = Arg(",
            "    ('--conn-password',), help='Connection password, optional when adding a connection', type=str",
            ")",
            "ARG_CONN_SCHEMA = Arg(",
            "    ('--conn-schema',), help='Connection schema, optional when adding a connection', type=str",
            ")",
            "ARG_CONN_PORT = Arg(('--conn-port',), help='Connection port, optional when adding a connection', type=str)",
            "ARG_CONN_EXTRA = Arg(",
            "    ('--conn-extra',), help='Connection `Extra` field, optional when adding a connection', type=str",
            ")",
            "ARG_CONN_EXPORT = Arg(",
            "    ('file',),",
            "    help='Output file path for exporting the connections',",
            "    type=argparse.FileType('w', encoding='UTF-8'),",
            ")",
            "ARG_CONN_EXPORT_FORMAT = Arg(",
            "    ('--format',),",
            "    help='Deprecated -- use `--file-format` instead. File format to use for the export.',",
            "    type=str,",
            "    choices=['json', 'yaml', 'env'],",
            ")",
            "ARG_CONN_EXPORT_FILE_FORMAT = Arg(",
            "    ('--file-format',), help='File format for the export', type=str, choices=['json', 'yaml', 'env']",
            ")",
            "ARG_CONN_SERIALIZATION_FORMAT = Arg(",
            "    ('--serialization-format',),",
            "    help='When exporting as `.env` format, defines how connections should be serialized. Default is `uri`.',",
            "    type=string_lower_type,",
            "    choices=['json', 'uri'],",
            ")",
            "ARG_CONN_IMPORT = Arg((\"file\",), help=\"Import connections from a file\")",
            "",
            "# providers",
            "ARG_PROVIDER_NAME = Arg(",
            "    ('provider_name',), help='Provider name, required to get provider information', type=str",
            ")",
            "ARG_FULL = Arg(",
            "    ('-f', '--full'),",
            "    help='Full information about the provider, including documentation information.',",
            "    required=False,",
            "    action=\"store_true\",",
            ")",
            "",
            "# users",
            "ARG_USERNAME = Arg(('-u', '--username'), help='Username of the user', required=True, type=str)",
            "ARG_USERNAME_OPTIONAL = Arg(('-u', '--username'), help='Username of the user', type=str)",
            "ARG_FIRSTNAME = Arg(('-f', '--firstname'), help='First name of the user', required=True, type=str)",
            "ARG_LASTNAME = Arg(('-l', '--lastname'), help='Last name of the user', required=True, type=str)",
            "ARG_ROLE = Arg(",
            "    ('-r', '--role'),",
            "    help='Role of the user. Existing roles include Admin, User, Op, Viewer, and Public',",
            "    required=True,",
            "    type=str,",
            ")",
            "ARG_EMAIL = Arg(('-e', '--email'), help='Email of the user', required=True, type=str)",
            "ARG_EMAIL_OPTIONAL = Arg(('-e', '--email'), help='Email of the user', type=str)",
            "ARG_PASSWORD = Arg(",
            "    ('-p', '--password'),",
            "    help='Password of the user, required to create a user without --use-random-password',",
            "    type=str,",
            ")",
            "ARG_USE_RANDOM_PASSWORD = Arg(",
            "    ('--use-random-password',),",
            "    help='Do not prompt for password. Use random string instead.'",
            "    ' Required to create a user without --password ',",
            "    default=False,",
            "    action='store_true',",
            ")",
            "ARG_USER_IMPORT = Arg(",
            "    (\"import\",),",
            "    metavar=\"FILEPATH\",",
            "    help=\"Import users from JSON file. Example format::\\n\"",
            "    + textwrap.indent(",
            "        textwrap.dedent(",
            "            '''",
            "            [",
            "                {",
            "                    \"email\": \"foo@bar.org\",",
            "                    \"firstname\": \"Jon\",",
            "                    \"lastname\": \"Doe\",",
            "                    \"roles\": [\"Public\"],",
            "                    \"username\": \"jondoe\"",
            "                }",
            "            ]'''",
            "        ),",
            "        \" \" * 4,",
            "    ),",
            ")",
            "ARG_USER_EXPORT = Arg((\"export\",), metavar=\"FILEPATH\", help=\"Export all users to JSON file\")",
            "",
            "# roles",
            "ARG_CREATE_ROLE = Arg(('-c', '--create'), help='Create a new role', action='store_true')",
            "ARG_LIST_ROLES = Arg(('-l', '--list'), help='List roles', action='store_true')",
            "ARG_ROLES = Arg(('role',), help='The name of a role', nargs='*')",
            "ARG_AUTOSCALE = Arg(('-a', '--autoscale'), help=\"Minimum and Maximum number of worker to autoscale\")",
            "ARG_SKIP_SERVE_LOGS = Arg(",
            "    (\"-s\", \"--skip-serve-logs\"),",
            "    default=False,",
            "    help=\"Don't start the serve logs process along with the workers\",",
            "    action=\"store_true\",",
            ")",
            "ARG_ROLE_IMPORT = Arg((\"file\",), help=\"Import roles from JSON file\", nargs=None)",
            "ARG_ROLE_EXPORT = Arg((\"file\",), help=\"Export all roles to JSON file\", nargs=None)",
            "ARG_ROLE_EXPORT_FMT = Arg(",
            "    ('-p', '--pretty'),",
            "    help='Format output JSON file by sorting role names and indenting by 4 spaces',",
            "    action='store_true',",
            ")",
            "",
            "# info",
            "ARG_ANONYMIZE = Arg(",
            "    ('--anonymize',),",
            "    help='Minimize any personal identifiable information. Use it when sharing output with others.',",
            "    action='store_true',",
            ")",
            "ARG_FILE_IO = Arg(",
            "    ('--file-io',), help='Send output to file.io service and returns link.', action='store_true'",
            ")",
            "",
            "# config",
            "ARG_SECTION = Arg(",
            "    (\"section\",),",
            "    help=\"The section name\",",
            ")",
            "ARG_OPTION = Arg(",
            "    (\"option\",),",
            "    help=\"The option name\",",
            ")",
            "",
            "# kubernetes cleanup-pods",
            "ARG_NAMESPACE = Arg(",
            "    (\"--namespace\",),",
            "    default=conf.get('kubernetes', 'namespace'),",
            "    help=\"Kubernetes Namespace. Default value is `[kubernetes] namespace` in configuration.\",",
            ")",
            "",
            "ARG_MIN_PENDING_MINUTES = Arg(",
            "    (\"--min-pending-minutes\",),",
            "    default=30,",
            "    type=positive_int(allow_zero=False),",
            "    help=(",
            "        \"Pending pods created before the time interval are to be cleaned up, \"",
            "        \"measured in minutes. Default value is 30(m). The minimum value is 5(m).\"",
            "    ),",
            ")",
            "",
            "# jobs check",
            "ARG_JOB_TYPE_FILTER = Arg(",
            "    ('--job-type',),",
            "    choices=('BackfillJob', 'LocalTaskJob', 'SchedulerJob', 'TriggererJob'),",
            "    action='store',",
            "    help='The type of job(s) that will be checked.',",
            ")",
            "",
            "ARG_JOB_HOSTNAME_FILTER = Arg(",
            "    (\"--hostname\",),",
            "    default=None,",
            "    type=str,",
            "    help=\"The hostname of job(s) that will be checked.\",",
            ")",
            "",
            "ARG_JOB_LIMIT = Arg(",
            "    (\"--limit\",),",
            "    default=1,",
            "    type=positive_int(allow_zero=True),",
            "    help=\"The number of recent jobs that will be checked. To disable limit, set 0. \",",
            ")",
            "",
            "ARG_ALLOW_MULTIPLE = Arg(",
            "    (\"--allow-multiple\",),",
            "    action='store_true',",
            "    help=\"If passed, this command will be successful even if multiple matching alive jobs are found.\",",
            ")",
            "",
            "# sync-perm",
            "ARG_INCLUDE_DAGS = Arg(",
            "    (\"--include-dags\",), help=\"If passed, DAG specific permissions will also be synced.\", action=\"store_true\"",
            ")",
            "",
            "# triggerer",
            "ARG_CAPACITY = Arg(",
            "    (\"--capacity\",),",
            "    type=positive_int(allow_zero=False),",
            "    help=\"The maximum number of triggers that a Triggerer will run at one time.\",",
            ")",
            "",
            "# reserialize",
            "ARG_CLEAR_ONLY = Arg(",
            "    (\"--clear-only\",),",
            "    action=\"store_true\",",
            "    help=\"If passed, serialized DAGs will be cleared but not reserialized.\",",
            ")",
            "",
            "ALTERNATIVE_CONN_SPECS_ARGS = [",
            "    ARG_CONN_TYPE,",
            "    ARG_CONN_DESCRIPTION,",
            "    ARG_CONN_HOST,",
            "    ARG_CONN_LOGIN,",
            "    ARG_CONN_PASSWORD,",
            "    ARG_CONN_SCHEMA,",
            "    ARG_CONN_PORT,",
            "]",
            "",
            "",
            "class ActionCommand(NamedTuple):",
            "    \"\"\"Single CLI command\"\"\"",
            "",
            "    name: str",
            "    help: str",
            "    func: Callable",
            "    args: Iterable[Arg]",
            "    description: Optional[str] = None",
            "    epilog: Optional[str] = None",
            "",
            "",
            "class GroupCommand(NamedTuple):",
            "    \"\"\"ClI command with subcommands\"\"\"",
            "",
            "    name: str",
            "    help: str",
            "    subcommands: Iterable",
            "    description: Optional[str] = None",
            "    epilog: Optional[str] = None",
            "",
            "",
            "CLICommand = Union[ActionCommand, GroupCommand]",
            "",
            "DAGS_COMMANDS = (",
            "    ActionCommand(",
            "        name='list',",
            "        help=\"List all the DAGs\",",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_list_dags'),",
            "        args=(ARG_SUBDIR, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='list-import-errors',",
            "        help=\"List all the DAGs that have import errors\",",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_list_import_errors'),",
            "        args=(ARG_SUBDIR, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='report',",
            "        help='Show DagBag loading report',",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_report'),",
            "        args=(ARG_SUBDIR, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='list-runs',",
            "        help=\"List DAG runs given a DAG id\",",
            "        description=(",
            "            \"List DAG runs given a DAG id. If state option is given, it will only search for all the \"",
            "            \"dagruns with the given state. If no_backfill option is given, it will filter out all \"",
            "            \"backfill dagruns for given dag id. If start_date is given, it will filter out all the \"",
            "            \"dagruns that were executed before this date. If end_date is given, it will filter out \"",
            "            \"all the dagruns that were executed after this date. \"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_list_dag_runs'),",
            "        args=(",
            "            ARG_DAG_ID_OPT,",
            "            ARG_NO_BACKFILL,",
            "            ARG_STATE,",
            "            ARG_OUTPUT,",
            "            ARG_VERBOSE,",
            "            ARG_START_DATE,",
            "            ARG_END_DATE,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='list-jobs',",
            "        help=\"List the jobs\",",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_list_jobs'),",
            "        args=(ARG_DAG_ID_OPT, ARG_STATE, ARG_LIMIT, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='state',",
            "        help=\"Get the status of a dag run\",",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_state'),",
            "        args=(ARG_DAG_ID, ARG_EXECUTION_DATE, ARG_SUBDIR),",
            "    ),",
            "    ActionCommand(",
            "        name='next-execution',",
            "        help=\"Get the next execution datetimes of a DAG\",",
            "        description=(",
            "            \"Get the next execution datetimes of a DAG. It returns one execution unless the \"",
            "            \"num-executions option is given\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_next_execution'),",
            "        args=(ARG_DAG_ID, ARG_SUBDIR, ARG_NUM_EXECUTIONS),",
            "    ),",
            "    ActionCommand(",
            "        name='pause',",
            "        help='Pause a DAG',",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_pause'),",
            "        args=(ARG_DAG_ID, ARG_SUBDIR),",
            "    ),",
            "    ActionCommand(",
            "        name='unpause',",
            "        help='Resume a paused DAG',",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_unpause'),",
            "        args=(ARG_DAG_ID, ARG_SUBDIR),",
            "    ),",
            "    ActionCommand(",
            "        name='trigger',",
            "        help='Trigger a DAG run',",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_trigger'),",
            "        args=(ARG_DAG_ID, ARG_SUBDIR, ARG_RUN_ID, ARG_CONF, ARG_EXEC_DATE),",
            "    ),",
            "    ActionCommand(",
            "        name='delete',",
            "        help=\"Delete all DB records related to the specified DAG\",",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_delete'),",
            "        args=(ARG_DAG_ID, ARG_YES),",
            "    ),",
            "    ActionCommand(",
            "        name='show',",
            "        help=\"Displays DAG's tasks with their dependencies\",",
            "        description=(",
            "            \"The --imgcat option only works in iTerm.\\n\"",
            "            \"\\n\"",
            "            \"For more information, see: https://www.iterm2.com/documentation-images.html\\n\"",
            "            \"\\n\"",
            "            \"The --save option saves the result to the indicated file.\\n\"",
            "            \"\\n\"",
            "            \"The file format is determined by the file extension. \"",
            "            \"For more information about supported \"",
            "            \"format, see: https://www.graphviz.org/doc/info/output.html\\n\"",
            "            \"\\n\"",
            "            \"If you want to create a PNG file then you should execute the following command:\\n\"",
            "            \"airflow dags show <DAG_ID> --save output.png\\n\"",
            "            \"\\n\"",
            "            \"If you want to create a DOT file then you should execute the following command:\\n\"",
            "            \"airflow dags show <DAG_ID> --save output.dot\\n\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_show'),",
            "        args=(",
            "            ARG_DAG_ID,",
            "            ARG_SUBDIR,",
            "            ARG_SAVE,",
            "            ARG_IMGCAT,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='show-dependencies',",
            "        help=\"Displays DAGs with their dependencies\",",
            "        description=(",
            "            \"The --imgcat option only works in iTerm.\\n\"",
            "            \"\\n\"",
            "            \"For more information, see: https://www.iterm2.com/documentation-images.html\\n\"",
            "            \"\\n\"",
            "            \"The --save option saves the result to the indicated file.\\n\"",
            "            \"\\n\"",
            "            \"The file format is determined by the file extension. \"",
            "            \"For more information about supported \"",
            "            \"format, see: https://www.graphviz.org/doc/info/output.html\\n\"",
            "            \"\\n\"",
            "            \"If you want to create a PNG file then you should execute the following command:\\n\"",
            "            \"airflow dags show-dependencies --save output.png\\n\"",
            "            \"\\n\"",
            "            \"If you want to create a DOT file then you should execute the following command:\\n\"",
            "            \"airflow dags show-dependencies --save output.dot\\n\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_dependencies_show'),",
            "        args=(",
            "            ARG_SUBDIR,",
            "            ARG_SAVE,",
            "            ARG_IMGCAT,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='backfill',",
            "        help=\"Run subsections of a DAG for a specified date range\",",
            "        description=(",
            "            \"Run subsections of a DAG for a specified date range. If reset_dag_run option is used, \"",
            "            \"backfill will first prompt users whether airflow should clear all the previous dag_run and \"",
            "            \"task_instances within the backfill date range. If rerun_failed_tasks is used, backfill \"",
            "            \"will auto re-run the previous failed task instances  within the backfill date range\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_backfill'),",
            "        args=(",
            "            ARG_DAG_ID,",
            "            ARG_TASK_REGEX,",
            "            ARG_START_DATE,",
            "            ARG_END_DATE,",
            "            ARG_MARK_SUCCESS,",
            "            ARG_LOCAL,",
            "            ARG_DONOT_PICKLE,",
            "            ARG_YES,",
            "            ARG_CONTINUE_ON_FAILURES,",
            "            ARG_BF_IGNORE_DEPENDENCIES,",
            "            ARG_BF_IGNORE_FIRST_DEPENDS_ON_PAST,",
            "            ARG_SUBDIR,",
            "            ARG_POOL,",
            "            ARG_DELAY_ON_LIMIT,",
            "            ARG_DRY_RUN,",
            "            ARG_VERBOSE,",
            "            ARG_CONF,",
            "            ARG_RESET_DAG_RUN,",
            "            ARG_RERUN_FAILED_TASKS,",
            "            ARG_RUN_BACKWARDS,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='test',",
            "        help=\"Execute one single DagRun\",",
            "        description=(",
            "            \"Execute one single DagRun for a given DAG and execution date, \"",
            "            \"using the DebugExecutor.\\n\"",
            "            \"\\n\"",
            "            \"The --imgcat-dagrun option only works in iTerm.\\n\"",
            "            \"\\n\"",
            "            \"For more information, see: https://www.iterm2.com/documentation-images.html\\n\"",
            "            \"\\n\"",
            "            \"If --save-dagrun is used, then, after completing the backfill, saves the diagram \"",
            "            \"for current DAG Run to the indicated file.\\n\"",
            "            \"The file format is determined by the file extension. \"",
            "            \"For more information about supported format, \"",
            "            \"see: https://www.graphviz.org/doc/info/output.html\\n\"",
            "            \"\\n\"",
            "            \"If you want to create a PNG file then you should execute the following command:\\n\"",
            "            \"airflow dags test <DAG_ID> <EXECUTION_DATE> --save-dagrun output.png\\n\"",
            "            \"\\n\"",
            "            \"If you want to create a DOT file then you should execute the following command:\\n\"",
            "            \"airflow dags test <DAG_ID> <EXECUTION_DATE> --save-dagrun output.dot\\n\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_test'),",
            "        args=(",
            "            ARG_DAG_ID,",
            "            ARG_EXECUTION_DATE,",
            "            ARG_SUBDIR,",
            "            ARG_SHOW_DAGRUN,",
            "            ARG_IMGCAT_DAGRUN,",
            "            ARG_SAVE_DAGRUN,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='reserialize',",
            "        help=\"Reserialize all DAGs by parsing the DagBag files\",",
            "        description=(",
            "            \"Drop all serialized dags from the metadata DB. This will cause all DAGs to be reserialized \"",
            "            \"from the DagBag folder. This can be helpful if your serialized DAGs get out of sync with the \"",
            "            \"version of Airflow that you are running.\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.dag_command.dag_reserialize'),",
            "        args=(ARG_CLEAR_ONLY,),",
            "    ),",
            ")",
            "TASKS_COMMANDS = (",
            "    ActionCommand(",
            "        name='list',",
            "        help=\"List the tasks within a DAG\",",
            "        func=lazy_load_command('airflow.cli.commands.task_command.task_list'),",
            "        args=(ARG_DAG_ID, ARG_TREE, ARG_SUBDIR, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='clear',",
            "        help=\"Clear a set of task instance, as if they never ran\",",
            "        func=lazy_load_command('airflow.cli.commands.task_command.task_clear'),",
            "        args=(",
            "            ARG_DAG_ID,",
            "            ARG_TASK_REGEX,",
            "            ARG_START_DATE,",
            "            ARG_END_DATE,",
            "            ARG_SUBDIR,",
            "            ARG_UPSTREAM,",
            "            ARG_DOWNSTREAM,",
            "            ARG_YES,",
            "            ARG_ONLY_FAILED,",
            "            ARG_ONLY_RUNNING,",
            "            ARG_EXCLUDE_SUBDAGS,",
            "            ARG_EXCLUDE_PARENTDAG,",
            "            ARG_DAG_REGEX,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='state',",
            "        help=\"Get the status of a task instance\",",
            "        func=lazy_load_command('airflow.cli.commands.task_command.task_state'),",
            "        args=(",
            "            ARG_DAG_ID,",
            "            ARG_TASK_ID,",
            "            ARG_EXECUTION_DATE_OR_RUN_ID,",
            "            ARG_SUBDIR,",
            "            ARG_VERBOSE,",
            "            ARG_MAP_INDEX,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='failed-deps',",
            "        help=\"Returns the unmet dependencies for a task instance\",",
            "        description=(",
            "            \"Returns the unmet dependencies for a task instance from the perspective of the scheduler. \"",
            "            \"In other words, why a task instance doesn't get scheduled and then queued by the scheduler, \"",
            "            \"and then run by an executor.\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.task_command.task_failed_deps'),",
            "        args=(ARG_DAG_ID, ARG_TASK_ID, ARG_EXECUTION_DATE_OR_RUN_ID, ARG_SUBDIR, ARG_MAP_INDEX),",
            "    ),",
            "    ActionCommand(",
            "        name='render',",
            "        help=\"Render a task instance's template(s)\",",
            "        func=lazy_load_command('airflow.cli.commands.task_command.task_render'),",
            "        args=(",
            "            ARG_DAG_ID,",
            "            ARG_TASK_ID,",
            "            ARG_EXECUTION_DATE_OR_RUN_ID,",
            "            ARG_SUBDIR,",
            "            ARG_VERBOSE,",
            "            ARG_MAP_INDEX,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='run',",
            "        help=\"Run a single task instance\",",
            "        func=lazy_load_command('airflow.cli.commands.task_command.task_run'),",
            "        args=(",
            "            ARG_DAG_ID,",
            "            ARG_TASK_ID,",
            "            ARG_EXECUTION_DATE_OR_RUN_ID,",
            "            ARG_SUBDIR,",
            "            ARG_MARK_SUCCESS,",
            "            ARG_FORCE,",
            "            ARG_POOL,",
            "            ARG_CFG_PATH,",
            "            ARG_LOCAL,",
            "            ARG_RAW,",
            "            ARG_IGNORE_ALL_DEPENDENCIES,",
            "            ARG_IGNORE_DEPENDENCIES,",
            "            ARG_IGNORE_DEPENDS_ON_PAST,",
            "            ARG_SHIP_DAG,",
            "            ARG_PICKLE,",
            "            ARG_JOB_ID,",
            "            ARG_INTERACTIVE,",
            "            ARG_ERROR_FILE,",
            "            ARG_SHUT_DOWN_LOGGING,",
            "            ARG_MAP_INDEX,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='test',",
            "        help=\"Test a task instance\",",
            "        description=(",
            "            \"Test a task instance. This will run a task without checking for dependencies or recording \"",
            "            \"its state in the database\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.task_command.task_test'),",
            "        args=(",
            "            ARG_DAG_ID,",
            "            ARG_TASK_ID,",
            "            ARG_EXECUTION_DATE_OR_RUN_ID,",
            "            ARG_SUBDIR,",
            "            ARG_DRY_RUN,",
            "            ARG_TASK_PARAMS,",
            "            ARG_POST_MORTEM,",
            "            ARG_ENV_VARS,",
            "            ARG_MAP_INDEX,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='states-for-dag-run',",
            "        help=\"Get the status of all task instances in a dag run\",",
            "        func=lazy_load_command('airflow.cli.commands.task_command.task_states_for_dag_run'),",
            "        args=(ARG_DAG_ID, ARG_EXECUTION_DATE_OR_RUN_ID, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            ")",
            "POOLS_COMMANDS = (",
            "    ActionCommand(",
            "        name='list',",
            "        help='List pools',",
            "        func=lazy_load_command('airflow.cli.commands.pool_command.pool_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='get',",
            "        help='Get pool size',",
            "        func=lazy_load_command('airflow.cli.commands.pool_command.pool_get'),",
            "        args=(ARG_POOL_NAME, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='set',",
            "        help='Configure pool',",
            "        func=lazy_load_command('airflow.cli.commands.pool_command.pool_set'),",
            "        args=(ARG_POOL_NAME, ARG_POOL_SLOTS, ARG_POOL_DESCRIPTION, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='delete',",
            "        help='Delete pool',",
            "        func=lazy_load_command('airflow.cli.commands.pool_command.pool_delete'),",
            "        args=(ARG_POOL_NAME, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='import',",
            "        help='Import pools',",
            "        func=lazy_load_command('airflow.cli.commands.pool_command.pool_import'),",
            "        args=(ARG_POOL_IMPORT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='export',",
            "        help='Export all pools',",
            "        func=lazy_load_command('airflow.cli.commands.pool_command.pool_export'),",
            "        args=(ARG_POOL_EXPORT,),",
            "    ),",
            ")",
            "VARIABLES_COMMANDS = (",
            "    ActionCommand(",
            "        name='list',",
            "        help='List variables',",
            "        func=lazy_load_command('airflow.cli.commands.variable_command.variables_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='get',",
            "        help='Get variable',",
            "        func=lazy_load_command('airflow.cli.commands.variable_command.variables_get'),",
            "        args=(ARG_VAR, ARG_JSON, ARG_DEFAULT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='set',",
            "        help='Set variable',",
            "        func=lazy_load_command('airflow.cli.commands.variable_command.variables_set'),",
            "        args=(ARG_VAR, ARG_VAR_VALUE, ARG_JSON),",
            "    ),",
            "    ActionCommand(",
            "        name='delete',",
            "        help='Delete variable',",
            "        func=lazy_load_command('airflow.cli.commands.variable_command.variables_delete'),",
            "        args=(ARG_VAR,),",
            "    ),",
            "    ActionCommand(",
            "        name='import',",
            "        help='Import variables',",
            "        func=lazy_load_command('airflow.cli.commands.variable_command.variables_import'),",
            "        args=(ARG_VAR_IMPORT,),",
            "    ),",
            "    ActionCommand(",
            "        name='export',",
            "        help='Export all variables',",
            "        func=lazy_load_command('airflow.cli.commands.variable_command.variables_export'),",
            "        args=(ARG_VAR_EXPORT,),",
            "    ),",
            ")",
            "DB_COMMANDS = (",
            "    ActionCommand(",
            "        name='init',",
            "        help=\"Initialize the metadata database\",",
            "        func=lazy_load_command('airflow.cli.commands.db_command.initdb'),",
            "        args=(),",
            "    ),",
            "    ActionCommand(",
            "        name=\"check-migrations\",",
            "        help=\"Check if migration have finished\",",
            "        description=\"Check if migration have finished (or continually check until timeout)\",",
            "        func=lazy_load_command('airflow.cli.commands.db_command.check_migrations'),",
            "        args=(ARG_MIGRATION_TIMEOUT,),",
            "    ),",
            "    ActionCommand(",
            "        name='reset',",
            "        help=\"Burn down and rebuild the metadata database\",",
            "        func=lazy_load_command('airflow.cli.commands.db_command.resetdb'),",
            "        args=(ARG_YES, ARG_DB_SKIP_INIT),",
            "    ),",
            "    ActionCommand(",
            "        name='upgrade',",
            "        help=\"Upgrade the metadata database to latest version\",",
            "        description=(",
            "            \"Upgrade the schema of the metadata database. \"",
            "            \"To print but not execute commands, use option ``--show-sql-only``. \"",
            "            \"If using options ``--from-revision`` or ``--from-version``, you must also use \"",
            "            \"``--show-sql-only``, because if actually *running* migrations, we should only \"",
            "            \"migrate from the *current* Alembic revision.\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.db_command.upgradedb'),",
            "        args=(",
            "            ARG_DB_REVISION__UPGRADE,",
            "            ARG_DB_VERSION__UPGRADE,",
            "            ARG_DB_SQL_ONLY,",
            "            ARG_DB_FROM_REVISION,",
            "            ARG_DB_FROM_VERSION,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='downgrade',",
            "        help=\"Downgrade the schema of the metadata database.\",",
            "        description=(",
            "            \"Downgrade the schema of the metadata database. \"",
            "            \"You must provide either `--to-revision` or `--to-version`. \"",
            "            \"To print but not execute commands, use option `--show-sql-only`. \"",
            "            \"If using options `--from-revision` or `--from-version`, you must also use `--show-sql-only`, \"",
            "            \"because if actually *running* migrations, we should only migrate from the *current* Alembic \"",
            "            \"revision.\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.db_command.downgrade'),",
            "        args=(",
            "            ARG_DB_REVISION__DOWNGRADE,",
            "            ARG_DB_VERSION__DOWNGRADE,",
            "            ARG_DB_SQL_ONLY,",
            "            ARG_YES,",
            "            ARG_DB_FROM_REVISION,",
            "            ARG_DB_FROM_VERSION,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='shell',",
            "        help=\"Runs a shell to access the database\",",
            "        func=lazy_load_command('airflow.cli.commands.db_command.shell'),",
            "        args=(),",
            "    ),",
            "    ActionCommand(",
            "        name='check',",
            "        help=\"Check if the database can be reached\",",
            "        func=lazy_load_command('airflow.cli.commands.db_command.check'),",
            "        args=(),",
            "    ),",
            "    ActionCommand(",
            "        name='clean',",
            "        help=\"Purge old records in metastore tables\",",
            "        func=lazy_load_command('airflow.cli.commands.db_command.cleanup_tables'),",
            "        args=(",
            "            ARG_DB_TABLES,",
            "            ARG_DB_DRY_RUN,",
            "            ARG_DB_CLEANUP_TIMESTAMP,",
            "            ARG_VERBOSE,",
            "            ARG_YES,",
            "            ARG_DB_SKIP_ARCHIVE,",
            "        ),",
            "    ),",
            ")",
            "CONNECTIONS_COMMANDS = (",
            "    ActionCommand(",
            "        name='get',",
            "        help='Get a connection',",
            "        func=lazy_load_command('airflow.cli.commands.connection_command.connections_get'),",
            "        args=(ARG_CONN_ID, ARG_COLOR, ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='list',",
            "        help='List connections',",
            "        func=lazy_load_command('airflow.cli.commands.connection_command.connections_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE, ARG_CONN_ID_FILTER),",
            "    ),",
            "    ActionCommand(",
            "        name='add',",
            "        help='Add a connection',",
            "        func=lazy_load_command('airflow.cli.commands.connection_command.connections_add'),",
            "        args=(ARG_CONN_ID, ARG_CONN_URI, ARG_CONN_JSON, ARG_CONN_EXTRA) + tuple(ALTERNATIVE_CONN_SPECS_ARGS),",
            "    ),",
            "    ActionCommand(",
            "        name='delete',",
            "        help='Delete a connection',",
            "        func=lazy_load_command('airflow.cli.commands.connection_command.connections_delete'),",
            "        args=(ARG_CONN_ID, ARG_COLOR),",
            "    ),",
            "    ActionCommand(",
            "        name='export',",
            "        help='Export all connections',",
            "        description=(",
            "            \"All connections can be exported in STDOUT using the following command:\\n\"",
            "            \"airflow connections export -\\n\"",
            "            \"The file format can be determined by the provided file extension. E.g., The following \"",
            "            \"command will export the connections in JSON format:\\n\"",
            "            \"airflow connections export /tmp/connections.json\\n\"",
            "            \"The --file-format parameter can be used to control the file format. E.g., \"",
            "            \"the default format is JSON in STDOUT mode, which can be overridden using: \\n\"",
            "            \"airflow connections export - --file-format yaml\\n\"",
            "            \"The --file-format parameter can also be used for the files, for example:\\n\"",
            "            \"airflow connections export /tmp/connections --file-format json.\\n\"",
            "            \"When exporting in `env` file format, you control whether URI format or JSON format \"",
            "            \"is used to serialize the connection by passing `uri` or `json` with option \"",
            "            \"`--serialization-format`.\\n\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.connection_command.connections_export'),",
            "        args=(",
            "            ARG_CONN_EXPORT,",
            "            ARG_CONN_EXPORT_FORMAT,",
            "            ARG_CONN_EXPORT_FILE_FORMAT,",
            "            ARG_CONN_SERIALIZATION_FORMAT,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='import',",
            "        help='Import connections from a file',",
            "        description=(",
            "            \"Connections can be imported from the output of the export command.\\n\"",
            "            \"The filetype must by json, yaml or env and will be automatically inferred.\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.connection_command.connections_import'),",
            "        args=(ARG_CONN_IMPORT,),",
            "    ),",
            ")",
            "PROVIDERS_COMMANDS = (",
            "    ActionCommand(",
            "        name='list',",
            "        help='List installed providers',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.providers_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='get',",
            "        help='Get detailed information about a provider',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.provider_get'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE, ARG_FULL, ARG_COLOR, ARG_PROVIDER_NAME),",
            "    ),",
            "    ActionCommand(",
            "        name='links',",
            "        help='List extra links registered by the providers',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.extra_links_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='widgets',",
            "        help='Get information about registered connection form widgets',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.connection_form_widget_list'),",
            "        args=(",
            "            ARG_OUTPUT,",
            "            ARG_VERBOSE,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='hooks',",
            "        help='List registered provider hooks',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.hooks_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='behaviours',",
            "        help='Get information about registered connection types with custom behaviours',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.connection_field_behaviours'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='logging',",
            "        help='Get information about task logging handlers provided',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.logging_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='secrets',",
            "        help='Get information about secrets backends provided',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.secrets_backends_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='auth',",
            "        help='Get information about API auth backends provided',",
            "        func=lazy_load_command('airflow.cli.commands.provider_command.auth_backend_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            ")",
            "",
            "USERS_COMMANDS = (",
            "    ActionCommand(",
            "        name='list',",
            "        help='List users',",
            "        func=lazy_load_command('airflow.cli.commands.user_command.users_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='create',",
            "        help='Create a user',",
            "        func=lazy_load_command('airflow.cli.commands.user_command.users_create'),",
            "        args=(",
            "            ARG_ROLE,",
            "            ARG_USERNAME,",
            "            ARG_EMAIL,",
            "            ARG_FIRSTNAME,",
            "            ARG_LASTNAME,",
            "            ARG_PASSWORD,",
            "            ARG_USE_RANDOM_PASSWORD,",
            "        ),",
            "        epilog=(",
            "            'examples:\\n'",
            "            'To create an user with \"Admin\" role and username equals to \"admin\", run:\\n'",
            "            '\\n'",
            "            '    $ airflow users create \\\\\\n'",
            "            '          --username admin \\\\\\n'",
            "            '          --firstname FIRST_NAME \\\\\\n'",
            "            '          --lastname LAST_NAME \\\\\\n'",
            "            '          --role Admin \\\\\\n'",
            "            '          --email admin@example.org'",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='delete',",
            "        help='Delete a user',",
            "        func=lazy_load_command('airflow.cli.commands.user_command.users_delete'),",
            "        args=(ARG_USERNAME_OPTIONAL, ARG_EMAIL_OPTIONAL),",
            "    ),",
            "    ActionCommand(",
            "        name='add-role',",
            "        help='Add role to a user',",
            "        func=lazy_load_command('airflow.cli.commands.user_command.add_role'),",
            "        args=(ARG_USERNAME_OPTIONAL, ARG_EMAIL_OPTIONAL, ARG_ROLE),",
            "    ),",
            "    ActionCommand(",
            "        name='remove-role',",
            "        help='Remove role from a user',",
            "        func=lazy_load_command('airflow.cli.commands.user_command.remove_role'),",
            "        args=(ARG_USERNAME_OPTIONAL, ARG_EMAIL_OPTIONAL, ARG_ROLE),",
            "    ),",
            "    ActionCommand(",
            "        name='import',",
            "        help='Import users',",
            "        func=lazy_load_command('airflow.cli.commands.user_command.users_import'),",
            "        args=(ARG_USER_IMPORT,),",
            "    ),",
            "    ActionCommand(",
            "        name='export',",
            "        help='Export all users',",
            "        func=lazy_load_command('airflow.cli.commands.user_command.users_export'),",
            "        args=(ARG_USER_EXPORT,),",
            "    ),",
            ")",
            "ROLES_COMMANDS = (",
            "    ActionCommand(",
            "        name='list',",
            "        help='List roles',",
            "        func=lazy_load_command('airflow.cli.commands.role_command.roles_list'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='create',",
            "        help='Create role',",
            "        func=lazy_load_command('airflow.cli.commands.role_command.roles_create'),",
            "        args=(ARG_ROLES, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='export',",
            "        help='Export roles (without permissions) from db to JSON file',",
            "        func=lazy_load_command('airflow.cli.commands.role_command.roles_export'),",
            "        args=(ARG_ROLE_EXPORT, ARG_ROLE_EXPORT_FMT, ARG_VERBOSE),",
            "    ),",
            "    ActionCommand(",
            "        name='import',",
            "        help='Import roles (without permissions) from JSON file to db',",
            "        func=lazy_load_command('airflow.cli.commands.role_command.roles_import'),",
            "        args=(ARG_ROLE_IMPORT, ARG_VERBOSE),",
            "    ),",
            ")",
            "",
            "CELERY_COMMANDS = (",
            "    ActionCommand(",
            "        name='worker',",
            "        help=\"Start a Celery worker node\",",
            "        func=lazy_load_command('airflow.cli.commands.celery_command.worker'),",
            "        args=(",
            "            ARG_QUEUES,",
            "            ARG_CONCURRENCY,",
            "            ARG_CELERY_HOSTNAME,",
            "            ARG_PID,",
            "            ARG_DAEMON,",
            "            ARG_UMASK,",
            "            ARG_STDOUT,",
            "            ARG_STDERR,",
            "            ARG_LOG_FILE,",
            "            ARG_AUTOSCALE,",
            "            ARG_SKIP_SERVE_LOGS,",
            "            ARG_WITHOUT_MINGLE,",
            "            ARG_WITHOUT_GOSSIP,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='flower',",
            "        help=\"Start a Celery Flower\",",
            "        func=lazy_load_command('airflow.cli.commands.celery_command.flower'),",
            "        args=(",
            "            ARG_FLOWER_HOSTNAME,",
            "            ARG_FLOWER_PORT,",
            "            ARG_FLOWER_CONF,",
            "            ARG_FLOWER_URL_PREFIX,",
            "            ARG_FLOWER_BASIC_AUTH,",
            "            ARG_BROKER_API,",
            "            ARG_PID,",
            "            ARG_DAEMON,",
            "            ARG_STDOUT,",
            "            ARG_STDERR,",
            "            ARG_LOG_FILE,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='stop',",
            "        help=\"Stop the Celery worker gracefully\",",
            "        func=lazy_load_command('airflow.cli.commands.celery_command.stop_worker'),",
            "        args=(ARG_PID,),",
            "    ),",
            ")",
            "",
            "CONFIG_COMMANDS = (",
            "    ActionCommand(",
            "        name='get-value',",
            "        help='Print the value of the configuration',",
            "        func=lazy_load_command('airflow.cli.commands.config_command.get_value'),",
            "        args=(",
            "            ARG_SECTION,",
            "            ARG_OPTION,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='list',",
            "        help='List options for the configuration',",
            "        func=lazy_load_command('airflow.cli.commands.config_command.show_config'),",
            "        args=(ARG_COLOR,),",
            "    ),",
            ")",
            "",
            "KUBERNETES_COMMANDS = (",
            "    ActionCommand(",
            "        name='cleanup-pods',",
            "        help=(",
            "            \"Clean up Kubernetes pods \"",
            "            \"(created by KubernetesExecutor/KubernetesPodOperator) \"",
            "            \"in evicted/failed/succeeded/pending states\"",
            "        ),",
            "        func=lazy_load_command('airflow.cli.commands.kubernetes_command.cleanup_pods'),",
            "        args=(ARG_NAMESPACE, ARG_MIN_PENDING_MINUTES),",
            "    ),",
            "    ActionCommand(",
            "        name='generate-dag-yaml',",
            "        help=\"Generate YAML files for all tasks in DAG. Useful for debugging tasks without \"",
            "        \"launching into a cluster\",",
            "        func=lazy_load_command('airflow.cli.commands.kubernetes_command.generate_pod_yaml'),",
            "        args=(ARG_DAG_ID, ARG_EXECUTION_DATE, ARG_SUBDIR, ARG_OUTPUT_PATH),",
            "    ),",
            ")",
            "",
            "JOBS_COMMANDS = (",
            "    ActionCommand(",
            "        name='check',",
            "        help=\"Checks if job(s) are still alive\",",
            "        func=lazy_load_command('airflow.cli.commands.jobs_command.check'),",
            "        args=(ARG_JOB_TYPE_FILTER, ARG_JOB_HOSTNAME_FILTER, ARG_JOB_LIMIT, ARG_ALLOW_MULTIPLE),",
            "        epilog=(",
            "            'examples:\\n'",
            "            'To check if the local scheduler is still working properly, run:\\n'",
            "            '\\n'",
            "            '    $ airflow jobs check --job-type SchedulerJob --hostname \"$(hostname)\"\\n'",
            "            '\\n'",
            "            'To check if any scheduler is running when you are using high availability, run:\\n'",
            "            '\\n'",
            "            '    $ airflow jobs check --job-type SchedulerJob --allow-multiple --limit 100'",
            "        ),",
            "    ),",
            ")",
            "",
            "airflow_commands: List[CLICommand] = [",
            "    GroupCommand(",
            "        name='dags',",
            "        help='Manage DAGs',",
            "        subcommands=DAGS_COMMANDS,",
            "    ),",
            "    GroupCommand(",
            "        name=\"kubernetes\", help='Tools to help run the KubernetesExecutor', subcommands=KUBERNETES_COMMANDS",
            "    ),",
            "    GroupCommand(",
            "        name='tasks',",
            "        help='Manage tasks',",
            "        subcommands=TASKS_COMMANDS,",
            "    ),",
            "    GroupCommand(",
            "        name='pools',",
            "        help=\"Manage pools\",",
            "        subcommands=POOLS_COMMANDS,",
            "    ),",
            "    GroupCommand(",
            "        name='variables',",
            "        help=\"Manage variables\",",
            "        subcommands=VARIABLES_COMMANDS,",
            "    ),",
            "    GroupCommand(",
            "        name='jobs',",
            "        help=\"Manage jobs\",",
            "        subcommands=JOBS_COMMANDS,",
            "    ),",
            "    GroupCommand(",
            "        name='db',",
            "        help=\"Database operations\",",
            "        subcommands=DB_COMMANDS,",
            "    ),",
            "    ActionCommand(",
            "        name='kerberos',",
            "        help=\"Start a kerberos ticket renewer\",",
            "        func=lazy_load_command('airflow.cli.commands.kerberos_command.kerberos'),",
            "        args=(ARG_PRINCIPAL, ARG_KEYTAB, ARG_PID, ARG_DAEMON, ARG_STDOUT, ARG_STDERR, ARG_LOG_FILE),",
            "    ),",
            "    ActionCommand(",
            "        name='webserver',",
            "        help=\"Start a Airflow webserver instance\",",
            "        func=lazy_load_command('airflow.cli.commands.webserver_command.webserver'),",
            "        args=(",
            "            ARG_PORT,",
            "            ARG_WORKERS,",
            "            ARG_WORKERCLASS,",
            "            ARG_WORKER_TIMEOUT,",
            "            ARG_HOSTNAME,",
            "            ARG_PID,",
            "            ARG_DAEMON,",
            "            ARG_STDOUT,",
            "            ARG_STDERR,",
            "            ARG_ACCESS_LOGFILE,",
            "            ARG_ERROR_LOGFILE,",
            "            ARG_ACCESS_LOGFORMAT,",
            "            ARG_LOG_FILE,",
            "            ARG_SSL_CERT,",
            "            ARG_SSL_KEY,",
            "            ARG_DEBUG,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='scheduler',",
            "        help=\"Start a scheduler instance\",",
            "        func=lazy_load_command('airflow.cli.commands.scheduler_command.scheduler'),",
            "        args=(",
            "            ARG_SUBDIR,",
            "            ARG_NUM_RUNS,",
            "            ARG_DO_PICKLE,",
            "            ARG_PID,",
            "            ARG_DAEMON,",
            "            ARG_STDOUT,",
            "            ARG_STDERR,",
            "            ARG_LOG_FILE,",
            "            ARG_SKIP_SERVE_LOGS,",
            "        ),",
            "        epilog=(",
            "            'Signals:\\n'",
            "            '\\n'",
            "            '  - SIGUSR2: Dump a snapshot of task state being tracked by the executor.\\n'",
            "            '\\n'",
            "            '    Example:\\n'",
            "            '        pkill -f -USR2 \"airflow scheduler\"'",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='triggerer',",
            "        help=\"Start a triggerer instance\",",
            "        func=lazy_load_command('airflow.cli.commands.triggerer_command.triggerer'),",
            "        args=(",
            "            ARG_PID,",
            "            ARG_DAEMON,",
            "            ARG_STDOUT,",
            "            ARG_STDERR,",
            "            ARG_LOG_FILE,",
            "            ARG_CAPACITY,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='dag-processor',",
            "        help=\"Start a standalone Dag Processor instance\",",
            "        func=lazy_load_command('airflow.cli.commands.dag_processor_command.dag_processor'),",
            "        args=(",
            "            ARG_PID,",
            "            ARG_DAEMON,",
            "            ARG_SUBDIR,",
            "            ARG_NUM_RUNS,",
            "            ARG_DO_PICKLE,",
            "            ARG_STDOUT,",
            "            ARG_STDERR,",
            "            ARG_LOG_FILE,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='version',",
            "        help=\"Show the version\",",
            "        func=lazy_load_command('airflow.cli.commands.version_command.version'),",
            "        args=(),",
            "    ),",
            "    ActionCommand(",
            "        name='cheat-sheet',",
            "        help=\"Display cheat sheet\",",
            "        func=lazy_load_command('airflow.cli.commands.cheat_sheet_command.cheat_sheet'),",
            "        args=(ARG_VERBOSE,),",
            "    ),",
            "    GroupCommand(",
            "        name='connections',",
            "        help=\"Manage connections\",",
            "        subcommands=CONNECTIONS_COMMANDS,",
            "    ),",
            "    GroupCommand(",
            "        name='providers',",
            "        help=\"Display providers\",",
            "        subcommands=PROVIDERS_COMMANDS,",
            "    ),",
            "    GroupCommand(",
            "        name='users',",
            "        help=\"Manage users\",",
            "        subcommands=USERS_COMMANDS,",
            "    ),",
            "    GroupCommand(",
            "        name='roles',",
            "        help='Manage roles',",
            "        subcommands=ROLES_COMMANDS,",
            "    ),",
            "    ActionCommand(",
            "        name='sync-perm',",
            "        help=\"Update permissions for existing roles and optionally DAGs\",",
            "        func=lazy_load_command('airflow.cli.commands.sync_perm_command.sync_perm'),",
            "        args=(ARG_INCLUDE_DAGS,),",
            "    ),",
            "    ActionCommand(",
            "        name='rotate-fernet-key',",
            "        func=lazy_load_command('airflow.cli.commands.rotate_fernet_key_command.rotate_fernet_key'),",
            "        help='Rotate encrypted connection credentials and variables',",
            "        description=(",
            "            'Rotate all encrypted connection credentials and variables; see '",
            "            'https://airflow.apache.org/docs/apache-airflow/stable/howto/secure-connections.html'",
            "            '#rotating-encryption-keys'",
            "        ),",
            "        args=(),",
            "    ),",
            "    GroupCommand(name=\"config\", help='View configuration', subcommands=CONFIG_COMMANDS),",
            "    ActionCommand(",
            "        name='info',",
            "        help='Show information about current Airflow and environment',",
            "        func=lazy_load_command('airflow.cli.commands.info_command.show_info'),",
            "        args=(",
            "            ARG_ANONYMIZE,",
            "            ARG_FILE_IO,",
            "            ARG_VERBOSE,",
            "            ARG_OUTPUT,",
            "        ),",
            "    ),",
            "    ActionCommand(",
            "        name='plugins',",
            "        help='Dump information about loaded plugins',",
            "        func=lazy_load_command('airflow.cli.commands.plugins_command.dump_plugins'),",
            "        args=(ARG_OUTPUT, ARG_VERBOSE),",
            "    ),",
            "    GroupCommand(",
            "        name=\"celery\",",
            "        help='Celery components',",
            "        description=(",
            "            'Start celery components. Works only when using CeleryExecutor. For more information, see '",
            "            'https://airflow.apache.org/docs/apache-airflow/stable/executor/celery.html'",
            "        ),",
            "        subcommands=CELERY_COMMANDS,",
            "    ),",
            "    ActionCommand(",
            "        name='standalone',",
            "        help='Run an all-in-one copy of Airflow',",
            "        func=lazy_load_command('airflow.cli.commands.standalone_command.standalone'),",
            "        args=tuple(),",
            "    ),",
            "]",
            "ALL_COMMANDS_DICT: Dict[str, CLICommand] = {sp.name: sp for sp in airflow_commands}",
            "",
            "",
            "def _remove_dag_id_opt(command: ActionCommand):",
            "    cmd = command._asdict()",
            "    cmd['args'] = (arg for arg in command.args if arg is not ARG_DAG_ID)",
            "    return ActionCommand(**cmd)",
            "",
            "",
            "dag_cli_commands: List[CLICommand] = [",
            "    GroupCommand(",
            "        name='dags',",
            "        help='Manage DAGs',",
            "        subcommands=[",
            "            _remove_dag_id_opt(sp)",
            "            for sp in DAGS_COMMANDS",
            "            if sp.name in ['backfill', 'list-runs', 'pause', 'unpause']",
            "        ],",
            "    ),",
            "    GroupCommand(",
            "        name='tasks',",
            "        help='Manage tasks',",
            "        subcommands=[_remove_dag_id_opt(sp) for sp in TASKS_COMMANDS if sp.name in ['list', 'test', 'run']],",
            "    ),",
            "]",
            "DAG_CLI_DICT: Dict[str, CLICommand] = {sp.name: sp for sp in dag_cli_commands}",
            "",
            "",
            "class AirflowHelpFormatter(argparse.HelpFormatter):",
            "    \"\"\"",
            "    Custom help formatter to display help message.",
            "",
            "    It displays simple commands and groups of commands in separate sections.",
            "    \"\"\"",
            "",
            "    def _format_action(self, action: Action):",
            "        if isinstance(action, argparse._SubParsersAction):",
            "",
            "            parts = []",
            "            action_header = self._format_action_invocation(action)",
            "            action_header = '%*s%s\\n' % (self._current_indent, '', action_header)",
            "            parts.append(action_header)",
            "",
            "            self._indent()",
            "            subactions = action._get_subactions()",
            "            action_subcommands, group_subcommands = partition(",
            "                lambda d: isinstance(ALL_COMMANDS_DICT[d.dest], GroupCommand), subactions",
            "            )",
            "            parts.append(\"\\n\")",
            "            parts.append('%*s%s:\\n' % (self._current_indent, '', \"Groups\"))",
            "            self._indent()",
            "            for subaction in group_subcommands:",
            "                parts.append(self._format_action(subaction))",
            "            self._dedent()",
            "",
            "            parts.append(\"\\n\")",
            "            parts.append('%*s%s:\\n' % (self._current_indent, '', \"Commands\"))",
            "            self._indent()",
            "",
            "            for subaction in action_subcommands:",
            "                parts.append(self._format_action(subaction))",
            "            self._dedent()",
            "            self._dedent()",
            "",
            "            # return a single string",
            "            return self._join_parts(parts)",
            "",
            "        return super()._format_action(action)",
            "",
            "",
            "@lru_cache(maxsize=None)",
            "def get_parser(dag_parser: bool = False) -> argparse.ArgumentParser:",
            "    \"\"\"Creates and returns command line argument parser\"\"\"",
            "    parser = DefaultHelpParser(prog=\"airflow\", formatter_class=AirflowHelpFormatter)",
            "    subparsers = parser.add_subparsers(dest='subcommand', metavar=\"GROUP_OR_COMMAND\")",
            "    subparsers.required = True",
            "",
            "    command_dict = DAG_CLI_DICT if dag_parser else ALL_COMMANDS_DICT",
            "    subparser_list = command_dict.keys()",
            "    sub_name: str",
            "    for sub_name in sorted(subparser_list):",
            "        sub: CLICommand = command_dict[sub_name]",
            "        _add_command(subparsers, sub)",
            "    return parser",
            "",
            "",
            "def _sort_args(args: Iterable[Arg]) -> Iterable[Arg]:",
            "    \"\"\"Sort subcommand optional args, keep positional args\"\"\"",
            "",
            "    def get_long_option(arg: Arg):",
            "        \"\"\"Get long option from Arg.flags\"\"\"",
            "        return arg.flags[0] if len(arg.flags) == 1 else arg.flags[1]",
            "",
            "    positional, optional = partition(lambda x: x.flags[0].startswith(\"-\"), args)",
            "    yield from positional",
            "    yield from sorted(optional, key=lambda x: get_long_option(x).lower())",
            "",
            "",
            "def _add_command(subparsers: argparse._SubParsersAction, sub: CLICommand) -> None:",
            "    sub_proc = subparsers.add_parser(",
            "        sub.name, help=sub.help, description=sub.description or sub.help, epilog=sub.epilog",
            "    )",
            "    sub_proc.formatter_class = RawTextHelpFormatter",
            "",
            "    if isinstance(sub, GroupCommand):",
            "        _add_group_command(sub, sub_proc)",
            "    elif isinstance(sub, ActionCommand):",
            "        _add_action_command(sub, sub_proc)",
            "    else:",
            "        raise AirflowException(\"Invalid command definition.\")",
            "",
            "",
            "def _add_action_command(sub: ActionCommand, sub_proc: argparse.ArgumentParser) -> None:",
            "    for arg in _sort_args(sub.args):",
            "        arg.add_to_parser(sub_proc)",
            "    sub_proc.set_defaults(func=sub.func)",
            "",
            "",
            "def _add_group_command(sub: GroupCommand, sub_proc: argparse.ArgumentParser) -> None:",
            "    subcommands = sub.subcommands",
            "    sub_subparsers = sub_proc.add_subparsers(dest=\"subcommand\", metavar=\"COMMAND\")",
            "    sub_subparsers.required = True",
            "",
            "    for command in sorted(subcommands, key=lambda x: x.name):",
            "        _add_command(sub_subparsers, command)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "670": []
        },
        "addLocation": []
    },
    "airflow/cli/commands/celery_command.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "                 pidfile=TimeoutPIDLockFile(pidfile, -1),"
            },
            "1": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "                 stdout=stdout,"
            },
            "2": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 74,
                "PatchRowcode": "                 stderr=stderr,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+                umask=int(settings.DAEMON_UMASK, 8),"
            },
            "4": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 76,
                "PatchRowcode": "             )"
            },
            "5": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 77,
                "PatchRowcode": "             with ctx:"
            },
            "6": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 78,
                "PatchRowcode": "                 celery_app.start(options)"
            },
            "7": {
                "beforePatchRowNumber": 180,
                "afterPatchRowNumber": 181,
                "PatchRowcode": "         with open(stdout, 'w+') as stdout_handle, open(stderr, 'w+') as stderr_handle:"
            },
            "8": {
                "beforePatchRowNumber": 181,
                "afterPatchRowNumber": 182,
                "PatchRowcode": "             if args.umask:"
            },
            "9": {
                "beforePatchRowNumber": 182,
                "afterPatchRowNumber": 183,
                "PatchRowcode": "                 umask = args.umask"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 184,
                "PatchRowcode": "+            else:"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 185,
                "PatchRowcode": "+                umask = conf.get('celery', 'worker_umask', fallback=settings.DAEMON_UMASK)"
            },
            "12": {
                "beforePatchRowNumber": 183,
                "afterPatchRowNumber": 186,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 184,
                "afterPatchRowNumber": 187,
                "PatchRowcode": "             ctx = daemon.DaemonContext("
            },
            "14": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 188,
                "PatchRowcode": "                 files_preserve=[handle],"
            }
        },
        "frontPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"Celery command\"\"\"",
            "",
            "from multiprocessing import Process",
            "from typing import Optional",
            "",
            "import daemon",
            "import psutil",
            "import sqlalchemy.exc",
            "from celery import maybe_patch_concurrency  # type: ignore[attr-defined]",
            "from daemon.pidfile import TimeoutPIDLockFile",
            "from lockfile.pidlockfile import read_pid_from_pidfile, remove_existing_pidfile",
            "",
            "from airflow import settings",
            "from airflow.configuration import conf",
            "from airflow.executors.celery_executor import app as celery_app",
            "from airflow.utils import cli as cli_utils",
            "from airflow.utils.cli import setup_locations, setup_logging",
            "from airflow.utils.serve_logs import serve_logs",
            "",
            "WORKER_PROCESS_NAME = \"worker\"",
            "",
            "",
            "@cli_utils.action_cli",
            "def flower(args):",
            "    \"\"\"Starts Flower, Celery monitoring tool\"\"\"",
            "    options = [",
            "        \"flower\",",
            "        conf.get('celery', 'BROKER_URL'),",
            "        f\"--address={args.hostname}\",",
            "        f\"--port={args.port}\",",
            "    ]",
            "",
            "    if args.broker_api:",
            "        options.append(f\"--broker-api={args.broker_api}\")",
            "",
            "    if args.url_prefix:",
            "        options.append(f\"--url-prefix={args.url_prefix}\")",
            "",
            "    if args.basic_auth:",
            "        options.append(f\"--basic-auth={args.basic_auth}\")",
            "",
            "    if args.flower_conf:",
            "        options.append(f\"--conf={args.flower_conf}\")",
            "",
            "    if args.daemon:",
            "        pidfile, stdout, stderr, _ = setup_locations(",
            "            process=\"flower\",",
            "            pid=args.pid,",
            "            stdout=args.stdout,",
            "            stderr=args.stderr,",
            "            log=args.log_file,",
            "        )",
            "        with open(stdout, \"w+\") as stdout, open(stderr, \"w+\") as stderr:",
            "            ctx = daemon.DaemonContext(",
            "                pidfile=TimeoutPIDLockFile(pidfile, -1),",
            "                stdout=stdout,",
            "                stderr=stderr,",
            "            )",
            "            with ctx:",
            "                celery_app.start(options)",
            "    else:",
            "        celery_app.start(options)",
            "",
            "",
            "def _serve_logs(skip_serve_logs: bool = False) -> Optional[Process]:",
            "    \"\"\"Starts serve_logs sub-process\"\"\"",
            "    if skip_serve_logs is False:",
            "        sub_proc = Process(target=serve_logs)",
            "        sub_proc.start()",
            "        return sub_proc",
            "    return None",
            "",
            "",
            "def _run_worker(options, skip_serve_logs):",
            "    sub_proc = _serve_logs(skip_serve_logs)",
            "    try:",
            "        celery_app.worker_main(options)",
            "    finally:",
            "        if sub_proc:",
            "            sub_proc.terminate()",
            "",
            "",
            "@cli_utils.action_cli",
            "def worker(args):",
            "    \"\"\"Starts Airflow Celery worker\"\"\"",
            "    # Disable connection pool so that celery worker does not hold an unnecessary db connection",
            "    settings.reconfigure_orm(disable_connection_pool=True)",
            "    if not settings.validate_session():",
            "        raise SystemExit(\"Worker exiting, database connection precheck failed.\")",
            "",
            "    autoscale = args.autoscale",
            "    skip_serve_logs = args.skip_serve_logs",
            "",
            "    if autoscale is None and conf.has_option(\"celery\", \"worker_autoscale\"):",
            "        autoscale = conf.get(\"celery\", \"worker_autoscale\")",
            "",
            "    # Setup locations",
            "    pid_file_path, stdout, stderr, log_file = setup_locations(",
            "        process=WORKER_PROCESS_NAME,",
            "        pid=args.pid,",
            "        stdout=args.stdout,",
            "        stderr=args.stderr,",
            "        log=args.log_file,",
            "    )",
            "",
            "    if hasattr(celery_app.backend, 'ResultSession'):",
            "        # Pre-create the database tables now, otherwise SQLA via Celery has a",
            "        # race condition where one of the subprocesses can die with \"Table",
            "        # already exists\" error, because SQLA checks for which tables exist,",
            "        # then issues a CREATE TABLE, rather than doing CREATE TABLE IF NOT",
            "        # EXISTS",
            "        try:",
            "            session = celery_app.backend.ResultSession()",
            "            session.close()",
            "        except sqlalchemy.exc.IntegrityError:",
            "            # At least on postgres, trying to create a table that already exist",
            "            # gives a unique constraint violation or the",
            "            # \"pg_type_typname_nsp_index\" table. If this happens we can ignore",
            "            # it, we raced to create the tables and lost.",
            "            pass",
            "",
            "    # backwards-compatible: https://github.com/apache/airflow/pull/21506#pullrequestreview-879893763",
            "    celery_log_level = conf.get('logging', 'CELERY_LOGGING_LEVEL')",
            "    if not celery_log_level:",
            "        celery_log_level = conf.get('logging', 'LOGGING_LEVEL')",
            "    # Setup Celery worker",
            "    options = [",
            "        'worker',",
            "        '-O',",
            "        'fair',",
            "        '--queues',",
            "        args.queues,",
            "        '--concurrency',",
            "        args.concurrency,",
            "        '--hostname',",
            "        args.celery_hostname,",
            "        '--loglevel',",
            "        celery_log_level,",
            "        '--pidfile',",
            "        pid_file_path,",
            "    ]",
            "    if autoscale:",
            "        options.extend(['--autoscale', autoscale])",
            "    if args.without_mingle:",
            "        options.append('--without-mingle')",
            "    if args.without_gossip:",
            "        options.append('--without-gossip')",
            "",
            "    if conf.has_option(\"celery\", \"pool\"):",
            "        pool = conf.get(\"celery\", \"pool\")",
            "        options.extend([\"--pool\", pool])",
            "        # Celery pools of type eventlet and gevent use greenlets, which",
            "        # requires monkey patching the app:",
            "        # https://eventlet.net/doc/patching.html#monkey-patch",
            "        # Otherwise task instances hang on the workers and are never",
            "        # executed.",
            "        maybe_patch_concurrency(['-P', pool])",
            "",
            "    if args.daemon:",
            "        # Run Celery worker as daemon",
            "        handle = setup_logging(log_file)",
            "",
            "        with open(stdout, 'w+') as stdout_handle, open(stderr, 'w+') as stderr_handle:",
            "            if args.umask:",
            "                umask = args.umask",
            "",
            "            ctx = daemon.DaemonContext(",
            "                files_preserve=[handle],",
            "                umask=int(umask, 8),",
            "                stdout=stdout_handle,",
            "                stderr=stderr_handle,",
            "            )",
            "            with ctx:",
            "                _run_worker(options=options, skip_serve_logs=skip_serve_logs)",
            "    else:",
            "        # Run Celery worker in the same process",
            "        _run_worker(options=options, skip_serve_logs=skip_serve_logs)",
            "",
            "",
            "@cli_utils.action_cli",
            "def stop_worker(args):",
            "    \"\"\"Sends SIGTERM to Celery worker\"\"\"",
            "    # Read PID from file",
            "    if args.pid:",
            "        pid_file_path = args.pid",
            "    else:",
            "        pid_file_path, _, _, _ = setup_locations(process=WORKER_PROCESS_NAME)",
            "    pid = read_pid_from_pidfile(pid_file_path)",
            "",
            "    # Send SIGTERM",
            "    if pid:",
            "        worker_process = psutil.Process(pid)",
            "        worker_process.terminate()",
            "",
            "    # Remove pid file",
            "    remove_existing_pidfile(pid_file_path)"
        ],
        "afterPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"Celery command\"\"\"",
            "",
            "from multiprocessing import Process",
            "from typing import Optional",
            "",
            "import daemon",
            "import psutil",
            "import sqlalchemy.exc",
            "from celery import maybe_patch_concurrency  # type: ignore[attr-defined]",
            "from daemon.pidfile import TimeoutPIDLockFile",
            "from lockfile.pidlockfile import read_pid_from_pidfile, remove_existing_pidfile",
            "",
            "from airflow import settings",
            "from airflow.configuration import conf",
            "from airflow.executors.celery_executor import app as celery_app",
            "from airflow.utils import cli as cli_utils",
            "from airflow.utils.cli import setup_locations, setup_logging",
            "from airflow.utils.serve_logs import serve_logs",
            "",
            "WORKER_PROCESS_NAME = \"worker\"",
            "",
            "",
            "@cli_utils.action_cli",
            "def flower(args):",
            "    \"\"\"Starts Flower, Celery monitoring tool\"\"\"",
            "    options = [",
            "        \"flower\",",
            "        conf.get('celery', 'BROKER_URL'),",
            "        f\"--address={args.hostname}\",",
            "        f\"--port={args.port}\",",
            "    ]",
            "",
            "    if args.broker_api:",
            "        options.append(f\"--broker-api={args.broker_api}\")",
            "",
            "    if args.url_prefix:",
            "        options.append(f\"--url-prefix={args.url_prefix}\")",
            "",
            "    if args.basic_auth:",
            "        options.append(f\"--basic-auth={args.basic_auth}\")",
            "",
            "    if args.flower_conf:",
            "        options.append(f\"--conf={args.flower_conf}\")",
            "",
            "    if args.daemon:",
            "        pidfile, stdout, stderr, _ = setup_locations(",
            "            process=\"flower\",",
            "            pid=args.pid,",
            "            stdout=args.stdout,",
            "            stderr=args.stderr,",
            "            log=args.log_file,",
            "        )",
            "        with open(stdout, \"w+\") as stdout, open(stderr, \"w+\") as stderr:",
            "            ctx = daemon.DaemonContext(",
            "                pidfile=TimeoutPIDLockFile(pidfile, -1),",
            "                stdout=stdout,",
            "                stderr=stderr,",
            "                umask=int(settings.DAEMON_UMASK, 8),",
            "            )",
            "            with ctx:",
            "                celery_app.start(options)",
            "    else:",
            "        celery_app.start(options)",
            "",
            "",
            "def _serve_logs(skip_serve_logs: bool = False) -> Optional[Process]:",
            "    \"\"\"Starts serve_logs sub-process\"\"\"",
            "    if skip_serve_logs is False:",
            "        sub_proc = Process(target=serve_logs)",
            "        sub_proc.start()",
            "        return sub_proc",
            "    return None",
            "",
            "",
            "def _run_worker(options, skip_serve_logs):",
            "    sub_proc = _serve_logs(skip_serve_logs)",
            "    try:",
            "        celery_app.worker_main(options)",
            "    finally:",
            "        if sub_proc:",
            "            sub_proc.terminate()",
            "",
            "",
            "@cli_utils.action_cli",
            "def worker(args):",
            "    \"\"\"Starts Airflow Celery worker\"\"\"",
            "    # Disable connection pool so that celery worker does not hold an unnecessary db connection",
            "    settings.reconfigure_orm(disable_connection_pool=True)",
            "    if not settings.validate_session():",
            "        raise SystemExit(\"Worker exiting, database connection precheck failed.\")",
            "",
            "    autoscale = args.autoscale",
            "    skip_serve_logs = args.skip_serve_logs",
            "",
            "    if autoscale is None and conf.has_option(\"celery\", \"worker_autoscale\"):",
            "        autoscale = conf.get(\"celery\", \"worker_autoscale\")",
            "",
            "    # Setup locations",
            "    pid_file_path, stdout, stderr, log_file = setup_locations(",
            "        process=WORKER_PROCESS_NAME,",
            "        pid=args.pid,",
            "        stdout=args.stdout,",
            "        stderr=args.stderr,",
            "        log=args.log_file,",
            "    )",
            "",
            "    if hasattr(celery_app.backend, 'ResultSession'):",
            "        # Pre-create the database tables now, otherwise SQLA via Celery has a",
            "        # race condition where one of the subprocesses can die with \"Table",
            "        # already exists\" error, because SQLA checks for which tables exist,",
            "        # then issues a CREATE TABLE, rather than doing CREATE TABLE IF NOT",
            "        # EXISTS",
            "        try:",
            "            session = celery_app.backend.ResultSession()",
            "            session.close()",
            "        except sqlalchemy.exc.IntegrityError:",
            "            # At least on postgres, trying to create a table that already exist",
            "            # gives a unique constraint violation or the",
            "            # \"pg_type_typname_nsp_index\" table. If this happens we can ignore",
            "            # it, we raced to create the tables and lost.",
            "            pass",
            "",
            "    # backwards-compatible: https://github.com/apache/airflow/pull/21506#pullrequestreview-879893763",
            "    celery_log_level = conf.get('logging', 'CELERY_LOGGING_LEVEL')",
            "    if not celery_log_level:",
            "        celery_log_level = conf.get('logging', 'LOGGING_LEVEL')",
            "    # Setup Celery worker",
            "    options = [",
            "        'worker',",
            "        '-O',",
            "        'fair',",
            "        '--queues',",
            "        args.queues,",
            "        '--concurrency',",
            "        args.concurrency,",
            "        '--hostname',",
            "        args.celery_hostname,",
            "        '--loglevel',",
            "        celery_log_level,",
            "        '--pidfile',",
            "        pid_file_path,",
            "    ]",
            "    if autoscale:",
            "        options.extend(['--autoscale', autoscale])",
            "    if args.without_mingle:",
            "        options.append('--without-mingle')",
            "    if args.without_gossip:",
            "        options.append('--without-gossip')",
            "",
            "    if conf.has_option(\"celery\", \"pool\"):",
            "        pool = conf.get(\"celery\", \"pool\")",
            "        options.extend([\"--pool\", pool])",
            "        # Celery pools of type eventlet and gevent use greenlets, which",
            "        # requires monkey patching the app:",
            "        # https://eventlet.net/doc/patching.html#monkey-patch",
            "        # Otherwise task instances hang on the workers and are never",
            "        # executed.",
            "        maybe_patch_concurrency(['-P', pool])",
            "",
            "    if args.daemon:",
            "        # Run Celery worker as daemon",
            "        handle = setup_logging(log_file)",
            "",
            "        with open(stdout, 'w+') as stdout_handle, open(stderr, 'w+') as stderr_handle:",
            "            if args.umask:",
            "                umask = args.umask",
            "            else:",
            "                umask = conf.get('celery', 'worker_umask', fallback=settings.DAEMON_UMASK)",
            "",
            "            ctx = daemon.DaemonContext(",
            "                files_preserve=[handle],",
            "                umask=int(umask, 8),",
            "                stdout=stdout_handle,",
            "                stderr=stderr_handle,",
            "            )",
            "            with ctx:",
            "                _run_worker(options=options, skip_serve_logs=skip_serve_logs)",
            "    else:",
            "        # Run Celery worker in the same process",
            "        _run_worker(options=options, skip_serve_logs=skip_serve_logs)",
            "",
            "",
            "@cli_utils.action_cli",
            "def stop_worker(args):",
            "    \"\"\"Sends SIGTERM to Celery worker\"\"\"",
            "    # Read PID from file",
            "    if args.pid:",
            "        pid_file_path = args.pid",
            "    else:",
            "        pid_file_path, _, _, _ = setup_locations(process=WORKER_PROCESS_NAME)",
            "    pid = read_pid_from_pidfile(pid_file_path)",
            "",
            "    # Send SIGTERM",
            "    if pid:",
            "        worker_process = psutil.Process(pid)",
            "        worker_process.terminate()",
            "",
            "    # Remove pid file",
            "    remove_existing_pidfile(pid_file_path)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "airflow.cli.commands.celery_command.flower.options",
            "airflow.cli.cli_parser",
            "airflow.cli.commands.celery_command.worker.options",
            "airflow.cli.commands.celery_command._run_worker.options"
        ]
    },
    "airflow/cli/commands/dag_processor_command.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " import daemon"
            },
            "1": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " from daemon.pidfile import TimeoutPIDLockFile"
            },
            "2": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+from airflow import settings"
            },
            "4": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from airflow.configuration import conf"
            },
            "5": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " from airflow.dag_processing.manager import DagFileProcessorManager"
            },
            "6": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from airflow.utils import cli as cli_utils"
            },
            "7": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 67,
                "PatchRowcode": "                 files_preserve=[handle],"
            },
            "8": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 68,
                "PatchRowcode": "                 stdout=stdout_handle,"
            },
            "9": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 69,
                "PatchRowcode": "                 stderr=stderr_handle,"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 70,
                "PatchRowcode": "+                umask=int(settings.DAEMON_UMASK, 8),"
            },
            "11": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 71,
                "PatchRowcode": "             )"
            },
            "12": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "             with ctx:"
            },
            "13": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "                 try:"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "\"\"\"DagProcessor command\"\"\"",
            "import logging",
            "from datetime import timedelta",
            "",
            "import daemon",
            "from daemon.pidfile import TimeoutPIDLockFile",
            "",
            "from airflow.configuration import conf",
            "from airflow.dag_processing.manager import DagFileProcessorManager",
            "from airflow.utils import cli as cli_utils",
            "from airflow.utils.cli import setup_locations, setup_logging",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "def _create_dag_processor_manager(args) -> DagFileProcessorManager:",
            "    \"\"\"Creates DagFileProcessorProcess instance.\"\"\"",
            "    processor_timeout_seconds: int = conf.getint('core', 'dag_file_processor_timeout')",
            "    processor_timeout = timedelta(seconds=processor_timeout_seconds)",
            "    return DagFileProcessorManager(",
            "        dag_directory=args.subdir,",
            "        max_runs=args.num_runs,",
            "        processor_timeout=processor_timeout,",
            "        dag_ids=[],",
            "        pickle_dags=args.do_pickle,",
            "    )",
            "",
            "",
            "@cli_utils.action_cli",
            "def dag_processor(args):",
            "    \"\"\"Starts Airflow Dag Processor Job\"\"\"",
            "    if not conf.getboolean(\"scheduler\", \"standalone_dag_processor\"):",
            "        raise SystemExit('The option [scheduler/standalone_dag_processor] must be True.')",
            "",
            "    sql_conn: str = conf.get('database', 'sql_alchemy_conn').lower()",
            "    if sql_conn.startswith('sqlite'):",
            "        raise SystemExit('Standalone DagProcessor is not supported when using sqlite.')",
            "",
            "    manager = _create_dag_processor_manager(args)",
            "",
            "    if args.daemon:",
            "        pid, stdout, stderr, log_file = setup_locations(",
            "            \"dag-processor\", args.pid, args.stdout, args.stderr, args.log_file",
            "        )",
            "        handle = setup_logging(log_file)",
            "        with open(stdout, 'w+') as stdout_handle, open(stderr, 'w+') as stderr_handle:",
            "            ctx = daemon.DaemonContext(",
            "                pidfile=TimeoutPIDLockFile(pid, -1),",
            "                files_preserve=[handle],",
            "                stdout=stdout_handle,",
            "                stderr=stderr_handle,",
            "            )",
            "            with ctx:",
            "                try:",
            "                    manager.start()",
            "                finally:",
            "                    manager.terminate()",
            "                    manager.end()",
            "    else:",
            "        manager.start()"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "\"\"\"DagProcessor command\"\"\"",
            "import logging",
            "from datetime import timedelta",
            "",
            "import daemon",
            "from daemon.pidfile import TimeoutPIDLockFile",
            "",
            "from airflow import settings",
            "from airflow.configuration import conf",
            "from airflow.dag_processing.manager import DagFileProcessorManager",
            "from airflow.utils import cli as cli_utils",
            "from airflow.utils.cli import setup_locations, setup_logging",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "def _create_dag_processor_manager(args) -> DagFileProcessorManager:",
            "    \"\"\"Creates DagFileProcessorProcess instance.\"\"\"",
            "    processor_timeout_seconds: int = conf.getint('core', 'dag_file_processor_timeout')",
            "    processor_timeout = timedelta(seconds=processor_timeout_seconds)",
            "    return DagFileProcessorManager(",
            "        dag_directory=args.subdir,",
            "        max_runs=args.num_runs,",
            "        processor_timeout=processor_timeout,",
            "        dag_ids=[],",
            "        pickle_dags=args.do_pickle,",
            "    )",
            "",
            "",
            "@cli_utils.action_cli",
            "def dag_processor(args):",
            "    \"\"\"Starts Airflow Dag Processor Job\"\"\"",
            "    if not conf.getboolean(\"scheduler\", \"standalone_dag_processor\"):",
            "        raise SystemExit('The option [scheduler/standalone_dag_processor] must be True.')",
            "",
            "    sql_conn: str = conf.get('database', 'sql_alchemy_conn').lower()",
            "    if sql_conn.startswith('sqlite'):",
            "        raise SystemExit('Standalone DagProcessor is not supported when using sqlite.')",
            "",
            "    manager = _create_dag_processor_manager(args)",
            "",
            "    if args.daemon:",
            "        pid, stdout, stderr, log_file = setup_locations(",
            "            \"dag-processor\", args.pid, args.stdout, args.stderr, args.log_file",
            "        )",
            "        handle = setup_logging(log_file)",
            "        with open(stdout, 'w+') as stdout_handle, open(stderr, 'w+') as stderr_handle:",
            "            ctx = daemon.DaemonContext(",
            "                pidfile=TimeoutPIDLockFile(pid, -1),",
            "                files_preserve=[handle],",
            "                stdout=stdout_handle,",
            "                stderr=stderr_handle,",
            "                umask=int(settings.DAEMON_UMASK, 8),",
            "            )",
            "            with ctx:",
            "                try:",
            "                    manager.start()",
            "                finally:",
            "                    manager.terminate()",
            "                    manager.end()",
            "    else:",
            "        manager.start()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "airflow.cli.cli_parser"
        ]
    },
    "airflow/cli/commands/kerberos_command.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 39,
                "PatchRowcode": "                 pidfile=TimeoutPIDLockFile(pid, -1),"
            },
            "1": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 40,
                "PatchRowcode": "                 stdout=stdout_handle,"
            },
            "2": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 41,
                "PatchRowcode": "                 stderr=stderr_handle,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+                umask=int(settings.DAEMON_UMASK, 8),"
            },
            "4": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 43,
                "PatchRowcode": "             )"
            },
            "5": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 44,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 45,
                "PatchRowcode": "             with ctx:"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "\"\"\"Kerberos command\"\"\"",
            "import daemon",
            "from daemon.pidfile import TimeoutPIDLockFile",
            "",
            "from airflow import settings",
            "from airflow.security import kerberos as krb",
            "from airflow.utils import cli as cli_utils",
            "from airflow.utils.cli import setup_locations",
            "",
            "",
            "@cli_utils.action_cli",
            "def kerberos(args):",
            "    \"\"\"Start a kerberos ticket renewer\"\"\"",
            "    print(settings.HEADER)",
            "",
            "    if args.daemon:",
            "        pid, stdout, stderr, _ = setup_locations(",
            "            \"kerberos\", args.pid, args.stdout, args.stderr, args.log_file",
            "        )",
            "        with open(stdout, 'w+') as stdout_handle, open(stderr, 'w+') as stderr_handle:",
            "            ctx = daemon.DaemonContext(",
            "                pidfile=TimeoutPIDLockFile(pid, -1),",
            "                stdout=stdout_handle,",
            "                stderr=stderr_handle,",
            "            )",
            "",
            "            with ctx:",
            "                krb.run(principal=args.principal, keytab=args.keytab)",
            "    else:",
            "        krb.run(principal=args.principal, keytab=args.keytab)"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "\"\"\"Kerberos command\"\"\"",
            "import daemon",
            "from daemon.pidfile import TimeoutPIDLockFile",
            "",
            "from airflow import settings",
            "from airflow.security import kerberos as krb",
            "from airflow.utils import cli as cli_utils",
            "from airflow.utils.cli import setup_locations",
            "",
            "",
            "@cli_utils.action_cli",
            "def kerberos(args):",
            "    \"\"\"Start a kerberos ticket renewer\"\"\"",
            "    print(settings.HEADER)",
            "",
            "    if args.daemon:",
            "        pid, stdout, stderr, _ = setup_locations(",
            "            \"kerberos\", args.pid, args.stdout, args.stderr, args.log_file",
            "        )",
            "        with open(stdout, 'w+') as stdout_handle, open(stderr, 'w+') as stderr_handle:",
            "            ctx = daemon.DaemonContext(",
            "                pidfile=TimeoutPIDLockFile(pid, -1),",
            "                stdout=stdout_handle,",
            "                stderr=stderr_handle,",
            "                umask=int(settings.DAEMON_UMASK, 8),",
            "            )",
            "",
            "            with ctx:",
            "                krb.run(principal=args.principal, keytab=args.keytab)",
            "    else:",
            "        krb.run(principal=args.principal, keytab=args.keytab)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "airflow.cli.commands.kerberos_command",
            "airflow.cli.cli_parser",
            "airflow.cli.commands.kerberos_command.krb"
        ]
    },
    "airflow/cli/commands/scheduler_command.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 65,
                "PatchRowcode": "                 files_preserve=[handle],"
            },
            "1": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 66,
                "PatchRowcode": "                 stdout=stdout_handle,"
            },
            "2": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 67,
                "PatchRowcode": "                 stderr=stderr_handle,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 68,
                "PatchRowcode": "+                umask=int(settings.DAEMON_UMASK, 8),"
            },
            "4": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 69,
                "PatchRowcode": "             )"
            },
            "5": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 70,
                "PatchRowcode": "             with ctx:"
            },
            "6": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 71,
                "PatchRowcode": "                 _run_scheduler_job(args=args)"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "\"\"\"Scheduler command\"\"\"",
            "import signal",
            "from multiprocessing import Process",
            "from typing import Optional",
            "",
            "import daemon",
            "from daemon.pidfile import TimeoutPIDLockFile",
            "",
            "from airflow import settings",
            "from airflow.jobs.scheduler_job import SchedulerJob",
            "from airflow.utils import cli as cli_utils",
            "from airflow.utils.cli import process_subdir, setup_locations, setup_logging, sigint_handler, sigquit_handler",
            "",
            "",
            "def _create_scheduler_job(args):",
            "    job = SchedulerJob(",
            "        subdir=process_subdir(args.subdir),",
            "        num_runs=args.num_runs,",
            "        do_pickle=args.do_pickle,",
            "    )",
            "    return job",
            "",
            "",
            "def _run_scheduler_job(args):",
            "    skip_serve_logs = args.skip_serve_logs",
            "    job = _create_scheduler_job(args)",
            "    sub_proc = _serve_logs(skip_serve_logs)",
            "    try:",
            "        job.run()",
            "    finally:",
            "        if sub_proc:",
            "            sub_proc.terminate()",
            "",
            "",
            "@cli_utils.action_cli",
            "def scheduler(args):",
            "    \"\"\"Starts Airflow Scheduler\"\"\"",
            "    print(settings.HEADER)",
            "",
            "    if args.daemon:",
            "        pid, stdout, stderr, log_file = setup_locations(",
            "            \"scheduler\", args.pid, args.stdout, args.stderr, args.log_file",
            "        )",
            "        handle = setup_logging(log_file)",
            "        with open(stdout, 'w+') as stdout_handle, open(stderr, 'w+') as stderr_handle:",
            "            ctx = daemon.DaemonContext(",
            "                pidfile=TimeoutPIDLockFile(pid, -1),",
            "                files_preserve=[handle],",
            "                stdout=stdout_handle,",
            "                stderr=stderr_handle,",
            "            )",
            "            with ctx:",
            "                _run_scheduler_job(args=args)",
            "    else:",
            "        signal.signal(signal.SIGINT, sigint_handler)",
            "        signal.signal(signal.SIGTERM, sigint_handler)",
            "        signal.signal(signal.SIGQUIT, sigquit_handler)",
            "        _run_scheduler_job(args=args)",
            "",
            "",
            "def _serve_logs(skip_serve_logs: bool = False) -> Optional[Process]:",
            "    \"\"\"Starts serve_logs sub-process\"\"\"",
            "    from airflow.configuration import conf",
            "    from airflow.utils.serve_logs import serve_logs",
            "",
            "    if conf.get(\"core\", \"executor\") in [\"LocalExecutor\", \"SequentialExecutor\"]:",
            "        if skip_serve_logs is False:",
            "            sub_proc = Process(target=serve_logs)",
            "            sub_proc.start()",
            "            return sub_proc",
            "    return None"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "\"\"\"Scheduler command\"\"\"",
            "import signal",
            "from multiprocessing import Process",
            "from typing import Optional",
            "",
            "import daemon",
            "from daemon.pidfile import TimeoutPIDLockFile",
            "",
            "from airflow import settings",
            "from airflow.jobs.scheduler_job import SchedulerJob",
            "from airflow.utils import cli as cli_utils",
            "from airflow.utils.cli import process_subdir, setup_locations, setup_logging, sigint_handler, sigquit_handler",
            "",
            "",
            "def _create_scheduler_job(args):",
            "    job = SchedulerJob(",
            "        subdir=process_subdir(args.subdir),",
            "        num_runs=args.num_runs,",
            "        do_pickle=args.do_pickle,",
            "    )",
            "    return job",
            "",
            "",
            "def _run_scheduler_job(args):",
            "    skip_serve_logs = args.skip_serve_logs",
            "    job = _create_scheduler_job(args)",
            "    sub_proc = _serve_logs(skip_serve_logs)",
            "    try:",
            "        job.run()",
            "    finally:",
            "        if sub_proc:",
            "            sub_proc.terminate()",
            "",
            "",
            "@cli_utils.action_cli",
            "def scheduler(args):",
            "    \"\"\"Starts Airflow Scheduler\"\"\"",
            "    print(settings.HEADER)",
            "",
            "    if args.daemon:",
            "        pid, stdout, stderr, log_file = setup_locations(",
            "            \"scheduler\", args.pid, args.stdout, args.stderr, args.log_file",
            "        )",
            "        handle = setup_logging(log_file)",
            "        with open(stdout, 'w+') as stdout_handle, open(stderr, 'w+') as stderr_handle:",
            "            ctx = daemon.DaemonContext(",
            "                pidfile=TimeoutPIDLockFile(pid, -1),",
            "                files_preserve=[handle],",
            "                stdout=stdout_handle,",
            "                stderr=stderr_handle,",
            "                umask=int(settings.DAEMON_UMASK, 8),",
            "            )",
            "            with ctx:",
            "                _run_scheduler_job(args=args)",
            "    else:",
            "        signal.signal(signal.SIGINT, sigint_handler)",
            "        signal.signal(signal.SIGTERM, sigint_handler)",
            "        signal.signal(signal.SIGQUIT, sigquit_handler)",
            "        _run_scheduler_job(args=args)",
            "",
            "",
            "def _serve_logs(skip_serve_logs: bool = False) -> Optional[Process]:",
            "    \"\"\"Starts serve_logs sub-process\"\"\"",
            "    from airflow.configuration import conf",
            "    from airflow.utils.serve_logs import serve_logs",
            "",
            "    if conf.get(\"core\", \"executor\") in [\"LocalExecutor\", \"SequentialExecutor\"]:",
            "        if skip_serve_logs is False:",
            "            sub_proc = Process(target=serve_logs)",
            "            sub_proc.start()",
            "            return sub_proc",
            "    return None"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "airflow.cli.cli_parser"
        ]
    },
    "airflow/cli/commands/triggerer_command.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": 45,
                "PatchRowcode": "                 files_preserve=[handle],"
            },
            "1": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 46,
                "PatchRowcode": "                 stdout=stdout_handle,"
            },
            "2": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 47,
                "PatchRowcode": "                 stderr=stderr_handle,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+                umask=int(settings.DAEMON_UMASK, 8),"
            },
            "4": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 49,
                "PatchRowcode": "             )"
            },
            "5": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 50,
                "PatchRowcode": "             with ctx:"
            },
            "6": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 51,
                "PatchRowcode": "                 job.run()"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "\"\"\"Triggerer command\"\"\"",
            "import signal",
            "",
            "import daemon",
            "from daemon.pidfile import TimeoutPIDLockFile",
            "",
            "from airflow import settings",
            "from airflow.jobs.triggerer_job import TriggererJob",
            "from airflow.utils import cli as cli_utils",
            "from airflow.utils.cli import setup_locations, setup_logging, sigint_handler, sigquit_handler",
            "",
            "",
            "@cli_utils.action_cli",
            "def triggerer(args):",
            "    \"\"\"Starts Airflow Triggerer\"\"\"",
            "    settings.MASK_SECRETS_IN_LOGS = True",
            "    print(settings.HEADER)",
            "    job = TriggererJob(capacity=args.capacity)",
            "",
            "    if args.daemon:",
            "        pid, stdout, stderr, log_file = setup_locations(",
            "            \"triggerer\", args.pid, args.stdout, args.stderr, args.log_file",
            "        )",
            "        handle = setup_logging(log_file)",
            "        with open(stdout, 'w+') as stdout_handle, open(stderr, 'w+') as stderr_handle:",
            "            ctx = daemon.DaemonContext(",
            "                pidfile=TimeoutPIDLockFile(pid, -1),",
            "                files_preserve=[handle],",
            "                stdout=stdout_handle,",
            "                stderr=stderr_handle,",
            "            )",
            "            with ctx:",
            "                job.run()",
            "",
            "    else:",
            "        signal.signal(signal.SIGINT, sigint_handler)",
            "        signal.signal(signal.SIGTERM, sigint_handler)",
            "        signal.signal(signal.SIGQUIT, sigquit_handler)",
            "        job.run()"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "\"\"\"Triggerer command\"\"\"",
            "import signal",
            "",
            "import daemon",
            "from daemon.pidfile import TimeoutPIDLockFile",
            "",
            "from airflow import settings",
            "from airflow.jobs.triggerer_job import TriggererJob",
            "from airflow.utils import cli as cli_utils",
            "from airflow.utils.cli import setup_locations, setup_logging, sigint_handler, sigquit_handler",
            "",
            "",
            "@cli_utils.action_cli",
            "def triggerer(args):",
            "    \"\"\"Starts Airflow Triggerer\"\"\"",
            "    settings.MASK_SECRETS_IN_LOGS = True",
            "    print(settings.HEADER)",
            "    job = TriggererJob(capacity=args.capacity)",
            "",
            "    if args.daemon:",
            "        pid, stdout, stderr, log_file = setup_locations(",
            "            \"triggerer\", args.pid, args.stdout, args.stderr, args.log_file",
            "        )",
            "        handle = setup_logging(log_file)",
            "        with open(stdout, 'w+') as stdout_handle, open(stderr, 'w+') as stderr_handle:",
            "            ctx = daemon.DaemonContext(",
            "                pidfile=TimeoutPIDLockFile(pid, -1),",
            "                files_preserve=[handle],",
            "                stdout=stdout_handle,",
            "                stderr=stderr_handle,",
            "                umask=int(settings.DAEMON_UMASK, 8),",
            "            )",
            "            with ctx:",
            "                job.run()",
            "",
            "    else:",
            "        signal.signal(signal.SIGINT, sigint_handler)",
            "        signal.signal(signal.SIGTERM, sigint_handler)",
            "        signal.signal(signal.SIGQUIT, sigquit_handler)",
            "        job.run()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "airflow.cli.cli_parser"
        ]
    },
    "airflow/cli/commands/webserver_command.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 455,
                "afterPatchRowNumber": 455,
                "PatchRowcode": "                     files_preserve=[handle],"
            },
            "1": {
                "beforePatchRowNumber": 456,
                "afterPatchRowNumber": 456,
                "PatchRowcode": "                     stdout=stdout,"
            },
            "2": {
                "beforePatchRowNumber": 457,
                "afterPatchRowNumber": 457,
                "PatchRowcode": "                     stderr=stderr,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 458,
                "PatchRowcode": "+                    umask=int(settings.DAEMON_UMASK, 8),"
            },
            "4": {
                "beforePatchRowNumber": 458,
                "afterPatchRowNumber": 459,
                "PatchRowcode": "                 )"
            },
            "5": {
                "beforePatchRowNumber": 459,
                "afterPatchRowNumber": 460,
                "PatchRowcode": "                 with ctx:"
            },
            "6": {
                "beforePatchRowNumber": 460,
                "afterPatchRowNumber": 461,
                "PatchRowcode": "                     subprocess.Popen(run_args, close_fds=True)"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "\"\"\"Webserver command\"\"\"",
            "import hashlib",
            "import logging",
            "import os",
            "import signal",
            "import subprocess",
            "import sys",
            "import textwrap",
            "import time",
            "from contextlib import suppress",
            "from time import sleep",
            "from typing import Dict, List, NoReturn",
            "",
            "import daemon",
            "import psutil",
            "from daemon.pidfile import TimeoutPIDLockFile",
            "from lockfile.pidlockfile import read_pid_from_pidfile",
            "",
            "from airflow import settings",
            "from airflow.configuration import conf",
            "from airflow.exceptions import AirflowException, AirflowWebServerTimeout",
            "from airflow.utils import cli as cli_utils",
            "from airflow.utils.cli import setup_locations, setup_logging",
            "from airflow.utils.log.logging_mixin import LoggingMixin",
            "from airflow.utils.process_utils import check_if_pidfile_process_is_running",
            "from airflow.www.app import create_app",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "class GunicornMonitor(LoggingMixin):",
            "    \"\"\"",
            "    Runs forever, monitoring the child processes of @gunicorn_master_proc and",
            "    restarting workers occasionally or when files in the plug-in directory",
            "    has been modified.",
            "",
            "    Each iteration of the loop traverses one edge of this state transition",
            "    diagram, where each state (node) represents",
            "    [ num_ready_workers_running / num_workers_running ]. We expect most time to",
            "    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.",
            "    The horizontal transition at ? happens after the new worker parses all the",
            "    dags (so it could take a while!)",
            "       V \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
            "    [n / n] \u2500\u2500TTIN\u2500\u2500> [ [n, n+bs) / n + bs ]  \u2500\u2500\u2500\u2500?\u2500\u2500\u2500> [n + bs / n + bs] \u2500\u2500TTOU\u2500\u2518",
            "       ^                          ^\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "       \u2502",
            "       \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v",
            "       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500 [ [0, n) / n ] <\u2500\u2500\u2500 start",
            "    We change the number of workers by sending TTIN and TTOU to the gunicorn",
            "    master process, which increases and decreases the number of child workers",
            "    respectively. Gunicorn guarantees that on TTOU workers are terminated",
            "    gracefully and that the oldest worker is terminated.",
            "",
            "    :param gunicorn_master_pid: PID for the main Gunicorn process",
            "    :param num_workers_expected: Number of workers to run the Gunicorn web server",
            "    :param master_timeout: Number of seconds the webserver waits before killing gunicorn master that",
            "        doesn't respond",
            "    :param worker_refresh_interval: Number of seconds to wait before refreshing a batch of workers.",
            "    :param worker_refresh_batch_size: Number of workers to refresh at a time. When set to 0, worker",
            "        refresh is disabled. When nonzero, airflow periodically refreshes webserver workers by",
            "        bringing up new ones and killing old ones.",
            "    :param reload_on_plugin_change: If set to True, Airflow will track files in plugins_folder directory.",
            "        When it detects changes, then reload the gunicorn.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        gunicorn_master_pid: int,",
            "        num_workers_expected: int,",
            "        master_timeout: int,",
            "        worker_refresh_interval: int,",
            "        worker_refresh_batch_size: int,",
            "        reload_on_plugin_change: bool,",
            "    ):",
            "        super().__init__()",
            "        self.gunicorn_master_proc = psutil.Process(gunicorn_master_pid)",
            "        self.num_workers_expected = num_workers_expected",
            "        self.master_timeout = master_timeout",
            "        self.worker_refresh_interval = worker_refresh_interval",
            "        self.worker_refresh_batch_size = worker_refresh_batch_size",
            "        self.reload_on_plugin_change = reload_on_plugin_change",
            "",
            "        self._num_workers_running = 0",
            "        self._num_ready_workers_running = 0",
            "        self._last_refresh_time = time.monotonic() if worker_refresh_interval > 0 else None",
            "        self._last_plugin_state = self._generate_plugin_state() if reload_on_plugin_change else None",
            "        self._restart_on_next_plugin_check = False",
            "",
            "    def _generate_plugin_state(self) -> Dict[str, float]:",
            "        \"\"\"",
            "        Generate dict of filenames and last modification time of all files in settings.PLUGINS_FOLDER",
            "        directory.",
            "        \"\"\"",
            "        if not settings.PLUGINS_FOLDER:",
            "            return {}",
            "",
            "        all_filenames: List[str] = []",
            "        for (root, _, filenames) in os.walk(settings.PLUGINS_FOLDER):",
            "            all_filenames.extend(os.path.join(root, f) for f in filenames)",
            "        plugin_state = {f: self._get_file_hash(f) for f in sorted(all_filenames)}",
            "        return plugin_state",
            "",
            "    @staticmethod",
            "    def _get_file_hash(fname: str):",
            "        \"\"\"Calculate MD5 hash for file\"\"\"",
            "        hash_md5 = hashlib.md5()",
            "        with open(fname, \"rb\") as f:",
            "            for chunk in iter(lambda: f.read(4096), b\"\"):",
            "                hash_md5.update(chunk)",
            "        return hash_md5.hexdigest()",
            "",
            "    def _get_num_ready_workers_running(self) -> int:",
            "        \"\"\"Returns number of ready Gunicorn workers by looking for READY_PREFIX in process name\"\"\"",
            "        workers = psutil.Process(self.gunicorn_master_proc.pid).children()",
            "",
            "        def ready_prefix_on_cmdline(proc):",
            "            try:",
            "                cmdline = proc.cmdline()",
            "                if len(cmdline) > 0:",
            "                    return settings.GUNICORN_WORKER_READY_PREFIX in cmdline[0]",
            "            except psutil.NoSuchProcess:",
            "                pass",
            "            return False",
            "",
            "        ready_workers = [proc for proc in workers if ready_prefix_on_cmdline(proc)]",
            "        return len(ready_workers)",
            "",
            "    def _get_num_workers_running(self) -> int:",
            "        \"\"\"Returns number of running Gunicorn workers processes\"\"\"",
            "        workers = psutil.Process(self.gunicorn_master_proc.pid).children()",
            "        return len(workers)",
            "",
            "    def _wait_until_true(self, fn, timeout: int = 0) -> None:",
            "        \"\"\"Sleeps until fn is true\"\"\"",
            "        start_time = time.monotonic()",
            "        while not fn():",
            "            if 0 < timeout <= time.monotonic() - start_time:",
            "                raise AirflowWebServerTimeout(f\"No response from gunicorn master within {timeout} seconds\")",
            "            sleep(0.1)",
            "",
            "    def _spawn_new_workers(self, count: int) -> None:",
            "        \"\"\"",
            "        Send signal to kill the worker.",
            "",
            "        :param count: The number of workers to spawn",
            "        \"\"\"",
            "        excess = 0",
            "        for _ in range(count):",
            "            # TTIN: Increment the number of processes by one",
            "            self.gunicorn_master_proc.send_signal(signal.SIGTTIN)",
            "            excess += 1",
            "            self._wait_until_true(",
            "                lambda: self.num_workers_expected + excess == self._get_num_workers_running(),",
            "                timeout=self.master_timeout,",
            "            )",
            "",
            "    def _kill_old_workers(self, count: int) -> None:",
            "        \"\"\"",
            "        Send signal to kill the worker.",
            "",
            "        :param count: The number of workers to kill",
            "        \"\"\"",
            "        for _ in range(count):",
            "            count -= 1",
            "            # TTOU: Decrement the number of processes by one",
            "            self.gunicorn_master_proc.send_signal(signal.SIGTTOU)",
            "            self._wait_until_true(",
            "                lambda: self.num_workers_expected + count == self._get_num_workers_running(),",
            "                timeout=self.master_timeout,",
            "            )",
            "",
            "    def _reload_gunicorn(self) -> None:",
            "        \"\"\"",
            "        Send signal to reload the gunicorn configuration. When gunicorn receive signals, it reload the",
            "        configuration, start the new worker processes with a new configuration and gracefully",
            "        shutdown older workers.",
            "        \"\"\"",
            "        # HUP: Reload the configuration.",
            "        self.gunicorn_master_proc.send_signal(signal.SIGHUP)",
            "        sleep(1)",
            "        self._wait_until_true(",
            "            lambda: self.num_workers_expected == self._get_num_workers_running(), timeout=self.master_timeout",
            "        )",
            "",
            "    def start(self) -> NoReturn:",
            "        \"\"\"Starts monitoring the webserver.\"\"\"",
            "        try:",
            "            self._wait_until_true(",
            "                lambda: self.num_workers_expected == self._get_num_workers_running(),",
            "                timeout=self.master_timeout,",
            "            )",
            "            while True:",
            "                if not self.gunicorn_master_proc.is_running():",
            "                    sys.exit(1)",
            "                self._check_workers()",
            "                # Throttle loop",
            "                sleep(1)",
            "",
            "        except (AirflowWebServerTimeout, OSError) as err:",
            "            self.log.error(err)",
            "            self.log.error(\"Shutting down webserver\")",
            "            try:",
            "                self.gunicorn_master_proc.terminate()",
            "                self.gunicorn_master_proc.wait()",
            "            finally:",
            "                sys.exit(1)",
            "",
            "    def _check_workers(self) -> None:",
            "        num_workers_running = self._get_num_workers_running()",
            "        num_ready_workers_running = self._get_num_ready_workers_running()",
            "",
            "        # Whenever some workers are not ready, wait until all workers are ready",
            "        if num_ready_workers_running < num_workers_running:",
            "            self.log.debug(",
            "                '[%d / %d] Some workers are starting up, waiting...',",
            "                num_ready_workers_running,",
            "                num_workers_running,",
            "            )",
            "            sleep(1)",
            "            return",
            "",
            "        # If there are too many workers, then kill a worker gracefully by asking gunicorn to reduce",
            "        # number of workers",
            "        if num_workers_running > self.num_workers_expected:",
            "            excess = min(num_workers_running - self.num_workers_expected, self.worker_refresh_batch_size)",
            "            self.log.debug(",
            "                '[%d / %d] Killing %s workers', num_ready_workers_running, num_workers_running, excess",
            "            )",
            "            self._kill_old_workers(excess)",
            "            return",
            "",
            "        # If there are too few workers, start a new worker by asking gunicorn",
            "        # to increase number of workers",
            "        if num_workers_running < self.num_workers_expected:",
            "            self.log.error(",
            "                \"[%d / %d] Some workers seem to have died and gunicorn did not restart them as expected\",",
            "                num_ready_workers_running,",
            "                num_workers_running,",
            "            )",
            "            sleep(10)",
            "            num_workers_running = self._get_num_workers_running()",
            "            if num_workers_running < self.num_workers_expected:",
            "                new_worker_count = min(",
            "                    self.num_workers_expected - num_workers_running, self.worker_refresh_batch_size",
            "                )",
            "                # log at info since we are trying fix an error logged just above",
            "                self.log.info(",
            "                    '[%d / %d] Spawning %d workers',",
            "                    num_ready_workers_running,",
            "                    num_workers_running,",
            "                    new_worker_count,",
            "                )",
            "                self._spawn_new_workers(new_worker_count)",
            "            return",
            "",
            "        # Now the number of running and expected worker should be equal",
            "",
            "        # If workers should be restarted periodically.",
            "        if self.worker_refresh_interval > 0 and self._last_refresh_time:",
            "            # and we refreshed the workers a long time ago, refresh the workers",
            "            last_refresh_diff = time.monotonic() - self._last_refresh_time",
            "            if self.worker_refresh_interval < last_refresh_diff:",
            "                num_new_workers = self.worker_refresh_batch_size",
            "                self.log.debug(",
            "                    '[%d / %d] Starting doing a refresh. Starting %d workers.',",
            "                    num_ready_workers_running,",
            "                    num_workers_running,",
            "                    num_new_workers,",
            "                )",
            "                self._spawn_new_workers(num_new_workers)",
            "                self._last_refresh_time = time.monotonic()",
            "                return",
            "",
            "        # if we should check the directory with the plugin,",
            "        if self.reload_on_plugin_change:",
            "            # compare the previous and current contents of the directory",
            "            new_state = self._generate_plugin_state()",
            "            # If changed, wait until its content is fully saved.",
            "            if new_state != self._last_plugin_state:",
            "                self.log.debug(",
            "                    '[%d / %d] Plugins folder changed. The gunicorn will be restarted the next time the '",
            "                    'plugin directory is checked, if there is no change in it.',",
            "                    num_ready_workers_running,",
            "                    num_workers_running,",
            "                )",
            "                self._restart_on_next_plugin_check = True",
            "                self._last_plugin_state = new_state",
            "            elif self._restart_on_next_plugin_check:",
            "                self.log.debug(",
            "                    '[%d / %d] Starts reloading the gunicorn configuration.',",
            "                    num_ready_workers_running,",
            "                    num_workers_running,",
            "                )",
            "                self._restart_on_next_plugin_check = False",
            "                self._last_refresh_time = time.monotonic()",
            "                self._reload_gunicorn()",
            "",
            "",
            "@cli_utils.action_cli",
            "def webserver(args):",
            "    \"\"\"Starts Airflow Webserver\"\"\"",
            "    print(settings.HEADER)",
            "",
            "    # Check for old/insecure config, and fail safe (i.e. don't launch) if the config is wildly insecure.",
            "    if conf.get('webserver', 'secret_key') == 'temporary_key':",
            "        from rich import print as rich_print",
            "",
            "        rich_print(",
            "            \"[red][bold]ERROR:[/bold] The `secret_key` setting under the webserver config has an insecure \"",
            "            \"value - Airflow has failed safe and refuses to start. Please change this value to a new, \"",
            "            \"per-environment, randomly generated string, for example using this command `[cyan]openssl rand \"",
            "            \"-hex 30[/cyan]`\",",
            "            file=sys.stderr,",
            "        )",
            "        sys.exit(1)",
            "",
            "    access_logfile = args.access_logfile or conf.get('webserver', 'access_logfile')",
            "    error_logfile = args.error_logfile or conf.get('webserver', 'error_logfile')",
            "    access_logformat = args.access_logformat or conf.get('webserver', 'access_logformat')",
            "    num_workers = args.workers or conf.get('webserver', 'workers')",
            "    worker_timeout = args.worker_timeout or conf.get('webserver', 'web_server_worker_timeout')",
            "    ssl_cert = args.ssl_cert or conf.get('webserver', 'web_server_ssl_cert')",
            "    ssl_key = args.ssl_key or conf.get('webserver', 'web_server_ssl_key')",
            "    if not ssl_cert and ssl_key:",
            "        raise AirflowException('An SSL certificate must also be provided for use with ' + ssl_key)",
            "    if ssl_cert and not ssl_key:",
            "        raise AirflowException('An SSL key must also be provided for use with ' + ssl_cert)",
            "",
            "    if args.debug:",
            "        print(f\"Starting the web server on port {args.port} and host {args.hostname}.\")",
            "        app = create_app(testing=conf.getboolean('core', 'unit_test_mode'))",
            "        app.run(",
            "            debug=True,",
            "            use_reloader=not app.config['TESTING'],",
            "            port=args.port,",
            "            host=args.hostname,",
            "            ssl_context=(ssl_cert, ssl_key) if ssl_cert and ssl_key else None,",
            "        )",
            "    else:",
            "",
            "        pid_file, stdout, stderr, log_file = setup_locations(",
            "            \"webserver\", args.pid, args.stdout, args.stderr, args.log_file",
            "        )",
            "",
            "        # Check if webserver is already running if not, remove old pidfile",
            "        check_if_pidfile_process_is_running(pid_file=pid_file, process_name=\"webserver\")",
            "",
            "        print(",
            "            textwrap.dedent(",
            "                f'''\\",
            "                Running the Gunicorn Server with:",
            "                Workers: {num_workers} {args.workerclass}",
            "                Host: {args.hostname}:{args.port}",
            "                Timeout: {worker_timeout}",
            "                Logfiles: {access_logfile} {error_logfile}",
            "                Access Logformat: {access_logformat}",
            "                ================================================================='''",
            "            )",
            "        )",
            "",
            "        run_args = [",
            "            sys.executable,",
            "            '-m',",
            "            'gunicorn',",
            "            '--workers',",
            "            str(num_workers),",
            "            '--worker-class',",
            "            str(args.workerclass),",
            "            '--timeout',",
            "            str(worker_timeout),",
            "            '--bind',",
            "            args.hostname + ':' + str(args.port),",
            "            '--name',",
            "            'airflow-webserver',",
            "            '--pid',",
            "            pid_file,",
            "            '--config',",
            "            'python:airflow.www.gunicorn_config',",
            "        ]",
            "",
            "        if args.access_logfile:",
            "            run_args += ['--access-logfile', str(args.access_logfile)]",
            "",
            "        if args.error_logfile:",
            "            run_args += ['--error-logfile', str(args.error_logfile)]",
            "",
            "        if args.access_logformat and args.access_logformat.strip():",
            "            run_args += ['--access-logformat', str(args.access_logformat)]",
            "",
            "        if args.daemon:",
            "            run_args += ['--daemon']",
            "",
            "        if ssl_cert:",
            "            run_args += ['--certfile', ssl_cert, '--keyfile', ssl_key]",
            "",
            "        run_args += [\"airflow.www.app:cached_app()\"]",
            "",
            "        gunicorn_master_proc = None",
            "",
            "        def kill_proc(signum, _):",
            "            log.info(\"Received signal: %s. Closing gunicorn.\", signum)",
            "            gunicorn_master_proc.terminate()",
            "            with suppress(TimeoutError):",
            "                gunicorn_master_proc.wait(timeout=30)",
            "            if gunicorn_master_proc.poll() is not None:",
            "                gunicorn_master_proc.kill()",
            "            sys.exit(0)",
            "",
            "        def monitor_gunicorn(gunicorn_master_pid: int):",
            "            # Register signal handlers",
            "            signal.signal(signal.SIGINT, kill_proc)",
            "            signal.signal(signal.SIGTERM, kill_proc)",
            "",
            "            # These run forever until SIG{INT, TERM, KILL, ...} signal is sent",
            "            GunicornMonitor(",
            "                gunicorn_master_pid=gunicorn_master_pid,",
            "                num_workers_expected=num_workers,",
            "                master_timeout=conf.getint('webserver', 'web_server_master_timeout'),",
            "                worker_refresh_interval=conf.getint('webserver', 'worker_refresh_interval', fallback=30),",
            "                worker_refresh_batch_size=conf.getint('webserver', 'worker_refresh_batch_size', fallback=1),",
            "                reload_on_plugin_change=conf.getboolean(",
            "                    'webserver', 'reload_on_plugin_change', fallback=False",
            "                ),",
            "            ).start()",
            "",
            "        if args.daemon:",
            "            # This makes possible errors get reported before daemonization",
            "            os.environ['SKIP_DAGS_PARSING'] = 'True'",
            "            app = create_app(None)",
            "            os.environ.pop('SKIP_DAGS_PARSING')",
            "",
            "            handle = setup_logging(log_file)",
            "",
            "            base, ext = os.path.splitext(pid_file)",
            "            with open(stdout, 'w+') as stdout, open(stderr, 'w+') as stderr:",
            "                ctx = daemon.DaemonContext(",
            "                    pidfile=TimeoutPIDLockFile(f\"{base}-monitor{ext}\", -1),",
            "                    files_preserve=[handle],",
            "                    stdout=stdout,",
            "                    stderr=stderr,",
            "                )",
            "                with ctx:",
            "                    subprocess.Popen(run_args, close_fds=True)",
            "",
            "                    # Reading pid of gunicorn master as it will be different that",
            "                    # the one of process spawned above.",
            "                    while True:",
            "                        sleep(0.1)",
            "                        gunicorn_master_proc_pid = read_pid_from_pidfile(pid_file)",
            "                        if gunicorn_master_proc_pid:",
            "                            break",
            "",
            "                    # Run Gunicorn monitor",
            "                    gunicorn_master_proc = psutil.Process(gunicorn_master_proc_pid)",
            "                    monitor_gunicorn(gunicorn_master_proc.pid)",
            "",
            "        else:",
            "            with subprocess.Popen(run_args, close_fds=True) as gunicorn_master_proc:",
            "                monitor_gunicorn(gunicorn_master_proc.pid)"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "\"\"\"Webserver command\"\"\"",
            "import hashlib",
            "import logging",
            "import os",
            "import signal",
            "import subprocess",
            "import sys",
            "import textwrap",
            "import time",
            "from contextlib import suppress",
            "from time import sleep",
            "from typing import Dict, List, NoReturn",
            "",
            "import daemon",
            "import psutil",
            "from daemon.pidfile import TimeoutPIDLockFile",
            "from lockfile.pidlockfile import read_pid_from_pidfile",
            "",
            "from airflow import settings",
            "from airflow.configuration import conf",
            "from airflow.exceptions import AirflowException, AirflowWebServerTimeout",
            "from airflow.utils import cli as cli_utils",
            "from airflow.utils.cli import setup_locations, setup_logging",
            "from airflow.utils.log.logging_mixin import LoggingMixin",
            "from airflow.utils.process_utils import check_if_pidfile_process_is_running",
            "from airflow.www.app import create_app",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "class GunicornMonitor(LoggingMixin):",
            "    \"\"\"",
            "    Runs forever, monitoring the child processes of @gunicorn_master_proc and",
            "    restarting workers occasionally or when files in the plug-in directory",
            "    has been modified.",
            "",
            "    Each iteration of the loop traverses one edge of this state transition",
            "    diagram, where each state (node) represents",
            "    [ num_ready_workers_running / num_workers_running ]. We expect most time to",
            "    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.",
            "    The horizontal transition at ? happens after the new worker parses all the",
            "    dags (so it could take a while!)",
            "       V \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
            "    [n / n] \u2500\u2500TTIN\u2500\u2500> [ [n, n+bs) / n + bs ]  \u2500\u2500\u2500\u2500?\u2500\u2500\u2500> [n + bs / n + bs] \u2500\u2500TTOU\u2500\u2518",
            "       ^                          ^\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "       \u2502",
            "       \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v",
            "       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500 [ [0, n) / n ] <\u2500\u2500\u2500 start",
            "    We change the number of workers by sending TTIN and TTOU to the gunicorn",
            "    master process, which increases and decreases the number of child workers",
            "    respectively. Gunicorn guarantees that on TTOU workers are terminated",
            "    gracefully and that the oldest worker is terminated.",
            "",
            "    :param gunicorn_master_pid: PID for the main Gunicorn process",
            "    :param num_workers_expected: Number of workers to run the Gunicorn web server",
            "    :param master_timeout: Number of seconds the webserver waits before killing gunicorn master that",
            "        doesn't respond",
            "    :param worker_refresh_interval: Number of seconds to wait before refreshing a batch of workers.",
            "    :param worker_refresh_batch_size: Number of workers to refresh at a time. When set to 0, worker",
            "        refresh is disabled. When nonzero, airflow periodically refreshes webserver workers by",
            "        bringing up new ones and killing old ones.",
            "    :param reload_on_plugin_change: If set to True, Airflow will track files in plugins_folder directory.",
            "        When it detects changes, then reload the gunicorn.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        gunicorn_master_pid: int,",
            "        num_workers_expected: int,",
            "        master_timeout: int,",
            "        worker_refresh_interval: int,",
            "        worker_refresh_batch_size: int,",
            "        reload_on_plugin_change: bool,",
            "    ):",
            "        super().__init__()",
            "        self.gunicorn_master_proc = psutil.Process(gunicorn_master_pid)",
            "        self.num_workers_expected = num_workers_expected",
            "        self.master_timeout = master_timeout",
            "        self.worker_refresh_interval = worker_refresh_interval",
            "        self.worker_refresh_batch_size = worker_refresh_batch_size",
            "        self.reload_on_plugin_change = reload_on_plugin_change",
            "",
            "        self._num_workers_running = 0",
            "        self._num_ready_workers_running = 0",
            "        self._last_refresh_time = time.monotonic() if worker_refresh_interval > 0 else None",
            "        self._last_plugin_state = self._generate_plugin_state() if reload_on_plugin_change else None",
            "        self._restart_on_next_plugin_check = False",
            "",
            "    def _generate_plugin_state(self) -> Dict[str, float]:",
            "        \"\"\"",
            "        Generate dict of filenames and last modification time of all files in settings.PLUGINS_FOLDER",
            "        directory.",
            "        \"\"\"",
            "        if not settings.PLUGINS_FOLDER:",
            "            return {}",
            "",
            "        all_filenames: List[str] = []",
            "        for (root, _, filenames) in os.walk(settings.PLUGINS_FOLDER):",
            "            all_filenames.extend(os.path.join(root, f) for f in filenames)",
            "        plugin_state = {f: self._get_file_hash(f) for f in sorted(all_filenames)}",
            "        return plugin_state",
            "",
            "    @staticmethod",
            "    def _get_file_hash(fname: str):",
            "        \"\"\"Calculate MD5 hash for file\"\"\"",
            "        hash_md5 = hashlib.md5()",
            "        with open(fname, \"rb\") as f:",
            "            for chunk in iter(lambda: f.read(4096), b\"\"):",
            "                hash_md5.update(chunk)",
            "        return hash_md5.hexdigest()",
            "",
            "    def _get_num_ready_workers_running(self) -> int:",
            "        \"\"\"Returns number of ready Gunicorn workers by looking for READY_PREFIX in process name\"\"\"",
            "        workers = psutil.Process(self.gunicorn_master_proc.pid).children()",
            "",
            "        def ready_prefix_on_cmdline(proc):",
            "            try:",
            "                cmdline = proc.cmdline()",
            "                if len(cmdline) > 0:",
            "                    return settings.GUNICORN_WORKER_READY_PREFIX in cmdline[0]",
            "            except psutil.NoSuchProcess:",
            "                pass",
            "            return False",
            "",
            "        ready_workers = [proc for proc in workers if ready_prefix_on_cmdline(proc)]",
            "        return len(ready_workers)",
            "",
            "    def _get_num_workers_running(self) -> int:",
            "        \"\"\"Returns number of running Gunicorn workers processes\"\"\"",
            "        workers = psutil.Process(self.gunicorn_master_proc.pid).children()",
            "        return len(workers)",
            "",
            "    def _wait_until_true(self, fn, timeout: int = 0) -> None:",
            "        \"\"\"Sleeps until fn is true\"\"\"",
            "        start_time = time.monotonic()",
            "        while not fn():",
            "            if 0 < timeout <= time.monotonic() - start_time:",
            "                raise AirflowWebServerTimeout(f\"No response from gunicorn master within {timeout} seconds\")",
            "            sleep(0.1)",
            "",
            "    def _spawn_new_workers(self, count: int) -> None:",
            "        \"\"\"",
            "        Send signal to kill the worker.",
            "",
            "        :param count: The number of workers to spawn",
            "        \"\"\"",
            "        excess = 0",
            "        for _ in range(count):",
            "            # TTIN: Increment the number of processes by one",
            "            self.gunicorn_master_proc.send_signal(signal.SIGTTIN)",
            "            excess += 1",
            "            self._wait_until_true(",
            "                lambda: self.num_workers_expected + excess == self._get_num_workers_running(),",
            "                timeout=self.master_timeout,",
            "            )",
            "",
            "    def _kill_old_workers(self, count: int) -> None:",
            "        \"\"\"",
            "        Send signal to kill the worker.",
            "",
            "        :param count: The number of workers to kill",
            "        \"\"\"",
            "        for _ in range(count):",
            "            count -= 1",
            "            # TTOU: Decrement the number of processes by one",
            "            self.gunicorn_master_proc.send_signal(signal.SIGTTOU)",
            "            self._wait_until_true(",
            "                lambda: self.num_workers_expected + count == self._get_num_workers_running(),",
            "                timeout=self.master_timeout,",
            "            )",
            "",
            "    def _reload_gunicorn(self) -> None:",
            "        \"\"\"",
            "        Send signal to reload the gunicorn configuration. When gunicorn receive signals, it reload the",
            "        configuration, start the new worker processes with a new configuration and gracefully",
            "        shutdown older workers.",
            "        \"\"\"",
            "        # HUP: Reload the configuration.",
            "        self.gunicorn_master_proc.send_signal(signal.SIGHUP)",
            "        sleep(1)",
            "        self._wait_until_true(",
            "            lambda: self.num_workers_expected == self._get_num_workers_running(), timeout=self.master_timeout",
            "        )",
            "",
            "    def start(self) -> NoReturn:",
            "        \"\"\"Starts monitoring the webserver.\"\"\"",
            "        try:",
            "            self._wait_until_true(",
            "                lambda: self.num_workers_expected == self._get_num_workers_running(),",
            "                timeout=self.master_timeout,",
            "            )",
            "            while True:",
            "                if not self.gunicorn_master_proc.is_running():",
            "                    sys.exit(1)",
            "                self._check_workers()",
            "                # Throttle loop",
            "                sleep(1)",
            "",
            "        except (AirflowWebServerTimeout, OSError) as err:",
            "            self.log.error(err)",
            "            self.log.error(\"Shutting down webserver\")",
            "            try:",
            "                self.gunicorn_master_proc.terminate()",
            "                self.gunicorn_master_proc.wait()",
            "            finally:",
            "                sys.exit(1)",
            "",
            "    def _check_workers(self) -> None:",
            "        num_workers_running = self._get_num_workers_running()",
            "        num_ready_workers_running = self._get_num_ready_workers_running()",
            "",
            "        # Whenever some workers are not ready, wait until all workers are ready",
            "        if num_ready_workers_running < num_workers_running:",
            "            self.log.debug(",
            "                '[%d / %d] Some workers are starting up, waiting...',",
            "                num_ready_workers_running,",
            "                num_workers_running,",
            "            )",
            "            sleep(1)",
            "            return",
            "",
            "        # If there are too many workers, then kill a worker gracefully by asking gunicorn to reduce",
            "        # number of workers",
            "        if num_workers_running > self.num_workers_expected:",
            "            excess = min(num_workers_running - self.num_workers_expected, self.worker_refresh_batch_size)",
            "            self.log.debug(",
            "                '[%d / %d] Killing %s workers', num_ready_workers_running, num_workers_running, excess",
            "            )",
            "            self._kill_old_workers(excess)",
            "            return",
            "",
            "        # If there are too few workers, start a new worker by asking gunicorn",
            "        # to increase number of workers",
            "        if num_workers_running < self.num_workers_expected:",
            "            self.log.error(",
            "                \"[%d / %d] Some workers seem to have died and gunicorn did not restart them as expected\",",
            "                num_ready_workers_running,",
            "                num_workers_running,",
            "            )",
            "            sleep(10)",
            "            num_workers_running = self._get_num_workers_running()",
            "            if num_workers_running < self.num_workers_expected:",
            "                new_worker_count = min(",
            "                    self.num_workers_expected - num_workers_running, self.worker_refresh_batch_size",
            "                )",
            "                # log at info since we are trying fix an error logged just above",
            "                self.log.info(",
            "                    '[%d / %d] Spawning %d workers',",
            "                    num_ready_workers_running,",
            "                    num_workers_running,",
            "                    new_worker_count,",
            "                )",
            "                self._spawn_new_workers(new_worker_count)",
            "            return",
            "",
            "        # Now the number of running and expected worker should be equal",
            "",
            "        # If workers should be restarted periodically.",
            "        if self.worker_refresh_interval > 0 and self._last_refresh_time:",
            "            # and we refreshed the workers a long time ago, refresh the workers",
            "            last_refresh_diff = time.monotonic() - self._last_refresh_time",
            "            if self.worker_refresh_interval < last_refresh_diff:",
            "                num_new_workers = self.worker_refresh_batch_size",
            "                self.log.debug(",
            "                    '[%d / %d] Starting doing a refresh. Starting %d workers.',",
            "                    num_ready_workers_running,",
            "                    num_workers_running,",
            "                    num_new_workers,",
            "                )",
            "                self._spawn_new_workers(num_new_workers)",
            "                self._last_refresh_time = time.monotonic()",
            "                return",
            "",
            "        # if we should check the directory with the plugin,",
            "        if self.reload_on_plugin_change:",
            "            # compare the previous and current contents of the directory",
            "            new_state = self._generate_plugin_state()",
            "            # If changed, wait until its content is fully saved.",
            "            if new_state != self._last_plugin_state:",
            "                self.log.debug(",
            "                    '[%d / %d] Plugins folder changed. The gunicorn will be restarted the next time the '",
            "                    'plugin directory is checked, if there is no change in it.',",
            "                    num_ready_workers_running,",
            "                    num_workers_running,",
            "                )",
            "                self._restart_on_next_plugin_check = True",
            "                self._last_plugin_state = new_state",
            "            elif self._restart_on_next_plugin_check:",
            "                self.log.debug(",
            "                    '[%d / %d] Starts reloading the gunicorn configuration.',",
            "                    num_ready_workers_running,",
            "                    num_workers_running,",
            "                )",
            "                self._restart_on_next_plugin_check = False",
            "                self._last_refresh_time = time.monotonic()",
            "                self._reload_gunicorn()",
            "",
            "",
            "@cli_utils.action_cli",
            "def webserver(args):",
            "    \"\"\"Starts Airflow Webserver\"\"\"",
            "    print(settings.HEADER)",
            "",
            "    # Check for old/insecure config, and fail safe (i.e. don't launch) if the config is wildly insecure.",
            "    if conf.get('webserver', 'secret_key') == 'temporary_key':",
            "        from rich import print as rich_print",
            "",
            "        rich_print(",
            "            \"[red][bold]ERROR:[/bold] The `secret_key` setting under the webserver config has an insecure \"",
            "            \"value - Airflow has failed safe and refuses to start. Please change this value to a new, \"",
            "            \"per-environment, randomly generated string, for example using this command `[cyan]openssl rand \"",
            "            \"-hex 30[/cyan]`\",",
            "            file=sys.stderr,",
            "        )",
            "        sys.exit(1)",
            "",
            "    access_logfile = args.access_logfile or conf.get('webserver', 'access_logfile')",
            "    error_logfile = args.error_logfile or conf.get('webserver', 'error_logfile')",
            "    access_logformat = args.access_logformat or conf.get('webserver', 'access_logformat')",
            "    num_workers = args.workers or conf.get('webserver', 'workers')",
            "    worker_timeout = args.worker_timeout or conf.get('webserver', 'web_server_worker_timeout')",
            "    ssl_cert = args.ssl_cert or conf.get('webserver', 'web_server_ssl_cert')",
            "    ssl_key = args.ssl_key or conf.get('webserver', 'web_server_ssl_key')",
            "    if not ssl_cert and ssl_key:",
            "        raise AirflowException('An SSL certificate must also be provided for use with ' + ssl_key)",
            "    if ssl_cert and not ssl_key:",
            "        raise AirflowException('An SSL key must also be provided for use with ' + ssl_cert)",
            "",
            "    if args.debug:",
            "        print(f\"Starting the web server on port {args.port} and host {args.hostname}.\")",
            "        app = create_app(testing=conf.getboolean('core', 'unit_test_mode'))",
            "        app.run(",
            "            debug=True,",
            "            use_reloader=not app.config['TESTING'],",
            "            port=args.port,",
            "            host=args.hostname,",
            "            ssl_context=(ssl_cert, ssl_key) if ssl_cert and ssl_key else None,",
            "        )",
            "    else:",
            "",
            "        pid_file, stdout, stderr, log_file = setup_locations(",
            "            \"webserver\", args.pid, args.stdout, args.stderr, args.log_file",
            "        )",
            "",
            "        # Check if webserver is already running if not, remove old pidfile",
            "        check_if_pidfile_process_is_running(pid_file=pid_file, process_name=\"webserver\")",
            "",
            "        print(",
            "            textwrap.dedent(",
            "                f'''\\",
            "                Running the Gunicorn Server with:",
            "                Workers: {num_workers} {args.workerclass}",
            "                Host: {args.hostname}:{args.port}",
            "                Timeout: {worker_timeout}",
            "                Logfiles: {access_logfile} {error_logfile}",
            "                Access Logformat: {access_logformat}",
            "                ================================================================='''",
            "            )",
            "        )",
            "",
            "        run_args = [",
            "            sys.executable,",
            "            '-m',",
            "            'gunicorn',",
            "            '--workers',",
            "            str(num_workers),",
            "            '--worker-class',",
            "            str(args.workerclass),",
            "            '--timeout',",
            "            str(worker_timeout),",
            "            '--bind',",
            "            args.hostname + ':' + str(args.port),",
            "            '--name',",
            "            'airflow-webserver',",
            "            '--pid',",
            "            pid_file,",
            "            '--config',",
            "            'python:airflow.www.gunicorn_config',",
            "        ]",
            "",
            "        if args.access_logfile:",
            "            run_args += ['--access-logfile', str(args.access_logfile)]",
            "",
            "        if args.error_logfile:",
            "            run_args += ['--error-logfile', str(args.error_logfile)]",
            "",
            "        if args.access_logformat and args.access_logformat.strip():",
            "            run_args += ['--access-logformat', str(args.access_logformat)]",
            "",
            "        if args.daemon:",
            "            run_args += ['--daemon']",
            "",
            "        if ssl_cert:",
            "            run_args += ['--certfile', ssl_cert, '--keyfile', ssl_key]",
            "",
            "        run_args += [\"airflow.www.app:cached_app()\"]",
            "",
            "        gunicorn_master_proc = None",
            "",
            "        def kill_proc(signum, _):",
            "            log.info(\"Received signal: %s. Closing gunicorn.\", signum)",
            "            gunicorn_master_proc.terminate()",
            "            with suppress(TimeoutError):",
            "                gunicorn_master_proc.wait(timeout=30)",
            "            if gunicorn_master_proc.poll() is not None:",
            "                gunicorn_master_proc.kill()",
            "            sys.exit(0)",
            "",
            "        def monitor_gunicorn(gunicorn_master_pid: int):",
            "            # Register signal handlers",
            "            signal.signal(signal.SIGINT, kill_proc)",
            "            signal.signal(signal.SIGTERM, kill_proc)",
            "",
            "            # These run forever until SIG{INT, TERM, KILL, ...} signal is sent",
            "            GunicornMonitor(",
            "                gunicorn_master_pid=gunicorn_master_pid,",
            "                num_workers_expected=num_workers,",
            "                master_timeout=conf.getint('webserver', 'web_server_master_timeout'),",
            "                worker_refresh_interval=conf.getint('webserver', 'worker_refresh_interval', fallback=30),",
            "                worker_refresh_batch_size=conf.getint('webserver', 'worker_refresh_batch_size', fallback=1),",
            "                reload_on_plugin_change=conf.getboolean(",
            "                    'webserver', 'reload_on_plugin_change', fallback=False",
            "                ),",
            "            ).start()",
            "",
            "        if args.daemon:",
            "            # This makes possible errors get reported before daemonization",
            "            os.environ['SKIP_DAGS_PARSING'] = 'True'",
            "            app = create_app(None)",
            "            os.environ.pop('SKIP_DAGS_PARSING')",
            "",
            "            handle = setup_logging(log_file)",
            "",
            "            base, ext = os.path.splitext(pid_file)",
            "            with open(stdout, 'w+') as stdout, open(stderr, 'w+') as stderr:",
            "                ctx = daemon.DaemonContext(",
            "                    pidfile=TimeoutPIDLockFile(f\"{base}-monitor{ext}\", -1),",
            "                    files_preserve=[handle],",
            "                    stdout=stdout,",
            "                    stderr=stderr,",
            "                    umask=int(settings.DAEMON_UMASK, 8),",
            "                )",
            "                with ctx:",
            "                    subprocess.Popen(run_args, close_fds=True)",
            "",
            "                    # Reading pid of gunicorn master as it will be different that",
            "                    # the one of process spawned above.",
            "                    while True:",
            "                        sleep(0.1)",
            "                        gunicorn_master_proc_pid = read_pid_from_pidfile(pid_file)",
            "                        if gunicorn_master_proc_pid:",
            "                            break",
            "",
            "                    # Run Gunicorn monitor",
            "                    gunicorn_master_proc = psutil.Process(gunicorn_master_proc_pid)",
            "                    monitor_gunicorn(gunicorn_master_proc.pid)",
            "",
            "        else:",
            "            with subprocess.Popen(run_args, close_fds=True) as gunicorn_master_proc:",
            "                monitor_gunicorn(gunicorn_master_proc.pid)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "airflow.cli.commands.webserver_command.webserver.airflow.cli.commands.webserver_command.webserver",
            "airflow.cli.commands.webserver_command.webserver.rich_print",
            "airflow.cli.cli_parser",
            "airflow.cli.commands.webserver_command.webserver.run_args"
        ]
    },
    "airflow/configuration.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 517,
                "afterPatchRowNumber": 517,
                "PatchRowcode": "             raise ValueError(f\"The value {section}/{key} should be set!\")"
            },
            "1": {
                "beforePatchRowNumber": 518,
                "afterPatchRowNumber": 518,
                "PatchRowcode": "         return value"
            },
            "2": {
                "beforePatchRowNumber": 519,
                "afterPatchRowNumber": 519,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 520,
                "PatchRowcode": "+    @overload  # type: ignore[override]"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 521,
                "PatchRowcode": "+    def get(self, section: str, key: str, fallback: str = ..., **kwargs) -> str:  # type: ignore[override]"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 522,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 523,
                "PatchRowcode": "+        ..."
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 524,
                "PatchRowcode": "+"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 525,
                "PatchRowcode": "+    @overload  # type: ignore[override]"
            },
            "9": {
                "beforePatchRowNumber": 520,
                "afterPatchRowNumber": 526,
                "PatchRowcode": "     def get(self, section: str, key: str, **kwargs) -> Optional[str]:  # type: ignore[override]"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 527,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 528,
                "PatchRowcode": "+        ..."
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 529,
                "PatchRowcode": "+"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 530,
                "PatchRowcode": "+    def get(self, section: str, key: str, **kwargs) -> Optional[str]:  # type: ignore[override, misc]"
            },
            "14": {
                "beforePatchRowNumber": 521,
                "afterPatchRowNumber": 531,
                "PatchRowcode": "         section = str(section).lower()"
            },
            "15": {
                "beforePatchRowNumber": 522,
                "afterPatchRowNumber": 532,
                "PatchRowcode": "         key = str(key).lower()"
            },
            "16": {
                "beforePatchRowNumber": 523,
                "afterPatchRowNumber": 533,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import datetime",
            "import functools",
            "import json",
            "import logging",
            "import multiprocessing",
            "import os",
            "import pathlib",
            "import re",
            "import shlex",
            "import subprocess",
            "import sys",
            "import warnings",
            "from base64 import b64encode",
            "from collections import OrderedDict",
            "",
            "# Ignored Mypy on configparser because it thinks the configparser module has no _UNSET attribute",
            "from configparser import _UNSET, ConfigParser, NoOptionError, NoSectionError  # type: ignore",
            "from contextlib import suppress",
            "from json.decoder import JSONDecodeError",
            "from re import Pattern",
            "from typing import IO, Any, Dict, Iterable, List, Optional, Set, Tuple, Union",
            "from urllib.parse import urlparse",
            "",
            "from typing_extensions import overload",
            "",
            "from airflow.exceptions import AirflowConfigException",
            "from airflow.secrets import DEFAULT_SECRETS_SEARCH_PATH, BaseSecretsBackend",
            "from airflow.utils import yaml",
            "from airflow.utils.module_loading import import_string",
            "from airflow.utils.weight_rule import WeightRule",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "# show Airflow's deprecation warnings",
            "if not sys.warnoptions:",
            "    warnings.filterwarnings(action='default', category=DeprecationWarning, module='airflow')",
            "    warnings.filterwarnings(action='default', category=PendingDeprecationWarning, module='airflow')",
            "",
            "_SQLITE3_VERSION_PATTERN = re.compile(r\"(?P<version>^\\d+(?:\\.\\d+)*)\\D?.*$\")",
            "",
            "ConfigType = Union[str, int, float, bool]",
            "ConfigOptionsDictType = Dict[str, ConfigType]",
            "ConfigSectionSourcesType = Dict[str, Union[str, Tuple[str, str]]]",
            "ConfigSourcesType = Dict[str, ConfigSectionSourcesType]",
            "",
            "ENV_VAR_PREFIX = 'AIRFLOW__'",
            "",
            "",
            "def _parse_sqlite_version(s: str) -> Tuple[int, ...]:",
            "    match = _SQLITE3_VERSION_PATTERN.match(s)",
            "    if match is None:",
            "        return ()",
            "    return tuple(int(p) for p in match.group(\"version\").split(\".\"))",
            "",
            "",
            "@overload",
            "def expand_env_var(env_var: None) -> None:",
            "    ...",
            "",
            "",
            "@overload",
            "def expand_env_var(env_var: str) -> str:",
            "    ...",
            "",
            "",
            "def expand_env_var(env_var: Union[str, None]) -> Optional[Union[str, None]]:",
            "    \"\"\"",
            "    Expands (potentially nested) env vars by repeatedly applying",
            "    `expandvars` and `expanduser` until interpolation stops having",
            "    any effect.",
            "    \"\"\"",
            "    if not env_var:",
            "        return env_var",
            "    while True:",
            "        interpolated = os.path.expanduser(os.path.expandvars(str(env_var)))",
            "        if interpolated == env_var:",
            "            return interpolated",
            "        else:",
            "            env_var = interpolated",
            "",
            "",
            "def run_command(command: str) -> str:",
            "    \"\"\"Runs command and returns stdout\"\"\"",
            "    process = subprocess.Popen(",
            "        shlex.split(command), stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True",
            "    )",
            "    output, stderr = (stream.decode(sys.getdefaultencoding(), 'ignore') for stream in process.communicate())",
            "",
            "    if process.returncode != 0:",
            "        raise AirflowConfigException(",
            "            f\"Cannot execute {command}. Error code is: {process.returncode}. \"",
            "            f\"Output: {output}, Stderr: {stderr}\"",
            "        )",
            "",
            "    return output",
            "",
            "",
            "def _get_config_value_from_secret_backend(config_key: str) -> Optional[str]:",
            "    \"\"\"Get Config option values from Secret Backend\"\"\"",
            "    try:",
            "        secrets_client = get_custom_secret_backend()",
            "        if not secrets_client:",
            "            return None",
            "        return secrets_client.get_config(config_key)",
            "    except Exception as e:",
            "        raise AirflowConfigException(",
            "            'Cannot retrieve config from alternative secrets backend. '",
            "            'Make sure it is configured properly and that the Backend '",
            "            'is accessible.\\n'",
            "            f'{e}'",
            "        )",
            "",
            "",
            "def _default_config_file_path(file_name: str) -> str:",
            "    templates_dir = os.path.join(os.path.dirname(__file__), 'config_templates')",
            "    return os.path.join(templates_dir, file_name)",
            "",
            "",
            "def default_config_yaml() -> List[Dict[str, Any]]:",
            "    \"\"\"",
            "    Read Airflow configs from YAML file",
            "",
            "    :return: Python dictionary containing configs & their info",
            "    \"\"\"",
            "    with open(_default_config_file_path('config.yml')) as config_file:",
            "        return yaml.safe_load(config_file)",
            "",
            "",
            "class AirflowConfigParser(ConfigParser):",
            "    \"\"\"Custom Airflow Configparser supporting defaults and deprecated options\"\"\"",
            "",
            "    # These configuration elements can be fetched as the stdout of commands",
            "    # following the \"{section}__{name}_cmd\" pattern, the idea behind this",
            "    # is to not store password on boxes in text files.",
            "    # These configs can also be fetched from Secrets backend",
            "    # following the \"{section}__{name}__secret\" pattern",
            "    sensitive_config_values: Set[Tuple[str, str]] = {",
            "        ('database', 'sql_alchemy_conn'),",
            "        ('core', 'fernet_key'),",
            "        ('celery', 'broker_url'),",
            "        ('celery', 'flower_basic_auth'),",
            "        ('celery', 'result_backend'),",
            "        ('atlas', 'password'),",
            "        ('smtp', 'smtp_password'),",
            "        ('webserver', 'secret_key'),",
            "        # The following options are deprecated",
            "        ('core', 'sql_alchemy_conn'),",
            "    }",
            "",
            "    # A mapping of (new section, new option) -> (old section, old option, since_version).",
            "    # When reading new option, the old option will be checked to see if it exists. If it does a",
            "    # DeprecationWarning will be issued and the old option will be used instead",
            "    deprecated_options: Dict[Tuple[str, str], Tuple[str, str, str]] = {",
            "        ('celery', 'worker_precheck'): ('core', 'worker_precheck', '2.0.0'),",
            "        ('logging', 'base_log_folder'): ('core', 'base_log_folder', '2.0.0'),",
            "        ('logging', 'remote_logging'): ('core', 'remote_logging', '2.0.0'),",
            "        ('logging', 'remote_log_conn_id'): ('core', 'remote_log_conn_id', '2.0.0'),",
            "        ('logging', 'remote_base_log_folder'): ('core', 'remote_base_log_folder', '2.0.0'),",
            "        ('logging', 'encrypt_s3_logs'): ('core', 'encrypt_s3_logs', '2.0.0'),",
            "        ('logging', 'logging_level'): ('core', 'logging_level', '2.0.0'),",
            "        ('logging', 'fab_logging_level'): ('core', 'fab_logging_level', '2.0.0'),",
            "        ('logging', 'logging_config_class'): ('core', 'logging_config_class', '2.0.0'),",
            "        ('logging', 'colored_console_log'): ('core', 'colored_console_log', '2.0.0'),",
            "        ('logging', 'colored_log_format'): ('core', 'colored_log_format', '2.0.0'),",
            "        ('logging', 'colored_formatter_class'): ('core', 'colored_formatter_class', '2.0.0'),",
            "        ('logging', 'log_format'): ('core', 'log_format', '2.0.0'),",
            "        ('logging', 'simple_log_format'): ('core', 'simple_log_format', '2.0.0'),",
            "        ('logging', 'task_log_prefix_template'): ('core', 'task_log_prefix_template', '2.0.0'),",
            "        ('logging', 'log_filename_template'): ('core', 'log_filename_template', '2.0.0'),",
            "        ('logging', 'log_processor_filename_template'): ('core', 'log_processor_filename_template', '2.0.0'),",
            "        ('logging', 'dag_processor_manager_log_location'): (",
            "            'core',",
            "            'dag_processor_manager_log_location',",
            "            '2.0.0',",
            "        ),",
            "        ('logging', 'task_log_reader'): ('core', 'task_log_reader', '2.0.0'),",
            "        ('metrics', 'statsd_on'): ('scheduler', 'statsd_on', '2.0.0'),",
            "        ('metrics', 'statsd_host'): ('scheduler', 'statsd_host', '2.0.0'),",
            "        ('metrics', 'statsd_port'): ('scheduler', 'statsd_port', '2.0.0'),",
            "        ('metrics', 'statsd_prefix'): ('scheduler', 'statsd_prefix', '2.0.0'),",
            "        ('metrics', 'statsd_allow_list'): ('scheduler', 'statsd_allow_list', '2.0.0'),",
            "        ('metrics', 'stat_name_handler'): ('scheduler', 'stat_name_handler', '2.0.0'),",
            "        ('metrics', 'statsd_datadog_enabled'): ('scheduler', 'statsd_datadog_enabled', '2.0.0'),",
            "        ('metrics', 'statsd_datadog_tags'): ('scheduler', 'statsd_datadog_tags', '2.0.0'),",
            "        ('metrics', 'statsd_custom_client_path'): ('scheduler', 'statsd_custom_client_path', '2.0.0'),",
            "        ('scheduler', 'parsing_processes'): ('scheduler', 'max_threads', '1.10.14'),",
            "        ('scheduler', 'scheduler_idle_sleep_time'): ('scheduler', 'processor_poll_interval', '2.2.0'),",
            "        ('operators', 'default_queue'): ('celery', 'default_queue', '2.1.0'),",
            "        ('core', 'hide_sensitive_var_conn_fields'): ('admin', 'hide_sensitive_variable_fields', '2.1.0'),",
            "        ('core', 'sensitive_var_conn_names'): ('admin', 'sensitive_variable_fields', '2.1.0'),",
            "        ('core', 'default_pool_task_slot_count'): ('core', 'non_pooled_task_slot_count', '1.10.4'),",
            "        ('core', 'max_active_tasks_per_dag'): ('core', 'dag_concurrency', '2.2.0'),",
            "        ('logging', 'worker_log_server_port'): ('celery', 'worker_log_server_port', '2.2.0'),",
            "        ('api', 'access_control_allow_origins'): ('api', 'access_control_allow_origin', '2.2.0'),",
            "        ('api', 'auth_backends'): ('api', 'auth_backend', '2.3.0'),",
            "        ('database', 'sql_alchemy_conn'): ('core', 'sql_alchemy_conn', '2.3.0'),",
            "        ('database', 'sql_engine_encoding'): ('core', 'sql_engine_encoding', '2.3.0'),",
            "        ('database', 'sql_engine_collation_for_ids'): ('core', 'sql_engine_collation_for_ids', '2.3.0'),",
            "        ('database', 'sql_alchemy_pool_enabled'): ('core', 'sql_alchemy_pool_enabled', '2.3.0'),",
            "        ('database', 'sql_alchemy_pool_size'): ('core', 'sql_alchemy_pool_size', '2.3.0'),",
            "        ('database', 'sql_alchemy_max_overflow'): ('core', 'sql_alchemy_max_overflow', '2.3.0'),",
            "        ('database', 'sql_alchemy_pool_recycle'): ('core', 'sql_alchemy_pool_recycle', '2.3.0'),",
            "        ('database', 'sql_alchemy_pool_pre_ping'): ('core', 'sql_alchemy_pool_pre_ping', '2.3.0'),",
            "        ('database', 'sql_alchemy_schema'): ('core', 'sql_alchemy_schema', '2.3.0'),",
            "        ('database', 'sql_alchemy_connect_args'): ('core', 'sql_alchemy_connect_args', '2.3.0'),",
            "        ('database', 'load_default_connections'): ('core', 'load_default_connections', '2.3.0'),",
            "        ('database', 'max_db_retries'): ('core', 'max_db_retries', '2.3.0'),",
            "    }",
            "",
            "    # A mapping of old default values that we want to change and warn the user",
            "    # about. Mapping of section -> setting -> { old, replace, by_version }",
            "    deprecated_values: Dict[str, Dict[str, Tuple[Pattern, str, str]]] = {",
            "        'core': {",
            "            'hostname_callable': (re.compile(r':'), r'.', '2.1'),",
            "        },",
            "        'webserver': {",
            "            'navbar_color': (re.compile(r'\\A#007A87\\Z', re.IGNORECASE), '#fff', '2.1'),",
            "            'dag_default_view': (re.compile(r'^tree$'), 'grid', '3.0'),",
            "        },",
            "        'email': {",
            "            'email_backend': (",
            "                re.compile(r'^airflow\\.contrib\\.utils\\.sendgrid\\.send_email$'),",
            "                r'airflow.providers.sendgrid.utils.emailer.send_email',",
            "                '2.1',",
            "            ),",
            "        },",
            "        'logging': {",
            "            'log_filename_template': (",
            "                re.compile(re.escape(\"{{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log\")),",
            "                \"XX-set-after-default-config-loaded-XX\",",
            "                '3.0',",
            "            ),",
            "        },",
            "        'api': {",
            "            'auth_backends': (",
            "                re.compile(r'^airflow\\.api\\.auth\\.backend\\.deny_all$|^$'),",
            "                'airflow.api.auth.backend.session',",
            "                '3.0',",
            "            ),",
            "        },",
            "        'elasticsearch': {",
            "            'log_id_template': (",
            "                re.compile('^' + re.escape('{dag_id}-{task_id}-{execution_date}-{try_number}') + '$'),",
            "                '{dag_id}-{task_id}-{run_id}-{map_index}-{try_number}',",
            "                '3.0',",
            "            )",
            "        },",
            "    }",
            "",
            "    _available_logging_levels = ['CRITICAL', 'FATAL', 'ERROR', 'WARN', 'WARNING', 'INFO', 'DEBUG']",
            "    enums_options = {",
            "        (\"core\", \"default_task_weight_rule\"): sorted(WeightRule.all_weight_rules()),",
            "        (\"core\", \"dag_ignore_file_syntax\"): [\"regexp\", \"glob\"],",
            "        ('core', 'mp_start_method'): multiprocessing.get_all_start_methods(),",
            "        (\"scheduler\", \"file_parsing_sort_mode\"): [\"modified_time\", \"random_seeded_by_host\", \"alphabetical\"],",
            "        (\"logging\", \"logging_level\"): _available_logging_levels,",
            "        (\"logging\", \"fab_logging_level\"): _available_logging_levels,",
            "        # celery_logging_level can be empty, which uses logging_level as fallback",
            "        (\"logging\", \"celery_logging_level\"): _available_logging_levels + [''],",
            "    }",
            "",
            "    upgraded_values: Dict[Tuple[str, str], str]",
            "    \"\"\"Mapping of (section,option) to the old value that was upgraded\"\"\"",
            "",
            "    # This method transforms option names on every read, get, or set operation.",
            "    # This changes from the default behaviour of ConfigParser from lower-casing",
            "    # to instead be case-preserving",
            "    def optionxform(self, optionstr: str) -> str:",
            "        return optionstr",
            "",
            "    def __init__(self, default_config: Optional[str] = None, *args, **kwargs):",
            "        super().__init__(*args, **kwargs)",
            "        self.upgraded_values = {}",
            "",
            "        self.airflow_defaults = ConfigParser(*args, **kwargs)",
            "        if default_config is not None:",
            "            self.airflow_defaults.read_string(default_config)",
            "            # Set the upgrade value based on the current loaded default",
            "            default = self.airflow_defaults.get('logging', 'log_filename_template', fallback=None)",
            "            if default:",
            "                replacement = self.deprecated_values['logging']['log_filename_template']",
            "                self.deprecated_values['logging']['log_filename_template'] = (",
            "                    replacement[0],",
            "                    default,",
            "                    replacement[2],",
            "                )",
            "            else:",
            "                # In case of tests it might not exist",
            "                with suppress(KeyError):",
            "                    del self.deprecated_values['logging']['log_filename_template']",
            "        else:",
            "            with suppress(KeyError):",
            "                del self.deprecated_values['logging']['log_filename_template']",
            "",
            "        self.is_validated = False",
            "",
            "    def validate(self):",
            "        self._validate_config_dependencies()",
            "        self._validate_enums()",
            "",
            "        for section, replacement in self.deprecated_values.items():",
            "            for name, info in replacement.items():",
            "                old, new, version = info",
            "                current_value = self.get(section, name, fallback=\"\")",
            "                if self._using_old_value(old, current_value):",
            "                    self.upgraded_values[(section, name)] = current_value",
            "                    new_value = old.sub(new, current_value)",
            "                    self._update_env_var(section=section, name=name, new_value=new_value)",
            "                    self._create_future_warning(",
            "                        name=name,",
            "                        section=section,",
            "                        current_value=current_value,",
            "                        new_value=new_value,",
            "                        version=version,",
            "                    )",
            "",
            "        self._upgrade_auth_backends()",
            "        self._upgrade_postgres_metastore_conn()",
            "        self.is_validated = True",
            "",
            "    def _upgrade_auth_backends(self):",
            "        \"\"\"",
            "        Ensure a custom auth_backends setting contains session,",
            "        which is needed by the UI for ajax queries.",
            "        \"\"\"",
            "        old_value = self.get(\"api\", \"auth_backends\", fallback=\"\")",
            "        if old_value in ('airflow.api.auth.backend.default', ''):",
            "            # handled by deprecated_values",
            "            pass",
            "        elif old_value.find('airflow.api.auth.backend.session') == -1:",
            "            new_value = old_value + \",airflow.api.auth.backend.session\"",
            "            self._update_env_var(section=\"api\", name=\"auth_backends\", new_value=new_value)",
            "            self.upgraded_values[(\"api\", \"auth_backends\")] = old_value",
            "",
            "            # if the old value is set via env var, we need to wipe it",
            "            # otherwise, it'll \"win\" over our adjusted value",
            "            old_env_var = self._env_var_name(\"api\", \"auth_backend\")",
            "            os.environ.pop(old_env_var, None)",
            "",
            "            warnings.warn(",
            "                'The auth_backends setting in [api] has had airflow.api.auth.backend.session added '",
            "                'in the running config, which is needed by the UI. Please update your config before '",
            "                'Apache Airflow 3.0.',",
            "                FutureWarning,",
            "            )",
            "",
            "    def _upgrade_postgres_metastore_conn(self):",
            "        \"\"\"",
            "        As of SQLAlchemy 1.4, schemes `postgres+psycopg2` and `postgres`",
            "        must be replaced with `postgresql`.",
            "        \"\"\"",
            "        section, key = 'database', 'sql_alchemy_conn'",
            "        old_value = self.get(section, key)",
            "        bad_schemes = ['postgres+psycopg2', 'postgres']",
            "        good_scheme = 'postgresql'",
            "        parsed = urlparse(old_value)",
            "        if parsed.scheme in bad_schemes:",
            "            warnings.warn(",
            "                f\"Bad scheme in Airflow configuration core > sql_alchemy_conn: `{parsed.scheme}`. \"",
            "                \"As of SQLAlchemy 1.4 (adopted in Airflow 2.3) this is no longer supported.  You must \"",
            "                f\"change to `{good_scheme}` before the next Airflow release.\",",
            "                FutureWarning,",
            "            )",
            "            self.upgraded_values[(section, key)] = old_value",
            "            new_value = re.sub('^' + re.escape(f\"{parsed.scheme}://\"), f\"{good_scheme}://\", old_value)",
            "            self._update_env_var(section=section, name=key, new_value=new_value)",
            "",
            "            # if the old value is set via env var, we need to wipe it",
            "            # otherwise, it'll \"win\" over our adjusted value",
            "            old_env_var = self._env_var_name(\"core\", key)",
            "            os.environ.pop(old_env_var, None)",
            "",
            "    def _validate_enums(self):",
            "        \"\"\"Validate that enum type config has an accepted value\"\"\"",
            "        for (section_key, option_key), enum_options in self.enums_options.items():",
            "            if self.has_option(section_key, option_key):",
            "                value = self.get(section_key, option_key)",
            "                if value not in enum_options:",
            "                    raise AirflowConfigException(",
            "                        f\"`[{section_key}] {option_key}` should not be \"",
            "                        f\"{value!r}. Possible values: {', '.join(enum_options)}.\"",
            "                    )",
            "",
            "    def _validate_config_dependencies(self):",
            "        \"\"\"",
            "        Validate that config values aren't invalid given other config values",
            "        or system-level limitations and requirements.",
            "        \"\"\"",
            "        is_executor_without_sqlite_support = self.get(\"core\", \"executor\") not in (",
            "            'DebugExecutor',",
            "            'SequentialExecutor',",
            "        )",
            "        is_sqlite = \"sqlite\" in self.get('database', 'sql_alchemy_conn')",
            "        if is_sqlite and is_executor_without_sqlite_support:",
            "            raise AirflowConfigException(f\"error: cannot use sqlite with the {self.get('core', 'executor')}\")",
            "        if is_sqlite:",
            "            import sqlite3",
            "",
            "            from airflow.utils.docs import get_docs_url",
            "",
            "            # Some features in storing rendered fields require sqlite version >= 3.15.0",
            "            min_sqlite_version = (3, 15, 0)",
            "            if _parse_sqlite_version(sqlite3.sqlite_version) < min_sqlite_version:",
            "                min_sqlite_version_str = \".\".join(str(s) for s in min_sqlite_version)",
            "                raise AirflowConfigException(",
            "                    f\"error: sqlite C library version too old (< {min_sqlite_version_str}). \"",
            "                    f\"See {get_docs_url('howto/set-up-database.html#setting-up-a-sqlite-database')}\"",
            "                )",
            "",
            "    def _using_old_value(self, old: Pattern, current_value: str) -> bool:",
            "        return old.search(current_value) is not None",
            "",
            "    def _update_env_var(self, section: str, name: str, new_value: Union[str]):",
            "        env_var = self._env_var_name(section, name)",
            "        # Set it as an env var so that any subprocesses keep the same override!",
            "        os.environ[env_var] = new_value",
            "",
            "    @staticmethod",
            "    def _create_future_warning(name: str, section: str, current_value: Any, new_value: Any, version: str):",
            "        warnings.warn(",
            "            f'The {name!r} setting in [{section}] has the old default value of {current_value!r}. '",
            "            f'This value has been changed to {new_value!r} in the running config, but '",
            "            f'please update your config before Apache Airflow {version}.',",
            "            FutureWarning,",
            "        )",
            "",
            "    def _env_var_name(self, section: str, key: str) -> str:",
            "        return f'{ENV_VAR_PREFIX}{section.upper()}__{key.upper()}'",
            "",
            "    def _get_env_var_option(self, section: str, key: str):",
            "        # must have format AIRFLOW__{SECTION}__{KEY} (note double underscore)",
            "        env_var = self._env_var_name(section, key)",
            "        if env_var in os.environ:",
            "            return expand_env_var(os.environ[env_var])",
            "        # alternatively AIRFLOW__{SECTION}__{KEY}_CMD (for a command)",
            "        env_var_cmd = env_var + '_CMD'",
            "        if env_var_cmd in os.environ:",
            "            # if this is a valid command key...",
            "            if (section, key) in self.sensitive_config_values:",
            "                return run_command(os.environ[env_var_cmd])",
            "        # alternatively AIRFLOW__{SECTION}__{KEY}_SECRET (to get from Secrets Backend)",
            "        env_var_secret_path = env_var + '_SECRET'",
            "        if env_var_secret_path in os.environ:",
            "            # if this is a valid secret path...",
            "            if (section, key) in self.sensitive_config_values:",
            "                return _get_config_value_from_secret_backend(os.environ[env_var_secret_path])",
            "        return None",
            "",
            "    def _get_cmd_option(self, section: str, key: str):",
            "        fallback_key = key + '_cmd'",
            "        if (section, key) in self.sensitive_config_values:",
            "            if super().has_option(section, fallback_key):",
            "                command = super().get(section, fallback_key)",
            "                return run_command(command)",
            "        return None",
            "",
            "    def _get_cmd_option_from_config_sources(",
            "        self, config_sources: ConfigSourcesType, section: str, key: str",
            "    ) -> Optional[str]:",
            "        fallback_key = key + '_cmd'",
            "        if (section, key) in self.sensitive_config_values:",
            "            section_dict = config_sources.get(section)",
            "            if section_dict is not None:",
            "                command_value = section_dict.get(fallback_key)",
            "                if command_value is not None:",
            "                    if isinstance(command_value, str):",
            "                        command = command_value",
            "                    else:",
            "                        command = command_value[0]",
            "                    return run_command(command)",
            "        return None",
            "",
            "    def _get_secret_option(self, section: str, key: str) -> Optional[str]:",
            "        \"\"\"Get Config option values from Secret Backend\"\"\"",
            "        fallback_key = key + '_secret'",
            "        if (section, key) in self.sensitive_config_values:",
            "            if super().has_option(section, fallback_key):",
            "                secrets_path = super().get(section, fallback_key)",
            "                return _get_config_value_from_secret_backend(secrets_path)",
            "        return None",
            "",
            "    def _get_secret_option_from_config_sources(",
            "        self, config_sources: ConfigSourcesType, section: str, key: str",
            "    ) -> Optional[str]:",
            "        fallback_key = key + '_secret'",
            "        if (section, key) in self.sensitive_config_values:",
            "            section_dict = config_sources.get(section)",
            "            if section_dict is not None:",
            "                secrets_path_value = section_dict.get(fallback_key)",
            "                if secrets_path_value is not None:",
            "                    if isinstance(secrets_path_value, str):",
            "                        secrets_path = secrets_path_value",
            "                    else:",
            "                        secrets_path = secrets_path_value[0]",
            "                    return _get_config_value_from_secret_backend(secrets_path)",
            "        return None",
            "",
            "    def get_mandatory_value(self, section: str, key: str, **kwargs) -> str:",
            "        value = self.get(section, key, **kwargs)",
            "        if value is None:",
            "            raise ValueError(f\"The value {section}/{key} should be set!\")",
            "        return value",
            "",
            "    def get(self, section: str, key: str, **kwargs) -> Optional[str]:  # type: ignore[override]",
            "        section = str(section).lower()",
            "        key = str(key).lower()",
            "",
            "        deprecated_section, deprecated_key, _ = self.deprecated_options.get(",
            "            (section, key), (None, None, None)",
            "        )",
            "",
            "        option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)",
            "        if option is not None:",
            "            return option",
            "",
            "        option = self._get_option_from_config_file(deprecated_key, deprecated_section, key, kwargs, section)",
            "        if option is not None:",
            "            return option",
            "",
            "        option = self._get_option_from_commands(deprecated_key, deprecated_section, key, section)",
            "        if option is not None:",
            "            return option",
            "",
            "        option = self._get_option_from_secrets(deprecated_key, deprecated_section, key, section)",
            "        if option is not None:",
            "            return option",
            "",
            "        return self._get_option_from_default_config(section, key, **kwargs)",
            "",
            "    def _get_option_from_default_config(self, section: str, key: str, **kwargs) -> Optional[str]:",
            "        # ...then the default config",
            "        if self.airflow_defaults.has_option(section, key) or 'fallback' in kwargs:",
            "            return expand_env_var(self.airflow_defaults.get(section, key, **kwargs))",
            "",
            "        else:",
            "            log.warning(\"section/key [%s/%s] not found in config\", section, key)",
            "",
            "            raise AirflowConfigException(f\"section/key [{section}/{key}] not found in config\")",
            "",
            "    def _get_option_from_secrets(",
            "        self, deprecated_key: Optional[str], deprecated_section: Optional[str], key: str, section: str",
            "    ) -> Optional[str]:",
            "        # ...then from secret backends",
            "        option = self._get_secret_option(section, key)",
            "        if option:",
            "            return option",
            "        if deprecated_section and deprecated_key:",
            "            option = self._get_secret_option(deprecated_section, deprecated_key)",
            "            if option:",
            "                self._warn_deprecate(section, key, deprecated_section, deprecated_key)",
            "                return option",
            "        return None",
            "",
            "    def _get_option_from_commands(",
            "        self, deprecated_key: Optional[str], deprecated_section: Optional[str], key: str, section: str",
            "    ) -> Optional[str]:",
            "        # ...then commands",
            "        option = self._get_cmd_option(section, key)",
            "        if option:",
            "            return option",
            "        if deprecated_section and deprecated_key:",
            "            option = self._get_cmd_option(deprecated_section, deprecated_key)",
            "            if option:",
            "                self._warn_deprecate(section, key, deprecated_section, deprecated_key)",
            "                return option",
            "        return None",
            "",
            "    def _get_option_from_config_file(",
            "        self,",
            "        deprecated_key: Optional[str],",
            "        deprecated_section: Optional[str],",
            "        key: str,",
            "        kwargs: Dict[str, Any],",
            "        section: str,",
            "    ) -> Optional[str]:",
            "        # ...then the config file",
            "        if super().has_option(section, key):",
            "            # Use the parent's methods to get the actual config here to be able to",
            "            # separate the config from default config.",
            "            return expand_env_var(super().get(section, key, **kwargs))",
            "        if deprecated_section and deprecated_key:",
            "            if super().has_option(deprecated_section, deprecated_key):",
            "                self._warn_deprecate(section, key, deprecated_section, deprecated_key)",
            "                return expand_env_var(super().get(deprecated_section, deprecated_key, **kwargs))",
            "        return None",
            "",
            "    def _get_environment_variables(",
            "        self, deprecated_key: Optional[str], deprecated_section: Optional[str], key: str, section: str",
            "    ) -> Optional[str]:",
            "        # first check environment variables",
            "        option = self._get_env_var_option(section, key)",
            "        if option is not None:",
            "            return option",
            "        if deprecated_section and deprecated_key:",
            "            option = self._get_env_var_option(deprecated_section, deprecated_key)",
            "            if option is not None:",
            "                self._warn_deprecate(section, key, deprecated_section, deprecated_key)",
            "                return option",
            "        return None",
            "",
            "    def getboolean(self, section: str, key: str, **kwargs) -> bool:  # type: ignore[override]",
            "        val = str(self.get(section, key, **kwargs)).lower().strip()",
            "        if '#' in val:",
            "            val = val.split('#')[0].strip()",
            "        if val in ('t', 'true', '1'):",
            "            return True",
            "        elif val in ('f', 'false', '0'):",
            "            return False",
            "        else:",
            "            raise AirflowConfigException(",
            "                f'Failed to convert value to bool. Please check \"{key}\" key in \"{section}\" section. '",
            "                f'Current value: \"{val}\".'",
            "            )",
            "",
            "    def getint(self, section: str, key: str, **kwargs) -> int:  # type: ignore[override]",
            "        val = self.get(section, key, **kwargs)",
            "        if val is None:",
            "            raise AirflowConfigException(",
            "                f'Failed to convert value None to int. '",
            "                f'Please check \"{key}\" key in \"{section}\" section is set.'",
            "            )",
            "        try:",
            "            return int(val)",
            "        except ValueError:",
            "            raise AirflowConfigException(",
            "                f'Failed to convert value to int. Please check \"{key}\" key in \"{section}\" section. '",
            "                f'Current value: \"{val}\".'",
            "            )",
            "",
            "    def getfloat(self, section: str, key: str, **kwargs) -> float:  # type: ignore[override]",
            "        val = self.get(section, key, **kwargs)",
            "        if val is None:",
            "            raise AirflowConfigException(",
            "                f'Failed to convert value None to float. '",
            "                f'Please check \"{key}\" key in \"{section}\" section is set.'",
            "            )",
            "        try:",
            "            return float(val)",
            "        except ValueError:",
            "            raise AirflowConfigException(",
            "                f'Failed to convert value to float. Please check \"{key}\" key in \"{section}\" section. '",
            "                f'Current value: \"{val}\".'",
            "            )",
            "",
            "    def getimport(self, section: str, key: str, **kwargs) -> Any:",
            "        \"\"\"",
            "        Reads options, imports the full qualified name, and returns the object.",
            "",
            "        In case of failure, it throws an exception with the key and section names",
            "",
            "        :return: The object or None, if the option is empty",
            "        \"\"\"",
            "        full_qualified_path = conf.get(section=section, key=key, **kwargs)",
            "        if not full_qualified_path:",
            "            return None",
            "",
            "        try:",
            "            return import_string(full_qualified_path)",
            "        except ImportError as e:",
            "            log.error(e)",
            "            raise AirflowConfigException(",
            "                f'The object could not be loaded. Please check \"{key}\" key in \"{section}\" section. '",
            "                f'Current value: \"{full_qualified_path}\".'",
            "            )",
            "",
            "    def getjson(",
            "        self, section: str, key: str, fallback=_UNSET, **kwargs",
            "    ) -> Union[dict, list, str, int, float, None]:",
            "        \"\"\"",
            "        Return a config value parsed from a JSON string.",
            "",
            "        ``fallback`` is *not* JSON parsed but used verbatim when no config value is given.",
            "        \"\"\"",
            "        # get always returns the fallback value as a string, so for this if",
            "        # someone gives us an object we want to keep that",
            "        default = _UNSET",
            "        if fallback is not _UNSET:",
            "            default = fallback",
            "            fallback = _UNSET",
            "",
            "        try:",
            "            data = self.get(section=section, key=key, fallback=fallback, **kwargs)",
            "        except (NoSectionError, NoOptionError):",
            "            return default",
            "",
            "        if not data:",
            "            return default if default is not _UNSET else None",
            "",
            "        try:",
            "            return json.loads(data)",
            "        except JSONDecodeError as e:",
            "            raise AirflowConfigException(f'Unable to parse [{section}] {key!r} as valid json') from e",
            "",
            "    def gettimedelta(",
            "        self, section: str, key: str, fallback: Any = None, **kwargs",
            "    ) -> Optional[datetime.timedelta]:",
            "        \"\"\"",
            "        Gets the config value for the given section and key, and converts it into datetime.timedelta object.",
            "        If the key is missing, then it is considered as `None`.",
            "",
            "        :param section: the section from the config",
            "        :param key: the key defined in the given section",
            "        :param fallback: fallback value when no config value is given, defaults to None",
            "        :raises AirflowConfigException: raised because ValueError or OverflowError",
            "        :return: datetime.timedelta(seconds=<config_value>) or None",
            "        \"\"\"",
            "        val = self.get(section, key, fallback=fallback, **kwargs)",
            "",
            "        if val:",
            "            # the given value must be convertible to integer",
            "            try:",
            "                int_val = int(val)",
            "            except ValueError:",
            "                raise AirflowConfigException(",
            "                    f'Failed to convert value to int. Please check \"{key}\" key in \"{section}\" section. '",
            "                    f'Current value: \"{val}\".'",
            "                )",
            "",
            "            try:",
            "                return datetime.timedelta(seconds=int_val)",
            "            except OverflowError as err:",
            "                raise AirflowConfigException(",
            "                    f'Failed to convert value to timedelta in `seconds`. '",
            "                    f'{err}. '",
            "                    f'Please check \"{key}\" key in \"{section}\" section. Current value: \"{val}\".'",
            "                )",
            "",
            "        return fallback",
            "",
            "    def read(",
            "        self,",
            "        filenames: Union[",
            "            str,",
            "            bytes,",
            "            os.PathLike,",
            "            Iterable[Union[str, bytes, os.PathLike]],",
            "        ],",
            "        encoding=None,",
            "    ):",
            "        super().read(filenames=filenames, encoding=encoding)",
            "",
            "    # The RawConfigParser defines \"Mapping\" from abc.collections is not subscriptable - so we have",
            "    # to use Dict here.",
            "    def read_dict(  # type: ignore[override]",
            "        self, dictionary: Dict[str, Dict[str, Any]], source: str = '<dict>'",
            "    ):",
            "        super().read_dict(dictionary=dictionary, source=source)",
            "",
            "    def has_option(self, section: str, option: str) -> bool:",
            "        try:",
            "            # Using self.get() to avoid reimplementing the priority order",
            "            # of config variables (env, config, cmd, defaults)",
            "            # UNSET to avoid logging a warning about missing values",
            "            self.get(section, option, fallback=_UNSET)",
            "            return True",
            "        except (NoOptionError, NoSectionError):",
            "            return False",
            "",
            "    def remove_option(self, section: str, option: str, remove_default: bool = True):",
            "        \"\"\"",
            "        Remove an option if it exists in config from a file or",
            "        default config. If both of config have the same option, this removes",
            "        the option in both configs unless remove_default=False.",
            "        \"\"\"",
            "        if super().has_option(section, option):",
            "            super().remove_option(section, option)",
            "",
            "        if self.airflow_defaults.has_option(section, option) and remove_default:",
            "            self.airflow_defaults.remove_option(section, option)",
            "",
            "    def getsection(self, section: str) -> Optional[ConfigOptionsDictType]:",
            "        \"\"\"",
            "        Returns the section as a dict. Values are converted to int, float, bool",
            "        as required.",
            "",
            "        :param section: section from the config",
            "        :rtype: dict",
            "        \"\"\"",
            "        if not self.has_section(section) and not self.airflow_defaults.has_section(section):",
            "            return None",
            "        if self.airflow_defaults.has_section(section):",
            "            _section: ConfigOptionsDictType = OrderedDict(self.airflow_defaults.items(section))",
            "        else:",
            "            _section = OrderedDict()",
            "",
            "        if self.has_section(section):",
            "            _section.update(OrderedDict(self.items(section)))",
            "",
            "        section_prefix = self._env_var_name(section, '')",
            "        for env_var in sorted(os.environ.keys()):",
            "            if env_var.startswith(section_prefix):",
            "                key = env_var.replace(section_prefix, '')",
            "                if key.endswith(\"_CMD\"):",
            "                    key = key[:-4]",
            "                key = key.lower()",
            "                _section[key] = self._get_env_var_option(section, key)",
            "",
            "        for key, val in _section.items():",
            "            if val is None:",
            "                raise AirflowConfigException(",
            "                    f'Failed to convert value automatically. '",
            "                    f'Please check \"{key}\" key in \"{section}\" section is set.'",
            "                )",
            "            try:",
            "                _section[key] = int(val)",
            "            except ValueError:",
            "                try:",
            "                    _section[key] = float(val)",
            "                except ValueError:",
            "                    if isinstance(val, str) and val.lower() in ('t', 'true'):",
            "                        _section[key] = True",
            "                    elif isinstance(val, str) and val.lower() in ('f', 'false'):",
            "                        _section[key] = False",
            "        return _section",
            "",
            "    def write(self, fp: IO, space_around_delimiters: bool = True):  # type: ignore[override]",
            "        # This is based on the configparser.RawConfigParser.write method code to add support for",
            "        # reading options from environment variables.",
            "        # Various type ignores below deal with less-than-perfect RawConfigParser superclass typing",
            "        if space_around_delimiters:",
            "            delimiter = f\" {self._delimiters[0]} \"  # type: ignore[attr-defined]",
            "        else:",
            "            delimiter = self._delimiters[0]  # type: ignore[attr-defined]",
            "        if self._defaults:  # type: ignore",
            "            self._write_section(  # type: ignore[attr-defined]",
            "                fp, self.default_section, self._defaults.items(), delimiter  # type: ignore[attr-defined]",
            "            )",
            "        for section in self._sections:  # type: ignore[attr-defined]",
            "            item_section: ConfigOptionsDictType = self.getsection(section)  # type: ignore[assignment]",
            "            self._write_section(fp, section, item_section.items(), delimiter)  # type: ignore[attr-defined]",
            "",
            "    def as_dict(",
            "        self,",
            "        display_source: bool = False,",
            "        display_sensitive: bool = False,",
            "        raw: bool = False,",
            "        include_env: bool = True,",
            "        include_cmds: bool = True,",
            "        include_secret: bool = True,",
            "    ) -> ConfigSourcesType:",
            "        \"\"\"",
            "        Returns the current configuration as an OrderedDict of OrderedDicts.",
            "",
            "        When materializing current configuration Airflow defaults are",
            "        materialized along with user set configs. If any of the `include_*`",
            "        options are False then the result of calling command or secret key",
            "        configs do not override Airflow defaults and instead are passed through.",
            "        In order to then avoid Airflow defaults from overwriting user set",
            "        command or secret key configs we filter out bare sensitive_config_values",
            "        that are set to Airflow defaults when command or secret key configs",
            "        produce different values.",
            "",
            "        :param display_source: If False, the option value is returned. If True,",
            "            a tuple of (option_value, source) is returned. Source is either",
            "            'airflow.cfg', 'default', 'env var', or 'cmd'.",
            "        :param display_sensitive: If True, the values of options set by env",
            "            vars and bash commands will be displayed. If False, those options",
            "            are shown as '< hidden >'",
            "        :param raw: Should the values be output as interpolated values, or the",
            "            \"raw\" form that can be fed back in to ConfigParser",
            "        :param include_env: Should the value of configuration from AIRFLOW__",
            "            environment variables be included or not",
            "        :param include_cmds: Should the result of calling any *_cmd config be",
            "            set (True, default), or should the _cmd options be left as the",
            "            command to run (False)",
            "        :param include_secret: Should the result of calling any *_secret config be",
            "            set (True, default), or should the _secret options be left as the",
            "            path to get the secret from (False)",
            "        :rtype: Dict[str, Dict[str, str]]",
            "        :return: Dictionary, where the key is the name of the section and the content is",
            "            the dictionary with the name of the parameter and its value.",
            "        \"\"\"",
            "        config_sources: ConfigSourcesType = {}",
            "        configs = [",
            "            ('default', self.airflow_defaults),",
            "            ('airflow.cfg', self),",
            "        ]",
            "",
            "        self._replace_config_with_display_sources(",
            "            config_sources,",
            "            configs,",
            "            display_source,",
            "            raw,",
            "            self.deprecated_options,",
            "            include_cmds=include_cmds,",
            "            include_env=include_env,",
            "            include_secret=include_secret,",
            "        )",
            "",
            "        # add env vars and overwrite because they have priority",
            "        if include_env:",
            "            self._include_envs(config_sources, display_sensitive, display_source, raw)",
            "        else:",
            "            self._filter_by_source(config_sources, display_source, self._get_env_var_option)",
            "",
            "        # add bash commands",
            "        if include_cmds:",
            "            self._include_commands(config_sources, display_sensitive, display_source, raw)",
            "        else:",
            "            self._filter_by_source(config_sources, display_source, self._get_cmd_option)",
            "",
            "        # add config from secret backends",
            "        if include_secret:",
            "            self._include_secrets(config_sources, display_sensitive, display_source, raw)",
            "        else:",
            "            self._filter_by_source(config_sources, display_source, self._get_secret_option)",
            "",
            "        return config_sources",
            "",
            "    def _include_secrets(",
            "        self,",
            "        config_sources: ConfigSourcesType,",
            "        display_sensitive: bool,",
            "        display_source: bool,",
            "        raw: bool,",
            "    ):",
            "        for (section, key) in self.sensitive_config_values:",
            "            value: Optional[str] = self._get_secret_option_from_config_sources(config_sources, section, key)",
            "            if value:",
            "                if not display_sensitive:",
            "                    value = '< hidden >'",
            "                if display_source:",
            "                    opt: Union[str, Tuple[str, str]] = (value, 'secret')",
            "                elif raw:",
            "                    opt = value.replace('%', '%%')",
            "                else:",
            "                    opt = value",
            "                config_sources.setdefault(section, OrderedDict()).update({key: opt})",
            "                del config_sources[section][key + '_secret']",
            "",
            "    def _include_commands(",
            "        self,",
            "        config_sources: ConfigSourcesType,",
            "        display_sensitive: bool,",
            "        display_source: bool,",
            "        raw: bool,",
            "    ):",
            "        for (section, key) in self.sensitive_config_values:",
            "            opt = self._get_cmd_option_from_config_sources(config_sources, section, key)",
            "            if not opt:",
            "                continue",
            "            opt_to_set: Union[str, Tuple[str, str], None] = opt",
            "            if not display_sensitive:",
            "                opt_to_set = '< hidden >'",
            "            if display_source:",
            "                opt_to_set = (str(opt_to_set), 'cmd')",
            "            elif raw:",
            "                opt_to_set = str(opt_to_set).replace('%', '%%')",
            "            if opt_to_set is not None:",
            "                dict_to_update: Dict[str, Union[str, Tuple[str, str]]] = {key: opt_to_set}",
            "                config_sources.setdefault(section, OrderedDict()).update(dict_to_update)",
            "                del config_sources[section][key + '_cmd']",
            "",
            "    def _include_envs(",
            "        self,",
            "        config_sources: ConfigSourcesType,",
            "        display_sensitive: bool,",
            "        display_source: bool,",
            "        raw: bool,",
            "    ):",
            "        for env_var in [",
            "            os_environment for os_environment in os.environ if os_environment.startswith(ENV_VAR_PREFIX)",
            "        ]:",
            "            try:",
            "                _, section, key = env_var.split('__', 2)",
            "                opt = self._get_env_var_option(section, key)",
            "            except ValueError:",
            "                continue",
            "            if opt is None:",
            "                log.warning(\"Ignoring unknown env var '%s'\", env_var)",
            "                continue",
            "            if not display_sensitive and env_var != self._env_var_name('core', 'unit_test_mode'):",
            "                opt = '< hidden >'",
            "            elif raw:",
            "                opt = opt.replace('%', '%%')",
            "            if display_source:",
            "                opt = (opt, 'env var')",
            "",
            "            section = section.lower()",
            "            # if we lower key for kubernetes_environment_variables section,",
            "            # then we won't be able to set any Airflow environment",
            "            # variables. Airflow only parse environment variables starts",
            "            # with AIRFLOW_. Therefore, we need to make it a special case.",
            "            if section != 'kubernetes_environment_variables':",
            "                key = key.lower()",
            "            config_sources.setdefault(section, OrderedDict()).update({key: opt})",
            "",
            "    def _filter_by_source(",
            "        self,",
            "        config_sources: ConfigSourcesType,",
            "        display_source: bool,",
            "        getter_func,",
            "    ):",
            "        \"\"\"",
            "        Deletes default configs from current configuration (an OrderedDict of",
            "        OrderedDicts) if it would conflict with special sensitive_config_values.",
            "",
            "        This is necessary because bare configs take precedence over the command",
            "        or secret key equivalents so if the current running config is",
            "        materialized with Airflow defaults they in turn override user set",
            "        command or secret key configs.",
            "",
            "        :param config_sources: The current configuration to operate on",
            "        :param display_source: If False, configuration options contain raw",
            "            values. If True, options are a tuple of (option_value, source).",
            "            Source is either 'airflow.cfg', 'default', 'env var', or 'cmd'.",
            "        :param getter_func: A callback function that gets the user configured",
            "            override value for a particular sensitive_config_values config.",
            "        :rtype: None",
            "        :return: None, the given config_sources is filtered if necessary,",
            "            otherwise untouched.",
            "        \"\"\"",
            "        for (section, key) in self.sensitive_config_values:",
            "            # Don't bother if we don't have section / key",
            "            if section not in config_sources or key not in config_sources[section]:",
            "                continue",
            "            # Check that there is something to override defaults",
            "            try:",
            "                getter_opt = getter_func(section, key)",
            "            except ValueError:",
            "                continue",
            "            if not getter_opt:",
            "                continue",
            "            # Check to see that there is a default value",
            "            if not self.airflow_defaults.has_option(section, key):",
            "                continue",
            "            # Check to see if bare setting is the same as defaults",
            "            if display_source:",
            "                # when display_source = true, we know that the config_sources contains tuple",
            "                opt, source = config_sources[section][key]  # type: ignore",
            "            else:",
            "                opt = config_sources[section][key]",
            "            if opt == self.airflow_defaults.get(section, key):",
            "                del config_sources[section][key]",
            "",
            "    @staticmethod",
            "    def _replace_config_with_display_sources(",
            "        config_sources: ConfigSourcesType,",
            "        configs: Iterable[Tuple[str, ConfigParser]],",
            "        display_source: bool,",
            "        raw: bool,",
            "        deprecated_options: Dict[Tuple[str, str], Tuple[str, str, str]],",
            "        include_env: bool,",
            "        include_cmds: bool,",
            "        include_secret: bool,",
            "    ):",
            "        for (source_name, config) in configs:",
            "            for section in config.sections():",
            "                AirflowConfigParser._replace_section_config_with_display_sources(",
            "                    config,",
            "                    config_sources,",
            "                    display_source,",
            "                    raw,",
            "                    section,",
            "                    source_name,",
            "                    deprecated_options,",
            "                    configs,",
            "                    include_env=include_env,",
            "                    include_cmds=include_cmds,",
            "                    include_secret=include_secret,",
            "                )",
            "",
            "    @staticmethod",
            "    def _deprecated_value_is_set_in_config(",
            "        deprecated_section: str,",
            "        deprecated_key: str,",
            "        configs: Iterable[Tuple[str, ConfigParser]],",
            "    ) -> bool:",
            "        for config_type, config in configs:",
            "            if config_type == 'default':",
            "                continue",
            "            try:",
            "                deprecated_section_array = config.items(section=deprecated_section, raw=True)",
            "                for (key_candidate, _) in deprecated_section_array:",
            "                    if key_candidate == deprecated_key:",
            "                        return True",
            "            except NoSectionError:",
            "                pass",
            "        return False",
            "",
            "    @staticmethod",
            "    def _deprecated_variable_is_set(deprecated_section: str, deprecated_key: str) -> bool:",
            "        return (",
            "            os.environ.get(f'{ENV_VAR_PREFIX}{deprecated_section.upper()}__{deprecated_key.upper()}')",
            "            is not None",
            "        )",
            "",
            "    @staticmethod",
            "    def _deprecated_command_is_set_in_config(",
            "        deprecated_section: str, deprecated_key: str, configs: Iterable[Tuple[str, ConfigParser]]",
            "    ) -> bool:",
            "        return AirflowConfigParser._deprecated_value_is_set_in_config(",
            "            deprecated_section=deprecated_section, deprecated_key=deprecated_key + \"_cmd\", configs=configs",
            "        )",
            "",
            "    @staticmethod",
            "    def _deprecated_variable_command_is_set(deprecated_section: str, deprecated_key: str) -> bool:",
            "        return (",
            "            os.environ.get(f'{ENV_VAR_PREFIX}{deprecated_section.upper()}__{deprecated_key.upper()}_CMD')",
            "            is not None",
            "        )",
            "",
            "    @staticmethod",
            "    def _deprecated_secret_is_set_in_config(",
            "        deprecated_section: str, deprecated_key: str, configs: Iterable[Tuple[str, ConfigParser]]",
            "    ) -> bool:",
            "        return AirflowConfigParser._deprecated_value_is_set_in_config(",
            "            deprecated_section=deprecated_section, deprecated_key=deprecated_key + \"_secret\", configs=configs",
            "        )",
            "",
            "    @staticmethod",
            "    def _deprecated_variable_secret_is_set(deprecated_section: str, deprecated_key: str) -> bool:",
            "        return (",
            "            os.environ.get(f'{ENV_VAR_PREFIX}{deprecated_section.upper()}__{deprecated_key.upper()}_SECRET')",
            "            is not None",
            "        )",
            "",
            "    @staticmethod",
            "    def _replace_section_config_with_display_sources(",
            "        config: ConfigParser,",
            "        config_sources: ConfigSourcesType,",
            "        display_source: bool,",
            "        raw: bool,",
            "        section: str,",
            "        source_name: str,",
            "        deprecated_options: Dict[Tuple[str, str], Tuple[str, str, str]],",
            "        configs: Iterable[Tuple[str, ConfigParser]],",
            "        include_env: bool,",
            "        include_cmds: bool,",
            "        include_secret: bool,",
            "    ):",
            "        sect = config_sources.setdefault(section, OrderedDict())",
            "        for (k, val) in config.items(section=section, raw=raw):",
            "            deprecated_section, deprecated_key, _ = deprecated_options.get((section, k), (None, None, None))",
            "            if deprecated_section and deprecated_key:",
            "                if source_name == 'default':",
            "                    # If deprecated entry has some non-default value set for any of the sources requested,",
            "                    # We should NOT set default for the new entry (because it will override anything",
            "                    # coming from the deprecated ones)",
            "                    if AirflowConfigParser._deprecated_value_is_set_in_config(",
            "                        deprecated_section, deprecated_key, configs",
            "                    ):",
            "                        continue",
            "                    if include_env and AirflowConfigParser._deprecated_variable_is_set(",
            "                        deprecated_section, deprecated_key",
            "                    ):",
            "                        continue",
            "                    if include_cmds and (",
            "                        AirflowConfigParser._deprecated_variable_command_is_set(",
            "                            deprecated_section, deprecated_key",
            "                        )",
            "                        or AirflowConfigParser._deprecated_command_is_set_in_config(",
            "                            deprecated_section, deprecated_key, configs",
            "                        )",
            "                    ):",
            "                        continue",
            "                    if include_secret and (",
            "                        AirflowConfigParser._deprecated_variable_secret_is_set(",
            "                            deprecated_section, deprecated_key",
            "                        )",
            "                        or AirflowConfigParser._deprecated_secret_is_set_in_config(",
            "                            deprecated_section, deprecated_key, configs",
            "                        )",
            "                    ):",
            "                        continue",
            "            if display_source:",
            "                sect[k] = (val, source_name)",
            "            else:",
            "                sect[k] = val",
            "",
            "    def load_test_config(self):",
            "        \"\"\"",
            "        Load the unit test configuration.",
            "",
            "        Note: this is not reversible.",
            "        \"\"\"",
            "        # remove all sections, falling back to defaults",
            "        for section in self.sections():",
            "            self.remove_section(section)",
            "",
            "        # then read test config",
            "",
            "        path = _default_config_file_path('default_test.cfg')",
            "        log.info(\"Reading default test configuration from %s\", path)",
            "        self.read_string(_parameterized_config_from_template('default_test.cfg'))",
            "        # then read any \"custom\" test settings",
            "        log.info(\"Reading test configuration from %s\", TEST_CONFIG_FILE)",
            "        self.read(TEST_CONFIG_FILE)",
            "",
            "    @staticmethod",
            "    def _warn_deprecate(section: str, key: str, deprecated_section: str, deprecated_name: str):",
            "        if section == deprecated_section:",
            "            warnings.warn(",
            "                f'The {deprecated_name} option in [{section}] has been renamed to {key} - '",
            "                f'the old setting has been used, but please update your config.',",
            "                DeprecationWarning,",
            "                stacklevel=3,",
            "            )",
            "        else:",
            "            warnings.warn(",
            "                f'The {deprecated_name} option in [{deprecated_section}] has been moved to the {key} option '",
            "                f'in [{section}] - the old setting has been used, but please update your config.',",
            "                DeprecationWarning,",
            "                stacklevel=3,",
            "            )",
            "",
            "    def __getstate__(self):",
            "        return {",
            "            name: getattr(self, name)",
            "            for name in [",
            "                '_sections',",
            "                'is_validated',",
            "                'airflow_defaults',",
            "            ]",
            "        }",
            "",
            "    def __setstate__(self, state):",
            "        self.__init__()",
            "        config = state.pop('_sections')",
            "        self.read_dict(config)",
            "        self.__dict__.update(state)",
            "",
            "",
            "def get_airflow_home() -> str:",
            "    \"\"\"Get path to Airflow Home\"\"\"",
            "    return expand_env_var(os.environ.get('AIRFLOW_HOME', '~/airflow'))",
            "",
            "",
            "def get_airflow_config(airflow_home) -> str:",
            "    \"\"\"Get Path to airflow.cfg path\"\"\"",
            "    airflow_config_var = os.environ.get('AIRFLOW_CONFIG')",
            "    if airflow_config_var is None:",
            "        return os.path.join(airflow_home, 'airflow.cfg')",
            "    return expand_env_var(airflow_config_var)",
            "",
            "",
            "def _parameterized_config_from_template(filename) -> str:",
            "    TEMPLATE_START = '# ----------------------- TEMPLATE BEGINS HERE -----------------------\\n'",
            "",
            "    path = _default_config_file_path(filename)",
            "    with open(path) as fh:",
            "        for line in fh:",
            "            if line != TEMPLATE_START:",
            "                continue",
            "            return parameterized_config(fh.read().strip())",
            "    raise RuntimeError(f\"Template marker not found in {path!r}\")",
            "",
            "",
            "def parameterized_config(template) -> str:",
            "    \"\"\"",
            "    Generates a configuration from the provided template + variables defined in",
            "    current scope",
            "",
            "    :param template: a config content templated with {{variables}}",
            "    \"\"\"",
            "    all_vars = {k: v for d in [globals(), locals()] for k, v in d.items()}",
            "    return template.format(**all_vars)",
            "",
            "",
            "def get_airflow_test_config(airflow_home) -> str:",
            "    \"\"\"Get path to unittests.cfg\"\"\"",
            "    if 'AIRFLOW_TEST_CONFIG' not in os.environ:",
            "        return os.path.join(airflow_home, 'unittests.cfg')",
            "    # It will never return None",
            "    return expand_env_var(os.environ['AIRFLOW_TEST_CONFIG'])  # type: ignore[return-value]",
            "",
            "",
            "def _generate_fernet_key() -> str:",
            "    from cryptography.fernet import Fernet",
            "",
            "    return Fernet.generate_key().decode()",
            "",
            "",
            "def initialize_config() -> AirflowConfigParser:",
            "    \"\"\"",
            "    Load the Airflow config files.",
            "",
            "    Called for you automatically as part of the Airflow boot process.",
            "    \"\"\"",
            "    global FERNET_KEY, AIRFLOW_HOME",
            "",
            "    default_config = _parameterized_config_from_template('default_airflow.cfg')",
            "",
            "    local_conf = AirflowConfigParser(default_config=default_config)",
            "",
            "    if local_conf.getboolean('core', 'unit_test_mode'):",
            "        # Load test config only",
            "        if not os.path.isfile(TEST_CONFIG_FILE):",
            "            from cryptography.fernet import Fernet",
            "",
            "            log.info('Creating new Airflow config file for unit tests in: %s', TEST_CONFIG_FILE)",
            "            pathlib.Path(AIRFLOW_HOME).mkdir(parents=True, exist_ok=True)",
            "",
            "            FERNET_KEY = Fernet.generate_key().decode()",
            "",
            "            with open(TEST_CONFIG_FILE, 'w') as file:",
            "                cfg = _parameterized_config_from_template('default_test.cfg')",
            "                file.write(cfg)",
            "",
            "        local_conf.load_test_config()",
            "    else:",
            "        # Load normal config",
            "        if not os.path.isfile(AIRFLOW_CONFIG):",
            "            from cryptography.fernet import Fernet",
            "",
            "            log.info('Creating new Airflow config file in: %s', AIRFLOW_CONFIG)",
            "            pathlib.Path(AIRFLOW_HOME).mkdir(parents=True, exist_ok=True)",
            "",
            "            FERNET_KEY = Fernet.generate_key().decode()",
            "",
            "            with open(AIRFLOW_CONFIG, 'w') as file:",
            "                file.write(default_config)",
            "",
            "        log.info(\"Reading the config from %s\", AIRFLOW_CONFIG)",
            "",
            "        local_conf.read(AIRFLOW_CONFIG)",
            "",
            "        if local_conf.has_option('core', 'AIRFLOW_HOME'):",
            "            msg = (",
            "                'Specifying both AIRFLOW_HOME environment variable and airflow_home '",
            "                'in the config file is deprecated. Please use only the AIRFLOW_HOME '",
            "                'environment variable and remove the config file entry.'",
            "            )",
            "            if 'AIRFLOW_HOME' in os.environ:",
            "                warnings.warn(msg, category=DeprecationWarning)",
            "            elif local_conf.get('core', 'airflow_home') == AIRFLOW_HOME:",
            "                warnings.warn(",
            "                    'Specifying airflow_home in the config file is deprecated. As you '",
            "                    'have left it at the default value you should remove the setting '",
            "                    'from your airflow.cfg and suffer no change in behaviour.',",
            "                    category=DeprecationWarning,",
            "                )",
            "            else:",
            "                # there",
            "                AIRFLOW_HOME = local_conf.get('core', 'airflow_home')  # type: ignore[assignment]",
            "                warnings.warn(msg, category=DeprecationWarning)",
            "",
            "        # They _might_ have set unit_test_mode in the airflow.cfg, we still",
            "        # want to respect that and then load the unittests.cfg",
            "        if local_conf.getboolean('core', 'unit_test_mode'):",
            "            local_conf.load_test_config()",
            "",
            "    # Make it no longer a proxy variable, just set it to an actual string",
            "    global WEBSERVER_CONFIG",
            "    WEBSERVER_CONFIG = AIRFLOW_HOME + '/webserver_config.py'",
            "",
            "    if not os.path.isfile(WEBSERVER_CONFIG):",
            "        import shutil",
            "",
            "        log.info('Creating new FAB webserver config file in: %s', WEBSERVER_CONFIG)",
            "        shutil.copy(_default_config_file_path('default_webserver_config.py'), WEBSERVER_CONFIG)",
            "    return local_conf",
            "",
            "",
            "# Historical convenience functions to access config entries",
            "def load_test_config():",
            "    \"\"\"Historical load_test_config\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'load_test_config' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.load_test_config'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    conf.load_test_config()",
            "",
            "",
            "def get(*args, **kwargs) -> Optional[ConfigType]:",
            "    \"\"\"Historical get\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'get' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.get'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    return conf.get(*args, **kwargs)",
            "",
            "",
            "def getboolean(*args, **kwargs) -> bool:",
            "    \"\"\"Historical getboolean\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'getboolean' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.getboolean'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    return conf.getboolean(*args, **kwargs)",
            "",
            "",
            "def getfloat(*args, **kwargs) -> float:",
            "    \"\"\"Historical getfloat\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'getfloat' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.getfloat'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    return conf.getfloat(*args, **kwargs)",
            "",
            "",
            "def getint(*args, **kwargs) -> int:",
            "    \"\"\"Historical getint\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'getint' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.getint'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    return conf.getint(*args, **kwargs)",
            "",
            "",
            "def getsection(*args, **kwargs) -> Optional[ConfigOptionsDictType]:",
            "    \"\"\"Historical getsection\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'getsection' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.getsection'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    return conf.getsection(*args, **kwargs)",
            "",
            "",
            "def has_option(*args, **kwargs) -> bool:",
            "    \"\"\"Historical has_option\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'has_option' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.has_option'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    return conf.has_option(*args, **kwargs)",
            "",
            "",
            "def remove_option(*args, **kwargs) -> bool:",
            "    \"\"\"Historical remove_option\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'remove_option' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.remove_option'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    return conf.remove_option(*args, **kwargs)",
            "",
            "",
            "def as_dict(*args, **kwargs) -> ConfigSourcesType:",
            "    \"\"\"Historical as_dict\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'as_dict' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.as_dict'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    return conf.as_dict(*args, **kwargs)",
            "",
            "",
            "def set(*args, **kwargs) -> None:",
            "    \"\"\"Historical set\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'set' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.set'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    conf.set(*args, **kwargs)",
            "",
            "",
            "def ensure_secrets_loaded() -> List[BaseSecretsBackend]:",
            "    \"\"\"",
            "    Ensure that all secrets backends are loaded.",
            "    If the secrets_backend_list contains only 2 default backends, reload it.",
            "    \"\"\"",
            "    # Check if the secrets_backend_list contains only 2 default backends",
            "    if len(secrets_backend_list) == 2:",
            "        return initialize_secrets_backends()",
            "    return secrets_backend_list",
            "",
            "",
            "def get_custom_secret_backend() -> Optional[BaseSecretsBackend]:",
            "    \"\"\"Get Secret Backend if defined in airflow.cfg\"\"\"",
            "    secrets_backend_cls = conf.getimport(section='secrets', key='backend')",
            "",
            "    if secrets_backend_cls:",
            "        try:",
            "            backends: Any = conf.get(section='secrets', key='backend_kwargs', fallback='{}')",
            "            alternative_secrets_config_dict = json.loads(backends)",
            "        except JSONDecodeError:",
            "            alternative_secrets_config_dict = {}",
            "",
            "        return secrets_backend_cls(**alternative_secrets_config_dict)",
            "    return None",
            "",
            "",
            "def initialize_secrets_backends() -> List[BaseSecretsBackend]:",
            "    \"\"\"",
            "    * import secrets backend classes",
            "    * instantiate them and return them in a list",
            "    \"\"\"",
            "    backend_list = []",
            "",
            "    custom_secret_backend = get_custom_secret_backend()",
            "",
            "    if custom_secret_backend is not None:",
            "        backend_list.append(custom_secret_backend)",
            "",
            "    for class_name in DEFAULT_SECRETS_SEARCH_PATH:",
            "        secrets_backend_cls = import_string(class_name)",
            "        backend_list.append(secrets_backend_cls())",
            "",
            "    return backend_list",
            "",
            "",
            "@functools.lru_cache(maxsize=None)",
            "def _DEFAULT_CONFIG() -> str:",
            "    path = _default_config_file_path('default_airflow.cfg')",
            "    with open(path) as fh:",
            "        return fh.read()",
            "",
            "",
            "@functools.lru_cache(maxsize=None)",
            "def _TEST_CONFIG() -> str:",
            "    path = _default_config_file_path('default_test.cfg')",
            "    with open(path) as fh:",
            "        return fh.read()",
            "",
            "",
            "_deprecated = {",
            "    'DEFAULT_CONFIG': _DEFAULT_CONFIG,",
            "    'TEST_CONFIG': _TEST_CONFIG,",
            "    'TEST_CONFIG_FILE_PATH': functools.partial(_default_config_file_path, 'default_test.cfg'),",
            "    'DEFAULT_CONFIG_FILE_PATH': functools.partial(_default_config_file_path, 'default_airflow.cfg'),",
            "}",
            "",
            "",
            "def __getattr__(name):",
            "    if name in _deprecated:",
            "        warnings.warn(",
            "            f\"{__name__}.{name} is deprecated and will be removed in future\",",
            "            DeprecationWarning,",
            "            stacklevel=2,",
            "        )",
            "        return _deprecated[name]()",
            "    raise AttributeError(f\"module {__name__} has no attribute {name}\")",
            "",
            "",
            "# Setting AIRFLOW_HOME and AIRFLOW_CONFIG from environment variables, using",
            "# \"~/airflow\" and \"$AIRFLOW_HOME/airflow.cfg\" respectively as defaults.",
            "AIRFLOW_HOME = get_airflow_home()",
            "AIRFLOW_CONFIG = get_airflow_config(AIRFLOW_HOME)",
            "",
            "",
            "# Set up dags folder for unit tests",
            "# this directory won't exist if users install via pip",
            "_TEST_DAGS_FOLDER = os.path.join(",
            "    os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'tests', 'dags'",
            ")",
            "if os.path.exists(_TEST_DAGS_FOLDER):",
            "    TEST_DAGS_FOLDER = _TEST_DAGS_FOLDER",
            "else:",
            "    TEST_DAGS_FOLDER = os.path.join(AIRFLOW_HOME, 'dags')",
            "",
            "# Set up plugins folder for unit tests",
            "_TEST_PLUGINS_FOLDER = os.path.join(",
            "    os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'tests', 'plugins'",
            ")",
            "if os.path.exists(_TEST_PLUGINS_FOLDER):",
            "    TEST_PLUGINS_FOLDER = _TEST_PLUGINS_FOLDER",
            "else:",
            "    TEST_PLUGINS_FOLDER = os.path.join(AIRFLOW_HOME, 'plugins')",
            "",
            "",
            "TEST_CONFIG_FILE = get_airflow_test_config(AIRFLOW_HOME)",
            "",
            "SECRET_KEY = b64encode(os.urandom(16)).decode('utf-8')",
            "FERNET_KEY = ''  # Set only if needed when generating a new file",
            "WEBSERVER_CONFIG = ''  # Set by initialize_config",
            "",
            "conf = initialize_config()",
            "secrets_backend_list = initialize_secrets_backends()",
            "conf.validate()"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import datetime",
            "import functools",
            "import json",
            "import logging",
            "import multiprocessing",
            "import os",
            "import pathlib",
            "import re",
            "import shlex",
            "import subprocess",
            "import sys",
            "import warnings",
            "from base64 import b64encode",
            "from collections import OrderedDict",
            "",
            "# Ignored Mypy on configparser because it thinks the configparser module has no _UNSET attribute",
            "from configparser import _UNSET, ConfigParser, NoOptionError, NoSectionError  # type: ignore",
            "from contextlib import suppress",
            "from json.decoder import JSONDecodeError",
            "from re import Pattern",
            "from typing import IO, Any, Dict, Iterable, List, Optional, Set, Tuple, Union",
            "from urllib.parse import urlparse",
            "",
            "from typing_extensions import overload",
            "",
            "from airflow.exceptions import AirflowConfigException",
            "from airflow.secrets import DEFAULT_SECRETS_SEARCH_PATH, BaseSecretsBackend",
            "from airflow.utils import yaml",
            "from airflow.utils.module_loading import import_string",
            "from airflow.utils.weight_rule import WeightRule",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "# show Airflow's deprecation warnings",
            "if not sys.warnoptions:",
            "    warnings.filterwarnings(action='default', category=DeprecationWarning, module='airflow')",
            "    warnings.filterwarnings(action='default', category=PendingDeprecationWarning, module='airflow')",
            "",
            "_SQLITE3_VERSION_PATTERN = re.compile(r\"(?P<version>^\\d+(?:\\.\\d+)*)\\D?.*$\")",
            "",
            "ConfigType = Union[str, int, float, bool]",
            "ConfigOptionsDictType = Dict[str, ConfigType]",
            "ConfigSectionSourcesType = Dict[str, Union[str, Tuple[str, str]]]",
            "ConfigSourcesType = Dict[str, ConfigSectionSourcesType]",
            "",
            "ENV_VAR_PREFIX = 'AIRFLOW__'",
            "",
            "",
            "def _parse_sqlite_version(s: str) -> Tuple[int, ...]:",
            "    match = _SQLITE3_VERSION_PATTERN.match(s)",
            "    if match is None:",
            "        return ()",
            "    return tuple(int(p) for p in match.group(\"version\").split(\".\"))",
            "",
            "",
            "@overload",
            "def expand_env_var(env_var: None) -> None:",
            "    ...",
            "",
            "",
            "@overload",
            "def expand_env_var(env_var: str) -> str:",
            "    ...",
            "",
            "",
            "def expand_env_var(env_var: Union[str, None]) -> Optional[Union[str, None]]:",
            "    \"\"\"",
            "    Expands (potentially nested) env vars by repeatedly applying",
            "    `expandvars` and `expanduser` until interpolation stops having",
            "    any effect.",
            "    \"\"\"",
            "    if not env_var:",
            "        return env_var",
            "    while True:",
            "        interpolated = os.path.expanduser(os.path.expandvars(str(env_var)))",
            "        if interpolated == env_var:",
            "            return interpolated",
            "        else:",
            "            env_var = interpolated",
            "",
            "",
            "def run_command(command: str) -> str:",
            "    \"\"\"Runs command and returns stdout\"\"\"",
            "    process = subprocess.Popen(",
            "        shlex.split(command), stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True",
            "    )",
            "    output, stderr = (stream.decode(sys.getdefaultencoding(), 'ignore') for stream in process.communicate())",
            "",
            "    if process.returncode != 0:",
            "        raise AirflowConfigException(",
            "            f\"Cannot execute {command}. Error code is: {process.returncode}. \"",
            "            f\"Output: {output}, Stderr: {stderr}\"",
            "        )",
            "",
            "    return output",
            "",
            "",
            "def _get_config_value_from_secret_backend(config_key: str) -> Optional[str]:",
            "    \"\"\"Get Config option values from Secret Backend\"\"\"",
            "    try:",
            "        secrets_client = get_custom_secret_backend()",
            "        if not secrets_client:",
            "            return None",
            "        return secrets_client.get_config(config_key)",
            "    except Exception as e:",
            "        raise AirflowConfigException(",
            "            'Cannot retrieve config from alternative secrets backend. '",
            "            'Make sure it is configured properly and that the Backend '",
            "            'is accessible.\\n'",
            "            f'{e}'",
            "        )",
            "",
            "",
            "def _default_config_file_path(file_name: str) -> str:",
            "    templates_dir = os.path.join(os.path.dirname(__file__), 'config_templates')",
            "    return os.path.join(templates_dir, file_name)",
            "",
            "",
            "def default_config_yaml() -> List[Dict[str, Any]]:",
            "    \"\"\"",
            "    Read Airflow configs from YAML file",
            "",
            "    :return: Python dictionary containing configs & their info",
            "    \"\"\"",
            "    with open(_default_config_file_path('config.yml')) as config_file:",
            "        return yaml.safe_load(config_file)",
            "",
            "",
            "class AirflowConfigParser(ConfigParser):",
            "    \"\"\"Custom Airflow Configparser supporting defaults and deprecated options\"\"\"",
            "",
            "    # These configuration elements can be fetched as the stdout of commands",
            "    # following the \"{section}__{name}_cmd\" pattern, the idea behind this",
            "    # is to not store password on boxes in text files.",
            "    # These configs can also be fetched from Secrets backend",
            "    # following the \"{section}__{name}__secret\" pattern",
            "    sensitive_config_values: Set[Tuple[str, str]] = {",
            "        ('database', 'sql_alchemy_conn'),",
            "        ('core', 'fernet_key'),",
            "        ('celery', 'broker_url'),",
            "        ('celery', 'flower_basic_auth'),",
            "        ('celery', 'result_backend'),",
            "        ('atlas', 'password'),",
            "        ('smtp', 'smtp_password'),",
            "        ('webserver', 'secret_key'),",
            "        # The following options are deprecated",
            "        ('core', 'sql_alchemy_conn'),",
            "    }",
            "",
            "    # A mapping of (new section, new option) -> (old section, old option, since_version).",
            "    # When reading new option, the old option will be checked to see if it exists. If it does a",
            "    # DeprecationWarning will be issued and the old option will be used instead",
            "    deprecated_options: Dict[Tuple[str, str], Tuple[str, str, str]] = {",
            "        ('celery', 'worker_precheck'): ('core', 'worker_precheck', '2.0.0'),",
            "        ('logging', 'base_log_folder'): ('core', 'base_log_folder', '2.0.0'),",
            "        ('logging', 'remote_logging'): ('core', 'remote_logging', '2.0.0'),",
            "        ('logging', 'remote_log_conn_id'): ('core', 'remote_log_conn_id', '2.0.0'),",
            "        ('logging', 'remote_base_log_folder'): ('core', 'remote_base_log_folder', '2.0.0'),",
            "        ('logging', 'encrypt_s3_logs'): ('core', 'encrypt_s3_logs', '2.0.0'),",
            "        ('logging', 'logging_level'): ('core', 'logging_level', '2.0.0'),",
            "        ('logging', 'fab_logging_level'): ('core', 'fab_logging_level', '2.0.0'),",
            "        ('logging', 'logging_config_class'): ('core', 'logging_config_class', '2.0.0'),",
            "        ('logging', 'colored_console_log'): ('core', 'colored_console_log', '2.0.0'),",
            "        ('logging', 'colored_log_format'): ('core', 'colored_log_format', '2.0.0'),",
            "        ('logging', 'colored_formatter_class'): ('core', 'colored_formatter_class', '2.0.0'),",
            "        ('logging', 'log_format'): ('core', 'log_format', '2.0.0'),",
            "        ('logging', 'simple_log_format'): ('core', 'simple_log_format', '2.0.0'),",
            "        ('logging', 'task_log_prefix_template'): ('core', 'task_log_prefix_template', '2.0.0'),",
            "        ('logging', 'log_filename_template'): ('core', 'log_filename_template', '2.0.0'),",
            "        ('logging', 'log_processor_filename_template'): ('core', 'log_processor_filename_template', '2.0.0'),",
            "        ('logging', 'dag_processor_manager_log_location'): (",
            "            'core',",
            "            'dag_processor_manager_log_location',",
            "            '2.0.0',",
            "        ),",
            "        ('logging', 'task_log_reader'): ('core', 'task_log_reader', '2.0.0'),",
            "        ('metrics', 'statsd_on'): ('scheduler', 'statsd_on', '2.0.0'),",
            "        ('metrics', 'statsd_host'): ('scheduler', 'statsd_host', '2.0.0'),",
            "        ('metrics', 'statsd_port'): ('scheduler', 'statsd_port', '2.0.0'),",
            "        ('metrics', 'statsd_prefix'): ('scheduler', 'statsd_prefix', '2.0.0'),",
            "        ('metrics', 'statsd_allow_list'): ('scheduler', 'statsd_allow_list', '2.0.0'),",
            "        ('metrics', 'stat_name_handler'): ('scheduler', 'stat_name_handler', '2.0.0'),",
            "        ('metrics', 'statsd_datadog_enabled'): ('scheduler', 'statsd_datadog_enabled', '2.0.0'),",
            "        ('metrics', 'statsd_datadog_tags'): ('scheduler', 'statsd_datadog_tags', '2.0.0'),",
            "        ('metrics', 'statsd_custom_client_path'): ('scheduler', 'statsd_custom_client_path', '2.0.0'),",
            "        ('scheduler', 'parsing_processes'): ('scheduler', 'max_threads', '1.10.14'),",
            "        ('scheduler', 'scheduler_idle_sleep_time'): ('scheduler', 'processor_poll_interval', '2.2.0'),",
            "        ('operators', 'default_queue'): ('celery', 'default_queue', '2.1.0'),",
            "        ('core', 'hide_sensitive_var_conn_fields'): ('admin', 'hide_sensitive_variable_fields', '2.1.0'),",
            "        ('core', 'sensitive_var_conn_names'): ('admin', 'sensitive_variable_fields', '2.1.0'),",
            "        ('core', 'default_pool_task_slot_count'): ('core', 'non_pooled_task_slot_count', '1.10.4'),",
            "        ('core', 'max_active_tasks_per_dag'): ('core', 'dag_concurrency', '2.2.0'),",
            "        ('logging', 'worker_log_server_port'): ('celery', 'worker_log_server_port', '2.2.0'),",
            "        ('api', 'access_control_allow_origins'): ('api', 'access_control_allow_origin', '2.2.0'),",
            "        ('api', 'auth_backends'): ('api', 'auth_backend', '2.3.0'),",
            "        ('database', 'sql_alchemy_conn'): ('core', 'sql_alchemy_conn', '2.3.0'),",
            "        ('database', 'sql_engine_encoding'): ('core', 'sql_engine_encoding', '2.3.0'),",
            "        ('database', 'sql_engine_collation_for_ids'): ('core', 'sql_engine_collation_for_ids', '2.3.0'),",
            "        ('database', 'sql_alchemy_pool_enabled'): ('core', 'sql_alchemy_pool_enabled', '2.3.0'),",
            "        ('database', 'sql_alchemy_pool_size'): ('core', 'sql_alchemy_pool_size', '2.3.0'),",
            "        ('database', 'sql_alchemy_max_overflow'): ('core', 'sql_alchemy_max_overflow', '2.3.0'),",
            "        ('database', 'sql_alchemy_pool_recycle'): ('core', 'sql_alchemy_pool_recycle', '2.3.0'),",
            "        ('database', 'sql_alchemy_pool_pre_ping'): ('core', 'sql_alchemy_pool_pre_ping', '2.3.0'),",
            "        ('database', 'sql_alchemy_schema'): ('core', 'sql_alchemy_schema', '2.3.0'),",
            "        ('database', 'sql_alchemy_connect_args'): ('core', 'sql_alchemy_connect_args', '2.3.0'),",
            "        ('database', 'load_default_connections'): ('core', 'load_default_connections', '2.3.0'),",
            "        ('database', 'max_db_retries'): ('core', 'max_db_retries', '2.3.0'),",
            "    }",
            "",
            "    # A mapping of old default values that we want to change and warn the user",
            "    # about. Mapping of section -> setting -> { old, replace, by_version }",
            "    deprecated_values: Dict[str, Dict[str, Tuple[Pattern, str, str]]] = {",
            "        'core': {",
            "            'hostname_callable': (re.compile(r':'), r'.', '2.1'),",
            "        },",
            "        'webserver': {",
            "            'navbar_color': (re.compile(r'\\A#007A87\\Z', re.IGNORECASE), '#fff', '2.1'),",
            "            'dag_default_view': (re.compile(r'^tree$'), 'grid', '3.0'),",
            "        },",
            "        'email': {",
            "            'email_backend': (",
            "                re.compile(r'^airflow\\.contrib\\.utils\\.sendgrid\\.send_email$'),",
            "                r'airflow.providers.sendgrid.utils.emailer.send_email',",
            "                '2.1',",
            "            ),",
            "        },",
            "        'logging': {",
            "            'log_filename_template': (",
            "                re.compile(re.escape(\"{{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log\")),",
            "                \"XX-set-after-default-config-loaded-XX\",",
            "                '3.0',",
            "            ),",
            "        },",
            "        'api': {",
            "            'auth_backends': (",
            "                re.compile(r'^airflow\\.api\\.auth\\.backend\\.deny_all$|^$'),",
            "                'airflow.api.auth.backend.session',",
            "                '3.0',",
            "            ),",
            "        },",
            "        'elasticsearch': {",
            "            'log_id_template': (",
            "                re.compile('^' + re.escape('{dag_id}-{task_id}-{execution_date}-{try_number}') + '$'),",
            "                '{dag_id}-{task_id}-{run_id}-{map_index}-{try_number}',",
            "                '3.0',",
            "            )",
            "        },",
            "    }",
            "",
            "    _available_logging_levels = ['CRITICAL', 'FATAL', 'ERROR', 'WARN', 'WARNING', 'INFO', 'DEBUG']",
            "    enums_options = {",
            "        (\"core\", \"default_task_weight_rule\"): sorted(WeightRule.all_weight_rules()),",
            "        (\"core\", \"dag_ignore_file_syntax\"): [\"regexp\", \"glob\"],",
            "        ('core', 'mp_start_method'): multiprocessing.get_all_start_methods(),",
            "        (\"scheduler\", \"file_parsing_sort_mode\"): [\"modified_time\", \"random_seeded_by_host\", \"alphabetical\"],",
            "        (\"logging\", \"logging_level\"): _available_logging_levels,",
            "        (\"logging\", \"fab_logging_level\"): _available_logging_levels,",
            "        # celery_logging_level can be empty, which uses logging_level as fallback",
            "        (\"logging\", \"celery_logging_level\"): _available_logging_levels + [''],",
            "    }",
            "",
            "    upgraded_values: Dict[Tuple[str, str], str]",
            "    \"\"\"Mapping of (section,option) to the old value that was upgraded\"\"\"",
            "",
            "    # This method transforms option names on every read, get, or set operation.",
            "    # This changes from the default behaviour of ConfigParser from lower-casing",
            "    # to instead be case-preserving",
            "    def optionxform(self, optionstr: str) -> str:",
            "        return optionstr",
            "",
            "    def __init__(self, default_config: Optional[str] = None, *args, **kwargs):",
            "        super().__init__(*args, **kwargs)",
            "        self.upgraded_values = {}",
            "",
            "        self.airflow_defaults = ConfigParser(*args, **kwargs)",
            "        if default_config is not None:",
            "            self.airflow_defaults.read_string(default_config)",
            "            # Set the upgrade value based on the current loaded default",
            "            default = self.airflow_defaults.get('logging', 'log_filename_template', fallback=None)",
            "            if default:",
            "                replacement = self.deprecated_values['logging']['log_filename_template']",
            "                self.deprecated_values['logging']['log_filename_template'] = (",
            "                    replacement[0],",
            "                    default,",
            "                    replacement[2],",
            "                )",
            "            else:",
            "                # In case of tests it might not exist",
            "                with suppress(KeyError):",
            "                    del self.deprecated_values['logging']['log_filename_template']",
            "        else:",
            "            with suppress(KeyError):",
            "                del self.deprecated_values['logging']['log_filename_template']",
            "",
            "        self.is_validated = False",
            "",
            "    def validate(self):",
            "        self._validate_config_dependencies()",
            "        self._validate_enums()",
            "",
            "        for section, replacement in self.deprecated_values.items():",
            "            for name, info in replacement.items():",
            "                old, new, version = info",
            "                current_value = self.get(section, name, fallback=\"\")",
            "                if self._using_old_value(old, current_value):",
            "                    self.upgraded_values[(section, name)] = current_value",
            "                    new_value = old.sub(new, current_value)",
            "                    self._update_env_var(section=section, name=name, new_value=new_value)",
            "                    self._create_future_warning(",
            "                        name=name,",
            "                        section=section,",
            "                        current_value=current_value,",
            "                        new_value=new_value,",
            "                        version=version,",
            "                    )",
            "",
            "        self._upgrade_auth_backends()",
            "        self._upgrade_postgres_metastore_conn()",
            "        self.is_validated = True",
            "",
            "    def _upgrade_auth_backends(self):",
            "        \"\"\"",
            "        Ensure a custom auth_backends setting contains session,",
            "        which is needed by the UI for ajax queries.",
            "        \"\"\"",
            "        old_value = self.get(\"api\", \"auth_backends\", fallback=\"\")",
            "        if old_value in ('airflow.api.auth.backend.default', ''):",
            "            # handled by deprecated_values",
            "            pass",
            "        elif old_value.find('airflow.api.auth.backend.session') == -1:",
            "            new_value = old_value + \",airflow.api.auth.backend.session\"",
            "            self._update_env_var(section=\"api\", name=\"auth_backends\", new_value=new_value)",
            "            self.upgraded_values[(\"api\", \"auth_backends\")] = old_value",
            "",
            "            # if the old value is set via env var, we need to wipe it",
            "            # otherwise, it'll \"win\" over our adjusted value",
            "            old_env_var = self._env_var_name(\"api\", \"auth_backend\")",
            "            os.environ.pop(old_env_var, None)",
            "",
            "            warnings.warn(",
            "                'The auth_backends setting in [api] has had airflow.api.auth.backend.session added '",
            "                'in the running config, which is needed by the UI. Please update your config before '",
            "                'Apache Airflow 3.0.',",
            "                FutureWarning,",
            "            )",
            "",
            "    def _upgrade_postgres_metastore_conn(self):",
            "        \"\"\"",
            "        As of SQLAlchemy 1.4, schemes `postgres+psycopg2` and `postgres`",
            "        must be replaced with `postgresql`.",
            "        \"\"\"",
            "        section, key = 'database', 'sql_alchemy_conn'",
            "        old_value = self.get(section, key)",
            "        bad_schemes = ['postgres+psycopg2', 'postgres']",
            "        good_scheme = 'postgresql'",
            "        parsed = urlparse(old_value)",
            "        if parsed.scheme in bad_schemes:",
            "            warnings.warn(",
            "                f\"Bad scheme in Airflow configuration core > sql_alchemy_conn: `{parsed.scheme}`. \"",
            "                \"As of SQLAlchemy 1.4 (adopted in Airflow 2.3) this is no longer supported.  You must \"",
            "                f\"change to `{good_scheme}` before the next Airflow release.\",",
            "                FutureWarning,",
            "            )",
            "            self.upgraded_values[(section, key)] = old_value",
            "            new_value = re.sub('^' + re.escape(f\"{parsed.scheme}://\"), f\"{good_scheme}://\", old_value)",
            "            self._update_env_var(section=section, name=key, new_value=new_value)",
            "",
            "            # if the old value is set via env var, we need to wipe it",
            "            # otherwise, it'll \"win\" over our adjusted value",
            "            old_env_var = self._env_var_name(\"core\", key)",
            "            os.environ.pop(old_env_var, None)",
            "",
            "    def _validate_enums(self):",
            "        \"\"\"Validate that enum type config has an accepted value\"\"\"",
            "        for (section_key, option_key), enum_options in self.enums_options.items():",
            "            if self.has_option(section_key, option_key):",
            "                value = self.get(section_key, option_key)",
            "                if value not in enum_options:",
            "                    raise AirflowConfigException(",
            "                        f\"`[{section_key}] {option_key}` should not be \"",
            "                        f\"{value!r}. Possible values: {', '.join(enum_options)}.\"",
            "                    )",
            "",
            "    def _validate_config_dependencies(self):",
            "        \"\"\"",
            "        Validate that config values aren't invalid given other config values",
            "        or system-level limitations and requirements.",
            "        \"\"\"",
            "        is_executor_without_sqlite_support = self.get(\"core\", \"executor\") not in (",
            "            'DebugExecutor',",
            "            'SequentialExecutor',",
            "        )",
            "        is_sqlite = \"sqlite\" in self.get('database', 'sql_alchemy_conn')",
            "        if is_sqlite and is_executor_without_sqlite_support:",
            "            raise AirflowConfigException(f\"error: cannot use sqlite with the {self.get('core', 'executor')}\")",
            "        if is_sqlite:",
            "            import sqlite3",
            "",
            "            from airflow.utils.docs import get_docs_url",
            "",
            "            # Some features in storing rendered fields require sqlite version >= 3.15.0",
            "            min_sqlite_version = (3, 15, 0)",
            "            if _parse_sqlite_version(sqlite3.sqlite_version) < min_sqlite_version:",
            "                min_sqlite_version_str = \".\".join(str(s) for s in min_sqlite_version)",
            "                raise AirflowConfigException(",
            "                    f\"error: sqlite C library version too old (< {min_sqlite_version_str}). \"",
            "                    f\"See {get_docs_url('howto/set-up-database.html#setting-up-a-sqlite-database')}\"",
            "                )",
            "",
            "    def _using_old_value(self, old: Pattern, current_value: str) -> bool:",
            "        return old.search(current_value) is not None",
            "",
            "    def _update_env_var(self, section: str, name: str, new_value: Union[str]):",
            "        env_var = self._env_var_name(section, name)",
            "        # Set it as an env var so that any subprocesses keep the same override!",
            "        os.environ[env_var] = new_value",
            "",
            "    @staticmethod",
            "    def _create_future_warning(name: str, section: str, current_value: Any, new_value: Any, version: str):",
            "        warnings.warn(",
            "            f'The {name!r} setting in [{section}] has the old default value of {current_value!r}. '",
            "            f'This value has been changed to {new_value!r} in the running config, but '",
            "            f'please update your config before Apache Airflow {version}.',",
            "            FutureWarning,",
            "        )",
            "",
            "    def _env_var_name(self, section: str, key: str) -> str:",
            "        return f'{ENV_VAR_PREFIX}{section.upper()}__{key.upper()}'",
            "",
            "    def _get_env_var_option(self, section: str, key: str):",
            "        # must have format AIRFLOW__{SECTION}__{KEY} (note double underscore)",
            "        env_var = self._env_var_name(section, key)",
            "        if env_var in os.environ:",
            "            return expand_env_var(os.environ[env_var])",
            "        # alternatively AIRFLOW__{SECTION}__{KEY}_CMD (for a command)",
            "        env_var_cmd = env_var + '_CMD'",
            "        if env_var_cmd in os.environ:",
            "            # if this is a valid command key...",
            "            if (section, key) in self.sensitive_config_values:",
            "                return run_command(os.environ[env_var_cmd])",
            "        # alternatively AIRFLOW__{SECTION}__{KEY}_SECRET (to get from Secrets Backend)",
            "        env_var_secret_path = env_var + '_SECRET'",
            "        if env_var_secret_path in os.environ:",
            "            # if this is a valid secret path...",
            "            if (section, key) in self.sensitive_config_values:",
            "                return _get_config_value_from_secret_backend(os.environ[env_var_secret_path])",
            "        return None",
            "",
            "    def _get_cmd_option(self, section: str, key: str):",
            "        fallback_key = key + '_cmd'",
            "        if (section, key) in self.sensitive_config_values:",
            "            if super().has_option(section, fallback_key):",
            "                command = super().get(section, fallback_key)",
            "                return run_command(command)",
            "        return None",
            "",
            "    def _get_cmd_option_from_config_sources(",
            "        self, config_sources: ConfigSourcesType, section: str, key: str",
            "    ) -> Optional[str]:",
            "        fallback_key = key + '_cmd'",
            "        if (section, key) in self.sensitive_config_values:",
            "            section_dict = config_sources.get(section)",
            "            if section_dict is not None:",
            "                command_value = section_dict.get(fallback_key)",
            "                if command_value is not None:",
            "                    if isinstance(command_value, str):",
            "                        command = command_value",
            "                    else:",
            "                        command = command_value[0]",
            "                    return run_command(command)",
            "        return None",
            "",
            "    def _get_secret_option(self, section: str, key: str) -> Optional[str]:",
            "        \"\"\"Get Config option values from Secret Backend\"\"\"",
            "        fallback_key = key + '_secret'",
            "        if (section, key) in self.sensitive_config_values:",
            "            if super().has_option(section, fallback_key):",
            "                secrets_path = super().get(section, fallback_key)",
            "                return _get_config_value_from_secret_backend(secrets_path)",
            "        return None",
            "",
            "    def _get_secret_option_from_config_sources(",
            "        self, config_sources: ConfigSourcesType, section: str, key: str",
            "    ) -> Optional[str]:",
            "        fallback_key = key + '_secret'",
            "        if (section, key) in self.sensitive_config_values:",
            "            section_dict = config_sources.get(section)",
            "            if section_dict is not None:",
            "                secrets_path_value = section_dict.get(fallback_key)",
            "                if secrets_path_value is not None:",
            "                    if isinstance(secrets_path_value, str):",
            "                        secrets_path = secrets_path_value",
            "                    else:",
            "                        secrets_path = secrets_path_value[0]",
            "                    return _get_config_value_from_secret_backend(secrets_path)",
            "        return None",
            "",
            "    def get_mandatory_value(self, section: str, key: str, **kwargs) -> str:",
            "        value = self.get(section, key, **kwargs)",
            "        if value is None:",
            "            raise ValueError(f\"The value {section}/{key} should be set!\")",
            "        return value",
            "",
            "    @overload  # type: ignore[override]",
            "    def get(self, section: str, key: str, fallback: str = ..., **kwargs) -> str:  # type: ignore[override]",
            "",
            "        ...",
            "",
            "    @overload  # type: ignore[override]",
            "    def get(self, section: str, key: str, **kwargs) -> Optional[str]:  # type: ignore[override]",
            "",
            "        ...",
            "",
            "    def get(self, section: str, key: str, **kwargs) -> Optional[str]:  # type: ignore[override, misc]",
            "        section = str(section).lower()",
            "        key = str(key).lower()",
            "",
            "        deprecated_section, deprecated_key, _ = self.deprecated_options.get(",
            "            (section, key), (None, None, None)",
            "        )",
            "",
            "        option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)",
            "        if option is not None:",
            "            return option",
            "",
            "        option = self._get_option_from_config_file(deprecated_key, deprecated_section, key, kwargs, section)",
            "        if option is not None:",
            "            return option",
            "",
            "        option = self._get_option_from_commands(deprecated_key, deprecated_section, key, section)",
            "        if option is not None:",
            "            return option",
            "",
            "        option = self._get_option_from_secrets(deprecated_key, deprecated_section, key, section)",
            "        if option is not None:",
            "            return option",
            "",
            "        return self._get_option_from_default_config(section, key, **kwargs)",
            "",
            "    def _get_option_from_default_config(self, section: str, key: str, **kwargs) -> Optional[str]:",
            "        # ...then the default config",
            "        if self.airflow_defaults.has_option(section, key) or 'fallback' in kwargs:",
            "            return expand_env_var(self.airflow_defaults.get(section, key, **kwargs))",
            "",
            "        else:",
            "            log.warning(\"section/key [%s/%s] not found in config\", section, key)",
            "",
            "            raise AirflowConfigException(f\"section/key [{section}/{key}] not found in config\")",
            "",
            "    def _get_option_from_secrets(",
            "        self, deprecated_key: Optional[str], deprecated_section: Optional[str], key: str, section: str",
            "    ) -> Optional[str]:",
            "        # ...then from secret backends",
            "        option = self._get_secret_option(section, key)",
            "        if option:",
            "            return option",
            "        if deprecated_section and deprecated_key:",
            "            option = self._get_secret_option(deprecated_section, deprecated_key)",
            "            if option:",
            "                self._warn_deprecate(section, key, deprecated_section, deprecated_key)",
            "                return option",
            "        return None",
            "",
            "    def _get_option_from_commands(",
            "        self, deprecated_key: Optional[str], deprecated_section: Optional[str], key: str, section: str",
            "    ) -> Optional[str]:",
            "        # ...then commands",
            "        option = self._get_cmd_option(section, key)",
            "        if option:",
            "            return option",
            "        if deprecated_section and deprecated_key:",
            "            option = self._get_cmd_option(deprecated_section, deprecated_key)",
            "            if option:",
            "                self._warn_deprecate(section, key, deprecated_section, deprecated_key)",
            "                return option",
            "        return None",
            "",
            "    def _get_option_from_config_file(",
            "        self,",
            "        deprecated_key: Optional[str],",
            "        deprecated_section: Optional[str],",
            "        key: str,",
            "        kwargs: Dict[str, Any],",
            "        section: str,",
            "    ) -> Optional[str]:",
            "        # ...then the config file",
            "        if super().has_option(section, key):",
            "            # Use the parent's methods to get the actual config here to be able to",
            "            # separate the config from default config.",
            "            return expand_env_var(super().get(section, key, **kwargs))",
            "        if deprecated_section and deprecated_key:",
            "            if super().has_option(deprecated_section, deprecated_key):",
            "                self._warn_deprecate(section, key, deprecated_section, deprecated_key)",
            "                return expand_env_var(super().get(deprecated_section, deprecated_key, **kwargs))",
            "        return None",
            "",
            "    def _get_environment_variables(",
            "        self, deprecated_key: Optional[str], deprecated_section: Optional[str], key: str, section: str",
            "    ) -> Optional[str]:",
            "        # first check environment variables",
            "        option = self._get_env_var_option(section, key)",
            "        if option is not None:",
            "            return option",
            "        if deprecated_section and deprecated_key:",
            "            option = self._get_env_var_option(deprecated_section, deprecated_key)",
            "            if option is not None:",
            "                self._warn_deprecate(section, key, deprecated_section, deprecated_key)",
            "                return option",
            "        return None",
            "",
            "    def getboolean(self, section: str, key: str, **kwargs) -> bool:  # type: ignore[override]",
            "        val = str(self.get(section, key, **kwargs)).lower().strip()",
            "        if '#' in val:",
            "            val = val.split('#')[0].strip()",
            "        if val in ('t', 'true', '1'):",
            "            return True",
            "        elif val in ('f', 'false', '0'):",
            "            return False",
            "        else:",
            "            raise AirflowConfigException(",
            "                f'Failed to convert value to bool. Please check \"{key}\" key in \"{section}\" section. '",
            "                f'Current value: \"{val}\".'",
            "            )",
            "",
            "    def getint(self, section: str, key: str, **kwargs) -> int:  # type: ignore[override]",
            "        val = self.get(section, key, **kwargs)",
            "        if val is None:",
            "            raise AirflowConfigException(",
            "                f'Failed to convert value None to int. '",
            "                f'Please check \"{key}\" key in \"{section}\" section is set.'",
            "            )",
            "        try:",
            "            return int(val)",
            "        except ValueError:",
            "            raise AirflowConfigException(",
            "                f'Failed to convert value to int. Please check \"{key}\" key in \"{section}\" section. '",
            "                f'Current value: \"{val}\".'",
            "            )",
            "",
            "    def getfloat(self, section: str, key: str, **kwargs) -> float:  # type: ignore[override]",
            "        val = self.get(section, key, **kwargs)",
            "        if val is None:",
            "            raise AirflowConfigException(",
            "                f'Failed to convert value None to float. '",
            "                f'Please check \"{key}\" key in \"{section}\" section is set.'",
            "            )",
            "        try:",
            "            return float(val)",
            "        except ValueError:",
            "            raise AirflowConfigException(",
            "                f'Failed to convert value to float. Please check \"{key}\" key in \"{section}\" section. '",
            "                f'Current value: \"{val}\".'",
            "            )",
            "",
            "    def getimport(self, section: str, key: str, **kwargs) -> Any:",
            "        \"\"\"",
            "        Reads options, imports the full qualified name, and returns the object.",
            "",
            "        In case of failure, it throws an exception with the key and section names",
            "",
            "        :return: The object or None, if the option is empty",
            "        \"\"\"",
            "        full_qualified_path = conf.get(section=section, key=key, **kwargs)",
            "        if not full_qualified_path:",
            "            return None",
            "",
            "        try:",
            "            return import_string(full_qualified_path)",
            "        except ImportError as e:",
            "            log.error(e)",
            "            raise AirflowConfigException(",
            "                f'The object could not be loaded. Please check \"{key}\" key in \"{section}\" section. '",
            "                f'Current value: \"{full_qualified_path}\".'",
            "            )",
            "",
            "    def getjson(",
            "        self, section: str, key: str, fallback=_UNSET, **kwargs",
            "    ) -> Union[dict, list, str, int, float, None]:",
            "        \"\"\"",
            "        Return a config value parsed from a JSON string.",
            "",
            "        ``fallback`` is *not* JSON parsed but used verbatim when no config value is given.",
            "        \"\"\"",
            "        # get always returns the fallback value as a string, so for this if",
            "        # someone gives us an object we want to keep that",
            "        default = _UNSET",
            "        if fallback is not _UNSET:",
            "            default = fallback",
            "            fallback = _UNSET",
            "",
            "        try:",
            "            data = self.get(section=section, key=key, fallback=fallback, **kwargs)",
            "        except (NoSectionError, NoOptionError):",
            "            return default",
            "",
            "        if not data:",
            "            return default if default is not _UNSET else None",
            "",
            "        try:",
            "            return json.loads(data)",
            "        except JSONDecodeError as e:",
            "            raise AirflowConfigException(f'Unable to parse [{section}] {key!r} as valid json') from e",
            "",
            "    def gettimedelta(",
            "        self, section: str, key: str, fallback: Any = None, **kwargs",
            "    ) -> Optional[datetime.timedelta]:",
            "        \"\"\"",
            "        Gets the config value for the given section and key, and converts it into datetime.timedelta object.",
            "        If the key is missing, then it is considered as `None`.",
            "",
            "        :param section: the section from the config",
            "        :param key: the key defined in the given section",
            "        :param fallback: fallback value when no config value is given, defaults to None",
            "        :raises AirflowConfigException: raised because ValueError or OverflowError",
            "        :return: datetime.timedelta(seconds=<config_value>) or None",
            "        \"\"\"",
            "        val = self.get(section, key, fallback=fallback, **kwargs)",
            "",
            "        if val:",
            "            # the given value must be convertible to integer",
            "            try:",
            "                int_val = int(val)",
            "            except ValueError:",
            "                raise AirflowConfigException(",
            "                    f'Failed to convert value to int. Please check \"{key}\" key in \"{section}\" section. '",
            "                    f'Current value: \"{val}\".'",
            "                )",
            "",
            "            try:",
            "                return datetime.timedelta(seconds=int_val)",
            "            except OverflowError as err:",
            "                raise AirflowConfigException(",
            "                    f'Failed to convert value to timedelta in `seconds`. '",
            "                    f'{err}. '",
            "                    f'Please check \"{key}\" key in \"{section}\" section. Current value: \"{val}\".'",
            "                )",
            "",
            "        return fallback",
            "",
            "    def read(",
            "        self,",
            "        filenames: Union[",
            "            str,",
            "            bytes,",
            "            os.PathLike,",
            "            Iterable[Union[str, bytes, os.PathLike]],",
            "        ],",
            "        encoding=None,",
            "    ):",
            "        super().read(filenames=filenames, encoding=encoding)",
            "",
            "    # The RawConfigParser defines \"Mapping\" from abc.collections is not subscriptable - so we have",
            "    # to use Dict here.",
            "    def read_dict(  # type: ignore[override]",
            "        self, dictionary: Dict[str, Dict[str, Any]], source: str = '<dict>'",
            "    ):",
            "        super().read_dict(dictionary=dictionary, source=source)",
            "",
            "    def has_option(self, section: str, option: str) -> bool:",
            "        try:",
            "            # Using self.get() to avoid reimplementing the priority order",
            "            # of config variables (env, config, cmd, defaults)",
            "            # UNSET to avoid logging a warning about missing values",
            "            self.get(section, option, fallback=_UNSET)",
            "            return True",
            "        except (NoOptionError, NoSectionError):",
            "            return False",
            "",
            "    def remove_option(self, section: str, option: str, remove_default: bool = True):",
            "        \"\"\"",
            "        Remove an option if it exists in config from a file or",
            "        default config. If both of config have the same option, this removes",
            "        the option in both configs unless remove_default=False.",
            "        \"\"\"",
            "        if super().has_option(section, option):",
            "            super().remove_option(section, option)",
            "",
            "        if self.airflow_defaults.has_option(section, option) and remove_default:",
            "            self.airflow_defaults.remove_option(section, option)",
            "",
            "    def getsection(self, section: str) -> Optional[ConfigOptionsDictType]:",
            "        \"\"\"",
            "        Returns the section as a dict. Values are converted to int, float, bool",
            "        as required.",
            "",
            "        :param section: section from the config",
            "        :rtype: dict",
            "        \"\"\"",
            "        if not self.has_section(section) and not self.airflow_defaults.has_section(section):",
            "            return None",
            "        if self.airflow_defaults.has_section(section):",
            "            _section: ConfigOptionsDictType = OrderedDict(self.airflow_defaults.items(section))",
            "        else:",
            "            _section = OrderedDict()",
            "",
            "        if self.has_section(section):",
            "            _section.update(OrderedDict(self.items(section)))",
            "",
            "        section_prefix = self._env_var_name(section, '')",
            "        for env_var in sorted(os.environ.keys()):",
            "            if env_var.startswith(section_prefix):",
            "                key = env_var.replace(section_prefix, '')",
            "                if key.endswith(\"_CMD\"):",
            "                    key = key[:-4]",
            "                key = key.lower()",
            "                _section[key] = self._get_env_var_option(section, key)",
            "",
            "        for key, val in _section.items():",
            "            if val is None:",
            "                raise AirflowConfigException(",
            "                    f'Failed to convert value automatically. '",
            "                    f'Please check \"{key}\" key in \"{section}\" section is set.'",
            "                )",
            "            try:",
            "                _section[key] = int(val)",
            "            except ValueError:",
            "                try:",
            "                    _section[key] = float(val)",
            "                except ValueError:",
            "                    if isinstance(val, str) and val.lower() in ('t', 'true'):",
            "                        _section[key] = True",
            "                    elif isinstance(val, str) and val.lower() in ('f', 'false'):",
            "                        _section[key] = False",
            "        return _section",
            "",
            "    def write(self, fp: IO, space_around_delimiters: bool = True):  # type: ignore[override]",
            "        # This is based on the configparser.RawConfigParser.write method code to add support for",
            "        # reading options from environment variables.",
            "        # Various type ignores below deal with less-than-perfect RawConfigParser superclass typing",
            "        if space_around_delimiters:",
            "            delimiter = f\" {self._delimiters[0]} \"  # type: ignore[attr-defined]",
            "        else:",
            "            delimiter = self._delimiters[0]  # type: ignore[attr-defined]",
            "        if self._defaults:  # type: ignore",
            "            self._write_section(  # type: ignore[attr-defined]",
            "                fp, self.default_section, self._defaults.items(), delimiter  # type: ignore[attr-defined]",
            "            )",
            "        for section in self._sections:  # type: ignore[attr-defined]",
            "            item_section: ConfigOptionsDictType = self.getsection(section)  # type: ignore[assignment]",
            "            self._write_section(fp, section, item_section.items(), delimiter)  # type: ignore[attr-defined]",
            "",
            "    def as_dict(",
            "        self,",
            "        display_source: bool = False,",
            "        display_sensitive: bool = False,",
            "        raw: bool = False,",
            "        include_env: bool = True,",
            "        include_cmds: bool = True,",
            "        include_secret: bool = True,",
            "    ) -> ConfigSourcesType:",
            "        \"\"\"",
            "        Returns the current configuration as an OrderedDict of OrderedDicts.",
            "",
            "        When materializing current configuration Airflow defaults are",
            "        materialized along with user set configs. If any of the `include_*`",
            "        options are False then the result of calling command or secret key",
            "        configs do not override Airflow defaults and instead are passed through.",
            "        In order to then avoid Airflow defaults from overwriting user set",
            "        command or secret key configs we filter out bare sensitive_config_values",
            "        that are set to Airflow defaults when command or secret key configs",
            "        produce different values.",
            "",
            "        :param display_source: If False, the option value is returned. If True,",
            "            a tuple of (option_value, source) is returned. Source is either",
            "            'airflow.cfg', 'default', 'env var', or 'cmd'.",
            "        :param display_sensitive: If True, the values of options set by env",
            "            vars and bash commands will be displayed. If False, those options",
            "            are shown as '< hidden >'",
            "        :param raw: Should the values be output as interpolated values, or the",
            "            \"raw\" form that can be fed back in to ConfigParser",
            "        :param include_env: Should the value of configuration from AIRFLOW__",
            "            environment variables be included or not",
            "        :param include_cmds: Should the result of calling any *_cmd config be",
            "            set (True, default), or should the _cmd options be left as the",
            "            command to run (False)",
            "        :param include_secret: Should the result of calling any *_secret config be",
            "            set (True, default), or should the _secret options be left as the",
            "            path to get the secret from (False)",
            "        :rtype: Dict[str, Dict[str, str]]",
            "        :return: Dictionary, where the key is the name of the section and the content is",
            "            the dictionary with the name of the parameter and its value.",
            "        \"\"\"",
            "        config_sources: ConfigSourcesType = {}",
            "        configs = [",
            "            ('default', self.airflow_defaults),",
            "            ('airflow.cfg', self),",
            "        ]",
            "",
            "        self._replace_config_with_display_sources(",
            "            config_sources,",
            "            configs,",
            "            display_source,",
            "            raw,",
            "            self.deprecated_options,",
            "            include_cmds=include_cmds,",
            "            include_env=include_env,",
            "            include_secret=include_secret,",
            "        )",
            "",
            "        # add env vars and overwrite because they have priority",
            "        if include_env:",
            "            self._include_envs(config_sources, display_sensitive, display_source, raw)",
            "        else:",
            "            self._filter_by_source(config_sources, display_source, self._get_env_var_option)",
            "",
            "        # add bash commands",
            "        if include_cmds:",
            "            self._include_commands(config_sources, display_sensitive, display_source, raw)",
            "        else:",
            "            self._filter_by_source(config_sources, display_source, self._get_cmd_option)",
            "",
            "        # add config from secret backends",
            "        if include_secret:",
            "            self._include_secrets(config_sources, display_sensitive, display_source, raw)",
            "        else:",
            "            self._filter_by_source(config_sources, display_source, self._get_secret_option)",
            "",
            "        return config_sources",
            "",
            "    def _include_secrets(",
            "        self,",
            "        config_sources: ConfigSourcesType,",
            "        display_sensitive: bool,",
            "        display_source: bool,",
            "        raw: bool,",
            "    ):",
            "        for (section, key) in self.sensitive_config_values:",
            "            value: Optional[str] = self._get_secret_option_from_config_sources(config_sources, section, key)",
            "            if value:",
            "                if not display_sensitive:",
            "                    value = '< hidden >'",
            "                if display_source:",
            "                    opt: Union[str, Tuple[str, str]] = (value, 'secret')",
            "                elif raw:",
            "                    opt = value.replace('%', '%%')",
            "                else:",
            "                    opt = value",
            "                config_sources.setdefault(section, OrderedDict()).update({key: opt})",
            "                del config_sources[section][key + '_secret']",
            "",
            "    def _include_commands(",
            "        self,",
            "        config_sources: ConfigSourcesType,",
            "        display_sensitive: bool,",
            "        display_source: bool,",
            "        raw: bool,",
            "    ):",
            "        for (section, key) in self.sensitive_config_values:",
            "            opt = self._get_cmd_option_from_config_sources(config_sources, section, key)",
            "            if not opt:",
            "                continue",
            "            opt_to_set: Union[str, Tuple[str, str], None] = opt",
            "            if not display_sensitive:",
            "                opt_to_set = '< hidden >'",
            "            if display_source:",
            "                opt_to_set = (str(opt_to_set), 'cmd')",
            "            elif raw:",
            "                opt_to_set = str(opt_to_set).replace('%', '%%')",
            "            if opt_to_set is not None:",
            "                dict_to_update: Dict[str, Union[str, Tuple[str, str]]] = {key: opt_to_set}",
            "                config_sources.setdefault(section, OrderedDict()).update(dict_to_update)",
            "                del config_sources[section][key + '_cmd']",
            "",
            "    def _include_envs(",
            "        self,",
            "        config_sources: ConfigSourcesType,",
            "        display_sensitive: bool,",
            "        display_source: bool,",
            "        raw: bool,",
            "    ):",
            "        for env_var in [",
            "            os_environment for os_environment in os.environ if os_environment.startswith(ENV_VAR_PREFIX)",
            "        ]:",
            "            try:",
            "                _, section, key = env_var.split('__', 2)",
            "                opt = self._get_env_var_option(section, key)",
            "            except ValueError:",
            "                continue",
            "            if opt is None:",
            "                log.warning(\"Ignoring unknown env var '%s'\", env_var)",
            "                continue",
            "            if not display_sensitive and env_var != self._env_var_name('core', 'unit_test_mode'):",
            "                opt = '< hidden >'",
            "            elif raw:",
            "                opt = opt.replace('%', '%%')",
            "            if display_source:",
            "                opt = (opt, 'env var')",
            "",
            "            section = section.lower()",
            "            # if we lower key for kubernetes_environment_variables section,",
            "            # then we won't be able to set any Airflow environment",
            "            # variables. Airflow only parse environment variables starts",
            "            # with AIRFLOW_. Therefore, we need to make it a special case.",
            "            if section != 'kubernetes_environment_variables':",
            "                key = key.lower()",
            "            config_sources.setdefault(section, OrderedDict()).update({key: opt})",
            "",
            "    def _filter_by_source(",
            "        self,",
            "        config_sources: ConfigSourcesType,",
            "        display_source: bool,",
            "        getter_func,",
            "    ):",
            "        \"\"\"",
            "        Deletes default configs from current configuration (an OrderedDict of",
            "        OrderedDicts) if it would conflict with special sensitive_config_values.",
            "",
            "        This is necessary because bare configs take precedence over the command",
            "        or secret key equivalents so if the current running config is",
            "        materialized with Airflow defaults they in turn override user set",
            "        command or secret key configs.",
            "",
            "        :param config_sources: The current configuration to operate on",
            "        :param display_source: If False, configuration options contain raw",
            "            values. If True, options are a tuple of (option_value, source).",
            "            Source is either 'airflow.cfg', 'default', 'env var', or 'cmd'.",
            "        :param getter_func: A callback function that gets the user configured",
            "            override value for a particular sensitive_config_values config.",
            "        :rtype: None",
            "        :return: None, the given config_sources is filtered if necessary,",
            "            otherwise untouched.",
            "        \"\"\"",
            "        for (section, key) in self.sensitive_config_values:",
            "            # Don't bother if we don't have section / key",
            "            if section not in config_sources or key not in config_sources[section]:",
            "                continue",
            "            # Check that there is something to override defaults",
            "            try:",
            "                getter_opt = getter_func(section, key)",
            "            except ValueError:",
            "                continue",
            "            if not getter_opt:",
            "                continue",
            "            # Check to see that there is a default value",
            "            if not self.airflow_defaults.has_option(section, key):",
            "                continue",
            "            # Check to see if bare setting is the same as defaults",
            "            if display_source:",
            "                # when display_source = true, we know that the config_sources contains tuple",
            "                opt, source = config_sources[section][key]  # type: ignore",
            "            else:",
            "                opt = config_sources[section][key]",
            "            if opt == self.airflow_defaults.get(section, key):",
            "                del config_sources[section][key]",
            "",
            "    @staticmethod",
            "    def _replace_config_with_display_sources(",
            "        config_sources: ConfigSourcesType,",
            "        configs: Iterable[Tuple[str, ConfigParser]],",
            "        display_source: bool,",
            "        raw: bool,",
            "        deprecated_options: Dict[Tuple[str, str], Tuple[str, str, str]],",
            "        include_env: bool,",
            "        include_cmds: bool,",
            "        include_secret: bool,",
            "    ):",
            "        for (source_name, config) in configs:",
            "            for section in config.sections():",
            "                AirflowConfigParser._replace_section_config_with_display_sources(",
            "                    config,",
            "                    config_sources,",
            "                    display_source,",
            "                    raw,",
            "                    section,",
            "                    source_name,",
            "                    deprecated_options,",
            "                    configs,",
            "                    include_env=include_env,",
            "                    include_cmds=include_cmds,",
            "                    include_secret=include_secret,",
            "                )",
            "",
            "    @staticmethod",
            "    def _deprecated_value_is_set_in_config(",
            "        deprecated_section: str,",
            "        deprecated_key: str,",
            "        configs: Iterable[Tuple[str, ConfigParser]],",
            "    ) -> bool:",
            "        for config_type, config in configs:",
            "            if config_type == 'default':",
            "                continue",
            "            try:",
            "                deprecated_section_array = config.items(section=deprecated_section, raw=True)",
            "                for (key_candidate, _) in deprecated_section_array:",
            "                    if key_candidate == deprecated_key:",
            "                        return True",
            "            except NoSectionError:",
            "                pass",
            "        return False",
            "",
            "    @staticmethod",
            "    def _deprecated_variable_is_set(deprecated_section: str, deprecated_key: str) -> bool:",
            "        return (",
            "            os.environ.get(f'{ENV_VAR_PREFIX}{deprecated_section.upper()}__{deprecated_key.upper()}')",
            "            is not None",
            "        )",
            "",
            "    @staticmethod",
            "    def _deprecated_command_is_set_in_config(",
            "        deprecated_section: str, deprecated_key: str, configs: Iterable[Tuple[str, ConfigParser]]",
            "    ) -> bool:",
            "        return AirflowConfigParser._deprecated_value_is_set_in_config(",
            "            deprecated_section=deprecated_section, deprecated_key=deprecated_key + \"_cmd\", configs=configs",
            "        )",
            "",
            "    @staticmethod",
            "    def _deprecated_variable_command_is_set(deprecated_section: str, deprecated_key: str) -> bool:",
            "        return (",
            "            os.environ.get(f'{ENV_VAR_PREFIX}{deprecated_section.upper()}__{deprecated_key.upper()}_CMD')",
            "            is not None",
            "        )",
            "",
            "    @staticmethod",
            "    def _deprecated_secret_is_set_in_config(",
            "        deprecated_section: str, deprecated_key: str, configs: Iterable[Tuple[str, ConfigParser]]",
            "    ) -> bool:",
            "        return AirflowConfigParser._deprecated_value_is_set_in_config(",
            "            deprecated_section=deprecated_section, deprecated_key=deprecated_key + \"_secret\", configs=configs",
            "        )",
            "",
            "    @staticmethod",
            "    def _deprecated_variable_secret_is_set(deprecated_section: str, deprecated_key: str) -> bool:",
            "        return (",
            "            os.environ.get(f'{ENV_VAR_PREFIX}{deprecated_section.upper()}__{deprecated_key.upper()}_SECRET')",
            "            is not None",
            "        )",
            "",
            "    @staticmethod",
            "    def _replace_section_config_with_display_sources(",
            "        config: ConfigParser,",
            "        config_sources: ConfigSourcesType,",
            "        display_source: bool,",
            "        raw: bool,",
            "        section: str,",
            "        source_name: str,",
            "        deprecated_options: Dict[Tuple[str, str], Tuple[str, str, str]],",
            "        configs: Iterable[Tuple[str, ConfigParser]],",
            "        include_env: bool,",
            "        include_cmds: bool,",
            "        include_secret: bool,",
            "    ):",
            "        sect = config_sources.setdefault(section, OrderedDict())",
            "        for (k, val) in config.items(section=section, raw=raw):",
            "            deprecated_section, deprecated_key, _ = deprecated_options.get((section, k), (None, None, None))",
            "            if deprecated_section and deprecated_key:",
            "                if source_name == 'default':",
            "                    # If deprecated entry has some non-default value set for any of the sources requested,",
            "                    # We should NOT set default for the new entry (because it will override anything",
            "                    # coming from the deprecated ones)",
            "                    if AirflowConfigParser._deprecated_value_is_set_in_config(",
            "                        deprecated_section, deprecated_key, configs",
            "                    ):",
            "                        continue",
            "                    if include_env and AirflowConfigParser._deprecated_variable_is_set(",
            "                        deprecated_section, deprecated_key",
            "                    ):",
            "                        continue",
            "                    if include_cmds and (",
            "                        AirflowConfigParser._deprecated_variable_command_is_set(",
            "                            deprecated_section, deprecated_key",
            "                        )",
            "                        or AirflowConfigParser._deprecated_command_is_set_in_config(",
            "                            deprecated_section, deprecated_key, configs",
            "                        )",
            "                    ):",
            "                        continue",
            "                    if include_secret and (",
            "                        AirflowConfigParser._deprecated_variable_secret_is_set(",
            "                            deprecated_section, deprecated_key",
            "                        )",
            "                        or AirflowConfigParser._deprecated_secret_is_set_in_config(",
            "                            deprecated_section, deprecated_key, configs",
            "                        )",
            "                    ):",
            "                        continue",
            "            if display_source:",
            "                sect[k] = (val, source_name)",
            "            else:",
            "                sect[k] = val",
            "",
            "    def load_test_config(self):",
            "        \"\"\"",
            "        Load the unit test configuration.",
            "",
            "        Note: this is not reversible.",
            "        \"\"\"",
            "        # remove all sections, falling back to defaults",
            "        for section in self.sections():",
            "            self.remove_section(section)",
            "",
            "        # then read test config",
            "",
            "        path = _default_config_file_path('default_test.cfg')",
            "        log.info(\"Reading default test configuration from %s\", path)",
            "        self.read_string(_parameterized_config_from_template('default_test.cfg'))",
            "        # then read any \"custom\" test settings",
            "        log.info(\"Reading test configuration from %s\", TEST_CONFIG_FILE)",
            "        self.read(TEST_CONFIG_FILE)",
            "",
            "    @staticmethod",
            "    def _warn_deprecate(section: str, key: str, deprecated_section: str, deprecated_name: str):",
            "        if section == deprecated_section:",
            "            warnings.warn(",
            "                f'The {deprecated_name} option in [{section}] has been renamed to {key} - '",
            "                f'the old setting has been used, but please update your config.',",
            "                DeprecationWarning,",
            "                stacklevel=3,",
            "            )",
            "        else:",
            "            warnings.warn(",
            "                f'The {deprecated_name} option in [{deprecated_section}] has been moved to the {key} option '",
            "                f'in [{section}] - the old setting has been used, but please update your config.',",
            "                DeprecationWarning,",
            "                stacklevel=3,",
            "            )",
            "",
            "    def __getstate__(self):",
            "        return {",
            "            name: getattr(self, name)",
            "            for name in [",
            "                '_sections',",
            "                'is_validated',",
            "                'airflow_defaults',",
            "            ]",
            "        }",
            "",
            "    def __setstate__(self, state):",
            "        self.__init__()",
            "        config = state.pop('_sections')",
            "        self.read_dict(config)",
            "        self.__dict__.update(state)",
            "",
            "",
            "def get_airflow_home() -> str:",
            "    \"\"\"Get path to Airflow Home\"\"\"",
            "    return expand_env_var(os.environ.get('AIRFLOW_HOME', '~/airflow'))",
            "",
            "",
            "def get_airflow_config(airflow_home) -> str:",
            "    \"\"\"Get Path to airflow.cfg path\"\"\"",
            "    airflow_config_var = os.environ.get('AIRFLOW_CONFIG')",
            "    if airflow_config_var is None:",
            "        return os.path.join(airflow_home, 'airflow.cfg')",
            "    return expand_env_var(airflow_config_var)",
            "",
            "",
            "def _parameterized_config_from_template(filename) -> str:",
            "    TEMPLATE_START = '# ----------------------- TEMPLATE BEGINS HERE -----------------------\\n'",
            "",
            "    path = _default_config_file_path(filename)",
            "    with open(path) as fh:",
            "        for line in fh:",
            "            if line != TEMPLATE_START:",
            "                continue",
            "            return parameterized_config(fh.read().strip())",
            "    raise RuntimeError(f\"Template marker not found in {path!r}\")",
            "",
            "",
            "def parameterized_config(template) -> str:",
            "    \"\"\"",
            "    Generates a configuration from the provided template + variables defined in",
            "    current scope",
            "",
            "    :param template: a config content templated with {{variables}}",
            "    \"\"\"",
            "    all_vars = {k: v for d in [globals(), locals()] for k, v in d.items()}",
            "    return template.format(**all_vars)",
            "",
            "",
            "def get_airflow_test_config(airflow_home) -> str:",
            "    \"\"\"Get path to unittests.cfg\"\"\"",
            "    if 'AIRFLOW_TEST_CONFIG' not in os.environ:",
            "        return os.path.join(airflow_home, 'unittests.cfg')",
            "    # It will never return None",
            "    return expand_env_var(os.environ['AIRFLOW_TEST_CONFIG'])  # type: ignore[return-value]",
            "",
            "",
            "def _generate_fernet_key() -> str:",
            "    from cryptography.fernet import Fernet",
            "",
            "    return Fernet.generate_key().decode()",
            "",
            "",
            "def initialize_config() -> AirflowConfigParser:",
            "    \"\"\"",
            "    Load the Airflow config files.",
            "",
            "    Called for you automatically as part of the Airflow boot process.",
            "    \"\"\"",
            "    global FERNET_KEY, AIRFLOW_HOME",
            "",
            "    default_config = _parameterized_config_from_template('default_airflow.cfg')",
            "",
            "    local_conf = AirflowConfigParser(default_config=default_config)",
            "",
            "    if local_conf.getboolean('core', 'unit_test_mode'):",
            "        # Load test config only",
            "        if not os.path.isfile(TEST_CONFIG_FILE):",
            "            from cryptography.fernet import Fernet",
            "",
            "            log.info('Creating new Airflow config file for unit tests in: %s', TEST_CONFIG_FILE)",
            "            pathlib.Path(AIRFLOW_HOME).mkdir(parents=True, exist_ok=True)",
            "",
            "            FERNET_KEY = Fernet.generate_key().decode()",
            "",
            "            with open(TEST_CONFIG_FILE, 'w') as file:",
            "                cfg = _parameterized_config_from_template('default_test.cfg')",
            "                file.write(cfg)",
            "",
            "        local_conf.load_test_config()",
            "    else:",
            "        # Load normal config",
            "        if not os.path.isfile(AIRFLOW_CONFIG):",
            "            from cryptography.fernet import Fernet",
            "",
            "            log.info('Creating new Airflow config file in: %s', AIRFLOW_CONFIG)",
            "            pathlib.Path(AIRFLOW_HOME).mkdir(parents=True, exist_ok=True)",
            "",
            "            FERNET_KEY = Fernet.generate_key().decode()",
            "",
            "            with open(AIRFLOW_CONFIG, 'w') as file:",
            "                file.write(default_config)",
            "",
            "        log.info(\"Reading the config from %s\", AIRFLOW_CONFIG)",
            "",
            "        local_conf.read(AIRFLOW_CONFIG)",
            "",
            "        if local_conf.has_option('core', 'AIRFLOW_HOME'):",
            "            msg = (",
            "                'Specifying both AIRFLOW_HOME environment variable and airflow_home '",
            "                'in the config file is deprecated. Please use only the AIRFLOW_HOME '",
            "                'environment variable and remove the config file entry.'",
            "            )",
            "            if 'AIRFLOW_HOME' in os.environ:",
            "                warnings.warn(msg, category=DeprecationWarning)",
            "            elif local_conf.get('core', 'airflow_home') == AIRFLOW_HOME:",
            "                warnings.warn(",
            "                    'Specifying airflow_home in the config file is deprecated. As you '",
            "                    'have left it at the default value you should remove the setting '",
            "                    'from your airflow.cfg and suffer no change in behaviour.',",
            "                    category=DeprecationWarning,",
            "                )",
            "            else:",
            "                # there",
            "                AIRFLOW_HOME = local_conf.get('core', 'airflow_home')  # type: ignore[assignment]",
            "                warnings.warn(msg, category=DeprecationWarning)",
            "",
            "        # They _might_ have set unit_test_mode in the airflow.cfg, we still",
            "        # want to respect that and then load the unittests.cfg",
            "        if local_conf.getboolean('core', 'unit_test_mode'):",
            "            local_conf.load_test_config()",
            "",
            "    # Make it no longer a proxy variable, just set it to an actual string",
            "    global WEBSERVER_CONFIG",
            "    WEBSERVER_CONFIG = AIRFLOW_HOME + '/webserver_config.py'",
            "",
            "    if not os.path.isfile(WEBSERVER_CONFIG):",
            "        import shutil",
            "",
            "        log.info('Creating new FAB webserver config file in: %s', WEBSERVER_CONFIG)",
            "        shutil.copy(_default_config_file_path('default_webserver_config.py'), WEBSERVER_CONFIG)",
            "    return local_conf",
            "",
            "",
            "# Historical convenience functions to access config entries",
            "def load_test_config():",
            "    \"\"\"Historical load_test_config\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'load_test_config' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.load_test_config'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    conf.load_test_config()",
            "",
            "",
            "def get(*args, **kwargs) -> Optional[ConfigType]:",
            "    \"\"\"Historical get\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'get' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.get'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    return conf.get(*args, **kwargs)",
            "",
            "",
            "def getboolean(*args, **kwargs) -> bool:",
            "    \"\"\"Historical getboolean\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'getboolean' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.getboolean'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    return conf.getboolean(*args, **kwargs)",
            "",
            "",
            "def getfloat(*args, **kwargs) -> float:",
            "    \"\"\"Historical getfloat\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'getfloat' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.getfloat'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    return conf.getfloat(*args, **kwargs)",
            "",
            "",
            "def getint(*args, **kwargs) -> int:",
            "    \"\"\"Historical getint\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'getint' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.getint'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    return conf.getint(*args, **kwargs)",
            "",
            "",
            "def getsection(*args, **kwargs) -> Optional[ConfigOptionsDictType]:",
            "    \"\"\"Historical getsection\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'getsection' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.getsection'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    return conf.getsection(*args, **kwargs)",
            "",
            "",
            "def has_option(*args, **kwargs) -> bool:",
            "    \"\"\"Historical has_option\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'has_option' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.has_option'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    return conf.has_option(*args, **kwargs)",
            "",
            "",
            "def remove_option(*args, **kwargs) -> bool:",
            "    \"\"\"Historical remove_option\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'remove_option' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.remove_option'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    return conf.remove_option(*args, **kwargs)",
            "",
            "",
            "def as_dict(*args, **kwargs) -> ConfigSourcesType:",
            "    \"\"\"Historical as_dict\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'as_dict' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.as_dict'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    return conf.as_dict(*args, **kwargs)",
            "",
            "",
            "def set(*args, **kwargs) -> None:",
            "    \"\"\"Historical set\"\"\"",
            "    warnings.warn(",
            "        \"Accessing configuration method 'set' directly from the configuration module is \"",
            "        \"deprecated. Please access the configuration from the 'configuration.conf' object via \"",
            "        \"'conf.set'\",",
            "        DeprecationWarning,",
            "        stacklevel=2,",
            "    )",
            "    conf.set(*args, **kwargs)",
            "",
            "",
            "def ensure_secrets_loaded() -> List[BaseSecretsBackend]:",
            "    \"\"\"",
            "    Ensure that all secrets backends are loaded.",
            "    If the secrets_backend_list contains only 2 default backends, reload it.",
            "    \"\"\"",
            "    # Check if the secrets_backend_list contains only 2 default backends",
            "    if len(secrets_backend_list) == 2:",
            "        return initialize_secrets_backends()",
            "    return secrets_backend_list",
            "",
            "",
            "def get_custom_secret_backend() -> Optional[BaseSecretsBackend]:",
            "    \"\"\"Get Secret Backend if defined in airflow.cfg\"\"\"",
            "    secrets_backend_cls = conf.getimport(section='secrets', key='backend')",
            "",
            "    if secrets_backend_cls:",
            "        try:",
            "            backends: Any = conf.get(section='secrets', key='backend_kwargs', fallback='{}')",
            "            alternative_secrets_config_dict = json.loads(backends)",
            "        except JSONDecodeError:",
            "            alternative_secrets_config_dict = {}",
            "",
            "        return secrets_backend_cls(**alternative_secrets_config_dict)",
            "    return None",
            "",
            "",
            "def initialize_secrets_backends() -> List[BaseSecretsBackend]:",
            "    \"\"\"",
            "    * import secrets backend classes",
            "    * instantiate them and return them in a list",
            "    \"\"\"",
            "    backend_list = []",
            "",
            "    custom_secret_backend = get_custom_secret_backend()",
            "",
            "    if custom_secret_backend is not None:",
            "        backend_list.append(custom_secret_backend)",
            "",
            "    for class_name in DEFAULT_SECRETS_SEARCH_PATH:",
            "        secrets_backend_cls = import_string(class_name)",
            "        backend_list.append(secrets_backend_cls())",
            "",
            "    return backend_list",
            "",
            "",
            "@functools.lru_cache(maxsize=None)",
            "def _DEFAULT_CONFIG() -> str:",
            "    path = _default_config_file_path('default_airflow.cfg')",
            "    with open(path) as fh:",
            "        return fh.read()",
            "",
            "",
            "@functools.lru_cache(maxsize=None)",
            "def _TEST_CONFIG() -> str:",
            "    path = _default_config_file_path('default_test.cfg')",
            "    with open(path) as fh:",
            "        return fh.read()",
            "",
            "",
            "_deprecated = {",
            "    'DEFAULT_CONFIG': _DEFAULT_CONFIG,",
            "    'TEST_CONFIG': _TEST_CONFIG,",
            "    'TEST_CONFIG_FILE_PATH': functools.partial(_default_config_file_path, 'default_test.cfg'),",
            "    'DEFAULT_CONFIG_FILE_PATH': functools.partial(_default_config_file_path, 'default_airflow.cfg'),",
            "}",
            "",
            "",
            "def __getattr__(name):",
            "    if name in _deprecated:",
            "        warnings.warn(",
            "            f\"{__name__}.{name} is deprecated and will be removed in future\",",
            "            DeprecationWarning,",
            "            stacklevel=2,",
            "        )",
            "        return _deprecated[name]()",
            "    raise AttributeError(f\"module {__name__} has no attribute {name}\")",
            "",
            "",
            "# Setting AIRFLOW_HOME and AIRFLOW_CONFIG from environment variables, using",
            "# \"~/airflow\" and \"$AIRFLOW_HOME/airflow.cfg\" respectively as defaults.",
            "AIRFLOW_HOME = get_airflow_home()",
            "AIRFLOW_CONFIG = get_airflow_config(AIRFLOW_HOME)",
            "",
            "",
            "# Set up dags folder for unit tests",
            "# this directory won't exist if users install via pip",
            "_TEST_DAGS_FOLDER = os.path.join(",
            "    os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'tests', 'dags'",
            ")",
            "if os.path.exists(_TEST_DAGS_FOLDER):",
            "    TEST_DAGS_FOLDER = _TEST_DAGS_FOLDER",
            "else:",
            "    TEST_DAGS_FOLDER = os.path.join(AIRFLOW_HOME, 'dags')",
            "",
            "# Set up plugins folder for unit tests",
            "_TEST_PLUGINS_FOLDER = os.path.join(",
            "    os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'tests', 'plugins'",
            ")",
            "if os.path.exists(_TEST_PLUGINS_FOLDER):",
            "    TEST_PLUGINS_FOLDER = _TEST_PLUGINS_FOLDER",
            "else:",
            "    TEST_PLUGINS_FOLDER = os.path.join(AIRFLOW_HOME, 'plugins')",
            "",
            "",
            "TEST_CONFIG_FILE = get_airflow_test_config(AIRFLOW_HOME)",
            "",
            "SECRET_KEY = b64encode(os.urandom(16)).decode('utf-8')",
            "FERNET_KEY = ''  # Set only if needed when generating a new file",
            "WEBSERVER_CONFIG = ''  # Set by initialize_config",
            "",
            "conf = initialize_config()",
            "secrets_backend_list = initialize_secrets_backends()",
            "conf.validate()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "airflow/settings.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 640,
                "afterPatchRowNumber": 640,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 641,
                "afterPatchRowNumber": 641,
                "PatchRowcode": " # Prefix used to identify tables holding data moved during migration."
            },
            "2": {
                "beforePatchRowNumber": 642,
                "afterPatchRowNumber": 642,
                "PatchRowcode": " AIRFLOW_MOVED_TABLE_PREFIX = \"_airflow_moved\""
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 643,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 644,
                "PatchRowcode": "+DAEMON_UMASK: str = conf.get('core', 'daemon_umask', fallback='0o077')"
            }
        },
        "frontPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import atexit",
            "import functools",
            "import json",
            "import logging",
            "import os",
            "import sys",
            "import warnings",
            "from typing import TYPE_CHECKING, Callable, List, Optional, Union",
            "",
            "import pendulum",
            "import sqlalchemy",
            "from sqlalchemy import create_engine, exc",
            "from sqlalchemy.engine import Engine",
            "from sqlalchemy.orm import scoped_session, sessionmaker",
            "from sqlalchemy.orm.session import Session as SASession",
            "from sqlalchemy.pool import NullPool",
            "",
            "from airflow.configuration import AIRFLOW_HOME, WEBSERVER_CONFIG, conf  # NOQA F401",
            "from airflow.executors import executor_constants",
            "from airflow.logging_config import configure_logging",
            "from airflow.utils.orm_event_handlers import setup_event_handlers",
            "",
            "if TYPE_CHECKING:",
            "    from airflow.www.utils import UIAlert",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "TIMEZONE = pendulum.tz.timezone('UTC')",
            "try:",
            "    tz = conf.get_mandatory_value(\"core\", \"default_timezone\")",
            "    if tz == \"system\":",
            "        TIMEZONE = pendulum.tz.local_timezone()",
            "    else:",
            "        TIMEZONE = pendulum.tz.timezone(tz)",
            "except Exception:",
            "    pass",
            "log.info(\"Configured default timezone %s\", TIMEZONE)",
            "",
            "",
            "HEADER = '\\n'.join(",
            "    [",
            "        r'  ____________       _____________',",
            "        r' ____    |__( )_________  __/__  /________      __',",
            "        r'____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /',",
            "        r'___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /',",
            "        r' _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/',",
            "    ]",
            ")",
            "",
            "LOGGING_LEVEL = logging.INFO",
            "",
            "# the prefix to append to gunicorn worker processes after init",
            "GUNICORN_WORKER_READY_PREFIX = \"[ready] \"",
            "",
            "LOG_FORMAT = conf.get('logging', 'log_format')",
            "SIMPLE_LOG_FORMAT = conf.get('logging', 'simple_log_format')",
            "",
            "SQL_ALCHEMY_CONN: Optional[str] = None",
            "PLUGINS_FOLDER: Optional[str] = None",
            "LOGGING_CLASS_PATH: Optional[str] = None",
            "DONOT_MODIFY_HANDLERS: Optional[bool] = None",
            "DAGS_FOLDER: str = os.path.expanduser(conf.get_mandatory_value('core', 'DAGS_FOLDER'))",
            "",
            "engine: Engine",
            "Session: Callable[..., SASession]",
            "",
            "# The JSON library to use for DAG Serialization and De-Serialization",
            "json = json",
            "",
            "# Dictionary containing State and colors associated to each state to",
            "# display on the Webserver",
            "STATE_COLORS = {",
            "    \"deferred\": \"mediumpurple\",",
            "    \"failed\": \"red\",",
            "    \"queued\": \"gray\",",
            "    \"running\": \"lime\",",
            "    \"scheduled\": \"tan\",",
            "    \"skipped\": \"hotpink\",",
            "    \"success\": \"green\",",
            "    \"up_for_reschedule\": \"turquoise\",",
            "    \"up_for_retry\": \"gold\",",
            "    \"upstream_failed\": \"orange\",",
            "}",
            "",
            "",
            "@functools.lru_cache(maxsize=None)",
            "def _get_rich_console(file):",
            "    # Delay imports until we need it",
            "    import rich.console",
            "",
            "    return rich.console.Console(file=file)",
            "",
            "",
            "def custom_show_warning(message, category, filename, lineno, file=None, line=None):",
            "    \"\"\"Custom function to print rich and visible warnings\"\"\"",
            "    # Delay imports until we need it",
            "    from rich.markup import escape",
            "",
            "    msg = f\"[bold]{line}\" if line else f\"[bold][yellow]{filename}:{lineno}\"",
            "    msg += f\" {category.__name__}[/bold]: {escape(str(message))}[/yellow]\"",
            "    write_console = _get_rich_console(file or sys.stderr)",
            "    write_console.print(msg, soft_wrap=True)",
            "",
            "",
            "def replace_showwarning(replacement):",
            "    \"\"\"Replace ``warnings.showwarning``, returning the original.",
            "",
            "    This is useful since we want to \"reset\" the ``showwarning`` hook on exit to",
            "    avoid lazy-loading issues. If a warning is emitted after Python cleaned up",
            "    the import system, we would no longer be able to import ``rich``.",
            "    \"\"\"",
            "    original = warnings.showwarning",
            "    warnings.showwarning = replacement",
            "    return original",
            "",
            "",
            "original_show_warning = replace_showwarning(custom_show_warning)",
            "atexit.register(functools.partial(replace_showwarning, original_show_warning))",
            "",
            "",
            "def task_policy(task) -> None:",
            "    \"\"\"",
            "    This policy setting allows altering tasks after they are loaded in",
            "    the DagBag. It allows administrator to rewire some task's parameters.",
            "    Alternatively you can raise ``AirflowClusterPolicyViolation`` exception",
            "    to stop DAG from being executed.",
            "",
            "    To define policy, add a ``airflow_local_settings`` module",
            "    to your PYTHONPATH that defines this ``task_policy`` function.",
            "",
            "    Here are a few examples of how this can be useful:",
            "",
            "    * You could enforce a specific queue (say the ``spark`` queue)",
            "        for tasks using the ``SparkOperator`` to make sure that these",
            "        tasks get wired to the right workers",
            "    * You could enforce a task timeout policy, making sure that no tasks run",
            "        for more than 48 hours",
            "",
            "    :param task: task to be mutated",
            "    \"\"\"",
            "",
            "",
            "def dag_policy(dag) -> None:",
            "    \"\"\"",
            "    This policy setting allows altering DAGs after they are loaded in",
            "    the DagBag. It allows administrator to rewire some DAG's parameters.",
            "    Alternatively you can raise ``AirflowClusterPolicyViolation`` exception",
            "    to stop DAG from being executed.",
            "",
            "    To define policy, add a ``airflow_local_settings`` module",
            "    to your PYTHONPATH that defines this ``dag_policy`` function.",
            "",
            "    Here are a few examples of how this can be useful:",
            "",
            "    * You could enforce default user for DAGs",
            "    * Check if every DAG has configured tags",
            "",
            "    :param dag: dag to be mutated",
            "    \"\"\"",
            "",
            "",
            "def task_instance_mutation_hook(task_instance):",
            "    \"\"\"",
            "    This setting allows altering task instances before they are queued by",
            "    the Airflow scheduler.",
            "",
            "    To define task_instance_mutation_hook, add a ``airflow_local_settings`` module",
            "    to your PYTHONPATH that defines this ``task_instance_mutation_hook`` function.",
            "",
            "    This could be used, for instance, to modify the task instance during retries.",
            "",
            "    :param task_instance: task instance to be mutated",
            "    \"\"\"",
            "",
            "",
            "task_instance_mutation_hook.is_noop = True  # type: ignore",
            "",
            "",
            "def pod_mutation_hook(pod):",
            "    \"\"\"",
            "    This setting allows altering ``kubernetes.client.models.V1Pod`` object",
            "    before they are passed to the Kubernetes client for scheduling.",
            "",
            "    To define a pod mutation hook, add a ``airflow_local_settings`` module",
            "    to your PYTHONPATH that defines this ``pod_mutation_hook`` function.",
            "    It receives a ``Pod`` object and can alter it where needed.",
            "",
            "    This could be used, for instance, to add sidecar or init containers",
            "    to every worker pod launched by KubernetesExecutor or KubernetesPodOperator.",
            "    \"\"\"",
            "",
            "",
            "def get_airflow_context_vars(context):",
            "    \"\"\"",
            "    This setting allows getting the airflow context vars, which are key value pairs.",
            "    They are then injected to default airflow context vars, which in the end are",
            "    available as environment variables when running tasks",
            "    dag_id, task_id, execution_date, dag_run_id, try_number are reserved keys.",
            "    To define it, add a ``airflow_local_settings`` module",
            "    to your PYTHONPATH that defines this ``get_airflow_context_vars`` function.",
            "",
            "    :param context: The context for the task_instance of interest.",
            "    \"\"\"",
            "    return {}",
            "",
            "",
            "def get_dagbag_import_timeout(dag_file_path: str) -> Union[int, float]:",
            "    \"\"\"",
            "    This setting allows for dynamic control of the DAG file parsing timeout based on the DAG file path.",
            "",
            "    It is useful when there are a few DAG files requiring longer parsing times, while others do not.",
            "    You can control them separately instead of having one value for all DAG files.",
            "",
            "    If the return value is less than or equal to 0, it means no timeout during the DAG parsing.",
            "    \"\"\"",
            "    return conf.getfloat('core', 'DAGBAG_IMPORT_TIMEOUT')",
            "",
            "",
            "def configure_vars():",
            "    \"\"\"Configure Global Variables from airflow.cfg\"\"\"",
            "    global SQL_ALCHEMY_CONN",
            "    global DAGS_FOLDER",
            "    global PLUGINS_FOLDER",
            "    global DONOT_MODIFY_HANDLERS",
            "    SQL_ALCHEMY_CONN = conf.get('database', 'SQL_ALCHEMY_CONN')",
            "    DAGS_FOLDER = os.path.expanduser(conf.get('core', 'DAGS_FOLDER'))",
            "",
            "    PLUGINS_FOLDER = conf.get('core', 'plugins_folder', fallback=os.path.join(AIRFLOW_HOME, 'plugins'))",
            "",
            "    # If donot_modify_handlers=True, we do not modify logging handlers in task_run command",
            "    # If the flag is set to False, we remove all handlers from the root logger",
            "    # and add all handlers from 'airflow.task' logger to the root Logger. This is done",
            "    # to get all the logs from the print & log statements in the DAG files before a task is run",
            "    # The handlers are restored after the task completes execution.",
            "    DONOT_MODIFY_HANDLERS = conf.getboolean('logging', 'donot_modify_handlers', fallback=False)",
            "",
            "",
            "def configure_orm(disable_connection_pool=False):",
            "    \"\"\"Configure ORM using SQLAlchemy\"\"\"",
            "    from airflow.utils.log.secrets_masker import mask_secret",
            "",
            "    log.debug(\"Setting up DB connection pool (PID %s)\", os.getpid())",
            "    global engine",
            "    global Session",
            "    engine_args = prepare_engine_args(disable_connection_pool)",
            "",
            "    if conf.has_option('database', 'sql_alchemy_connect_args'):",
            "        connect_args = conf.getimport('database', 'sql_alchemy_connect_args')",
            "    else:",
            "        connect_args = {}",
            "",
            "    engine = create_engine(SQL_ALCHEMY_CONN, connect_args=connect_args, **engine_args)",
            "",
            "    mask_secret(engine.url.password)",
            "",
            "    setup_event_handlers(engine)",
            "",
            "    Session = scoped_session(",
            "        sessionmaker(",
            "            autocommit=False,",
            "            autoflush=False,",
            "            bind=engine,",
            "            expire_on_commit=False,",
            "        )",
            "    )",
            "    if engine.dialect.name == 'mssql':",
            "        session = Session()",
            "        try:",
            "            result = session.execute(",
            "                sqlalchemy.text(",
            "                    'SELECT is_read_committed_snapshot_on FROM sys.databases WHERE name=:database_name'",
            "                ),",
            "                params={\"database_name\": engine.url.database},",
            "            )",
            "            data = result.fetchone()[0]",
            "            if data != 1:",
            "                log.critical(\"MSSQL database MUST have READ_COMMITTED_SNAPSHOT enabled.\")",
            "                log.critical(f\"The database {engine.url.database} has it disabled.\")",
            "                log.critical(\"This will cause random deadlocks, Refusing to start.\")",
            "                log.critical(",
            "                    \"See https://airflow.apache.org/docs/apache-airflow/stable/howto/\"",
            "                    \"set-up-database.html#setting-up-a-mssql-database\"",
            "                )",
            "                raise Exception(\"MSSQL database MUST have READ_COMMITTED_SNAPSHOT enabled.\")",
            "        finally:",
            "            session.close()",
            "",
            "",
            "DEFAULT_ENGINE_ARGS = {",
            "    'postgresql': {",
            "        'executemany_mode': 'values',",
            "        'executemany_values_page_size': 10000,",
            "        'executemany_batch_page_size': 2000,",
            "    },",
            "}",
            "",
            "",
            "def prepare_engine_args(disable_connection_pool=False):",
            "    \"\"\"Prepare SQLAlchemy engine args\"\"\"",
            "    default_args = {}",
            "    for dialect, default in DEFAULT_ENGINE_ARGS.items():",
            "        if SQL_ALCHEMY_CONN.startswith(dialect):",
            "            default_args = default.copy()",
            "            break",
            "",
            "    engine_args: dict = conf.getjson(",
            "        'database', 'sql_alchemy_engine_args', fallback=default_args",
            "    )  # type: ignore",
            "",
            "    if disable_connection_pool or not conf.getboolean('database', 'SQL_ALCHEMY_POOL_ENABLED'):",
            "        engine_args['poolclass'] = NullPool",
            "        log.debug(\"settings.prepare_engine_args(): Using NullPool\")",
            "    elif not SQL_ALCHEMY_CONN.startswith('sqlite'):",
            "        # Pool size engine args not supported by sqlite.",
            "        # If no config value is defined for the pool size, select a reasonable value.",
            "        # 0 means no limit, which could lead to exceeding the Database connection limit.",
            "        pool_size = conf.getint('database', 'SQL_ALCHEMY_POOL_SIZE', fallback=5)",
            "",
            "        # The maximum overflow size of the pool.",
            "        # When the number of checked-out connections reaches the size set in pool_size,",
            "        # additional connections will be returned up to this limit.",
            "        # When those additional connections are returned to the pool, they are disconnected and discarded.",
            "        # It follows then that the total number of simultaneous connections",
            "        # the pool will allow is pool_size + max_overflow,",
            "        # and the total number of \u201csleeping\u201d connections the pool will allow is pool_size.",
            "        # max_overflow can be set to -1 to indicate no overflow limit;",
            "        # no limit will be placed on the total number",
            "        # of concurrent connections. Defaults to 10.",
            "        max_overflow = conf.getint('database', 'SQL_ALCHEMY_MAX_OVERFLOW', fallback=10)",
            "",
            "        # The DB server already has a value for wait_timeout (number of seconds after",
            "        # which an idle sleeping connection should be killed). Since other DBs may",
            "        # co-exist on the same server, SQLAlchemy should set its",
            "        # pool_recycle to an equal or smaller value.",
            "        pool_recycle = conf.getint('database', 'SQL_ALCHEMY_POOL_RECYCLE', fallback=1800)",
            "",
            "        # Check connection at the start of each connection pool checkout.",
            "        # Typically, this is a simple statement like \u201cSELECT 1\u201d, but may also make use",
            "        # of some DBAPI-specific method to test the connection for liveness.",
            "        # More information here:",
            "        # https://docs.sqlalchemy.org/en/13/core/pooling.html#disconnect-handling-pessimistic",
            "        pool_pre_ping = conf.getboolean('database', 'SQL_ALCHEMY_POOL_PRE_PING', fallback=True)",
            "",
            "        log.debug(",
            "            \"settings.prepare_engine_args(): Using pool settings. pool_size=%d, max_overflow=%d, \"",
            "            \"pool_recycle=%d, pid=%d\",",
            "            pool_size,",
            "            max_overflow,",
            "            pool_recycle,",
            "            os.getpid(),",
            "        )",
            "        engine_args['pool_size'] = pool_size",
            "        engine_args['pool_recycle'] = pool_recycle",
            "        engine_args['pool_pre_ping'] = pool_pre_ping",
            "        engine_args['max_overflow'] = max_overflow",
            "",
            "    # The default isolation level for MySQL (REPEATABLE READ) can introduce inconsistencies when",
            "    # running multiple schedulers, as repeated queries on the same session may read from stale snapshots.",
            "    # 'READ COMMITTED' is the default value for PostgreSQL.",
            "    # More information here:",
            "    # https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html\"",
            "",
            "    # Similarly MSSQL default isolation level should be set to READ COMMITTED.",
            "    # We also make sure that READ_COMMITTED_SNAPSHOT option is on, in order to avoid deadlocks when",
            "    # Select queries are running. This is by default enforced during init/upgrade. More information:",
            "    # https://docs.microsoft.com/en-us/sql/t-sql/statements/set-transaction-isolation-level-transact-sql",
            "",
            "    if SQL_ALCHEMY_CONN.startswith(('mysql', 'mssql')):",
            "        engine_args['isolation_level'] = 'READ COMMITTED'",
            "",
            "    # Allow the user to specify an encoding for their DB otherwise default",
            "    # to utf-8 so jobs & users with non-latin1 characters can still use us.",
            "    engine_args['encoding'] = conf.get('database', 'SQL_ENGINE_ENCODING', fallback='utf-8')",
            "",
            "    return engine_args",
            "",
            "",
            "def dispose_orm():",
            "    \"\"\"Properly close pooled database connections\"\"\"",
            "    log.debug(\"Disposing DB connection pool (PID %s)\", os.getpid())",
            "    global engine",
            "    global Session",
            "",
            "    if Session:",
            "        Session.remove()",
            "        Session = None",
            "    if engine:",
            "        engine.dispose()",
            "        engine = None",
            "",
            "",
            "def reconfigure_orm(disable_connection_pool=False):",
            "    \"\"\"Properly close database connections and re-configure ORM\"\"\"",
            "    dispose_orm()",
            "    configure_orm(disable_connection_pool=disable_connection_pool)",
            "",
            "",
            "def configure_adapters():",
            "    \"\"\"Register Adapters and DB Converters\"\"\"",
            "    from pendulum import DateTime as Pendulum",
            "",
            "    if SQL_ALCHEMY_CONN.startswith('sqlite'):",
            "        from sqlite3 import register_adapter",
            "",
            "        register_adapter(Pendulum, lambda val: val.isoformat(' '))",
            "",
            "    if SQL_ALCHEMY_CONN.startswith('mysql'):",
            "        try:",
            "            import MySQLdb.converters",
            "",
            "            MySQLdb.converters.conversions[Pendulum] = MySQLdb.converters.DateTime2literal",
            "        except ImportError:",
            "            pass",
            "        try:",
            "            import pymysql.converters",
            "",
            "            pymysql.converters.conversions[Pendulum] = pymysql.converters.escape_datetime",
            "        except ImportError:",
            "            pass",
            "",
            "",
            "def validate_session():",
            "    \"\"\"Validate ORM Session\"\"\"",
            "    global engine",
            "",
            "    worker_precheck = conf.getboolean('celery', 'worker_precheck', fallback=False)",
            "    if not worker_precheck:",
            "        return True",
            "    else:",
            "        check_session = sessionmaker(bind=engine)",
            "        session = check_session()",
            "        try:",
            "            session.execute(\"select 1\")",
            "            conn_status = True",
            "        except exc.DBAPIError as err:",
            "            log.error(err)",
            "            conn_status = False",
            "        session.close()",
            "        return conn_status",
            "",
            "",
            "def configure_action_logging():",
            "    \"\"\"",
            "    Any additional configuration (register callback) for airflow.utils.action_loggers",
            "    module",
            "    :rtype: None",
            "    \"\"\"",
            "",
            "",
            "def prepare_syspath():",
            "    \"\"\"Ensures that certain subfolders of AIRFLOW_HOME are on the classpath\"\"\"",
            "    if DAGS_FOLDER not in sys.path:",
            "        sys.path.append(DAGS_FOLDER)",
            "",
            "    # Add ./config/ for loading custom log parsers etc, or",
            "    # airflow_local_settings etc.",
            "    config_path = os.path.join(AIRFLOW_HOME, 'config')",
            "    if config_path not in sys.path:",
            "        sys.path.append(config_path)",
            "",
            "    if PLUGINS_FOLDER not in sys.path:",
            "        sys.path.append(PLUGINS_FOLDER)",
            "",
            "",
            "def get_session_lifetime_config():",
            "    \"\"\"Gets session timeout configs and handles outdated configs gracefully.\"\"\"",
            "    session_lifetime_minutes = conf.get('webserver', 'session_lifetime_minutes', fallback=None)",
            "    session_lifetime_days = conf.get('webserver', 'session_lifetime_days', fallback=None)",
            "    uses_deprecated_lifetime_configs = session_lifetime_days or conf.get(",
            "        'webserver', 'force_log_out_after', fallback=None",
            "    )",
            "",
            "    minutes_per_day = 24 * 60",
            "    default_lifetime_minutes = '43200'",
            "    if uses_deprecated_lifetime_configs and session_lifetime_minutes == default_lifetime_minutes:",
            "        warnings.warn(",
            "            '`session_lifetime_days` option from `[webserver]` section has been '",
            "            'renamed to `session_lifetime_minutes`. The new option allows to configure '",
            "            'session lifetime in minutes. The `force_log_out_after` option has been removed '",
            "            'from `[webserver]` section. Please update your configuration.',",
            "            category=DeprecationWarning,",
            "        )",
            "        if session_lifetime_days:",
            "            session_lifetime_minutes = minutes_per_day * int(session_lifetime_days)",
            "",
            "    if not session_lifetime_minutes:",
            "        session_lifetime_days = 30",
            "        session_lifetime_minutes = minutes_per_day * session_lifetime_days",
            "",
            "    logging.debug('User session lifetime is set to %s minutes.', session_lifetime_minutes)",
            "",
            "    return int(session_lifetime_minutes)",
            "",
            "",
            "def import_local_settings():",
            "    \"\"\"Import airflow_local_settings.py files to allow overriding any configs in settings.py file\"\"\"",
            "    try:",
            "        import airflow_local_settings",
            "",
            "        if hasattr(airflow_local_settings, \"__all__\"):",
            "            for i in airflow_local_settings.__all__:",
            "                globals()[i] = getattr(airflow_local_settings, i)",
            "        else:",
            "            for k, v in airflow_local_settings.__dict__.items():",
            "                if not k.startswith(\"__\"):",
            "                    globals()[k] = v",
            "",
            "        # TODO: Remove once deprecated",
            "        if \"policy\" in globals() and \"task_policy\" not in globals():",
            "            warnings.warn(",
            "                \"Using `policy` in airflow_local_settings.py is deprecated. \"",
            "                \"Please rename your `policy` to `task_policy`.\",",
            "                DeprecationWarning,",
            "                stacklevel=2,",
            "            )",
            "            globals()[\"task_policy\"] = globals()[\"policy\"]",
            "            del globals()[\"policy\"]",
            "",
            "        if not hasattr(task_instance_mutation_hook, 'is_noop'):",
            "            task_instance_mutation_hook.is_noop = False",
            "",
            "        log.info(\"Loaded airflow_local_settings from %s .\", airflow_local_settings.__file__)",
            "    except ModuleNotFoundError as e:",
            "        if e.name == \"airflow_local_settings\":",
            "            log.debug(\"No airflow_local_settings to import.\", exc_info=True)",
            "        else:",
            "            log.critical(",
            "                \"Failed to import airflow_local_settings due to a transitive module not found error.\",",
            "                exc_info=True,",
            "            )",
            "            raise",
            "    except ImportError:",
            "        log.critical(\"Failed to import airflow_local_settings.\", exc_info=True)",
            "        raise",
            "",
            "",
            "def initialize():",
            "    \"\"\"Initialize Airflow with all the settings from this file\"\"\"",
            "    configure_vars()",
            "    prepare_syspath()",
            "    import_local_settings()",
            "    global LOGGING_CLASS_PATH",
            "    LOGGING_CLASS_PATH = configure_logging()",
            "    configure_adapters()",
            "    # The webservers import this file from models.py with the default settings.",
            "    configure_orm()",
            "    configure_action_logging()",
            "",
            "    # Ensure we close DB connections at scheduler and gunicorn worker terminations",
            "    atexit.register(dispose_orm)",
            "",
            "",
            "# Const stuff",
            "",
            "KILOBYTE = 1024",
            "MEGABYTE = KILOBYTE * KILOBYTE",
            "WEB_COLORS = {'LIGHTBLUE': '#4d9de0', 'LIGHTORANGE': '#FF9933'}",
            "",
            "",
            "# Updating serialized DAG can not be faster than a minimum interval to reduce database",
            "# write rate.",
            "MIN_SERIALIZED_DAG_UPDATE_INTERVAL = conf.getint('core', 'min_serialized_dag_update_interval', fallback=30)",
            "",
            "# If set to True, serialized DAGs is compressed before writing to DB,",
            "COMPRESS_SERIALIZED_DAGS = conf.getboolean('core', 'compress_serialized_dags', fallback=False)",
            "",
            "# Fetching serialized DAG can not be faster than a minimum interval to reduce database",
            "# read rate. This config controls when your DAGs are updated in the Webserver",
            "MIN_SERIALIZED_DAG_FETCH_INTERVAL = conf.getint('core', 'min_serialized_dag_fetch_interval', fallback=10)",
            "",
            "CAN_FORK = hasattr(os, \"fork\")",
            "",
            "EXECUTE_TASKS_NEW_PYTHON_INTERPRETER = not CAN_FORK or conf.getboolean(",
            "    'core',",
            "    'execute_tasks_new_python_interpreter',",
            "    fallback=False,",
            ")",
            "",
            "ALLOW_FUTURE_EXEC_DATES = conf.getboolean('scheduler', 'allow_trigger_in_future', fallback=False)",
            "",
            "# Whether or not to check each dagrun against defined SLAs",
            "CHECK_SLAS = conf.getboolean('core', 'check_slas', fallback=True)",
            "",
            "USE_JOB_SCHEDULE = conf.getboolean('scheduler', 'use_job_schedule', fallback=True)",
            "",
            "# By default Airflow plugins are lazily-loaded (only loaded when required). Set it to False,",
            "# if you want to load plugins whenever 'airflow' is invoked via cli or loaded from module.",
            "LAZY_LOAD_PLUGINS = conf.getboolean('core', 'lazy_load_plugins', fallback=True)",
            "",
            "# By default Airflow providers are lazily-discovered (discovery and imports happen only when required).",
            "# Set it to False, if you want to discover providers whenever 'airflow' is invoked via cli or",
            "# loaded from module.",
            "LAZY_LOAD_PROVIDERS = conf.getboolean('core', 'lazy_discover_providers', fallback=True)",
            "",
            "# Determines if the executor utilizes Kubernetes",
            "IS_K8S_OR_K8SCELERY_EXECUTOR = conf.get('core', 'EXECUTOR') in {",
            "    executor_constants.KUBERNETES_EXECUTOR,",
            "    executor_constants.CELERY_KUBERNETES_EXECUTOR,",
            "    executor_constants.LOCAL_KUBERNETES_EXECUTOR,",
            "}",
            "",
            "HIDE_SENSITIVE_VAR_CONN_FIELDS = conf.getboolean('core', 'hide_sensitive_var_conn_fields')",
            "",
            "# By default this is off, but is automatically configured on when running task",
            "# instances",
            "MASK_SECRETS_IN_LOGS = False",
            "",
            "# Display alerts on the dashboard",
            "# Useful for warning about setup issues or announcing changes to end users",
            "# List of UIAlerts, which allows for specifying the message, category, and roles the",
            "# message should be shown to. For example:",
            "#   from airflow.www.utils import UIAlert",
            "#",
            "#   DASHBOARD_UIALERTS = [",
            "#       UIAlert(\"Welcome to Airflow\"),  # All users",
            "#       UIAlert(\"Airflow update happening next week\", roles=[\"User\"]),  # Only users with the User role",
            "#       # A flash message with html:",
            "#       UIAlert('Visit <a href=\"http://airflow.apache.org\">airflow.apache.org</a>', html=True),",
            "#   ]",
            "#",
            "DASHBOARD_UIALERTS: List[\"UIAlert\"] = []",
            "",
            "# Prefix used to identify tables holding data moved during migration.",
            "AIRFLOW_MOVED_TABLE_PREFIX = \"_airflow_moved\""
        ],
        "afterPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import atexit",
            "import functools",
            "import json",
            "import logging",
            "import os",
            "import sys",
            "import warnings",
            "from typing import TYPE_CHECKING, Callable, List, Optional, Union",
            "",
            "import pendulum",
            "import sqlalchemy",
            "from sqlalchemy import create_engine, exc",
            "from sqlalchemy.engine import Engine",
            "from sqlalchemy.orm import scoped_session, sessionmaker",
            "from sqlalchemy.orm.session import Session as SASession",
            "from sqlalchemy.pool import NullPool",
            "",
            "from airflow.configuration import AIRFLOW_HOME, WEBSERVER_CONFIG, conf  # NOQA F401",
            "from airflow.executors import executor_constants",
            "from airflow.logging_config import configure_logging",
            "from airflow.utils.orm_event_handlers import setup_event_handlers",
            "",
            "if TYPE_CHECKING:",
            "    from airflow.www.utils import UIAlert",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "TIMEZONE = pendulum.tz.timezone('UTC')",
            "try:",
            "    tz = conf.get_mandatory_value(\"core\", \"default_timezone\")",
            "    if tz == \"system\":",
            "        TIMEZONE = pendulum.tz.local_timezone()",
            "    else:",
            "        TIMEZONE = pendulum.tz.timezone(tz)",
            "except Exception:",
            "    pass",
            "log.info(\"Configured default timezone %s\", TIMEZONE)",
            "",
            "",
            "HEADER = '\\n'.join(",
            "    [",
            "        r'  ____________       _____________',",
            "        r' ____    |__( )_________  __/__  /________      __',",
            "        r'____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /',",
            "        r'___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /',",
            "        r' _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/',",
            "    ]",
            ")",
            "",
            "LOGGING_LEVEL = logging.INFO",
            "",
            "# the prefix to append to gunicorn worker processes after init",
            "GUNICORN_WORKER_READY_PREFIX = \"[ready] \"",
            "",
            "LOG_FORMAT = conf.get('logging', 'log_format')",
            "SIMPLE_LOG_FORMAT = conf.get('logging', 'simple_log_format')",
            "",
            "SQL_ALCHEMY_CONN: Optional[str] = None",
            "PLUGINS_FOLDER: Optional[str] = None",
            "LOGGING_CLASS_PATH: Optional[str] = None",
            "DONOT_MODIFY_HANDLERS: Optional[bool] = None",
            "DAGS_FOLDER: str = os.path.expanduser(conf.get_mandatory_value('core', 'DAGS_FOLDER'))",
            "",
            "engine: Engine",
            "Session: Callable[..., SASession]",
            "",
            "# The JSON library to use for DAG Serialization and De-Serialization",
            "json = json",
            "",
            "# Dictionary containing State and colors associated to each state to",
            "# display on the Webserver",
            "STATE_COLORS = {",
            "    \"deferred\": \"mediumpurple\",",
            "    \"failed\": \"red\",",
            "    \"queued\": \"gray\",",
            "    \"running\": \"lime\",",
            "    \"scheduled\": \"tan\",",
            "    \"skipped\": \"hotpink\",",
            "    \"success\": \"green\",",
            "    \"up_for_reschedule\": \"turquoise\",",
            "    \"up_for_retry\": \"gold\",",
            "    \"upstream_failed\": \"orange\",",
            "}",
            "",
            "",
            "@functools.lru_cache(maxsize=None)",
            "def _get_rich_console(file):",
            "    # Delay imports until we need it",
            "    import rich.console",
            "",
            "    return rich.console.Console(file=file)",
            "",
            "",
            "def custom_show_warning(message, category, filename, lineno, file=None, line=None):",
            "    \"\"\"Custom function to print rich and visible warnings\"\"\"",
            "    # Delay imports until we need it",
            "    from rich.markup import escape",
            "",
            "    msg = f\"[bold]{line}\" if line else f\"[bold][yellow]{filename}:{lineno}\"",
            "    msg += f\" {category.__name__}[/bold]: {escape(str(message))}[/yellow]\"",
            "    write_console = _get_rich_console(file or sys.stderr)",
            "    write_console.print(msg, soft_wrap=True)",
            "",
            "",
            "def replace_showwarning(replacement):",
            "    \"\"\"Replace ``warnings.showwarning``, returning the original.",
            "",
            "    This is useful since we want to \"reset\" the ``showwarning`` hook on exit to",
            "    avoid lazy-loading issues. If a warning is emitted after Python cleaned up",
            "    the import system, we would no longer be able to import ``rich``.",
            "    \"\"\"",
            "    original = warnings.showwarning",
            "    warnings.showwarning = replacement",
            "    return original",
            "",
            "",
            "original_show_warning = replace_showwarning(custom_show_warning)",
            "atexit.register(functools.partial(replace_showwarning, original_show_warning))",
            "",
            "",
            "def task_policy(task) -> None:",
            "    \"\"\"",
            "    This policy setting allows altering tasks after they are loaded in",
            "    the DagBag. It allows administrator to rewire some task's parameters.",
            "    Alternatively you can raise ``AirflowClusterPolicyViolation`` exception",
            "    to stop DAG from being executed.",
            "",
            "    To define policy, add a ``airflow_local_settings`` module",
            "    to your PYTHONPATH that defines this ``task_policy`` function.",
            "",
            "    Here are a few examples of how this can be useful:",
            "",
            "    * You could enforce a specific queue (say the ``spark`` queue)",
            "        for tasks using the ``SparkOperator`` to make sure that these",
            "        tasks get wired to the right workers",
            "    * You could enforce a task timeout policy, making sure that no tasks run",
            "        for more than 48 hours",
            "",
            "    :param task: task to be mutated",
            "    \"\"\"",
            "",
            "",
            "def dag_policy(dag) -> None:",
            "    \"\"\"",
            "    This policy setting allows altering DAGs after they are loaded in",
            "    the DagBag. It allows administrator to rewire some DAG's parameters.",
            "    Alternatively you can raise ``AirflowClusterPolicyViolation`` exception",
            "    to stop DAG from being executed.",
            "",
            "    To define policy, add a ``airflow_local_settings`` module",
            "    to your PYTHONPATH that defines this ``dag_policy`` function.",
            "",
            "    Here are a few examples of how this can be useful:",
            "",
            "    * You could enforce default user for DAGs",
            "    * Check if every DAG has configured tags",
            "",
            "    :param dag: dag to be mutated",
            "    \"\"\"",
            "",
            "",
            "def task_instance_mutation_hook(task_instance):",
            "    \"\"\"",
            "    This setting allows altering task instances before they are queued by",
            "    the Airflow scheduler.",
            "",
            "    To define task_instance_mutation_hook, add a ``airflow_local_settings`` module",
            "    to your PYTHONPATH that defines this ``task_instance_mutation_hook`` function.",
            "",
            "    This could be used, for instance, to modify the task instance during retries.",
            "",
            "    :param task_instance: task instance to be mutated",
            "    \"\"\"",
            "",
            "",
            "task_instance_mutation_hook.is_noop = True  # type: ignore",
            "",
            "",
            "def pod_mutation_hook(pod):",
            "    \"\"\"",
            "    This setting allows altering ``kubernetes.client.models.V1Pod`` object",
            "    before they are passed to the Kubernetes client for scheduling.",
            "",
            "    To define a pod mutation hook, add a ``airflow_local_settings`` module",
            "    to your PYTHONPATH that defines this ``pod_mutation_hook`` function.",
            "    It receives a ``Pod`` object and can alter it where needed.",
            "",
            "    This could be used, for instance, to add sidecar or init containers",
            "    to every worker pod launched by KubernetesExecutor or KubernetesPodOperator.",
            "    \"\"\"",
            "",
            "",
            "def get_airflow_context_vars(context):",
            "    \"\"\"",
            "    This setting allows getting the airflow context vars, which are key value pairs.",
            "    They are then injected to default airflow context vars, which in the end are",
            "    available as environment variables when running tasks",
            "    dag_id, task_id, execution_date, dag_run_id, try_number are reserved keys.",
            "    To define it, add a ``airflow_local_settings`` module",
            "    to your PYTHONPATH that defines this ``get_airflow_context_vars`` function.",
            "",
            "    :param context: The context for the task_instance of interest.",
            "    \"\"\"",
            "    return {}",
            "",
            "",
            "def get_dagbag_import_timeout(dag_file_path: str) -> Union[int, float]:",
            "    \"\"\"",
            "    This setting allows for dynamic control of the DAG file parsing timeout based on the DAG file path.",
            "",
            "    It is useful when there are a few DAG files requiring longer parsing times, while others do not.",
            "    You can control them separately instead of having one value for all DAG files.",
            "",
            "    If the return value is less than or equal to 0, it means no timeout during the DAG parsing.",
            "    \"\"\"",
            "    return conf.getfloat('core', 'DAGBAG_IMPORT_TIMEOUT')",
            "",
            "",
            "def configure_vars():",
            "    \"\"\"Configure Global Variables from airflow.cfg\"\"\"",
            "    global SQL_ALCHEMY_CONN",
            "    global DAGS_FOLDER",
            "    global PLUGINS_FOLDER",
            "    global DONOT_MODIFY_HANDLERS",
            "    SQL_ALCHEMY_CONN = conf.get('database', 'SQL_ALCHEMY_CONN')",
            "    DAGS_FOLDER = os.path.expanduser(conf.get('core', 'DAGS_FOLDER'))",
            "",
            "    PLUGINS_FOLDER = conf.get('core', 'plugins_folder', fallback=os.path.join(AIRFLOW_HOME, 'plugins'))",
            "",
            "    # If donot_modify_handlers=True, we do not modify logging handlers in task_run command",
            "    # If the flag is set to False, we remove all handlers from the root logger",
            "    # and add all handlers from 'airflow.task' logger to the root Logger. This is done",
            "    # to get all the logs from the print & log statements in the DAG files before a task is run",
            "    # The handlers are restored after the task completes execution.",
            "    DONOT_MODIFY_HANDLERS = conf.getboolean('logging', 'donot_modify_handlers', fallback=False)",
            "",
            "",
            "def configure_orm(disable_connection_pool=False):",
            "    \"\"\"Configure ORM using SQLAlchemy\"\"\"",
            "    from airflow.utils.log.secrets_masker import mask_secret",
            "",
            "    log.debug(\"Setting up DB connection pool (PID %s)\", os.getpid())",
            "    global engine",
            "    global Session",
            "    engine_args = prepare_engine_args(disable_connection_pool)",
            "",
            "    if conf.has_option('database', 'sql_alchemy_connect_args'):",
            "        connect_args = conf.getimport('database', 'sql_alchemy_connect_args')",
            "    else:",
            "        connect_args = {}",
            "",
            "    engine = create_engine(SQL_ALCHEMY_CONN, connect_args=connect_args, **engine_args)",
            "",
            "    mask_secret(engine.url.password)",
            "",
            "    setup_event_handlers(engine)",
            "",
            "    Session = scoped_session(",
            "        sessionmaker(",
            "            autocommit=False,",
            "            autoflush=False,",
            "            bind=engine,",
            "            expire_on_commit=False,",
            "        )",
            "    )",
            "    if engine.dialect.name == 'mssql':",
            "        session = Session()",
            "        try:",
            "            result = session.execute(",
            "                sqlalchemy.text(",
            "                    'SELECT is_read_committed_snapshot_on FROM sys.databases WHERE name=:database_name'",
            "                ),",
            "                params={\"database_name\": engine.url.database},",
            "            )",
            "            data = result.fetchone()[0]",
            "            if data != 1:",
            "                log.critical(\"MSSQL database MUST have READ_COMMITTED_SNAPSHOT enabled.\")",
            "                log.critical(f\"The database {engine.url.database} has it disabled.\")",
            "                log.critical(\"This will cause random deadlocks, Refusing to start.\")",
            "                log.critical(",
            "                    \"See https://airflow.apache.org/docs/apache-airflow/stable/howto/\"",
            "                    \"set-up-database.html#setting-up-a-mssql-database\"",
            "                )",
            "                raise Exception(\"MSSQL database MUST have READ_COMMITTED_SNAPSHOT enabled.\")",
            "        finally:",
            "            session.close()",
            "",
            "",
            "DEFAULT_ENGINE_ARGS = {",
            "    'postgresql': {",
            "        'executemany_mode': 'values',",
            "        'executemany_values_page_size': 10000,",
            "        'executemany_batch_page_size': 2000,",
            "    },",
            "}",
            "",
            "",
            "def prepare_engine_args(disable_connection_pool=False):",
            "    \"\"\"Prepare SQLAlchemy engine args\"\"\"",
            "    default_args = {}",
            "    for dialect, default in DEFAULT_ENGINE_ARGS.items():",
            "        if SQL_ALCHEMY_CONN.startswith(dialect):",
            "            default_args = default.copy()",
            "            break",
            "",
            "    engine_args: dict = conf.getjson(",
            "        'database', 'sql_alchemy_engine_args', fallback=default_args",
            "    )  # type: ignore",
            "",
            "    if disable_connection_pool or not conf.getboolean('database', 'SQL_ALCHEMY_POOL_ENABLED'):",
            "        engine_args['poolclass'] = NullPool",
            "        log.debug(\"settings.prepare_engine_args(): Using NullPool\")",
            "    elif not SQL_ALCHEMY_CONN.startswith('sqlite'):",
            "        # Pool size engine args not supported by sqlite.",
            "        # If no config value is defined for the pool size, select a reasonable value.",
            "        # 0 means no limit, which could lead to exceeding the Database connection limit.",
            "        pool_size = conf.getint('database', 'SQL_ALCHEMY_POOL_SIZE', fallback=5)",
            "",
            "        # The maximum overflow size of the pool.",
            "        # When the number of checked-out connections reaches the size set in pool_size,",
            "        # additional connections will be returned up to this limit.",
            "        # When those additional connections are returned to the pool, they are disconnected and discarded.",
            "        # It follows then that the total number of simultaneous connections",
            "        # the pool will allow is pool_size + max_overflow,",
            "        # and the total number of \u201csleeping\u201d connections the pool will allow is pool_size.",
            "        # max_overflow can be set to -1 to indicate no overflow limit;",
            "        # no limit will be placed on the total number",
            "        # of concurrent connections. Defaults to 10.",
            "        max_overflow = conf.getint('database', 'SQL_ALCHEMY_MAX_OVERFLOW', fallback=10)",
            "",
            "        # The DB server already has a value for wait_timeout (number of seconds after",
            "        # which an idle sleeping connection should be killed). Since other DBs may",
            "        # co-exist on the same server, SQLAlchemy should set its",
            "        # pool_recycle to an equal or smaller value.",
            "        pool_recycle = conf.getint('database', 'SQL_ALCHEMY_POOL_RECYCLE', fallback=1800)",
            "",
            "        # Check connection at the start of each connection pool checkout.",
            "        # Typically, this is a simple statement like \u201cSELECT 1\u201d, but may also make use",
            "        # of some DBAPI-specific method to test the connection for liveness.",
            "        # More information here:",
            "        # https://docs.sqlalchemy.org/en/13/core/pooling.html#disconnect-handling-pessimistic",
            "        pool_pre_ping = conf.getboolean('database', 'SQL_ALCHEMY_POOL_PRE_PING', fallback=True)",
            "",
            "        log.debug(",
            "            \"settings.prepare_engine_args(): Using pool settings. pool_size=%d, max_overflow=%d, \"",
            "            \"pool_recycle=%d, pid=%d\",",
            "            pool_size,",
            "            max_overflow,",
            "            pool_recycle,",
            "            os.getpid(),",
            "        )",
            "        engine_args['pool_size'] = pool_size",
            "        engine_args['pool_recycle'] = pool_recycle",
            "        engine_args['pool_pre_ping'] = pool_pre_ping",
            "        engine_args['max_overflow'] = max_overflow",
            "",
            "    # The default isolation level for MySQL (REPEATABLE READ) can introduce inconsistencies when",
            "    # running multiple schedulers, as repeated queries on the same session may read from stale snapshots.",
            "    # 'READ COMMITTED' is the default value for PostgreSQL.",
            "    # More information here:",
            "    # https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html\"",
            "",
            "    # Similarly MSSQL default isolation level should be set to READ COMMITTED.",
            "    # We also make sure that READ_COMMITTED_SNAPSHOT option is on, in order to avoid deadlocks when",
            "    # Select queries are running. This is by default enforced during init/upgrade. More information:",
            "    # https://docs.microsoft.com/en-us/sql/t-sql/statements/set-transaction-isolation-level-transact-sql",
            "",
            "    if SQL_ALCHEMY_CONN.startswith(('mysql', 'mssql')):",
            "        engine_args['isolation_level'] = 'READ COMMITTED'",
            "",
            "    # Allow the user to specify an encoding for their DB otherwise default",
            "    # to utf-8 so jobs & users with non-latin1 characters can still use us.",
            "    engine_args['encoding'] = conf.get('database', 'SQL_ENGINE_ENCODING', fallback='utf-8')",
            "",
            "    return engine_args",
            "",
            "",
            "def dispose_orm():",
            "    \"\"\"Properly close pooled database connections\"\"\"",
            "    log.debug(\"Disposing DB connection pool (PID %s)\", os.getpid())",
            "    global engine",
            "    global Session",
            "",
            "    if Session:",
            "        Session.remove()",
            "        Session = None",
            "    if engine:",
            "        engine.dispose()",
            "        engine = None",
            "",
            "",
            "def reconfigure_orm(disable_connection_pool=False):",
            "    \"\"\"Properly close database connections and re-configure ORM\"\"\"",
            "    dispose_orm()",
            "    configure_orm(disable_connection_pool=disable_connection_pool)",
            "",
            "",
            "def configure_adapters():",
            "    \"\"\"Register Adapters and DB Converters\"\"\"",
            "    from pendulum import DateTime as Pendulum",
            "",
            "    if SQL_ALCHEMY_CONN.startswith('sqlite'):",
            "        from sqlite3 import register_adapter",
            "",
            "        register_adapter(Pendulum, lambda val: val.isoformat(' '))",
            "",
            "    if SQL_ALCHEMY_CONN.startswith('mysql'):",
            "        try:",
            "            import MySQLdb.converters",
            "",
            "            MySQLdb.converters.conversions[Pendulum] = MySQLdb.converters.DateTime2literal",
            "        except ImportError:",
            "            pass",
            "        try:",
            "            import pymysql.converters",
            "",
            "            pymysql.converters.conversions[Pendulum] = pymysql.converters.escape_datetime",
            "        except ImportError:",
            "            pass",
            "",
            "",
            "def validate_session():",
            "    \"\"\"Validate ORM Session\"\"\"",
            "    global engine",
            "",
            "    worker_precheck = conf.getboolean('celery', 'worker_precheck', fallback=False)",
            "    if not worker_precheck:",
            "        return True",
            "    else:",
            "        check_session = sessionmaker(bind=engine)",
            "        session = check_session()",
            "        try:",
            "            session.execute(\"select 1\")",
            "            conn_status = True",
            "        except exc.DBAPIError as err:",
            "            log.error(err)",
            "            conn_status = False",
            "        session.close()",
            "        return conn_status",
            "",
            "",
            "def configure_action_logging():",
            "    \"\"\"",
            "    Any additional configuration (register callback) for airflow.utils.action_loggers",
            "    module",
            "    :rtype: None",
            "    \"\"\"",
            "",
            "",
            "def prepare_syspath():",
            "    \"\"\"Ensures that certain subfolders of AIRFLOW_HOME are on the classpath\"\"\"",
            "    if DAGS_FOLDER not in sys.path:",
            "        sys.path.append(DAGS_FOLDER)",
            "",
            "    # Add ./config/ for loading custom log parsers etc, or",
            "    # airflow_local_settings etc.",
            "    config_path = os.path.join(AIRFLOW_HOME, 'config')",
            "    if config_path not in sys.path:",
            "        sys.path.append(config_path)",
            "",
            "    if PLUGINS_FOLDER not in sys.path:",
            "        sys.path.append(PLUGINS_FOLDER)",
            "",
            "",
            "def get_session_lifetime_config():",
            "    \"\"\"Gets session timeout configs and handles outdated configs gracefully.\"\"\"",
            "    session_lifetime_minutes = conf.get('webserver', 'session_lifetime_minutes', fallback=None)",
            "    session_lifetime_days = conf.get('webserver', 'session_lifetime_days', fallback=None)",
            "    uses_deprecated_lifetime_configs = session_lifetime_days or conf.get(",
            "        'webserver', 'force_log_out_after', fallback=None",
            "    )",
            "",
            "    minutes_per_day = 24 * 60",
            "    default_lifetime_minutes = '43200'",
            "    if uses_deprecated_lifetime_configs and session_lifetime_minutes == default_lifetime_minutes:",
            "        warnings.warn(",
            "            '`session_lifetime_days` option from `[webserver]` section has been '",
            "            'renamed to `session_lifetime_minutes`. The new option allows to configure '",
            "            'session lifetime in minutes. The `force_log_out_after` option has been removed '",
            "            'from `[webserver]` section. Please update your configuration.',",
            "            category=DeprecationWarning,",
            "        )",
            "        if session_lifetime_days:",
            "            session_lifetime_minutes = minutes_per_day * int(session_lifetime_days)",
            "",
            "    if not session_lifetime_minutes:",
            "        session_lifetime_days = 30",
            "        session_lifetime_minutes = minutes_per_day * session_lifetime_days",
            "",
            "    logging.debug('User session lifetime is set to %s minutes.', session_lifetime_minutes)",
            "",
            "    return int(session_lifetime_minutes)",
            "",
            "",
            "def import_local_settings():",
            "    \"\"\"Import airflow_local_settings.py files to allow overriding any configs in settings.py file\"\"\"",
            "    try:",
            "        import airflow_local_settings",
            "",
            "        if hasattr(airflow_local_settings, \"__all__\"):",
            "            for i in airflow_local_settings.__all__:",
            "                globals()[i] = getattr(airflow_local_settings, i)",
            "        else:",
            "            for k, v in airflow_local_settings.__dict__.items():",
            "                if not k.startswith(\"__\"):",
            "                    globals()[k] = v",
            "",
            "        # TODO: Remove once deprecated",
            "        if \"policy\" in globals() and \"task_policy\" not in globals():",
            "            warnings.warn(",
            "                \"Using `policy` in airflow_local_settings.py is deprecated. \"",
            "                \"Please rename your `policy` to `task_policy`.\",",
            "                DeprecationWarning,",
            "                stacklevel=2,",
            "            )",
            "            globals()[\"task_policy\"] = globals()[\"policy\"]",
            "            del globals()[\"policy\"]",
            "",
            "        if not hasattr(task_instance_mutation_hook, 'is_noop'):",
            "            task_instance_mutation_hook.is_noop = False",
            "",
            "        log.info(\"Loaded airflow_local_settings from %s .\", airflow_local_settings.__file__)",
            "    except ModuleNotFoundError as e:",
            "        if e.name == \"airflow_local_settings\":",
            "            log.debug(\"No airflow_local_settings to import.\", exc_info=True)",
            "        else:",
            "            log.critical(",
            "                \"Failed to import airflow_local_settings due to a transitive module not found error.\",",
            "                exc_info=True,",
            "            )",
            "            raise",
            "    except ImportError:",
            "        log.critical(\"Failed to import airflow_local_settings.\", exc_info=True)",
            "        raise",
            "",
            "",
            "def initialize():",
            "    \"\"\"Initialize Airflow with all the settings from this file\"\"\"",
            "    configure_vars()",
            "    prepare_syspath()",
            "    import_local_settings()",
            "    global LOGGING_CLASS_PATH",
            "    LOGGING_CLASS_PATH = configure_logging()",
            "    configure_adapters()",
            "    # The webservers import this file from models.py with the default settings.",
            "    configure_orm()",
            "    configure_action_logging()",
            "",
            "    # Ensure we close DB connections at scheduler and gunicorn worker terminations",
            "    atexit.register(dispose_orm)",
            "",
            "",
            "# Const stuff",
            "",
            "KILOBYTE = 1024",
            "MEGABYTE = KILOBYTE * KILOBYTE",
            "WEB_COLORS = {'LIGHTBLUE': '#4d9de0', 'LIGHTORANGE': '#FF9933'}",
            "",
            "",
            "# Updating serialized DAG can not be faster than a minimum interval to reduce database",
            "# write rate.",
            "MIN_SERIALIZED_DAG_UPDATE_INTERVAL = conf.getint('core', 'min_serialized_dag_update_interval', fallback=30)",
            "",
            "# If set to True, serialized DAGs is compressed before writing to DB,",
            "COMPRESS_SERIALIZED_DAGS = conf.getboolean('core', 'compress_serialized_dags', fallback=False)",
            "",
            "# Fetching serialized DAG can not be faster than a minimum interval to reduce database",
            "# read rate. This config controls when your DAGs are updated in the Webserver",
            "MIN_SERIALIZED_DAG_FETCH_INTERVAL = conf.getint('core', 'min_serialized_dag_fetch_interval', fallback=10)",
            "",
            "CAN_FORK = hasattr(os, \"fork\")",
            "",
            "EXECUTE_TASKS_NEW_PYTHON_INTERPRETER = not CAN_FORK or conf.getboolean(",
            "    'core',",
            "    'execute_tasks_new_python_interpreter',",
            "    fallback=False,",
            ")",
            "",
            "ALLOW_FUTURE_EXEC_DATES = conf.getboolean('scheduler', 'allow_trigger_in_future', fallback=False)",
            "",
            "# Whether or not to check each dagrun against defined SLAs",
            "CHECK_SLAS = conf.getboolean('core', 'check_slas', fallback=True)",
            "",
            "USE_JOB_SCHEDULE = conf.getboolean('scheduler', 'use_job_schedule', fallback=True)",
            "",
            "# By default Airflow plugins are lazily-loaded (only loaded when required). Set it to False,",
            "# if you want to load plugins whenever 'airflow' is invoked via cli or loaded from module.",
            "LAZY_LOAD_PLUGINS = conf.getboolean('core', 'lazy_load_plugins', fallback=True)",
            "",
            "# By default Airflow providers are lazily-discovered (discovery and imports happen only when required).",
            "# Set it to False, if you want to discover providers whenever 'airflow' is invoked via cli or",
            "# loaded from module.",
            "LAZY_LOAD_PROVIDERS = conf.getboolean('core', 'lazy_discover_providers', fallback=True)",
            "",
            "# Determines if the executor utilizes Kubernetes",
            "IS_K8S_OR_K8SCELERY_EXECUTOR = conf.get('core', 'EXECUTOR') in {",
            "    executor_constants.KUBERNETES_EXECUTOR,",
            "    executor_constants.CELERY_KUBERNETES_EXECUTOR,",
            "    executor_constants.LOCAL_KUBERNETES_EXECUTOR,",
            "}",
            "",
            "HIDE_SENSITIVE_VAR_CONN_FIELDS = conf.getboolean('core', 'hide_sensitive_var_conn_fields')",
            "",
            "# By default this is off, but is automatically configured on when running task",
            "# instances",
            "MASK_SECRETS_IN_LOGS = False",
            "",
            "# Display alerts on the dashboard",
            "# Useful for warning about setup issues or announcing changes to end users",
            "# List of UIAlerts, which allows for specifying the message, category, and roles the",
            "# message should be shown to. For example:",
            "#   from airflow.www.utils import UIAlert",
            "#",
            "#   DASHBOARD_UIALERTS = [",
            "#       UIAlert(\"Welcome to Airflow\"),  # All users",
            "#       UIAlert(\"Airflow update happening next week\", roles=[\"User\"]),  # Only users with the User role",
            "#       # A flash message with html:",
            "#       UIAlert('Visit <a href=\"http://airflow.apache.org\">airflow.apache.org</a>', html=True),",
            "#   ]",
            "#",
            "DASHBOARD_UIALERTS: List[\"UIAlert\"] = []",
            "",
            "# Prefix used to identify tables holding data moved during migration.",
            "AIRFLOW_MOVED_TABLE_PREFIX = \"_airflow_moved\"",
            "",
            "DAEMON_UMASK: str = conf.get('core', 'daemon_umask', fallback='0o077')"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    }
}