{
    "yt_dlp/cookies.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1327,
                "afterPatchRowNumber": 1327,
                "PatchRowcode": "         self.add_cookie_header(cookie_req)"
            },
            "1": {
                "beforePatchRowNumber": 1328,
                "afterPatchRowNumber": 1328,
                "PatchRowcode": "         return cookie_req.get_header('Cookie')"
            },
            "2": {
                "beforePatchRowNumber": 1329,
                "afterPatchRowNumber": 1329,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1330,
                "PatchRowcode": "+    def get_cookies_for_url(self, url):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1331,
                "PatchRowcode": "+        \"\"\"Generate a list of Cookie objects for a given url\"\"\""
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1332,
                "PatchRowcode": "+        # Policy `_now` attribute must be set before calling `_cookies_for_request`"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1333,
                "PatchRowcode": "+        # Ref: https://github.com/python/cpython/blob/3.7/Lib/http/cookiejar.py#L1360"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1334,
                "PatchRowcode": "+        self._policy._now = self._now = int(time.time())"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1335,
                "PatchRowcode": "+        return self._cookies_for_request(urllib.request.Request(escape_url(sanitize_url(url))))"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1336,
                "PatchRowcode": "+"
            },
            "10": {
                "beforePatchRowNumber": 1330,
                "afterPatchRowNumber": 1337,
                "PatchRowcode": "     def clear(self, *args, **kwargs):"
            },
            "11": {
                "beforePatchRowNumber": 1331,
                "afterPatchRowNumber": 1338,
                "PatchRowcode": "         with contextlib.suppress(KeyError):"
            },
            "12": {
                "beforePatchRowNumber": 1332,
                "afterPatchRowNumber": 1339,
                "PatchRowcode": "             return super().clear(*args, **kwargs)"
            }
        },
        "frontPatchFile": [
            "import base64",
            "import collections",
            "import contextlib",
            "import http.cookiejar",
            "import http.cookies",
            "import io",
            "import json",
            "import os",
            "import re",
            "import shutil",
            "import struct",
            "import subprocess",
            "import sys",
            "import tempfile",
            "import time",
            "import urllib.request",
            "from datetime import datetime, timedelta, timezone",
            "from enum import Enum, auto",
            "from hashlib import pbkdf2_hmac",
            "",
            "from .aes import (",
            "    aes_cbc_decrypt_bytes,",
            "    aes_gcm_decrypt_and_verify_bytes,",
            "    unpad_pkcs7,",
            ")",
            "from .compat import functools",
            "from .dependencies import (",
            "    _SECRETSTORAGE_UNAVAILABLE_REASON,",
            "    secretstorage,",
            "    sqlite3,",
            ")",
            "from .minicurses import MultilinePrinter, QuietMultilinePrinter",
            "from .utils import (",
            "    Popen,",
            "    error_to_str,",
            "    escape_url,",
            "    expand_path,",
            "    is_path_like,",
            "    sanitize_url,",
            "    str_or_none,",
            "    try_call,",
            "    write_string,",
            ")",
            "",
            "CHROMIUM_BASED_BROWSERS = {'brave', 'chrome', 'chromium', 'edge', 'opera', 'vivaldi'}",
            "SUPPORTED_BROWSERS = CHROMIUM_BASED_BROWSERS | {'firefox', 'safari'}",
            "",
            "",
            "class YDLLogger:",
            "    def __init__(self, ydl=None):",
            "        self._ydl = ydl",
            "",
            "    def debug(self, message):",
            "        if self._ydl:",
            "            self._ydl.write_debug(message)",
            "",
            "    def info(self, message):",
            "        if self._ydl:",
            "            self._ydl.to_screen(f'[Cookies] {message}')",
            "",
            "    def warning(self, message, only_once=False):",
            "        if self._ydl:",
            "            self._ydl.report_warning(message, only_once)",
            "",
            "    def error(self, message):",
            "        if self._ydl:",
            "            self._ydl.report_error(message)",
            "",
            "    class ProgressBar(MultilinePrinter):",
            "        _DELAY, _timer = 0.1, 0",
            "",
            "        def print(self, message):",
            "            if time.time() - self._timer > self._DELAY:",
            "                self.print_at_line(f'[Cookies] {message}', 0)",
            "                self._timer = time.time()",
            "",
            "    def progress_bar(self):",
            "        \"\"\"Return a context manager with a print method. (Optional)\"\"\"",
            "        # Do not print to files/pipes, loggers, or when --no-progress is used",
            "        if not self._ydl or self._ydl.params.get('noprogress') or self._ydl.params.get('logger'):",
            "            return",
            "        file = self._ydl._out_files.error",
            "        try:",
            "            if not file.isatty():",
            "                return",
            "        except BaseException:",
            "            return",
            "        return self.ProgressBar(file, preserve_output=False)",
            "",
            "",
            "def _create_progress_bar(logger):",
            "    if hasattr(logger, 'progress_bar'):",
            "        printer = logger.progress_bar()",
            "        if printer:",
            "            return printer",
            "    printer = QuietMultilinePrinter()",
            "    printer.print = lambda _: None",
            "    return printer",
            "",
            "",
            "def load_cookies(cookie_file, browser_specification, ydl):",
            "    cookie_jars = []",
            "    if browser_specification is not None:",
            "        browser_name, profile, keyring, container = _parse_browser_specification(*browser_specification)",
            "        cookie_jars.append(",
            "            extract_cookies_from_browser(browser_name, profile, YDLLogger(ydl), keyring=keyring, container=container))",
            "",
            "    if cookie_file is not None:",
            "        is_filename = is_path_like(cookie_file)",
            "        if is_filename:",
            "            cookie_file = expand_path(cookie_file)",
            "",
            "        jar = YoutubeDLCookieJar(cookie_file)",
            "        if not is_filename or os.access(cookie_file, os.R_OK):",
            "            jar.load(ignore_discard=True, ignore_expires=True)",
            "        cookie_jars.append(jar)",
            "",
            "    return _merge_cookie_jars(cookie_jars)",
            "",
            "",
            "def extract_cookies_from_browser(browser_name, profile=None, logger=YDLLogger(), *, keyring=None, container=None):",
            "    if browser_name == 'firefox':",
            "        return _extract_firefox_cookies(profile, container, logger)",
            "    elif browser_name == 'safari':",
            "        return _extract_safari_cookies(profile, logger)",
            "    elif browser_name in CHROMIUM_BASED_BROWSERS:",
            "        return _extract_chrome_cookies(browser_name, profile, keyring, logger)",
            "    else:",
            "        raise ValueError(f'unknown browser: {browser_name}')",
            "",
            "",
            "def _extract_firefox_cookies(profile, container, logger):",
            "    logger.info('Extracting cookies from firefox')",
            "    if not sqlite3:",
            "        logger.warning('Cannot extract cookies from firefox without sqlite3 support. '",
            "                       'Please use a python interpreter compiled with sqlite3 support')",
            "        return YoutubeDLCookieJar()",
            "",
            "    if profile is None:",
            "        search_root = _firefox_browser_dir()",
            "    elif _is_path(profile):",
            "        search_root = profile",
            "    else:",
            "        search_root = os.path.join(_firefox_browser_dir(), profile)",
            "",
            "    cookie_database_path = _find_most_recently_used_file(search_root, 'cookies.sqlite', logger)",
            "    if cookie_database_path is None:",
            "        raise FileNotFoundError(f'could not find firefox cookies database in {search_root}')",
            "    logger.debug(f'Extracting cookies from: \"{cookie_database_path}\"')",
            "",
            "    container_id = None",
            "    if container not in (None, 'none'):",
            "        containers_path = os.path.join(os.path.dirname(cookie_database_path), 'containers.json')",
            "        if not os.path.isfile(containers_path) or not os.access(containers_path, os.R_OK):",
            "            raise FileNotFoundError(f'could not read containers.json in {search_root}')",
            "        with open(containers_path) as containers:",
            "            identities = json.load(containers).get('identities', [])",
            "        container_id = next((context.get('userContextId') for context in identities if container in (",
            "            context.get('name'),",
            "            try_call(lambda: re.fullmatch(r'userContext([^\\.]+)\\.label', context['l10nID']).group())",
            "        )), None)",
            "        if not isinstance(container_id, int):",
            "            raise ValueError(f'could not find firefox container \"{container}\" in containers.json')",
            "",
            "    with tempfile.TemporaryDirectory(prefix='yt_dlp') as tmpdir:",
            "        cursor = None",
            "        try:",
            "            cursor = _open_database_copy(cookie_database_path, tmpdir)",
            "            if isinstance(container_id, int):",
            "                logger.debug(",
            "                    f'Only loading cookies from firefox container \"{container}\", ID {container_id}')",
            "                cursor.execute(",
            "                    'SELECT host, name, value, path, expiry, isSecure FROM moz_cookies WHERE originAttributes LIKE ? OR originAttributes LIKE ?',",
            "                    (f'%userContextId={container_id}', f'%userContextId={container_id}&%'))",
            "            elif container == 'none':",
            "                logger.debug('Only loading cookies not belonging to any container')",
            "                cursor.execute(",
            "                    'SELECT host, name, value, path, expiry, isSecure FROM moz_cookies WHERE NOT INSTR(originAttributes,\"userContextId=\")')",
            "            else:",
            "                cursor.execute('SELECT host, name, value, path, expiry, isSecure FROM moz_cookies')",
            "            jar = YoutubeDLCookieJar()",
            "            with _create_progress_bar(logger) as progress_bar:",
            "                table = cursor.fetchall()",
            "                total_cookie_count = len(table)",
            "                for i, (host, name, value, path, expiry, is_secure) in enumerate(table):",
            "                    progress_bar.print(f'Loading cookie {i: 6d}/{total_cookie_count: 6d}')",
            "                    cookie = http.cookiejar.Cookie(",
            "                        version=0, name=name, value=value, port=None, port_specified=False,",
            "                        domain=host, domain_specified=bool(host), domain_initial_dot=host.startswith('.'),",
            "                        path=path, path_specified=bool(path), secure=is_secure, expires=expiry, discard=False,",
            "                        comment=None, comment_url=None, rest={})",
            "                    jar.set_cookie(cookie)",
            "            logger.info(f'Extracted {len(jar)} cookies from firefox')",
            "            return jar",
            "        finally:",
            "            if cursor is not None:",
            "                cursor.connection.close()",
            "",
            "",
            "def _firefox_browser_dir():",
            "    if sys.platform in ('cygwin', 'win32'):",
            "        return os.path.expandvars(R'%APPDATA%\\Mozilla\\Firefox\\Profiles')",
            "    elif sys.platform == 'darwin':",
            "        return os.path.expanduser('~/Library/Application Support/Firefox')",
            "    return os.path.expanduser('~/.mozilla/firefox')",
            "",
            "",
            "def _get_chromium_based_browser_settings(browser_name):",
            "    # https://chromium.googlesource.com/chromium/src/+/HEAD/docs/user_data_dir.md",
            "    if sys.platform in ('cygwin', 'win32'):",
            "        appdata_local = os.path.expandvars('%LOCALAPPDATA%')",
            "        appdata_roaming = os.path.expandvars('%APPDATA%')",
            "        browser_dir = {",
            "            'brave': os.path.join(appdata_local, R'BraveSoftware\\Brave-Browser\\User Data'),",
            "            'chrome': os.path.join(appdata_local, R'Google\\Chrome\\User Data'),",
            "            'chromium': os.path.join(appdata_local, R'Chromium\\User Data'),",
            "            'edge': os.path.join(appdata_local, R'Microsoft\\Edge\\User Data'),",
            "            'opera': os.path.join(appdata_roaming, R'Opera Software\\Opera Stable'),",
            "            'vivaldi': os.path.join(appdata_local, R'Vivaldi\\User Data'),",
            "        }[browser_name]",
            "",
            "    elif sys.platform == 'darwin':",
            "        appdata = os.path.expanduser('~/Library/Application Support')",
            "        browser_dir = {",
            "            'brave': os.path.join(appdata, 'BraveSoftware/Brave-Browser'),",
            "            'chrome': os.path.join(appdata, 'Google/Chrome'),",
            "            'chromium': os.path.join(appdata, 'Chromium'),",
            "            'edge': os.path.join(appdata, 'Microsoft Edge'),",
            "            'opera': os.path.join(appdata, 'com.operasoftware.Opera'),",
            "            'vivaldi': os.path.join(appdata, 'Vivaldi'),",
            "        }[browser_name]",
            "",
            "    else:",
            "        config = _config_home()",
            "        browser_dir = {",
            "            'brave': os.path.join(config, 'BraveSoftware/Brave-Browser'),",
            "            'chrome': os.path.join(config, 'google-chrome'),",
            "            'chromium': os.path.join(config, 'chromium'),",
            "            'edge': os.path.join(config, 'microsoft-edge'),",
            "            'opera': os.path.join(config, 'opera'),",
            "            'vivaldi': os.path.join(config, 'vivaldi'),",
            "        }[browser_name]",
            "",
            "    # Linux keyring names can be determined by snooping on dbus while opening the browser in KDE:",
            "    # dbus-monitor \"interface='org.kde.KWallet'\" \"type=method_return\"",
            "    keyring_name = {",
            "        'brave': 'Brave',",
            "        'chrome': 'Chrome',",
            "        'chromium': 'Chromium',",
            "        'edge': 'Microsoft Edge' if sys.platform == 'darwin' else 'Chromium',",
            "        'opera': 'Opera' if sys.platform == 'darwin' else 'Chromium',",
            "        'vivaldi': 'Vivaldi' if sys.platform == 'darwin' else 'Chrome',",
            "    }[browser_name]",
            "",
            "    browsers_without_profiles = {'opera'}",
            "",
            "    return {",
            "        'browser_dir': browser_dir,",
            "        'keyring_name': keyring_name,",
            "        'supports_profiles': browser_name not in browsers_without_profiles",
            "    }",
            "",
            "",
            "def _extract_chrome_cookies(browser_name, profile, keyring, logger):",
            "    logger.info(f'Extracting cookies from {browser_name}')",
            "",
            "    if not sqlite3:",
            "        logger.warning(f'Cannot extract cookies from {browser_name} without sqlite3 support. '",
            "                       'Please use a python interpreter compiled with sqlite3 support')",
            "        return YoutubeDLCookieJar()",
            "",
            "    config = _get_chromium_based_browser_settings(browser_name)",
            "",
            "    if profile is None:",
            "        search_root = config['browser_dir']",
            "    elif _is_path(profile):",
            "        search_root = profile",
            "        config['browser_dir'] = os.path.dirname(profile) if config['supports_profiles'] else profile",
            "    else:",
            "        if config['supports_profiles']:",
            "            search_root = os.path.join(config['browser_dir'], profile)",
            "        else:",
            "            logger.error(f'{browser_name} does not support profiles')",
            "            search_root = config['browser_dir']",
            "",
            "    cookie_database_path = _find_most_recently_used_file(search_root, 'Cookies', logger)",
            "    if cookie_database_path is None:",
            "        raise FileNotFoundError(f'could not find {browser_name} cookies database in \"{search_root}\"')",
            "    logger.debug(f'Extracting cookies from: \"{cookie_database_path}\"')",
            "",
            "    decryptor = get_cookie_decryptor(config['browser_dir'], config['keyring_name'], logger, keyring=keyring)",
            "",
            "    with tempfile.TemporaryDirectory(prefix='yt_dlp') as tmpdir:",
            "        cursor = None",
            "        try:",
            "            cursor = _open_database_copy(cookie_database_path, tmpdir)",
            "            cursor.connection.text_factory = bytes",
            "            column_names = _get_column_names(cursor, 'cookies')",
            "            secure_column = 'is_secure' if 'is_secure' in column_names else 'secure'",
            "            cursor.execute(f'SELECT host_key, name, value, encrypted_value, path, expires_utc, {secure_column} FROM cookies')",
            "            jar = YoutubeDLCookieJar()",
            "            failed_cookies = 0",
            "            unencrypted_cookies = 0",
            "            with _create_progress_bar(logger) as progress_bar:",
            "                table = cursor.fetchall()",
            "                total_cookie_count = len(table)",
            "                for i, line in enumerate(table):",
            "                    progress_bar.print(f'Loading cookie {i: 6d}/{total_cookie_count: 6d}')",
            "                    is_encrypted, cookie = _process_chrome_cookie(decryptor, *line)",
            "                    if not cookie:",
            "                        failed_cookies += 1",
            "                        continue",
            "                    elif not is_encrypted:",
            "                        unencrypted_cookies += 1",
            "                    jar.set_cookie(cookie)",
            "            if failed_cookies > 0:",
            "                failed_message = f' ({failed_cookies} could not be decrypted)'",
            "            else:",
            "                failed_message = ''",
            "            logger.info(f'Extracted {len(jar)} cookies from {browser_name}{failed_message}')",
            "            counts = decryptor._cookie_counts.copy()",
            "            counts['unencrypted'] = unencrypted_cookies",
            "            logger.debug(f'cookie version breakdown: {counts}')",
            "            return jar",
            "        finally:",
            "            if cursor is not None:",
            "                cursor.connection.close()",
            "",
            "",
            "def _process_chrome_cookie(decryptor, host_key, name, value, encrypted_value, path, expires_utc, is_secure):",
            "    host_key = host_key.decode()",
            "    name = name.decode()",
            "    value = value.decode()",
            "    path = path.decode()",
            "    is_encrypted = not value and encrypted_value",
            "",
            "    if is_encrypted:",
            "        value = decryptor.decrypt(encrypted_value)",
            "        if value is None:",
            "            return is_encrypted, None",
            "",
            "    return is_encrypted, http.cookiejar.Cookie(",
            "        version=0, name=name, value=value, port=None, port_specified=False,",
            "        domain=host_key, domain_specified=bool(host_key), domain_initial_dot=host_key.startswith('.'),",
            "        path=path, path_specified=bool(path), secure=is_secure, expires=expires_utc, discard=False,",
            "        comment=None, comment_url=None, rest={})",
            "",
            "",
            "class ChromeCookieDecryptor:",
            "    \"\"\"",
            "    Overview:",
            "",
            "        Linux:",
            "        - cookies are either v10 or v11",
            "            - v10: AES-CBC encrypted with a fixed key",
            "                - also attempts empty password if decryption fails",
            "            - v11: AES-CBC encrypted with an OS protected key (keyring)",
            "                - also attempts empty password if decryption fails",
            "            - v11 keys can be stored in various places depending on the activate desktop environment [2]",
            "",
            "        Mac:",
            "        - cookies are either v10 or not v10",
            "            - v10: AES-CBC encrypted with an OS protected key (keyring) and more key derivation iterations than linux",
            "            - not v10: 'old data' stored as plaintext",
            "",
            "        Windows:",
            "        - cookies are either v10 or not v10",
            "            - v10: AES-GCM encrypted with a key which is encrypted with DPAPI",
            "            - not v10: encrypted with DPAPI",
            "",
            "    Sources:",
            "    - [1] https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/",
            "    - [2] https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/key_storage_linux.cc",
            "        - KeyStorageLinux::CreateService",
            "    \"\"\"",
            "",
            "    _cookie_counts = {}",
            "",
            "    def decrypt(self, encrypted_value):",
            "        raise NotImplementedError('Must be implemented by sub classes')",
            "",
            "",
            "def get_cookie_decryptor(browser_root, browser_keyring_name, logger, *, keyring=None):",
            "    if sys.platform == 'darwin':",
            "        return MacChromeCookieDecryptor(browser_keyring_name, logger)",
            "    elif sys.platform in ('win32', 'cygwin'):",
            "        return WindowsChromeCookieDecryptor(browser_root, logger)",
            "    return LinuxChromeCookieDecryptor(browser_keyring_name, logger, keyring=keyring)",
            "",
            "",
            "class LinuxChromeCookieDecryptor(ChromeCookieDecryptor):",
            "    def __init__(self, browser_keyring_name, logger, *, keyring=None):",
            "        self._logger = logger",
            "        self._v10_key = self.derive_key(b'peanuts')",
            "        self._empty_key = self.derive_key(b'')",
            "        self._cookie_counts = {'v10': 0, 'v11': 0, 'other': 0}",
            "        self._browser_keyring_name = browser_keyring_name",
            "        self._keyring = keyring",
            "",
            "    @functools.cached_property",
            "    def _v11_key(self):",
            "        password = _get_linux_keyring_password(self._browser_keyring_name, self._keyring, self._logger)",
            "        return None if password is None else self.derive_key(password)",
            "",
            "    @staticmethod",
            "    def derive_key(password):",
            "        # values from",
            "        # https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/os_crypt_linux.cc",
            "        return pbkdf2_sha1(password, salt=b'saltysalt', iterations=1, key_length=16)",
            "",
            "    def decrypt(self, encrypted_value):",
            "        \"\"\"",
            "",
            "        following the same approach as the fix in [1]: if cookies fail to decrypt then attempt to decrypt",
            "        with an empty password. The failure detection is not the same as what chromium uses so the",
            "        results won't be perfect",
            "",
            "        References:",
            "            - [1] https://chromium.googlesource.com/chromium/src/+/bbd54702284caca1f92d656fdcadf2ccca6f4165%5E%21/",
            "                - a bugfix to try an empty password as a fallback",
            "        \"\"\"",
            "        version = encrypted_value[:3]",
            "        ciphertext = encrypted_value[3:]",
            "",
            "        if version == b'v10':",
            "            self._cookie_counts['v10'] += 1",
            "            return _decrypt_aes_cbc_multi(ciphertext, (self._v10_key, self._empty_key), self._logger)",
            "",
            "        elif version == b'v11':",
            "            self._cookie_counts['v11'] += 1",
            "            if self._v11_key is None:",
            "                self._logger.warning('cannot decrypt v11 cookies: no key found', only_once=True)",
            "                return None",
            "            return _decrypt_aes_cbc_multi(ciphertext, (self._v11_key, self._empty_key), self._logger)",
            "",
            "        else:",
            "            self._logger.warning(f'unknown cookie version: \"{version}\"', only_once=True)",
            "            self._cookie_counts['other'] += 1",
            "            return None",
            "",
            "",
            "class MacChromeCookieDecryptor(ChromeCookieDecryptor):",
            "    def __init__(self, browser_keyring_name, logger):",
            "        self._logger = logger",
            "        password = _get_mac_keyring_password(browser_keyring_name, logger)",
            "        self._v10_key = None if password is None else self.derive_key(password)",
            "        self._cookie_counts = {'v10': 0, 'other': 0}",
            "",
            "    @staticmethod",
            "    def derive_key(password):",
            "        # values from",
            "        # https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/os_crypt_mac.mm",
            "        return pbkdf2_sha1(password, salt=b'saltysalt', iterations=1003, key_length=16)",
            "",
            "    def decrypt(self, encrypted_value):",
            "        version = encrypted_value[:3]",
            "        ciphertext = encrypted_value[3:]",
            "",
            "        if version == b'v10':",
            "            self._cookie_counts['v10'] += 1",
            "            if self._v10_key is None:",
            "                self._logger.warning('cannot decrypt v10 cookies: no key found', only_once=True)",
            "                return None",
            "",
            "            return _decrypt_aes_cbc_multi(ciphertext, (self._v10_key,), self._logger)",
            "",
            "        else:",
            "            self._cookie_counts['other'] += 1",
            "            # other prefixes are considered 'old data' which were stored as plaintext",
            "            # https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/os_crypt_mac.mm",
            "            return encrypted_value",
            "",
            "",
            "class WindowsChromeCookieDecryptor(ChromeCookieDecryptor):",
            "    def __init__(self, browser_root, logger):",
            "        self._logger = logger",
            "        self._v10_key = _get_windows_v10_key(browser_root, logger)",
            "        self._cookie_counts = {'v10': 0, 'other': 0}",
            "",
            "    def decrypt(self, encrypted_value):",
            "        version = encrypted_value[:3]",
            "        ciphertext = encrypted_value[3:]",
            "",
            "        if version == b'v10':",
            "            self._cookie_counts['v10'] += 1",
            "            if self._v10_key is None:",
            "                self._logger.warning('cannot decrypt v10 cookies: no key found', only_once=True)",
            "                return None",
            "",
            "            # https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/os_crypt_win.cc",
            "            #   kNonceLength",
            "            nonce_length = 96 // 8",
            "            # boringssl",
            "            #   EVP_AEAD_AES_GCM_TAG_LEN",
            "            authentication_tag_length = 16",
            "",
            "            raw_ciphertext = ciphertext",
            "            nonce = raw_ciphertext[:nonce_length]",
            "            ciphertext = raw_ciphertext[nonce_length:-authentication_tag_length]",
            "            authentication_tag = raw_ciphertext[-authentication_tag_length:]",
            "",
            "            return _decrypt_aes_gcm(ciphertext, self._v10_key, nonce, authentication_tag, self._logger)",
            "",
            "        else:",
            "            self._cookie_counts['other'] += 1",
            "            # any other prefix means the data is DPAPI encrypted",
            "            # https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/os_crypt_win.cc",
            "            return _decrypt_windows_dpapi(encrypted_value, self._logger).decode()",
            "",
            "",
            "def _extract_safari_cookies(profile, logger):",
            "    if sys.platform != 'darwin':",
            "        raise ValueError(f'unsupported platform: {sys.platform}')",
            "",
            "    if profile:",
            "        cookies_path = os.path.expanduser(profile)",
            "        if not os.path.isfile(cookies_path):",
            "            raise FileNotFoundError('custom safari cookies database not found')",
            "",
            "    else:",
            "        cookies_path = os.path.expanduser('~/Library/Cookies/Cookies.binarycookies')",
            "",
            "        if not os.path.isfile(cookies_path):",
            "            logger.debug('Trying secondary cookie location')",
            "            cookies_path = os.path.expanduser('~/Library/Containers/com.apple.Safari/Data/Library/Cookies/Cookies.binarycookies')",
            "            if not os.path.isfile(cookies_path):",
            "                raise FileNotFoundError('could not find safari cookies database')",
            "",
            "    with open(cookies_path, 'rb') as f:",
            "        cookies_data = f.read()",
            "",
            "    jar = parse_safari_cookies(cookies_data, logger=logger)",
            "    logger.info(f'Extracted {len(jar)} cookies from safari')",
            "    return jar",
            "",
            "",
            "class ParserError(Exception):",
            "    pass",
            "",
            "",
            "class DataParser:",
            "    def __init__(self, data, logger):",
            "        self._data = data",
            "        self.cursor = 0",
            "        self._logger = logger",
            "",
            "    def read_bytes(self, num_bytes):",
            "        if num_bytes < 0:",
            "            raise ParserError(f'invalid read of {num_bytes} bytes')",
            "        end = self.cursor + num_bytes",
            "        if end > len(self._data):",
            "            raise ParserError('reached end of input')",
            "        data = self._data[self.cursor:end]",
            "        self.cursor = end",
            "        return data",
            "",
            "    def expect_bytes(self, expected_value, message):",
            "        value = self.read_bytes(len(expected_value))",
            "        if value != expected_value:",
            "            raise ParserError(f'unexpected value: {value} != {expected_value} ({message})')",
            "",
            "    def read_uint(self, big_endian=False):",
            "        data_format = '>I' if big_endian else '<I'",
            "        return struct.unpack(data_format, self.read_bytes(4))[0]",
            "",
            "    def read_double(self, big_endian=False):",
            "        data_format = '>d' if big_endian else '<d'",
            "        return struct.unpack(data_format, self.read_bytes(8))[0]",
            "",
            "    def read_cstring(self):",
            "        buffer = []",
            "        while True:",
            "            c = self.read_bytes(1)",
            "            if c == b'\\x00':",
            "                return b''.join(buffer).decode()",
            "            else:",
            "                buffer.append(c)",
            "",
            "    def skip(self, num_bytes, description='unknown'):",
            "        if num_bytes > 0:",
            "            self._logger.debug(f'skipping {num_bytes} bytes ({description}): {self.read_bytes(num_bytes)!r}')",
            "        elif num_bytes < 0:",
            "            raise ParserError(f'invalid skip of {num_bytes} bytes')",
            "",
            "    def skip_to(self, offset, description='unknown'):",
            "        self.skip(offset - self.cursor, description)",
            "",
            "    def skip_to_end(self, description='unknown'):",
            "        self.skip_to(len(self._data), description)",
            "",
            "",
            "def _mac_absolute_time_to_posix(timestamp):",
            "    return int((datetime(2001, 1, 1, 0, 0, tzinfo=timezone.utc) + timedelta(seconds=timestamp)).timestamp())",
            "",
            "",
            "def _parse_safari_cookies_header(data, logger):",
            "    p = DataParser(data, logger)",
            "    p.expect_bytes(b'cook', 'database signature')",
            "    number_of_pages = p.read_uint(big_endian=True)",
            "    page_sizes = [p.read_uint(big_endian=True) for _ in range(number_of_pages)]",
            "    return page_sizes, p.cursor",
            "",
            "",
            "def _parse_safari_cookies_page(data, jar, logger):",
            "    p = DataParser(data, logger)",
            "    p.expect_bytes(b'\\x00\\x00\\x01\\x00', 'page signature')",
            "    number_of_cookies = p.read_uint()",
            "    record_offsets = [p.read_uint() for _ in range(number_of_cookies)]",
            "    if number_of_cookies == 0:",
            "        logger.debug(f'a cookies page of size {len(data)} has no cookies')",
            "        return",
            "",
            "    p.skip_to(record_offsets[0], 'unknown page header field')",
            "",
            "    with _create_progress_bar(logger) as progress_bar:",
            "        for i, record_offset in enumerate(record_offsets):",
            "            progress_bar.print(f'Loading cookie {i: 6d}/{number_of_cookies: 6d}')",
            "            p.skip_to(record_offset, 'space between records')",
            "            record_length = _parse_safari_cookies_record(data[record_offset:], jar, logger)",
            "            p.read_bytes(record_length)",
            "    p.skip_to_end('space in between pages')",
            "",
            "",
            "def _parse_safari_cookies_record(data, jar, logger):",
            "    p = DataParser(data, logger)",
            "    record_size = p.read_uint()",
            "    p.skip(4, 'unknown record field 1')",
            "    flags = p.read_uint()",
            "    is_secure = bool(flags & 0x0001)",
            "    p.skip(4, 'unknown record field 2')",
            "    domain_offset = p.read_uint()",
            "    name_offset = p.read_uint()",
            "    path_offset = p.read_uint()",
            "    value_offset = p.read_uint()",
            "    p.skip(8, 'unknown record field 3')",
            "    expiration_date = _mac_absolute_time_to_posix(p.read_double())",
            "    _creation_date = _mac_absolute_time_to_posix(p.read_double())  # noqa: F841",
            "",
            "    try:",
            "        p.skip_to(domain_offset)",
            "        domain = p.read_cstring()",
            "",
            "        p.skip_to(name_offset)",
            "        name = p.read_cstring()",
            "",
            "        p.skip_to(path_offset)",
            "        path = p.read_cstring()",
            "",
            "        p.skip_to(value_offset)",
            "        value = p.read_cstring()",
            "    except UnicodeDecodeError:",
            "        logger.warning('failed to parse Safari cookie because UTF-8 decoding failed', only_once=True)",
            "        return record_size",
            "",
            "    p.skip_to(record_size, 'space at the end of the record')",
            "",
            "    cookie = http.cookiejar.Cookie(",
            "        version=0, name=name, value=value, port=None, port_specified=False,",
            "        domain=domain, domain_specified=bool(domain), domain_initial_dot=domain.startswith('.'),",
            "        path=path, path_specified=bool(path), secure=is_secure, expires=expiration_date, discard=False,",
            "        comment=None, comment_url=None, rest={})",
            "    jar.set_cookie(cookie)",
            "    return record_size",
            "",
            "",
            "def parse_safari_cookies(data, jar=None, logger=YDLLogger()):",
            "    \"\"\"",
            "    References:",
            "        - https://github.com/libyal/dtformats/blob/main/documentation/Safari%20Cookies.asciidoc",
            "            - this data appears to be out of date but the important parts of the database structure is the same",
            "            - there are a few bytes here and there which are skipped during parsing",
            "    \"\"\"",
            "    if jar is None:",
            "        jar = YoutubeDLCookieJar()",
            "    page_sizes, body_start = _parse_safari_cookies_header(data, logger)",
            "    p = DataParser(data[body_start:], logger)",
            "    for page_size in page_sizes:",
            "        _parse_safari_cookies_page(p.read_bytes(page_size), jar, logger)",
            "    p.skip_to_end('footer')",
            "    return jar",
            "",
            "",
            "class _LinuxDesktopEnvironment(Enum):",
            "    \"\"\"",
            "    https://chromium.googlesource.com/chromium/src/+/refs/heads/main/base/nix/xdg_util.h",
            "    DesktopEnvironment",
            "    \"\"\"",
            "    OTHER = auto()",
            "    CINNAMON = auto()",
            "    DEEPIN = auto()",
            "    GNOME = auto()",
            "    KDE3 = auto()",
            "    KDE4 = auto()",
            "    KDE5 = auto()",
            "    KDE6 = auto()",
            "    PANTHEON = auto()",
            "    UKUI = auto()",
            "    UNITY = auto()",
            "    XFCE = auto()",
            "    LXQT = auto()",
            "",
            "",
            "class _LinuxKeyring(Enum):",
            "    \"\"\"",
            "    https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/key_storage_util_linux.h",
            "    SelectedLinuxBackend",
            "    \"\"\"",
            "    KWALLET = auto()  # KDE4",
            "    KWALLET5 = auto()",
            "    KWALLET6 = auto()",
            "    GNOMEKEYRING = auto()",
            "    BASICTEXT = auto()",
            "",
            "",
            "SUPPORTED_KEYRINGS = _LinuxKeyring.__members__.keys()",
            "",
            "",
            "def _get_linux_desktop_environment(env, logger):",
            "    \"\"\"",
            "    https://chromium.googlesource.com/chromium/src/+/refs/heads/main/base/nix/xdg_util.cc",
            "    GetDesktopEnvironment",
            "    \"\"\"",
            "    xdg_current_desktop = env.get('XDG_CURRENT_DESKTOP', None)",
            "    desktop_session = env.get('DESKTOP_SESSION', None)",
            "    if xdg_current_desktop is not None:",
            "        xdg_current_desktop = xdg_current_desktop.split(':')[0].strip()",
            "",
            "        if xdg_current_desktop == 'Unity':",
            "            if desktop_session is not None and 'gnome-fallback' in desktop_session:",
            "                return _LinuxDesktopEnvironment.GNOME",
            "            else:",
            "                return _LinuxDesktopEnvironment.UNITY",
            "        elif xdg_current_desktop == 'Deepin':",
            "            return _LinuxDesktopEnvironment.DEEPIN",
            "        elif xdg_current_desktop == 'GNOME':",
            "            return _LinuxDesktopEnvironment.GNOME",
            "        elif xdg_current_desktop == 'X-Cinnamon':",
            "            return _LinuxDesktopEnvironment.CINNAMON",
            "        elif xdg_current_desktop == 'KDE':",
            "            kde_version = env.get('KDE_SESSION_VERSION', None)",
            "            if kde_version == '5':",
            "                return _LinuxDesktopEnvironment.KDE5",
            "            elif kde_version == '6':",
            "                return _LinuxDesktopEnvironment.KDE6",
            "            elif kde_version == '4':",
            "                return _LinuxDesktopEnvironment.KDE4",
            "            else:",
            "                logger.info(f'unknown KDE version: \"{kde_version}\". Assuming KDE4')",
            "                return _LinuxDesktopEnvironment.KDE4",
            "        elif xdg_current_desktop == 'Pantheon':",
            "            return _LinuxDesktopEnvironment.PANTHEON",
            "        elif xdg_current_desktop == 'XFCE':",
            "            return _LinuxDesktopEnvironment.XFCE",
            "        elif xdg_current_desktop == 'UKUI':",
            "            return _LinuxDesktopEnvironment.UKUI",
            "        elif xdg_current_desktop == 'LXQt':",
            "            return _LinuxDesktopEnvironment.LXQT",
            "        else:",
            "            logger.info(f'XDG_CURRENT_DESKTOP is set to an unknown value: \"{xdg_current_desktop}\"')",
            "",
            "    elif desktop_session is not None:",
            "        if desktop_session == 'deepin':",
            "            return _LinuxDesktopEnvironment.DEEPIN",
            "        elif desktop_session in ('mate', 'gnome'):",
            "            return _LinuxDesktopEnvironment.GNOME",
            "        elif desktop_session in ('kde4', 'kde-plasma'):",
            "            return _LinuxDesktopEnvironment.KDE4",
            "        elif desktop_session == 'kde':",
            "            if 'KDE_SESSION_VERSION' in env:",
            "                return _LinuxDesktopEnvironment.KDE4",
            "            else:",
            "                return _LinuxDesktopEnvironment.KDE3",
            "        elif 'xfce' in desktop_session or desktop_session == 'xubuntu':",
            "            return _LinuxDesktopEnvironment.XFCE",
            "        elif desktop_session == 'ukui':",
            "            return _LinuxDesktopEnvironment.UKUI",
            "        else:",
            "            logger.info(f'DESKTOP_SESSION is set to an unknown value: \"{desktop_session}\"')",
            "",
            "    else:",
            "        if 'GNOME_DESKTOP_SESSION_ID' in env:",
            "            return _LinuxDesktopEnvironment.GNOME",
            "        elif 'KDE_FULL_SESSION' in env:",
            "            if 'KDE_SESSION_VERSION' in env:",
            "                return _LinuxDesktopEnvironment.KDE4",
            "            else:",
            "                return _LinuxDesktopEnvironment.KDE3",
            "    return _LinuxDesktopEnvironment.OTHER",
            "",
            "",
            "def _choose_linux_keyring(logger):",
            "    \"\"\"",
            "    SelectBackend in [1]",
            "",
            "    There is currently support for forcing chromium to use BASIC_TEXT by creating a file called",
            "    `Disable Local Encryption` [1] in the user data dir. The function to write this file (`WriteBackendUse()` [1])",
            "    does not appear to be called anywhere other than in tests, so the user would have to create this file manually",
            "    and so would be aware enough to tell yt-dlp to use the BASIC_TEXT keyring.",
            "",
            "    References:",
            "        - [1] https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/key_storage_util_linux.cc",
            "    \"\"\"",
            "    desktop_environment = _get_linux_desktop_environment(os.environ, logger)",
            "    logger.debug(f'detected desktop environment: {desktop_environment.name}')",
            "    if desktop_environment == _LinuxDesktopEnvironment.KDE4:",
            "        linux_keyring = _LinuxKeyring.KWALLET",
            "    elif desktop_environment == _LinuxDesktopEnvironment.KDE5:",
            "        linux_keyring = _LinuxKeyring.KWALLET5",
            "    elif desktop_environment == _LinuxDesktopEnvironment.KDE6:",
            "        linux_keyring = _LinuxKeyring.KWALLET6",
            "    elif desktop_environment in (",
            "        _LinuxDesktopEnvironment.KDE3, _LinuxDesktopEnvironment.LXQT, _LinuxDesktopEnvironment.OTHER",
            "    ):",
            "        linux_keyring = _LinuxKeyring.BASICTEXT",
            "    else:",
            "        linux_keyring = _LinuxKeyring.GNOMEKEYRING",
            "    return linux_keyring",
            "",
            "",
            "def _get_kwallet_network_wallet(keyring, logger):",
            "    \"\"\" The name of the wallet used to store network passwords.",
            "",
            "    https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/kwallet_dbus.cc",
            "    KWalletDBus::NetworkWallet",
            "    which does a dbus call to the following function:",
            "    https://api.kde.org/frameworks/kwallet/html/classKWallet_1_1Wallet.html",
            "    Wallet::NetworkWallet",
            "    \"\"\"",
            "    default_wallet = 'kdewallet'",
            "    try:",
            "        if keyring == _LinuxKeyring.KWALLET:",
            "            service_name = 'org.kde.kwalletd'",
            "            wallet_path = '/modules/kwalletd'",
            "        elif keyring == _LinuxKeyring.KWALLET5:",
            "            service_name = 'org.kde.kwalletd5'",
            "            wallet_path = '/modules/kwalletd5'",
            "        elif keyring == _LinuxKeyring.KWALLET6:",
            "            service_name = 'org.kde.kwalletd6'",
            "            wallet_path = '/modules/kwalletd6'",
            "        else:",
            "            raise ValueError(keyring)",
            "",
            "        stdout, _, returncode = Popen.run([",
            "            'dbus-send', '--session', '--print-reply=literal',",
            "            f'--dest={service_name}',",
            "            wallet_path,",
            "            'org.kde.KWallet.networkWallet'",
            "        ], text=True, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)",
            "",
            "        if returncode:",
            "            logger.warning('failed to read NetworkWallet')",
            "            return default_wallet",
            "        else:",
            "            logger.debug(f'NetworkWallet = \"{stdout.strip()}\"')",
            "            return stdout.strip()",
            "    except Exception as e:",
            "        logger.warning(f'exception while obtaining NetworkWallet: {e}')",
            "        return default_wallet",
            "",
            "",
            "def _get_kwallet_password(browser_keyring_name, keyring, logger):",
            "    logger.debug(f'using kwallet-query to obtain password from {keyring.name}')",
            "",
            "    if shutil.which('kwallet-query') is None:",
            "        logger.error('kwallet-query command not found. KWallet and kwallet-query '",
            "                     'must be installed to read from KWallet. kwallet-query should be'",
            "                     'included in the kwallet package for your distribution')",
            "        return b''",
            "",
            "    network_wallet = _get_kwallet_network_wallet(keyring, logger)",
            "",
            "    try:",
            "        stdout, _, returncode = Popen.run([",
            "            'kwallet-query',",
            "            '--read-password', f'{browser_keyring_name} Safe Storage',",
            "            '--folder', f'{browser_keyring_name} Keys',",
            "            network_wallet",
            "        ], stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)",
            "",
            "        if returncode:",
            "            logger.error(f'kwallet-query failed with return code {returncode}. '",
            "                         'Please consult the kwallet-query man page for details')",
            "            return b''",
            "        else:",
            "            if stdout.lower().startswith(b'failed to read'):",
            "                logger.debug('failed to read password from kwallet. Using empty string instead')",
            "                # this sometimes occurs in KDE because chrome does not check hasEntry and instead",
            "                # just tries to read the value (which kwallet returns \"\") whereas kwallet-query",
            "                # checks hasEntry. To verify this:",
            "                # dbus-monitor \"interface='org.kde.KWallet'\" \"type=method_return\"",
            "                # while starting chrome.",
            "                # this was identified as a bug later and fixed in",
            "                # https://chromium.googlesource.com/chromium/src/+/bbd54702284caca1f92d656fdcadf2ccca6f4165%5E%21/#F0",
            "                # https://chromium.googlesource.com/chromium/src/+/5463af3c39d7f5b6d11db7fbd51e38cc1974d764",
            "                return b''",
            "            else:",
            "                logger.debug('password found')",
            "                return stdout.rstrip(b'\\n')",
            "    except Exception as e:",
            "        logger.warning(f'exception running kwallet-query: {error_to_str(e)}')",
            "        return b''",
            "",
            "",
            "def _get_gnome_keyring_password(browser_keyring_name, logger):",
            "    if not secretstorage:",
            "        logger.error(f'secretstorage not available {_SECRETSTORAGE_UNAVAILABLE_REASON}')",
            "        return b''",
            "    # the Gnome keyring does not seem to organise keys in the same way as KWallet,",
            "    # using `dbus-monitor` during startup, it can be observed that chromium lists all keys",
            "    # and presumably searches for its key in the list. It appears that we must do the same.",
            "    # https://github.com/jaraco/keyring/issues/556",
            "    with contextlib.closing(secretstorage.dbus_init()) as con:",
            "        col = secretstorage.get_default_collection(con)",
            "        for item in col.get_all_items():",
            "            if item.get_label() == f'{browser_keyring_name} Safe Storage':",
            "                return item.get_secret()",
            "        else:",
            "            logger.error('failed to read from keyring')",
            "            return b''",
            "",
            "",
            "def _get_linux_keyring_password(browser_keyring_name, keyring, logger):",
            "    # note: chrome/chromium can be run with the following flags to determine which keyring backend",
            "    # it has chosen to use",
            "    # chromium --enable-logging=stderr --v=1 2>&1 | grep key_storage_",
            "    # Chromium supports a flag: --password-store=<basic|gnome|kwallet> so the automatic detection",
            "    # will not be sufficient in all cases.",
            "",
            "    keyring = _LinuxKeyring[keyring] if keyring else _choose_linux_keyring(logger)",
            "    logger.debug(f'Chosen keyring: {keyring.name}')",
            "",
            "    if keyring in (_LinuxKeyring.KWALLET, _LinuxKeyring.KWALLET5, _LinuxKeyring.KWALLET6):",
            "        return _get_kwallet_password(browser_keyring_name, keyring, logger)",
            "    elif keyring == _LinuxKeyring.GNOMEKEYRING:",
            "        return _get_gnome_keyring_password(browser_keyring_name, logger)",
            "    elif keyring == _LinuxKeyring.BASICTEXT:",
            "        # when basic text is chosen, all cookies are stored as v10 (so no keyring password is required)",
            "        return None",
            "    assert False, f'Unknown keyring {keyring}'",
            "",
            "",
            "def _get_mac_keyring_password(browser_keyring_name, logger):",
            "    logger.debug('using find-generic-password to obtain password from OSX keychain')",
            "    try:",
            "        stdout, _, returncode = Popen.run(",
            "            ['security', 'find-generic-password',",
            "             '-w',  # write password to stdout",
            "             '-a', browser_keyring_name,  # match 'account'",
            "             '-s', f'{browser_keyring_name} Safe Storage'],  # match 'service'",
            "            stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)",
            "        if returncode:",
            "            logger.warning('find-generic-password failed')",
            "            return None",
            "        return stdout.rstrip(b'\\n')",
            "    except Exception as e:",
            "        logger.warning(f'exception running find-generic-password: {error_to_str(e)}')",
            "        return None",
            "",
            "",
            "def _get_windows_v10_key(browser_root, logger):",
            "    \"\"\"",
            "    References:",
            "        - [1] https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/os_crypt_win.cc",
            "    \"\"\"",
            "    path = _find_most_recently_used_file(browser_root, 'Local State', logger)",
            "    if path is None:",
            "        logger.error('could not find local state file')",
            "        return None",
            "    logger.debug(f'Found local state file at \"{path}\"')",
            "    with open(path, encoding='utf8') as f:",
            "        data = json.load(f)",
            "    try:",
            "        # kOsCryptEncryptedKeyPrefName in [1]",
            "        base64_key = data['os_crypt']['encrypted_key']",
            "    except KeyError:",
            "        logger.error('no encrypted key in Local State')",
            "        return None",
            "    encrypted_key = base64.b64decode(base64_key)",
            "    # kDPAPIKeyPrefix in [1]",
            "    prefix = b'DPAPI'",
            "    if not encrypted_key.startswith(prefix):",
            "        logger.error('invalid key')",
            "        return None",
            "    return _decrypt_windows_dpapi(encrypted_key[len(prefix):], logger)",
            "",
            "",
            "def pbkdf2_sha1(password, salt, iterations, key_length):",
            "    return pbkdf2_hmac('sha1', password, salt, iterations, key_length)",
            "",
            "",
            "def _decrypt_aes_cbc_multi(ciphertext, keys, logger, initialization_vector=b' ' * 16):",
            "    for key in keys:",
            "        plaintext = unpad_pkcs7(aes_cbc_decrypt_bytes(ciphertext, key, initialization_vector))",
            "        try:",
            "            return plaintext.decode()",
            "        except UnicodeDecodeError:",
            "            pass",
            "    logger.warning('failed to decrypt cookie (AES-CBC) because UTF-8 decoding failed. Possibly the key is wrong?', only_once=True)",
            "    return None",
            "",
            "",
            "def _decrypt_aes_gcm(ciphertext, key, nonce, authentication_tag, logger):",
            "    try:",
            "        plaintext = aes_gcm_decrypt_and_verify_bytes(ciphertext, key, authentication_tag, nonce)",
            "    except ValueError:",
            "        logger.warning('failed to decrypt cookie (AES-GCM) because the MAC check failed. Possibly the key is wrong?', only_once=True)",
            "        return None",
            "",
            "    try:",
            "        return plaintext.decode()",
            "    except UnicodeDecodeError:",
            "        logger.warning('failed to decrypt cookie (AES-GCM) because UTF-8 decoding failed. Possibly the key is wrong?', only_once=True)",
            "        return None",
            "",
            "",
            "def _decrypt_windows_dpapi(ciphertext, logger):",
            "    \"\"\"",
            "    References:",
            "        - https://docs.microsoft.com/en-us/windows/win32/api/dpapi/nf-dpapi-cryptunprotectdata",
            "    \"\"\"",
            "",
            "    import ctypes",
            "    import ctypes.wintypes",
            "",
            "    class DATA_BLOB(ctypes.Structure):",
            "        _fields_ = [('cbData', ctypes.wintypes.DWORD),",
            "                    ('pbData', ctypes.POINTER(ctypes.c_char))]",
            "",
            "    buffer = ctypes.create_string_buffer(ciphertext)",
            "    blob_in = DATA_BLOB(ctypes.sizeof(buffer), buffer)",
            "    blob_out = DATA_BLOB()",
            "    ret = ctypes.windll.crypt32.CryptUnprotectData(",
            "        ctypes.byref(blob_in),  # pDataIn",
            "        None,  # ppszDataDescr: human readable description of pDataIn",
            "        None,  # pOptionalEntropy: salt?",
            "        None,  # pvReserved: must be NULL",
            "        None,  # pPromptStruct: information about prompts to display",
            "        0,  # dwFlags",
            "        ctypes.byref(blob_out)  # pDataOut",
            "    )",
            "    if not ret:",
            "        logger.warning('failed to decrypt with DPAPI', only_once=True)",
            "        return None",
            "",
            "    result = ctypes.string_at(blob_out.pbData, blob_out.cbData)",
            "    ctypes.windll.kernel32.LocalFree(blob_out.pbData)",
            "    return result",
            "",
            "",
            "def _config_home():",
            "    return os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config'))",
            "",
            "",
            "def _open_database_copy(database_path, tmpdir):",
            "    # cannot open sqlite databases if they are already in use (e.g. by the browser)",
            "    database_copy_path = os.path.join(tmpdir, 'temporary.sqlite')",
            "    shutil.copy(database_path, database_copy_path)",
            "    conn = sqlite3.connect(database_copy_path)",
            "    return conn.cursor()",
            "",
            "",
            "def _get_column_names(cursor, table_name):",
            "    table_info = cursor.execute(f'PRAGMA table_info({table_name})').fetchall()",
            "    return [row[1].decode() for row in table_info]",
            "",
            "",
            "def _find_most_recently_used_file(root, filename, logger):",
            "    # if there are multiple browser profiles, take the most recently used one",
            "    i, paths = 0, []",
            "    with _create_progress_bar(logger) as progress_bar:",
            "        for curr_root, dirs, files in os.walk(root):",
            "            for file in files:",
            "                i += 1",
            "                progress_bar.print(f'Searching for \"{filename}\": {i: 6d} files searched')",
            "                if file == filename:",
            "                    paths.append(os.path.join(curr_root, file))",
            "    return None if not paths else max(paths, key=lambda path: os.lstat(path).st_mtime)",
            "",
            "",
            "def _merge_cookie_jars(jars):",
            "    output_jar = YoutubeDLCookieJar()",
            "    for jar in jars:",
            "        for cookie in jar:",
            "            output_jar.set_cookie(cookie)",
            "        if jar.filename is not None:",
            "            output_jar.filename = jar.filename",
            "    return output_jar",
            "",
            "",
            "def _is_path(value):",
            "    return os.path.sep in value",
            "",
            "",
            "def _parse_browser_specification(browser_name, profile=None, keyring=None, container=None):",
            "    if browser_name not in SUPPORTED_BROWSERS:",
            "        raise ValueError(f'unsupported browser: \"{browser_name}\"')",
            "    if keyring not in (None, *SUPPORTED_KEYRINGS):",
            "        raise ValueError(f'unsupported keyring: \"{keyring}\"')",
            "    if profile is not None and _is_path(expand_path(profile)):",
            "        profile = expand_path(profile)",
            "    return browser_name, profile, keyring, container",
            "",
            "",
            "class LenientSimpleCookie(http.cookies.SimpleCookie):",
            "    \"\"\"More lenient version of http.cookies.SimpleCookie\"\"\"",
            "    # From https://github.com/python/cpython/blob/v3.10.7/Lib/http/cookies.py",
            "    # We use Morsel's legal key chars to avoid errors on setting values",
            "    _LEGAL_KEY_CHARS = r'\\w\\d' + re.escape('!#$%&\\'*+-.:^_`|~')",
            "    _LEGAL_VALUE_CHARS = _LEGAL_KEY_CHARS + re.escape('(),/<=>?@[]{}')",
            "",
            "    _RESERVED = {",
            "        \"expires\",",
            "        \"path\",",
            "        \"comment\",",
            "        \"domain\",",
            "        \"max-age\",",
            "        \"secure\",",
            "        \"httponly\",",
            "        \"version\",",
            "        \"samesite\",",
            "    }",
            "",
            "    _FLAGS = {\"secure\", \"httponly\"}",
            "",
            "    # Added 'bad' group to catch the remaining value",
            "    _COOKIE_PATTERN = re.compile(r\"\"\"",
            "        \\s*                            # Optional whitespace at start of cookie",
            "        (?P<key>                       # Start of group 'key'",
            "        [\"\"\" + _LEGAL_KEY_CHARS + r\"\"\"]+?# Any word of at least one letter",
            "        )                              # End of group 'key'",
            "        (                              # Optional group: there may not be a value.",
            "        \\s*=\\s*                          # Equal Sign",
            "        (                                # Start of potential value",
            "        (?P<val>                           # Start of group 'val'",
            "        \"(?:[^\\\\\"]|\\\\.)*\"                    # Any doublequoted string",
            "        |                                    # or",
            "        \\w{3},\\s[\\w\\d\\s-]{9,11}\\s[\\d:]{8}\\sGMT # Special case for \"expires\" attr",
            "        |                                    # or",
            "        [\"\"\" + _LEGAL_VALUE_CHARS + r\"\"\"]*     # Any word or empty string",
            "        )                                  # End of group 'val'",
            "        |                                  # or",
            "        (?P<bad>(?:\\\\;|[^;])*?)            # 'bad' group fallback for invalid values",
            "        )                                # End of potential value",
            "        )?                             # End of optional value group",
            "        \\s*                            # Any number of spaces.",
            "        (\\s+|;|$)                      # Ending either at space, semicolon, or EOS.",
            "        \"\"\", re.ASCII | re.VERBOSE)",
            "",
            "    def load(self, data):",
            "        # Workaround for https://github.com/yt-dlp/yt-dlp/issues/4776",
            "        if not isinstance(data, str):",
            "            return super().load(data)",
            "",
            "        morsel = None",
            "        for match in self._COOKIE_PATTERN.finditer(data):",
            "            if match.group('bad'):",
            "                morsel = None",
            "                continue",
            "",
            "            key, value = match.group('key', 'val')",
            "",
            "            is_attribute = False",
            "            if key.startswith('$'):",
            "                key = key[1:]",
            "                is_attribute = True",
            "",
            "            lower_key = key.lower()",
            "            if lower_key in self._RESERVED:",
            "                if morsel is None:",
            "                    continue",
            "",
            "                if value is None:",
            "                    if lower_key not in self._FLAGS:",
            "                        morsel = None",
            "                        continue",
            "                    value = True",
            "                else:",
            "                    value, _ = self.value_decode(value)",
            "",
            "                morsel[key] = value",
            "",
            "            elif is_attribute:",
            "                morsel = None",
            "",
            "            elif value is not None:",
            "                morsel = self.get(key, http.cookies.Morsel())",
            "                real_value, coded_value = self.value_decode(value)",
            "                morsel.set(key, real_value, coded_value)",
            "                self[key] = morsel",
            "",
            "            else:",
            "                morsel = None",
            "",
            "",
            "class YoutubeDLCookieJar(http.cookiejar.MozillaCookieJar):",
            "    \"\"\"",
            "    See [1] for cookie file format.",
            "",
            "    1. https://curl.haxx.se/docs/http-cookies.html",
            "    \"\"\"",
            "    _HTTPONLY_PREFIX = '#HttpOnly_'",
            "    _ENTRY_LEN = 7",
            "    _HEADER = '''# Netscape HTTP Cookie File",
            "# This file is generated by yt-dlp.  Do not edit.",
            "",
            "'''",
            "    _CookieFileEntry = collections.namedtuple(",
            "        'CookieFileEntry',",
            "        ('domain_name', 'include_subdomains', 'path', 'https_only', 'expires_at', 'name', 'value'))",
            "",
            "    def __init__(self, filename=None, *args, **kwargs):",
            "        super().__init__(None, *args, **kwargs)",
            "        if is_path_like(filename):",
            "            filename = os.fspath(filename)",
            "        self.filename = filename",
            "",
            "    @staticmethod",
            "    def _true_or_false(cndn):",
            "        return 'TRUE' if cndn else 'FALSE'",
            "",
            "    @contextlib.contextmanager",
            "    def open(self, file, *, write=False):",
            "        if is_path_like(file):",
            "            with open(file, 'w' if write else 'r', encoding='utf-8') as f:",
            "                yield f",
            "        else:",
            "            if write:",
            "                file.truncate(0)",
            "            yield file",
            "",
            "    def _really_save(self, f, ignore_discard=False, ignore_expires=False):",
            "        now = time.time()",
            "        for cookie in self:",
            "            if (not ignore_discard and cookie.discard",
            "                    or not ignore_expires and cookie.is_expired(now)):",
            "                continue",
            "            name, value = cookie.name, cookie.value",
            "            if value is None:",
            "                # cookies.txt regards 'Set-Cookie: foo' as a cookie",
            "                # with no name, whereas http.cookiejar regards it as a",
            "                # cookie with no value.",
            "                name, value = '', name",
            "            f.write('%s\\n' % '\\t'.join((",
            "                cookie.domain,",
            "                self._true_or_false(cookie.domain.startswith('.')),",
            "                cookie.path,",
            "                self._true_or_false(cookie.secure),",
            "                str_or_none(cookie.expires, default=''),",
            "                name, value",
            "            )))",
            "",
            "    def save(self, filename=None, *args, **kwargs):",
            "        \"\"\"",
            "        Save cookies to a file.",
            "        Code is taken from CPython 3.6",
            "        https://github.com/python/cpython/blob/8d999cbf4adea053be6dbb612b9844635c4dfb8e/Lib/http/cookiejar.py#L2091-L2117 \"\"\"",
            "",
            "        if filename is None:",
            "            if self.filename is not None:",
            "                filename = self.filename",
            "            else:",
            "                raise ValueError(http.cookiejar.MISSING_FILENAME_TEXT)",
            "",
            "        # Store session cookies with `expires` set to 0 instead of an empty string",
            "        for cookie in self:",
            "            if cookie.expires is None:",
            "                cookie.expires = 0",
            "",
            "        with self.open(filename, write=True) as f:",
            "            f.write(self._HEADER)",
            "            self._really_save(f, *args, **kwargs)",
            "",
            "    def load(self, filename=None, ignore_discard=False, ignore_expires=False):",
            "        \"\"\"Load cookies from a file.\"\"\"",
            "        if filename is None:",
            "            if self.filename is not None:",
            "                filename = self.filename",
            "            else:",
            "                raise ValueError(http.cookiejar.MISSING_FILENAME_TEXT)",
            "",
            "        def prepare_line(line):",
            "            if line.startswith(self._HTTPONLY_PREFIX):",
            "                line = line[len(self._HTTPONLY_PREFIX):]",
            "            # comments and empty lines are fine",
            "            if line.startswith('#') or not line.strip():",
            "                return line",
            "            cookie_list = line.split('\\t')",
            "            if len(cookie_list) != self._ENTRY_LEN:",
            "                raise http.cookiejar.LoadError('invalid length %d' % len(cookie_list))",
            "            cookie = self._CookieFileEntry(*cookie_list)",
            "            if cookie.expires_at and not cookie.expires_at.isdigit():",
            "                raise http.cookiejar.LoadError('invalid expires at %s' % cookie.expires_at)",
            "            return line",
            "",
            "        cf = io.StringIO()",
            "        with self.open(filename) as f:",
            "            for line in f:",
            "                try:",
            "                    cf.write(prepare_line(line))",
            "                except http.cookiejar.LoadError as e:",
            "                    if f'{line.strip()} '[0] in '[{\"':",
            "                        raise http.cookiejar.LoadError(",
            "                            'Cookies file must be Netscape formatted, not JSON. See  '",
            "                            'https://github.com/yt-dlp/yt-dlp/wiki/FAQ#how-do-i-pass-cookies-to-yt-dlp')",
            "                    write_string(f'WARNING: skipping cookie file entry due to {e}: {line!r}\\n')",
            "                    continue",
            "        cf.seek(0)",
            "        self._really_load(cf, filename, ignore_discard, ignore_expires)",
            "        # Session cookies are denoted by either `expires` field set to",
            "        # an empty string or 0. MozillaCookieJar only recognizes the former",
            "        # (see [1]). So we need force the latter to be recognized as session",
            "        # cookies on our own.",
            "        # Session cookies may be important for cookies-based authentication,",
            "        # e.g. usually, when user does not check 'Remember me' check box while",
            "        # logging in on a site, some important cookies are stored as session",
            "        # cookies so that not recognizing them will result in failed login.",
            "        # 1. https://bugs.python.org/issue17164",
            "        for cookie in self:",
            "            # Treat `expires=0` cookies as session cookies",
            "            if cookie.expires == 0:",
            "                cookie.expires = None",
            "                cookie.discard = True",
            "",
            "    def get_cookie_header(self, url):",
            "        \"\"\"Generate a Cookie HTTP header for a given url\"\"\"",
            "        cookie_req = urllib.request.Request(escape_url(sanitize_url(url)))",
            "        self.add_cookie_header(cookie_req)",
            "        return cookie_req.get_header('Cookie')",
            "",
            "    def clear(self, *args, **kwargs):",
            "        with contextlib.suppress(KeyError):",
            "            return super().clear(*args, **kwargs)"
        ],
        "afterPatchFile": [
            "import base64",
            "import collections",
            "import contextlib",
            "import http.cookiejar",
            "import http.cookies",
            "import io",
            "import json",
            "import os",
            "import re",
            "import shutil",
            "import struct",
            "import subprocess",
            "import sys",
            "import tempfile",
            "import time",
            "import urllib.request",
            "from datetime import datetime, timedelta, timezone",
            "from enum import Enum, auto",
            "from hashlib import pbkdf2_hmac",
            "",
            "from .aes import (",
            "    aes_cbc_decrypt_bytes,",
            "    aes_gcm_decrypt_and_verify_bytes,",
            "    unpad_pkcs7,",
            ")",
            "from .compat import functools",
            "from .dependencies import (",
            "    _SECRETSTORAGE_UNAVAILABLE_REASON,",
            "    secretstorage,",
            "    sqlite3,",
            ")",
            "from .minicurses import MultilinePrinter, QuietMultilinePrinter",
            "from .utils import (",
            "    Popen,",
            "    error_to_str,",
            "    escape_url,",
            "    expand_path,",
            "    is_path_like,",
            "    sanitize_url,",
            "    str_or_none,",
            "    try_call,",
            "    write_string,",
            ")",
            "",
            "CHROMIUM_BASED_BROWSERS = {'brave', 'chrome', 'chromium', 'edge', 'opera', 'vivaldi'}",
            "SUPPORTED_BROWSERS = CHROMIUM_BASED_BROWSERS | {'firefox', 'safari'}",
            "",
            "",
            "class YDLLogger:",
            "    def __init__(self, ydl=None):",
            "        self._ydl = ydl",
            "",
            "    def debug(self, message):",
            "        if self._ydl:",
            "            self._ydl.write_debug(message)",
            "",
            "    def info(self, message):",
            "        if self._ydl:",
            "            self._ydl.to_screen(f'[Cookies] {message}')",
            "",
            "    def warning(self, message, only_once=False):",
            "        if self._ydl:",
            "            self._ydl.report_warning(message, only_once)",
            "",
            "    def error(self, message):",
            "        if self._ydl:",
            "            self._ydl.report_error(message)",
            "",
            "    class ProgressBar(MultilinePrinter):",
            "        _DELAY, _timer = 0.1, 0",
            "",
            "        def print(self, message):",
            "            if time.time() - self._timer > self._DELAY:",
            "                self.print_at_line(f'[Cookies] {message}', 0)",
            "                self._timer = time.time()",
            "",
            "    def progress_bar(self):",
            "        \"\"\"Return a context manager with a print method. (Optional)\"\"\"",
            "        # Do not print to files/pipes, loggers, or when --no-progress is used",
            "        if not self._ydl or self._ydl.params.get('noprogress') or self._ydl.params.get('logger'):",
            "            return",
            "        file = self._ydl._out_files.error",
            "        try:",
            "            if not file.isatty():",
            "                return",
            "        except BaseException:",
            "            return",
            "        return self.ProgressBar(file, preserve_output=False)",
            "",
            "",
            "def _create_progress_bar(logger):",
            "    if hasattr(logger, 'progress_bar'):",
            "        printer = logger.progress_bar()",
            "        if printer:",
            "            return printer",
            "    printer = QuietMultilinePrinter()",
            "    printer.print = lambda _: None",
            "    return printer",
            "",
            "",
            "def load_cookies(cookie_file, browser_specification, ydl):",
            "    cookie_jars = []",
            "    if browser_specification is not None:",
            "        browser_name, profile, keyring, container = _parse_browser_specification(*browser_specification)",
            "        cookie_jars.append(",
            "            extract_cookies_from_browser(browser_name, profile, YDLLogger(ydl), keyring=keyring, container=container))",
            "",
            "    if cookie_file is not None:",
            "        is_filename = is_path_like(cookie_file)",
            "        if is_filename:",
            "            cookie_file = expand_path(cookie_file)",
            "",
            "        jar = YoutubeDLCookieJar(cookie_file)",
            "        if not is_filename or os.access(cookie_file, os.R_OK):",
            "            jar.load(ignore_discard=True, ignore_expires=True)",
            "        cookie_jars.append(jar)",
            "",
            "    return _merge_cookie_jars(cookie_jars)",
            "",
            "",
            "def extract_cookies_from_browser(browser_name, profile=None, logger=YDLLogger(), *, keyring=None, container=None):",
            "    if browser_name == 'firefox':",
            "        return _extract_firefox_cookies(profile, container, logger)",
            "    elif browser_name == 'safari':",
            "        return _extract_safari_cookies(profile, logger)",
            "    elif browser_name in CHROMIUM_BASED_BROWSERS:",
            "        return _extract_chrome_cookies(browser_name, profile, keyring, logger)",
            "    else:",
            "        raise ValueError(f'unknown browser: {browser_name}')",
            "",
            "",
            "def _extract_firefox_cookies(profile, container, logger):",
            "    logger.info('Extracting cookies from firefox')",
            "    if not sqlite3:",
            "        logger.warning('Cannot extract cookies from firefox without sqlite3 support. '",
            "                       'Please use a python interpreter compiled with sqlite3 support')",
            "        return YoutubeDLCookieJar()",
            "",
            "    if profile is None:",
            "        search_root = _firefox_browser_dir()",
            "    elif _is_path(profile):",
            "        search_root = profile",
            "    else:",
            "        search_root = os.path.join(_firefox_browser_dir(), profile)",
            "",
            "    cookie_database_path = _find_most_recently_used_file(search_root, 'cookies.sqlite', logger)",
            "    if cookie_database_path is None:",
            "        raise FileNotFoundError(f'could not find firefox cookies database in {search_root}')",
            "    logger.debug(f'Extracting cookies from: \"{cookie_database_path}\"')",
            "",
            "    container_id = None",
            "    if container not in (None, 'none'):",
            "        containers_path = os.path.join(os.path.dirname(cookie_database_path), 'containers.json')",
            "        if not os.path.isfile(containers_path) or not os.access(containers_path, os.R_OK):",
            "            raise FileNotFoundError(f'could not read containers.json in {search_root}')",
            "        with open(containers_path) as containers:",
            "            identities = json.load(containers).get('identities', [])",
            "        container_id = next((context.get('userContextId') for context in identities if container in (",
            "            context.get('name'),",
            "            try_call(lambda: re.fullmatch(r'userContext([^\\.]+)\\.label', context['l10nID']).group())",
            "        )), None)",
            "        if not isinstance(container_id, int):",
            "            raise ValueError(f'could not find firefox container \"{container}\" in containers.json')",
            "",
            "    with tempfile.TemporaryDirectory(prefix='yt_dlp') as tmpdir:",
            "        cursor = None",
            "        try:",
            "            cursor = _open_database_copy(cookie_database_path, tmpdir)",
            "            if isinstance(container_id, int):",
            "                logger.debug(",
            "                    f'Only loading cookies from firefox container \"{container}\", ID {container_id}')",
            "                cursor.execute(",
            "                    'SELECT host, name, value, path, expiry, isSecure FROM moz_cookies WHERE originAttributes LIKE ? OR originAttributes LIKE ?',",
            "                    (f'%userContextId={container_id}', f'%userContextId={container_id}&%'))",
            "            elif container == 'none':",
            "                logger.debug('Only loading cookies not belonging to any container')",
            "                cursor.execute(",
            "                    'SELECT host, name, value, path, expiry, isSecure FROM moz_cookies WHERE NOT INSTR(originAttributes,\"userContextId=\")')",
            "            else:",
            "                cursor.execute('SELECT host, name, value, path, expiry, isSecure FROM moz_cookies')",
            "            jar = YoutubeDLCookieJar()",
            "            with _create_progress_bar(logger) as progress_bar:",
            "                table = cursor.fetchall()",
            "                total_cookie_count = len(table)",
            "                for i, (host, name, value, path, expiry, is_secure) in enumerate(table):",
            "                    progress_bar.print(f'Loading cookie {i: 6d}/{total_cookie_count: 6d}')",
            "                    cookie = http.cookiejar.Cookie(",
            "                        version=0, name=name, value=value, port=None, port_specified=False,",
            "                        domain=host, domain_specified=bool(host), domain_initial_dot=host.startswith('.'),",
            "                        path=path, path_specified=bool(path), secure=is_secure, expires=expiry, discard=False,",
            "                        comment=None, comment_url=None, rest={})",
            "                    jar.set_cookie(cookie)",
            "            logger.info(f'Extracted {len(jar)} cookies from firefox')",
            "            return jar",
            "        finally:",
            "            if cursor is not None:",
            "                cursor.connection.close()",
            "",
            "",
            "def _firefox_browser_dir():",
            "    if sys.platform in ('cygwin', 'win32'):",
            "        return os.path.expandvars(R'%APPDATA%\\Mozilla\\Firefox\\Profiles')",
            "    elif sys.platform == 'darwin':",
            "        return os.path.expanduser('~/Library/Application Support/Firefox')",
            "    return os.path.expanduser('~/.mozilla/firefox')",
            "",
            "",
            "def _get_chromium_based_browser_settings(browser_name):",
            "    # https://chromium.googlesource.com/chromium/src/+/HEAD/docs/user_data_dir.md",
            "    if sys.platform in ('cygwin', 'win32'):",
            "        appdata_local = os.path.expandvars('%LOCALAPPDATA%')",
            "        appdata_roaming = os.path.expandvars('%APPDATA%')",
            "        browser_dir = {",
            "            'brave': os.path.join(appdata_local, R'BraveSoftware\\Brave-Browser\\User Data'),",
            "            'chrome': os.path.join(appdata_local, R'Google\\Chrome\\User Data'),",
            "            'chromium': os.path.join(appdata_local, R'Chromium\\User Data'),",
            "            'edge': os.path.join(appdata_local, R'Microsoft\\Edge\\User Data'),",
            "            'opera': os.path.join(appdata_roaming, R'Opera Software\\Opera Stable'),",
            "            'vivaldi': os.path.join(appdata_local, R'Vivaldi\\User Data'),",
            "        }[browser_name]",
            "",
            "    elif sys.platform == 'darwin':",
            "        appdata = os.path.expanduser('~/Library/Application Support')",
            "        browser_dir = {",
            "            'brave': os.path.join(appdata, 'BraveSoftware/Brave-Browser'),",
            "            'chrome': os.path.join(appdata, 'Google/Chrome'),",
            "            'chromium': os.path.join(appdata, 'Chromium'),",
            "            'edge': os.path.join(appdata, 'Microsoft Edge'),",
            "            'opera': os.path.join(appdata, 'com.operasoftware.Opera'),",
            "            'vivaldi': os.path.join(appdata, 'Vivaldi'),",
            "        }[browser_name]",
            "",
            "    else:",
            "        config = _config_home()",
            "        browser_dir = {",
            "            'brave': os.path.join(config, 'BraveSoftware/Brave-Browser'),",
            "            'chrome': os.path.join(config, 'google-chrome'),",
            "            'chromium': os.path.join(config, 'chromium'),",
            "            'edge': os.path.join(config, 'microsoft-edge'),",
            "            'opera': os.path.join(config, 'opera'),",
            "            'vivaldi': os.path.join(config, 'vivaldi'),",
            "        }[browser_name]",
            "",
            "    # Linux keyring names can be determined by snooping on dbus while opening the browser in KDE:",
            "    # dbus-monitor \"interface='org.kde.KWallet'\" \"type=method_return\"",
            "    keyring_name = {",
            "        'brave': 'Brave',",
            "        'chrome': 'Chrome',",
            "        'chromium': 'Chromium',",
            "        'edge': 'Microsoft Edge' if sys.platform == 'darwin' else 'Chromium',",
            "        'opera': 'Opera' if sys.platform == 'darwin' else 'Chromium',",
            "        'vivaldi': 'Vivaldi' if sys.platform == 'darwin' else 'Chrome',",
            "    }[browser_name]",
            "",
            "    browsers_without_profiles = {'opera'}",
            "",
            "    return {",
            "        'browser_dir': browser_dir,",
            "        'keyring_name': keyring_name,",
            "        'supports_profiles': browser_name not in browsers_without_profiles",
            "    }",
            "",
            "",
            "def _extract_chrome_cookies(browser_name, profile, keyring, logger):",
            "    logger.info(f'Extracting cookies from {browser_name}')",
            "",
            "    if not sqlite3:",
            "        logger.warning(f'Cannot extract cookies from {browser_name} without sqlite3 support. '",
            "                       'Please use a python interpreter compiled with sqlite3 support')",
            "        return YoutubeDLCookieJar()",
            "",
            "    config = _get_chromium_based_browser_settings(browser_name)",
            "",
            "    if profile is None:",
            "        search_root = config['browser_dir']",
            "    elif _is_path(profile):",
            "        search_root = profile",
            "        config['browser_dir'] = os.path.dirname(profile) if config['supports_profiles'] else profile",
            "    else:",
            "        if config['supports_profiles']:",
            "            search_root = os.path.join(config['browser_dir'], profile)",
            "        else:",
            "            logger.error(f'{browser_name} does not support profiles')",
            "            search_root = config['browser_dir']",
            "",
            "    cookie_database_path = _find_most_recently_used_file(search_root, 'Cookies', logger)",
            "    if cookie_database_path is None:",
            "        raise FileNotFoundError(f'could not find {browser_name} cookies database in \"{search_root}\"')",
            "    logger.debug(f'Extracting cookies from: \"{cookie_database_path}\"')",
            "",
            "    decryptor = get_cookie_decryptor(config['browser_dir'], config['keyring_name'], logger, keyring=keyring)",
            "",
            "    with tempfile.TemporaryDirectory(prefix='yt_dlp') as tmpdir:",
            "        cursor = None",
            "        try:",
            "            cursor = _open_database_copy(cookie_database_path, tmpdir)",
            "            cursor.connection.text_factory = bytes",
            "            column_names = _get_column_names(cursor, 'cookies')",
            "            secure_column = 'is_secure' if 'is_secure' in column_names else 'secure'",
            "            cursor.execute(f'SELECT host_key, name, value, encrypted_value, path, expires_utc, {secure_column} FROM cookies')",
            "            jar = YoutubeDLCookieJar()",
            "            failed_cookies = 0",
            "            unencrypted_cookies = 0",
            "            with _create_progress_bar(logger) as progress_bar:",
            "                table = cursor.fetchall()",
            "                total_cookie_count = len(table)",
            "                for i, line in enumerate(table):",
            "                    progress_bar.print(f'Loading cookie {i: 6d}/{total_cookie_count: 6d}')",
            "                    is_encrypted, cookie = _process_chrome_cookie(decryptor, *line)",
            "                    if not cookie:",
            "                        failed_cookies += 1",
            "                        continue",
            "                    elif not is_encrypted:",
            "                        unencrypted_cookies += 1",
            "                    jar.set_cookie(cookie)",
            "            if failed_cookies > 0:",
            "                failed_message = f' ({failed_cookies} could not be decrypted)'",
            "            else:",
            "                failed_message = ''",
            "            logger.info(f'Extracted {len(jar)} cookies from {browser_name}{failed_message}')",
            "            counts = decryptor._cookie_counts.copy()",
            "            counts['unencrypted'] = unencrypted_cookies",
            "            logger.debug(f'cookie version breakdown: {counts}')",
            "            return jar",
            "        finally:",
            "            if cursor is not None:",
            "                cursor.connection.close()",
            "",
            "",
            "def _process_chrome_cookie(decryptor, host_key, name, value, encrypted_value, path, expires_utc, is_secure):",
            "    host_key = host_key.decode()",
            "    name = name.decode()",
            "    value = value.decode()",
            "    path = path.decode()",
            "    is_encrypted = not value and encrypted_value",
            "",
            "    if is_encrypted:",
            "        value = decryptor.decrypt(encrypted_value)",
            "        if value is None:",
            "            return is_encrypted, None",
            "",
            "    return is_encrypted, http.cookiejar.Cookie(",
            "        version=0, name=name, value=value, port=None, port_specified=False,",
            "        domain=host_key, domain_specified=bool(host_key), domain_initial_dot=host_key.startswith('.'),",
            "        path=path, path_specified=bool(path), secure=is_secure, expires=expires_utc, discard=False,",
            "        comment=None, comment_url=None, rest={})",
            "",
            "",
            "class ChromeCookieDecryptor:",
            "    \"\"\"",
            "    Overview:",
            "",
            "        Linux:",
            "        - cookies are either v10 or v11",
            "            - v10: AES-CBC encrypted with a fixed key",
            "                - also attempts empty password if decryption fails",
            "            - v11: AES-CBC encrypted with an OS protected key (keyring)",
            "                - also attempts empty password if decryption fails",
            "            - v11 keys can be stored in various places depending on the activate desktop environment [2]",
            "",
            "        Mac:",
            "        - cookies are either v10 or not v10",
            "            - v10: AES-CBC encrypted with an OS protected key (keyring) and more key derivation iterations than linux",
            "            - not v10: 'old data' stored as plaintext",
            "",
            "        Windows:",
            "        - cookies are either v10 or not v10",
            "            - v10: AES-GCM encrypted with a key which is encrypted with DPAPI",
            "            - not v10: encrypted with DPAPI",
            "",
            "    Sources:",
            "    - [1] https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/",
            "    - [2] https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/key_storage_linux.cc",
            "        - KeyStorageLinux::CreateService",
            "    \"\"\"",
            "",
            "    _cookie_counts = {}",
            "",
            "    def decrypt(self, encrypted_value):",
            "        raise NotImplementedError('Must be implemented by sub classes')",
            "",
            "",
            "def get_cookie_decryptor(browser_root, browser_keyring_name, logger, *, keyring=None):",
            "    if sys.platform == 'darwin':",
            "        return MacChromeCookieDecryptor(browser_keyring_name, logger)",
            "    elif sys.platform in ('win32', 'cygwin'):",
            "        return WindowsChromeCookieDecryptor(browser_root, logger)",
            "    return LinuxChromeCookieDecryptor(browser_keyring_name, logger, keyring=keyring)",
            "",
            "",
            "class LinuxChromeCookieDecryptor(ChromeCookieDecryptor):",
            "    def __init__(self, browser_keyring_name, logger, *, keyring=None):",
            "        self._logger = logger",
            "        self._v10_key = self.derive_key(b'peanuts')",
            "        self._empty_key = self.derive_key(b'')",
            "        self._cookie_counts = {'v10': 0, 'v11': 0, 'other': 0}",
            "        self._browser_keyring_name = browser_keyring_name",
            "        self._keyring = keyring",
            "",
            "    @functools.cached_property",
            "    def _v11_key(self):",
            "        password = _get_linux_keyring_password(self._browser_keyring_name, self._keyring, self._logger)",
            "        return None if password is None else self.derive_key(password)",
            "",
            "    @staticmethod",
            "    def derive_key(password):",
            "        # values from",
            "        # https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/os_crypt_linux.cc",
            "        return pbkdf2_sha1(password, salt=b'saltysalt', iterations=1, key_length=16)",
            "",
            "    def decrypt(self, encrypted_value):",
            "        \"\"\"",
            "",
            "        following the same approach as the fix in [1]: if cookies fail to decrypt then attempt to decrypt",
            "        with an empty password. The failure detection is not the same as what chromium uses so the",
            "        results won't be perfect",
            "",
            "        References:",
            "            - [1] https://chromium.googlesource.com/chromium/src/+/bbd54702284caca1f92d656fdcadf2ccca6f4165%5E%21/",
            "                - a bugfix to try an empty password as a fallback",
            "        \"\"\"",
            "        version = encrypted_value[:3]",
            "        ciphertext = encrypted_value[3:]",
            "",
            "        if version == b'v10':",
            "            self._cookie_counts['v10'] += 1",
            "            return _decrypt_aes_cbc_multi(ciphertext, (self._v10_key, self._empty_key), self._logger)",
            "",
            "        elif version == b'v11':",
            "            self._cookie_counts['v11'] += 1",
            "            if self._v11_key is None:",
            "                self._logger.warning('cannot decrypt v11 cookies: no key found', only_once=True)",
            "                return None",
            "            return _decrypt_aes_cbc_multi(ciphertext, (self._v11_key, self._empty_key), self._logger)",
            "",
            "        else:",
            "            self._logger.warning(f'unknown cookie version: \"{version}\"', only_once=True)",
            "            self._cookie_counts['other'] += 1",
            "            return None",
            "",
            "",
            "class MacChromeCookieDecryptor(ChromeCookieDecryptor):",
            "    def __init__(self, browser_keyring_name, logger):",
            "        self._logger = logger",
            "        password = _get_mac_keyring_password(browser_keyring_name, logger)",
            "        self._v10_key = None if password is None else self.derive_key(password)",
            "        self._cookie_counts = {'v10': 0, 'other': 0}",
            "",
            "    @staticmethod",
            "    def derive_key(password):",
            "        # values from",
            "        # https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/os_crypt_mac.mm",
            "        return pbkdf2_sha1(password, salt=b'saltysalt', iterations=1003, key_length=16)",
            "",
            "    def decrypt(self, encrypted_value):",
            "        version = encrypted_value[:3]",
            "        ciphertext = encrypted_value[3:]",
            "",
            "        if version == b'v10':",
            "            self._cookie_counts['v10'] += 1",
            "            if self._v10_key is None:",
            "                self._logger.warning('cannot decrypt v10 cookies: no key found', only_once=True)",
            "                return None",
            "",
            "            return _decrypt_aes_cbc_multi(ciphertext, (self._v10_key,), self._logger)",
            "",
            "        else:",
            "            self._cookie_counts['other'] += 1",
            "            # other prefixes are considered 'old data' which were stored as plaintext",
            "            # https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/os_crypt_mac.mm",
            "            return encrypted_value",
            "",
            "",
            "class WindowsChromeCookieDecryptor(ChromeCookieDecryptor):",
            "    def __init__(self, browser_root, logger):",
            "        self._logger = logger",
            "        self._v10_key = _get_windows_v10_key(browser_root, logger)",
            "        self._cookie_counts = {'v10': 0, 'other': 0}",
            "",
            "    def decrypt(self, encrypted_value):",
            "        version = encrypted_value[:3]",
            "        ciphertext = encrypted_value[3:]",
            "",
            "        if version == b'v10':",
            "            self._cookie_counts['v10'] += 1",
            "            if self._v10_key is None:",
            "                self._logger.warning('cannot decrypt v10 cookies: no key found', only_once=True)",
            "                return None",
            "",
            "            # https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/os_crypt_win.cc",
            "            #   kNonceLength",
            "            nonce_length = 96 // 8",
            "            # boringssl",
            "            #   EVP_AEAD_AES_GCM_TAG_LEN",
            "            authentication_tag_length = 16",
            "",
            "            raw_ciphertext = ciphertext",
            "            nonce = raw_ciphertext[:nonce_length]",
            "            ciphertext = raw_ciphertext[nonce_length:-authentication_tag_length]",
            "            authentication_tag = raw_ciphertext[-authentication_tag_length:]",
            "",
            "            return _decrypt_aes_gcm(ciphertext, self._v10_key, nonce, authentication_tag, self._logger)",
            "",
            "        else:",
            "            self._cookie_counts['other'] += 1",
            "            # any other prefix means the data is DPAPI encrypted",
            "            # https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/os_crypt_win.cc",
            "            return _decrypt_windows_dpapi(encrypted_value, self._logger).decode()",
            "",
            "",
            "def _extract_safari_cookies(profile, logger):",
            "    if sys.platform != 'darwin':",
            "        raise ValueError(f'unsupported platform: {sys.platform}')",
            "",
            "    if profile:",
            "        cookies_path = os.path.expanduser(profile)",
            "        if not os.path.isfile(cookies_path):",
            "            raise FileNotFoundError('custom safari cookies database not found')",
            "",
            "    else:",
            "        cookies_path = os.path.expanduser('~/Library/Cookies/Cookies.binarycookies')",
            "",
            "        if not os.path.isfile(cookies_path):",
            "            logger.debug('Trying secondary cookie location')",
            "            cookies_path = os.path.expanduser('~/Library/Containers/com.apple.Safari/Data/Library/Cookies/Cookies.binarycookies')",
            "            if not os.path.isfile(cookies_path):",
            "                raise FileNotFoundError('could not find safari cookies database')",
            "",
            "    with open(cookies_path, 'rb') as f:",
            "        cookies_data = f.read()",
            "",
            "    jar = parse_safari_cookies(cookies_data, logger=logger)",
            "    logger.info(f'Extracted {len(jar)} cookies from safari')",
            "    return jar",
            "",
            "",
            "class ParserError(Exception):",
            "    pass",
            "",
            "",
            "class DataParser:",
            "    def __init__(self, data, logger):",
            "        self._data = data",
            "        self.cursor = 0",
            "        self._logger = logger",
            "",
            "    def read_bytes(self, num_bytes):",
            "        if num_bytes < 0:",
            "            raise ParserError(f'invalid read of {num_bytes} bytes')",
            "        end = self.cursor + num_bytes",
            "        if end > len(self._data):",
            "            raise ParserError('reached end of input')",
            "        data = self._data[self.cursor:end]",
            "        self.cursor = end",
            "        return data",
            "",
            "    def expect_bytes(self, expected_value, message):",
            "        value = self.read_bytes(len(expected_value))",
            "        if value != expected_value:",
            "            raise ParserError(f'unexpected value: {value} != {expected_value} ({message})')",
            "",
            "    def read_uint(self, big_endian=False):",
            "        data_format = '>I' if big_endian else '<I'",
            "        return struct.unpack(data_format, self.read_bytes(4))[0]",
            "",
            "    def read_double(self, big_endian=False):",
            "        data_format = '>d' if big_endian else '<d'",
            "        return struct.unpack(data_format, self.read_bytes(8))[0]",
            "",
            "    def read_cstring(self):",
            "        buffer = []",
            "        while True:",
            "            c = self.read_bytes(1)",
            "            if c == b'\\x00':",
            "                return b''.join(buffer).decode()",
            "            else:",
            "                buffer.append(c)",
            "",
            "    def skip(self, num_bytes, description='unknown'):",
            "        if num_bytes > 0:",
            "            self._logger.debug(f'skipping {num_bytes} bytes ({description}): {self.read_bytes(num_bytes)!r}')",
            "        elif num_bytes < 0:",
            "            raise ParserError(f'invalid skip of {num_bytes} bytes')",
            "",
            "    def skip_to(self, offset, description='unknown'):",
            "        self.skip(offset - self.cursor, description)",
            "",
            "    def skip_to_end(self, description='unknown'):",
            "        self.skip_to(len(self._data), description)",
            "",
            "",
            "def _mac_absolute_time_to_posix(timestamp):",
            "    return int((datetime(2001, 1, 1, 0, 0, tzinfo=timezone.utc) + timedelta(seconds=timestamp)).timestamp())",
            "",
            "",
            "def _parse_safari_cookies_header(data, logger):",
            "    p = DataParser(data, logger)",
            "    p.expect_bytes(b'cook', 'database signature')",
            "    number_of_pages = p.read_uint(big_endian=True)",
            "    page_sizes = [p.read_uint(big_endian=True) for _ in range(number_of_pages)]",
            "    return page_sizes, p.cursor",
            "",
            "",
            "def _parse_safari_cookies_page(data, jar, logger):",
            "    p = DataParser(data, logger)",
            "    p.expect_bytes(b'\\x00\\x00\\x01\\x00', 'page signature')",
            "    number_of_cookies = p.read_uint()",
            "    record_offsets = [p.read_uint() for _ in range(number_of_cookies)]",
            "    if number_of_cookies == 0:",
            "        logger.debug(f'a cookies page of size {len(data)} has no cookies')",
            "        return",
            "",
            "    p.skip_to(record_offsets[0], 'unknown page header field')",
            "",
            "    with _create_progress_bar(logger) as progress_bar:",
            "        for i, record_offset in enumerate(record_offsets):",
            "            progress_bar.print(f'Loading cookie {i: 6d}/{number_of_cookies: 6d}')",
            "            p.skip_to(record_offset, 'space between records')",
            "            record_length = _parse_safari_cookies_record(data[record_offset:], jar, logger)",
            "            p.read_bytes(record_length)",
            "    p.skip_to_end('space in between pages')",
            "",
            "",
            "def _parse_safari_cookies_record(data, jar, logger):",
            "    p = DataParser(data, logger)",
            "    record_size = p.read_uint()",
            "    p.skip(4, 'unknown record field 1')",
            "    flags = p.read_uint()",
            "    is_secure = bool(flags & 0x0001)",
            "    p.skip(4, 'unknown record field 2')",
            "    domain_offset = p.read_uint()",
            "    name_offset = p.read_uint()",
            "    path_offset = p.read_uint()",
            "    value_offset = p.read_uint()",
            "    p.skip(8, 'unknown record field 3')",
            "    expiration_date = _mac_absolute_time_to_posix(p.read_double())",
            "    _creation_date = _mac_absolute_time_to_posix(p.read_double())  # noqa: F841",
            "",
            "    try:",
            "        p.skip_to(domain_offset)",
            "        domain = p.read_cstring()",
            "",
            "        p.skip_to(name_offset)",
            "        name = p.read_cstring()",
            "",
            "        p.skip_to(path_offset)",
            "        path = p.read_cstring()",
            "",
            "        p.skip_to(value_offset)",
            "        value = p.read_cstring()",
            "    except UnicodeDecodeError:",
            "        logger.warning('failed to parse Safari cookie because UTF-8 decoding failed', only_once=True)",
            "        return record_size",
            "",
            "    p.skip_to(record_size, 'space at the end of the record')",
            "",
            "    cookie = http.cookiejar.Cookie(",
            "        version=0, name=name, value=value, port=None, port_specified=False,",
            "        domain=domain, domain_specified=bool(domain), domain_initial_dot=domain.startswith('.'),",
            "        path=path, path_specified=bool(path), secure=is_secure, expires=expiration_date, discard=False,",
            "        comment=None, comment_url=None, rest={})",
            "    jar.set_cookie(cookie)",
            "    return record_size",
            "",
            "",
            "def parse_safari_cookies(data, jar=None, logger=YDLLogger()):",
            "    \"\"\"",
            "    References:",
            "        - https://github.com/libyal/dtformats/blob/main/documentation/Safari%20Cookies.asciidoc",
            "            - this data appears to be out of date but the important parts of the database structure is the same",
            "            - there are a few bytes here and there which are skipped during parsing",
            "    \"\"\"",
            "    if jar is None:",
            "        jar = YoutubeDLCookieJar()",
            "    page_sizes, body_start = _parse_safari_cookies_header(data, logger)",
            "    p = DataParser(data[body_start:], logger)",
            "    for page_size in page_sizes:",
            "        _parse_safari_cookies_page(p.read_bytes(page_size), jar, logger)",
            "    p.skip_to_end('footer')",
            "    return jar",
            "",
            "",
            "class _LinuxDesktopEnvironment(Enum):",
            "    \"\"\"",
            "    https://chromium.googlesource.com/chromium/src/+/refs/heads/main/base/nix/xdg_util.h",
            "    DesktopEnvironment",
            "    \"\"\"",
            "    OTHER = auto()",
            "    CINNAMON = auto()",
            "    DEEPIN = auto()",
            "    GNOME = auto()",
            "    KDE3 = auto()",
            "    KDE4 = auto()",
            "    KDE5 = auto()",
            "    KDE6 = auto()",
            "    PANTHEON = auto()",
            "    UKUI = auto()",
            "    UNITY = auto()",
            "    XFCE = auto()",
            "    LXQT = auto()",
            "",
            "",
            "class _LinuxKeyring(Enum):",
            "    \"\"\"",
            "    https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/key_storage_util_linux.h",
            "    SelectedLinuxBackend",
            "    \"\"\"",
            "    KWALLET = auto()  # KDE4",
            "    KWALLET5 = auto()",
            "    KWALLET6 = auto()",
            "    GNOMEKEYRING = auto()",
            "    BASICTEXT = auto()",
            "",
            "",
            "SUPPORTED_KEYRINGS = _LinuxKeyring.__members__.keys()",
            "",
            "",
            "def _get_linux_desktop_environment(env, logger):",
            "    \"\"\"",
            "    https://chromium.googlesource.com/chromium/src/+/refs/heads/main/base/nix/xdg_util.cc",
            "    GetDesktopEnvironment",
            "    \"\"\"",
            "    xdg_current_desktop = env.get('XDG_CURRENT_DESKTOP', None)",
            "    desktop_session = env.get('DESKTOP_SESSION', None)",
            "    if xdg_current_desktop is not None:",
            "        xdg_current_desktop = xdg_current_desktop.split(':')[0].strip()",
            "",
            "        if xdg_current_desktop == 'Unity':",
            "            if desktop_session is not None and 'gnome-fallback' in desktop_session:",
            "                return _LinuxDesktopEnvironment.GNOME",
            "            else:",
            "                return _LinuxDesktopEnvironment.UNITY",
            "        elif xdg_current_desktop == 'Deepin':",
            "            return _LinuxDesktopEnvironment.DEEPIN",
            "        elif xdg_current_desktop == 'GNOME':",
            "            return _LinuxDesktopEnvironment.GNOME",
            "        elif xdg_current_desktop == 'X-Cinnamon':",
            "            return _LinuxDesktopEnvironment.CINNAMON",
            "        elif xdg_current_desktop == 'KDE':",
            "            kde_version = env.get('KDE_SESSION_VERSION', None)",
            "            if kde_version == '5':",
            "                return _LinuxDesktopEnvironment.KDE5",
            "            elif kde_version == '6':",
            "                return _LinuxDesktopEnvironment.KDE6",
            "            elif kde_version == '4':",
            "                return _LinuxDesktopEnvironment.KDE4",
            "            else:",
            "                logger.info(f'unknown KDE version: \"{kde_version}\". Assuming KDE4')",
            "                return _LinuxDesktopEnvironment.KDE4",
            "        elif xdg_current_desktop == 'Pantheon':",
            "            return _LinuxDesktopEnvironment.PANTHEON",
            "        elif xdg_current_desktop == 'XFCE':",
            "            return _LinuxDesktopEnvironment.XFCE",
            "        elif xdg_current_desktop == 'UKUI':",
            "            return _LinuxDesktopEnvironment.UKUI",
            "        elif xdg_current_desktop == 'LXQt':",
            "            return _LinuxDesktopEnvironment.LXQT",
            "        else:",
            "            logger.info(f'XDG_CURRENT_DESKTOP is set to an unknown value: \"{xdg_current_desktop}\"')",
            "",
            "    elif desktop_session is not None:",
            "        if desktop_session == 'deepin':",
            "            return _LinuxDesktopEnvironment.DEEPIN",
            "        elif desktop_session in ('mate', 'gnome'):",
            "            return _LinuxDesktopEnvironment.GNOME",
            "        elif desktop_session in ('kde4', 'kde-plasma'):",
            "            return _LinuxDesktopEnvironment.KDE4",
            "        elif desktop_session == 'kde':",
            "            if 'KDE_SESSION_VERSION' in env:",
            "                return _LinuxDesktopEnvironment.KDE4",
            "            else:",
            "                return _LinuxDesktopEnvironment.KDE3",
            "        elif 'xfce' in desktop_session or desktop_session == 'xubuntu':",
            "            return _LinuxDesktopEnvironment.XFCE",
            "        elif desktop_session == 'ukui':",
            "            return _LinuxDesktopEnvironment.UKUI",
            "        else:",
            "            logger.info(f'DESKTOP_SESSION is set to an unknown value: \"{desktop_session}\"')",
            "",
            "    else:",
            "        if 'GNOME_DESKTOP_SESSION_ID' in env:",
            "            return _LinuxDesktopEnvironment.GNOME",
            "        elif 'KDE_FULL_SESSION' in env:",
            "            if 'KDE_SESSION_VERSION' in env:",
            "                return _LinuxDesktopEnvironment.KDE4",
            "            else:",
            "                return _LinuxDesktopEnvironment.KDE3",
            "    return _LinuxDesktopEnvironment.OTHER",
            "",
            "",
            "def _choose_linux_keyring(logger):",
            "    \"\"\"",
            "    SelectBackend in [1]",
            "",
            "    There is currently support for forcing chromium to use BASIC_TEXT by creating a file called",
            "    `Disable Local Encryption` [1] in the user data dir. The function to write this file (`WriteBackendUse()` [1])",
            "    does not appear to be called anywhere other than in tests, so the user would have to create this file manually",
            "    and so would be aware enough to tell yt-dlp to use the BASIC_TEXT keyring.",
            "",
            "    References:",
            "        - [1] https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/key_storage_util_linux.cc",
            "    \"\"\"",
            "    desktop_environment = _get_linux_desktop_environment(os.environ, logger)",
            "    logger.debug(f'detected desktop environment: {desktop_environment.name}')",
            "    if desktop_environment == _LinuxDesktopEnvironment.KDE4:",
            "        linux_keyring = _LinuxKeyring.KWALLET",
            "    elif desktop_environment == _LinuxDesktopEnvironment.KDE5:",
            "        linux_keyring = _LinuxKeyring.KWALLET5",
            "    elif desktop_environment == _LinuxDesktopEnvironment.KDE6:",
            "        linux_keyring = _LinuxKeyring.KWALLET6",
            "    elif desktop_environment in (",
            "        _LinuxDesktopEnvironment.KDE3, _LinuxDesktopEnvironment.LXQT, _LinuxDesktopEnvironment.OTHER",
            "    ):",
            "        linux_keyring = _LinuxKeyring.BASICTEXT",
            "    else:",
            "        linux_keyring = _LinuxKeyring.GNOMEKEYRING",
            "    return linux_keyring",
            "",
            "",
            "def _get_kwallet_network_wallet(keyring, logger):",
            "    \"\"\" The name of the wallet used to store network passwords.",
            "",
            "    https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/kwallet_dbus.cc",
            "    KWalletDBus::NetworkWallet",
            "    which does a dbus call to the following function:",
            "    https://api.kde.org/frameworks/kwallet/html/classKWallet_1_1Wallet.html",
            "    Wallet::NetworkWallet",
            "    \"\"\"",
            "    default_wallet = 'kdewallet'",
            "    try:",
            "        if keyring == _LinuxKeyring.KWALLET:",
            "            service_name = 'org.kde.kwalletd'",
            "            wallet_path = '/modules/kwalletd'",
            "        elif keyring == _LinuxKeyring.KWALLET5:",
            "            service_name = 'org.kde.kwalletd5'",
            "            wallet_path = '/modules/kwalletd5'",
            "        elif keyring == _LinuxKeyring.KWALLET6:",
            "            service_name = 'org.kde.kwalletd6'",
            "            wallet_path = '/modules/kwalletd6'",
            "        else:",
            "            raise ValueError(keyring)",
            "",
            "        stdout, _, returncode = Popen.run([",
            "            'dbus-send', '--session', '--print-reply=literal',",
            "            f'--dest={service_name}',",
            "            wallet_path,",
            "            'org.kde.KWallet.networkWallet'",
            "        ], text=True, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)",
            "",
            "        if returncode:",
            "            logger.warning('failed to read NetworkWallet')",
            "            return default_wallet",
            "        else:",
            "            logger.debug(f'NetworkWallet = \"{stdout.strip()}\"')",
            "            return stdout.strip()",
            "    except Exception as e:",
            "        logger.warning(f'exception while obtaining NetworkWallet: {e}')",
            "        return default_wallet",
            "",
            "",
            "def _get_kwallet_password(browser_keyring_name, keyring, logger):",
            "    logger.debug(f'using kwallet-query to obtain password from {keyring.name}')",
            "",
            "    if shutil.which('kwallet-query') is None:",
            "        logger.error('kwallet-query command not found. KWallet and kwallet-query '",
            "                     'must be installed to read from KWallet. kwallet-query should be'",
            "                     'included in the kwallet package for your distribution')",
            "        return b''",
            "",
            "    network_wallet = _get_kwallet_network_wallet(keyring, logger)",
            "",
            "    try:",
            "        stdout, _, returncode = Popen.run([",
            "            'kwallet-query',",
            "            '--read-password', f'{browser_keyring_name} Safe Storage',",
            "            '--folder', f'{browser_keyring_name} Keys',",
            "            network_wallet",
            "        ], stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)",
            "",
            "        if returncode:",
            "            logger.error(f'kwallet-query failed with return code {returncode}. '",
            "                         'Please consult the kwallet-query man page for details')",
            "            return b''",
            "        else:",
            "            if stdout.lower().startswith(b'failed to read'):",
            "                logger.debug('failed to read password from kwallet. Using empty string instead')",
            "                # this sometimes occurs in KDE because chrome does not check hasEntry and instead",
            "                # just tries to read the value (which kwallet returns \"\") whereas kwallet-query",
            "                # checks hasEntry. To verify this:",
            "                # dbus-monitor \"interface='org.kde.KWallet'\" \"type=method_return\"",
            "                # while starting chrome.",
            "                # this was identified as a bug later and fixed in",
            "                # https://chromium.googlesource.com/chromium/src/+/bbd54702284caca1f92d656fdcadf2ccca6f4165%5E%21/#F0",
            "                # https://chromium.googlesource.com/chromium/src/+/5463af3c39d7f5b6d11db7fbd51e38cc1974d764",
            "                return b''",
            "            else:",
            "                logger.debug('password found')",
            "                return stdout.rstrip(b'\\n')",
            "    except Exception as e:",
            "        logger.warning(f'exception running kwallet-query: {error_to_str(e)}')",
            "        return b''",
            "",
            "",
            "def _get_gnome_keyring_password(browser_keyring_name, logger):",
            "    if not secretstorage:",
            "        logger.error(f'secretstorage not available {_SECRETSTORAGE_UNAVAILABLE_REASON}')",
            "        return b''",
            "    # the Gnome keyring does not seem to organise keys in the same way as KWallet,",
            "    # using `dbus-monitor` during startup, it can be observed that chromium lists all keys",
            "    # and presumably searches for its key in the list. It appears that we must do the same.",
            "    # https://github.com/jaraco/keyring/issues/556",
            "    with contextlib.closing(secretstorage.dbus_init()) as con:",
            "        col = secretstorage.get_default_collection(con)",
            "        for item in col.get_all_items():",
            "            if item.get_label() == f'{browser_keyring_name} Safe Storage':",
            "                return item.get_secret()",
            "        else:",
            "            logger.error('failed to read from keyring')",
            "            return b''",
            "",
            "",
            "def _get_linux_keyring_password(browser_keyring_name, keyring, logger):",
            "    # note: chrome/chromium can be run with the following flags to determine which keyring backend",
            "    # it has chosen to use",
            "    # chromium --enable-logging=stderr --v=1 2>&1 | grep key_storage_",
            "    # Chromium supports a flag: --password-store=<basic|gnome|kwallet> so the automatic detection",
            "    # will not be sufficient in all cases.",
            "",
            "    keyring = _LinuxKeyring[keyring] if keyring else _choose_linux_keyring(logger)",
            "    logger.debug(f'Chosen keyring: {keyring.name}')",
            "",
            "    if keyring in (_LinuxKeyring.KWALLET, _LinuxKeyring.KWALLET5, _LinuxKeyring.KWALLET6):",
            "        return _get_kwallet_password(browser_keyring_name, keyring, logger)",
            "    elif keyring == _LinuxKeyring.GNOMEKEYRING:",
            "        return _get_gnome_keyring_password(browser_keyring_name, logger)",
            "    elif keyring == _LinuxKeyring.BASICTEXT:",
            "        # when basic text is chosen, all cookies are stored as v10 (so no keyring password is required)",
            "        return None",
            "    assert False, f'Unknown keyring {keyring}'",
            "",
            "",
            "def _get_mac_keyring_password(browser_keyring_name, logger):",
            "    logger.debug('using find-generic-password to obtain password from OSX keychain')",
            "    try:",
            "        stdout, _, returncode = Popen.run(",
            "            ['security', 'find-generic-password',",
            "             '-w',  # write password to stdout",
            "             '-a', browser_keyring_name,  # match 'account'",
            "             '-s', f'{browser_keyring_name} Safe Storage'],  # match 'service'",
            "            stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)",
            "        if returncode:",
            "            logger.warning('find-generic-password failed')",
            "            return None",
            "        return stdout.rstrip(b'\\n')",
            "    except Exception as e:",
            "        logger.warning(f'exception running find-generic-password: {error_to_str(e)}')",
            "        return None",
            "",
            "",
            "def _get_windows_v10_key(browser_root, logger):",
            "    \"\"\"",
            "    References:",
            "        - [1] https://chromium.googlesource.com/chromium/src/+/refs/heads/main/components/os_crypt/sync/os_crypt_win.cc",
            "    \"\"\"",
            "    path = _find_most_recently_used_file(browser_root, 'Local State', logger)",
            "    if path is None:",
            "        logger.error('could not find local state file')",
            "        return None",
            "    logger.debug(f'Found local state file at \"{path}\"')",
            "    with open(path, encoding='utf8') as f:",
            "        data = json.load(f)",
            "    try:",
            "        # kOsCryptEncryptedKeyPrefName in [1]",
            "        base64_key = data['os_crypt']['encrypted_key']",
            "    except KeyError:",
            "        logger.error('no encrypted key in Local State')",
            "        return None",
            "    encrypted_key = base64.b64decode(base64_key)",
            "    # kDPAPIKeyPrefix in [1]",
            "    prefix = b'DPAPI'",
            "    if not encrypted_key.startswith(prefix):",
            "        logger.error('invalid key')",
            "        return None",
            "    return _decrypt_windows_dpapi(encrypted_key[len(prefix):], logger)",
            "",
            "",
            "def pbkdf2_sha1(password, salt, iterations, key_length):",
            "    return pbkdf2_hmac('sha1', password, salt, iterations, key_length)",
            "",
            "",
            "def _decrypt_aes_cbc_multi(ciphertext, keys, logger, initialization_vector=b' ' * 16):",
            "    for key in keys:",
            "        plaintext = unpad_pkcs7(aes_cbc_decrypt_bytes(ciphertext, key, initialization_vector))",
            "        try:",
            "            return plaintext.decode()",
            "        except UnicodeDecodeError:",
            "            pass",
            "    logger.warning('failed to decrypt cookie (AES-CBC) because UTF-8 decoding failed. Possibly the key is wrong?', only_once=True)",
            "    return None",
            "",
            "",
            "def _decrypt_aes_gcm(ciphertext, key, nonce, authentication_tag, logger):",
            "    try:",
            "        plaintext = aes_gcm_decrypt_and_verify_bytes(ciphertext, key, authentication_tag, nonce)",
            "    except ValueError:",
            "        logger.warning('failed to decrypt cookie (AES-GCM) because the MAC check failed. Possibly the key is wrong?', only_once=True)",
            "        return None",
            "",
            "    try:",
            "        return plaintext.decode()",
            "    except UnicodeDecodeError:",
            "        logger.warning('failed to decrypt cookie (AES-GCM) because UTF-8 decoding failed. Possibly the key is wrong?', only_once=True)",
            "        return None",
            "",
            "",
            "def _decrypt_windows_dpapi(ciphertext, logger):",
            "    \"\"\"",
            "    References:",
            "        - https://docs.microsoft.com/en-us/windows/win32/api/dpapi/nf-dpapi-cryptunprotectdata",
            "    \"\"\"",
            "",
            "    import ctypes",
            "    import ctypes.wintypes",
            "",
            "    class DATA_BLOB(ctypes.Structure):",
            "        _fields_ = [('cbData', ctypes.wintypes.DWORD),",
            "                    ('pbData', ctypes.POINTER(ctypes.c_char))]",
            "",
            "    buffer = ctypes.create_string_buffer(ciphertext)",
            "    blob_in = DATA_BLOB(ctypes.sizeof(buffer), buffer)",
            "    blob_out = DATA_BLOB()",
            "    ret = ctypes.windll.crypt32.CryptUnprotectData(",
            "        ctypes.byref(blob_in),  # pDataIn",
            "        None,  # ppszDataDescr: human readable description of pDataIn",
            "        None,  # pOptionalEntropy: salt?",
            "        None,  # pvReserved: must be NULL",
            "        None,  # pPromptStruct: information about prompts to display",
            "        0,  # dwFlags",
            "        ctypes.byref(blob_out)  # pDataOut",
            "    )",
            "    if not ret:",
            "        logger.warning('failed to decrypt with DPAPI', only_once=True)",
            "        return None",
            "",
            "    result = ctypes.string_at(blob_out.pbData, blob_out.cbData)",
            "    ctypes.windll.kernel32.LocalFree(blob_out.pbData)",
            "    return result",
            "",
            "",
            "def _config_home():",
            "    return os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config'))",
            "",
            "",
            "def _open_database_copy(database_path, tmpdir):",
            "    # cannot open sqlite databases if they are already in use (e.g. by the browser)",
            "    database_copy_path = os.path.join(tmpdir, 'temporary.sqlite')",
            "    shutil.copy(database_path, database_copy_path)",
            "    conn = sqlite3.connect(database_copy_path)",
            "    return conn.cursor()",
            "",
            "",
            "def _get_column_names(cursor, table_name):",
            "    table_info = cursor.execute(f'PRAGMA table_info({table_name})').fetchall()",
            "    return [row[1].decode() for row in table_info]",
            "",
            "",
            "def _find_most_recently_used_file(root, filename, logger):",
            "    # if there are multiple browser profiles, take the most recently used one",
            "    i, paths = 0, []",
            "    with _create_progress_bar(logger) as progress_bar:",
            "        for curr_root, dirs, files in os.walk(root):",
            "            for file in files:",
            "                i += 1",
            "                progress_bar.print(f'Searching for \"{filename}\": {i: 6d} files searched')",
            "                if file == filename:",
            "                    paths.append(os.path.join(curr_root, file))",
            "    return None if not paths else max(paths, key=lambda path: os.lstat(path).st_mtime)",
            "",
            "",
            "def _merge_cookie_jars(jars):",
            "    output_jar = YoutubeDLCookieJar()",
            "    for jar in jars:",
            "        for cookie in jar:",
            "            output_jar.set_cookie(cookie)",
            "        if jar.filename is not None:",
            "            output_jar.filename = jar.filename",
            "    return output_jar",
            "",
            "",
            "def _is_path(value):",
            "    return os.path.sep in value",
            "",
            "",
            "def _parse_browser_specification(browser_name, profile=None, keyring=None, container=None):",
            "    if browser_name not in SUPPORTED_BROWSERS:",
            "        raise ValueError(f'unsupported browser: \"{browser_name}\"')",
            "    if keyring not in (None, *SUPPORTED_KEYRINGS):",
            "        raise ValueError(f'unsupported keyring: \"{keyring}\"')",
            "    if profile is not None and _is_path(expand_path(profile)):",
            "        profile = expand_path(profile)",
            "    return browser_name, profile, keyring, container",
            "",
            "",
            "class LenientSimpleCookie(http.cookies.SimpleCookie):",
            "    \"\"\"More lenient version of http.cookies.SimpleCookie\"\"\"",
            "    # From https://github.com/python/cpython/blob/v3.10.7/Lib/http/cookies.py",
            "    # We use Morsel's legal key chars to avoid errors on setting values",
            "    _LEGAL_KEY_CHARS = r'\\w\\d' + re.escape('!#$%&\\'*+-.:^_`|~')",
            "    _LEGAL_VALUE_CHARS = _LEGAL_KEY_CHARS + re.escape('(),/<=>?@[]{}')",
            "",
            "    _RESERVED = {",
            "        \"expires\",",
            "        \"path\",",
            "        \"comment\",",
            "        \"domain\",",
            "        \"max-age\",",
            "        \"secure\",",
            "        \"httponly\",",
            "        \"version\",",
            "        \"samesite\",",
            "    }",
            "",
            "    _FLAGS = {\"secure\", \"httponly\"}",
            "",
            "    # Added 'bad' group to catch the remaining value",
            "    _COOKIE_PATTERN = re.compile(r\"\"\"",
            "        \\s*                            # Optional whitespace at start of cookie",
            "        (?P<key>                       # Start of group 'key'",
            "        [\"\"\" + _LEGAL_KEY_CHARS + r\"\"\"]+?# Any word of at least one letter",
            "        )                              # End of group 'key'",
            "        (                              # Optional group: there may not be a value.",
            "        \\s*=\\s*                          # Equal Sign",
            "        (                                # Start of potential value",
            "        (?P<val>                           # Start of group 'val'",
            "        \"(?:[^\\\\\"]|\\\\.)*\"                    # Any doublequoted string",
            "        |                                    # or",
            "        \\w{3},\\s[\\w\\d\\s-]{9,11}\\s[\\d:]{8}\\sGMT # Special case for \"expires\" attr",
            "        |                                    # or",
            "        [\"\"\" + _LEGAL_VALUE_CHARS + r\"\"\"]*     # Any word or empty string",
            "        )                                  # End of group 'val'",
            "        |                                  # or",
            "        (?P<bad>(?:\\\\;|[^;])*?)            # 'bad' group fallback for invalid values",
            "        )                                # End of potential value",
            "        )?                             # End of optional value group",
            "        \\s*                            # Any number of spaces.",
            "        (\\s+|;|$)                      # Ending either at space, semicolon, or EOS.",
            "        \"\"\", re.ASCII | re.VERBOSE)",
            "",
            "    def load(self, data):",
            "        # Workaround for https://github.com/yt-dlp/yt-dlp/issues/4776",
            "        if not isinstance(data, str):",
            "            return super().load(data)",
            "",
            "        morsel = None",
            "        for match in self._COOKIE_PATTERN.finditer(data):",
            "            if match.group('bad'):",
            "                morsel = None",
            "                continue",
            "",
            "            key, value = match.group('key', 'val')",
            "",
            "            is_attribute = False",
            "            if key.startswith('$'):",
            "                key = key[1:]",
            "                is_attribute = True",
            "",
            "            lower_key = key.lower()",
            "            if lower_key in self._RESERVED:",
            "                if morsel is None:",
            "                    continue",
            "",
            "                if value is None:",
            "                    if lower_key not in self._FLAGS:",
            "                        morsel = None",
            "                        continue",
            "                    value = True",
            "                else:",
            "                    value, _ = self.value_decode(value)",
            "",
            "                morsel[key] = value",
            "",
            "            elif is_attribute:",
            "                morsel = None",
            "",
            "            elif value is not None:",
            "                morsel = self.get(key, http.cookies.Morsel())",
            "                real_value, coded_value = self.value_decode(value)",
            "                morsel.set(key, real_value, coded_value)",
            "                self[key] = morsel",
            "",
            "            else:",
            "                morsel = None",
            "",
            "",
            "class YoutubeDLCookieJar(http.cookiejar.MozillaCookieJar):",
            "    \"\"\"",
            "    See [1] for cookie file format.",
            "",
            "    1. https://curl.haxx.se/docs/http-cookies.html",
            "    \"\"\"",
            "    _HTTPONLY_PREFIX = '#HttpOnly_'",
            "    _ENTRY_LEN = 7",
            "    _HEADER = '''# Netscape HTTP Cookie File",
            "# This file is generated by yt-dlp.  Do not edit.",
            "",
            "'''",
            "    _CookieFileEntry = collections.namedtuple(",
            "        'CookieFileEntry',",
            "        ('domain_name', 'include_subdomains', 'path', 'https_only', 'expires_at', 'name', 'value'))",
            "",
            "    def __init__(self, filename=None, *args, **kwargs):",
            "        super().__init__(None, *args, **kwargs)",
            "        if is_path_like(filename):",
            "            filename = os.fspath(filename)",
            "        self.filename = filename",
            "",
            "    @staticmethod",
            "    def _true_or_false(cndn):",
            "        return 'TRUE' if cndn else 'FALSE'",
            "",
            "    @contextlib.contextmanager",
            "    def open(self, file, *, write=False):",
            "        if is_path_like(file):",
            "            with open(file, 'w' if write else 'r', encoding='utf-8') as f:",
            "                yield f",
            "        else:",
            "            if write:",
            "                file.truncate(0)",
            "            yield file",
            "",
            "    def _really_save(self, f, ignore_discard=False, ignore_expires=False):",
            "        now = time.time()",
            "        for cookie in self:",
            "            if (not ignore_discard and cookie.discard",
            "                    or not ignore_expires and cookie.is_expired(now)):",
            "                continue",
            "            name, value = cookie.name, cookie.value",
            "            if value is None:",
            "                # cookies.txt regards 'Set-Cookie: foo' as a cookie",
            "                # with no name, whereas http.cookiejar regards it as a",
            "                # cookie with no value.",
            "                name, value = '', name",
            "            f.write('%s\\n' % '\\t'.join((",
            "                cookie.domain,",
            "                self._true_or_false(cookie.domain.startswith('.')),",
            "                cookie.path,",
            "                self._true_or_false(cookie.secure),",
            "                str_or_none(cookie.expires, default=''),",
            "                name, value",
            "            )))",
            "",
            "    def save(self, filename=None, *args, **kwargs):",
            "        \"\"\"",
            "        Save cookies to a file.",
            "        Code is taken from CPython 3.6",
            "        https://github.com/python/cpython/blob/8d999cbf4adea053be6dbb612b9844635c4dfb8e/Lib/http/cookiejar.py#L2091-L2117 \"\"\"",
            "",
            "        if filename is None:",
            "            if self.filename is not None:",
            "                filename = self.filename",
            "            else:",
            "                raise ValueError(http.cookiejar.MISSING_FILENAME_TEXT)",
            "",
            "        # Store session cookies with `expires` set to 0 instead of an empty string",
            "        for cookie in self:",
            "            if cookie.expires is None:",
            "                cookie.expires = 0",
            "",
            "        with self.open(filename, write=True) as f:",
            "            f.write(self._HEADER)",
            "            self._really_save(f, *args, **kwargs)",
            "",
            "    def load(self, filename=None, ignore_discard=False, ignore_expires=False):",
            "        \"\"\"Load cookies from a file.\"\"\"",
            "        if filename is None:",
            "            if self.filename is not None:",
            "                filename = self.filename",
            "            else:",
            "                raise ValueError(http.cookiejar.MISSING_FILENAME_TEXT)",
            "",
            "        def prepare_line(line):",
            "            if line.startswith(self._HTTPONLY_PREFIX):",
            "                line = line[len(self._HTTPONLY_PREFIX):]",
            "            # comments and empty lines are fine",
            "            if line.startswith('#') or not line.strip():",
            "                return line",
            "            cookie_list = line.split('\\t')",
            "            if len(cookie_list) != self._ENTRY_LEN:",
            "                raise http.cookiejar.LoadError('invalid length %d' % len(cookie_list))",
            "            cookie = self._CookieFileEntry(*cookie_list)",
            "            if cookie.expires_at and not cookie.expires_at.isdigit():",
            "                raise http.cookiejar.LoadError('invalid expires at %s' % cookie.expires_at)",
            "            return line",
            "",
            "        cf = io.StringIO()",
            "        with self.open(filename) as f:",
            "            for line in f:",
            "                try:",
            "                    cf.write(prepare_line(line))",
            "                except http.cookiejar.LoadError as e:",
            "                    if f'{line.strip()} '[0] in '[{\"':",
            "                        raise http.cookiejar.LoadError(",
            "                            'Cookies file must be Netscape formatted, not JSON. See  '",
            "                            'https://github.com/yt-dlp/yt-dlp/wiki/FAQ#how-do-i-pass-cookies-to-yt-dlp')",
            "                    write_string(f'WARNING: skipping cookie file entry due to {e}: {line!r}\\n')",
            "                    continue",
            "        cf.seek(0)",
            "        self._really_load(cf, filename, ignore_discard, ignore_expires)",
            "        # Session cookies are denoted by either `expires` field set to",
            "        # an empty string or 0. MozillaCookieJar only recognizes the former",
            "        # (see [1]). So we need force the latter to be recognized as session",
            "        # cookies on our own.",
            "        # Session cookies may be important for cookies-based authentication,",
            "        # e.g. usually, when user does not check 'Remember me' check box while",
            "        # logging in on a site, some important cookies are stored as session",
            "        # cookies so that not recognizing them will result in failed login.",
            "        # 1. https://bugs.python.org/issue17164",
            "        for cookie in self:",
            "            # Treat `expires=0` cookies as session cookies",
            "            if cookie.expires == 0:",
            "                cookie.expires = None",
            "                cookie.discard = True",
            "",
            "    def get_cookie_header(self, url):",
            "        \"\"\"Generate a Cookie HTTP header for a given url\"\"\"",
            "        cookie_req = urllib.request.Request(escape_url(sanitize_url(url)))",
            "        self.add_cookie_header(cookie_req)",
            "        return cookie_req.get_header('Cookie')",
            "",
            "    def get_cookies_for_url(self, url):",
            "        \"\"\"Generate a list of Cookie objects for a given url\"\"\"",
            "        # Policy `_now` attribute must be set before calling `_cookies_for_request`",
            "        # Ref: https://github.com/python/cpython/blob/3.7/Lib/http/cookiejar.py#L1360",
            "        self._policy._now = self._now = int(time.time())",
            "        return self._cookies_for_request(urllib.request.Request(escape_url(sanitize_url(url))))",
            "",
            "    def clear(self, *args, **kwargs):",
            "        with contextlib.suppress(KeyError):",
            "            return super().clear(*args, **kwargs)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "yt_dlp/downloader/external.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 1,
                "PatchRowcode": " import enum"
            },
            "1": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " import json"
            },
            "2": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import os.path"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3,
                "PatchRowcode": "+import os"
            },
            "4": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " import re"
            },
            "5": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " import subprocess"
            },
            "6": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " import sys"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 7,
                "PatchRowcode": "+import tempfile"
            },
            "8": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " import time"
            },
            "9": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " import uuid"
            },
            "10": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 43,
                "PatchRowcode": "     def real_download(self, filename, info_dict):"
            },
            "12": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 44,
                "PatchRowcode": "         self.report_destination(filename)"
            },
            "13": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 45,
                "PatchRowcode": "         tmpfilename = self.temp_name(filename)"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+        self._cookies_tempfile = None"
            },
            "15": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": 47,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 48,
                "PatchRowcode": "         try:"
            },
            "17": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 49,
                "PatchRowcode": "             started = time.time()"
            },
            "18": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 56,
                "PatchRowcode": "             # should take place"
            },
            "19": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": 57,
                "PatchRowcode": "             retval = 0"
            },
            "20": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 58,
                "PatchRowcode": "             self.to_screen('[%s] Interrupted by user' % self.get_basename())"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+        finally:"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 60,
                "PatchRowcode": "+            if self._cookies_tempfile:"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 61,
                "PatchRowcode": "+                self.try_remove(self._cookies_tempfile)"
            },
            "24": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 62,
                "PatchRowcode": " "
            },
            "25": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": 63,
                "PatchRowcode": "         if retval == 0:"
            },
            "26": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 64,
                "PatchRowcode": "             status = {"
            },
            "27": {
                "beforePatchRowNumber": 125,
                "afterPatchRowNumber": 130,
                "PatchRowcode": "             self.get_basename(), self.params.get('external_downloader_args'), self.EXE_NAME,"
            },
            "28": {
                "beforePatchRowNumber": 126,
                "afterPatchRowNumber": 131,
                "PatchRowcode": "             keys, *args, **kwargs)"
            },
            "29": {
                "beforePatchRowNumber": 127,
                "afterPatchRowNumber": 132,
                "PatchRowcode": " "
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 133,
                "PatchRowcode": "+    def _write_cookies(self):"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 134,
                "PatchRowcode": "+        if not self.ydl.cookiejar.filename:"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 135,
                "PatchRowcode": "+            tmp_cookies = tempfile.NamedTemporaryFile(suffix='.cookies', delete=False)"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 136,
                "PatchRowcode": "+            tmp_cookies.close()"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 137,
                "PatchRowcode": "+            self._cookies_tempfile = tmp_cookies.name"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 138,
                "PatchRowcode": "+            self.to_screen(f'[download] Writing temporary cookies file to \"{self._cookies_tempfile}\"')"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 139,
                "PatchRowcode": "+        # real_download resets _cookies_tempfile; if it's None then save() will write to cookiejar.filename"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 140,
                "PatchRowcode": "+        self.ydl.cookiejar.save(self._cookies_tempfile)"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 141,
                "PatchRowcode": "+        return self.ydl.cookiejar.filename or self._cookies_tempfile"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 142,
                "PatchRowcode": "+"
            },
            "40": {
                "beforePatchRowNumber": 128,
                "afterPatchRowNumber": 143,
                "PatchRowcode": "     def _call_downloader(self, tmpfilename, info_dict):"
            },
            "41": {
                "beforePatchRowNumber": 129,
                "afterPatchRowNumber": 144,
                "PatchRowcode": "         \"\"\" Either overwrite this or implement _make_cmd \"\"\""
            },
            "42": {
                "beforePatchRowNumber": 130,
                "afterPatchRowNumber": 145,
                "PatchRowcode": "         cmd = [encodeArgument(a) for a in self._make_cmd(tmpfilename, info_dict)]"
            },
            "43": {
                "beforePatchRowNumber": 184,
                "afterPatchRowNumber": 199,
                "PatchRowcode": " "
            },
            "44": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 200,
                "PatchRowcode": "     def _make_cmd(self, tmpfilename, info_dict):"
            },
            "45": {
                "beforePatchRowNumber": 186,
                "afterPatchRowNumber": 201,
                "PatchRowcode": "         cmd = [self.exe, '--location', '-o', tmpfilename, '--compressed']"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 202,
                "PatchRowcode": "+        if self.ydl.cookiejar.get_cookie_header(info_dict['url']):"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 203,
                "PatchRowcode": "+            cmd += ['--cookie-jar', self._write_cookies()]"
            },
            "48": {
                "beforePatchRowNumber": 187,
                "afterPatchRowNumber": 204,
                "PatchRowcode": "         if info_dict.get('http_headers') is not None:"
            },
            "49": {
                "beforePatchRowNumber": 188,
                "afterPatchRowNumber": 205,
                "PatchRowcode": "             for key, val in info_dict['http_headers'].items():"
            },
            "50": {
                "beforePatchRowNumber": 189,
                "afterPatchRowNumber": 206,
                "PatchRowcode": "                 cmd += ['--header', f'{key}: {val}']"
            },
            "51": {
                "beforePatchRowNumber": 214,
                "afterPatchRowNumber": 231,
                "PatchRowcode": "         if info_dict.get('http_headers') is not None:"
            },
            "52": {
                "beforePatchRowNumber": 215,
                "afterPatchRowNumber": 232,
                "PatchRowcode": "             for key, val in info_dict['http_headers'].items():"
            },
            "53": {
                "beforePatchRowNumber": 216,
                "afterPatchRowNumber": 233,
                "PatchRowcode": "                 cmd += ['-H', f'{key}: {val}']"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 234,
                "PatchRowcode": "+        cookie_header = self.ydl.cookiejar.get_cookie_header(info_dict['url'])"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 235,
                "PatchRowcode": "+        if cookie_header:"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 236,
                "PatchRowcode": "+            cmd += [f'Cookie: {cookie_header}', '--max-redirect=0']"
            },
            "57": {
                "beforePatchRowNumber": 217,
                "afterPatchRowNumber": 237,
                "PatchRowcode": "         cmd += self._configuration_args()"
            },
            "58": {
                "beforePatchRowNumber": 218,
                "afterPatchRowNumber": 238,
                "PatchRowcode": "         cmd += ['--', info_dict['url']]"
            },
            "59": {
                "beforePatchRowNumber": 219,
                "afterPatchRowNumber": 239,
                "PatchRowcode": "         return cmd"
            },
            "60": {
                "beforePatchRowNumber": 223,
                "afterPatchRowNumber": 243,
                "PatchRowcode": "     AVAILABLE_OPT = '--version'"
            },
            "61": {
                "beforePatchRowNumber": 224,
                "afterPatchRowNumber": 244,
                "PatchRowcode": " "
            },
            "62": {
                "beforePatchRowNumber": 225,
                "afterPatchRowNumber": 245,
                "PatchRowcode": "     def _make_cmd(self, tmpfilename, info_dict):"
            },
            "63": {
                "beforePatchRowNumber": 226,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        cmd = [self.exe, '-O', tmpfilename, '-nv', '--no-cookies', '--compression=auto']"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 246,
                "PatchRowcode": "+        cmd = [self.exe, '-O', tmpfilename, '-nv', '--compression=auto']"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 247,
                "PatchRowcode": "+        if self.ydl.cookiejar.get_cookie_header(info_dict['url']):"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 248,
                "PatchRowcode": "+            cmd += ['--load-cookies', self._write_cookies()]"
            },
            "67": {
                "beforePatchRowNumber": 227,
                "afterPatchRowNumber": 249,
                "PatchRowcode": "         if info_dict.get('http_headers') is not None:"
            },
            "68": {
                "beforePatchRowNumber": 228,
                "afterPatchRowNumber": 250,
                "PatchRowcode": "             for key, val in info_dict['http_headers'].items():"
            },
            "69": {
                "beforePatchRowNumber": 229,
                "afterPatchRowNumber": 251,
                "PatchRowcode": "                 cmd += ['--header', f'{key}: {val}']"
            },
            "70": {
                "beforePatchRowNumber": 279,
                "afterPatchRowNumber": 301,
                "PatchRowcode": "         else:"
            },
            "71": {
                "beforePatchRowNumber": 280,
                "afterPatchRowNumber": 302,
                "PatchRowcode": "             cmd += ['--min-split-size', '1M']"
            },
            "72": {
                "beforePatchRowNumber": 281,
                "afterPatchRowNumber": 303,
                "PatchRowcode": " "
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 304,
                "PatchRowcode": "+        if self.ydl.cookiejar.get_cookie_header(info_dict['url']):"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 305,
                "PatchRowcode": "+            cmd += [f'--load-cookies={self._write_cookies()}']"
            },
            "75": {
                "beforePatchRowNumber": 282,
                "afterPatchRowNumber": 306,
                "PatchRowcode": "         if info_dict.get('http_headers') is not None:"
            },
            "76": {
                "beforePatchRowNumber": 283,
                "afterPatchRowNumber": 307,
                "PatchRowcode": "             for key, val in info_dict['http_headers'].items():"
            },
            "77": {
                "beforePatchRowNumber": 284,
                "afterPatchRowNumber": 308,
                "PatchRowcode": "                 cmd += ['--header', f'{key}: {val}']"
            },
            "78": {
                "beforePatchRowNumber": 417,
                "afterPatchRowNumber": 441,
                "PatchRowcode": "         if info_dict.get('http_headers') is not None:"
            },
            "79": {
                "beforePatchRowNumber": 418,
                "afterPatchRowNumber": 442,
                "PatchRowcode": "             for key, val in info_dict['http_headers'].items():"
            },
            "80": {
                "beforePatchRowNumber": 419,
                "afterPatchRowNumber": 443,
                "PatchRowcode": "                 cmd += [f'{key}:{val}']"
            },
            "81": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 444,
                "PatchRowcode": "+"
            },
            "82": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 445,
                "PatchRowcode": "+        # httpie 3.1.0+ removes the Cookie header on redirect, so this should be safe for now. [1]"
            },
            "83": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 446,
                "PatchRowcode": "+        # If we ever need cookie handling for redirects, we can export the cookiejar into a session. [2]"
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 447,
                "PatchRowcode": "+        # 1: https://github.com/httpie/httpie/security/advisories/GHSA-9w4w-cpc8-h2fq"
            },
            "85": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 448,
                "PatchRowcode": "+        # 2: https://httpie.io/docs/cli/sessions"
            },
            "86": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 449,
                "PatchRowcode": "+        cookie_header = self.ydl.cookiejar.get_cookie_header(info_dict['url'])"
            },
            "87": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 450,
                "PatchRowcode": "+        if cookie_header:"
            },
            "88": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 451,
                "PatchRowcode": "+            cmd += [f'Cookie:{cookie_header}']"
            },
            "89": {
                "beforePatchRowNumber": 420,
                "afterPatchRowNumber": 452,
                "PatchRowcode": "         return cmd"
            },
            "90": {
                "beforePatchRowNumber": 421,
                "afterPatchRowNumber": 453,
                "PatchRowcode": " "
            },
            "91": {
                "beforePatchRowNumber": 422,
                "afterPatchRowNumber": 454,
                "PatchRowcode": " "
            },
            "92": {
                "beforePatchRowNumber": 527,
                "afterPatchRowNumber": 559,
                "PatchRowcode": " "
            },
            "93": {
                "beforePatchRowNumber": 528,
                "afterPatchRowNumber": 560,
                "PatchRowcode": "         selected_formats = info_dict.get('requested_formats') or [info_dict]"
            },
            "94": {
                "beforePatchRowNumber": 529,
                "afterPatchRowNumber": 561,
                "PatchRowcode": "         for i, fmt in enumerate(selected_formats):"
            },
            "95": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 562,
                "PatchRowcode": "+            cookies = self.ydl.cookiejar.get_cookies_for_url(fmt['url'])"
            },
            "96": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 563,
                "PatchRowcode": "+            if cookies:"
            },
            "97": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 564,
                "PatchRowcode": "+                args.extend(['-cookies', ''.join("
            },
            "98": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 565,
                "PatchRowcode": "+                    f'{cookie.name}={cookie.value}; path={cookie.path}; domain={cookie.domain};\\r\\n'"
            },
            "99": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 566,
                "PatchRowcode": "+                    for cookie in cookies)])"
            },
            "100": {
                "beforePatchRowNumber": 530,
                "afterPatchRowNumber": 567,
                "PatchRowcode": "             if fmt.get('http_headers') and re.match(r'^https?://', fmt['url']):"
            },
            "101": {
                "beforePatchRowNumber": 531,
                "afterPatchRowNumber": 568,
                "PatchRowcode": "                 # Trailing \\r\\n after each HTTP header is important to prevent warning from ffmpeg/avconv:"
            },
            "102": {
                "beforePatchRowNumber": 532,
                "afterPatchRowNumber": 569,
                "PatchRowcode": "                 # [http @ 00000000003d2fa0] No trailing CRLF found in HTTP header."
            }
        },
        "frontPatchFile": [
            "import enum",
            "import json",
            "import os.path",
            "import re",
            "import subprocess",
            "import sys",
            "import time",
            "import uuid",
            "",
            "from .fragment import FragmentFD",
            "from ..compat import functools",
            "from ..postprocessor.ffmpeg import EXT_TO_OUT_FORMATS, FFmpegPostProcessor",
            "from ..utils import (",
            "    Popen,",
            "    RetryManager,",
            "    _configuration_args,",
            "    check_executable,",
            "    classproperty,",
            "    cli_bool_option,",
            "    cli_option,",
            "    cli_valueless_option,",
            "    determine_ext,",
            "    encodeArgument,",
            "    encodeFilename,",
            "    find_available_port,",
            "    remove_end,",
            "    sanitized_Request,",
            "    traverse_obj,",
            ")",
            "",
            "",
            "class Features(enum.Enum):",
            "    TO_STDOUT = enum.auto()",
            "    MULTIPLE_FORMATS = enum.auto()",
            "",
            "",
            "class ExternalFD(FragmentFD):",
            "    SUPPORTED_PROTOCOLS = ('http', 'https', 'ftp', 'ftps')",
            "    SUPPORTED_FEATURES = ()",
            "    _CAPTURE_STDERR = True",
            "",
            "    def real_download(self, filename, info_dict):",
            "        self.report_destination(filename)",
            "        tmpfilename = self.temp_name(filename)",
            "",
            "        try:",
            "            started = time.time()",
            "            retval = self._call_downloader(tmpfilename, info_dict)",
            "        except KeyboardInterrupt:",
            "            if not info_dict.get('is_live'):",
            "                raise",
            "            # Live stream downloading cancellation should be considered as",
            "            # correct and expected termination thus all postprocessing",
            "            # should take place",
            "            retval = 0",
            "            self.to_screen('[%s] Interrupted by user' % self.get_basename())",
            "",
            "        if retval == 0:",
            "            status = {",
            "                'filename': filename,",
            "                'status': 'finished',",
            "                'elapsed': time.time() - started,",
            "            }",
            "            if filename != '-':",
            "                fsize = os.path.getsize(encodeFilename(tmpfilename))",
            "                self.try_rename(tmpfilename, filename)",
            "                status.update({",
            "                    'downloaded_bytes': fsize,",
            "                    'total_bytes': fsize,",
            "                })",
            "            self._hook_progress(status, info_dict)",
            "            return True",
            "        else:",
            "            self.to_stderr('\\n')",
            "            self.report_error('%s exited with code %d' % (",
            "                self.get_basename(), retval))",
            "            return False",
            "",
            "    @classmethod",
            "    def get_basename(cls):",
            "        return cls.__name__[:-2].lower()",
            "",
            "    @classproperty",
            "    def EXE_NAME(cls):",
            "        return cls.get_basename()",
            "",
            "    @functools.cached_property",
            "    def exe(self):",
            "        return self.EXE_NAME",
            "",
            "    @classmethod",
            "    def available(cls, path=None):",
            "        path = check_executable(",
            "            cls.EXE_NAME if path in (None, cls.get_basename()) else path,",
            "            [cls.AVAILABLE_OPT])",
            "        if not path:",
            "            return False",
            "        cls.exe = path",
            "        return path",
            "",
            "    @classmethod",
            "    def supports(cls, info_dict):",
            "        return all((",
            "            not info_dict.get('to_stdout') or Features.TO_STDOUT in cls.SUPPORTED_FEATURES,",
            "            '+' not in info_dict['protocol'] or Features.MULTIPLE_FORMATS in cls.SUPPORTED_FEATURES,",
            "            not traverse_obj(info_dict, ('hls_aes', ...), 'extra_param_to_segment_url'),",
            "            all(proto in cls.SUPPORTED_PROTOCOLS for proto in info_dict['protocol'].split('+')),",
            "        ))",
            "",
            "    @classmethod",
            "    def can_download(cls, info_dict, path=None):",
            "        return cls.available(path) and cls.supports(info_dict)",
            "",
            "    def _option(self, command_option, param):",
            "        return cli_option(self.params, command_option, param)",
            "",
            "    def _bool_option(self, command_option, param, true_value='true', false_value='false', separator=None):",
            "        return cli_bool_option(self.params, command_option, param, true_value, false_value, separator)",
            "",
            "    def _valueless_option(self, command_option, param, expected_value=True):",
            "        return cli_valueless_option(self.params, command_option, param, expected_value)",
            "",
            "    def _configuration_args(self, keys=None, *args, **kwargs):",
            "        return _configuration_args(",
            "            self.get_basename(), self.params.get('external_downloader_args'), self.EXE_NAME,",
            "            keys, *args, **kwargs)",
            "",
            "    def _call_downloader(self, tmpfilename, info_dict):",
            "        \"\"\" Either overwrite this or implement _make_cmd \"\"\"",
            "        cmd = [encodeArgument(a) for a in self._make_cmd(tmpfilename, info_dict)]",
            "",
            "        self._debug_cmd(cmd)",
            "",
            "        if 'fragments' not in info_dict:",
            "            _, stderr, returncode = self._call_process(cmd, info_dict)",
            "            if returncode and stderr:",
            "                self.to_stderr(stderr)",
            "            return returncode",
            "",
            "        skip_unavailable_fragments = self.params.get('skip_unavailable_fragments', True)",
            "",
            "        retry_manager = RetryManager(self.params.get('fragment_retries'), self.report_retry,",
            "                                     frag_index=None, fatal=not skip_unavailable_fragments)",
            "        for retry in retry_manager:",
            "            _, stderr, returncode = self._call_process(cmd, info_dict)",
            "            if not returncode:",
            "                break",
            "            # TODO: Decide whether to retry based on error code",
            "            # https://aria2.github.io/manual/en/html/aria2c.html#exit-status",
            "            if stderr:",
            "                self.to_stderr(stderr)",
            "            retry.error = Exception()",
            "            continue",
            "        if not skip_unavailable_fragments and retry_manager.error:",
            "            return -1",
            "",
            "        decrypt_fragment = self.decrypter(info_dict)",
            "        dest, _ = self.sanitize_open(tmpfilename, 'wb')",
            "        for frag_index, fragment in enumerate(info_dict['fragments']):",
            "            fragment_filename = '%s-Frag%d' % (tmpfilename, frag_index)",
            "            try:",
            "                src, _ = self.sanitize_open(fragment_filename, 'rb')",
            "            except OSError as err:",
            "                if skip_unavailable_fragments and frag_index > 1:",
            "                    self.report_skip_fragment(frag_index, err)",
            "                    continue",
            "                self.report_error(f'Unable to open fragment {frag_index}; {err}')",
            "                return -1",
            "            dest.write(decrypt_fragment(fragment, src.read()))",
            "            src.close()",
            "            if not self.params.get('keep_fragments', False):",
            "                self.try_remove(encodeFilename(fragment_filename))",
            "        dest.close()",
            "        self.try_remove(encodeFilename('%s.frag.urls' % tmpfilename))",
            "        return 0",
            "",
            "    def _call_process(self, cmd, info_dict):",
            "        return Popen.run(cmd, text=True, stderr=subprocess.PIPE if self._CAPTURE_STDERR else None)",
            "",
            "",
            "class CurlFD(ExternalFD):",
            "    AVAILABLE_OPT = '-V'",
            "    _CAPTURE_STDERR = False  # curl writes the progress to stderr",
            "",
            "    def _make_cmd(self, tmpfilename, info_dict):",
            "        cmd = [self.exe, '--location', '-o', tmpfilename, '--compressed']",
            "        if info_dict.get('http_headers') is not None:",
            "            for key, val in info_dict['http_headers'].items():",
            "                cmd += ['--header', f'{key}: {val}']",
            "",
            "        cmd += self._bool_option('--continue-at', 'continuedl', '-', '0')",
            "        cmd += self._valueless_option('--silent', 'noprogress')",
            "        cmd += self._valueless_option('--verbose', 'verbose')",
            "        cmd += self._option('--limit-rate', 'ratelimit')",
            "        retry = self._option('--retry', 'retries')",
            "        if len(retry) == 2:",
            "            if retry[1] in ('inf', 'infinite'):",
            "                retry[1] = '2147483647'",
            "            cmd += retry",
            "        cmd += self._option('--max-filesize', 'max_filesize')",
            "        cmd += self._option('--interface', 'source_address')",
            "        cmd += self._option('--proxy', 'proxy')",
            "        cmd += self._valueless_option('--insecure', 'nocheckcertificate')",
            "        cmd += self._configuration_args()",
            "        cmd += ['--', info_dict['url']]",
            "        return cmd",
            "",
            "",
            "class AxelFD(ExternalFD):",
            "    AVAILABLE_OPT = '-V'",
            "",
            "    def _make_cmd(self, tmpfilename, info_dict):",
            "        cmd = [self.exe, '-o', tmpfilename]",
            "        if info_dict.get('http_headers') is not None:",
            "            for key, val in info_dict['http_headers'].items():",
            "                cmd += ['-H', f'{key}: {val}']",
            "        cmd += self._configuration_args()",
            "        cmd += ['--', info_dict['url']]",
            "        return cmd",
            "",
            "",
            "class WgetFD(ExternalFD):",
            "    AVAILABLE_OPT = '--version'",
            "",
            "    def _make_cmd(self, tmpfilename, info_dict):",
            "        cmd = [self.exe, '-O', tmpfilename, '-nv', '--no-cookies', '--compression=auto']",
            "        if info_dict.get('http_headers') is not None:",
            "            for key, val in info_dict['http_headers'].items():",
            "                cmd += ['--header', f'{key}: {val}']",
            "        cmd += self._option('--limit-rate', 'ratelimit')",
            "        retry = self._option('--tries', 'retries')",
            "        if len(retry) == 2:",
            "            if retry[1] in ('inf', 'infinite'):",
            "                retry[1] = '0'",
            "            cmd += retry",
            "        cmd += self._option('--bind-address', 'source_address')",
            "        proxy = self.params.get('proxy')",
            "        if proxy:",
            "            for var in ('http_proxy', 'https_proxy'):",
            "                cmd += ['--execute', f'{var}={proxy}']",
            "        cmd += self._valueless_option('--no-check-certificate', 'nocheckcertificate')",
            "        cmd += self._configuration_args()",
            "        cmd += ['--', info_dict['url']]",
            "        return cmd",
            "",
            "",
            "class Aria2cFD(ExternalFD):",
            "    AVAILABLE_OPT = '-v'",
            "    SUPPORTED_PROTOCOLS = ('http', 'https', 'ftp', 'ftps', 'dash_frag_urls', 'm3u8_frag_urls')",
            "",
            "    @staticmethod",
            "    def supports_manifest(manifest):",
            "        UNSUPPORTED_FEATURES = [",
            "            r'#EXT-X-BYTERANGE',  # playlists composed of byte ranges of media files [1]",
            "            # 1. https://tools.ietf.org/html/draft-pantos-http-live-streaming-17#section-4.3.2.2",
            "        ]",
            "        check_results = (not re.search(feature, manifest) for feature in UNSUPPORTED_FEATURES)",
            "        return all(check_results)",
            "",
            "    @staticmethod",
            "    def _aria2c_filename(fn):",
            "        return fn if os.path.isabs(fn) else f'.{os.path.sep}{fn}'",
            "",
            "    def _call_downloader(self, tmpfilename, info_dict):",
            "        # FIXME: Disabled due to https://github.com/yt-dlp/yt-dlp/issues/5931",
            "        if False and 'no-external-downloader-progress' not in self.params.get('compat_opts', []):",
            "            info_dict['__rpc'] = {",
            "                'port': find_available_port() or 19190,",
            "                'secret': str(uuid.uuid4()),",
            "            }",
            "        return super()._call_downloader(tmpfilename, info_dict)",
            "",
            "    def _make_cmd(self, tmpfilename, info_dict):",
            "        cmd = [self.exe, '-c', '--no-conf',",
            "               '--console-log-level=warn', '--summary-interval=0', '--download-result=hide',",
            "               '--http-accept-gzip=true', '--file-allocation=none', '-x16', '-j16', '-s16']",
            "        if 'fragments' in info_dict:",
            "            cmd += ['--allow-overwrite=true', '--allow-piece-length-change=true']",
            "        else:",
            "            cmd += ['--min-split-size', '1M']",
            "",
            "        if info_dict.get('http_headers') is not None:",
            "            for key, val in info_dict['http_headers'].items():",
            "                cmd += ['--header', f'{key}: {val}']",
            "        cmd += self._option('--max-overall-download-limit', 'ratelimit')",
            "        cmd += self._option('--interface', 'source_address')",
            "        cmd += self._option('--all-proxy', 'proxy')",
            "        cmd += self._bool_option('--check-certificate', 'nocheckcertificate', 'false', 'true', '=')",
            "        cmd += self._bool_option('--remote-time', 'updatetime', 'true', 'false', '=')",
            "        cmd += self._bool_option('--show-console-readout', 'noprogress', 'false', 'true', '=')",
            "        cmd += self._configuration_args()",
            "",
            "        if '__rpc' in info_dict:",
            "            cmd += [",
            "                '--enable-rpc',",
            "                f'--rpc-listen-port={info_dict[\"__rpc\"][\"port\"]}',",
            "                f'--rpc-secret={info_dict[\"__rpc\"][\"secret\"]}']",
            "",
            "        # aria2c strips out spaces from the beginning/end of filenames and paths.",
            "        # We work around this issue by adding a \"./\" to the beginning of the",
            "        # filename and relative path, and adding a \"/\" at the end of the path.",
            "        # See: https://github.com/yt-dlp/yt-dlp/issues/276",
            "        # https://github.com/ytdl-org/youtube-dl/issues/20312",
            "        # https://github.com/aria2/aria2/issues/1373",
            "        dn = os.path.dirname(tmpfilename)",
            "        if dn:",
            "            cmd += ['--dir', self._aria2c_filename(dn) + os.path.sep]",
            "        if 'fragments' not in info_dict:",
            "            cmd += ['--out', self._aria2c_filename(os.path.basename(tmpfilename))]",
            "        cmd += ['--auto-file-renaming=false']",
            "",
            "        if 'fragments' in info_dict:",
            "            cmd += ['--file-allocation=none', '--uri-selector=inorder']",
            "            url_list_file = '%s.frag.urls' % tmpfilename",
            "            url_list = []",
            "            for frag_index, fragment in enumerate(info_dict['fragments']):",
            "                fragment_filename = '%s-Frag%d' % (os.path.basename(tmpfilename), frag_index)",
            "                url_list.append('%s\\n\\tout=%s' % (fragment['url'], self._aria2c_filename(fragment_filename)))",
            "            stream, _ = self.sanitize_open(url_list_file, 'wb')",
            "            stream.write('\\n'.join(url_list).encode())",
            "            stream.close()",
            "            cmd += ['-i', self._aria2c_filename(url_list_file)]",
            "        else:",
            "            cmd += ['--', info_dict['url']]",
            "        return cmd",
            "",
            "    def aria2c_rpc(self, rpc_port, rpc_secret, method, params=()):",
            "        # Does not actually need to be UUID, just unique",
            "        sanitycheck = str(uuid.uuid4())",
            "        d = json.dumps({",
            "            'jsonrpc': '2.0',",
            "            'id': sanitycheck,",
            "            'method': method,",
            "            'params': [f'token:{rpc_secret}', *params],",
            "        }).encode('utf-8')",
            "        request = sanitized_Request(",
            "            f'http://localhost:{rpc_port}/jsonrpc',",
            "            data=d, headers={",
            "                'Content-Type': 'application/json',",
            "                'Content-Length': f'{len(d)}',",
            "                'Ytdl-request-proxy': '__noproxy__',",
            "            })",
            "        with self.ydl.urlopen(request) as r:",
            "            resp = json.load(r)",
            "        assert resp.get('id') == sanitycheck, 'Something went wrong with RPC server'",
            "        return resp['result']",
            "",
            "    def _call_process(self, cmd, info_dict):",
            "        if '__rpc' not in info_dict:",
            "            return super()._call_process(cmd, info_dict)",
            "",
            "        send_rpc = functools.partial(self.aria2c_rpc, info_dict['__rpc']['port'], info_dict['__rpc']['secret'])",
            "        started = time.time()",
            "",
            "        fragmented = 'fragments' in info_dict",
            "        frag_count = len(info_dict['fragments']) if fragmented else 1",
            "        status = {",
            "            'filename': info_dict.get('_filename'),",
            "            'status': 'downloading',",
            "            'elapsed': 0,",
            "            'downloaded_bytes': 0,",
            "            'fragment_count': frag_count if fragmented else None,",
            "            'fragment_index': 0 if fragmented else None,",
            "        }",
            "        self._hook_progress(status, info_dict)",
            "",
            "        def get_stat(key, *obj, average=False):",
            "            val = tuple(filter(None, map(float, traverse_obj(obj, (..., ..., key))))) or [0]",
            "            return sum(val) / (len(val) if average else 1)",
            "",
            "        with Popen(cmd, text=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE) as p:",
            "            # Add a small sleep so that RPC client can receive response,",
            "            # or the connection stalls infinitely",
            "            time.sleep(0.2)",
            "            retval = p.poll()",
            "            while retval is None:",
            "                # We don't use tellStatus as we won't know the GID without reading stdout",
            "                # Ref: https://aria2.github.io/manual/en/html/aria2c.html#aria2.tellActive",
            "                active = send_rpc('aria2.tellActive')",
            "                completed = send_rpc('aria2.tellStopped', [0, frag_count])",
            "",
            "                downloaded = get_stat('totalLength', completed) + get_stat('completedLength', active)",
            "                speed = get_stat('downloadSpeed', active)",
            "                total = frag_count * get_stat('totalLength', active, completed, average=True)",
            "                if total < downloaded:",
            "                    total = None",
            "",
            "                status.update({",
            "                    'downloaded_bytes': int(downloaded),",
            "                    'speed': speed,",
            "                    'total_bytes': None if fragmented else total,",
            "                    'total_bytes_estimate': total,",
            "                    'eta': (total - downloaded) / (speed or 1),",
            "                    'fragment_index': min(frag_count, len(completed) + 1) if fragmented else None,",
            "                    'elapsed': time.time() - started",
            "                })",
            "                self._hook_progress(status, info_dict)",
            "",
            "                if not active and len(completed) >= frag_count:",
            "                    send_rpc('aria2.shutdown')",
            "                    retval = p.wait()",
            "                    break",
            "",
            "                time.sleep(0.1)",
            "                retval = p.poll()",
            "",
            "            return '', p.stderr.read(), retval",
            "",
            "",
            "class HttpieFD(ExternalFD):",
            "    AVAILABLE_OPT = '--version'",
            "    EXE_NAME = 'http'",
            "",
            "    def _make_cmd(self, tmpfilename, info_dict):",
            "        cmd = ['http', '--download', '--output', tmpfilename, info_dict['url']]",
            "",
            "        if info_dict.get('http_headers') is not None:",
            "            for key, val in info_dict['http_headers'].items():",
            "                cmd += [f'{key}:{val}']",
            "        return cmd",
            "",
            "",
            "class FFmpegFD(ExternalFD):",
            "    SUPPORTED_PROTOCOLS = ('http', 'https', 'ftp', 'ftps', 'm3u8', 'm3u8_native', 'rtsp', 'rtmp', 'rtmp_ffmpeg', 'mms', 'http_dash_segments')",
            "    SUPPORTED_FEATURES = (Features.TO_STDOUT, Features.MULTIPLE_FORMATS)",
            "",
            "    @classmethod",
            "    def available(cls, path=None):",
            "        # TODO: Fix path for ffmpeg",
            "        # Fixme: This may be wrong when --ffmpeg-location is used",
            "        return FFmpegPostProcessor().available",
            "",
            "    def on_process_started(self, proc, stdin):",
            "        \"\"\" Override this in subclasses  \"\"\"",
            "        pass",
            "",
            "    @classmethod",
            "    def can_merge_formats(cls, info_dict, params):",
            "        return (",
            "            info_dict.get('requested_formats')",
            "            and info_dict.get('protocol')",
            "            and not params.get('allow_unplayable_formats')",
            "            and 'no-direct-merge' not in params.get('compat_opts', [])",
            "            and cls.can_download(info_dict))",
            "",
            "    def _call_downloader(self, tmpfilename, info_dict):",
            "        ffpp = FFmpegPostProcessor(downloader=self)",
            "        if not ffpp.available:",
            "            self.report_error('m3u8 download detected but ffmpeg could not be found. Please install')",
            "            return False",
            "        ffpp.check_version()",
            "",
            "        args = [ffpp.executable, '-y']",
            "",
            "        for log_level in ('quiet', 'verbose'):",
            "            if self.params.get(log_level, False):",
            "                args += ['-loglevel', log_level]",
            "                break",
            "        if not self.params.get('verbose'):",
            "            args += ['-hide_banner']",
            "",
            "        args += traverse_obj(info_dict, ('downloader_options', 'ffmpeg_args'), default=[])",
            "",
            "        # These exists only for compatibility. Extractors should use",
            "        # info_dict['downloader_options']['ffmpeg_args'] instead",
            "        args += info_dict.get('_ffmpeg_args') or []",
            "        seekable = info_dict.get('_seekable')",
            "        if seekable is not None:",
            "            # setting -seekable prevents ffmpeg from guessing if the server",
            "            # supports seeking(by adding the header `Range: bytes=0-`), which",
            "            # can cause problems in some cases",
            "            # https://github.com/ytdl-org/youtube-dl/issues/11800#issuecomment-275037127",
            "            # http://trac.ffmpeg.org/ticket/6125#comment:10",
            "            args += ['-seekable', '1' if seekable else '0']",
            "",
            "        env = None",
            "        proxy = self.params.get('proxy')",
            "        if proxy:",
            "            if not re.match(r'^[\\da-zA-Z]+://', proxy):",
            "                proxy = 'http://%s' % proxy",
            "",
            "            if proxy.startswith('socks'):",
            "                self.report_warning(",
            "                    '%s does not support SOCKS proxies. Downloading is likely to fail. '",
            "                    'Consider adding --hls-prefer-native to your command.' % self.get_basename())",
            "",
            "            # Since December 2015 ffmpeg supports -http_proxy option (see",
            "            # http://git.videolan.org/?p=ffmpeg.git;a=commit;h=b4eb1f29ebddd60c41a2eb39f5af701e38e0d3fd)",
            "            # We could switch to the following code if we are able to detect version properly",
            "            # args += ['-http_proxy', proxy]",
            "            env = os.environ.copy()",
            "            env['HTTP_PROXY'] = proxy",
            "            env['http_proxy'] = proxy",
            "",
            "        protocol = info_dict.get('protocol')",
            "",
            "        if protocol == 'rtmp':",
            "            player_url = info_dict.get('player_url')",
            "            page_url = info_dict.get('page_url')",
            "            app = info_dict.get('app')",
            "            play_path = info_dict.get('play_path')",
            "            tc_url = info_dict.get('tc_url')",
            "            flash_version = info_dict.get('flash_version')",
            "            live = info_dict.get('rtmp_live', False)",
            "            conn = info_dict.get('rtmp_conn')",
            "            if player_url is not None:",
            "                args += ['-rtmp_swfverify', player_url]",
            "            if page_url is not None:",
            "                args += ['-rtmp_pageurl', page_url]",
            "            if app is not None:",
            "                args += ['-rtmp_app', app]",
            "            if play_path is not None:",
            "                args += ['-rtmp_playpath', play_path]",
            "            if tc_url is not None:",
            "                args += ['-rtmp_tcurl', tc_url]",
            "            if flash_version is not None:",
            "                args += ['-rtmp_flashver', flash_version]",
            "            if live:",
            "                args += ['-rtmp_live', 'live']",
            "            if isinstance(conn, list):",
            "                for entry in conn:",
            "                    args += ['-rtmp_conn', entry]",
            "            elif isinstance(conn, str):",
            "                args += ['-rtmp_conn', conn]",
            "",
            "        start_time, end_time = info_dict.get('section_start') or 0, info_dict.get('section_end')",
            "",
            "        selected_formats = info_dict.get('requested_formats') or [info_dict]",
            "        for i, fmt in enumerate(selected_formats):",
            "            if fmt.get('http_headers') and re.match(r'^https?://', fmt['url']):",
            "                # Trailing \\r\\n after each HTTP header is important to prevent warning from ffmpeg/avconv:",
            "                # [http @ 00000000003d2fa0] No trailing CRLF found in HTTP header.",
            "                args.extend(['-headers', ''.join(f'{key}: {val}\\r\\n' for key, val in fmt['http_headers'].items())])",
            "",
            "            if start_time:",
            "                args += ['-ss', str(start_time)]",
            "            if end_time:",
            "                args += ['-t', str(end_time - start_time)]",
            "",
            "            args += self._configuration_args((f'_i{i + 1}', '_i')) + ['-i', fmt['url']]",
            "",
            "        if not (start_time or end_time) or not self.params.get('force_keyframes_at_cuts'):",
            "            args += ['-c', 'copy']",
            "",
            "        if info_dict.get('requested_formats') or protocol == 'http_dash_segments':",
            "            for i, fmt in enumerate(selected_formats):",
            "                stream_number = fmt.get('manifest_stream_number', 0)",
            "                args.extend(['-map', f'{i}:{stream_number}'])",
            "",
            "        if self.params.get('test', False):",
            "            args += ['-fs', str(self._TEST_FILE_SIZE)]",
            "",
            "        ext = info_dict['ext']",
            "        if protocol in ('m3u8', 'm3u8_native'):",
            "            use_mpegts = (tmpfilename == '-') or self.params.get('hls_use_mpegts')",
            "            if use_mpegts is None:",
            "                use_mpegts = info_dict.get('is_live')",
            "            if use_mpegts:",
            "                args += ['-f', 'mpegts']",
            "            else:",
            "                args += ['-f', 'mp4']",
            "                if (ffpp.basename == 'ffmpeg' and ffpp._features.get('needs_adtstoasc')) and (not info_dict.get('acodec') or info_dict['acodec'].split('.')[0] in ('aac', 'mp4a')):",
            "                    args += ['-bsf:a', 'aac_adtstoasc']",
            "        elif protocol == 'rtmp':",
            "            args += ['-f', 'flv']",
            "        elif ext == 'mp4' and tmpfilename == '-':",
            "            args += ['-f', 'mpegts']",
            "        elif ext == 'unknown_video':",
            "            ext = determine_ext(remove_end(tmpfilename, '.part'))",
            "            if ext == 'unknown_video':",
            "                self.report_warning(",
            "                    'The video format is unknown and cannot be downloaded by ffmpeg. '",
            "                    'Explicitly set the extension in the filename to attempt download in that format')",
            "            else:",
            "                self.report_warning(f'The video format is unknown. Trying to download as {ext} according to the filename')",
            "                args += ['-f', EXT_TO_OUT_FORMATS.get(ext, ext)]",
            "        else:",
            "            args += ['-f', EXT_TO_OUT_FORMATS.get(ext, ext)]",
            "",
            "        args += self._configuration_args(('_o1', '_o', ''))",
            "",
            "        args = [encodeArgument(opt) for opt in args]",
            "        args.append(encodeFilename(ffpp._ffmpeg_filename_argument(tmpfilename), True))",
            "        self._debug_cmd(args)",
            "",
            "        piped = any(fmt['url'] in ('-', 'pipe:') for fmt in selected_formats)",
            "        with Popen(args, stdin=subprocess.PIPE, env=env) as proc:",
            "            if piped:",
            "                self.on_process_started(proc, proc.stdin)",
            "            try:",
            "                retval = proc.wait()",
            "            except BaseException as e:",
            "                # subprocces.run would send the SIGKILL signal to ffmpeg and the",
            "                # mp4 file couldn't be played, but if we ask ffmpeg to quit it",
            "                # produces a file that is playable (this is mostly useful for live",
            "                # streams). Note that Windows is not affected and produces playable",
            "                # files (see https://github.com/ytdl-org/youtube-dl/issues/8300).",
            "                if isinstance(e, KeyboardInterrupt) and sys.platform != 'win32' and not piped:",
            "                    proc.communicate_or_kill(b'q')",
            "                else:",
            "                    proc.kill(timeout=None)",
            "                raise",
            "            return retval",
            "",
            "",
            "class AVconvFD(FFmpegFD):",
            "    pass",
            "",
            "",
            "_BY_NAME = {",
            "    klass.get_basename(): klass",
            "    for name, klass in globals().items()",
            "    if name.endswith('FD') and name not in ('ExternalFD', 'FragmentFD')",
            "}",
            "",
            "",
            "def list_external_downloaders():",
            "    return sorted(_BY_NAME.keys())",
            "",
            "",
            "def get_external_downloader(external_downloader):",
            "    \"\"\" Given the name of the executable, see whether we support the given downloader \"\"\"",
            "    bn = os.path.splitext(os.path.basename(external_downloader))[0]",
            "    return _BY_NAME.get(bn) or next((",
            "        klass for klass in _BY_NAME.values() if klass.EXE_NAME in bn",
            "    ), None)"
        ],
        "afterPatchFile": [
            "import enum",
            "import json",
            "import os",
            "import re",
            "import subprocess",
            "import sys",
            "import tempfile",
            "import time",
            "import uuid",
            "",
            "from .fragment import FragmentFD",
            "from ..compat import functools",
            "from ..postprocessor.ffmpeg import EXT_TO_OUT_FORMATS, FFmpegPostProcessor",
            "from ..utils import (",
            "    Popen,",
            "    RetryManager,",
            "    _configuration_args,",
            "    check_executable,",
            "    classproperty,",
            "    cli_bool_option,",
            "    cli_option,",
            "    cli_valueless_option,",
            "    determine_ext,",
            "    encodeArgument,",
            "    encodeFilename,",
            "    find_available_port,",
            "    remove_end,",
            "    sanitized_Request,",
            "    traverse_obj,",
            ")",
            "",
            "",
            "class Features(enum.Enum):",
            "    TO_STDOUT = enum.auto()",
            "    MULTIPLE_FORMATS = enum.auto()",
            "",
            "",
            "class ExternalFD(FragmentFD):",
            "    SUPPORTED_PROTOCOLS = ('http', 'https', 'ftp', 'ftps')",
            "    SUPPORTED_FEATURES = ()",
            "    _CAPTURE_STDERR = True",
            "",
            "    def real_download(self, filename, info_dict):",
            "        self.report_destination(filename)",
            "        tmpfilename = self.temp_name(filename)",
            "        self._cookies_tempfile = None",
            "",
            "        try:",
            "            started = time.time()",
            "            retval = self._call_downloader(tmpfilename, info_dict)",
            "        except KeyboardInterrupt:",
            "            if not info_dict.get('is_live'):",
            "                raise",
            "            # Live stream downloading cancellation should be considered as",
            "            # correct and expected termination thus all postprocessing",
            "            # should take place",
            "            retval = 0",
            "            self.to_screen('[%s] Interrupted by user' % self.get_basename())",
            "        finally:",
            "            if self._cookies_tempfile:",
            "                self.try_remove(self._cookies_tempfile)",
            "",
            "        if retval == 0:",
            "            status = {",
            "                'filename': filename,",
            "                'status': 'finished',",
            "                'elapsed': time.time() - started,",
            "            }",
            "            if filename != '-':",
            "                fsize = os.path.getsize(encodeFilename(tmpfilename))",
            "                self.try_rename(tmpfilename, filename)",
            "                status.update({",
            "                    'downloaded_bytes': fsize,",
            "                    'total_bytes': fsize,",
            "                })",
            "            self._hook_progress(status, info_dict)",
            "            return True",
            "        else:",
            "            self.to_stderr('\\n')",
            "            self.report_error('%s exited with code %d' % (",
            "                self.get_basename(), retval))",
            "            return False",
            "",
            "    @classmethod",
            "    def get_basename(cls):",
            "        return cls.__name__[:-2].lower()",
            "",
            "    @classproperty",
            "    def EXE_NAME(cls):",
            "        return cls.get_basename()",
            "",
            "    @functools.cached_property",
            "    def exe(self):",
            "        return self.EXE_NAME",
            "",
            "    @classmethod",
            "    def available(cls, path=None):",
            "        path = check_executable(",
            "            cls.EXE_NAME if path in (None, cls.get_basename()) else path,",
            "            [cls.AVAILABLE_OPT])",
            "        if not path:",
            "            return False",
            "        cls.exe = path",
            "        return path",
            "",
            "    @classmethod",
            "    def supports(cls, info_dict):",
            "        return all((",
            "            not info_dict.get('to_stdout') or Features.TO_STDOUT in cls.SUPPORTED_FEATURES,",
            "            '+' not in info_dict['protocol'] or Features.MULTIPLE_FORMATS in cls.SUPPORTED_FEATURES,",
            "            not traverse_obj(info_dict, ('hls_aes', ...), 'extra_param_to_segment_url'),",
            "            all(proto in cls.SUPPORTED_PROTOCOLS for proto in info_dict['protocol'].split('+')),",
            "        ))",
            "",
            "    @classmethod",
            "    def can_download(cls, info_dict, path=None):",
            "        return cls.available(path) and cls.supports(info_dict)",
            "",
            "    def _option(self, command_option, param):",
            "        return cli_option(self.params, command_option, param)",
            "",
            "    def _bool_option(self, command_option, param, true_value='true', false_value='false', separator=None):",
            "        return cli_bool_option(self.params, command_option, param, true_value, false_value, separator)",
            "",
            "    def _valueless_option(self, command_option, param, expected_value=True):",
            "        return cli_valueless_option(self.params, command_option, param, expected_value)",
            "",
            "    def _configuration_args(self, keys=None, *args, **kwargs):",
            "        return _configuration_args(",
            "            self.get_basename(), self.params.get('external_downloader_args'), self.EXE_NAME,",
            "            keys, *args, **kwargs)",
            "",
            "    def _write_cookies(self):",
            "        if not self.ydl.cookiejar.filename:",
            "            tmp_cookies = tempfile.NamedTemporaryFile(suffix='.cookies', delete=False)",
            "            tmp_cookies.close()",
            "            self._cookies_tempfile = tmp_cookies.name",
            "            self.to_screen(f'[download] Writing temporary cookies file to \"{self._cookies_tempfile}\"')",
            "        # real_download resets _cookies_tempfile; if it's None then save() will write to cookiejar.filename",
            "        self.ydl.cookiejar.save(self._cookies_tempfile)",
            "        return self.ydl.cookiejar.filename or self._cookies_tempfile",
            "",
            "    def _call_downloader(self, tmpfilename, info_dict):",
            "        \"\"\" Either overwrite this or implement _make_cmd \"\"\"",
            "        cmd = [encodeArgument(a) for a in self._make_cmd(tmpfilename, info_dict)]",
            "",
            "        self._debug_cmd(cmd)",
            "",
            "        if 'fragments' not in info_dict:",
            "            _, stderr, returncode = self._call_process(cmd, info_dict)",
            "            if returncode and stderr:",
            "                self.to_stderr(stderr)",
            "            return returncode",
            "",
            "        skip_unavailable_fragments = self.params.get('skip_unavailable_fragments', True)",
            "",
            "        retry_manager = RetryManager(self.params.get('fragment_retries'), self.report_retry,",
            "                                     frag_index=None, fatal=not skip_unavailable_fragments)",
            "        for retry in retry_manager:",
            "            _, stderr, returncode = self._call_process(cmd, info_dict)",
            "            if not returncode:",
            "                break",
            "            # TODO: Decide whether to retry based on error code",
            "            # https://aria2.github.io/manual/en/html/aria2c.html#exit-status",
            "            if stderr:",
            "                self.to_stderr(stderr)",
            "            retry.error = Exception()",
            "            continue",
            "        if not skip_unavailable_fragments and retry_manager.error:",
            "            return -1",
            "",
            "        decrypt_fragment = self.decrypter(info_dict)",
            "        dest, _ = self.sanitize_open(tmpfilename, 'wb')",
            "        for frag_index, fragment in enumerate(info_dict['fragments']):",
            "            fragment_filename = '%s-Frag%d' % (tmpfilename, frag_index)",
            "            try:",
            "                src, _ = self.sanitize_open(fragment_filename, 'rb')",
            "            except OSError as err:",
            "                if skip_unavailable_fragments and frag_index > 1:",
            "                    self.report_skip_fragment(frag_index, err)",
            "                    continue",
            "                self.report_error(f'Unable to open fragment {frag_index}; {err}')",
            "                return -1",
            "            dest.write(decrypt_fragment(fragment, src.read()))",
            "            src.close()",
            "            if not self.params.get('keep_fragments', False):",
            "                self.try_remove(encodeFilename(fragment_filename))",
            "        dest.close()",
            "        self.try_remove(encodeFilename('%s.frag.urls' % tmpfilename))",
            "        return 0",
            "",
            "    def _call_process(self, cmd, info_dict):",
            "        return Popen.run(cmd, text=True, stderr=subprocess.PIPE if self._CAPTURE_STDERR else None)",
            "",
            "",
            "class CurlFD(ExternalFD):",
            "    AVAILABLE_OPT = '-V'",
            "    _CAPTURE_STDERR = False  # curl writes the progress to stderr",
            "",
            "    def _make_cmd(self, tmpfilename, info_dict):",
            "        cmd = [self.exe, '--location', '-o', tmpfilename, '--compressed']",
            "        if self.ydl.cookiejar.get_cookie_header(info_dict['url']):",
            "            cmd += ['--cookie-jar', self._write_cookies()]",
            "        if info_dict.get('http_headers') is not None:",
            "            for key, val in info_dict['http_headers'].items():",
            "                cmd += ['--header', f'{key}: {val}']",
            "",
            "        cmd += self._bool_option('--continue-at', 'continuedl', '-', '0')",
            "        cmd += self._valueless_option('--silent', 'noprogress')",
            "        cmd += self._valueless_option('--verbose', 'verbose')",
            "        cmd += self._option('--limit-rate', 'ratelimit')",
            "        retry = self._option('--retry', 'retries')",
            "        if len(retry) == 2:",
            "            if retry[1] in ('inf', 'infinite'):",
            "                retry[1] = '2147483647'",
            "            cmd += retry",
            "        cmd += self._option('--max-filesize', 'max_filesize')",
            "        cmd += self._option('--interface', 'source_address')",
            "        cmd += self._option('--proxy', 'proxy')",
            "        cmd += self._valueless_option('--insecure', 'nocheckcertificate')",
            "        cmd += self._configuration_args()",
            "        cmd += ['--', info_dict['url']]",
            "        return cmd",
            "",
            "",
            "class AxelFD(ExternalFD):",
            "    AVAILABLE_OPT = '-V'",
            "",
            "    def _make_cmd(self, tmpfilename, info_dict):",
            "        cmd = [self.exe, '-o', tmpfilename]",
            "        if info_dict.get('http_headers') is not None:",
            "            for key, val in info_dict['http_headers'].items():",
            "                cmd += ['-H', f'{key}: {val}']",
            "        cookie_header = self.ydl.cookiejar.get_cookie_header(info_dict['url'])",
            "        if cookie_header:",
            "            cmd += [f'Cookie: {cookie_header}', '--max-redirect=0']",
            "        cmd += self._configuration_args()",
            "        cmd += ['--', info_dict['url']]",
            "        return cmd",
            "",
            "",
            "class WgetFD(ExternalFD):",
            "    AVAILABLE_OPT = '--version'",
            "",
            "    def _make_cmd(self, tmpfilename, info_dict):",
            "        cmd = [self.exe, '-O', tmpfilename, '-nv', '--compression=auto']",
            "        if self.ydl.cookiejar.get_cookie_header(info_dict['url']):",
            "            cmd += ['--load-cookies', self._write_cookies()]",
            "        if info_dict.get('http_headers') is not None:",
            "            for key, val in info_dict['http_headers'].items():",
            "                cmd += ['--header', f'{key}: {val}']",
            "        cmd += self._option('--limit-rate', 'ratelimit')",
            "        retry = self._option('--tries', 'retries')",
            "        if len(retry) == 2:",
            "            if retry[1] in ('inf', 'infinite'):",
            "                retry[1] = '0'",
            "            cmd += retry",
            "        cmd += self._option('--bind-address', 'source_address')",
            "        proxy = self.params.get('proxy')",
            "        if proxy:",
            "            for var in ('http_proxy', 'https_proxy'):",
            "                cmd += ['--execute', f'{var}={proxy}']",
            "        cmd += self._valueless_option('--no-check-certificate', 'nocheckcertificate')",
            "        cmd += self._configuration_args()",
            "        cmd += ['--', info_dict['url']]",
            "        return cmd",
            "",
            "",
            "class Aria2cFD(ExternalFD):",
            "    AVAILABLE_OPT = '-v'",
            "    SUPPORTED_PROTOCOLS = ('http', 'https', 'ftp', 'ftps', 'dash_frag_urls', 'm3u8_frag_urls')",
            "",
            "    @staticmethod",
            "    def supports_manifest(manifest):",
            "        UNSUPPORTED_FEATURES = [",
            "            r'#EXT-X-BYTERANGE',  # playlists composed of byte ranges of media files [1]",
            "            # 1. https://tools.ietf.org/html/draft-pantos-http-live-streaming-17#section-4.3.2.2",
            "        ]",
            "        check_results = (not re.search(feature, manifest) for feature in UNSUPPORTED_FEATURES)",
            "        return all(check_results)",
            "",
            "    @staticmethod",
            "    def _aria2c_filename(fn):",
            "        return fn if os.path.isabs(fn) else f'.{os.path.sep}{fn}'",
            "",
            "    def _call_downloader(self, tmpfilename, info_dict):",
            "        # FIXME: Disabled due to https://github.com/yt-dlp/yt-dlp/issues/5931",
            "        if False and 'no-external-downloader-progress' not in self.params.get('compat_opts', []):",
            "            info_dict['__rpc'] = {",
            "                'port': find_available_port() or 19190,",
            "                'secret': str(uuid.uuid4()),",
            "            }",
            "        return super()._call_downloader(tmpfilename, info_dict)",
            "",
            "    def _make_cmd(self, tmpfilename, info_dict):",
            "        cmd = [self.exe, '-c', '--no-conf',",
            "               '--console-log-level=warn', '--summary-interval=0', '--download-result=hide',",
            "               '--http-accept-gzip=true', '--file-allocation=none', '-x16', '-j16', '-s16']",
            "        if 'fragments' in info_dict:",
            "            cmd += ['--allow-overwrite=true', '--allow-piece-length-change=true']",
            "        else:",
            "            cmd += ['--min-split-size', '1M']",
            "",
            "        if self.ydl.cookiejar.get_cookie_header(info_dict['url']):",
            "            cmd += [f'--load-cookies={self._write_cookies()}']",
            "        if info_dict.get('http_headers') is not None:",
            "            for key, val in info_dict['http_headers'].items():",
            "                cmd += ['--header', f'{key}: {val}']",
            "        cmd += self._option('--max-overall-download-limit', 'ratelimit')",
            "        cmd += self._option('--interface', 'source_address')",
            "        cmd += self._option('--all-proxy', 'proxy')",
            "        cmd += self._bool_option('--check-certificate', 'nocheckcertificate', 'false', 'true', '=')",
            "        cmd += self._bool_option('--remote-time', 'updatetime', 'true', 'false', '=')",
            "        cmd += self._bool_option('--show-console-readout', 'noprogress', 'false', 'true', '=')",
            "        cmd += self._configuration_args()",
            "",
            "        if '__rpc' in info_dict:",
            "            cmd += [",
            "                '--enable-rpc',",
            "                f'--rpc-listen-port={info_dict[\"__rpc\"][\"port\"]}',",
            "                f'--rpc-secret={info_dict[\"__rpc\"][\"secret\"]}']",
            "",
            "        # aria2c strips out spaces from the beginning/end of filenames and paths.",
            "        # We work around this issue by adding a \"./\" to the beginning of the",
            "        # filename and relative path, and adding a \"/\" at the end of the path.",
            "        # See: https://github.com/yt-dlp/yt-dlp/issues/276",
            "        # https://github.com/ytdl-org/youtube-dl/issues/20312",
            "        # https://github.com/aria2/aria2/issues/1373",
            "        dn = os.path.dirname(tmpfilename)",
            "        if dn:",
            "            cmd += ['--dir', self._aria2c_filename(dn) + os.path.sep]",
            "        if 'fragments' not in info_dict:",
            "            cmd += ['--out', self._aria2c_filename(os.path.basename(tmpfilename))]",
            "        cmd += ['--auto-file-renaming=false']",
            "",
            "        if 'fragments' in info_dict:",
            "            cmd += ['--file-allocation=none', '--uri-selector=inorder']",
            "            url_list_file = '%s.frag.urls' % tmpfilename",
            "            url_list = []",
            "            for frag_index, fragment in enumerate(info_dict['fragments']):",
            "                fragment_filename = '%s-Frag%d' % (os.path.basename(tmpfilename), frag_index)",
            "                url_list.append('%s\\n\\tout=%s' % (fragment['url'], self._aria2c_filename(fragment_filename)))",
            "            stream, _ = self.sanitize_open(url_list_file, 'wb')",
            "            stream.write('\\n'.join(url_list).encode())",
            "            stream.close()",
            "            cmd += ['-i', self._aria2c_filename(url_list_file)]",
            "        else:",
            "            cmd += ['--', info_dict['url']]",
            "        return cmd",
            "",
            "    def aria2c_rpc(self, rpc_port, rpc_secret, method, params=()):",
            "        # Does not actually need to be UUID, just unique",
            "        sanitycheck = str(uuid.uuid4())",
            "        d = json.dumps({",
            "            'jsonrpc': '2.0',",
            "            'id': sanitycheck,",
            "            'method': method,",
            "            'params': [f'token:{rpc_secret}', *params],",
            "        }).encode('utf-8')",
            "        request = sanitized_Request(",
            "            f'http://localhost:{rpc_port}/jsonrpc',",
            "            data=d, headers={",
            "                'Content-Type': 'application/json',",
            "                'Content-Length': f'{len(d)}',",
            "                'Ytdl-request-proxy': '__noproxy__',",
            "            })",
            "        with self.ydl.urlopen(request) as r:",
            "            resp = json.load(r)",
            "        assert resp.get('id') == sanitycheck, 'Something went wrong with RPC server'",
            "        return resp['result']",
            "",
            "    def _call_process(self, cmd, info_dict):",
            "        if '__rpc' not in info_dict:",
            "            return super()._call_process(cmd, info_dict)",
            "",
            "        send_rpc = functools.partial(self.aria2c_rpc, info_dict['__rpc']['port'], info_dict['__rpc']['secret'])",
            "        started = time.time()",
            "",
            "        fragmented = 'fragments' in info_dict",
            "        frag_count = len(info_dict['fragments']) if fragmented else 1",
            "        status = {",
            "            'filename': info_dict.get('_filename'),",
            "            'status': 'downloading',",
            "            'elapsed': 0,",
            "            'downloaded_bytes': 0,",
            "            'fragment_count': frag_count if fragmented else None,",
            "            'fragment_index': 0 if fragmented else None,",
            "        }",
            "        self._hook_progress(status, info_dict)",
            "",
            "        def get_stat(key, *obj, average=False):",
            "            val = tuple(filter(None, map(float, traverse_obj(obj, (..., ..., key))))) or [0]",
            "            return sum(val) / (len(val) if average else 1)",
            "",
            "        with Popen(cmd, text=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE) as p:",
            "            # Add a small sleep so that RPC client can receive response,",
            "            # or the connection stalls infinitely",
            "            time.sleep(0.2)",
            "            retval = p.poll()",
            "            while retval is None:",
            "                # We don't use tellStatus as we won't know the GID without reading stdout",
            "                # Ref: https://aria2.github.io/manual/en/html/aria2c.html#aria2.tellActive",
            "                active = send_rpc('aria2.tellActive')",
            "                completed = send_rpc('aria2.tellStopped', [0, frag_count])",
            "",
            "                downloaded = get_stat('totalLength', completed) + get_stat('completedLength', active)",
            "                speed = get_stat('downloadSpeed', active)",
            "                total = frag_count * get_stat('totalLength', active, completed, average=True)",
            "                if total < downloaded:",
            "                    total = None",
            "",
            "                status.update({",
            "                    'downloaded_bytes': int(downloaded),",
            "                    'speed': speed,",
            "                    'total_bytes': None if fragmented else total,",
            "                    'total_bytes_estimate': total,",
            "                    'eta': (total - downloaded) / (speed or 1),",
            "                    'fragment_index': min(frag_count, len(completed) + 1) if fragmented else None,",
            "                    'elapsed': time.time() - started",
            "                })",
            "                self._hook_progress(status, info_dict)",
            "",
            "                if not active and len(completed) >= frag_count:",
            "                    send_rpc('aria2.shutdown')",
            "                    retval = p.wait()",
            "                    break",
            "",
            "                time.sleep(0.1)",
            "                retval = p.poll()",
            "",
            "            return '', p.stderr.read(), retval",
            "",
            "",
            "class HttpieFD(ExternalFD):",
            "    AVAILABLE_OPT = '--version'",
            "    EXE_NAME = 'http'",
            "",
            "    def _make_cmd(self, tmpfilename, info_dict):",
            "        cmd = ['http', '--download', '--output', tmpfilename, info_dict['url']]",
            "",
            "        if info_dict.get('http_headers') is not None:",
            "            for key, val in info_dict['http_headers'].items():",
            "                cmd += [f'{key}:{val}']",
            "",
            "        # httpie 3.1.0+ removes the Cookie header on redirect, so this should be safe for now. [1]",
            "        # If we ever need cookie handling for redirects, we can export the cookiejar into a session. [2]",
            "        # 1: https://github.com/httpie/httpie/security/advisories/GHSA-9w4w-cpc8-h2fq",
            "        # 2: https://httpie.io/docs/cli/sessions",
            "        cookie_header = self.ydl.cookiejar.get_cookie_header(info_dict['url'])",
            "        if cookie_header:",
            "            cmd += [f'Cookie:{cookie_header}']",
            "        return cmd",
            "",
            "",
            "class FFmpegFD(ExternalFD):",
            "    SUPPORTED_PROTOCOLS = ('http', 'https', 'ftp', 'ftps', 'm3u8', 'm3u8_native', 'rtsp', 'rtmp', 'rtmp_ffmpeg', 'mms', 'http_dash_segments')",
            "    SUPPORTED_FEATURES = (Features.TO_STDOUT, Features.MULTIPLE_FORMATS)",
            "",
            "    @classmethod",
            "    def available(cls, path=None):",
            "        # TODO: Fix path for ffmpeg",
            "        # Fixme: This may be wrong when --ffmpeg-location is used",
            "        return FFmpegPostProcessor().available",
            "",
            "    def on_process_started(self, proc, stdin):",
            "        \"\"\" Override this in subclasses  \"\"\"",
            "        pass",
            "",
            "    @classmethod",
            "    def can_merge_formats(cls, info_dict, params):",
            "        return (",
            "            info_dict.get('requested_formats')",
            "            and info_dict.get('protocol')",
            "            and not params.get('allow_unplayable_formats')",
            "            and 'no-direct-merge' not in params.get('compat_opts', [])",
            "            and cls.can_download(info_dict))",
            "",
            "    def _call_downloader(self, tmpfilename, info_dict):",
            "        ffpp = FFmpegPostProcessor(downloader=self)",
            "        if not ffpp.available:",
            "            self.report_error('m3u8 download detected but ffmpeg could not be found. Please install')",
            "            return False",
            "        ffpp.check_version()",
            "",
            "        args = [ffpp.executable, '-y']",
            "",
            "        for log_level in ('quiet', 'verbose'):",
            "            if self.params.get(log_level, False):",
            "                args += ['-loglevel', log_level]",
            "                break",
            "        if not self.params.get('verbose'):",
            "            args += ['-hide_banner']",
            "",
            "        args += traverse_obj(info_dict, ('downloader_options', 'ffmpeg_args'), default=[])",
            "",
            "        # These exists only for compatibility. Extractors should use",
            "        # info_dict['downloader_options']['ffmpeg_args'] instead",
            "        args += info_dict.get('_ffmpeg_args') or []",
            "        seekable = info_dict.get('_seekable')",
            "        if seekable is not None:",
            "            # setting -seekable prevents ffmpeg from guessing if the server",
            "            # supports seeking(by adding the header `Range: bytes=0-`), which",
            "            # can cause problems in some cases",
            "            # https://github.com/ytdl-org/youtube-dl/issues/11800#issuecomment-275037127",
            "            # http://trac.ffmpeg.org/ticket/6125#comment:10",
            "            args += ['-seekable', '1' if seekable else '0']",
            "",
            "        env = None",
            "        proxy = self.params.get('proxy')",
            "        if proxy:",
            "            if not re.match(r'^[\\da-zA-Z]+://', proxy):",
            "                proxy = 'http://%s' % proxy",
            "",
            "            if proxy.startswith('socks'):",
            "                self.report_warning(",
            "                    '%s does not support SOCKS proxies. Downloading is likely to fail. '",
            "                    'Consider adding --hls-prefer-native to your command.' % self.get_basename())",
            "",
            "            # Since December 2015 ffmpeg supports -http_proxy option (see",
            "            # http://git.videolan.org/?p=ffmpeg.git;a=commit;h=b4eb1f29ebddd60c41a2eb39f5af701e38e0d3fd)",
            "            # We could switch to the following code if we are able to detect version properly",
            "            # args += ['-http_proxy', proxy]",
            "            env = os.environ.copy()",
            "            env['HTTP_PROXY'] = proxy",
            "            env['http_proxy'] = proxy",
            "",
            "        protocol = info_dict.get('protocol')",
            "",
            "        if protocol == 'rtmp':",
            "            player_url = info_dict.get('player_url')",
            "            page_url = info_dict.get('page_url')",
            "            app = info_dict.get('app')",
            "            play_path = info_dict.get('play_path')",
            "            tc_url = info_dict.get('tc_url')",
            "            flash_version = info_dict.get('flash_version')",
            "            live = info_dict.get('rtmp_live', False)",
            "            conn = info_dict.get('rtmp_conn')",
            "            if player_url is not None:",
            "                args += ['-rtmp_swfverify', player_url]",
            "            if page_url is not None:",
            "                args += ['-rtmp_pageurl', page_url]",
            "            if app is not None:",
            "                args += ['-rtmp_app', app]",
            "            if play_path is not None:",
            "                args += ['-rtmp_playpath', play_path]",
            "            if tc_url is not None:",
            "                args += ['-rtmp_tcurl', tc_url]",
            "            if flash_version is not None:",
            "                args += ['-rtmp_flashver', flash_version]",
            "            if live:",
            "                args += ['-rtmp_live', 'live']",
            "            if isinstance(conn, list):",
            "                for entry in conn:",
            "                    args += ['-rtmp_conn', entry]",
            "            elif isinstance(conn, str):",
            "                args += ['-rtmp_conn', conn]",
            "",
            "        start_time, end_time = info_dict.get('section_start') or 0, info_dict.get('section_end')",
            "",
            "        selected_formats = info_dict.get('requested_formats') or [info_dict]",
            "        for i, fmt in enumerate(selected_formats):",
            "            cookies = self.ydl.cookiejar.get_cookies_for_url(fmt['url'])",
            "            if cookies:",
            "                args.extend(['-cookies', ''.join(",
            "                    f'{cookie.name}={cookie.value}; path={cookie.path}; domain={cookie.domain};\\r\\n'",
            "                    for cookie in cookies)])",
            "            if fmt.get('http_headers') and re.match(r'^https?://', fmt['url']):",
            "                # Trailing \\r\\n after each HTTP header is important to prevent warning from ffmpeg/avconv:",
            "                # [http @ 00000000003d2fa0] No trailing CRLF found in HTTP header.",
            "                args.extend(['-headers', ''.join(f'{key}: {val}\\r\\n' for key, val in fmt['http_headers'].items())])",
            "",
            "            if start_time:",
            "                args += ['-ss', str(start_time)]",
            "            if end_time:",
            "                args += ['-t', str(end_time - start_time)]",
            "",
            "            args += self._configuration_args((f'_i{i + 1}', '_i')) + ['-i', fmt['url']]",
            "",
            "        if not (start_time or end_time) or not self.params.get('force_keyframes_at_cuts'):",
            "            args += ['-c', 'copy']",
            "",
            "        if info_dict.get('requested_formats') or protocol == 'http_dash_segments':",
            "            for i, fmt in enumerate(selected_formats):",
            "                stream_number = fmt.get('manifest_stream_number', 0)",
            "                args.extend(['-map', f'{i}:{stream_number}'])",
            "",
            "        if self.params.get('test', False):",
            "            args += ['-fs', str(self._TEST_FILE_SIZE)]",
            "",
            "        ext = info_dict['ext']",
            "        if protocol in ('m3u8', 'm3u8_native'):",
            "            use_mpegts = (tmpfilename == '-') or self.params.get('hls_use_mpegts')",
            "            if use_mpegts is None:",
            "                use_mpegts = info_dict.get('is_live')",
            "            if use_mpegts:",
            "                args += ['-f', 'mpegts']",
            "            else:",
            "                args += ['-f', 'mp4']",
            "                if (ffpp.basename == 'ffmpeg' and ffpp._features.get('needs_adtstoasc')) and (not info_dict.get('acodec') or info_dict['acodec'].split('.')[0] in ('aac', 'mp4a')):",
            "                    args += ['-bsf:a', 'aac_adtstoasc']",
            "        elif protocol == 'rtmp':",
            "            args += ['-f', 'flv']",
            "        elif ext == 'mp4' and tmpfilename == '-':",
            "            args += ['-f', 'mpegts']",
            "        elif ext == 'unknown_video':",
            "            ext = determine_ext(remove_end(tmpfilename, '.part'))",
            "            if ext == 'unknown_video':",
            "                self.report_warning(",
            "                    'The video format is unknown and cannot be downloaded by ffmpeg. '",
            "                    'Explicitly set the extension in the filename to attempt download in that format')",
            "            else:",
            "                self.report_warning(f'The video format is unknown. Trying to download as {ext} according to the filename')",
            "                args += ['-f', EXT_TO_OUT_FORMATS.get(ext, ext)]",
            "        else:",
            "            args += ['-f', EXT_TO_OUT_FORMATS.get(ext, ext)]",
            "",
            "        args += self._configuration_args(('_o1', '_o', ''))",
            "",
            "        args = [encodeArgument(opt) for opt in args]",
            "        args.append(encodeFilename(ffpp._ffmpeg_filename_argument(tmpfilename), True))",
            "        self._debug_cmd(args)",
            "",
            "        piped = any(fmt['url'] in ('-', 'pipe:') for fmt in selected_formats)",
            "        with Popen(args, stdin=subprocess.PIPE, env=env) as proc:",
            "            if piped:",
            "                self.on_process_started(proc, proc.stdin)",
            "            try:",
            "                retval = proc.wait()",
            "            except BaseException as e:",
            "                # subprocces.run would send the SIGKILL signal to ffmpeg and the",
            "                # mp4 file couldn't be played, but if we ask ffmpeg to quit it",
            "                # produces a file that is playable (this is mostly useful for live",
            "                # streams). Note that Windows is not affected and produces playable",
            "                # files (see https://github.com/ytdl-org/youtube-dl/issues/8300).",
            "                if isinstance(e, KeyboardInterrupt) and sys.platform != 'win32' and not piped:",
            "                    proc.communicate_or_kill(b'q')",
            "                else:",
            "                    proc.kill(timeout=None)",
            "                raise",
            "            return retval",
            "",
            "",
            "class AVconvFD(FFmpegFD):",
            "    pass",
            "",
            "",
            "_BY_NAME = {",
            "    klass.get_basename(): klass",
            "    for name, klass in globals().items()",
            "    if name.endswith('FD') and name not in ('ExternalFD', 'FragmentFD')",
            "}",
            "",
            "",
            "def list_external_downloaders():",
            "    return sorted(_BY_NAME.keys())",
            "",
            "",
            "def get_external_downloader(external_downloader):",
            "    \"\"\" Given the name of the executable, see whether we support the given downloader \"\"\"",
            "    bn = os.path.splitext(os.path.basename(external_downloader))[0]",
            "    return _BY_NAME.get(bn) or next((",
            "        klass for klass in _BY_NAME.values() if klass.EXE_NAME in bn",
            "    ), None)"
        ],
        "action": [
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "3": [],
            "226": [
                "WgetFD",
                "_make_cmd"
            ]
        },
        "addLocation": []
    }
}