{
    "gradio/flagging.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " from typing import Any, List, Optional"
            },
            "1": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " import gradio as gr"
            },
            "3": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from gradio import encryptor"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 12,
                "PatchRowcode": "+from gradio import encryptor, utils"
            },
            "5": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " class FlaggingCallback(ABC):"
            },
            "8": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 99,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": 100,
                "PatchRowcode": "         with open(log_filepath, \"a\", newline=\"\") as csvfile:"
            },
            "10": {
                "beforePatchRowNumber": 101,
                "afterPatchRowNumber": 101,
                "PatchRowcode": "             writer = csv.writer(csvfile)"
            },
            "11": {
                "beforePatchRowNumber": 102,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            writer.writerow(csv_data)"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+            writer.writerow(utils.santize_for_csv(csv_data))"
            },
            "13": {
                "beforePatchRowNumber": 103,
                "afterPatchRowNumber": 103,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 104,
                "afterPatchRowNumber": 104,
                "PatchRowcode": "         with open(log_filepath, \"r\") as csvfile:"
            },
            "15": {
                "beforePatchRowNumber": 105,
                "afterPatchRowNumber": 105,
                "PatchRowcode": "             line_count = len([None for row in csv.reader(csvfile)]) - 1"
            },
            "16": {
                "beforePatchRowNumber": 186,
                "afterPatchRowNumber": 186,
                "PatchRowcode": "             content[flag_index][flag_col_index] = flag_option"
            },
            "17": {
                "beforePatchRowNumber": 187,
                "afterPatchRowNumber": 187,
                "PatchRowcode": "             output = io.StringIO()"
            },
            "18": {
                "beforePatchRowNumber": 188,
                "afterPatchRowNumber": 188,
                "PatchRowcode": "             writer = csv.writer(output)"
            },
            "19": {
                "beforePatchRowNumber": 189,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            writer.writerows(content)"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 189,
                "PatchRowcode": "+            writer.writerows(utils.santize_for_csv(content))"
            },
            "21": {
                "beforePatchRowNumber": 190,
                "afterPatchRowNumber": 190,
                "PatchRowcode": "             return output.getvalue()"
            },
            "22": {
                "beforePatchRowNumber": 191,
                "afterPatchRowNumber": 191,
                "PatchRowcode": " "
            },
            "23": {
                "beforePatchRowNumber": 192,
                "afterPatchRowNumber": 192,
                "PatchRowcode": "         if interface.encrypt:"
            },
            "24": {
                "beforePatchRowNumber": 200,
                "afterPatchRowNumber": 200,
                "PatchRowcode": "                     file_content = decrypted_csv.decode()"
            },
            "25": {
                "beforePatchRowNumber": 201,
                "afterPatchRowNumber": 201,
                "PatchRowcode": "                     if flag_index is not None:"
            },
            "26": {
                "beforePatchRowNumber": 202,
                "afterPatchRowNumber": 202,
                "PatchRowcode": "                         file_content = replace_flag_at_index(file_content)"
            },
            "27": {
                "beforePatchRowNumber": 203,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    output.write(file_content)"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 203,
                "PatchRowcode": "+                    output.write(utils.santize_for_csv(file_content))"
            },
            "29": {
                "beforePatchRowNumber": 204,
                "afterPatchRowNumber": 204,
                "PatchRowcode": "             writer = csv.writer(output)"
            },
            "30": {
                "beforePatchRowNumber": 205,
                "afterPatchRowNumber": 205,
                "PatchRowcode": "             if flag_index is None:"
            },
            "31": {
                "beforePatchRowNumber": 206,
                "afterPatchRowNumber": 206,
                "PatchRowcode": "                 if is_new:"
            },
            "32": {
                "beforePatchRowNumber": 207,
                "afterPatchRowNumber": 207,
                "PatchRowcode": "                     writer.writerow(headers)"
            },
            "33": {
                "beforePatchRowNumber": 208,
                "afterPatchRowNumber": 208,
                "PatchRowcode": "                 writer.writerow(csv_data)"
            },
            "34": {
                "beforePatchRowNumber": 209,
                "afterPatchRowNumber": 209,
                "PatchRowcode": "             with open(log_fp, \"wb\") as csvfile:"
            },
            "35": {
                "beforePatchRowNumber": 210,
                "afterPatchRowNumber": 210,
                "PatchRowcode": "                 csvfile.write("
            },
            "36": {
                "beforePatchRowNumber": 211,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    encryptor.encrypt("
            },
            "37": {
                "beforePatchRowNumber": 212,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        interface.encryption_key, output.getvalue().encode()"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 211,
                "PatchRowcode": "+                    utils.santize_for_csv("
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 212,
                "PatchRowcode": "+                        encryptor.encrypt("
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 213,
                "PatchRowcode": "+                            interface.encryption_key, output.getvalue().encode()"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 214,
                "PatchRowcode": "+                        )"
            },
            "42": {
                "beforePatchRowNumber": 213,
                "afterPatchRowNumber": 215,
                "PatchRowcode": "                     )"
            },
            "43": {
                "beforePatchRowNumber": 214,
                "afterPatchRowNumber": 216,
                "PatchRowcode": "                 )"
            },
            "44": {
                "beforePatchRowNumber": 215,
                "afterPatchRowNumber": 217,
                "PatchRowcode": "         else:"
            },
            "45": {
                "beforePatchRowNumber": 216,
                "afterPatchRowNumber": 218,
                "PatchRowcode": "             if flag_index is None:"
            },
            "46": {
                "beforePatchRowNumber": 217,
                "afterPatchRowNumber": 219,
                "PatchRowcode": "                 with open(log_fp, \"a\", newline=\"\") as csvfile:"
            },
            "47": {
                "beforePatchRowNumber": 218,
                "afterPatchRowNumber": 220,
                "PatchRowcode": "                     writer = csv.writer(csvfile)"
            },
            "48": {
                "beforePatchRowNumber": 219,
                "afterPatchRowNumber": 221,
                "PatchRowcode": "                     if is_new:"
            },
            "49": {
                "beforePatchRowNumber": 220,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        writer.writerow(headers)"
            },
            "50": {
                "beforePatchRowNumber": 221,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    writer.writerow(csv_data)"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 222,
                "PatchRowcode": "+                        writer.writerow(utils.santize_for_csv(headers))"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 223,
                "PatchRowcode": "+                    writer.writerow(utils.santize_for_csv(csv_data))"
            },
            "53": {
                "beforePatchRowNumber": 222,
                "afterPatchRowNumber": 224,
                "PatchRowcode": "             else:"
            },
            "54": {
                "beforePatchRowNumber": 223,
                "afterPatchRowNumber": 225,
                "PatchRowcode": "                 with open(log_fp) as csvfile:"
            },
            "55": {
                "beforePatchRowNumber": 224,
                "afterPatchRowNumber": 226,
                "PatchRowcode": "                     file_content = csvfile.read()"
            },
            "56": {
                "beforePatchRowNumber": 225,
                "afterPatchRowNumber": 227,
                "PatchRowcode": "                     file_content = replace_flag_at_index(file_content)"
            },
            "57": {
                "beforePatchRowNumber": 226,
                "afterPatchRowNumber": 228,
                "PatchRowcode": "                 with open("
            },
            "58": {
                "beforePatchRowNumber": 227,
                "afterPatchRowNumber": 229,
                "PatchRowcode": "                     log_fp, \"w\", newline=\"\""
            },
            "59": {
                "beforePatchRowNumber": 228,
                "afterPatchRowNumber": 230,
                "PatchRowcode": "                 ) as csvfile:  # newline parameter needed for Windows"
            },
            "60": {
                "beforePatchRowNumber": 229,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    csvfile.write(file_content)"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 231,
                "PatchRowcode": "+                    csvfile.write(utils.santize_for_csv(file_content))"
            },
            "62": {
                "beforePatchRowNumber": 230,
                "afterPatchRowNumber": 232,
                "PatchRowcode": "         with open(log_fp, \"r\") as csvfile:"
            },
            "63": {
                "beforePatchRowNumber": 231,
                "afterPatchRowNumber": 233,
                "PatchRowcode": "             line_count = len([None for row in csv.reader(csvfile)]) - 1"
            },
            "64": {
                "beforePatchRowNumber": 232,
                "afterPatchRowNumber": 234,
                "PatchRowcode": "         return line_count"
            },
            "65": {
                "beforePatchRowNumber": 368,
                "afterPatchRowNumber": 370,
                "PatchRowcode": "                         \"_type\": \"Value\","
            },
            "66": {
                "beforePatchRowNumber": 369,
                "afterPatchRowNumber": 371,
                "PatchRowcode": "                     }"
            },
            "67": {
                "beforePatchRowNumber": 370,
                "afterPatchRowNumber": 372,
                "PatchRowcode": " "
            },
            "68": {
                "beforePatchRowNumber": 371,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                writer.writerow(headers)"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 373,
                "PatchRowcode": "+                writer.writerow(utils.santize_for_csv(headers))"
            },
            "70": {
                "beforePatchRowNumber": 372,
                "afterPatchRowNumber": 374,
                "PatchRowcode": " "
            },
            "71": {
                "beforePatchRowNumber": 373,
                "afterPatchRowNumber": 375,
                "PatchRowcode": "             # Generate the row corresponding to the flagged sample"
            },
            "72": {
                "beforePatchRowNumber": 374,
                "afterPatchRowNumber": 376,
                "PatchRowcode": "             csv_data = []"
            },
            "73": {
                "beforePatchRowNumber": 403,
                "afterPatchRowNumber": 405,
                "PatchRowcode": "             if flag_option is not None:"
            },
            "74": {
                "beforePatchRowNumber": 404,
                "afterPatchRowNumber": 406,
                "PatchRowcode": "                 csv_data.append(flag_option)"
            },
            "75": {
                "beforePatchRowNumber": 405,
                "afterPatchRowNumber": 407,
                "PatchRowcode": " "
            },
            "76": {
                "beforePatchRowNumber": 406,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            writer.writerow(csv_data)"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 408,
                "PatchRowcode": "+            writer.writerow(utils.santize_for_csv(csv_data))"
            },
            "78": {
                "beforePatchRowNumber": 407,
                "afterPatchRowNumber": 409,
                "PatchRowcode": " "
            },
            "79": {
                "beforePatchRowNumber": 408,
                "afterPatchRowNumber": 410,
                "PatchRowcode": "         if is_new:"
            },
            "80": {
                "beforePatchRowNumber": 409,
                "afterPatchRowNumber": 411,
                "PatchRowcode": "             json.dump(infos, open(self.infos_file, \"w\"))"
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import csv",
            "import datetime",
            "import io",
            "import json",
            "import os",
            "from abc import ABC, abstractmethod",
            "from typing import Any, List, Optional",
            "",
            "import gradio as gr",
            "from gradio import encryptor",
            "",
            "",
            "class FlaggingCallback(ABC):",
            "    \"\"\"",
            "    An abstract class for defining the methods that any FlaggingCallback should have.",
            "    \"\"\"",
            "",
            "    @abstractmethod",
            "    def setup(self, flagging_dir: str):",
            "        \"\"\"",
            "        This method should be overridden and ensure that everything is set up correctly for flag().",
            "        This method gets called once at the beginning of the Interface.launch() method.",
            "        Parameters:",
            "        flagging_dir: A string, typically containing the path to the directory where the flagging file should be storied (provided as an argument to Interface.__init__()).",
            "        \"\"\"",
            "        pass",
            "",
            "    @abstractmethod",
            "    def flag(",
            "        self,",
            "        interface: gr.Interface,",
            "        input_data: List[Any],",
            "        output_data: List[Any],",
            "        flag_option: Optional[str] = None,",
            "        flag_index: Optional[int] = None,",
            "        username: Optional[str] = None,",
            "    ) -> int:",
            "        \"\"\"",
            "        This method should be overridden by the FlaggingCallback subclass and may contain optional additional arguments.",
            "        This gets called every time the <flag> button is pressed.",
            "        Parameters:",
            "        interface: The Interface object that is being used to launch the flagging interface.",
            "        input_data: The input data to be flagged.",
            "        output_data: The output data to be flagged.",
            "        flag_option (optional): In the case that flagging_options are provided, the flag option that is being used.",
            "        flag_index (optional): The index of the sample that is being flagged.",
            "        username (optional): The username of the user that is flagging the data, if logged in.",
            "        Returns:",
            "        (int) The total number of samples that have been flagged.",
            "        \"\"\"",
            "        pass",
            "",
            "",
            "class SimpleCSVLogger(FlaggingCallback):",
            "    \"\"\"",
            "    A simple example implementation of the FlaggingCallback abstract class",
            "    provided for illustrative purposes.",
            "    \"\"\"",
            "",
            "    def setup(self, flagging_dir: str):",
            "        self.flagging_dir = flagging_dir",
            "        os.makedirs(flagging_dir, exist_ok=True)",
            "",
            "    def flag(",
            "        self,",
            "        interface: gr.Interface,",
            "        input_data: List[Any],",
            "        output_data: List[Any],",
            "        flag_option: Optional[str] = None,",
            "        flag_index: Optional[int] = None,",
            "        username: Optional[str] = None,",
            "    ) -> int:",
            "        flagging_dir = self.flagging_dir",
            "        log_filepath = \"{}/log.csv\".format(flagging_dir)",
            "",
            "        csv_data = []",
            "        for i, input in enumerate(interface.input_components):",
            "            csv_data.append(",
            "                input.save_flagged(",
            "                    flagging_dir,",
            "                    interface.config[\"input_components\"][i][\"label\"],",
            "                    input_data[i],",
            "                    None,",
            "                )",
            "            )",
            "        for i, output in enumerate(interface.output_components):",
            "            csv_data.append(",
            "                output.save_flagged(",
            "                    flagging_dir,",
            "                    interface.config[\"output_components\"][i][\"label\"],",
            "                    output_data[i],",
            "                    None,",
            "                )",
            "                if output_data[i] is not None",
            "                else \"\"",
            "            )",
            "",
            "        with open(log_filepath, \"a\", newline=\"\") as csvfile:",
            "            writer = csv.writer(csvfile)",
            "            writer.writerow(csv_data)",
            "",
            "        with open(log_filepath, \"r\") as csvfile:",
            "            line_count = len([None for row in csv.reader(csvfile)]) - 1",
            "        return line_count",
            "",
            "",
            "class CSVLogger(FlaggingCallback):",
            "    \"\"\"",
            "    The default implementation of the FlaggingCallback abstract class.",
            "    Logs the input and output data to a CSV file. Supports encryption.",
            "    \"\"\"",
            "",
            "    def setup(self, flagging_dir: str):",
            "        self.flagging_dir = flagging_dir",
            "        os.makedirs(flagging_dir, exist_ok=True)",
            "",
            "    def flag(",
            "        self,",
            "        interface: gr.Interface,",
            "        input_data: List[Any],",
            "        output_data: List[Any],",
            "        flag_option: Optional[str] = None,",
            "        flag_index: Optional[int] = None,",
            "        username: Optional[str] = None,",
            "    ) -> int:",
            "        flagging_dir = self.flagging_dir",
            "        log_fp = \"{}/log.csv\".format(flagging_dir)",
            "        encryption_key = interface.encryption_key if interface.encrypt else None",
            "        is_new = not os.path.exists(log_fp)",
            "        output_only_mode = input_data is None",
            "",
            "        if flag_index is None:",
            "            csv_data = []",
            "            if not output_only_mode:",
            "                for i, input in enumerate(interface.input_components):",
            "                    csv_data.append(",
            "                        input.save_flagged(",
            "                            flagging_dir,",
            "                            interface.config[\"input_components\"][i][\"label\"],",
            "                            input_data[i],",
            "                            encryption_key,",
            "                        )",
            "                    )",
            "            for i, output in enumerate(interface.output_components):",
            "                csv_data.append(",
            "                    output.save_flagged(",
            "                        flagging_dir,",
            "                        interface.config[\"output_components\"][i][\"label\"],",
            "                        output_data[i],",
            "                        encryption_key,",
            "                    )",
            "                    if output_data[i] is not None",
            "                    else \"\"",
            "                )",
            "            if not output_only_mode:",
            "                if flag_option is not None:",
            "                    csv_data.append(flag_option)",
            "                if username is not None:",
            "                    csv_data.append(username)",
            "                csv_data.append(str(datetime.datetime.now()))",
            "            if is_new:",
            "                headers = []",
            "                if not output_only_mode:",
            "                    headers += [",
            "                        interface[\"label\"]",
            "                        for interface in interface.config[\"input_components\"]",
            "                    ]",
            "                headers += [",
            "                    interface[\"label\"]",
            "                    for interface in interface.config[\"output_components\"]",
            "                ]",
            "                if not output_only_mode:",
            "                    if interface.flagging_options is not None:",
            "                        headers.append(\"flag\")",
            "                    if username is not None:",
            "                        headers.append(\"username\")",
            "                    headers.append(\"timestamp\")",
            "",
            "        def replace_flag_at_index(file_content):",
            "            file_content = io.StringIO(file_content)",
            "            content = list(csv.reader(file_content))",
            "            header = content[0]",
            "            flag_col_index = header.index(\"flag\")",
            "            content[flag_index][flag_col_index] = flag_option",
            "            output = io.StringIO()",
            "            writer = csv.writer(output)",
            "            writer.writerows(content)",
            "            return output.getvalue()",
            "",
            "        if interface.encrypt:",
            "            output = io.StringIO()",
            "            if not is_new:",
            "                with open(log_fp, \"rb\") as csvfile:",
            "                    encrypted_csv = csvfile.read()",
            "                    decrypted_csv = encryptor.decrypt(",
            "                        interface.encryption_key, encrypted_csv",
            "                    )",
            "                    file_content = decrypted_csv.decode()",
            "                    if flag_index is not None:",
            "                        file_content = replace_flag_at_index(file_content)",
            "                    output.write(file_content)",
            "            writer = csv.writer(output)",
            "            if flag_index is None:",
            "                if is_new:",
            "                    writer.writerow(headers)",
            "                writer.writerow(csv_data)",
            "            with open(log_fp, \"wb\") as csvfile:",
            "                csvfile.write(",
            "                    encryptor.encrypt(",
            "                        interface.encryption_key, output.getvalue().encode()",
            "                    )",
            "                )",
            "        else:",
            "            if flag_index is None:",
            "                with open(log_fp, \"a\", newline=\"\") as csvfile:",
            "                    writer = csv.writer(csvfile)",
            "                    if is_new:",
            "                        writer.writerow(headers)",
            "                    writer.writerow(csv_data)",
            "            else:",
            "                with open(log_fp) as csvfile:",
            "                    file_content = csvfile.read()",
            "                    file_content = replace_flag_at_index(file_content)",
            "                with open(",
            "                    log_fp, \"w\", newline=\"\"",
            "                ) as csvfile:  # newline parameter needed for Windows",
            "                    csvfile.write(file_content)",
            "        with open(log_fp, \"r\") as csvfile:",
            "            line_count = len([None for row in csv.reader(csvfile)]) - 1",
            "        return line_count",
            "",
            "",
            "class HuggingFaceDatasetSaver(FlaggingCallback):",
            "    \"\"\"",
            "    A FlaggingCallback that saves flagged data to a HuggingFace dataset.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        hf_foken: str,",
            "        dataset_name: str,",
            "        organization: Optional[str] = None,",
            "        private: bool = False,",
            "        verbose: bool = True,",
            "    ):",
            "        \"\"\"",
            "        Params:",
            "        hf_token (str): The token to use to access the huggingface API.",
            "        dataset_name (str): The name of the dataset to save the data to, e.g.",
            "            \"image-classifier-1\"",
            "        organization (str): The name of the organization to which to attach",
            "            the datasets. If None, the dataset attaches to the user only.",
            "        private (bool): If the dataset does not already exist, whether it",
            "            should be created as a private dataset or public. Private datasets",
            "            may require paid huggingface.co accounts",
            "        verbose (bool): Whether to print out the status of the dataset",
            "            creation.",
            "        \"\"\"",
            "        self.hf_foken = hf_foken",
            "        self.dataset_name = dataset_name",
            "        self.organization_name = organization",
            "        self.dataset_private = private",
            "        self.verbose = verbose",
            "",
            "    def setup(self, flagging_dir: str):",
            "        \"\"\"",
            "        Params:",
            "        flagging_dir (str): local directory where the dataset is cloned,",
            "        updated, and pushed from.",
            "        \"\"\"",
            "        try:",
            "            import huggingface_hub",
            "        except (ImportError, ModuleNotFoundError):",
            "            raise ImportError(",
            "                \"Package `huggingface_hub` not found is needed \"",
            "                \"for HuggingFaceDatasetSaver. Try 'pip install huggingface_hub'.\"",
            "            )",
            "        path_to_dataset_repo = huggingface_hub.create_repo(",
            "            name=self.dataset_name,",
            "            token=self.hf_foken,",
            "            private=self.dataset_private,",
            "            repo_type=\"dataset\",",
            "            exist_ok=True,",
            "        )",
            "        self.path_to_dataset_repo = path_to_dataset_repo  # e.g. \"https://huggingface.co/datasets/abidlabs/test-audio-10\"",
            "        self.flagging_dir = flagging_dir",
            "        self.dataset_dir = os.path.join(flagging_dir, self.dataset_name)",
            "        self.repo = huggingface_hub.Repository(",
            "            local_dir=self.dataset_dir,",
            "            clone_from=path_to_dataset_repo,",
            "            use_auth_token=self.hf_foken,",
            "        )",
            "        self.repo.git_pull()",
            "",
            "        # Should filename be user-specified?",
            "        self.log_file = os.path.join(self.dataset_dir, \"data.csv\")",
            "        self.infos_file = os.path.join(self.dataset_dir, \"dataset_infos.json\")",
            "",
            "    def flag(",
            "        self,",
            "        interface: gr.Interface,",
            "        input_data: List[Any],",
            "        output_data: List[Any],",
            "        flag_option: Optional[str] = None,",
            "        flag_index: Optional[int] = None,",
            "        username: Optional[str] = None,",
            "    ) -> int:",
            "        is_new = not os.path.exists(self.log_file)",
            "        infos = {\"flagged\": {\"features\": {}}}",
            "",
            "        with open(self.log_file, \"a\", newline=\"\") as csvfile:",
            "            writer = csv.writer(csvfile)",
            "",
            "            # File previews for certain input and output types",
            "            file_preview_types = {",
            "                gr.inputs.Audio: \"Audio\",",
            "                gr.outputs.Audio: \"Audio\",",
            "                gr.inputs.Image: \"Image\",",
            "                gr.outputs.Image: \"Image\",",
            "            }",
            "",
            "            # Generate the headers and dataset_infos",
            "            if is_new:",
            "                headers = []",
            "",
            "                for i, component in enumerate(interface.input_components):",
            "                    component_label = interface.config[\"input_components\"][i][",
            "                        \"label\"",
            "                    ] or \"Input_{}\".format(i)",
            "                    headers.append(component_label)",
            "                    infos[\"flagged\"][\"features\"][component_label] = {",
            "                        \"dtype\": \"string\",",
            "                        \"_type\": \"Value\",",
            "                    }",
            "                    if isinstance(component, tuple(file_preview_types)):",
            "                        headers.append(component_label + \" file\")",
            "                        for _component, _type in file_preview_types.items():",
            "                            if isinstance(component, _component):",
            "                                infos[\"flagged\"][\"features\"][",
            "                                    component_label + \" file\"",
            "                                ] = {\"_type\": _type}",
            "                                break",
            "",
            "                for i, component in enumerate(interface.output_components):",
            "                    component_label = interface.config[\"output_components\"][i][",
            "                        \"label\"",
            "                    ] or \"Output_{}\".format(i)",
            "                    headers.append(component_label)",
            "                    infos[\"flagged\"][\"features\"][component_label] = {",
            "                        \"dtype\": \"string\",",
            "                        \"_type\": \"Value\",",
            "                    }",
            "                    if isinstance(component, tuple(file_preview_types)):",
            "                        headers.append(component_label + \" file\")",
            "                        for _component, _type in file_preview_types.items():",
            "                            if isinstance(component, _component):",
            "                                infos[\"flagged\"][\"features\"][",
            "                                    component_label + \" file\"",
            "                                ] = {\"_type\": _type}",
            "                                break",
            "",
            "                if interface.flagging_options is not None:",
            "                    headers.append(\"flag\")",
            "                    infos[\"flagged\"][\"features\"][\"flag\"] = {",
            "                        \"dtype\": \"string\",",
            "                        \"_type\": \"Value\",",
            "                    }",
            "",
            "                writer.writerow(headers)",
            "",
            "            # Generate the row corresponding to the flagged sample",
            "            csv_data = []",
            "            for i, component in enumerate(interface.input_components):",
            "                label = interface.config[\"input_components\"][i][",
            "                    \"label\"",
            "                ] or \"Input_{}\".format(i)",
            "                filepath = component.save_flagged(",
            "                    self.dataset_dir, label, input_data[i], None",
            "                )",
            "                csv_data.append(filepath)",
            "                if isinstance(component, tuple(file_preview_types)):",
            "                    csv_data.append(",
            "                        \"{}/resolve/main/{}\".format(self.path_to_dataset_repo, filepath)",
            "                    )",
            "            for i, component in enumerate(interface.output_components):",
            "                label = interface.config[\"output_components\"][i][",
            "                    \"label\"",
            "                ] or \"Output_{}\".format(i)",
            "                filepath = (",
            "                    component.save_flagged(",
            "                        self.dataset_dir, label, output_data[i], None",
            "                    )",
            "                    if output_data[i] is not None",
            "                    else \"\"",
            "                )",
            "                csv_data.append(filepath)",
            "                if isinstance(component, tuple(file_preview_types)):",
            "                    csv_data.append(",
            "                        \"{}/resolve/main/{}\".format(self.path_to_dataset_repo, filepath)",
            "                    )",
            "            if flag_option is not None:",
            "                csv_data.append(flag_option)",
            "",
            "            writer.writerow(csv_data)",
            "",
            "        if is_new:",
            "            json.dump(infos, open(self.infos_file, \"w\"))",
            "",
            "        with open(self.log_file, \"r\") as csvfile:",
            "            line_count = len([None for row in csv.reader(csvfile)]) - 1",
            "",
            "        self.repo.push_to_hub(commit_message=\"Flagged sample #{}\".format(line_count))",
            "",
            "        return line_count"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import csv",
            "import datetime",
            "import io",
            "import json",
            "import os",
            "from abc import ABC, abstractmethod",
            "from typing import Any, List, Optional",
            "",
            "import gradio as gr",
            "from gradio import encryptor, utils",
            "",
            "",
            "class FlaggingCallback(ABC):",
            "    \"\"\"",
            "    An abstract class for defining the methods that any FlaggingCallback should have.",
            "    \"\"\"",
            "",
            "    @abstractmethod",
            "    def setup(self, flagging_dir: str):",
            "        \"\"\"",
            "        This method should be overridden and ensure that everything is set up correctly for flag().",
            "        This method gets called once at the beginning of the Interface.launch() method.",
            "        Parameters:",
            "        flagging_dir: A string, typically containing the path to the directory where the flagging file should be storied (provided as an argument to Interface.__init__()).",
            "        \"\"\"",
            "        pass",
            "",
            "    @abstractmethod",
            "    def flag(",
            "        self,",
            "        interface: gr.Interface,",
            "        input_data: List[Any],",
            "        output_data: List[Any],",
            "        flag_option: Optional[str] = None,",
            "        flag_index: Optional[int] = None,",
            "        username: Optional[str] = None,",
            "    ) -> int:",
            "        \"\"\"",
            "        This method should be overridden by the FlaggingCallback subclass and may contain optional additional arguments.",
            "        This gets called every time the <flag> button is pressed.",
            "        Parameters:",
            "        interface: The Interface object that is being used to launch the flagging interface.",
            "        input_data: The input data to be flagged.",
            "        output_data: The output data to be flagged.",
            "        flag_option (optional): In the case that flagging_options are provided, the flag option that is being used.",
            "        flag_index (optional): The index of the sample that is being flagged.",
            "        username (optional): The username of the user that is flagging the data, if logged in.",
            "        Returns:",
            "        (int) The total number of samples that have been flagged.",
            "        \"\"\"",
            "        pass",
            "",
            "",
            "class SimpleCSVLogger(FlaggingCallback):",
            "    \"\"\"",
            "    A simple example implementation of the FlaggingCallback abstract class",
            "    provided for illustrative purposes.",
            "    \"\"\"",
            "",
            "    def setup(self, flagging_dir: str):",
            "        self.flagging_dir = flagging_dir",
            "        os.makedirs(flagging_dir, exist_ok=True)",
            "",
            "    def flag(",
            "        self,",
            "        interface: gr.Interface,",
            "        input_data: List[Any],",
            "        output_data: List[Any],",
            "        flag_option: Optional[str] = None,",
            "        flag_index: Optional[int] = None,",
            "        username: Optional[str] = None,",
            "    ) -> int:",
            "        flagging_dir = self.flagging_dir",
            "        log_filepath = \"{}/log.csv\".format(flagging_dir)",
            "",
            "        csv_data = []",
            "        for i, input in enumerate(interface.input_components):",
            "            csv_data.append(",
            "                input.save_flagged(",
            "                    flagging_dir,",
            "                    interface.config[\"input_components\"][i][\"label\"],",
            "                    input_data[i],",
            "                    None,",
            "                )",
            "            )",
            "        for i, output in enumerate(interface.output_components):",
            "            csv_data.append(",
            "                output.save_flagged(",
            "                    flagging_dir,",
            "                    interface.config[\"output_components\"][i][\"label\"],",
            "                    output_data[i],",
            "                    None,",
            "                )",
            "                if output_data[i] is not None",
            "                else \"\"",
            "            )",
            "",
            "        with open(log_filepath, \"a\", newline=\"\") as csvfile:",
            "            writer = csv.writer(csvfile)",
            "            writer.writerow(utils.santize_for_csv(csv_data))",
            "",
            "        with open(log_filepath, \"r\") as csvfile:",
            "            line_count = len([None for row in csv.reader(csvfile)]) - 1",
            "        return line_count",
            "",
            "",
            "class CSVLogger(FlaggingCallback):",
            "    \"\"\"",
            "    The default implementation of the FlaggingCallback abstract class.",
            "    Logs the input and output data to a CSV file. Supports encryption.",
            "    \"\"\"",
            "",
            "    def setup(self, flagging_dir: str):",
            "        self.flagging_dir = flagging_dir",
            "        os.makedirs(flagging_dir, exist_ok=True)",
            "",
            "    def flag(",
            "        self,",
            "        interface: gr.Interface,",
            "        input_data: List[Any],",
            "        output_data: List[Any],",
            "        flag_option: Optional[str] = None,",
            "        flag_index: Optional[int] = None,",
            "        username: Optional[str] = None,",
            "    ) -> int:",
            "        flagging_dir = self.flagging_dir",
            "        log_fp = \"{}/log.csv\".format(flagging_dir)",
            "        encryption_key = interface.encryption_key if interface.encrypt else None",
            "        is_new = not os.path.exists(log_fp)",
            "        output_only_mode = input_data is None",
            "",
            "        if flag_index is None:",
            "            csv_data = []",
            "            if not output_only_mode:",
            "                for i, input in enumerate(interface.input_components):",
            "                    csv_data.append(",
            "                        input.save_flagged(",
            "                            flagging_dir,",
            "                            interface.config[\"input_components\"][i][\"label\"],",
            "                            input_data[i],",
            "                            encryption_key,",
            "                        )",
            "                    )",
            "            for i, output in enumerate(interface.output_components):",
            "                csv_data.append(",
            "                    output.save_flagged(",
            "                        flagging_dir,",
            "                        interface.config[\"output_components\"][i][\"label\"],",
            "                        output_data[i],",
            "                        encryption_key,",
            "                    )",
            "                    if output_data[i] is not None",
            "                    else \"\"",
            "                )",
            "            if not output_only_mode:",
            "                if flag_option is not None:",
            "                    csv_data.append(flag_option)",
            "                if username is not None:",
            "                    csv_data.append(username)",
            "                csv_data.append(str(datetime.datetime.now()))",
            "            if is_new:",
            "                headers = []",
            "                if not output_only_mode:",
            "                    headers += [",
            "                        interface[\"label\"]",
            "                        for interface in interface.config[\"input_components\"]",
            "                    ]",
            "                headers += [",
            "                    interface[\"label\"]",
            "                    for interface in interface.config[\"output_components\"]",
            "                ]",
            "                if not output_only_mode:",
            "                    if interface.flagging_options is not None:",
            "                        headers.append(\"flag\")",
            "                    if username is not None:",
            "                        headers.append(\"username\")",
            "                    headers.append(\"timestamp\")",
            "",
            "        def replace_flag_at_index(file_content):",
            "            file_content = io.StringIO(file_content)",
            "            content = list(csv.reader(file_content))",
            "            header = content[0]",
            "            flag_col_index = header.index(\"flag\")",
            "            content[flag_index][flag_col_index] = flag_option",
            "            output = io.StringIO()",
            "            writer = csv.writer(output)",
            "            writer.writerows(utils.santize_for_csv(content))",
            "            return output.getvalue()",
            "",
            "        if interface.encrypt:",
            "            output = io.StringIO()",
            "            if not is_new:",
            "                with open(log_fp, \"rb\") as csvfile:",
            "                    encrypted_csv = csvfile.read()",
            "                    decrypted_csv = encryptor.decrypt(",
            "                        interface.encryption_key, encrypted_csv",
            "                    )",
            "                    file_content = decrypted_csv.decode()",
            "                    if flag_index is not None:",
            "                        file_content = replace_flag_at_index(file_content)",
            "                    output.write(utils.santize_for_csv(file_content))",
            "            writer = csv.writer(output)",
            "            if flag_index is None:",
            "                if is_new:",
            "                    writer.writerow(headers)",
            "                writer.writerow(csv_data)",
            "            with open(log_fp, \"wb\") as csvfile:",
            "                csvfile.write(",
            "                    utils.santize_for_csv(",
            "                        encryptor.encrypt(",
            "                            interface.encryption_key, output.getvalue().encode()",
            "                        )",
            "                    )",
            "                )",
            "        else:",
            "            if flag_index is None:",
            "                with open(log_fp, \"a\", newline=\"\") as csvfile:",
            "                    writer = csv.writer(csvfile)",
            "                    if is_new:",
            "                        writer.writerow(utils.santize_for_csv(headers))",
            "                    writer.writerow(utils.santize_for_csv(csv_data))",
            "            else:",
            "                with open(log_fp) as csvfile:",
            "                    file_content = csvfile.read()",
            "                    file_content = replace_flag_at_index(file_content)",
            "                with open(",
            "                    log_fp, \"w\", newline=\"\"",
            "                ) as csvfile:  # newline parameter needed for Windows",
            "                    csvfile.write(utils.santize_for_csv(file_content))",
            "        with open(log_fp, \"r\") as csvfile:",
            "            line_count = len([None for row in csv.reader(csvfile)]) - 1",
            "        return line_count",
            "",
            "",
            "class HuggingFaceDatasetSaver(FlaggingCallback):",
            "    \"\"\"",
            "    A FlaggingCallback that saves flagged data to a HuggingFace dataset.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        hf_foken: str,",
            "        dataset_name: str,",
            "        organization: Optional[str] = None,",
            "        private: bool = False,",
            "        verbose: bool = True,",
            "    ):",
            "        \"\"\"",
            "        Params:",
            "        hf_token (str): The token to use to access the huggingface API.",
            "        dataset_name (str): The name of the dataset to save the data to, e.g.",
            "            \"image-classifier-1\"",
            "        organization (str): The name of the organization to which to attach",
            "            the datasets. If None, the dataset attaches to the user only.",
            "        private (bool): If the dataset does not already exist, whether it",
            "            should be created as a private dataset or public. Private datasets",
            "            may require paid huggingface.co accounts",
            "        verbose (bool): Whether to print out the status of the dataset",
            "            creation.",
            "        \"\"\"",
            "        self.hf_foken = hf_foken",
            "        self.dataset_name = dataset_name",
            "        self.organization_name = organization",
            "        self.dataset_private = private",
            "        self.verbose = verbose",
            "",
            "    def setup(self, flagging_dir: str):",
            "        \"\"\"",
            "        Params:",
            "        flagging_dir (str): local directory where the dataset is cloned,",
            "        updated, and pushed from.",
            "        \"\"\"",
            "        try:",
            "            import huggingface_hub",
            "        except (ImportError, ModuleNotFoundError):",
            "            raise ImportError(",
            "                \"Package `huggingface_hub` not found is needed \"",
            "                \"for HuggingFaceDatasetSaver. Try 'pip install huggingface_hub'.\"",
            "            )",
            "        path_to_dataset_repo = huggingface_hub.create_repo(",
            "            name=self.dataset_name,",
            "            token=self.hf_foken,",
            "            private=self.dataset_private,",
            "            repo_type=\"dataset\",",
            "            exist_ok=True,",
            "        )",
            "        self.path_to_dataset_repo = path_to_dataset_repo  # e.g. \"https://huggingface.co/datasets/abidlabs/test-audio-10\"",
            "        self.flagging_dir = flagging_dir",
            "        self.dataset_dir = os.path.join(flagging_dir, self.dataset_name)",
            "        self.repo = huggingface_hub.Repository(",
            "            local_dir=self.dataset_dir,",
            "            clone_from=path_to_dataset_repo,",
            "            use_auth_token=self.hf_foken,",
            "        )",
            "        self.repo.git_pull()",
            "",
            "        # Should filename be user-specified?",
            "        self.log_file = os.path.join(self.dataset_dir, \"data.csv\")",
            "        self.infos_file = os.path.join(self.dataset_dir, \"dataset_infos.json\")",
            "",
            "    def flag(",
            "        self,",
            "        interface: gr.Interface,",
            "        input_data: List[Any],",
            "        output_data: List[Any],",
            "        flag_option: Optional[str] = None,",
            "        flag_index: Optional[int] = None,",
            "        username: Optional[str] = None,",
            "    ) -> int:",
            "        is_new = not os.path.exists(self.log_file)",
            "        infos = {\"flagged\": {\"features\": {}}}",
            "",
            "        with open(self.log_file, \"a\", newline=\"\") as csvfile:",
            "            writer = csv.writer(csvfile)",
            "",
            "            # File previews for certain input and output types",
            "            file_preview_types = {",
            "                gr.inputs.Audio: \"Audio\",",
            "                gr.outputs.Audio: \"Audio\",",
            "                gr.inputs.Image: \"Image\",",
            "                gr.outputs.Image: \"Image\",",
            "            }",
            "",
            "            # Generate the headers and dataset_infos",
            "            if is_new:",
            "                headers = []",
            "",
            "                for i, component in enumerate(interface.input_components):",
            "                    component_label = interface.config[\"input_components\"][i][",
            "                        \"label\"",
            "                    ] or \"Input_{}\".format(i)",
            "                    headers.append(component_label)",
            "                    infos[\"flagged\"][\"features\"][component_label] = {",
            "                        \"dtype\": \"string\",",
            "                        \"_type\": \"Value\",",
            "                    }",
            "                    if isinstance(component, tuple(file_preview_types)):",
            "                        headers.append(component_label + \" file\")",
            "                        for _component, _type in file_preview_types.items():",
            "                            if isinstance(component, _component):",
            "                                infos[\"flagged\"][\"features\"][",
            "                                    component_label + \" file\"",
            "                                ] = {\"_type\": _type}",
            "                                break",
            "",
            "                for i, component in enumerate(interface.output_components):",
            "                    component_label = interface.config[\"output_components\"][i][",
            "                        \"label\"",
            "                    ] or \"Output_{}\".format(i)",
            "                    headers.append(component_label)",
            "                    infos[\"flagged\"][\"features\"][component_label] = {",
            "                        \"dtype\": \"string\",",
            "                        \"_type\": \"Value\",",
            "                    }",
            "                    if isinstance(component, tuple(file_preview_types)):",
            "                        headers.append(component_label + \" file\")",
            "                        for _component, _type in file_preview_types.items():",
            "                            if isinstance(component, _component):",
            "                                infos[\"flagged\"][\"features\"][",
            "                                    component_label + \" file\"",
            "                                ] = {\"_type\": _type}",
            "                                break",
            "",
            "                if interface.flagging_options is not None:",
            "                    headers.append(\"flag\")",
            "                    infos[\"flagged\"][\"features\"][\"flag\"] = {",
            "                        \"dtype\": \"string\",",
            "                        \"_type\": \"Value\",",
            "                    }",
            "",
            "                writer.writerow(utils.santize_for_csv(headers))",
            "",
            "            # Generate the row corresponding to the flagged sample",
            "            csv_data = []",
            "            for i, component in enumerate(interface.input_components):",
            "                label = interface.config[\"input_components\"][i][",
            "                    \"label\"",
            "                ] or \"Input_{}\".format(i)",
            "                filepath = component.save_flagged(",
            "                    self.dataset_dir, label, input_data[i], None",
            "                )",
            "                csv_data.append(filepath)",
            "                if isinstance(component, tuple(file_preview_types)):",
            "                    csv_data.append(",
            "                        \"{}/resolve/main/{}\".format(self.path_to_dataset_repo, filepath)",
            "                    )",
            "            for i, component in enumerate(interface.output_components):",
            "                label = interface.config[\"output_components\"][i][",
            "                    \"label\"",
            "                ] or \"Output_{}\".format(i)",
            "                filepath = (",
            "                    component.save_flagged(",
            "                        self.dataset_dir, label, output_data[i], None",
            "                    )",
            "                    if output_data[i] is not None",
            "                    else \"\"",
            "                )",
            "                csv_data.append(filepath)",
            "                if isinstance(component, tuple(file_preview_types)):",
            "                    csv_data.append(",
            "                        \"{}/resolve/main/{}\".format(self.path_to_dataset_repo, filepath)",
            "                    )",
            "            if flag_option is not None:",
            "                csv_data.append(flag_option)",
            "",
            "            writer.writerow(utils.santize_for_csv(csv_data))",
            "",
            "        if is_new:",
            "            json.dump(infos, open(self.infos_file, \"w\"))",
            "",
            "        with open(self.log_file, \"r\") as csvfile:",
            "            line_count = len([None for row in csv.reader(csvfile)]) - 1",
            "",
            "        self.repo.push_to_hub(commit_message=\"Flagged sample #{}\".format(line_count))",
            "",
            "        return line_count"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "12": [],
            "102": [
                "SimpleCSVLogger",
                "flag"
            ],
            "189": [
                "CSVLogger",
                "flag",
                "replace_flag_at_index"
            ],
            "203": [
                "CSVLogger",
                "flag"
            ],
            "211": [
                "CSVLogger",
                "flag"
            ],
            "212": [
                "CSVLogger",
                "flag"
            ],
            "220": [
                "CSVLogger",
                "flag"
            ],
            "221": [
                "CSVLogger",
                "flag"
            ],
            "229": [
                "CSVLogger",
                "flag"
            ],
            "371": [
                "HuggingFaceDatasetSaver",
                "flag"
            ],
            "406": [
                "HuggingFaceDatasetSaver",
                "flag"
            ]
        },
        "addLocation": []
    },
    "gradio/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " from __future__ import annotations"
            },
            "2": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 5,
                "PatchRowcode": "+import copy"
            },
            "4": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " import csv"
            },
            "5": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " import inspect"
            },
            "6": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " import json"
            },
            "7": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " import random"
            },
            "8": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " import warnings"
            },
            "9": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " from distutils.version import StrictVersion"
            },
            "10": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from typing import TYPE_CHECKING, Any, Callable, Dict"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 14,
                "PatchRowcode": "+from typing import TYPE_CHECKING, Any, Callable, Dict, List"
            },
            "12": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " import aiohttp"
            },
            "14": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " import analytics"
            },
            "15": {
                "beforePatchRowNumber": 286,
                "afterPatchRowNumber": 287,
                "PatchRowcode": "         v.default if v.default is not inspect.Parameter.empty else None"
            },
            "16": {
                "beforePatchRowNumber": 287,
                "afterPatchRowNumber": 288,
                "PatchRowcode": "         for v in signature.parameters.values()"
            },
            "17": {
                "beforePatchRowNumber": 288,
                "afterPatchRowNumber": 289,
                "PatchRowcode": "     ]"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 290,
                "PatchRowcode": "+"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 291,
                "PatchRowcode": "+"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 292,
                "PatchRowcode": "+def santize_for_csv(data: str | List[str] | List[List[str]]):"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 293,
                "PatchRowcode": "+    \"\"\"Sanitizes data so that it can be safely written to a CSV file.\"\"\""
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 294,
                "PatchRowcode": "+"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 295,
                "PatchRowcode": "+    def sanitize(item):"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 296,
                "PatchRowcode": "+        return \"'\" + item"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 297,
                "PatchRowcode": "+"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 298,
                "PatchRowcode": "+    unsafe_prefixes = (\"+\", \"=\", \"-\", \"@\")"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 299,
                "PatchRowcode": "+    warning_message = \"Sanitizing flagged data by escaping cell contents that begin \""
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 300,
                "PatchRowcode": "+    \"with one of the following characters: '+', '=', '-', '@'.\""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 301,
                "PatchRowcode": "+"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 302,
                "PatchRowcode": "+    if isinstance(data, str):"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 303,
                "PatchRowcode": "+        if data.startswith(unsafe_prefixes):"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 304,
                "PatchRowcode": "+            warnings.warn(warning_message)"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 305,
                "PatchRowcode": "+            return sanitize(data)"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 306,
                "PatchRowcode": "+        return data"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 307,
                "PatchRowcode": "+    elif isinstance(data, list) and isinstance(data[0], str):"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 308,
                "PatchRowcode": "+        sanitized_data = copy.deepcopy(data)"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 309,
                "PatchRowcode": "+        for index, item in enumerate(data):"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 310,
                "PatchRowcode": "+            if item.startswith(unsafe_prefixes):"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 311,
                "PatchRowcode": "+                warnings.warn(warning_message)"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 312,
                "PatchRowcode": "+                sanitized_data[index] = sanitize(item)"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 313,
                "PatchRowcode": "+        return sanitized_data"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 314,
                "PatchRowcode": "+    elif isinstance(data[0], list) and isinstance(data[0][0], str):"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 315,
                "PatchRowcode": "+        sanitized_data = copy.deepcopy(data)"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 316,
                "PatchRowcode": "+        for outer_index, sublist in enumerate(data):"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 317,
                "PatchRowcode": "+            for inner_index, item in enumerate(sublist):"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 318,
                "PatchRowcode": "+                if item.startswith(unsafe_prefixes):"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 319,
                "PatchRowcode": "+                    warnings.warn(warning_message)"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 320,
                "PatchRowcode": "+                    sanitized_data[outer_index][inner_index] = sanitize(item)"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 321,
                "PatchRowcode": "+        return sanitized_data"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 322,
                "PatchRowcode": "+    else:"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 323,
                "PatchRowcode": "+        raise ValueError(\"Unsupported data type: \" + str(type(data)))"
            }
        },
        "frontPatchFile": [
            "\"\"\" Handy utility functions.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import csv",
            "import inspect",
            "import json",
            "import json.decoder",
            "import os",
            "import random",
            "import warnings",
            "from distutils.version import StrictVersion",
            "from typing import TYPE_CHECKING, Any, Callable, Dict",
            "",
            "import aiohttp",
            "import analytics",
            "import pkg_resources",
            "import requests",
            "",
            "import gradio",
            "",
            "if TYPE_CHECKING:  # Only import for type checking (is False at runtime).",
            "    from gradio import Interface",
            "",
            "analytics_url = \"https://api.gradio.app/\"",
            "PKG_VERSION_URL = \"https://api.gradio.app/pkg-version\"",
            "analytics.write_key = \"uxIFddIEuuUcFLf9VgH2teTEtPlWdkNy\"",
            "JSON_PATH = os.path.join(os.path.dirname(gradio.__file__), \"launches.json\")",
            "",
            "",
            "def version_check():",
            "    try:",
            "        current_pkg_version = pkg_resources.require(\"gradio\")[0].version",
            "        latest_pkg_version = requests.get(url=PKG_VERSION_URL).json()[\"version\"]",
            "        if StrictVersion(latest_pkg_version) > StrictVersion(current_pkg_version):",
            "            print(",
            "                \"IMPORTANT: You are using gradio version {}, \"",
            "                \"however version {} \"",
            "                \"is available, please upgrade.\".format(",
            "                    current_pkg_version, latest_pkg_version",
            "                )",
            "            )",
            "            print(\"--------\")",
            "    except pkg_resources.DistributionNotFound:",
            "        warnings.warn(",
            "            \"gradio is not setup or installed properly. Unable to get version info.\"",
            "        )",
            "    except json.decoder.JSONDecodeError:",
            "        warnings.warn(\"unable to parse version details from package URL.\")",
            "    except KeyError:",
            "        warnings.warn(\"package URL does not contain version info.\")",
            "    except:",
            "        pass",
            "",
            "",
            "def get_local_ip_address() -> str:",
            "    try:",
            "        ip_address = requests.get(\"https://api.ipify.org\", timeout=3).text",
            "    except (requests.ConnectionError, requests.exceptions.ReadTimeout):",
            "        ip_address = \"No internet connection\"",
            "    return ip_address",
            "",
            "",
            "def initiated_analytics(data: Dict[str:Any]) -> None:",
            "    try:",
            "        requests.post(",
            "            analytics_url + \"gradio-initiated-analytics/\", data=data, timeout=3",
            "        )",
            "    except (requests.ConnectionError, requests.exceptions.ReadTimeout):",
            "        pass  # do not push analytics if no network",
            "",
            "",
            "def launch_analytics(data: Dict[str, Any]) -> None:",
            "    try:",
            "        requests.post(",
            "            analytics_url + \"gradio-launched-analytics/\", data=data, timeout=3",
            "        )",
            "    except (requests.ConnectionError, requests.exceptions.ReadTimeout):",
            "        pass  # do not push analytics if no network",
            "",
            "",
            "def integration_analytics(data: Dict[str, Any]) -> None:",
            "    try:",
            "        requests.post(",
            "            analytics_url + \"gradio-integration-analytics/\", data=data, timeout=3",
            "        )",
            "    except (requests.ConnectionError, requests.exceptions.ReadTimeout):",
            "        pass  # do not push analytics if no network",
            "",
            "",
            "def error_analytics(ip_address: str, message: str) -> None:",
            "    \"\"\"",
            "    Send error analytics if there is network",
            "    :param type: RuntimeError or NameError",
            "    \"\"\"",
            "    data = {\"ip_address\": ip_address, \"error\": message}",
            "    try:",
            "        requests.post(analytics_url + \"gradio-error-analytics/\", data=data, timeout=3)",
            "    except (requests.ConnectionError, requests.exceptions.ReadTimeout):",
            "        pass  # do not push analytics if no network",
            "",
            "",
            "async def log_feature_analytics(ip_address: str, feature: str) -> None:",
            "    data = {\"ip_address\": ip_address, \"feature\": feature}",
            "    async with aiohttp.ClientSession() as session:",
            "        try:",
            "            async with session.post(",
            "                analytics_url + \"gradio-feature-analytics/\", data=data",
            "            ):",
            "                pass",
            "        except (aiohttp.ClientError):",
            "            pass  # do not push analytics if no network",
            "",
            "",
            "def colab_check() -> bool:",
            "    \"\"\"",
            "    Check if interface is launching from Google Colab",
            "    :return is_colab (bool): True or False",
            "    \"\"\"",
            "    is_colab = False",
            "    try:  # Check if running interactively using ipython.",
            "        from IPython import get_ipython",
            "",
            "        from_ipynb = get_ipython()",
            "        if \"google.colab\" in str(from_ipynb):",
            "            is_colab = True",
            "    except (ImportError, NameError):",
            "        pass",
            "    return is_colab",
            "",
            "",
            "def ipython_check() -> bool:",
            "    \"\"\"",
            "    Check if interface is launching from iPython (not colab)",
            "    :return is_ipython (bool): True or False",
            "    \"\"\"",
            "    is_ipython = False",
            "    try:  # Check if running interactively using ipython.",
            "        from IPython import get_ipython",
            "",
            "        if get_ipython() is not None:",
            "            is_ipython = True",
            "    except (ImportError, NameError):",
            "        pass",
            "    return is_ipython",
            "",
            "",
            "def readme_to_html(article: str) -> str:",
            "    try:",
            "        response = requests.get(article, timeout=3)",
            "        if response.status_code == requests.codes.ok:  # pylint: disable=no-member",
            "            article = response.text",
            "    except requests.exceptions.RequestException:",
            "        pass",
            "    return article",
            "",
            "",
            "def show_tip(interface: Interface) -> None:",
            "    if interface.show_tips and random.random() < 1.5:",
            "        tip: str = random.choice(gradio.strings.en[\"TIPS\"])",
            "        print(f\"Tip: {tip}\")",
            "",
            "",
            "def launch_counter() -> None:",
            "    try:",
            "        if not os.path.exists(JSON_PATH):",
            "            launches = {\"launches\": 1}",
            "            with open(JSON_PATH, \"w+\") as j:",
            "                json.dump(launches, j)",
            "        else:",
            "            with open(JSON_PATH) as j:",
            "                launches = json.load(j)",
            "            launches[\"launches\"] += 1",
            "            if launches[\"launches\"] in [25, 50]:",
            "                print(gradio.strings.en[\"BETA_INVITE\"])",
            "            with open(JSON_PATH, \"w\") as j:",
            "                j.write(json.dumps(launches))",
            "    except:",
            "        pass",
            "",
            "",
            "def get_config_file(interface: Interface) -> Dict[str, Any]:",
            "    config = {",
            "        \"input_components\": [",
            "            iface.get_template_context() for iface in interface.input_components",
            "        ],",
            "        \"output_components\": [",
            "            iface.get_template_context() for iface in interface.output_components",
            "        ],",
            "        \"function_count\": len(interface.predict),",
            "        \"live\": interface.live,",
            "        \"examples_per_page\": interface.examples_per_page,",
            "        \"layout\": interface.layout,",
            "        \"show_input\": interface.show_input,",
            "        \"show_output\": interface.show_output,",
            "        \"title\": interface.title,",
            "        \"analytics_enabled\": interface.analytics_enabled,",
            "        \"description\": interface.description,",
            "        \"simple_description\": interface.simple_description,",
            "        \"article\": interface.article,",
            "        \"theme\": interface.theme,",
            "        \"css\": interface.css,",
            "        \"thumbnail\": interface.thumbnail,",
            "        \"allow_screenshot\": interface.allow_screenshot,",
            "        \"allow_flagging\": interface.allow_flagging,",
            "        \"flagging_options\": interface.flagging_options,",
            "        \"allow_interpretation\": interface.interpretation is not None,",
            "        \"queue\": interface.enable_queue,",
            "        \"cached_examples\": interface.cache_examples",
            "        if hasattr(interface, \"cache_examples\")",
            "        else False,",
            "        \"version\": pkg_resources.require(\"gradio\")[0].version,",
            "        \"favicon_path\": interface.favicon_path,",
            "    }",
            "    try:",
            "        param_names = inspect.getfullargspec(interface.predict[0])[0]",
            "        for iface, param in zip(config[\"input_components\"], param_names):",
            "            if not iface[\"label\"]:",
            "                iface[\"label\"] = param.replace(\"_\", \" \")",
            "        for i, iface in enumerate(config[\"output_components\"]):",
            "            outputs_per_function = int(",
            "                len(interface.output_components) / len(interface.predict)",
            "            )",
            "            function_index = i // outputs_per_function",
            "            component_index = i - function_index * outputs_per_function",
            "            ret_name = (",
            "                \"Output \" + str(component_index + 1)",
            "                if outputs_per_function > 1",
            "                else \"Output\"",
            "            )",
            "            if iface[\"label\"] is None:",
            "                iface[\"label\"] = ret_name",
            "            if len(interface.predict) > 1:",
            "                iface[\"label\"] = (",
            "                    interface.function_names[function_index].replace(\"_\", \" \")",
            "                    + \": \"",
            "                    + iface[\"label\"]",
            "                )",
            "",
            "    except ValueError:",
            "        pass",
            "    if interface.examples is not None:",
            "        if isinstance(interface.examples, str):",
            "            if not os.path.exists(interface.examples):",
            "                raise FileNotFoundError(",
            "                    \"Could not find examples directory: \" + interface.examples",
            "                )",
            "            log_file = os.path.join(interface.examples, \"log.csv\")",
            "            if not os.path.exists(log_file):",
            "                if len(interface.input_components) == 1:",
            "                    examples = [",
            "                        [os.path.join(interface.examples, item)]",
            "                        for item in os.listdir(interface.examples)",
            "                    ]",
            "                else:",
            "                    raise FileNotFoundError(",
            "                        \"Could not find log file (required for multiple inputs): \"",
            "                        + log_file",
            "                    )",
            "            else:",
            "                with open(log_file) as logs:",
            "                    examples = list(csv.reader(logs))",
            "                    examples = examples[1:]  # remove header",
            "            for i, example in enumerate(examples):",
            "                for j, (component, cell) in enumerate(",
            "                    zip(",
            "                        interface.input_components + interface.output_components,",
            "                        example,",
            "                    )",
            "                ):",
            "                    examples[i][j] = component.restore_flagged(",
            "                        interface.flagging_dir,",
            "                        cell,",
            "                        interface.encryption_key if interface.encrypt else None,",
            "                    )",
            "            config[\"examples\"] = examples",
            "            config[\"examples_dir\"] = interface.examples",
            "        else:",
            "            config[\"examples\"] = interface.examples",
            "    return config",
            "",
            "",
            "def get_default_args(func: Callable) -> Dict[str, Any]:",
            "    signature = inspect.signature(func)",
            "    return [",
            "        v.default if v.default is not inspect.Parameter.empty else None",
            "        for v in signature.parameters.values()",
            "    ]"
        ],
        "afterPatchFile": [
            "\"\"\" Handy utility functions.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import copy",
            "import csv",
            "import inspect",
            "import json",
            "import json.decoder",
            "import os",
            "import random",
            "import warnings",
            "from distutils.version import StrictVersion",
            "from typing import TYPE_CHECKING, Any, Callable, Dict, List",
            "",
            "import aiohttp",
            "import analytics",
            "import pkg_resources",
            "import requests",
            "",
            "import gradio",
            "",
            "if TYPE_CHECKING:  # Only import for type checking (is False at runtime).",
            "    from gradio import Interface",
            "",
            "analytics_url = \"https://api.gradio.app/\"",
            "PKG_VERSION_URL = \"https://api.gradio.app/pkg-version\"",
            "analytics.write_key = \"uxIFddIEuuUcFLf9VgH2teTEtPlWdkNy\"",
            "JSON_PATH = os.path.join(os.path.dirname(gradio.__file__), \"launches.json\")",
            "",
            "",
            "def version_check():",
            "    try:",
            "        current_pkg_version = pkg_resources.require(\"gradio\")[0].version",
            "        latest_pkg_version = requests.get(url=PKG_VERSION_URL).json()[\"version\"]",
            "        if StrictVersion(latest_pkg_version) > StrictVersion(current_pkg_version):",
            "            print(",
            "                \"IMPORTANT: You are using gradio version {}, \"",
            "                \"however version {} \"",
            "                \"is available, please upgrade.\".format(",
            "                    current_pkg_version, latest_pkg_version",
            "                )",
            "            )",
            "            print(\"--------\")",
            "    except pkg_resources.DistributionNotFound:",
            "        warnings.warn(",
            "            \"gradio is not setup or installed properly. Unable to get version info.\"",
            "        )",
            "    except json.decoder.JSONDecodeError:",
            "        warnings.warn(\"unable to parse version details from package URL.\")",
            "    except KeyError:",
            "        warnings.warn(\"package URL does not contain version info.\")",
            "    except:",
            "        pass",
            "",
            "",
            "def get_local_ip_address() -> str:",
            "    try:",
            "        ip_address = requests.get(\"https://api.ipify.org\", timeout=3).text",
            "    except (requests.ConnectionError, requests.exceptions.ReadTimeout):",
            "        ip_address = \"No internet connection\"",
            "    return ip_address",
            "",
            "",
            "def initiated_analytics(data: Dict[str:Any]) -> None:",
            "    try:",
            "        requests.post(",
            "            analytics_url + \"gradio-initiated-analytics/\", data=data, timeout=3",
            "        )",
            "    except (requests.ConnectionError, requests.exceptions.ReadTimeout):",
            "        pass  # do not push analytics if no network",
            "",
            "",
            "def launch_analytics(data: Dict[str, Any]) -> None:",
            "    try:",
            "        requests.post(",
            "            analytics_url + \"gradio-launched-analytics/\", data=data, timeout=3",
            "        )",
            "    except (requests.ConnectionError, requests.exceptions.ReadTimeout):",
            "        pass  # do not push analytics if no network",
            "",
            "",
            "def integration_analytics(data: Dict[str, Any]) -> None:",
            "    try:",
            "        requests.post(",
            "            analytics_url + \"gradio-integration-analytics/\", data=data, timeout=3",
            "        )",
            "    except (requests.ConnectionError, requests.exceptions.ReadTimeout):",
            "        pass  # do not push analytics if no network",
            "",
            "",
            "def error_analytics(ip_address: str, message: str) -> None:",
            "    \"\"\"",
            "    Send error analytics if there is network",
            "    :param type: RuntimeError or NameError",
            "    \"\"\"",
            "    data = {\"ip_address\": ip_address, \"error\": message}",
            "    try:",
            "        requests.post(analytics_url + \"gradio-error-analytics/\", data=data, timeout=3)",
            "    except (requests.ConnectionError, requests.exceptions.ReadTimeout):",
            "        pass  # do not push analytics if no network",
            "",
            "",
            "async def log_feature_analytics(ip_address: str, feature: str) -> None:",
            "    data = {\"ip_address\": ip_address, \"feature\": feature}",
            "    async with aiohttp.ClientSession() as session:",
            "        try:",
            "            async with session.post(",
            "                analytics_url + \"gradio-feature-analytics/\", data=data",
            "            ):",
            "                pass",
            "        except (aiohttp.ClientError):",
            "            pass  # do not push analytics if no network",
            "",
            "",
            "def colab_check() -> bool:",
            "    \"\"\"",
            "    Check if interface is launching from Google Colab",
            "    :return is_colab (bool): True or False",
            "    \"\"\"",
            "    is_colab = False",
            "    try:  # Check if running interactively using ipython.",
            "        from IPython import get_ipython",
            "",
            "        from_ipynb = get_ipython()",
            "        if \"google.colab\" in str(from_ipynb):",
            "            is_colab = True",
            "    except (ImportError, NameError):",
            "        pass",
            "    return is_colab",
            "",
            "",
            "def ipython_check() -> bool:",
            "    \"\"\"",
            "    Check if interface is launching from iPython (not colab)",
            "    :return is_ipython (bool): True or False",
            "    \"\"\"",
            "    is_ipython = False",
            "    try:  # Check if running interactively using ipython.",
            "        from IPython import get_ipython",
            "",
            "        if get_ipython() is not None:",
            "            is_ipython = True",
            "    except (ImportError, NameError):",
            "        pass",
            "    return is_ipython",
            "",
            "",
            "def readme_to_html(article: str) -> str:",
            "    try:",
            "        response = requests.get(article, timeout=3)",
            "        if response.status_code == requests.codes.ok:  # pylint: disable=no-member",
            "            article = response.text",
            "    except requests.exceptions.RequestException:",
            "        pass",
            "    return article",
            "",
            "",
            "def show_tip(interface: Interface) -> None:",
            "    if interface.show_tips and random.random() < 1.5:",
            "        tip: str = random.choice(gradio.strings.en[\"TIPS\"])",
            "        print(f\"Tip: {tip}\")",
            "",
            "",
            "def launch_counter() -> None:",
            "    try:",
            "        if not os.path.exists(JSON_PATH):",
            "            launches = {\"launches\": 1}",
            "            with open(JSON_PATH, \"w+\") as j:",
            "                json.dump(launches, j)",
            "        else:",
            "            with open(JSON_PATH) as j:",
            "                launches = json.load(j)",
            "            launches[\"launches\"] += 1",
            "            if launches[\"launches\"] in [25, 50]:",
            "                print(gradio.strings.en[\"BETA_INVITE\"])",
            "            with open(JSON_PATH, \"w\") as j:",
            "                j.write(json.dumps(launches))",
            "    except:",
            "        pass",
            "",
            "",
            "def get_config_file(interface: Interface) -> Dict[str, Any]:",
            "    config = {",
            "        \"input_components\": [",
            "            iface.get_template_context() for iface in interface.input_components",
            "        ],",
            "        \"output_components\": [",
            "            iface.get_template_context() for iface in interface.output_components",
            "        ],",
            "        \"function_count\": len(interface.predict),",
            "        \"live\": interface.live,",
            "        \"examples_per_page\": interface.examples_per_page,",
            "        \"layout\": interface.layout,",
            "        \"show_input\": interface.show_input,",
            "        \"show_output\": interface.show_output,",
            "        \"title\": interface.title,",
            "        \"analytics_enabled\": interface.analytics_enabled,",
            "        \"description\": interface.description,",
            "        \"simple_description\": interface.simple_description,",
            "        \"article\": interface.article,",
            "        \"theme\": interface.theme,",
            "        \"css\": interface.css,",
            "        \"thumbnail\": interface.thumbnail,",
            "        \"allow_screenshot\": interface.allow_screenshot,",
            "        \"allow_flagging\": interface.allow_flagging,",
            "        \"flagging_options\": interface.flagging_options,",
            "        \"allow_interpretation\": interface.interpretation is not None,",
            "        \"queue\": interface.enable_queue,",
            "        \"cached_examples\": interface.cache_examples",
            "        if hasattr(interface, \"cache_examples\")",
            "        else False,",
            "        \"version\": pkg_resources.require(\"gradio\")[0].version,",
            "        \"favicon_path\": interface.favicon_path,",
            "    }",
            "    try:",
            "        param_names = inspect.getfullargspec(interface.predict[0])[0]",
            "        for iface, param in zip(config[\"input_components\"], param_names):",
            "            if not iface[\"label\"]:",
            "                iface[\"label\"] = param.replace(\"_\", \" \")",
            "        for i, iface in enumerate(config[\"output_components\"]):",
            "            outputs_per_function = int(",
            "                len(interface.output_components) / len(interface.predict)",
            "            )",
            "            function_index = i // outputs_per_function",
            "            component_index = i - function_index * outputs_per_function",
            "            ret_name = (",
            "                \"Output \" + str(component_index + 1)",
            "                if outputs_per_function > 1",
            "                else \"Output\"",
            "            )",
            "            if iface[\"label\"] is None:",
            "                iface[\"label\"] = ret_name",
            "            if len(interface.predict) > 1:",
            "                iface[\"label\"] = (",
            "                    interface.function_names[function_index].replace(\"_\", \" \")",
            "                    + \": \"",
            "                    + iface[\"label\"]",
            "                )",
            "",
            "    except ValueError:",
            "        pass",
            "    if interface.examples is not None:",
            "        if isinstance(interface.examples, str):",
            "            if not os.path.exists(interface.examples):",
            "                raise FileNotFoundError(",
            "                    \"Could not find examples directory: \" + interface.examples",
            "                )",
            "            log_file = os.path.join(interface.examples, \"log.csv\")",
            "            if not os.path.exists(log_file):",
            "                if len(interface.input_components) == 1:",
            "                    examples = [",
            "                        [os.path.join(interface.examples, item)]",
            "                        for item in os.listdir(interface.examples)",
            "                    ]",
            "                else:",
            "                    raise FileNotFoundError(",
            "                        \"Could not find log file (required for multiple inputs): \"",
            "                        + log_file",
            "                    )",
            "            else:",
            "                with open(log_file) as logs:",
            "                    examples = list(csv.reader(logs))",
            "                    examples = examples[1:]  # remove header",
            "            for i, example in enumerate(examples):",
            "                for j, (component, cell) in enumerate(",
            "                    zip(",
            "                        interface.input_components + interface.output_components,",
            "                        example,",
            "                    )",
            "                ):",
            "                    examples[i][j] = component.restore_flagged(",
            "                        interface.flagging_dir,",
            "                        cell,",
            "                        interface.encryption_key if interface.encrypt else None,",
            "                    )",
            "            config[\"examples\"] = examples",
            "            config[\"examples_dir\"] = interface.examples",
            "        else:",
            "            config[\"examples\"] = interface.examples",
            "    return config",
            "",
            "",
            "def get_default_args(func: Callable) -> Dict[str, Any]:",
            "    signature = inspect.signature(func)",
            "    return [",
            "        v.default if v.default is not inspect.Parameter.empty else None",
            "        for v in signature.parameters.values()",
            "    ]",
            "",
            "",
            "def santize_for_csv(data: str | List[str] | List[List[str]]):",
            "    \"\"\"Sanitizes data so that it can be safely written to a CSV file.\"\"\"",
            "",
            "    def sanitize(item):",
            "        return \"'\" + item",
            "",
            "    unsafe_prefixes = (\"+\", \"=\", \"-\", \"@\")",
            "    warning_message = \"Sanitizing flagged data by escaping cell contents that begin \"",
            "    \"with one of the following characters: '+', '=', '-', '@'.\"",
            "",
            "    if isinstance(data, str):",
            "        if data.startswith(unsafe_prefixes):",
            "            warnings.warn(warning_message)",
            "            return sanitize(data)",
            "        return data",
            "    elif isinstance(data, list) and isinstance(data[0], str):",
            "        sanitized_data = copy.deepcopy(data)",
            "        for index, item in enumerate(data):",
            "            if item.startswith(unsafe_prefixes):",
            "                warnings.warn(warning_message)",
            "                sanitized_data[index] = sanitize(item)",
            "        return sanitized_data",
            "    elif isinstance(data[0], list) and isinstance(data[0][0], str):",
            "        sanitized_data = copy.deepcopy(data)",
            "        for outer_index, sublist in enumerate(data):",
            "            for inner_index, item in enumerate(sublist):",
            "                if item.startswith(unsafe_prefixes):",
            "                    warnings.warn(warning_message)",
            "                    sanitized_data[outer_index][inner_index] = sanitize(item)",
            "        return sanitized_data",
            "    else:",
            "        raise ValueError(\"Unsupported data type: \" + str(type(data)))"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "13": []
        },
        "addLocation": []
    }
}