{
    "gradio/blocks.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " import secrets"
            },
            "1": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " import string"
            },
            "2": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " import sys"
            },
            "3": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import tempfile"
            },
            "4": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " import threading"
            },
            "5": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " import time"
            },
            "6": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " import warnings"
            },
            "7": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 69,
                "PatchRowcode": "     get_cancel_function,"
            },
            "8": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 70,
                "PatchRowcode": "     get_continuous_fn,"
            },
            "9": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 71,
                "PatchRowcode": "     get_package_version,"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+    get_upload_folder,"
            },
            "11": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 73,
                "PatchRowcode": " )"
            },
            "12": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 74,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 75,
                "PatchRowcode": " try:"
            },
            "14": {
                "beforePatchRowNumber": 119,
                "afterPatchRowNumber": 119,
                "PatchRowcode": "         self._constructor_args: list[dict]"
            },
            "15": {
                "beforePatchRowNumber": 120,
                "afterPatchRowNumber": 120,
                "PatchRowcode": "         self.state_session_capacity = 10000"
            },
            "16": {
                "beforePatchRowNumber": 121,
                "afterPatchRowNumber": 121,
                "PatchRowcode": "         self.temp_files: set[str] = set()"
            },
            "17": {
                "beforePatchRowNumber": 122,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.GRADIO_CACHE = str("
            },
            "18": {
                "beforePatchRowNumber": 123,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            Path("
            },
            "19": {
                "beforePatchRowNumber": 124,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                os.environ.get(\"GRADIO_TEMP_DIR\")"
            },
            "20": {
                "beforePatchRowNumber": 125,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                or str(Path(tempfile.gettempdir()) / \"gradio\")"
            },
            "21": {
                "beforePatchRowNumber": 126,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            ).resolve()"
            },
            "22": {
                "beforePatchRowNumber": 127,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        )"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 122,
                "PatchRowcode": "+        self.GRADIO_CACHE = get_upload_folder()"
            },
            "24": {
                "beforePatchRowNumber": 128,
                "afterPatchRowNumber": 123,
                "PatchRowcode": " "
            },
            "25": {
                "beforePatchRowNumber": 129,
                "afterPatchRowNumber": 124,
                "PatchRowcode": "         if render:"
            },
            "26": {
                "beforePatchRowNumber": 130,
                "afterPatchRowNumber": 125,
                "PatchRowcode": "             self.render()"
            },
            "27": {
                "beforePatchRowNumber": 1110,
                "afterPatchRowNumber": 1105,
                "PatchRowcode": "             inputs=processed_inputs,"
            },
            "28": {
                "beforePatchRowNumber": 1111,
                "afterPatchRowNumber": 1106,
                "PatchRowcode": "             request=None,"
            },
            "29": {
                "beforePatchRowNumber": 1112,
                "afterPatchRowNumber": 1107,
                "PatchRowcode": "             state={},"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1108,
                "PatchRowcode": "+            explicit_call=True,"
            },
            "31": {
                "beforePatchRowNumber": 1113,
                "afterPatchRowNumber": 1109,
                "PatchRowcode": "         )"
            },
            "32": {
                "beforePatchRowNumber": 1114,
                "afterPatchRowNumber": 1110,
                "PatchRowcode": "         outputs = outputs[\"data\"]"
            },
            "33": {
                "beforePatchRowNumber": 1115,
                "afterPatchRowNumber": 1111,
                "PatchRowcode": " "
            },
            "34": {
                "beforePatchRowNumber": 1299,
                "afterPatchRowNumber": 1295,
                "PatchRowcode": "             )"
            },
            "35": {
                "beforePatchRowNumber": 1300,
                "afterPatchRowNumber": 1296,
                "PatchRowcode": " "
            },
            "36": {
                "beforePatchRowNumber": 1301,
                "afterPatchRowNumber": 1297,
                "PatchRowcode": "     def preprocess_data("
            },
            "37": {
                "beforePatchRowNumber": 1302,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self, fn_index: int, inputs: list[Any], state: SessionState | None"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1298,
                "PatchRowcode": "+        self,"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1299,
                "PatchRowcode": "+        fn_index: int,"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1300,
                "PatchRowcode": "+        inputs: list[Any],"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1301,
                "PatchRowcode": "+        state: SessionState | None,"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1302,
                "PatchRowcode": "+        explicit_call: bool = False,"
            },
            "43": {
                "beforePatchRowNumber": 1303,
                "afterPatchRowNumber": 1303,
                "PatchRowcode": "     ):"
            },
            "44": {
                "beforePatchRowNumber": 1304,
                "afterPatchRowNumber": 1304,
                "PatchRowcode": "         state = state or SessionState(self)"
            },
            "45": {
                "beforePatchRowNumber": 1305,
                "afterPatchRowNumber": 1305,
                "PatchRowcode": "         block_fn = self.fns[fn_index]"
            },
            "46": {
                "beforePatchRowNumber": 1326,
                "afterPatchRowNumber": 1326,
                "PatchRowcode": "                     if input_id in state:"
            },
            "47": {
                "beforePatchRowNumber": 1327,
                "afterPatchRowNumber": 1327,
                "PatchRowcode": "                         block = state[input_id]"
            },
            "48": {
                "beforePatchRowNumber": 1328,
                "afterPatchRowNumber": 1328,
                "PatchRowcode": "                     inputs_cached = processing_utils.move_files_to_cache("
            },
            "49": {
                "beforePatchRowNumber": 1329,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        inputs[i], block, add_urls=True"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1329,
                "PatchRowcode": "+                        inputs[i],"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1330,
                "PatchRowcode": "+                        block,"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1331,
                "PatchRowcode": "+                        add_urls=True,"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1332,
                "PatchRowcode": "+                        check_in_upload_folder=not explicit_call,"
            },
            "54": {
                "beforePatchRowNumber": 1330,
                "afterPatchRowNumber": 1333,
                "PatchRowcode": "                     )"
            },
            "55": {
                "beforePatchRowNumber": 1331,
                "afterPatchRowNumber": 1334,
                "PatchRowcode": "                     if getattr(block, \"data_model\", None) and inputs_cached is not None:"
            },
            "56": {
                "beforePatchRowNumber": 1332,
                "afterPatchRowNumber": 1335,
                "PatchRowcode": "                         if issubclass(block.data_model, GradioModel):  # type: ignore"
            },
            "57": {
                "beforePatchRowNumber": 1522,
                "afterPatchRowNumber": 1525,
                "PatchRowcode": " "
            },
            "58": {
                "beforePatchRowNumber": 1523,
                "afterPatchRowNumber": 1526,
                "PatchRowcode": "         return data"
            },
            "59": {
                "beforePatchRowNumber": 1524,
                "afterPatchRowNumber": 1527,
                "PatchRowcode": " "
            },
            "60": {
                "beforePatchRowNumber": 1525,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def run_fn_batch(self, fn, batch, fn_index, state):"
            },
            "61": {
                "beforePatchRowNumber": 1526,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return [fn(fn_index, list(i), state) for i in zip(*batch)]"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1528,
                "PatchRowcode": "+    def run_fn_batch(self, fn, batch, fn_index, state, explicit_call=None):"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1529,
                "PatchRowcode": "+        output = []"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1530,
                "PatchRowcode": "+        for i in zip(*batch):"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1531,
                "PatchRowcode": "+            args = [fn_index, list(i), state]"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1532,
                "PatchRowcode": "+            if explicit_call is not None:"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1533,
                "PatchRowcode": "+                args.append(explicit_call)"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1534,
                "PatchRowcode": "+            output.append(fn(*args))"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1535,
                "PatchRowcode": "+        return output"
            },
            "70": {
                "beforePatchRowNumber": 1527,
                "afterPatchRowNumber": 1536,
                "PatchRowcode": " "
            },
            "71": {
                "beforePatchRowNumber": 1528,
                "afterPatchRowNumber": 1537,
                "PatchRowcode": "     async def process_api("
            },
            "72": {
                "beforePatchRowNumber": 1529,
                "afterPatchRowNumber": 1538,
                "PatchRowcode": "         self,"
            },
            "73": {
                "beforePatchRowNumber": 1536,
                "afterPatchRowNumber": 1545,
                "PatchRowcode": "         event_id: str | None = None,"
            },
            "74": {
                "beforePatchRowNumber": 1537,
                "afterPatchRowNumber": 1546,
                "PatchRowcode": "         event_data: EventData | None = None,"
            },
            "75": {
                "beforePatchRowNumber": 1538,
                "afterPatchRowNumber": 1547,
                "PatchRowcode": "         in_event_listener: bool = True,"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1548,
                "PatchRowcode": "+        explicit_call: bool = False,"
            },
            "77": {
                "beforePatchRowNumber": 1539,
                "afterPatchRowNumber": 1549,
                "PatchRowcode": "     ) -> dict[str, Any]:"
            },
            "78": {
                "beforePatchRowNumber": 1540,
                "afterPatchRowNumber": 1550,
                "PatchRowcode": "         \"\"\""
            },
            "79": {
                "beforePatchRowNumber": 1541,
                "afterPatchRowNumber": 1551,
                "PatchRowcode": "         Processes API calls from the frontend. First preprocesses the data,"
            },
            "80": {
                "beforePatchRowNumber": 1548,
                "afterPatchRowNumber": 1558,
                "PatchRowcode": "             iterators: the in-progress iterators for each generator function (key is function index)"
            },
            "81": {
                "beforePatchRowNumber": 1549,
                "afterPatchRowNumber": 1559,
                "PatchRowcode": "             event_id: id of event that triggered this API call"
            },
            "82": {
                "beforePatchRowNumber": 1550,
                "afterPatchRowNumber": 1560,
                "PatchRowcode": "             event_data: data associated with the event trigger itself"
            },
            "83": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1561,
                "PatchRowcode": "+            in_event_listener: whether this API call is being made in response to an event listener"
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1562,
                "PatchRowcode": "+            explicit_call: whether this call is being made directly by calling the Blocks function, instead of through an event listener or API route"
            },
            "85": {
                "beforePatchRowNumber": 1551,
                "afterPatchRowNumber": 1563,
                "PatchRowcode": "         Returns: None"
            },
            "86": {
                "beforePatchRowNumber": 1552,
                "afterPatchRowNumber": 1564,
                "PatchRowcode": "         \"\"\""
            },
            "87": {
                "beforePatchRowNumber": 1553,
                "afterPatchRowNumber": 1565,
                "PatchRowcode": "         block_fn = self.fns[fn_index]"
            },
            "88": {
                "beforePatchRowNumber": 1575,
                "afterPatchRowNumber": 1587,
                "PatchRowcode": "                 inputs,"
            },
            "89": {
                "beforePatchRowNumber": 1576,
                "afterPatchRowNumber": 1588,
                "PatchRowcode": "                 fn_index,"
            },
            "90": {
                "beforePatchRowNumber": 1577,
                "afterPatchRowNumber": 1589,
                "PatchRowcode": "                 state,"
            },
            "91": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1590,
                "PatchRowcode": "+                explicit_call,"
            },
            "92": {
                "beforePatchRowNumber": 1578,
                "afterPatchRowNumber": 1591,
                "PatchRowcode": "                 limiter=self.limiter,"
            },
            "93": {
                "beforePatchRowNumber": 1579,
                "afterPatchRowNumber": 1592,
                "PatchRowcode": "             )"
            },
            "94": {
                "beforePatchRowNumber": 1580,
                "afterPatchRowNumber": 1593,
                "PatchRowcode": "             result = await self.call_function("
            },
            "95": {
                "beforePatchRowNumber": 1603,
                "afterPatchRowNumber": 1616,
                "PatchRowcode": "                 inputs = []"
            },
            "96": {
                "beforePatchRowNumber": 1604,
                "afterPatchRowNumber": 1617,
                "PatchRowcode": "             else:"
            },
            "97": {
                "beforePatchRowNumber": 1605,
                "afterPatchRowNumber": 1618,
                "PatchRowcode": "                 inputs = await anyio.to_thread.run_sync("
            },
            "98": {
                "beforePatchRowNumber": 1606,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    self.preprocess_data, fn_index, inputs, state, limiter=self.limiter"
            },
            "99": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1619,
                "PatchRowcode": "+                    self.preprocess_data,"
            },
            "100": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1620,
                "PatchRowcode": "+                    fn_index,"
            },
            "101": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1621,
                "PatchRowcode": "+                    inputs,"
            },
            "102": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1622,
                "PatchRowcode": "+                    state,"
            },
            "103": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1623,
                "PatchRowcode": "+                    explicit_call,"
            },
            "104": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1624,
                "PatchRowcode": "+                    limiter=self.limiter,"
            },
            "105": {
                "beforePatchRowNumber": 1607,
                "afterPatchRowNumber": 1625,
                "PatchRowcode": "                 )"
            },
            "106": {
                "beforePatchRowNumber": 1608,
                "afterPatchRowNumber": 1626,
                "PatchRowcode": "             was_generating = old_iterator is not None"
            },
            "107": {
                "beforePatchRowNumber": 1609,
                "afterPatchRowNumber": 1627,
                "PatchRowcode": "             result = await self.call_function("
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import copy",
            "import hashlib",
            "import inspect",
            "import json",
            "import os",
            "import random",
            "import secrets",
            "import string",
            "import sys",
            "import tempfile",
            "import threading",
            "import time",
            "import warnings",
            "import webbrowser",
            "from collections import defaultdict",
            "from pathlib import Path",
            "from types import ModuleType",
            "from typing import TYPE_CHECKING, Any, AsyncIterator, Callable, Literal, Sequence, cast",
            "from urllib.parse import urlparse, urlunparse",
            "",
            "import anyio",
            "import httpx",
            "from anyio import CapacityLimiter",
            "from gradio_client import utils as client_utils",
            "from gradio_client.documentation import document",
            "",
            "from gradio import (",
            "    analytics,",
            "    components,",
            "    networking,",
            "    processing_utils,",
            "    queueing,",
            "    routes,",
            "    strings,",
            "    themes,",
            "    utils,",
            "    wasm_utils,",
            ")",
            "from gradio.blocks_events import BlocksEvents, BlocksMeta",
            "from gradio.context import Context",
            "from gradio.data_classes import FileData, GradioModel, GradioRootModel",
            "from gradio.events import (",
            "    EventData,",
            "    EventListener,",
            "    EventListenerMethod,",
            ")",
            "from gradio.exceptions import (",
            "    DuplicateBlockError,",
            "    InvalidApiNameError,",
            "    InvalidBlockError,",
            "    InvalidComponentError,",
            ")",
            "from gradio.helpers import create_tracker, skip, special_args",
            "from gradio.state_holder import SessionState",
            "from gradio.themes import Default as DefaultTheme",
            "from gradio.themes import ThemeClass as Theme",
            "from gradio.tunneling import (",
            "    BINARY_FILENAME,",
            "    BINARY_FOLDER,",
            "    BINARY_PATH,",
            "    BINARY_URL,",
            "    CURRENT_TUNNELS,",
            ")",
            "from gradio.utils import (",
            "    TupleNoPrint,",
            "    check_function_inputs_match,",
            "    component_or_layout_class,",
            "    get_cancel_function,",
            "    get_continuous_fn,",
            "    get_package_version,",
            ")",
            "",
            "try:",
            "    import spaces  # type: ignore",
            "except Exception:",
            "    spaces = None",
            "",
            "",
            "if TYPE_CHECKING:  # Only import for type checking (is False at runtime).",
            "    from fastapi.applications import FastAPI",
            "",
            "    from gradio.components.base import Component",
            "",
            "BUILT_IN_THEMES: dict[str, Theme] = {",
            "    t.name: t",
            "    for t in [",
            "        themes.Base(),",
            "        themes.Default(),",
            "        themes.Monochrome(),",
            "        themes.Soft(),",
            "        themes.Glass(),",
            "    ]",
            "}",
            "",
            "",
            "class Block:",
            "    def __init__(",
            "        self,",
            "        *,",
            "        elem_id: str | None = None,",
            "        elem_classes: list[str] | str | None = None,",
            "        render: bool = True,",
            "        visible: bool = True,",
            "        proxy_url: str | None = None,",
            "    ):",
            "        self._id = Context.id",
            "        Context.id += 1",
            "        self.visible = visible",
            "        self.elem_id = elem_id",
            "        self.elem_classes = (",
            "            [elem_classes] if isinstance(elem_classes, str) else elem_classes",
            "        )",
            "        self.proxy_url = proxy_url",
            "        self.share_token = secrets.token_urlsafe(32)",
            "        self.parent: BlockContext | None = None",
            "        self.is_rendered: bool = False",
            "        self._constructor_args: list[dict]",
            "        self.state_session_capacity = 10000",
            "        self.temp_files: set[str] = set()",
            "        self.GRADIO_CACHE = str(",
            "            Path(",
            "                os.environ.get(\"GRADIO_TEMP_DIR\")",
            "                or str(Path(tempfile.gettempdir()) / \"gradio\")",
            "            ).resolve()",
            "        )",
            "",
            "        if render:",
            "            self.render()",
            "",
            "    @property",
            "    def skip_api(self):",
            "        return False",
            "",
            "    @property",
            "    def constructor_args(self) -> dict[str, Any]:",
            "        \"\"\"Get the arguments passed to the component's initializer.",
            "",
            "        Only set classes whose metaclass is ComponentMeta",
            "        \"\"\"",
            "        # the _constructor_args list is appended based on the mro of the class",
            "        # so the first entry is for the bottom of the hierarchy",
            "        return self._constructor_args[0] if self._constructor_args else {}",
            "",
            "    @property",
            "    def events(",
            "        self,",
            "    ) -> list[EventListener]:",
            "        return getattr(self, \"EVENTS\", [])",
            "",
            "    def render(self):",
            "        \"\"\"",
            "        Adds self into appropriate BlockContext",
            "        \"\"\"",
            "        if Context.root_block is not None and self._id in Context.root_block.blocks:",
            "            raise DuplicateBlockError(",
            "                f\"A block with id: {self._id} has already been rendered in the current Blocks.\"",
            "            )",
            "        if Context.block is not None:",
            "            Context.block.add(self)",
            "        if Context.root_block is not None:",
            "            Context.root_block.blocks[self._id] = self",
            "            self.is_rendered = True",
            "            if isinstance(self, components.Component):",
            "                Context.root_block.temp_file_sets.append(self.temp_files)",
            "        return self",
            "",
            "    def unrender(self):",
            "        \"\"\"",
            "        Removes self from BlockContext if it has been rendered (otherwise does nothing).",
            "        Removes self from the layout and collection of blocks, but does not delete any event triggers.",
            "        \"\"\"",
            "        if Context.block is not None:",
            "            try:",
            "                Context.block.children.remove(self)",
            "            except ValueError:",
            "                pass",
            "        if Context.root_block is not None:",
            "            try:",
            "                del Context.root_block.blocks[self._id]",
            "                self.is_rendered = False",
            "            except KeyError:",
            "                pass",
            "        return self",
            "",
            "    def get_block_name(self) -> str:",
            "        \"\"\"",
            "        Gets block's class name.",
            "",
            "        If it is template component it gets the parent's class name.",
            "",
            "        @return: class name",
            "        \"\"\"",
            "        return (",
            "            self.__class__.__base__.__name__.lower()",
            "            if hasattr(self, \"is_template\")",
            "            else self.__class__.__name__.lower()",
            "        )",
            "",
            "    def get_expected_parent(self) -> type[BlockContext] | None:",
            "        return None",
            "",
            "    def get_config(self):",
            "        config = {}",
            "        signature = inspect.signature(self.__class__.__init__)",
            "        for parameter in signature.parameters.values():",
            "            if hasattr(self, parameter.name):",
            "                value = getattr(self, parameter.name)",
            "                config[parameter.name] = utils.convert_to_dict_if_dataclass(value)",
            "        for e in self.events:",
            "            to_add = e.config_data()",
            "            if to_add:",
            "                config = {**to_add, **config}",
            "        config.pop(\"render\", None)",
            "        config = {**config, \"proxy_url\": self.proxy_url, \"name\": self.get_block_name()}",
            "        if (_selectable := getattr(self, \"_selectable\", None)) is not None:",
            "            config[\"_selectable\"] = _selectable",
            "        return config",
            "",
            "    @classmethod",
            "    def recover_kwargs(",
            "        cls, props: dict[str, Any], additional_keys: list[str] | None = None",
            "    ):",
            "        \"\"\"",
            "        Recovers kwargs from a dict of props.",
            "        \"\"\"",
            "        additional_keys = additional_keys or []",
            "        signature = inspect.signature(cls.__init__)",
            "        kwargs = {}",
            "        for parameter in signature.parameters.values():",
            "            if parameter.name in props and parameter.name not in additional_keys:",
            "                kwargs[parameter.name] = props[parameter.name]",
            "        return kwargs",
            "",
            "    def move_resource_to_block_cache(",
            "        self, url_or_file_path: str | Path | None",
            "    ) -> str | None:",
            "        \"\"\"Moves a file or downloads a file from a url to a block's cache directory, adds",
            "        to to the block's temp_files, and returns the path to the file in cache. This",
            "        ensures that the file is accessible to the Block and can be served to users.",
            "        \"\"\"",
            "        if url_or_file_path is None:",
            "            return None",
            "        if isinstance(url_or_file_path, Path):",
            "            url_or_file_path = str(url_or_file_path)",
            "",
            "        if client_utils.is_http_url_like(url_or_file_path):",
            "            temp_file_path = processing_utils.save_url_to_cache(",
            "                url_or_file_path, cache_dir=self.GRADIO_CACHE",
            "            )",
            "",
            "            self.temp_files.add(temp_file_path)",
            "        else:",
            "            url_or_file_path = str(utils.abspath(url_or_file_path))",
            "            if not utils.is_in_or_equal(url_or_file_path, self.GRADIO_CACHE):",
            "                temp_file_path = processing_utils.save_file_to_cache(",
            "                    url_or_file_path, cache_dir=self.GRADIO_CACHE",
            "                )",
            "            else:",
            "                temp_file_path = url_or_file_path",
            "            self.temp_files.add(temp_file_path)",
            "",
            "        return temp_file_path",
            "",
            "",
            "class BlockContext(Block):",
            "    def __init__(",
            "        self,",
            "        elem_id: str | None = None,",
            "        elem_classes: list[str] | str | None = None,",
            "        visible: bool = True,",
            "        render: bool = True,",
            "    ):",
            "        \"\"\"",
            "        Parameters:",
            "            elem_id: An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",
            "            elem_classes: An optional string or list of strings that are assigned as the class of this component in the HTML DOM. Can be used for targeting CSS styles.",
            "            visible: If False, this will be hidden but included in the Blocks config file (its visibility can later be updated).",
            "            render: If False, this will not be included in the Blocks config file at all.",
            "        \"\"\"",
            "        self.children: list[Block] = []",
            "        Block.__init__(",
            "            self,",
            "            elem_id=elem_id,",
            "            elem_classes=elem_classes,",
            "            visible=visible,",
            "            render=render,",
            "        )",
            "",
            "    TEMPLATE_DIR = \"./templates/\"",
            "    FRONTEND_DIR = \"../../frontend/\"",
            "",
            "    @property",
            "    def skip_api(self):",
            "        return True",
            "",
            "    @classmethod",
            "    def get_component_class_id(cls) -> str:",
            "        module_name = cls.__module__",
            "        module_path = sys.modules[module_name].__file__",
            "        module_hash = hashlib.md5(f\"{cls.__name__}_{module_path}\".encode()).hexdigest()",
            "        return module_hash",
            "",
            "    @property",
            "    def component_class_id(self):",
            "        return self.get_component_class_id()",
            "",
            "    def add_child(self, child: Block):",
            "        self.children.append(child)",
            "",
            "    def __enter__(self):",
            "        self.parent = Context.block",
            "        Context.block = self",
            "        return self",
            "",
            "    def add(self, child: Block):",
            "        child.parent = self",
            "        self.children.append(child)",
            "",
            "    def fill_expected_parents(self):",
            "        children = []",
            "        pseudo_parent = None",
            "        for child in self.children:",
            "            expected_parent = child.get_expected_parent()",
            "            if not expected_parent or isinstance(self, expected_parent):",
            "                pseudo_parent = None",
            "                children.append(child)",
            "            else:",
            "                if pseudo_parent is not None and isinstance(",
            "                    pseudo_parent, expected_parent",
            "                ):",
            "                    pseudo_parent.add_child(child)",
            "                else:",
            "                    pseudo_parent = expected_parent(render=False)",
            "                    pseudo_parent.parent = self",
            "                    children.append(pseudo_parent)",
            "                    pseudo_parent.add_child(child)",
            "                    if Context.root_block:",
            "                        Context.root_block.blocks[pseudo_parent._id] = pseudo_parent",
            "                child.parent = pseudo_parent",
            "        self.children = children",
            "",
            "    def __exit__(self, exc_type: type[BaseException] | None = None, *args):",
            "        Context.block = self.parent",
            "        if exc_type is not None:",
            "            return",
            "        if getattr(self, \"allow_expected_parents\", True):",
            "            self.fill_expected_parents()",
            "",
            "    def postprocess(self, y):",
            "        \"\"\"",
            "        Any postprocessing needed to be performed on a block context.",
            "        \"\"\"",
            "        return y",
            "",
            "",
            "class BlockFunction:",
            "    def __init__(",
            "        self,",
            "        fn: Callable | None,",
            "        inputs: list[Component],",
            "        outputs: list[Component],",
            "        preprocess: bool,",
            "        postprocess: bool,",
            "        inputs_as_dict: bool,",
            "        batch: bool = False,",
            "        max_batch_size: int = 4,",
            "        concurrency_limit: int | None | Literal[\"default\"] = \"default\",",
            "        concurrency_id: str | None = None,",
            "        tracks_progress: bool = False,",
            "    ):",
            "        self.fn = fn",
            "        self.inputs = inputs",
            "        self.outputs = outputs",
            "        self.preprocess = preprocess",
            "        self.postprocess = postprocess",
            "        self.tracks_progress = tracks_progress",
            "        self.concurrency_limit: int | None | Literal[\"default\"] = concurrency_limit",
            "        self.concurrency_id = concurrency_id or str(id(fn))",
            "        self.batch = batch",
            "        self.max_batch_size = max_batch_size",
            "        self.total_runtime = 0",
            "        self.total_runs = 0",
            "        self.inputs_as_dict = inputs_as_dict",
            "        self.name = getattr(fn, \"__name__\", \"fn\") if fn is not None else None",
            "        self.spaces_auto_wrap()",
            "",
            "    def spaces_auto_wrap(self):",
            "        if spaces is None:",
            "            return",
            "        if utils.get_space() is None:",
            "            return",
            "        self.fn = spaces.gradio_auto_wrap(self.fn)",
            "",
            "    def __str__(self):",
            "        return str(",
            "            {",
            "                \"fn\": self.name,",
            "                \"preprocess\": self.preprocess,",
            "                \"postprocess\": self.postprocess,",
            "            }",
            "        )",
            "",
            "    def __repr__(self):",
            "        return str(self)",
            "",
            "",
            "def postprocess_update_dict(",
            "    block: Component | BlockContext, update_dict: dict, postprocess: bool = True",
            "):",
            "    \"\"\"",
            "    Converts a dictionary of updates into a format that can be sent to the frontend to update the component.",
            "    E.g. {\"value\": \"2\", \"visible\": True, \"invalid_arg\": \"hello\"}",
            "    Into -> {\"__type__\": \"update\", \"value\": 2.0, \"visible\": True}",
            "    Parameters:",
            "        block: The Block that is being updated with this update dictionary.",
            "        update_dict: The original update dictionary",
            "        postprocess: Whether to postprocess the \"value\" key of the update dictionary.",
            "    \"\"\"",
            "    value = update_dict.pop(\"value\", components._Keywords.NO_VALUE)",
            "    update_dict = {k: getattr(block, k) for k in update_dict if hasattr(block, k)}",
            "    if value is not components._Keywords.NO_VALUE:",
            "        if postprocess:",
            "            update_dict[\"value\"] = block.postprocess(value)",
            "            if isinstance(update_dict[\"value\"], (GradioModel, GradioRootModel)):",
            "                update_dict[\"value\"] = update_dict[\"value\"].model_dump()",
            "        else:",
            "            update_dict[\"value\"] = value",
            "    update_dict[\"__type__\"] = \"update\"",
            "    return update_dict",
            "",
            "",
            "def convert_component_dict_to_list(",
            "    outputs_ids: list[int], predictions: dict",
            ") -> list | dict:",
            "    \"\"\"",
            "    Converts a dictionary of component updates into a list of updates in the order of",
            "    the outputs_ids and including every output component. Leaves other types of dictionaries unchanged.",
            "    E.g. {\"textbox\": \"hello\", \"number\": {\"__type__\": \"generic_update\", \"value\": \"2\"}}",
            "    Into -> [\"hello\", {\"__type__\": \"generic_update\"}, {\"__type__\": \"generic_update\", \"value\": \"2\"}]",
            "    \"\"\"",
            "    keys_are_blocks = [isinstance(key, Block) for key in predictions]",
            "    if all(keys_are_blocks):",
            "        reordered_predictions = [skip() for _ in outputs_ids]",
            "        for component, value in predictions.items():",
            "            if component._id not in outputs_ids:",
            "                raise ValueError(",
            "                    f\"Returned component {component} not specified as output of function.\"",
            "                )",
            "            output_index = outputs_ids.index(component._id)",
            "            reordered_predictions[output_index] = value",
            "        predictions = utils.resolve_singleton(reordered_predictions)",
            "    elif any(keys_are_blocks):",
            "        raise ValueError(",
            "            \"Returned dictionary included some keys as Components. Either all keys must be Components to assign Component values, or return a List of values to assign output values in order.\"",
            "        )",
            "    return predictions",
            "",
            "",
            "@document(\"launch\", \"queue\", \"integrate\", \"load\")",
            "class Blocks(BlockContext, BlocksEvents, metaclass=BlocksMeta):",
            "    \"\"\"",
            "    Blocks is Gradio's low-level API that allows you to create more custom web",
            "    applications and demos than Interfaces (yet still entirely in Python).",
            "",
            "",
            "    Compared to the Interface class, Blocks offers more flexibility and control over:",
            "    (1) the layout of components (2) the events that",
            "    trigger the execution of functions (3) data flows (e.g. inputs can trigger outputs,",
            "    which can trigger the next level of outputs). Blocks also offers ways to group",
            "    together related demos such as with tabs.",
            "",
            "",
            "    The basic usage of Blocks is as follows: create a Blocks object, then use it as a",
            "    context (with the \"with\" statement), and then define layouts, components, or events",
            "    within the Blocks context. Finally, call the launch() method to launch the demo.",
            "",
            "    Example:",
            "        import gradio as gr",
            "        def update(name):",
            "            return f\"Welcome to Gradio, {name}!\"",
            "",
            "        with gr.Blocks() as demo:",
            "            gr.Markdown(\"Start typing below and then click **Run** to see the output.\")",
            "            with gr.Row():",
            "                inp = gr.Textbox(placeholder=\"What is your name?\")",
            "                out = gr.Textbox()",
            "            btn = gr.Button(\"Run\")",
            "            btn.click(fn=update, inputs=inp, outputs=out)",
            "",
            "        demo.launch()",
            "    Demos: blocks_hello, blocks_flipper, blocks_speech_text_sentiment, generate_english_german",
            "    Guides: blocks-and-event-listeners, controlling-layout, state-in-blocks, custom-CSS-and-JS, using-blocks-like-functions",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        theme: Theme | str | None = None,",
            "        analytics_enabled: bool | None = None,",
            "        mode: str = \"blocks\",",
            "        title: str = \"Gradio\",",
            "        css: str | None = None,",
            "        js: str | None = None,",
            "        head: str | None = None,",
            "        fill_height: bool = False,",
            "        **kwargs,",
            "    ):",
            "        \"\"\"",
            "        Parameters:",
            "            theme: A Theme object or a string representing a theme. If a string, will look for a built-in theme with that name (e.g. \"soft\" or \"default\"), or will attempt to load a theme from the Hugging Face Hub (e.g. \"gradio/monochrome\"). If None, will use the Default theme.",
            "            analytics_enabled: Whether to allow basic telemetry. If None, will use GRADIO_ANALYTICS_ENABLED environment variable or default to True.",
            "            mode: A human-friendly name for the kind of Blocks or Interface being created. Used internally for analytics.",
            "            title: The tab title to display when this is opened in a browser window.",
            "            css: Custom css as a string or path to a css file. This css will be included in the demo webpage.",
            "            js: Custom js or path to js file to run when demo is first loaded. This javascript will be included in the demo webpage.",
            "            head: Custom html to insert into the head of the demo webpage. This can be used to add custom meta tags, scripts, stylesheets, etc. to the page.",
            "            fill_height: Whether to vertically expand top-level child components to the height of the window. If True, expansion occurs when the scale value of the child components >= 1.",
            "        \"\"\"",
            "        self.limiter = None",
            "        if theme is None:",
            "            theme = DefaultTheme()",
            "        elif isinstance(theme, str):",
            "            if theme.lower() in BUILT_IN_THEMES:",
            "                theme = BUILT_IN_THEMES[theme.lower()]",
            "            else:",
            "                try:",
            "                    theme = Theme.from_hub(theme)",
            "                except Exception as e:",
            "                    warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")",
            "                    theme = DefaultTheme()",
            "        if not isinstance(theme, Theme):",
            "            warnings.warn(\"Theme should be a class loaded from gradio.themes\")",
            "            theme = DefaultTheme()",
            "        self.theme: Theme = theme",
            "        self.theme_css = theme._get_theme_css()",
            "        self.stylesheets = theme._stylesheets",
            "        self.encrypt = False",
            "        self.share = False",
            "        self.enable_queue = True",
            "        self.max_threads = 40",
            "        self.pending_streams = defaultdict(dict)",
            "        self.pending_diff_streams = defaultdict(dict)",
            "        self.show_error = True",
            "        self.head = head",
            "        self.fill_height = fill_height",
            "        if css is not None and os.path.exists(css):",
            "            with open(css) as css_file:",
            "                self.css = css_file.read()",
            "        else:",
            "            self.css = css",
            "        if js is not None and os.path.exists(js):",
            "            with open(js) as js_file:",
            "                self.js = js_file.read()",
            "        else:",
            "            self.js = js",
            "",
            "        # For analytics_enabled and allow_flagging: (1) first check for",
            "        # parameter, (2) check for env variable, (3) default to True/\"manual\"",
            "        self.analytics_enabled = (",
            "            analytics_enabled",
            "            if analytics_enabled is not None",
            "            else analytics.analytics_enabled()",
            "        )",
            "        if self.analytics_enabled:",
            "            if not wasm_utils.IS_WASM:",
            "                t = threading.Thread(target=analytics.version_check)",
            "                t.start()",
            "        else:",
            "            os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"True\"",
            "        super().__init__(render=False, **kwargs)",
            "        self.blocks: dict[int, Component | Block] = {}",
            "        self.fns: list[BlockFunction] = []",
            "        self.dependencies = []",
            "        self.mode = mode",
            "",
            "        self.is_running = False",
            "        self.local_url = None",
            "        self.share_url = None",
            "        self.width = None",
            "        self.height = None",
            "        self.api_open = utils.get_space() is None",
            "",
            "        self.space_id = utils.get_space()",
            "        self.favicon_path = None",
            "        self.auth = None",
            "        self.dev_mode = bool(os.getenv(\"GRADIO_WATCH_DIRS\", \"\"))",
            "        self.app_id = random.getrandbits(64)",
            "        self.temp_file_sets = []",
            "        self.title = title",
            "        self.show_api = not wasm_utils.IS_WASM",
            "",
            "        # Only used when an Interface is loaded from a config",
            "        self.predict = None",
            "        self.input_components = None",
            "        self.output_components = None",
            "        self.__name__ = None",
            "        self.api_mode = None",
            "",
            "        self.progress_tracking = None",
            "        self.ssl_verify = True",
            "",
            "        self.allowed_paths = []",
            "        self.blocked_paths = []",
            "        self.root_path = os.environ.get(\"GRADIO_ROOT_PATH\", \"\")",
            "        self.proxy_urls = set()",
            "",
            "        if self.analytics_enabled:",
            "            is_custom_theme = not any(",
            "                self.theme.to_dict() == built_in_theme.to_dict()",
            "                for built_in_theme in BUILT_IN_THEMES.values()",
            "            )",
            "            data = {",
            "                \"mode\": self.mode,",
            "                \"custom_css\": self.css is not None,",
            "                \"theme\": self.theme.name,",
            "                \"is_custom_theme\": is_custom_theme,",
            "                \"version\": get_package_version(),",
            "            }",
            "            analytics.initiated_analytics(data)",
            "",
            "        self.queue()",
            "",
            "    def get_component(self, id: int) -> Component | BlockContext:",
            "        comp = self.blocks[id]",
            "        if not isinstance(comp, (components.Component, BlockContext)):",
            "            raise TypeError(f\"Block with id {id} is not a Component or BlockContext\")",
            "        return comp",
            "",
            "    @property",
            "    def _is_running_in_reload_thread(self):",
            "        from gradio.cli.commands.reload import reload_thread",
            "",
            "        return getattr(reload_thread, \"running_reload\", False)",
            "",
            "    @classmethod",
            "    def from_config(",
            "        cls,",
            "        config: dict,",
            "        fns: list[Callable],",
            "        proxy_url: str,",
            "    ) -> Blocks:",
            "        \"\"\"",
            "        Factory method that creates a Blocks from a config and list of functions. Used",
            "        internally by the gradio.external.load() method.",
            "",
            "        Parameters:",
            "        config: a dictionary containing the configuration of the Blocks.",
            "        fns: a list of functions that are used in the Blocks. Must be in the same order as the dependencies in the config.",
            "        proxy_url: an external url to use as a root URL when serving files for components in the Blocks.",
            "        \"\"\"",
            "        config = copy.deepcopy(config)",
            "        components_config = config[\"components\"]",
            "        theme = config.get(\"theme\", \"default\")",
            "        original_mapping: dict[int, Block] = {}",
            "        proxy_urls = {proxy_url}",
            "",
            "        def get_block_instance(id: int) -> Block:",
            "            for block_config in components_config:",
            "                if block_config[\"id\"] == id:",
            "                    break",
            "            else:",
            "                raise ValueError(f\"Cannot find block with id {id}\")",
            "            cls = component_or_layout_class(block_config[\"type\"])",
            "",
            "            # If a Gradio app B is loaded into a Gradio app A, and B itself loads a",
            "            # Gradio app C, then the proxy_urls of the components in A need to be the",
            "            # URL of C, not B. The else clause below handles this case.",
            "            if block_config[\"props\"].get(\"proxy_url\") is None:",
            "                block_config[\"props\"][\"proxy_url\"] = f\"{proxy_url}/\"",
            "            postprocessed_value = block_config[\"props\"].pop(\"value\", None)",
            "",
            "            constructor_args = cls.recover_kwargs(block_config[\"props\"])",
            "            block = cls(**constructor_args)",
            "            if postprocessed_value is not None:",
            "                block.value = postprocessed_value  # type: ignore",
            "",
            "            block_proxy_url = block_config[\"props\"][\"proxy_url\"]",
            "            block.proxy_url = block_proxy_url",
            "            proxy_urls.add(block_proxy_url)",
            "            if (",
            "                _selectable := block_config[\"props\"].pop(\"_selectable\", None)",
            "            ) is not None:",
            "                block._selectable = _selectable  # type: ignore",
            "",
            "            return block",
            "",
            "        def iterate_over_children(children_list):",
            "            for child_config in children_list:",
            "                id = child_config[\"id\"]",
            "                block = get_block_instance(id)",
            "",
            "                original_mapping[id] = block",
            "",
            "                children = child_config.get(\"children\")",
            "                if children is not None:",
            "                    if not isinstance(block, BlockContext):",
            "                        raise ValueError(",
            "                            f\"Invalid config, Block with id {id} has children but is not a BlockContext.\"",
            "                        )",
            "                    with block:",
            "                        iterate_over_children(children)",
            "",
            "        derived_fields = [\"types\"]",
            "",
            "        with Blocks(theme=theme) as blocks:",
            "            # ID 0 should be the root Blocks component",
            "            original_mapping[0] = Context.root_block or blocks",
            "",
            "            iterate_over_children(config[\"layout\"][\"children\"])",
            "",
            "            first_dependency = None",
            "",
            "            # add the event triggers",
            "            for dependency, fn in zip(config[\"dependencies\"], fns):",
            "                # We used to add a \"fake_event\" to the config to cache examples",
            "                # without removing it. This was causing bugs in calling gr.load",
            "                # We fixed the issue by removing \"fake_event\" from the config in examples.py",
            "                # but we still need to skip these events when loading the config to support",
            "                # older demos",
            "                if \"trigger\" in dependency and dependency[\"trigger\"] == \"fake_event\":",
            "                    continue",
            "                for field in derived_fields:",
            "                    dependency.pop(field, None)",
            "",
            "                # older versions had a separate trigger field, but now it is part of the",
            "                # targets field",
            "                _targets = dependency.pop(\"targets\")",
            "                trigger = dependency.pop(\"trigger\", None)",
            "                targets = [",
            "                    getattr(",
            "                        original_mapping[",
            "                            target if isinstance(target, int) else target[0]",
            "                        ],",
            "                        trigger if isinstance(target, int) else target[1],",
            "                    )",
            "                    for target in _targets",
            "                ]",
            "                dependency.pop(\"backend_fn\")",
            "                dependency.pop(\"documentation\", None)",
            "                dependency[\"inputs\"] = [",
            "                    original_mapping[i] for i in dependency[\"inputs\"]",
            "                ]",
            "                dependency[\"outputs\"] = [",
            "                    original_mapping[o] for o in dependency[\"outputs\"]",
            "                ]",
            "                dependency.pop(\"status_tracker\", None)",
            "                dependency[\"preprocess\"] = False",
            "                dependency[\"postprocess\"] = False",
            "                targets = [",
            "                    EventListenerMethod(",
            "                        t.__self__ if t.has_trigger else None, t.event_name",
            "                    )",
            "                    for t in targets",
            "                ]",
            "                dependency = blocks.set_event_trigger(",
            "                    targets=targets, fn=fn, **dependency",
            "                )[0]",
            "                if first_dependency is None:",
            "                    first_dependency = dependency",
            "",
            "            # Allows some use of Interface-specific methods with loaded Spaces",
            "            if first_dependency and Context.root_block:",
            "                blocks.predict = [fns[0]]",
            "                blocks.input_components = [",
            "                    Context.root_block.blocks[i] for i in first_dependency[\"inputs\"]",
            "                ]",
            "                blocks.output_components = [",
            "                    Context.root_block.blocks[o] for o in first_dependency[\"outputs\"]",
            "                ]",
            "                blocks.__name__ = \"Interface\"",
            "                blocks.api_mode = True",
            "        blocks.proxy_urls = proxy_urls",
            "        return blocks",
            "",
            "    def __str__(self):",
            "        return self.__repr__()",
            "",
            "    def __repr__(self):",
            "        num_backend_fns = len([d for d in self.dependencies if d[\"backend_fn\"]])",
            "        repr = f\"Gradio Blocks instance: {num_backend_fns} backend functions\"",
            "        repr += f\"\\n{'-' * len(repr)}\"",
            "        for d, dependency in enumerate(self.dependencies):",
            "            if dependency[\"backend_fn\"]:",
            "                repr += f\"\\nfn_index={d}\"",
            "                repr += \"\\n inputs:\"",
            "                for input_id in dependency[\"inputs\"]:",
            "                    block = self.blocks[input_id]",
            "                    repr += f\"\\n |-{block}\"",
            "                repr += \"\\n outputs:\"",
            "                for output_id in dependency[\"outputs\"]:",
            "                    block = self.blocks[output_id]",
            "                    repr += f\"\\n |-{block}\"",
            "        return repr",
            "",
            "    @property",
            "    def expects_oauth(self):",
            "        \"\"\"Return whether the app expects user to authenticate via OAuth.\"\"\"",
            "        return any(",
            "            isinstance(block, (components.LoginButton, components.LogoutButton))",
            "            for block in self.blocks.values()",
            "        )",
            "",
            "    def set_event_trigger(",
            "        self,",
            "        targets: Sequence[EventListenerMethod],",
            "        fn: Callable | None,",
            "        inputs: Component | list[Component] | set[Component] | None,",
            "        outputs: Component | list[Component] | None,",
            "        preprocess: bool = True,",
            "        postprocess: bool = True,",
            "        scroll_to_output: bool = False,",
            "        show_progress: Literal[\"full\", \"minimal\", \"hidden\"] = \"full\",",
            "        api_name: str | None | Literal[False] = None,",
            "        js: str | None = None,",
            "        no_target: bool = False,",
            "        queue: bool | None = None,",
            "        batch: bool = False,",
            "        max_batch_size: int = 4,",
            "        cancels: list[int] | None = None,",
            "        every: float | None = None,",
            "        collects_event_data: bool | None = None,",
            "        trigger_after: int | None = None,",
            "        trigger_only_on_success: bool = False,",
            "        trigger_mode: Literal[\"once\", \"multiple\", \"always_last\"] | None = \"once\",",
            "        concurrency_limit: int | None | Literal[\"default\"] = \"default\",",
            "        concurrency_id: str | None = None,",
            "        show_api: bool = True,",
            "    ) -> tuple[dict[str, Any], int]:",
            "        \"\"\"",
            "        Adds an event to the component's dependencies.",
            "        Parameters:",
            "            targets: a list of EventListenerMethod objects that define the event trigger",
            "            fn: Callable function",
            "            inputs: input list",
            "            outputs: output list",
            "            preprocess: whether to run the preprocess methods of components",
            "            postprocess: whether to run the postprocess methods of components",
            "            scroll_to_output: whether to scroll to output of dependency on trigger",
            "            show_progress: whether to show progress animation while running.",
            "            api_name: defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",
            "            js: Optional frontend js method to run before running 'fn'. Input arguments for js method are values of 'inputs' and 'outputs', return should be a list of values for output components",
            "            no_target: if True, sets \"targets\" to [], used for Blocks \"load\" event",
            "            queue: If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",
            "            batch: whether this function takes in a batch of inputs",
            "            max_batch_size: the maximum batch size to send to the function",
            "            cancels: a list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.",
            "            every: Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds.",
            "            collects_event_data: whether to collect event data for this event",
            "            trigger_after: if set, this event will be triggered after 'trigger_after' function index",
            "            trigger_only_on_success: if True, this event will only be triggered if the previous event was successful (only applies if `trigger_after` is set)",
            "            trigger_mode: If \"once\" (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to \"multiple\", unlimited submissions are allowed while pending, and \"always_last\" (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",
            "            concurrency_limit: If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to \"default\" to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `queue()`, which itself is 1 by default).",
            "            concurrency_id: If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",
            "            show_api: whether to show this event in the \"view API\" page of the Gradio app, or in the \".view_api()\" method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps to use this event. If fn is None, show_api will automatically be set to False.",
            "        Returns: dependency information, dependency index",
            "        \"\"\"",
            "        # Support for singular parameter",
            "        _targets = [",
            "            (",
            "                target.block._id if target.block and not no_target else None,",
            "                target.event_name,",
            "            )",
            "            for target in targets",
            "        ]",
            "        if isinstance(inputs, set):",
            "            inputs_as_dict = True",
            "            inputs = sorted(inputs, key=lambda x: x._id)",
            "        else:",
            "            inputs_as_dict = False",
            "            if inputs is None:",
            "                inputs = []",
            "            elif not isinstance(inputs, list):",
            "                inputs = [inputs]",
            "",
            "        if isinstance(outputs, set):",
            "            outputs = sorted(outputs, key=lambda x: x._id)",
            "        elif outputs is None:",
            "            outputs = []",
            "        elif not isinstance(outputs, list):",
            "            outputs = [outputs]",
            "",
            "        if fn is not None and not cancels:",
            "            check_function_inputs_match(fn, inputs, inputs_as_dict)",
            "        if every is not None and every <= 0:",
            "            raise ValueError(\"Parameter every must be positive or None\")",
            "        if every and batch:",
            "            raise ValueError(",
            "                f\"Cannot run event in a batch and every {every} seconds. \"",
            "                \"Either batch is True or every is non-zero but not both.\"",
            "            )",
            "",
            "        if every and fn:",
            "            fn = get_continuous_fn(fn, every)",
            "        elif every:",
            "            raise ValueError(\"Cannot set a value for `every` without a `fn`.\")",
            "        if every and concurrency_limit is not None:",
            "            if concurrency_limit == \"default\":",
            "                concurrency_limit = None",
            "            else:",
            "                raise ValueError(",
            "                    \"Cannot set a value for `concurrency_limit` with `every`.\"",
            "                )",
            "",
            "        if _targets[0][1] in [\"change\", \"key_up\"] and trigger_mode is None:",
            "            trigger_mode = \"always_last\"",
            "        elif trigger_mode is None:",
            "            trigger_mode = \"once\"",
            "        elif trigger_mode not in [\"once\", \"multiple\", \"always_last\"]:",
            "            raise ValueError(",
            "                f\"Invalid value for parameter `trigger_mode`: {trigger_mode}. Please choose from: {['once', 'multiple', 'always_last']}\"",
            "            )",
            "",
            "        _, progress_index, event_data_index = (",
            "            special_args(fn) if fn else (None, None, None)",
            "        )",
            "        self.fns.append(",
            "            BlockFunction(",
            "                fn,",
            "                inputs,",
            "                outputs,",
            "                preprocess,",
            "                postprocess,",
            "                inputs_as_dict=inputs_as_dict,",
            "                concurrency_limit=concurrency_limit,",
            "                concurrency_id=concurrency_id,",
            "                batch=batch,",
            "                max_batch_size=max_batch_size,",
            "                tracks_progress=progress_index is not None,",
            "            )",
            "        )",
            "",
            "        # If api_name is None or empty string, use the function name",
            "        if api_name is None or isinstance(api_name, str) and api_name.strip() == \"\":",
            "            if fn is not None:",
            "                if not hasattr(fn, \"__name__\"):",
            "                    if hasattr(fn, \"__class__\") and hasattr(fn.__class__, \"__name__\"):",
            "                        name = fn.__class__.__name__",
            "                    else:",
            "                        name = \"unnamed\"",
            "                else:",
            "                    name = fn.__name__",
            "                api_name = \"\".join(",
            "                    [s for s in name if s not in set(string.punctuation) - {\"-\", \"_\"}]",
            "                )",
            "            elif js is not None:",
            "                api_name = \"js_fn\"",
            "                show_api = False",
            "            else:",
            "                api_name = \"unnamed\"",
            "                show_api = False",
            "",
            "        if api_name is not False:",
            "            api_name = utils.append_unique_suffix(",
            "                api_name, [dep[\"api_name\"] for dep in self.dependencies]",
            "            )",
            "        else:",
            "            show_api = False",
            "",
            "        # The `show_api` parameter is False if: (1) the user explicitly sets it (2) the user sets `api_name` to False",
            "        # or (3) the user sets `fn` to None (there's no backend function)",
            "",
            "        if collects_event_data is None:",
            "            collects_event_data = event_data_index is not None",
            "",
            "        dependency = {",
            "            \"targets\": _targets,",
            "            \"inputs\": [block._id for block in inputs],",
            "            \"outputs\": [block._id for block in outputs],",
            "            \"backend_fn\": fn is not None,",
            "            \"js\": js,",
            "            \"queue\": False if fn is None else queue,",
            "            \"api_name\": api_name,",
            "            \"scroll_to_output\": False if utils.get_space() else scroll_to_output,",
            "            \"show_progress\": show_progress,",
            "            \"every\": every,",
            "            \"batch\": batch,",
            "            \"max_batch_size\": max_batch_size,",
            "            \"cancels\": cancels or [],",
            "            \"types\": {",
            "                \"continuous\": bool(every),",
            "                \"generator\": inspect.isgeneratorfunction(fn)",
            "                or inspect.isasyncgenfunction(fn)",
            "                or bool(every),",
            "            },",
            "            \"collects_event_data\": collects_event_data,",
            "            \"trigger_after\": trigger_after,",
            "            \"trigger_only_on_success\": trigger_only_on_success,",
            "            \"trigger_mode\": trigger_mode,",
            "            \"show_api\": show_api,",
            "        }",
            "        self.dependencies.append(dependency)",
            "        return dependency, len(self.dependencies) - 1",
            "",
            "    def render(self):",
            "        if Context.root_block is not None:",
            "            if self._id in Context.root_block.blocks:",
            "                raise DuplicateBlockError(",
            "                    f\"A block with id: {self._id} has already been rendered in the current Blocks.\"",
            "                )",
            "            overlapping_ids = set(Context.root_block.blocks).intersection(self.blocks)",
            "            for id in overlapping_ids:",
            "                # State components are allowed to be reused between Blocks",
            "                if not isinstance(self.blocks[id], components.State):",
            "                    raise DuplicateBlockError(",
            "                        \"At least one block in this Blocks has already been rendered.\"",
            "                    )",
            "",
            "            Context.root_block.blocks.update(self.blocks)",
            "            Context.root_block.fns.extend(self.fns)",
            "            dependency_offset = len(Context.root_block.dependencies)",
            "            for i, dependency in enumerate(self.dependencies):",
            "                api_name = dependency[\"api_name\"]",
            "                if api_name is not None and api_name is not False:",
            "                    api_name_ = utils.append_unique_suffix(",
            "                        api_name,",
            "                        [dep[\"api_name\"] for dep in Context.root_block.dependencies],",
            "                    )",
            "                    if api_name != api_name_:",
            "                        dependency[\"api_name\"] = api_name_",
            "                dependency[\"cancels\"] = [",
            "                    c + dependency_offset for c in dependency[\"cancels\"]",
            "                ]",
            "                if dependency.get(\"trigger_after\") is not None:",
            "                    dependency[\"trigger_after\"] += dependency_offset",
            "                # Recreate the cancel function so that it has the latest",
            "                # dependency fn indices. This is necessary to properly cancel",
            "                # events in the backend",
            "                if dependency[\"cancels\"]:",
            "                    updated_cancels = [",
            "                        Context.root_block.dependencies[i]",
            "                        for i in dependency[\"cancels\"]",
            "                    ]",
            "                    new_fn = BlockFunction(",
            "                        get_cancel_function(updated_cancels)[0],",
            "                        [],",
            "                        [],",
            "                        False,",
            "                        True,",
            "                        False,",
            "                    )",
            "                    Context.root_block.fns[dependency_offset + i] = new_fn",
            "                Context.root_block.dependencies.append(dependency)",
            "            Context.root_block.temp_file_sets.extend(self.temp_file_sets)",
            "            Context.root_block.proxy_urls.update(self.proxy_urls)",
            "",
            "        if Context.block is not None:",
            "            Context.block.children.extend(self.children)",
            "        return self",
            "",
            "    def is_callable(self, fn_index: int = 0) -> bool:",
            "        \"\"\"Checks if a particular Blocks function is callable (i.e. not stateful or a generator).\"\"\"",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "",
            "        if inspect.isasyncgenfunction(block_fn.fn):",
            "            return False",
            "        if inspect.isgeneratorfunction(block_fn.fn):",
            "            return False",
            "        for input_id in dependency[\"inputs\"]:",
            "            block = self.blocks[input_id]",
            "            if getattr(block, \"stateful\", False):",
            "                return False",
            "        for output_id in dependency[\"outputs\"]:",
            "            block = self.blocks[output_id]",
            "            if getattr(block, \"stateful\", False):",
            "                return False",
            "",
            "        return True",
            "",
            "    def __call__(self, *inputs, fn_index: int = 0, api_name: str | None = None):",
            "        \"\"\"",
            "        Allows Blocks objects to be called as functions. Supply the parameters to the",
            "        function as positional arguments. To choose which function to call, use the",
            "        fn_index parameter, which must be a keyword argument.",
            "",
            "        Parameters:",
            "        *inputs: the parameters to pass to the function",
            "        fn_index: the index of the function to call (defaults to 0, which for Interfaces, is the default prediction function)",
            "        api_name: The api_name of the dependency to call. Will take precedence over fn_index.",
            "        \"\"\"",
            "        if api_name is not None:",
            "            inferred_fn_index = next(",
            "                (",
            "                    i",
            "                    for i, d in enumerate(self.dependencies)",
            "                    if d.get(\"api_name\") == api_name",
            "                ),",
            "                None,",
            "            )",
            "            if inferred_fn_index is None:",
            "                raise InvalidApiNameError(",
            "                    f\"Cannot find a function with api_name {api_name}\"",
            "                )",
            "            fn_index = inferred_fn_index",
            "        if not (self.is_callable(fn_index)):",
            "            raise ValueError(",
            "                \"This function is not callable because it is either stateful or is a generator. Please use the .launch() method instead to create an interactive user interface.\"",
            "            )",
            "",
            "        inputs = list(inputs)",
            "        processed_inputs = self.serialize_data(fn_index, inputs)",
            "        batch = self.dependencies[fn_index][\"batch\"]",
            "        if batch:",
            "            processed_inputs = [[inp] for inp in processed_inputs]",
            "",
            "        outputs = client_utils.synchronize_async(",
            "            self.process_api,",
            "            fn_index=fn_index,",
            "            inputs=processed_inputs,",
            "            request=None,",
            "            state={},",
            "        )",
            "        outputs = outputs[\"data\"]",
            "",
            "        if batch:",
            "            outputs = [out[0] for out in outputs]",
            "",
            "        outputs = self.deserialize_data(fn_index, outputs)",
            "        processed_outputs = utils.resolve_singleton(outputs)",
            "",
            "        return processed_outputs",
            "",
            "    async def call_function(",
            "        self,",
            "        fn_index: int,",
            "        processed_input: list[Any],",
            "        iterator: AsyncIterator[Any] | None = None,",
            "        requests: routes.Request | list[routes.Request] | None = None,",
            "        event_id: str | None = None,",
            "        event_data: EventData | None = None,",
            "        in_event_listener: bool = False,",
            "    ):",
            "        \"\"\"",
            "        Calls function with given index and preprocessed input, and measures process time.",
            "        Parameters:",
            "            fn_index: index of function to call",
            "            processed_input: preprocessed input to pass to function",
            "            iterator: iterator to use if function is a generator",
            "            requests: requests to pass to function",
            "            event_id: id of event in queue",
            "            event_data: data associated with event trigger",
            "        \"\"\"",
            "        block_fn = self.fns[fn_index]",
            "        if not block_fn.fn:",
            "            raise IndexError(f\"function with index {fn_index} not defined.\")",
            "        is_generating = False",
            "        request = requests[0] if isinstance(requests, list) else requests",
            "        start = time.time()",
            "",
            "        fn = utils.get_function_with_locals(",
            "            fn=block_fn.fn,",
            "            blocks=self,",
            "            event_id=event_id,",
            "            in_event_listener=in_event_listener,",
            "            request=request,",
            "        )",
            "",
            "        if iterator is None:  # If not a generator function that has already run",
            "            if block_fn.inputs_as_dict:",
            "                processed_input = [dict(zip(block_fn.inputs, processed_input))]",
            "",
            "            processed_input, progress_index, _ = special_args(",
            "                block_fn.fn, processed_input, request, event_data",
            "            )",
            "            progress_tracker = (",
            "                processed_input[progress_index] if progress_index is not None else None",
            "            )",
            "",
            "            if progress_tracker is not None and progress_index is not None:",
            "                progress_tracker, fn = create_tracker(fn, progress_tracker.track_tqdm)",
            "                processed_input[progress_index] = progress_tracker",
            "",
            "            if inspect.iscoroutinefunction(fn):",
            "                prediction = await fn(*processed_input)",
            "            else:",
            "                prediction = await anyio.to_thread.run_sync(",
            "                    fn, *processed_input, limiter=self.limiter",
            "                )",
            "        else:",
            "            prediction = None",
            "",
            "        if inspect.isgeneratorfunction(fn) or inspect.isasyncgenfunction(fn):",
            "            try:",
            "                if iterator is None:",
            "                    iterator = cast(AsyncIterator[Any], prediction)",
            "                if inspect.isgenerator(iterator):",
            "                    iterator = utils.SyncToAsyncIterator(iterator, self.limiter)",
            "                prediction = await utils.async_iteration(iterator)",
            "                is_generating = True",
            "            except StopAsyncIteration:",
            "                n_outputs = len(self.dependencies[fn_index].get(\"outputs\"))",
            "                prediction = (",
            "                    components._Keywords.FINISHED_ITERATING",
            "                    if n_outputs == 1",
            "                    else (components._Keywords.FINISHED_ITERATING,) * n_outputs",
            "                )",
            "                iterator = None",
            "",
            "        duration = time.time() - start",
            "",
            "        return {",
            "            \"prediction\": prediction,",
            "            \"duration\": duration,",
            "            \"is_generating\": is_generating,",
            "            \"iterator\": iterator,",
            "        }",
            "",
            "    def serialize_data(self, fn_index: int, inputs: list[Any]) -> list[Any]:",
            "        dependency = self.dependencies[fn_index]",
            "        processed_input = []",
            "",
            "        def format_file(s):",
            "            return FileData(path=s).model_dump()",
            "",
            "        for i, input_id in enumerate(dependency[\"inputs\"]):",
            "            try:",
            "                block = self.blocks[input_id]",
            "            except KeyError as e:",
            "                raise InvalidBlockError(",
            "                    f\"Input component with id {input_id} used in {dependency['trigger']}() event is not defined in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"",
            "                ) from e",
            "            if not isinstance(block, components.Component):",
            "                raise InvalidComponentError(",
            "                    f\"{block.__class__} Component with id {input_id} not a valid input component.\"",
            "                )",
            "            api_info = block.api_info()",
            "            if client_utils.value_is_file(api_info):",
            "                serialized_input = client_utils.traverse(",
            "                    inputs[i],",
            "                    format_file,",
            "                    lambda s: client_utils.is_filepath(s) or client_utils.is_url(s),",
            "                )",
            "            else:",
            "                serialized_input = inputs[i]",
            "            processed_input.append(serialized_input)",
            "",
            "        return processed_input",
            "",
            "    def deserialize_data(self, fn_index: int, outputs: list[Any]) -> list[Any]:",
            "        dependency = self.dependencies[fn_index]",
            "        predictions = []",
            "",
            "        for o, output_id in enumerate(dependency[\"outputs\"]):",
            "            try:",
            "                block = self.blocks[output_id]",
            "            except KeyError as e:",
            "                raise InvalidBlockError(",
            "                    f\"Output component with id {output_id} used in {dependency['trigger']}() event not found in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"",
            "                ) from e",
            "            if not isinstance(block, components.Component):",
            "                raise InvalidComponentError(",
            "                    f\"{block.__class__} Component with id {output_id} not a valid output component.\"",
            "                )",
            "",
            "            deserialized = client_utils.traverse(",
            "                outputs[o], lambda s: s[\"path\"], client_utils.is_file_obj",
            "            )",
            "            predictions.append(deserialized)",
            "",
            "        return predictions",
            "",
            "    def validate_inputs(self, fn_index: int, inputs: list[Any]):",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "",
            "        dep_inputs = dependency[\"inputs\"]",
            "",
            "        # This handles incorrect inputs when args are changed by a JS function",
            "        # Only check not enough args case, ignore extra arguments (for now)",
            "        # TODO: make this stricter?",
            "        if len(inputs) < len(dep_inputs):",
            "            name = (",
            "                f\" ({block_fn.name})\"",
            "                if block_fn.name and block_fn.name != \"<lambda>\"",
            "                else \"\"",
            "            )",
            "",
            "            wanted_args = []",
            "            received_args = []",
            "            for input_id in dep_inputs:",
            "                block = self.blocks[input_id]",
            "                wanted_args.append(str(block))",
            "            for inp in inputs:",
            "                v = f'\"{inp}\"' if isinstance(inp, str) else str(inp)",
            "                received_args.append(v)",
            "",
            "            wanted = \", \".join(wanted_args)",
            "            received = \", \".join(received_args)",
            "",
            "            # JS func didn't pass enough arguments",
            "            raise ValueError(",
            "                f\"\"\"An event handler{name} didn't receive enough input values (needed: {len(dep_inputs)}, got: {len(inputs)}).",
            "Check if the event handler calls a Javascript function, and make sure its return value is correct.",
            "Wanted inputs:",
            "    [{wanted}]",
            "Received inputs:",
            "    [{received}]\"\"\"",
            "            )",
            "",
            "    def preprocess_data(",
            "        self, fn_index: int, inputs: list[Any], state: SessionState | None",
            "    ):",
            "        state = state or SessionState(self)",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "",
            "        self.validate_inputs(fn_index, inputs)",
            "",
            "        if block_fn.preprocess:",
            "            processed_input = []",
            "            for i, input_id in enumerate(dependency[\"inputs\"]):",
            "                try:",
            "                    block = self.blocks[input_id]",
            "                except KeyError as e:",
            "                    raise InvalidBlockError(",
            "                        f\"Input component with id {input_id} used in {dependency['trigger']}() event not found in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"",
            "                    ) from e",
            "                if not isinstance(block, components.Component):",
            "                    raise InvalidComponentError(",
            "                        f\"{block.__class__} Component with id {input_id} not a valid input component.\"",
            "                    )",
            "                if getattr(block, \"stateful\", False):",
            "                    processed_input.append(state[input_id])",
            "                else:",
            "                    if input_id in state:",
            "                        block = state[input_id]",
            "                    inputs_cached = processing_utils.move_files_to_cache(",
            "                        inputs[i], block, add_urls=True",
            "                    )",
            "                    if getattr(block, \"data_model\", None) and inputs_cached is not None:",
            "                        if issubclass(block.data_model, GradioModel):  # type: ignore",
            "                            inputs_cached = block.data_model(**inputs_cached)  # type: ignore",
            "                        elif issubclass(block.data_model, GradioRootModel):  # type: ignore",
            "                            inputs_cached = block.data_model(root=inputs_cached)  # type: ignore",
            "                    processed_input.append(block.preprocess(inputs_cached))",
            "        else:",
            "            processed_input = inputs",
            "        return processed_input",
            "",
            "    def validate_outputs(self, fn_index: int, predictions: Any | list[Any]):",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "",
            "        dep_outputs = dependency[\"outputs\"]",
            "",
            "        if not isinstance(predictions, (list, tuple)):",
            "            predictions = [predictions]",
            "",
            "        if len(predictions) < len(dep_outputs):",
            "            name = (",
            "                f\" ({block_fn.name})\"",
            "                if block_fn.name and block_fn.name != \"<lambda>\"",
            "                else \"\"",
            "            )",
            "",
            "            wanted_args = []",
            "            received_args = []",
            "            for output_id in dep_outputs:",
            "                block = self.blocks[output_id]",
            "                wanted_args.append(str(block))",
            "            for pred in predictions:",
            "                v = f'\"{pred}\"' if isinstance(pred, str) else str(pred)",
            "                received_args.append(v)",
            "",
            "            wanted = \", \".join(wanted_args)",
            "            received = \", \".join(received_args)",
            "",
            "            raise ValueError(",
            "                f\"\"\"An event handler{name} didn't receive enough output values (needed: {len(dep_outputs)}, received: {len(predictions)}).",
            "Wanted outputs:",
            "    [{wanted}]",
            "Received outputs:",
            "    [{received}]\"\"\"",
            "            )",
            "",
            "    def postprocess_data(",
            "        self, fn_index: int, predictions: list | dict, state: SessionState | None",
            "    ):",
            "        state = state or SessionState(self)",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "        batch = dependency[\"batch\"]",
            "",
            "        if isinstance(predictions, dict) and len(predictions) > 0:",
            "            predictions = convert_component_dict_to_list(",
            "                dependency[\"outputs\"], predictions",
            "            )",
            "",
            "        if len(dependency[\"outputs\"]) == 1 and not (batch):",
            "            predictions = [",
            "                predictions,",
            "            ]",
            "",
            "        self.validate_outputs(fn_index, predictions)  # type: ignore",
            "",
            "        output = []",
            "        for i, output_id in enumerate(dependency[\"outputs\"]):",
            "            try:",
            "                if predictions[i] is components._Keywords.FINISHED_ITERATING:",
            "                    output.append(None)",
            "                    continue",
            "            except (IndexError, KeyError) as err:",
            "                raise ValueError(",
            "                    \"Number of output components does not match number \"",
            "                    f\"of values returned from from function {block_fn.name}\"",
            "                ) from err",
            "",
            "            try:",
            "                block = self.blocks[output_id]",
            "            except KeyError as e:",
            "                raise InvalidBlockError(",
            "                    f\"Output component with id {output_id} used in {dependency['trigger']}() event not found in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"",
            "                ) from e",
            "",
            "            if getattr(block, \"stateful\", False):",
            "                if not utils.is_update(predictions[i]):",
            "                    state[output_id] = predictions[i]",
            "                output.append(None)",
            "            else:",
            "                prediction_value = predictions[i]",
            "                if utils.is_update(",
            "                    prediction_value",
            "                ):  # if update is passed directly (deprecated), remove Nones",
            "                    prediction_value = utils.delete_none(",
            "                        prediction_value, skip_value=True",
            "                    )",
            "",
            "                if isinstance(prediction_value, Block):",
            "                    prediction_value = prediction_value.constructor_args.copy()",
            "                    prediction_value[\"__type__\"] = \"update\"",
            "                if utils.is_update(prediction_value):",
            "                    if output_id in state:",
            "                        kwargs = state[output_id].constructor_args.copy()",
            "                    else:",
            "                        kwargs = self.blocks[output_id].constructor_args.copy()",
            "                    kwargs.update(prediction_value)",
            "                    kwargs.pop(\"value\", None)",
            "                    kwargs.pop(\"__type__\")",
            "                    kwargs[\"render\"] = False",
            "                    state[output_id] = self.blocks[output_id].__class__(**kwargs)",
            "",
            "                    prediction_value = postprocess_update_dict(",
            "                        block=state[output_id],",
            "                        update_dict=prediction_value,",
            "                        postprocess=block_fn.postprocess,",
            "                    )",
            "                elif block_fn.postprocess:",
            "                    if not isinstance(block, components.Component):",
            "                        raise InvalidComponentError(",
            "                            f\"{block.__class__} Component with id {output_id} not a valid output component.\"",
            "                        )",
            "                    if output_id in state:",
            "                        block = state[output_id]",
            "                    prediction_value = block.postprocess(prediction_value)",
            "",
            "                outputs_cached = processing_utils.move_files_to_cache(",
            "                    prediction_value,",
            "                    block,  # type: ignore",
            "                    postprocess=True,",
            "                    add_urls=True,",
            "                )",
            "                output.append(outputs_cached)",
            "",
            "        return output",
            "",
            "    def handle_streaming_outputs(",
            "        self,",
            "        fn_index: int,",
            "        data: list,",
            "        session_hash: str | None,",
            "        run: int | None,",
            "    ) -> list:",
            "        if session_hash is None or run is None:",
            "            return data",
            "        if run not in self.pending_streams[session_hash]:",
            "            self.pending_streams[session_hash][run] = {}",
            "        stream_run = self.pending_streams[session_hash][run]",
            "",
            "        for i, output_id in enumerate(self.dependencies[fn_index][\"outputs\"]):",
            "            block = self.blocks[output_id]",
            "            if isinstance(block, components.StreamingOutput) and block.streaming:",
            "                first_chunk = output_id not in stream_run",
            "                binary_data, output_data = block.stream_output(",
            "                    data[i], f\"{session_hash}/{run}/{output_id}\", first_chunk",
            "                )",
            "                if first_chunk:",
            "                    stream_run[output_id] = []",
            "                self.pending_streams[session_hash][run][output_id].append(binary_data)",
            "                data[i] = output_data",
            "        return data",
            "",
            "    def handle_streaming_diffs(",
            "        self,",
            "        fn_index: int,",
            "        data: list,",
            "        session_hash: str | None,",
            "        run: int | None,",
            "        final: bool,",
            "    ) -> list:",
            "        if session_hash is None or run is None:",
            "            return data",
            "        first_run = run not in self.pending_diff_streams[session_hash]",
            "        if first_run:",
            "            self.pending_diff_streams[session_hash][run] = [None] * len(data)",
            "        last_diffs = self.pending_diff_streams[session_hash][run]",
            "",
            "        for i in range(len(self.dependencies[fn_index][\"outputs\"])):",
            "            if final:",
            "                data[i] = last_diffs[i]",
            "                continue",
            "",
            "            if first_run:",
            "                last_diffs[i] = data[i]",
            "            else:",
            "                prev_chunk = last_diffs[i]",
            "                last_diffs[i] = data[i]",
            "                data[i] = utils.diff(prev_chunk, data[i])",
            "",
            "        if final:",
            "            del self.pending_diff_streams[session_hash][run]",
            "",
            "        return data",
            "",
            "    def run_fn_batch(self, fn, batch, fn_index, state):",
            "        return [fn(fn_index, list(i), state) for i in zip(*batch)]",
            "",
            "    async def process_api(",
            "        self,",
            "        fn_index: int,",
            "        inputs: list[Any],",
            "        state: SessionState | None = None,",
            "        request: routes.Request | list[routes.Request] | None = None,",
            "        iterator: AsyncIterator | None = None,",
            "        session_hash: str | None = None,",
            "        event_id: str | None = None,",
            "        event_data: EventData | None = None,",
            "        in_event_listener: bool = True,",
            "    ) -> dict[str, Any]:",
            "        \"\"\"",
            "        Processes API calls from the frontend. First preprocesses the data,",
            "        then runs the relevant function, then postprocesses the output.",
            "        Parameters:",
            "            fn_index: Index of function to run.",
            "            inputs: input data received from the frontend",
            "            state: data stored from stateful components for session (key is input block id)",
            "            request: the gr.Request object containing information about the network request (e.g. IP address, headers, query parameters, username)",
            "            iterators: the in-progress iterators for each generator function (key is function index)",
            "            event_id: id of event that triggered this API call",
            "            event_data: data associated with the event trigger itself",
            "        Returns: None",
            "        \"\"\"",
            "        block_fn = self.fns[fn_index]",
            "        batch = self.dependencies[fn_index][\"batch\"]",
            "",
            "        if batch:",
            "            max_batch_size = self.dependencies[fn_index][\"max_batch_size\"]",
            "            batch_sizes = [len(inp) for inp in inputs]",
            "            batch_size = batch_sizes[0]",
            "            if inspect.isasyncgenfunction(block_fn.fn) or inspect.isgeneratorfunction(",
            "                block_fn.fn",
            "            ):",
            "                raise ValueError(\"Gradio does not support generators in batch mode.\")",
            "            if not all(x == batch_size for x in batch_sizes):",
            "                raise ValueError(",
            "                    f\"All inputs to a batch function must have the same length but instead have sizes: {batch_sizes}.\"",
            "                )",
            "            if batch_size > max_batch_size:",
            "                raise ValueError(",
            "                    f\"Batch size ({batch_size}) exceeds the max_batch_size for this function ({max_batch_size})\"",
            "                )",
            "            inputs = await anyio.to_thread.run_sync(",
            "                self.run_fn_batch,",
            "                self.preprocess_data,",
            "                inputs,",
            "                fn_index,",
            "                state,",
            "                limiter=self.limiter,",
            "            )",
            "            result = await self.call_function(",
            "                fn_index,",
            "                list(zip(*inputs)),",
            "                None,",
            "                request,",
            "                event_id,",
            "                event_data,",
            "                in_event_listener,",
            "            )",
            "            preds = result[\"prediction\"]",
            "            data = await anyio.to_thread.run_sync(",
            "                self.run_fn_batch,",
            "                self.postprocess_data,",
            "                preds,",
            "                fn_index,",
            "                state,",
            "                limiter=self.limiter,",
            "            )",
            "            data = list(zip(*data))",
            "            is_generating, iterator = None, None",
            "        else:",
            "            old_iterator = iterator",
            "            if old_iterator:",
            "                inputs = []",
            "            else:",
            "                inputs = await anyio.to_thread.run_sync(",
            "                    self.preprocess_data, fn_index, inputs, state, limiter=self.limiter",
            "                )",
            "            was_generating = old_iterator is not None",
            "            result = await self.call_function(",
            "                fn_index,",
            "                inputs,",
            "                old_iterator,",
            "                request,",
            "                event_id,",
            "                event_data,",
            "                in_event_listener,",
            "            )",
            "            data = await anyio.to_thread.run_sync(",
            "                self.postprocess_data,",
            "                fn_index,  # type: ignore",
            "                result[\"prediction\"],",
            "                state,",
            "                limiter=self.limiter,",
            "            )",
            "            is_generating, iterator = result[\"is_generating\"], result[\"iterator\"]",
            "            if is_generating or was_generating:",
            "                run = id(old_iterator) if was_generating else id(iterator)",
            "                data = self.handle_streaming_outputs(",
            "                    fn_index,",
            "                    data,",
            "                    session_hash=session_hash,",
            "                    run=run,",
            "                )",
            "                data = self.handle_streaming_diffs(",
            "                    fn_index,",
            "                    data,",
            "                    session_hash=session_hash,",
            "                    run=run,",
            "                    final=not is_generating,",
            "                )",
            "",
            "        block_fn.total_runtime += result[\"duration\"]",
            "        block_fn.total_runs += 1",
            "        return {",
            "            \"data\": data,",
            "            \"is_generating\": is_generating,",
            "            \"iterator\": iterator,",
            "            \"duration\": result[\"duration\"],",
            "            \"average_duration\": block_fn.total_runtime / block_fn.total_runs,",
            "        }",
            "",
            "    def create_limiter(self):",
            "        self.limiter = (",
            "            None",
            "            if self.max_threads == 40",
            "            else CapacityLimiter(total_tokens=self.max_threads)",
            "        )",
            "",
            "    def get_config(self):",
            "        return {\"type\": \"column\"}",
            "",
            "    def get_config_file(self):",
            "        config = {",
            "            \"version\": routes.VERSION,",
            "            \"mode\": self.mode,",
            "            \"app_id\": self.app_id,",
            "            \"dev_mode\": self.dev_mode,",
            "            \"analytics_enabled\": self.analytics_enabled,",
            "            \"components\": [],",
            "            \"css\": self.css,",
            "            \"js\": self.js,",
            "            \"head\": self.head,",
            "            \"title\": self.title or \"Gradio\",",
            "            \"space_id\": self.space_id,",
            "            \"enable_queue\": True,  # launch attributes",
            "            \"show_error\": getattr(self, \"show_error\", False),",
            "            \"show_api\": self.show_api,",
            "            \"is_colab\": utils.colab_check(),",
            "            \"stylesheets\": self.stylesheets,",
            "            \"theme\": self.theme.name,",
            "            \"protocol\": \"sse_v2\",",
            "            \"body_css\": {",
            "                \"body_background_fill\": self.theme._get_computed_value(",
            "                    \"body_background_fill\"",
            "                ),",
            "                \"body_text_color\": self.theme._get_computed_value(\"body_text_color\"),",
            "                \"body_background_fill_dark\": self.theme._get_computed_value(",
            "                    \"body_background_fill_dark\"",
            "                ),",
            "                \"body_text_color_dark\": self.theme._get_computed_value(",
            "                    \"body_text_color_dark\"",
            "                ),",
            "            },",
            "            \"fill_height\": self.fill_height,",
            "        }",
            "",
            "        def get_layout(block):",
            "            if not isinstance(block, BlockContext):",
            "                return {\"id\": block._id}",
            "            children_layout = []",
            "            for child in block.children:",
            "                children_layout.append(get_layout(child))",
            "            return {\"id\": block._id, \"children\": children_layout}",
            "",
            "        config[\"layout\"] = get_layout(self)",
            "",
            "        for _id, block in self.blocks.items():",
            "            props = block.get_config() if hasattr(block, \"get_config\") else {}",
            "            block_config = {",
            "                \"id\": _id,",
            "                \"type\": block.get_block_name(),",
            "                \"props\": utils.delete_none(props),",
            "            }",
            "            block_config[\"skip_api\"] = block.skip_api",
            "            block_config[\"component_class_id\"] = getattr(",
            "                block, \"component_class_id\", None",
            "            )",
            "",
            "            if not block.skip_api:",
            "                block_config[\"api_info\"] = block.api_info()  # type: ignore",
            "                block_config[\"example_inputs\"] = block.example_inputs()  # type: ignore",
            "            config[\"components\"].append(block_config)",
            "        config[\"dependencies\"] = self.dependencies",
            "        return config",
            "",
            "    def __enter__(self):",
            "        if Context.block is None:",
            "            Context.root_block = self",
            "        self.parent = Context.block",
            "        Context.block = self",
            "        self.exited = False",
            "        return self",
            "",
            "    def __exit__(self, exc_type: type[BaseException] | None = None, *args):",
            "        if exc_type is not None:",
            "            Context.block = None",
            "            Context.root_block = None",
            "            return",
            "        super().fill_expected_parents()",
            "        Context.block = self.parent",
            "        # Configure the load events before root_block is reset",
            "        self.attach_load_events()",
            "        if self.parent is None:",
            "            Context.root_block = None",
            "        else:",
            "            self.parent.children.extend(self.children)",
            "        self.config = self.get_config_file()",
            "        self.app = routes.App.create_app(self)",
            "        self.progress_tracking = any(block_fn.tracks_progress for block_fn in self.fns)",
            "        self.exited = True",
            "",
            "    def clear(self):",
            "        \"\"\"Resets the layout of the Blocks object.\"\"\"",
            "        self.blocks = {}",
            "        self.fns = []",
            "        self.dependencies = []",
            "        self.children = []",
            "        return self",
            "",
            "    @document()",
            "    def queue(",
            "        self,",
            "        status_update_rate: float | Literal[\"auto\"] = \"auto\",",
            "        api_open: bool | None = None,",
            "        max_size: int | None = None,",
            "        concurrency_count: int | None = None,",
            "        *,",
            "        default_concurrency_limit: int | None | Literal[\"not_set\"] = \"not_set\",",
            "    ):",
            "        \"\"\"",
            "        By enabling the queue you can control when users know their position in the queue, and set a limit on maximum number of events allowed.",
            "        Parameters:",
            "            status_update_rate: If \"auto\", Queue will send status estimations to all clients whenever a job is finished. Otherwise Queue will send status at regular intervals set by this parameter as the number of seconds.",
            "            api_open: If True, the REST routes of the backend will be open, allowing requests made directly to those endpoints to skip the queue.",
            "            max_size: The maximum number of events the queue will store at any given moment. If the queue is full, new events will not be added and a user will receive a message saying that the queue is full. If None, the queue size will be unlimited.",
            "            concurrency_count: Deprecated. Set the concurrency_limit directly on event listeners e.g. btn.click(fn, ..., concurrency_limit=10) or gr.Interface(concurrency_limit=10). If necessary, the total number of workers can be configured via `max_threads` in launch().",
            "            default_concurrency_limit: The default value of `concurrency_limit` to use for event listeners that don't specify a value. Can be set by environment variable GRADIO_DEFAULT_CONCURRENCY_LIMIT. Defaults to 1 if not set otherwise.",
            "        Example: (Blocks)",
            "            with gr.Blocks() as demo:",
            "                button = gr.Button(label=\"Generate Image\")",
            "                button.click(fn=image_generator, inputs=gr.Textbox(), outputs=gr.Image())",
            "            demo.queue(max_size=10)",
            "            demo.launch()",
            "        Example: (Interface)",
            "            demo = gr.Interface(image_generator, gr.Textbox(), gr.Image())",
            "            demo.queue(max_size=20)",
            "            demo.launch()",
            "        \"\"\"",
            "        if concurrency_count:",
            "            raise DeprecationWarning(",
            "                \"concurrency_count has been deprecated. Set the concurrency_limit directly on event listeners e.g. btn.click(fn, ..., concurrency_limit=10) or gr.Interface(concurrency_limit=10). If necessary, the total number of workers can be configured via `max_threads` in launch().\"",
            "            )",
            "        if api_open is not None:",
            "            self.api_open = api_open",
            "        if utils.is_zero_gpu_space():",
            "            max_size = 1 if max_size is None else max_size",
            "        self._queue = queueing.Queue(",
            "            live_updates=status_update_rate == \"auto\",",
            "            concurrency_count=self.max_threads,",
            "            update_intervals=status_update_rate if status_update_rate != \"auto\" else 1,",
            "            max_size=max_size,",
            "            block_fns=self.fns,",
            "            default_concurrency_limit=default_concurrency_limit,",
            "        )",
            "        self.config = self.get_config_file()",
            "        self.app = routes.App.create_app(self)",
            "        return self",
            "",
            "    def validate_queue_settings(self):",
            "        for dep in self.dependencies:",
            "            for i in dep[\"cancels\"]:",
            "                if not self.queue_enabled_for_fn(i):",
            "                    raise ValueError(",
            "                        \"Queue needs to be enabled! \"",
            "                        \"You may get this error by either 1) passing a function that uses the yield keyword \"",
            "                        \"into an interface without enabling the queue or 2) defining an event that cancels \"",
            "                        \"another event without enabling the queue. Both can be solved by calling .queue() \"",
            "                        \"before .launch()\"",
            "                    )",
            "            if dep[\"batch\"] and dep[\"queue\"] is False:",
            "                raise ValueError(\"In order to use batching, the queue must be enabled.\")",
            "",
            "    def launch(",
            "        self,",
            "        inline: bool | None = None,",
            "        inbrowser: bool = False,",
            "        share: bool | None = None,",
            "        debug: bool = False,",
            "        max_threads: int = 40,",
            "        auth: Callable | tuple[str, str] | list[tuple[str, str]] | None = None,",
            "        auth_message: str | None = None,",
            "        prevent_thread_lock: bool = False,",
            "        show_error: bool = False,",
            "        server_name: str | None = None,",
            "        server_port: int | None = None,",
            "        *,",
            "        height: int = 500,",
            "        width: int | str = \"100%\",",
            "        favicon_path: str | None = None,",
            "        ssl_keyfile: str | None = None,",
            "        ssl_certfile: str | None = None,",
            "        ssl_keyfile_password: str | None = None,",
            "        ssl_verify: bool = True,",
            "        quiet: bool = False,",
            "        show_api: bool = True,",
            "        allowed_paths: list[str] | None = None,",
            "        blocked_paths: list[str] | None = None,",
            "        root_path: str | None = None,",
            "        app_kwargs: dict[str, Any] | None = None,",
            "        state_session_capacity: int = 10000,",
            "        share_server_address: str | None = None,",
            "        share_server_protocol: Literal[\"http\", \"https\"] | None = None,",
            "        _frontend: bool = True,",
            "    ) -> tuple[FastAPI, str, str]:",
            "        \"\"\"",
            "        Launches a simple web server that serves the demo. Can also be used to create a",
            "        public link used by anyone to access the demo from their browser by setting share=True.",
            "",
            "        Parameters:",
            "            inline: whether to display in the interface inline in an iframe. Defaults to True in python notebooks; False otherwise.",
            "            inbrowser: whether to automatically launch the interface in a new tab on the default browser.",
            "            share: whether to create a publicly shareable link for the interface. Creates an SSH tunnel to make your UI accessible from anywhere. If not provided, it is set to False by default every time, except when running in Google Colab. When localhost is not accessible (e.g. Google Colab), setting share=False is not supported.",
            "            debug: if True, blocks the main thread from running. If running in Google Colab, this is needed to print the errors in the cell output.",
            "            auth: If provided, username and password (or list of username-password tuples) required to access interface. Can also provide function that takes username and password and returns True if valid login.",
            "            auth_message: If provided, HTML message provided on login page.",
            "            prevent_thread_lock: If True, the interface will block the main thread while the server is running.",
            "            show_error: If True, any errors in the interface will be displayed in an alert modal and printed in the browser console log",
            "            server_port: will start gradio app on this port (if available). Can be set by environment variable GRADIO_SERVER_PORT. If None, will search for an available port starting at 7860.",
            "            server_name: to make app accessible on local network, set this to \"0.0.0.0\". Can be set by environment variable GRADIO_SERVER_NAME. If None, will use \"127.0.0.1\".",
            "            max_threads: the maximum number of total threads that the Gradio app can generate in parallel. The default is inherited from the starlette library (currently 40).",
            "            width: The width in pixels of the iframe element containing the interface (used if inline=True)",
            "            height: The height in pixels of the iframe element containing the interface (used if inline=True)",
            "            favicon_path: If a path to a file (.png, .gif, or .ico) is provided, it will be used as the favicon for the web page.",
            "            ssl_keyfile: If a path to a file is provided, will use this as the private key file to create a local server running on https.",
            "            ssl_certfile: If a path to a file is provided, will use this as the signed certificate for https. Needs to be provided if ssl_keyfile is provided.",
            "            ssl_keyfile_password: If a password is provided, will use this with the ssl certificate for https.",
            "            ssl_verify: If False, skips certificate validation which allows self-signed certificates to be used.",
            "            quiet: If True, suppresses most print statements.",
            "            show_api: If True, shows the api docs in the footer of the app. Default True.",
            "            allowed_paths: List of complete filepaths or parent directories that gradio is allowed to serve (in addition to the directory containing the gradio python file). Must be absolute paths. Warning: if you provide directories, any files in these directories or their subdirectories are accessible to all users of your app.",
            "            blocked_paths: List of complete filepaths or parent directories that gradio is not allowed to serve (i.e. users of your app are not allowed to access). Must be absolute paths. Warning: takes precedence over `allowed_paths` and all other directories exposed by Gradio by default.",
            "            root_path: The root path (or \"mount point\") of the application, if it's not served from the root (\"/\") of the domain. Often used when the application is behind a reverse proxy that forwards requests to the application. For example, if the application is served at \"https://example.com/myapp\", the `root_path` should be set to \"/myapp\". Can be set by environment variable GRADIO_ROOT_PATH. Defaults to \"\".",
            "            app_kwargs: Additional keyword arguments to pass to the underlying FastAPI app as a dictionary of parameter keys and argument values. For example, `{\"docs_url\": \"/docs\"}`",
            "            state_session_capacity: The maximum number of sessions whose information to store in memory. If the number of sessions exceeds this number, the oldest sessions will be removed. Reduce capacity to reduce memory usage when using gradio.State or returning updated components from functions. Defaults to 10000.",
            "            share_server_address: Use this to specify a custom FRP server and port for sharing Gradio apps (only applies if share=True). If not provided, will use the default FRP server at https://gradio.live. See https://github.com/huggingface/frp for more information.",
            "            share_server_protocol: Use this to specify the protocol to use for the share links. Defaults to \"https\", unless a custom share_server_address is provided, in which case it defaults to \"http\". If you are using a custom share_server_address and want to use https, you must set this to \"https\".",
            "        Returns:",
            "            app: FastAPI app object that is running the demo",
            "            local_url: Locally accessible link to the demo",
            "            share_url: Publicly accessible link to the demo (if share=True, otherwise None)",
            "        Example: (Blocks)",
            "            import gradio as gr",
            "            def reverse(text):",
            "                return text[::-1]",
            "            with gr.Blocks() as demo:",
            "                button = gr.Button(value=\"Reverse\")",
            "                button.click(reverse, gr.Textbox(), gr.Textbox())",
            "            demo.launch(share=True, auth=(\"username\", \"password\"))",
            "        Example:  (Interface)",
            "            import gradio as gr",
            "            def reverse(text):",
            "                return text[::-1]",
            "            demo = gr.Interface(reverse, \"text\", \"text\")",
            "            demo.launch(share=True, auth=(\"username\", \"password\"))",
            "        \"\"\"",
            "        if self._is_running_in_reload_thread:",
            "            # We have already launched the demo",
            "            return None, None, None  # type: ignore",
            "",
            "        if not self.exited:",
            "            self.__exit__()",
            "",
            "        if (",
            "            auth",
            "            and not callable(auth)",
            "            and not isinstance(auth[0], tuple)",
            "            and not isinstance(auth[0], list)",
            "        ):",
            "            self.auth = [auth]",
            "        else:",
            "            self.auth = auth",
            "        self.auth_message = auth_message",
            "        self.show_error = show_error",
            "        self.height = height",
            "        self.width = width",
            "        self.favicon_path = favicon_path",
            "        self.ssl_verify = ssl_verify",
            "        self.state_session_capacity = state_session_capacity",
            "        if root_path is None:",
            "            self.root_path = os.environ.get(\"GRADIO_ROOT_PATH\", \"\")",
            "        else:",
            "            self.root_path = root_path",
            "",
            "        self.show_api = show_api",
            "",
            "        self.allowed_paths = allowed_paths or []",
            "        self.blocked_paths = blocked_paths or []",
            "",
            "        if not isinstance(self.allowed_paths, list):",
            "            raise ValueError(\"`allowed_paths` must be a list of directories.\")",
            "        if not isinstance(self.blocked_paths, list):",
            "            raise ValueError(\"`blocked_paths` must be a list of directories.\")",
            "",
            "        self.validate_queue_settings()",
            "",
            "        self.config = self.get_config_file()",
            "        self.max_threads = max_threads",
            "        self._queue.max_thread_count = max_threads",
            "",
            "        if self.is_running:",
            "            if not isinstance(self.local_url, str):",
            "                raise ValueError(f\"Invalid local_url: {self.local_url}\")",
            "            if not (quiet):",
            "                print(",
            "                    \"Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\\n----\"",
            "                )",
            "        else:",
            "            if wasm_utils.IS_WASM:",
            "                server_name = \"xxx\"",
            "                server_port = 99999",
            "                local_url = \"\"",
            "                server = None",
            "",
            "                # In the Wasm environment, we only need the app object",
            "                # which the frontend app will directly communicate with through the Worker API,",
            "                # and we don't need to start a server.",
            "                # So we just create the app object and register it here,",
            "                # and avoid using `networking.start_server` that would start a server that don't work in the Wasm env.",
            "                from gradio.routes import App",
            "",
            "                app = App.create_app(self, app_kwargs=app_kwargs)",
            "                wasm_utils.register_app(app)",
            "            else:",
            "                (",
            "                    server_name,",
            "                    server_port,",
            "                    local_url,",
            "                    app,",
            "                    server,",
            "                ) = networking.start_server(",
            "                    self,",
            "                    server_name,",
            "                    server_port,",
            "                    ssl_keyfile,",
            "                    ssl_certfile,",
            "                    ssl_keyfile_password,",
            "                    app_kwargs=app_kwargs,",
            "                )",
            "            self.server_name = server_name",
            "            self.local_url = local_url",
            "            self.server_port = server_port",
            "            self.server_app = (",
            "                self.app",
            "            ) = app  # server_app is included for backwards compatibility",
            "            self.server = server",
            "            self.is_running = True",
            "            self.is_colab = utils.colab_check()",
            "            self.is_kaggle = utils.kaggle_check()",
            "            self.share_server_address = share_server_address",
            "            self.share_server_protocol = share_server_protocol or (",
            "                \"http\" if share_server_address is not None else \"https\"",
            "            )",
            "",
            "            self.protocol = (",
            "                \"https\"",
            "                if self.local_url.startswith(\"https\") or self.is_colab",
            "                else \"http\"",
            "            )",
            "            if not wasm_utils.IS_WASM and not self.is_colab:",
            "                print(",
            "                    strings.en[\"RUNNING_LOCALLY_SEPARATED\"].format(",
            "                        self.protocol, self.server_name, self.server_port",
            "                    )",
            "                )",
            "",
            "            self._queue.set_server_app(self.server_app)",
            "",
            "            if not wasm_utils.IS_WASM:",
            "                # Cannot run async functions in background other than app's scope.",
            "                # Workaround by triggering the app endpoint",
            "                httpx.get(f\"{self.local_url}startup-events\", verify=ssl_verify)",
            "            else:",
            "                # NOTE: One benefit of the code above dispatching `startup_events()` via a self HTTP request is",
            "                # that `self._queue.start()` is called in another thread which is managed by the HTTP server, `uvicorn`",
            "                # so all the asyncio tasks created by the queue runs in an event loop in that thread and",
            "                # will be cancelled just by stopping the server.",
            "                # In contrast, in the Wasm env, we can't do that because `threading` is not supported and all async tasks will run in the same event loop, `pyodide.webloop.WebLoop` in the main thread.",
            "                # So we need to manually cancel them. See `self.close()`..",
            "                self.startup_events()",
            "",
            "        utils.launch_counter()",
            "        self.is_sagemaker = utils.sagemaker_check()",
            "        if share is None:",
            "            if self.is_colab:",
            "                if not quiet:",
            "                    print(",
            "                        \"Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\\n\"",
            "                    )",
            "                self.share = True",
            "            elif self.is_kaggle:",
            "                if not quiet:",
            "                    print(",
            "                        \"Kaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\\n\"",
            "                    )",
            "                self.share = True",
            "            elif self.is_sagemaker:",
            "                if not quiet:",
            "                    print(",
            "                        \"Sagemaker notebooks may require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\\n\"",
            "                    )",
            "                self.share = True",
            "            else:",
            "                self.share = False",
            "        else:",
            "            self.share = share",
            "",
            "        # If running in a colab or not able to access localhost,",
            "        # a shareable link must be created.",
            "        if (",
            "            _frontend",
            "            and not wasm_utils.IS_WASM",
            "            and not networking.url_ok(self.local_url)",
            "            and not self.share",
            "        ):",
            "            raise ValueError(",
            "                \"When localhost is not accessible, a shareable link must be created. Please set share=True or check your proxy settings to allow access to localhost.\"",
            "            )",
            "",
            "        if self.is_colab and not quiet:",
            "            if debug:",
            "                print(strings.en[\"COLAB_DEBUG_TRUE\"])",
            "            else:",
            "                print(strings.en[\"COLAB_DEBUG_FALSE\"])",
            "            if not self.share:",
            "                print(strings.en[\"COLAB_WARNING\"].format(self.server_port))",
            "",
            "        if self.share:",
            "            if self.space_id:",
            "                warnings.warn(",
            "                    \"Setting share=True is not supported on Hugging Face Spaces\"",
            "                )",
            "                self.share = False",
            "            if wasm_utils.IS_WASM:",
            "                warnings.warn(",
            "                    \"Setting share=True is not supported in the Wasm environment\"",
            "                )",
            "                self.share = False",
            "",
            "        if self.share:",
            "            try:",
            "                if self.share_url is None:",
            "                    share_url = networking.setup_tunnel(",
            "                        local_host=self.server_name,",
            "                        local_port=self.server_port,",
            "                        share_token=self.share_token,",
            "                        share_server_address=self.share_server_address,",
            "                    )",
            "                    parsed_url = urlparse(share_url)",
            "                    self.share_url = urlunparse(",
            "                        (self.share_server_protocol,) + parsed_url[1:]",
            "                    )",
            "                print(strings.en[\"SHARE_LINK_DISPLAY\"].format(self.share_url))",
            "                if not (quiet):",
            "                    print(strings.en[\"SHARE_LINK_MESSAGE\"])",
            "            except (RuntimeError, httpx.ConnectError):",
            "                if self.analytics_enabled:",
            "                    analytics.error_analytics(\"Not able to set up tunnel\")",
            "                self.share_url = None",
            "                self.share = False",
            "                if Path(BINARY_PATH).exists():",
            "                    print(strings.en[\"COULD_NOT_GET_SHARE_LINK\"])",
            "                else:",
            "                    print(",
            "                        strings.en[\"COULD_NOT_GET_SHARE_LINK_MISSING_FILE\"].format(",
            "                            BINARY_PATH,",
            "                            BINARY_URL,",
            "                            BINARY_FILENAME,",
            "                            BINARY_FOLDER,",
            "                        )",
            "                    )",
            "        else:",
            "            if not quiet and not wasm_utils.IS_WASM:",
            "                print(strings.en[\"PUBLIC_SHARE_TRUE\"])",
            "            self.share_url = None",
            "",
            "        if inbrowser and not wasm_utils.IS_WASM:",
            "            link = self.share_url if self.share and self.share_url else self.local_url",
            "            webbrowser.open(link)",
            "",
            "        # Check if running in a Python notebook in which case, display inline",
            "        if inline is None:",
            "            inline = utils.ipython_check()",
            "        if inline:",
            "            try:",
            "                from IPython.display import HTML, Javascript, display  # type: ignore",
            "",
            "                if self.share and self.share_url:",
            "                    while not networking.url_ok(self.share_url):",
            "                        time.sleep(0.25)",
            "                    artifact = HTML(",
            "                        f'<div><iframe src=\"{self.share_url}\" width=\"{self.width}\" height=\"{self.height}\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>'",
            "                    )",
            "",
            "                elif self.is_colab:",
            "                    # modified from /usr/local/lib/python3.7/dist-packages/google/colab/output/_util.py within Colab environment",
            "                    code = \"\"\"(async (port, path, width, height, cache, element) => {",
            "                        if (!google.colab.kernel.accessAllowed && !cache) {",
            "                            return;",
            "                        }",
            "                        element.appendChild(document.createTextNode(''));",
            "                        const url = await google.colab.kernel.proxyPort(port, {cache});",
            "",
            "                        const external_link = document.createElement('div');",
            "                        external_link.innerHTML = `",
            "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">",
            "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">",
            "                                    https://localhost:${port}${path}",
            "                                </a>",
            "                            </div>",
            "                        `;",
            "                        element.appendChild(external_link);",
            "",
            "                        const iframe = document.createElement('iframe');",
            "                        iframe.src = new URL(path, url).toString();",
            "                        iframe.height = height;",
            "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"",
            "                        iframe.width = width;",
            "                        iframe.style.border = 0;",
            "                        element.appendChild(iframe);",
            "                    })\"\"\" + \"({port}, {path}, {width}, {height}, {cache}, window.element)\".format(",
            "                        port=json.dumps(self.server_port),",
            "                        path=json.dumps(\"/\"),",
            "                        width=json.dumps(self.width),",
            "                        height=json.dumps(self.height),",
            "                        cache=json.dumps(False),",
            "                    )",
            "",
            "                    artifact = Javascript(code)",
            "                else:",
            "                    artifact = HTML(",
            "                        f'<div><iframe src=\"{self.local_url}\" width=\"{self.width}\" height=\"{self.height}\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>'",
            "                    )",
            "                self.artifact = artifact",
            "                display(artifact)",
            "            except ImportError:",
            "                pass",
            "",
            "        if getattr(self, \"analytics_enabled\", False):",
            "            data = {",
            "                \"launch_method\": \"browser\" if inbrowser else \"inline\",",
            "                \"is_google_colab\": self.is_colab,",
            "                \"is_sharing_on\": self.share,",
            "                \"share_url\": self.share_url,",
            "                \"enable_queue\": True,",
            "                \"server_name\": server_name,",
            "                \"server_port\": server_port,",
            "                \"is_space\": self.space_id is not None,",
            "                \"mode\": self.mode,",
            "            }",
            "            analytics.launched_analytics(self, data)",
            "",
            "        # Block main thread if debug==True",
            "        if debug or int(os.getenv(\"GRADIO_DEBUG\", \"0\")) == 1 and not wasm_utils.IS_WASM:",
            "            self.block_thread()",
            "        # Block main thread if running in a script to stop script from exiting",
            "        is_in_interactive_mode = bool(getattr(sys, \"ps1\", sys.flags.interactive))",
            "",
            "        if (",
            "            not prevent_thread_lock",
            "            and not is_in_interactive_mode",
            "            # In the Wasm env, we don't have to block the main thread because the server won't be shut down after the execution finishes.",
            "            # Moreover, we MUST NOT do it because there is only one thread in the Wasm env and blocking it will stop the subsequent code from running.",
            "            and not wasm_utils.IS_WASM",
            "        ):",
            "            self.block_thread()",
            "",
            "        return TupleNoPrint((self.server_app, self.local_url, self.share_url))  # type: ignore",
            "",
            "    def integrate(",
            "        self,",
            "        comet_ml=None,",
            "        wandb: ModuleType | None = None,",
            "        mlflow: ModuleType | None = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        A catch-all method for integrating with other libraries. This method should be run after launch()",
            "        Parameters:",
            "            comet_ml: If a comet_ml Experiment object is provided, will integrate with the experiment and appear on Comet dashboard",
            "            wandb: If the wandb module is provided, will integrate with it and appear on WandB dashboard",
            "            mlflow: If the mlflow module  is provided, will integrate with the experiment and appear on ML Flow dashboard",
            "        \"\"\"",
            "        analytics_integration = \"\"",
            "        if comet_ml is not None:",
            "            analytics_integration = \"CometML\"",
            "            comet_ml.log_other(\"Created from\", \"Gradio\")",
            "            if self.share_url is not None:",
            "                comet_ml.log_text(f\"gradio: {self.share_url}\")",
            "                comet_ml.end()",
            "            elif self.local_url:",
            "                comet_ml.log_text(f\"gradio: {self.local_url}\")",
            "                comet_ml.end()",
            "            else:",
            "                raise ValueError(\"Please run `launch()` first.\")",
            "        if wandb is not None:",
            "            analytics_integration = \"WandB\"",
            "            if self.share_url is not None:",
            "                wandb.log(",
            "                    {",
            "                        \"Gradio panel\": wandb.Html(",
            "                            '<iframe src=\"'",
            "                            + self.share_url",
            "                            + '\" width=\"'",
            "                            + str(self.width)",
            "                            + '\" height=\"'",
            "                            + str(self.height)",
            "                            + '\" frameBorder=\"0\"></iframe>'",
            "                        )",
            "                    }",
            "                )",
            "            else:",
            "                print(",
            "                    \"The WandB integration requires you to \"",
            "                    \"`launch(share=True)` first.\"",
            "                )",
            "        if mlflow is not None:",
            "            analytics_integration = \"MLFlow\"",
            "            if self.share_url is not None:",
            "                mlflow.log_param(\"Gradio Interface Share Link\", self.share_url)",
            "            else:",
            "                mlflow.log_param(\"Gradio Interface Local Link\", self.local_url)",
            "        if self.analytics_enabled and analytics_integration:",
            "            data = {\"integration\": analytics_integration}",
            "            analytics.integration_analytics(data)",
            "",
            "    def close(self, verbose: bool = True) -> None:",
            "        \"\"\"",
            "        Closes the Interface that was launched and frees the port.",
            "        \"\"\"",
            "        try:",
            "            if wasm_utils.IS_WASM:",
            "                # NOTE:",
            "                # Normally, queue-related async tasks (e.g. continuous events created by `gr.Blocks.load(..., every=interval)`, whose async tasks are started at the `/queue/data` endpoint function)",
            "                # are running in an event loop in the server thread,",
            "                # so they will be cancelled by `self.server.close()` below.",
            "                # However, in the Wasm env, we don't have the `server` and",
            "                # all async tasks are running in the same event loop, `pyodide.webloop.WebLoop` in the main thread,",
            "                # so we have to cancel them explicitly so that these tasks won't run after a new app is launched.",
            "                self._queue._cancel_asyncio_tasks()",
            "                self.server_app._cancel_asyncio_tasks()",
            "            self._queue.close()",
            "            if self.server:",
            "                self.server.close()",
            "            self.is_running = False",
            "            # So that the startup events (starting the queue)",
            "            # happen the next time the app is launched",
            "            self.app.startup_events_triggered = False",
            "            if verbose:",
            "                print(f\"Closing server running on port: {self.server_port}\")",
            "        except (AttributeError, OSError):  # can't close if not running",
            "            pass",
            "",
            "    def block_thread(",
            "        self,",
            "    ) -> None:",
            "        \"\"\"Block main thread until interrupted by user.\"\"\"",
            "        try:",
            "            while True:",
            "                time.sleep(0.1)",
            "        except (KeyboardInterrupt, OSError):",
            "            print(\"Keyboard interruption in main thread... closing server.\")",
            "            if self.server:",
            "                self.server.close()",
            "            for tunnel in CURRENT_TUNNELS:",
            "                tunnel.kill()",
            "",
            "    def attach_load_events(self):",
            "        \"\"\"Add a load event for every component whose initial value should be randomized.\"\"\"",
            "        if Context.root_block:",
            "            for component in Context.root_block.blocks.values():",
            "                if (",
            "                    isinstance(component, components.Component)",
            "                    and component.load_event_to_attach",
            "                ):",
            "                    load_fn, every = component.load_event_to_attach",
            "                    # Use set_event_trigger to avoid ambiguity between load class/instance method",
            "",
            "                    dep = self.set_event_trigger(",
            "                        [EventListenerMethod(self, \"load\")],",
            "                        load_fn,",
            "                        None,",
            "                        component,",
            "                        no_target=True,",
            "                        # If every is None, for sure skip the queue",
            "                        # else, let the enable_queue parameter take precedence",
            "                        # this will raise a nice error message is every is used",
            "                        # without queue",
            "                        queue=False if every is None else None,",
            "                        every=every,",
            "                    )[0]",
            "                    component.load_event = dep",
            "",
            "    def startup_events(self):",
            "        \"\"\"Events that should be run when the app containing this block starts up.\"\"\"",
            "        self._queue.start()",
            "        # So that processing can resume in case the queue was stopped",
            "        self._queue.stopped = False",
            "        self.create_limiter()",
            "",
            "    def queue_enabled_for_fn(self, fn_index: int):",
            "        return self.dependencies[fn_index][\"queue\"] is not False",
            "",
            "    def get_api_info(self):",
            "        \"\"\"",
            "        Gets the information needed to generate the API docs from a Blocks.",
            "        \"\"\"",
            "        config = self.config",
            "        api_info = {\"named_endpoints\": {}, \"unnamed_endpoints\": {}}",
            "",
            "        for dependency in config[\"dependencies\"]:",
            "            if (",
            "                not dependency[\"backend_fn\"]",
            "                or not dependency[\"show_api\"]",
            "                or dependency[\"api_name\"] is False",
            "            ):",
            "                continue",
            "",
            "            dependency_info = {\"parameters\": [], \"returns\": []}",
            "            skip_endpoint = False",
            "",
            "            inputs = dependency[\"inputs\"]",
            "            for i in inputs:",
            "                for component in config[\"components\"]:",
            "                    if component[\"id\"] == i:",
            "                        break",
            "                else:",
            "                    skip_endpoint = True  # if component not found, skip endpoint",
            "                    break",
            "                type = component[\"type\"]",
            "                if self.blocks[component[\"id\"]].skip_api:",
            "                    continue",
            "                label = component[\"props\"].get(\"label\", f\"parameter_{i}\")",
            "                comp = self.get_component(component[\"id\"])",
            "                if not isinstance(comp, components.Component):",
            "                    raise TypeError(f\"{comp!r} is not a Component\")",
            "                info = component[\"api_info\"]",
            "                example = comp.example_inputs()",
            "                python_type = client_utils.json_schema_to_python_type(info)",
            "                dependency_info[\"parameters\"].append(",
            "                    {",
            "                        \"label\": label,",
            "                        \"type\": info,",
            "                        \"python_type\": {",
            "                            \"type\": python_type,",
            "                            \"description\": info.get(\"description\", \"\"),",
            "                        },",
            "                        \"component\": type.capitalize(),",
            "                        \"example_input\": example,",
            "                    }",
            "                )",
            "",
            "            outputs = dependency[\"outputs\"]",
            "            for o in outputs:",
            "                for component in config[\"components\"]:",
            "                    if component[\"id\"] == o:",
            "                        break",
            "                else:",
            "                    skip_endpoint = True  # if component not found, skip endpoint",
            "                    break",
            "                type = component[\"type\"]",
            "                if self.blocks[component[\"id\"]].skip_api:",
            "                    continue",
            "                label = component[\"props\"].get(\"label\", f\"value_{o}\")",
            "                comp = self.get_component(component[\"id\"])",
            "                if not isinstance(comp, components.Component):",
            "                    raise TypeError(f\"{comp!r} is not a Component\")",
            "                info = component[\"api_info\"]",
            "                example = comp.example_inputs()",
            "                python_type = client_utils.json_schema_to_python_type(info)",
            "                dependency_info[\"returns\"].append(",
            "                    {",
            "                        \"label\": label,",
            "                        \"type\": info,",
            "                        \"python_type\": {",
            "                            \"type\": python_type,",
            "                            \"description\": info.get(\"description\", \"\"),",
            "                        },",
            "                        \"component\": type.capitalize(),",
            "                    }",
            "                )",
            "",
            "            if not skip_endpoint:",
            "                api_info[\"named_endpoints\"][",
            "                    f\"/{dependency['api_name']}\"",
            "                ] = dependency_info",
            "",
            "        return api_info"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import copy",
            "import hashlib",
            "import inspect",
            "import json",
            "import os",
            "import random",
            "import secrets",
            "import string",
            "import sys",
            "import threading",
            "import time",
            "import warnings",
            "import webbrowser",
            "from collections import defaultdict",
            "from pathlib import Path",
            "from types import ModuleType",
            "from typing import TYPE_CHECKING, Any, AsyncIterator, Callable, Literal, Sequence, cast",
            "from urllib.parse import urlparse, urlunparse",
            "",
            "import anyio",
            "import httpx",
            "from anyio import CapacityLimiter",
            "from gradio_client import utils as client_utils",
            "from gradio_client.documentation import document",
            "",
            "from gradio import (",
            "    analytics,",
            "    components,",
            "    networking,",
            "    processing_utils,",
            "    queueing,",
            "    routes,",
            "    strings,",
            "    themes,",
            "    utils,",
            "    wasm_utils,",
            ")",
            "from gradio.blocks_events import BlocksEvents, BlocksMeta",
            "from gradio.context import Context",
            "from gradio.data_classes import FileData, GradioModel, GradioRootModel",
            "from gradio.events import (",
            "    EventData,",
            "    EventListener,",
            "    EventListenerMethod,",
            ")",
            "from gradio.exceptions import (",
            "    DuplicateBlockError,",
            "    InvalidApiNameError,",
            "    InvalidBlockError,",
            "    InvalidComponentError,",
            ")",
            "from gradio.helpers import create_tracker, skip, special_args",
            "from gradio.state_holder import SessionState",
            "from gradio.themes import Default as DefaultTheme",
            "from gradio.themes import ThemeClass as Theme",
            "from gradio.tunneling import (",
            "    BINARY_FILENAME,",
            "    BINARY_FOLDER,",
            "    BINARY_PATH,",
            "    BINARY_URL,",
            "    CURRENT_TUNNELS,",
            ")",
            "from gradio.utils import (",
            "    TupleNoPrint,",
            "    check_function_inputs_match,",
            "    component_or_layout_class,",
            "    get_cancel_function,",
            "    get_continuous_fn,",
            "    get_package_version,",
            "    get_upload_folder,",
            ")",
            "",
            "try:",
            "    import spaces  # type: ignore",
            "except Exception:",
            "    spaces = None",
            "",
            "",
            "if TYPE_CHECKING:  # Only import for type checking (is False at runtime).",
            "    from fastapi.applications import FastAPI",
            "",
            "    from gradio.components.base import Component",
            "",
            "BUILT_IN_THEMES: dict[str, Theme] = {",
            "    t.name: t",
            "    for t in [",
            "        themes.Base(),",
            "        themes.Default(),",
            "        themes.Monochrome(),",
            "        themes.Soft(),",
            "        themes.Glass(),",
            "    ]",
            "}",
            "",
            "",
            "class Block:",
            "    def __init__(",
            "        self,",
            "        *,",
            "        elem_id: str | None = None,",
            "        elem_classes: list[str] | str | None = None,",
            "        render: bool = True,",
            "        visible: bool = True,",
            "        proxy_url: str | None = None,",
            "    ):",
            "        self._id = Context.id",
            "        Context.id += 1",
            "        self.visible = visible",
            "        self.elem_id = elem_id",
            "        self.elem_classes = (",
            "            [elem_classes] if isinstance(elem_classes, str) else elem_classes",
            "        )",
            "        self.proxy_url = proxy_url",
            "        self.share_token = secrets.token_urlsafe(32)",
            "        self.parent: BlockContext | None = None",
            "        self.is_rendered: bool = False",
            "        self._constructor_args: list[dict]",
            "        self.state_session_capacity = 10000",
            "        self.temp_files: set[str] = set()",
            "        self.GRADIO_CACHE = get_upload_folder()",
            "",
            "        if render:",
            "            self.render()",
            "",
            "    @property",
            "    def skip_api(self):",
            "        return False",
            "",
            "    @property",
            "    def constructor_args(self) -> dict[str, Any]:",
            "        \"\"\"Get the arguments passed to the component's initializer.",
            "",
            "        Only set classes whose metaclass is ComponentMeta",
            "        \"\"\"",
            "        # the _constructor_args list is appended based on the mro of the class",
            "        # so the first entry is for the bottom of the hierarchy",
            "        return self._constructor_args[0] if self._constructor_args else {}",
            "",
            "    @property",
            "    def events(",
            "        self,",
            "    ) -> list[EventListener]:",
            "        return getattr(self, \"EVENTS\", [])",
            "",
            "    def render(self):",
            "        \"\"\"",
            "        Adds self into appropriate BlockContext",
            "        \"\"\"",
            "        if Context.root_block is not None and self._id in Context.root_block.blocks:",
            "            raise DuplicateBlockError(",
            "                f\"A block with id: {self._id} has already been rendered in the current Blocks.\"",
            "            )",
            "        if Context.block is not None:",
            "            Context.block.add(self)",
            "        if Context.root_block is not None:",
            "            Context.root_block.blocks[self._id] = self",
            "            self.is_rendered = True",
            "            if isinstance(self, components.Component):",
            "                Context.root_block.temp_file_sets.append(self.temp_files)",
            "        return self",
            "",
            "    def unrender(self):",
            "        \"\"\"",
            "        Removes self from BlockContext if it has been rendered (otherwise does nothing).",
            "        Removes self from the layout and collection of blocks, but does not delete any event triggers.",
            "        \"\"\"",
            "        if Context.block is not None:",
            "            try:",
            "                Context.block.children.remove(self)",
            "            except ValueError:",
            "                pass",
            "        if Context.root_block is not None:",
            "            try:",
            "                del Context.root_block.blocks[self._id]",
            "                self.is_rendered = False",
            "            except KeyError:",
            "                pass",
            "        return self",
            "",
            "    def get_block_name(self) -> str:",
            "        \"\"\"",
            "        Gets block's class name.",
            "",
            "        If it is template component it gets the parent's class name.",
            "",
            "        @return: class name",
            "        \"\"\"",
            "        return (",
            "            self.__class__.__base__.__name__.lower()",
            "            if hasattr(self, \"is_template\")",
            "            else self.__class__.__name__.lower()",
            "        )",
            "",
            "    def get_expected_parent(self) -> type[BlockContext] | None:",
            "        return None",
            "",
            "    def get_config(self):",
            "        config = {}",
            "        signature = inspect.signature(self.__class__.__init__)",
            "        for parameter in signature.parameters.values():",
            "            if hasattr(self, parameter.name):",
            "                value = getattr(self, parameter.name)",
            "                config[parameter.name] = utils.convert_to_dict_if_dataclass(value)",
            "        for e in self.events:",
            "            to_add = e.config_data()",
            "            if to_add:",
            "                config = {**to_add, **config}",
            "        config.pop(\"render\", None)",
            "        config = {**config, \"proxy_url\": self.proxy_url, \"name\": self.get_block_name()}",
            "        if (_selectable := getattr(self, \"_selectable\", None)) is not None:",
            "            config[\"_selectable\"] = _selectable",
            "        return config",
            "",
            "    @classmethod",
            "    def recover_kwargs(",
            "        cls, props: dict[str, Any], additional_keys: list[str] | None = None",
            "    ):",
            "        \"\"\"",
            "        Recovers kwargs from a dict of props.",
            "        \"\"\"",
            "        additional_keys = additional_keys or []",
            "        signature = inspect.signature(cls.__init__)",
            "        kwargs = {}",
            "        for parameter in signature.parameters.values():",
            "            if parameter.name in props and parameter.name not in additional_keys:",
            "                kwargs[parameter.name] = props[parameter.name]",
            "        return kwargs",
            "",
            "    def move_resource_to_block_cache(",
            "        self, url_or_file_path: str | Path | None",
            "    ) -> str | None:",
            "        \"\"\"Moves a file or downloads a file from a url to a block's cache directory, adds",
            "        to to the block's temp_files, and returns the path to the file in cache. This",
            "        ensures that the file is accessible to the Block and can be served to users.",
            "        \"\"\"",
            "        if url_or_file_path is None:",
            "            return None",
            "        if isinstance(url_or_file_path, Path):",
            "            url_or_file_path = str(url_or_file_path)",
            "",
            "        if client_utils.is_http_url_like(url_or_file_path):",
            "            temp_file_path = processing_utils.save_url_to_cache(",
            "                url_or_file_path, cache_dir=self.GRADIO_CACHE",
            "            )",
            "",
            "            self.temp_files.add(temp_file_path)",
            "        else:",
            "            url_or_file_path = str(utils.abspath(url_or_file_path))",
            "            if not utils.is_in_or_equal(url_or_file_path, self.GRADIO_CACHE):",
            "                temp_file_path = processing_utils.save_file_to_cache(",
            "                    url_or_file_path, cache_dir=self.GRADIO_CACHE",
            "                )",
            "            else:",
            "                temp_file_path = url_or_file_path",
            "            self.temp_files.add(temp_file_path)",
            "",
            "        return temp_file_path",
            "",
            "",
            "class BlockContext(Block):",
            "    def __init__(",
            "        self,",
            "        elem_id: str | None = None,",
            "        elem_classes: list[str] | str | None = None,",
            "        visible: bool = True,",
            "        render: bool = True,",
            "    ):",
            "        \"\"\"",
            "        Parameters:",
            "            elem_id: An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",
            "            elem_classes: An optional string or list of strings that are assigned as the class of this component in the HTML DOM. Can be used for targeting CSS styles.",
            "            visible: If False, this will be hidden but included in the Blocks config file (its visibility can later be updated).",
            "            render: If False, this will not be included in the Blocks config file at all.",
            "        \"\"\"",
            "        self.children: list[Block] = []",
            "        Block.__init__(",
            "            self,",
            "            elem_id=elem_id,",
            "            elem_classes=elem_classes,",
            "            visible=visible,",
            "            render=render,",
            "        )",
            "",
            "    TEMPLATE_DIR = \"./templates/\"",
            "    FRONTEND_DIR = \"../../frontend/\"",
            "",
            "    @property",
            "    def skip_api(self):",
            "        return True",
            "",
            "    @classmethod",
            "    def get_component_class_id(cls) -> str:",
            "        module_name = cls.__module__",
            "        module_path = sys.modules[module_name].__file__",
            "        module_hash = hashlib.md5(f\"{cls.__name__}_{module_path}\".encode()).hexdigest()",
            "        return module_hash",
            "",
            "    @property",
            "    def component_class_id(self):",
            "        return self.get_component_class_id()",
            "",
            "    def add_child(self, child: Block):",
            "        self.children.append(child)",
            "",
            "    def __enter__(self):",
            "        self.parent = Context.block",
            "        Context.block = self",
            "        return self",
            "",
            "    def add(self, child: Block):",
            "        child.parent = self",
            "        self.children.append(child)",
            "",
            "    def fill_expected_parents(self):",
            "        children = []",
            "        pseudo_parent = None",
            "        for child in self.children:",
            "            expected_parent = child.get_expected_parent()",
            "            if not expected_parent or isinstance(self, expected_parent):",
            "                pseudo_parent = None",
            "                children.append(child)",
            "            else:",
            "                if pseudo_parent is not None and isinstance(",
            "                    pseudo_parent, expected_parent",
            "                ):",
            "                    pseudo_parent.add_child(child)",
            "                else:",
            "                    pseudo_parent = expected_parent(render=False)",
            "                    pseudo_parent.parent = self",
            "                    children.append(pseudo_parent)",
            "                    pseudo_parent.add_child(child)",
            "                    if Context.root_block:",
            "                        Context.root_block.blocks[pseudo_parent._id] = pseudo_parent",
            "                child.parent = pseudo_parent",
            "        self.children = children",
            "",
            "    def __exit__(self, exc_type: type[BaseException] | None = None, *args):",
            "        Context.block = self.parent",
            "        if exc_type is not None:",
            "            return",
            "        if getattr(self, \"allow_expected_parents\", True):",
            "            self.fill_expected_parents()",
            "",
            "    def postprocess(self, y):",
            "        \"\"\"",
            "        Any postprocessing needed to be performed on a block context.",
            "        \"\"\"",
            "        return y",
            "",
            "",
            "class BlockFunction:",
            "    def __init__(",
            "        self,",
            "        fn: Callable | None,",
            "        inputs: list[Component],",
            "        outputs: list[Component],",
            "        preprocess: bool,",
            "        postprocess: bool,",
            "        inputs_as_dict: bool,",
            "        batch: bool = False,",
            "        max_batch_size: int = 4,",
            "        concurrency_limit: int | None | Literal[\"default\"] = \"default\",",
            "        concurrency_id: str | None = None,",
            "        tracks_progress: bool = False,",
            "    ):",
            "        self.fn = fn",
            "        self.inputs = inputs",
            "        self.outputs = outputs",
            "        self.preprocess = preprocess",
            "        self.postprocess = postprocess",
            "        self.tracks_progress = tracks_progress",
            "        self.concurrency_limit: int | None | Literal[\"default\"] = concurrency_limit",
            "        self.concurrency_id = concurrency_id or str(id(fn))",
            "        self.batch = batch",
            "        self.max_batch_size = max_batch_size",
            "        self.total_runtime = 0",
            "        self.total_runs = 0",
            "        self.inputs_as_dict = inputs_as_dict",
            "        self.name = getattr(fn, \"__name__\", \"fn\") if fn is not None else None",
            "        self.spaces_auto_wrap()",
            "",
            "    def spaces_auto_wrap(self):",
            "        if spaces is None:",
            "            return",
            "        if utils.get_space() is None:",
            "            return",
            "        self.fn = spaces.gradio_auto_wrap(self.fn)",
            "",
            "    def __str__(self):",
            "        return str(",
            "            {",
            "                \"fn\": self.name,",
            "                \"preprocess\": self.preprocess,",
            "                \"postprocess\": self.postprocess,",
            "            }",
            "        )",
            "",
            "    def __repr__(self):",
            "        return str(self)",
            "",
            "",
            "def postprocess_update_dict(",
            "    block: Component | BlockContext, update_dict: dict, postprocess: bool = True",
            "):",
            "    \"\"\"",
            "    Converts a dictionary of updates into a format that can be sent to the frontend to update the component.",
            "    E.g. {\"value\": \"2\", \"visible\": True, \"invalid_arg\": \"hello\"}",
            "    Into -> {\"__type__\": \"update\", \"value\": 2.0, \"visible\": True}",
            "    Parameters:",
            "        block: The Block that is being updated with this update dictionary.",
            "        update_dict: The original update dictionary",
            "        postprocess: Whether to postprocess the \"value\" key of the update dictionary.",
            "    \"\"\"",
            "    value = update_dict.pop(\"value\", components._Keywords.NO_VALUE)",
            "    update_dict = {k: getattr(block, k) for k in update_dict if hasattr(block, k)}",
            "    if value is not components._Keywords.NO_VALUE:",
            "        if postprocess:",
            "            update_dict[\"value\"] = block.postprocess(value)",
            "            if isinstance(update_dict[\"value\"], (GradioModel, GradioRootModel)):",
            "                update_dict[\"value\"] = update_dict[\"value\"].model_dump()",
            "        else:",
            "            update_dict[\"value\"] = value",
            "    update_dict[\"__type__\"] = \"update\"",
            "    return update_dict",
            "",
            "",
            "def convert_component_dict_to_list(",
            "    outputs_ids: list[int], predictions: dict",
            ") -> list | dict:",
            "    \"\"\"",
            "    Converts a dictionary of component updates into a list of updates in the order of",
            "    the outputs_ids and including every output component. Leaves other types of dictionaries unchanged.",
            "    E.g. {\"textbox\": \"hello\", \"number\": {\"__type__\": \"generic_update\", \"value\": \"2\"}}",
            "    Into -> [\"hello\", {\"__type__\": \"generic_update\"}, {\"__type__\": \"generic_update\", \"value\": \"2\"}]",
            "    \"\"\"",
            "    keys_are_blocks = [isinstance(key, Block) for key in predictions]",
            "    if all(keys_are_blocks):",
            "        reordered_predictions = [skip() for _ in outputs_ids]",
            "        for component, value in predictions.items():",
            "            if component._id not in outputs_ids:",
            "                raise ValueError(",
            "                    f\"Returned component {component} not specified as output of function.\"",
            "                )",
            "            output_index = outputs_ids.index(component._id)",
            "            reordered_predictions[output_index] = value",
            "        predictions = utils.resolve_singleton(reordered_predictions)",
            "    elif any(keys_are_blocks):",
            "        raise ValueError(",
            "            \"Returned dictionary included some keys as Components. Either all keys must be Components to assign Component values, or return a List of values to assign output values in order.\"",
            "        )",
            "    return predictions",
            "",
            "",
            "@document(\"launch\", \"queue\", \"integrate\", \"load\")",
            "class Blocks(BlockContext, BlocksEvents, metaclass=BlocksMeta):",
            "    \"\"\"",
            "    Blocks is Gradio's low-level API that allows you to create more custom web",
            "    applications and demos than Interfaces (yet still entirely in Python).",
            "",
            "",
            "    Compared to the Interface class, Blocks offers more flexibility and control over:",
            "    (1) the layout of components (2) the events that",
            "    trigger the execution of functions (3) data flows (e.g. inputs can trigger outputs,",
            "    which can trigger the next level of outputs). Blocks also offers ways to group",
            "    together related demos such as with tabs.",
            "",
            "",
            "    The basic usage of Blocks is as follows: create a Blocks object, then use it as a",
            "    context (with the \"with\" statement), and then define layouts, components, or events",
            "    within the Blocks context. Finally, call the launch() method to launch the demo.",
            "",
            "    Example:",
            "        import gradio as gr",
            "        def update(name):",
            "            return f\"Welcome to Gradio, {name}!\"",
            "",
            "        with gr.Blocks() as demo:",
            "            gr.Markdown(\"Start typing below and then click **Run** to see the output.\")",
            "            with gr.Row():",
            "                inp = gr.Textbox(placeholder=\"What is your name?\")",
            "                out = gr.Textbox()",
            "            btn = gr.Button(\"Run\")",
            "            btn.click(fn=update, inputs=inp, outputs=out)",
            "",
            "        demo.launch()",
            "    Demos: blocks_hello, blocks_flipper, blocks_speech_text_sentiment, generate_english_german",
            "    Guides: blocks-and-event-listeners, controlling-layout, state-in-blocks, custom-CSS-and-JS, using-blocks-like-functions",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        theme: Theme | str | None = None,",
            "        analytics_enabled: bool | None = None,",
            "        mode: str = \"blocks\",",
            "        title: str = \"Gradio\",",
            "        css: str | None = None,",
            "        js: str | None = None,",
            "        head: str | None = None,",
            "        fill_height: bool = False,",
            "        **kwargs,",
            "    ):",
            "        \"\"\"",
            "        Parameters:",
            "            theme: A Theme object or a string representing a theme. If a string, will look for a built-in theme with that name (e.g. \"soft\" or \"default\"), or will attempt to load a theme from the Hugging Face Hub (e.g. \"gradio/monochrome\"). If None, will use the Default theme.",
            "            analytics_enabled: Whether to allow basic telemetry. If None, will use GRADIO_ANALYTICS_ENABLED environment variable or default to True.",
            "            mode: A human-friendly name for the kind of Blocks or Interface being created. Used internally for analytics.",
            "            title: The tab title to display when this is opened in a browser window.",
            "            css: Custom css as a string or path to a css file. This css will be included in the demo webpage.",
            "            js: Custom js or path to js file to run when demo is first loaded. This javascript will be included in the demo webpage.",
            "            head: Custom html to insert into the head of the demo webpage. This can be used to add custom meta tags, scripts, stylesheets, etc. to the page.",
            "            fill_height: Whether to vertically expand top-level child components to the height of the window. If True, expansion occurs when the scale value of the child components >= 1.",
            "        \"\"\"",
            "        self.limiter = None",
            "        if theme is None:",
            "            theme = DefaultTheme()",
            "        elif isinstance(theme, str):",
            "            if theme.lower() in BUILT_IN_THEMES:",
            "                theme = BUILT_IN_THEMES[theme.lower()]",
            "            else:",
            "                try:",
            "                    theme = Theme.from_hub(theme)",
            "                except Exception as e:",
            "                    warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")",
            "                    theme = DefaultTheme()",
            "        if not isinstance(theme, Theme):",
            "            warnings.warn(\"Theme should be a class loaded from gradio.themes\")",
            "            theme = DefaultTheme()",
            "        self.theme: Theme = theme",
            "        self.theme_css = theme._get_theme_css()",
            "        self.stylesheets = theme._stylesheets",
            "        self.encrypt = False",
            "        self.share = False",
            "        self.enable_queue = True",
            "        self.max_threads = 40",
            "        self.pending_streams = defaultdict(dict)",
            "        self.pending_diff_streams = defaultdict(dict)",
            "        self.show_error = True",
            "        self.head = head",
            "        self.fill_height = fill_height",
            "        if css is not None and os.path.exists(css):",
            "            with open(css) as css_file:",
            "                self.css = css_file.read()",
            "        else:",
            "            self.css = css",
            "        if js is not None and os.path.exists(js):",
            "            with open(js) as js_file:",
            "                self.js = js_file.read()",
            "        else:",
            "            self.js = js",
            "",
            "        # For analytics_enabled and allow_flagging: (1) first check for",
            "        # parameter, (2) check for env variable, (3) default to True/\"manual\"",
            "        self.analytics_enabled = (",
            "            analytics_enabled",
            "            if analytics_enabled is not None",
            "            else analytics.analytics_enabled()",
            "        )",
            "        if self.analytics_enabled:",
            "            if not wasm_utils.IS_WASM:",
            "                t = threading.Thread(target=analytics.version_check)",
            "                t.start()",
            "        else:",
            "            os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"True\"",
            "        super().__init__(render=False, **kwargs)",
            "        self.blocks: dict[int, Component | Block] = {}",
            "        self.fns: list[BlockFunction] = []",
            "        self.dependencies = []",
            "        self.mode = mode",
            "",
            "        self.is_running = False",
            "        self.local_url = None",
            "        self.share_url = None",
            "        self.width = None",
            "        self.height = None",
            "        self.api_open = utils.get_space() is None",
            "",
            "        self.space_id = utils.get_space()",
            "        self.favicon_path = None",
            "        self.auth = None",
            "        self.dev_mode = bool(os.getenv(\"GRADIO_WATCH_DIRS\", \"\"))",
            "        self.app_id = random.getrandbits(64)",
            "        self.temp_file_sets = []",
            "        self.title = title",
            "        self.show_api = not wasm_utils.IS_WASM",
            "",
            "        # Only used when an Interface is loaded from a config",
            "        self.predict = None",
            "        self.input_components = None",
            "        self.output_components = None",
            "        self.__name__ = None",
            "        self.api_mode = None",
            "",
            "        self.progress_tracking = None",
            "        self.ssl_verify = True",
            "",
            "        self.allowed_paths = []",
            "        self.blocked_paths = []",
            "        self.root_path = os.environ.get(\"GRADIO_ROOT_PATH\", \"\")",
            "        self.proxy_urls = set()",
            "",
            "        if self.analytics_enabled:",
            "            is_custom_theme = not any(",
            "                self.theme.to_dict() == built_in_theme.to_dict()",
            "                for built_in_theme in BUILT_IN_THEMES.values()",
            "            )",
            "            data = {",
            "                \"mode\": self.mode,",
            "                \"custom_css\": self.css is not None,",
            "                \"theme\": self.theme.name,",
            "                \"is_custom_theme\": is_custom_theme,",
            "                \"version\": get_package_version(),",
            "            }",
            "            analytics.initiated_analytics(data)",
            "",
            "        self.queue()",
            "",
            "    def get_component(self, id: int) -> Component | BlockContext:",
            "        comp = self.blocks[id]",
            "        if not isinstance(comp, (components.Component, BlockContext)):",
            "            raise TypeError(f\"Block with id {id} is not a Component or BlockContext\")",
            "        return comp",
            "",
            "    @property",
            "    def _is_running_in_reload_thread(self):",
            "        from gradio.cli.commands.reload import reload_thread",
            "",
            "        return getattr(reload_thread, \"running_reload\", False)",
            "",
            "    @classmethod",
            "    def from_config(",
            "        cls,",
            "        config: dict,",
            "        fns: list[Callable],",
            "        proxy_url: str,",
            "    ) -> Blocks:",
            "        \"\"\"",
            "        Factory method that creates a Blocks from a config and list of functions. Used",
            "        internally by the gradio.external.load() method.",
            "",
            "        Parameters:",
            "        config: a dictionary containing the configuration of the Blocks.",
            "        fns: a list of functions that are used in the Blocks. Must be in the same order as the dependencies in the config.",
            "        proxy_url: an external url to use as a root URL when serving files for components in the Blocks.",
            "        \"\"\"",
            "        config = copy.deepcopy(config)",
            "        components_config = config[\"components\"]",
            "        theme = config.get(\"theme\", \"default\")",
            "        original_mapping: dict[int, Block] = {}",
            "        proxy_urls = {proxy_url}",
            "",
            "        def get_block_instance(id: int) -> Block:",
            "            for block_config in components_config:",
            "                if block_config[\"id\"] == id:",
            "                    break",
            "            else:",
            "                raise ValueError(f\"Cannot find block with id {id}\")",
            "            cls = component_or_layout_class(block_config[\"type\"])",
            "",
            "            # If a Gradio app B is loaded into a Gradio app A, and B itself loads a",
            "            # Gradio app C, then the proxy_urls of the components in A need to be the",
            "            # URL of C, not B. The else clause below handles this case.",
            "            if block_config[\"props\"].get(\"proxy_url\") is None:",
            "                block_config[\"props\"][\"proxy_url\"] = f\"{proxy_url}/\"",
            "            postprocessed_value = block_config[\"props\"].pop(\"value\", None)",
            "",
            "            constructor_args = cls.recover_kwargs(block_config[\"props\"])",
            "            block = cls(**constructor_args)",
            "            if postprocessed_value is not None:",
            "                block.value = postprocessed_value  # type: ignore",
            "",
            "            block_proxy_url = block_config[\"props\"][\"proxy_url\"]",
            "            block.proxy_url = block_proxy_url",
            "            proxy_urls.add(block_proxy_url)",
            "            if (",
            "                _selectable := block_config[\"props\"].pop(\"_selectable\", None)",
            "            ) is not None:",
            "                block._selectable = _selectable  # type: ignore",
            "",
            "            return block",
            "",
            "        def iterate_over_children(children_list):",
            "            for child_config in children_list:",
            "                id = child_config[\"id\"]",
            "                block = get_block_instance(id)",
            "",
            "                original_mapping[id] = block",
            "",
            "                children = child_config.get(\"children\")",
            "                if children is not None:",
            "                    if not isinstance(block, BlockContext):",
            "                        raise ValueError(",
            "                            f\"Invalid config, Block with id {id} has children but is not a BlockContext.\"",
            "                        )",
            "                    with block:",
            "                        iterate_over_children(children)",
            "",
            "        derived_fields = [\"types\"]",
            "",
            "        with Blocks(theme=theme) as blocks:",
            "            # ID 0 should be the root Blocks component",
            "            original_mapping[0] = Context.root_block or blocks",
            "",
            "            iterate_over_children(config[\"layout\"][\"children\"])",
            "",
            "            first_dependency = None",
            "",
            "            # add the event triggers",
            "            for dependency, fn in zip(config[\"dependencies\"], fns):",
            "                # We used to add a \"fake_event\" to the config to cache examples",
            "                # without removing it. This was causing bugs in calling gr.load",
            "                # We fixed the issue by removing \"fake_event\" from the config in examples.py",
            "                # but we still need to skip these events when loading the config to support",
            "                # older demos",
            "                if \"trigger\" in dependency and dependency[\"trigger\"] == \"fake_event\":",
            "                    continue",
            "                for field in derived_fields:",
            "                    dependency.pop(field, None)",
            "",
            "                # older versions had a separate trigger field, but now it is part of the",
            "                # targets field",
            "                _targets = dependency.pop(\"targets\")",
            "                trigger = dependency.pop(\"trigger\", None)",
            "                targets = [",
            "                    getattr(",
            "                        original_mapping[",
            "                            target if isinstance(target, int) else target[0]",
            "                        ],",
            "                        trigger if isinstance(target, int) else target[1],",
            "                    )",
            "                    for target in _targets",
            "                ]",
            "                dependency.pop(\"backend_fn\")",
            "                dependency.pop(\"documentation\", None)",
            "                dependency[\"inputs\"] = [",
            "                    original_mapping[i] for i in dependency[\"inputs\"]",
            "                ]",
            "                dependency[\"outputs\"] = [",
            "                    original_mapping[o] for o in dependency[\"outputs\"]",
            "                ]",
            "                dependency.pop(\"status_tracker\", None)",
            "                dependency[\"preprocess\"] = False",
            "                dependency[\"postprocess\"] = False",
            "                targets = [",
            "                    EventListenerMethod(",
            "                        t.__self__ if t.has_trigger else None, t.event_name",
            "                    )",
            "                    for t in targets",
            "                ]",
            "                dependency = blocks.set_event_trigger(",
            "                    targets=targets, fn=fn, **dependency",
            "                )[0]",
            "                if first_dependency is None:",
            "                    first_dependency = dependency",
            "",
            "            # Allows some use of Interface-specific methods with loaded Spaces",
            "            if first_dependency and Context.root_block:",
            "                blocks.predict = [fns[0]]",
            "                blocks.input_components = [",
            "                    Context.root_block.blocks[i] for i in first_dependency[\"inputs\"]",
            "                ]",
            "                blocks.output_components = [",
            "                    Context.root_block.blocks[o] for o in first_dependency[\"outputs\"]",
            "                ]",
            "                blocks.__name__ = \"Interface\"",
            "                blocks.api_mode = True",
            "        blocks.proxy_urls = proxy_urls",
            "        return blocks",
            "",
            "    def __str__(self):",
            "        return self.__repr__()",
            "",
            "    def __repr__(self):",
            "        num_backend_fns = len([d for d in self.dependencies if d[\"backend_fn\"]])",
            "        repr = f\"Gradio Blocks instance: {num_backend_fns} backend functions\"",
            "        repr += f\"\\n{'-' * len(repr)}\"",
            "        for d, dependency in enumerate(self.dependencies):",
            "            if dependency[\"backend_fn\"]:",
            "                repr += f\"\\nfn_index={d}\"",
            "                repr += \"\\n inputs:\"",
            "                for input_id in dependency[\"inputs\"]:",
            "                    block = self.blocks[input_id]",
            "                    repr += f\"\\n |-{block}\"",
            "                repr += \"\\n outputs:\"",
            "                for output_id in dependency[\"outputs\"]:",
            "                    block = self.blocks[output_id]",
            "                    repr += f\"\\n |-{block}\"",
            "        return repr",
            "",
            "    @property",
            "    def expects_oauth(self):",
            "        \"\"\"Return whether the app expects user to authenticate via OAuth.\"\"\"",
            "        return any(",
            "            isinstance(block, (components.LoginButton, components.LogoutButton))",
            "            for block in self.blocks.values()",
            "        )",
            "",
            "    def set_event_trigger(",
            "        self,",
            "        targets: Sequence[EventListenerMethod],",
            "        fn: Callable | None,",
            "        inputs: Component | list[Component] | set[Component] | None,",
            "        outputs: Component | list[Component] | None,",
            "        preprocess: bool = True,",
            "        postprocess: bool = True,",
            "        scroll_to_output: bool = False,",
            "        show_progress: Literal[\"full\", \"minimal\", \"hidden\"] = \"full\",",
            "        api_name: str | None | Literal[False] = None,",
            "        js: str | None = None,",
            "        no_target: bool = False,",
            "        queue: bool | None = None,",
            "        batch: bool = False,",
            "        max_batch_size: int = 4,",
            "        cancels: list[int] | None = None,",
            "        every: float | None = None,",
            "        collects_event_data: bool | None = None,",
            "        trigger_after: int | None = None,",
            "        trigger_only_on_success: bool = False,",
            "        trigger_mode: Literal[\"once\", \"multiple\", \"always_last\"] | None = \"once\",",
            "        concurrency_limit: int | None | Literal[\"default\"] = \"default\",",
            "        concurrency_id: str | None = None,",
            "        show_api: bool = True,",
            "    ) -> tuple[dict[str, Any], int]:",
            "        \"\"\"",
            "        Adds an event to the component's dependencies.",
            "        Parameters:",
            "            targets: a list of EventListenerMethod objects that define the event trigger",
            "            fn: Callable function",
            "            inputs: input list",
            "            outputs: output list",
            "            preprocess: whether to run the preprocess methods of components",
            "            postprocess: whether to run the postprocess methods of components",
            "            scroll_to_output: whether to scroll to output of dependency on trigger",
            "            show_progress: whether to show progress animation while running.",
            "            api_name: defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",
            "            js: Optional frontend js method to run before running 'fn'. Input arguments for js method are values of 'inputs' and 'outputs', return should be a list of values for output components",
            "            no_target: if True, sets \"targets\" to [], used for Blocks \"load\" event",
            "            queue: If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",
            "            batch: whether this function takes in a batch of inputs",
            "            max_batch_size: the maximum batch size to send to the function",
            "            cancels: a list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.",
            "            every: Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds.",
            "            collects_event_data: whether to collect event data for this event",
            "            trigger_after: if set, this event will be triggered after 'trigger_after' function index",
            "            trigger_only_on_success: if True, this event will only be triggered if the previous event was successful (only applies if `trigger_after` is set)",
            "            trigger_mode: If \"once\" (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to \"multiple\", unlimited submissions are allowed while pending, and \"always_last\" (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",
            "            concurrency_limit: If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to \"default\" to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `queue()`, which itself is 1 by default).",
            "            concurrency_id: If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",
            "            show_api: whether to show this event in the \"view API\" page of the Gradio app, or in the \".view_api()\" method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps to use this event. If fn is None, show_api will automatically be set to False.",
            "        Returns: dependency information, dependency index",
            "        \"\"\"",
            "        # Support for singular parameter",
            "        _targets = [",
            "            (",
            "                target.block._id if target.block and not no_target else None,",
            "                target.event_name,",
            "            )",
            "            for target in targets",
            "        ]",
            "        if isinstance(inputs, set):",
            "            inputs_as_dict = True",
            "            inputs = sorted(inputs, key=lambda x: x._id)",
            "        else:",
            "            inputs_as_dict = False",
            "            if inputs is None:",
            "                inputs = []",
            "            elif not isinstance(inputs, list):",
            "                inputs = [inputs]",
            "",
            "        if isinstance(outputs, set):",
            "            outputs = sorted(outputs, key=lambda x: x._id)",
            "        elif outputs is None:",
            "            outputs = []",
            "        elif not isinstance(outputs, list):",
            "            outputs = [outputs]",
            "",
            "        if fn is not None and not cancels:",
            "            check_function_inputs_match(fn, inputs, inputs_as_dict)",
            "        if every is not None and every <= 0:",
            "            raise ValueError(\"Parameter every must be positive or None\")",
            "        if every and batch:",
            "            raise ValueError(",
            "                f\"Cannot run event in a batch and every {every} seconds. \"",
            "                \"Either batch is True or every is non-zero but not both.\"",
            "            )",
            "",
            "        if every and fn:",
            "            fn = get_continuous_fn(fn, every)",
            "        elif every:",
            "            raise ValueError(\"Cannot set a value for `every` without a `fn`.\")",
            "        if every and concurrency_limit is not None:",
            "            if concurrency_limit == \"default\":",
            "                concurrency_limit = None",
            "            else:",
            "                raise ValueError(",
            "                    \"Cannot set a value for `concurrency_limit` with `every`.\"",
            "                )",
            "",
            "        if _targets[0][1] in [\"change\", \"key_up\"] and trigger_mode is None:",
            "            trigger_mode = \"always_last\"",
            "        elif trigger_mode is None:",
            "            trigger_mode = \"once\"",
            "        elif trigger_mode not in [\"once\", \"multiple\", \"always_last\"]:",
            "            raise ValueError(",
            "                f\"Invalid value for parameter `trigger_mode`: {trigger_mode}. Please choose from: {['once', 'multiple', 'always_last']}\"",
            "            )",
            "",
            "        _, progress_index, event_data_index = (",
            "            special_args(fn) if fn else (None, None, None)",
            "        )",
            "        self.fns.append(",
            "            BlockFunction(",
            "                fn,",
            "                inputs,",
            "                outputs,",
            "                preprocess,",
            "                postprocess,",
            "                inputs_as_dict=inputs_as_dict,",
            "                concurrency_limit=concurrency_limit,",
            "                concurrency_id=concurrency_id,",
            "                batch=batch,",
            "                max_batch_size=max_batch_size,",
            "                tracks_progress=progress_index is not None,",
            "            )",
            "        )",
            "",
            "        # If api_name is None or empty string, use the function name",
            "        if api_name is None or isinstance(api_name, str) and api_name.strip() == \"\":",
            "            if fn is not None:",
            "                if not hasattr(fn, \"__name__\"):",
            "                    if hasattr(fn, \"__class__\") and hasattr(fn.__class__, \"__name__\"):",
            "                        name = fn.__class__.__name__",
            "                    else:",
            "                        name = \"unnamed\"",
            "                else:",
            "                    name = fn.__name__",
            "                api_name = \"\".join(",
            "                    [s for s in name if s not in set(string.punctuation) - {\"-\", \"_\"}]",
            "                )",
            "            elif js is not None:",
            "                api_name = \"js_fn\"",
            "                show_api = False",
            "            else:",
            "                api_name = \"unnamed\"",
            "                show_api = False",
            "",
            "        if api_name is not False:",
            "            api_name = utils.append_unique_suffix(",
            "                api_name, [dep[\"api_name\"] for dep in self.dependencies]",
            "            )",
            "        else:",
            "            show_api = False",
            "",
            "        # The `show_api` parameter is False if: (1) the user explicitly sets it (2) the user sets `api_name` to False",
            "        # or (3) the user sets `fn` to None (there's no backend function)",
            "",
            "        if collects_event_data is None:",
            "            collects_event_data = event_data_index is not None",
            "",
            "        dependency = {",
            "            \"targets\": _targets,",
            "            \"inputs\": [block._id for block in inputs],",
            "            \"outputs\": [block._id for block in outputs],",
            "            \"backend_fn\": fn is not None,",
            "            \"js\": js,",
            "            \"queue\": False if fn is None else queue,",
            "            \"api_name\": api_name,",
            "            \"scroll_to_output\": False if utils.get_space() else scroll_to_output,",
            "            \"show_progress\": show_progress,",
            "            \"every\": every,",
            "            \"batch\": batch,",
            "            \"max_batch_size\": max_batch_size,",
            "            \"cancels\": cancels or [],",
            "            \"types\": {",
            "                \"continuous\": bool(every),",
            "                \"generator\": inspect.isgeneratorfunction(fn)",
            "                or inspect.isasyncgenfunction(fn)",
            "                or bool(every),",
            "            },",
            "            \"collects_event_data\": collects_event_data,",
            "            \"trigger_after\": trigger_after,",
            "            \"trigger_only_on_success\": trigger_only_on_success,",
            "            \"trigger_mode\": trigger_mode,",
            "            \"show_api\": show_api,",
            "        }",
            "        self.dependencies.append(dependency)",
            "        return dependency, len(self.dependencies) - 1",
            "",
            "    def render(self):",
            "        if Context.root_block is not None:",
            "            if self._id in Context.root_block.blocks:",
            "                raise DuplicateBlockError(",
            "                    f\"A block with id: {self._id} has already been rendered in the current Blocks.\"",
            "                )",
            "            overlapping_ids = set(Context.root_block.blocks).intersection(self.blocks)",
            "            for id in overlapping_ids:",
            "                # State components are allowed to be reused between Blocks",
            "                if not isinstance(self.blocks[id], components.State):",
            "                    raise DuplicateBlockError(",
            "                        \"At least one block in this Blocks has already been rendered.\"",
            "                    )",
            "",
            "            Context.root_block.blocks.update(self.blocks)",
            "            Context.root_block.fns.extend(self.fns)",
            "            dependency_offset = len(Context.root_block.dependencies)",
            "            for i, dependency in enumerate(self.dependencies):",
            "                api_name = dependency[\"api_name\"]",
            "                if api_name is not None and api_name is not False:",
            "                    api_name_ = utils.append_unique_suffix(",
            "                        api_name,",
            "                        [dep[\"api_name\"] for dep in Context.root_block.dependencies],",
            "                    )",
            "                    if api_name != api_name_:",
            "                        dependency[\"api_name\"] = api_name_",
            "                dependency[\"cancels\"] = [",
            "                    c + dependency_offset for c in dependency[\"cancels\"]",
            "                ]",
            "                if dependency.get(\"trigger_after\") is not None:",
            "                    dependency[\"trigger_after\"] += dependency_offset",
            "                # Recreate the cancel function so that it has the latest",
            "                # dependency fn indices. This is necessary to properly cancel",
            "                # events in the backend",
            "                if dependency[\"cancels\"]:",
            "                    updated_cancels = [",
            "                        Context.root_block.dependencies[i]",
            "                        for i in dependency[\"cancels\"]",
            "                    ]",
            "                    new_fn = BlockFunction(",
            "                        get_cancel_function(updated_cancels)[0],",
            "                        [],",
            "                        [],",
            "                        False,",
            "                        True,",
            "                        False,",
            "                    )",
            "                    Context.root_block.fns[dependency_offset + i] = new_fn",
            "                Context.root_block.dependencies.append(dependency)",
            "            Context.root_block.temp_file_sets.extend(self.temp_file_sets)",
            "            Context.root_block.proxy_urls.update(self.proxy_urls)",
            "",
            "        if Context.block is not None:",
            "            Context.block.children.extend(self.children)",
            "        return self",
            "",
            "    def is_callable(self, fn_index: int = 0) -> bool:",
            "        \"\"\"Checks if a particular Blocks function is callable (i.e. not stateful or a generator).\"\"\"",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "",
            "        if inspect.isasyncgenfunction(block_fn.fn):",
            "            return False",
            "        if inspect.isgeneratorfunction(block_fn.fn):",
            "            return False",
            "        for input_id in dependency[\"inputs\"]:",
            "            block = self.blocks[input_id]",
            "            if getattr(block, \"stateful\", False):",
            "                return False",
            "        for output_id in dependency[\"outputs\"]:",
            "            block = self.blocks[output_id]",
            "            if getattr(block, \"stateful\", False):",
            "                return False",
            "",
            "        return True",
            "",
            "    def __call__(self, *inputs, fn_index: int = 0, api_name: str | None = None):",
            "        \"\"\"",
            "        Allows Blocks objects to be called as functions. Supply the parameters to the",
            "        function as positional arguments. To choose which function to call, use the",
            "        fn_index parameter, which must be a keyword argument.",
            "",
            "        Parameters:",
            "        *inputs: the parameters to pass to the function",
            "        fn_index: the index of the function to call (defaults to 0, which for Interfaces, is the default prediction function)",
            "        api_name: The api_name of the dependency to call. Will take precedence over fn_index.",
            "        \"\"\"",
            "        if api_name is not None:",
            "            inferred_fn_index = next(",
            "                (",
            "                    i",
            "                    for i, d in enumerate(self.dependencies)",
            "                    if d.get(\"api_name\") == api_name",
            "                ),",
            "                None,",
            "            )",
            "            if inferred_fn_index is None:",
            "                raise InvalidApiNameError(",
            "                    f\"Cannot find a function with api_name {api_name}\"",
            "                )",
            "            fn_index = inferred_fn_index",
            "        if not (self.is_callable(fn_index)):",
            "            raise ValueError(",
            "                \"This function is not callable because it is either stateful or is a generator. Please use the .launch() method instead to create an interactive user interface.\"",
            "            )",
            "",
            "        inputs = list(inputs)",
            "        processed_inputs = self.serialize_data(fn_index, inputs)",
            "        batch = self.dependencies[fn_index][\"batch\"]",
            "        if batch:",
            "            processed_inputs = [[inp] for inp in processed_inputs]",
            "",
            "        outputs = client_utils.synchronize_async(",
            "            self.process_api,",
            "            fn_index=fn_index,",
            "            inputs=processed_inputs,",
            "            request=None,",
            "            state={},",
            "            explicit_call=True,",
            "        )",
            "        outputs = outputs[\"data\"]",
            "",
            "        if batch:",
            "            outputs = [out[0] for out in outputs]",
            "",
            "        outputs = self.deserialize_data(fn_index, outputs)",
            "        processed_outputs = utils.resolve_singleton(outputs)",
            "",
            "        return processed_outputs",
            "",
            "    async def call_function(",
            "        self,",
            "        fn_index: int,",
            "        processed_input: list[Any],",
            "        iterator: AsyncIterator[Any] | None = None,",
            "        requests: routes.Request | list[routes.Request] | None = None,",
            "        event_id: str | None = None,",
            "        event_data: EventData | None = None,",
            "        in_event_listener: bool = False,",
            "    ):",
            "        \"\"\"",
            "        Calls function with given index and preprocessed input, and measures process time.",
            "        Parameters:",
            "            fn_index: index of function to call",
            "            processed_input: preprocessed input to pass to function",
            "            iterator: iterator to use if function is a generator",
            "            requests: requests to pass to function",
            "            event_id: id of event in queue",
            "            event_data: data associated with event trigger",
            "        \"\"\"",
            "        block_fn = self.fns[fn_index]",
            "        if not block_fn.fn:",
            "            raise IndexError(f\"function with index {fn_index} not defined.\")",
            "        is_generating = False",
            "        request = requests[0] if isinstance(requests, list) else requests",
            "        start = time.time()",
            "",
            "        fn = utils.get_function_with_locals(",
            "            fn=block_fn.fn,",
            "            blocks=self,",
            "            event_id=event_id,",
            "            in_event_listener=in_event_listener,",
            "            request=request,",
            "        )",
            "",
            "        if iterator is None:  # If not a generator function that has already run",
            "            if block_fn.inputs_as_dict:",
            "                processed_input = [dict(zip(block_fn.inputs, processed_input))]",
            "",
            "            processed_input, progress_index, _ = special_args(",
            "                block_fn.fn, processed_input, request, event_data",
            "            )",
            "            progress_tracker = (",
            "                processed_input[progress_index] if progress_index is not None else None",
            "            )",
            "",
            "            if progress_tracker is not None and progress_index is not None:",
            "                progress_tracker, fn = create_tracker(fn, progress_tracker.track_tqdm)",
            "                processed_input[progress_index] = progress_tracker",
            "",
            "            if inspect.iscoroutinefunction(fn):",
            "                prediction = await fn(*processed_input)",
            "            else:",
            "                prediction = await anyio.to_thread.run_sync(",
            "                    fn, *processed_input, limiter=self.limiter",
            "                )",
            "        else:",
            "            prediction = None",
            "",
            "        if inspect.isgeneratorfunction(fn) or inspect.isasyncgenfunction(fn):",
            "            try:",
            "                if iterator is None:",
            "                    iterator = cast(AsyncIterator[Any], prediction)",
            "                if inspect.isgenerator(iterator):",
            "                    iterator = utils.SyncToAsyncIterator(iterator, self.limiter)",
            "                prediction = await utils.async_iteration(iterator)",
            "                is_generating = True",
            "            except StopAsyncIteration:",
            "                n_outputs = len(self.dependencies[fn_index].get(\"outputs\"))",
            "                prediction = (",
            "                    components._Keywords.FINISHED_ITERATING",
            "                    if n_outputs == 1",
            "                    else (components._Keywords.FINISHED_ITERATING,) * n_outputs",
            "                )",
            "                iterator = None",
            "",
            "        duration = time.time() - start",
            "",
            "        return {",
            "            \"prediction\": prediction,",
            "            \"duration\": duration,",
            "            \"is_generating\": is_generating,",
            "            \"iterator\": iterator,",
            "        }",
            "",
            "    def serialize_data(self, fn_index: int, inputs: list[Any]) -> list[Any]:",
            "        dependency = self.dependencies[fn_index]",
            "        processed_input = []",
            "",
            "        def format_file(s):",
            "            return FileData(path=s).model_dump()",
            "",
            "        for i, input_id in enumerate(dependency[\"inputs\"]):",
            "            try:",
            "                block = self.blocks[input_id]",
            "            except KeyError as e:",
            "                raise InvalidBlockError(",
            "                    f\"Input component with id {input_id} used in {dependency['trigger']}() event is not defined in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"",
            "                ) from e",
            "            if not isinstance(block, components.Component):",
            "                raise InvalidComponentError(",
            "                    f\"{block.__class__} Component with id {input_id} not a valid input component.\"",
            "                )",
            "            api_info = block.api_info()",
            "            if client_utils.value_is_file(api_info):",
            "                serialized_input = client_utils.traverse(",
            "                    inputs[i],",
            "                    format_file,",
            "                    lambda s: client_utils.is_filepath(s) or client_utils.is_url(s),",
            "                )",
            "            else:",
            "                serialized_input = inputs[i]",
            "            processed_input.append(serialized_input)",
            "",
            "        return processed_input",
            "",
            "    def deserialize_data(self, fn_index: int, outputs: list[Any]) -> list[Any]:",
            "        dependency = self.dependencies[fn_index]",
            "        predictions = []",
            "",
            "        for o, output_id in enumerate(dependency[\"outputs\"]):",
            "            try:",
            "                block = self.blocks[output_id]",
            "            except KeyError as e:",
            "                raise InvalidBlockError(",
            "                    f\"Output component with id {output_id} used in {dependency['trigger']}() event not found in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"",
            "                ) from e",
            "            if not isinstance(block, components.Component):",
            "                raise InvalidComponentError(",
            "                    f\"{block.__class__} Component with id {output_id} not a valid output component.\"",
            "                )",
            "",
            "            deserialized = client_utils.traverse(",
            "                outputs[o], lambda s: s[\"path\"], client_utils.is_file_obj",
            "            )",
            "            predictions.append(deserialized)",
            "",
            "        return predictions",
            "",
            "    def validate_inputs(self, fn_index: int, inputs: list[Any]):",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "",
            "        dep_inputs = dependency[\"inputs\"]",
            "",
            "        # This handles incorrect inputs when args are changed by a JS function",
            "        # Only check not enough args case, ignore extra arguments (for now)",
            "        # TODO: make this stricter?",
            "        if len(inputs) < len(dep_inputs):",
            "            name = (",
            "                f\" ({block_fn.name})\"",
            "                if block_fn.name and block_fn.name != \"<lambda>\"",
            "                else \"\"",
            "            )",
            "",
            "            wanted_args = []",
            "            received_args = []",
            "            for input_id in dep_inputs:",
            "                block = self.blocks[input_id]",
            "                wanted_args.append(str(block))",
            "            for inp in inputs:",
            "                v = f'\"{inp}\"' if isinstance(inp, str) else str(inp)",
            "                received_args.append(v)",
            "",
            "            wanted = \", \".join(wanted_args)",
            "            received = \", \".join(received_args)",
            "",
            "            # JS func didn't pass enough arguments",
            "            raise ValueError(",
            "                f\"\"\"An event handler{name} didn't receive enough input values (needed: {len(dep_inputs)}, got: {len(inputs)}).",
            "Check if the event handler calls a Javascript function, and make sure its return value is correct.",
            "Wanted inputs:",
            "    [{wanted}]",
            "Received inputs:",
            "    [{received}]\"\"\"",
            "            )",
            "",
            "    def preprocess_data(",
            "        self,",
            "        fn_index: int,",
            "        inputs: list[Any],",
            "        state: SessionState | None,",
            "        explicit_call: bool = False,",
            "    ):",
            "        state = state or SessionState(self)",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "",
            "        self.validate_inputs(fn_index, inputs)",
            "",
            "        if block_fn.preprocess:",
            "            processed_input = []",
            "            for i, input_id in enumerate(dependency[\"inputs\"]):",
            "                try:",
            "                    block = self.blocks[input_id]",
            "                except KeyError as e:",
            "                    raise InvalidBlockError(",
            "                        f\"Input component with id {input_id} used in {dependency['trigger']}() event not found in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"",
            "                    ) from e",
            "                if not isinstance(block, components.Component):",
            "                    raise InvalidComponentError(",
            "                        f\"{block.__class__} Component with id {input_id} not a valid input component.\"",
            "                    )",
            "                if getattr(block, \"stateful\", False):",
            "                    processed_input.append(state[input_id])",
            "                else:",
            "                    if input_id in state:",
            "                        block = state[input_id]",
            "                    inputs_cached = processing_utils.move_files_to_cache(",
            "                        inputs[i],",
            "                        block,",
            "                        add_urls=True,",
            "                        check_in_upload_folder=not explicit_call,",
            "                    )",
            "                    if getattr(block, \"data_model\", None) and inputs_cached is not None:",
            "                        if issubclass(block.data_model, GradioModel):  # type: ignore",
            "                            inputs_cached = block.data_model(**inputs_cached)  # type: ignore",
            "                        elif issubclass(block.data_model, GradioRootModel):  # type: ignore",
            "                            inputs_cached = block.data_model(root=inputs_cached)  # type: ignore",
            "                    processed_input.append(block.preprocess(inputs_cached))",
            "        else:",
            "            processed_input = inputs",
            "        return processed_input",
            "",
            "    def validate_outputs(self, fn_index: int, predictions: Any | list[Any]):",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "",
            "        dep_outputs = dependency[\"outputs\"]",
            "",
            "        if not isinstance(predictions, (list, tuple)):",
            "            predictions = [predictions]",
            "",
            "        if len(predictions) < len(dep_outputs):",
            "            name = (",
            "                f\" ({block_fn.name})\"",
            "                if block_fn.name and block_fn.name != \"<lambda>\"",
            "                else \"\"",
            "            )",
            "",
            "            wanted_args = []",
            "            received_args = []",
            "            for output_id in dep_outputs:",
            "                block = self.blocks[output_id]",
            "                wanted_args.append(str(block))",
            "            for pred in predictions:",
            "                v = f'\"{pred}\"' if isinstance(pred, str) else str(pred)",
            "                received_args.append(v)",
            "",
            "            wanted = \", \".join(wanted_args)",
            "            received = \", \".join(received_args)",
            "",
            "            raise ValueError(",
            "                f\"\"\"An event handler{name} didn't receive enough output values (needed: {len(dep_outputs)}, received: {len(predictions)}).",
            "Wanted outputs:",
            "    [{wanted}]",
            "Received outputs:",
            "    [{received}]\"\"\"",
            "            )",
            "",
            "    def postprocess_data(",
            "        self, fn_index: int, predictions: list | dict, state: SessionState | None",
            "    ):",
            "        state = state or SessionState(self)",
            "        block_fn = self.fns[fn_index]",
            "        dependency = self.dependencies[fn_index]",
            "        batch = dependency[\"batch\"]",
            "",
            "        if isinstance(predictions, dict) and len(predictions) > 0:",
            "            predictions = convert_component_dict_to_list(",
            "                dependency[\"outputs\"], predictions",
            "            )",
            "",
            "        if len(dependency[\"outputs\"]) == 1 and not (batch):",
            "            predictions = [",
            "                predictions,",
            "            ]",
            "",
            "        self.validate_outputs(fn_index, predictions)  # type: ignore",
            "",
            "        output = []",
            "        for i, output_id in enumerate(dependency[\"outputs\"]):",
            "            try:",
            "                if predictions[i] is components._Keywords.FINISHED_ITERATING:",
            "                    output.append(None)",
            "                    continue",
            "            except (IndexError, KeyError) as err:",
            "                raise ValueError(",
            "                    \"Number of output components does not match number \"",
            "                    f\"of values returned from from function {block_fn.name}\"",
            "                ) from err",
            "",
            "            try:",
            "                block = self.blocks[output_id]",
            "            except KeyError as e:",
            "                raise InvalidBlockError(",
            "                    f\"Output component with id {output_id} used in {dependency['trigger']}() event not found in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"",
            "                ) from e",
            "",
            "            if getattr(block, \"stateful\", False):",
            "                if not utils.is_update(predictions[i]):",
            "                    state[output_id] = predictions[i]",
            "                output.append(None)",
            "            else:",
            "                prediction_value = predictions[i]",
            "                if utils.is_update(",
            "                    prediction_value",
            "                ):  # if update is passed directly (deprecated), remove Nones",
            "                    prediction_value = utils.delete_none(",
            "                        prediction_value, skip_value=True",
            "                    )",
            "",
            "                if isinstance(prediction_value, Block):",
            "                    prediction_value = prediction_value.constructor_args.copy()",
            "                    prediction_value[\"__type__\"] = \"update\"",
            "                if utils.is_update(prediction_value):",
            "                    if output_id in state:",
            "                        kwargs = state[output_id].constructor_args.copy()",
            "                    else:",
            "                        kwargs = self.blocks[output_id].constructor_args.copy()",
            "                    kwargs.update(prediction_value)",
            "                    kwargs.pop(\"value\", None)",
            "                    kwargs.pop(\"__type__\")",
            "                    kwargs[\"render\"] = False",
            "                    state[output_id] = self.blocks[output_id].__class__(**kwargs)",
            "",
            "                    prediction_value = postprocess_update_dict(",
            "                        block=state[output_id],",
            "                        update_dict=prediction_value,",
            "                        postprocess=block_fn.postprocess,",
            "                    )",
            "                elif block_fn.postprocess:",
            "                    if not isinstance(block, components.Component):",
            "                        raise InvalidComponentError(",
            "                            f\"{block.__class__} Component with id {output_id} not a valid output component.\"",
            "                        )",
            "                    if output_id in state:",
            "                        block = state[output_id]",
            "                    prediction_value = block.postprocess(prediction_value)",
            "",
            "                outputs_cached = processing_utils.move_files_to_cache(",
            "                    prediction_value,",
            "                    block,  # type: ignore",
            "                    postprocess=True,",
            "                    add_urls=True,",
            "                )",
            "                output.append(outputs_cached)",
            "",
            "        return output",
            "",
            "    def handle_streaming_outputs(",
            "        self,",
            "        fn_index: int,",
            "        data: list,",
            "        session_hash: str | None,",
            "        run: int | None,",
            "    ) -> list:",
            "        if session_hash is None or run is None:",
            "            return data",
            "        if run not in self.pending_streams[session_hash]:",
            "            self.pending_streams[session_hash][run] = {}",
            "        stream_run = self.pending_streams[session_hash][run]",
            "",
            "        for i, output_id in enumerate(self.dependencies[fn_index][\"outputs\"]):",
            "            block = self.blocks[output_id]",
            "            if isinstance(block, components.StreamingOutput) and block.streaming:",
            "                first_chunk = output_id not in stream_run",
            "                binary_data, output_data = block.stream_output(",
            "                    data[i], f\"{session_hash}/{run}/{output_id}\", first_chunk",
            "                )",
            "                if first_chunk:",
            "                    stream_run[output_id] = []",
            "                self.pending_streams[session_hash][run][output_id].append(binary_data)",
            "                data[i] = output_data",
            "        return data",
            "",
            "    def handle_streaming_diffs(",
            "        self,",
            "        fn_index: int,",
            "        data: list,",
            "        session_hash: str | None,",
            "        run: int | None,",
            "        final: bool,",
            "    ) -> list:",
            "        if session_hash is None or run is None:",
            "            return data",
            "        first_run = run not in self.pending_diff_streams[session_hash]",
            "        if first_run:",
            "            self.pending_diff_streams[session_hash][run] = [None] * len(data)",
            "        last_diffs = self.pending_diff_streams[session_hash][run]",
            "",
            "        for i in range(len(self.dependencies[fn_index][\"outputs\"])):",
            "            if final:",
            "                data[i] = last_diffs[i]",
            "                continue",
            "",
            "            if first_run:",
            "                last_diffs[i] = data[i]",
            "            else:",
            "                prev_chunk = last_diffs[i]",
            "                last_diffs[i] = data[i]",
            "                data[i] = utils.diff(prev_chunk, data[i])",
            "",
            "        if final:",
            "            del self.pending_diff_streams[session_hash][run]",
            "",
            "        return data",
            "",
            "    def run_fn_batch(self, fn, batch, fn_index, state, explicit_call=None):",
            "        output = []",
            "        for i in zip(*batch):",
            "            args = [fn_index, list(i), state]",
            "            if explicit_call is not None:",
            "                args.append(explicit_call)",
            "            output.append(fn(*args))",
            "        return output",
            "",
            "    async def process_api(",
            "        self,",
            "        fn_index: int,",
            "        inputs: list[Any],",
            "        state: SessionState | None = None,",
            "        request: routes.Request | list[routes.Request] | None = None,",
            "        iterator: AsyncIterator | None = None,",
            "        session_hash: str | None = None,",
            "        event_id: str | None = None,",
            "        event_data: EventData | None = None,",
            "        in_event_listener: bool = True,",
            "        explicit_call: bool = False,",
            "    ) -> dict[str, Any]:",
            "        \"\"\"",
            "        Processes API calls from the frontend. First preprocesses the data,",
            "        then runs the relevant function, then postprocesses the output.",
            "        Parameters:",
            "            fn_index: Index of function to run.",
            "            inputs: input data received from the frontend",
            "            state: data stored from stateful components for session (key is input block id)",
            "            request: the gr.Request object containing information about the network request (e.g. IP address, headers, query parameters, username)",
            "            iterators: the in-progress iterators for each generator function (key is function index)",
            "            event_id: id of event that triggered this API call",
            "            event_data: data associated with the event trigger itself",
            "            in_event_listener: whether this API call is being made in response to an event listener",
            "            explicit_call: whether this call is being made directly by calling the Blocks function, instead of through an event listener or API route",
            "        Returns: None",
            "        \"\"\"",
            "        block_fn = self.fns[fn_index]",
            "        batch = self.dependencies[fn_index][\"batch\"]",
            "",
            "        if batch:",
            "            max_batch_size = self.dependencies[fn_index][\"max_batch_size\"]",
            "            batch_sizes = [len(inp) for inp in inputs]",
            "            batch_size = batch_sizes[0]",
            "            if inspect.isasyncgenfunction(block_fn.fn) or inspect.isgeneratorfunction(",
            "                block_fn.fn",
            "            ):",
            "                raise ValueError(\"Gradio does not support generators in batch mode.\")",
            "            if not all(x == batch_size for x in batch_sizes):",
            "                raise ValueError(",
            "                    f\"All inputs to a batch function must have the same length but instead have sizes: {batch_sizes}.\"",
            "                )",
            "            if batch_size > max_batch_size:",
            "                raise ValueError(",
            "                    f\"Batch size ({batch_size}) exceeds the max_batch_size for this function ({max_batch_size})\"",
            "                )",
            "            inputs = await anyio.to_thread.run_sync(",
            "                self.run_fn_batch,",
            "                self.preprocess_data,",
            "                inputs,",
            "                fn_index,",
            "                state,",
            "                explicit_call,",
            "                limiter=self.limiter,",
            "            )",
            "            result = await self.call_function(",
            "                fn_index,",
            "                list(zip(*inputs)),",
            "                None,",
            "                request,",
            "                event_id,",
            "                event_data,",
            "                in_event_listener,",
            "            )",
            "            preds = result[\"prediction\"]",
            "            data = await anyio.to_thread.run_sync(",
            "                self.run_fn_batch,",
            "                self.postprocess_data,",
            "                preds,",
            "                fn_index,",
            "                state,",
            "                limiter=self.limiter,",
            "            )",
            "            data = list(zip(*data))",
            "            is_generating, iterator = None, None",
            "        else:",
            "            old_iterator = iterator",
            "            if old_iterator:",
            "                inputs = []",
            "            else:",
            "                inputs = await anyio.to_thread.run_sync(",
            "                    self.preprocess_data,",
            "                    fn_index,",
            "                    inputs,",
            "                    state,",
            "                    explicit_call,",
            "                    limiter=self.limiter,",
            "                )",
            "            was_generating = old_iterator is not None",
            "            result = await self.call_function(",
            "                fn_index,",
            "                inputs,",
            "                old_iterator,",
            "                request,",
            "                event_id,",
            "                event_data,",
            "                in_event_listener,",
            "            )",
            "            data = await anyio.to_thread.run_sync(",
            "                self.postprocess_data,",
            "                fn_index,  # type: ignore",
            "                result[\"prediction\"],",
            "                state,",
            "                limiter=self.limiter,",
            "            )",
            "            is_generating, iterator = result[\"is_generating\"], result[\"iterator\"]",
            "            if is_generating or was_generating:",
            "                run = id(old_iterator) if was_generating else id(iterator)",
            "                data = self.handle_streaming_outputs(",
            "                    fn_index,",
            "                    data,",
            "                    session_hash=session_hash,",
            "                    run=run,",
            "                )",
            "                data = self.handle_streaming_diffs(",
            "                    fn_index,",
            "                    data,",
            "                    session_hash=session_hash,",
            "                    run=run,",
            "                    final=not is_generating,",
            "                )",
            "",
            "        block_fn.total_runtime += result[\"duration\"]",
            "        block_fn.total_runs += 1",
            "        return {",
            "            \"data\": data,",
            "            \"is_generating\": is_generating,",
            "            \"iterator\": iterator,",
            "            \"duration\": result[\"duration\"],",
            "            \"average_duration\": block_fn.total_runtime / block_fn.total_runs,",
            "        }",
            "",
            "    def create_limiter(self):",
            "        self.limiter = (",
            "            None",
            "            if self.max_threads == 40",
            "            else CapacityLimiter(total_tokens=self.max_threads)",
            "        )",
            "",
            "    def get_config(self):",
            "        return {\"type\": \"column\"}",
            "",
            "    def get_config_file(self):",
            "        config = {",
            "            \"version\": routes.VERSION,",
            "            \"mode\": self.mode,",
            "            \"app_id\": self.app_id,",
            "            \"dev_mode\": self.dev_mode,",
            "            \"analytics_enabled\": self.analytics_enabled,",
            "            \"components\": [],",
            "            \"css\": self.css,",
            "            \"js\": self.js,",
            "            \"head\": self.head,",
            "            \"title\": self.title or \"Gradio\",",
            "            \"space_id\": self.space_id,",
            "            \"enable_queue\": True,  # launch attributes",
            "            \"show_error\": getattr(self, \"show_error\", False),",
            "            \"show_api\": self.show_api,",
            "            \"is_colab\": utils.colab_check(),",
            "            \"stylesheets\": self.stylesheets,",
            "            \"theme\": self.theme.name,",
            "            \"protocol\": \"sse_v2\",",
            "            \"body_css\": {",
            "                \"body_background_fill\": self.theme._get_computed_value(",
            "                    \"body_background_fill\"",
            "                ),",
            "                \"body_text_color\": self.theme._get_computed_value(\"body_text_color\"),",
            "                \"body_background_fill_dark\": self.theme._get_computed_value(",
            "                    \"body_background_fill_dark\"",
            "                ),",
            "                \"body_text_color_dark\": self.theme._get_computed_value(",
            "                    \"body_text_color_dark\"",
            "                ),",
            "            },",
            "            \"fill_height\": self.fill_height,",
            "        }",
            "",
            "        def get_layout(block):",
            "            if not isinstance(block, BlockContext):",
            "                return {\"id\": block._id}",
            "            children_layout = []",
            "            for child in block.children:",
            "                children_layout.append(get_layout(child))",
            "            return {\"id\": block._id, \"children\": children_layout}",
            "",
            "        config[\"layout\"] = get_layout(self)",
            "",
            "        for _id, block in self.blocks.items():",
            "            props = block.get_config() if hasattr(block, \"get_config\") else {}",
            "            block_config = {",
            "                \"id\": _id,",
            "                \"type\": block.get_block_name(),",
            "                \"props\": utils.delete_none(props),",
            "            }",
            "            block_config[\"skip_api\"] = block.skip_api",
            "            block_config[\"component_class_id\"] = getattr(",
            "                block, \"component_class_id\", None",
            "            )",
            "",
            "            if not block.skip_api:",
            "                block_config[\"api_info\"] = block.api_info()  # type: ignore",
            "                block_config[\"example_inputs\"] = block.example_inputs()  # type: ignore",
            "            config[\"components\"].append(block_config)",
            "        config[\"dependencies\"] = self.dependencies",
            "        return config",
            "",
            "    def __enter__(self):",
            "        if Context.block is None:",
            "            Context.root_block = self",
            "        self.parent = Context.block",
            "        Context.block = self",
            "        self.exited = False",
            "        return self",
            "",
            "    def __exit__(self, exc_type: type[BaseException] | None = None, *args):",
            "        if exc_type is not None:",
            "            Context.block = None",
            "            Context.root_block = None",
            "            return",
            "        super().fill_expected_parents()",
            "        Context.block = self.parent",
            "        # Configure the load events before root_block is reset",
            "        self.attach_load_events()",
            "        if self.parent is None:",
            "            Context.root_block = None",
            "        else:",
            "            self.parent.children.extend(self.children)",
            "        self.config = self.get_config_file()",
            "        self.app = routes.App.create_app(self)",
            "        self.progress_tracking = any(block_fn.tracks_progress for block_fn in self.fns)",
            "        self.exited = True",
            "",
            "    def clear(self):",
            "        \"\"\"Resets the layout of the Blocks object.\"\"\"",
            "        self.blocks = {}",
            "        self.fns = []",
            "        self.dependencies = []",
            "        self.children = []",
            "        return self",
            "",
            "    @document()",
            "    def queue(",
            "        self,",
            "        status_update_rate: float | Literal[\"auto\"] = \"auto\",",
            "        api_open: bool | None = None,",
            "        max_size: int | None = None,",
            "        concurrency_count: int | None = None,",
            "        *,",
            "        default_concurrency_limit: int | None | Literal[\"not_set\"] = \"not_set\",",
            "    ):",
            "        \"\"\"",
            "        By enabling the queue you can control when users know their position in the queue, and set a limit on maximum number of events allowed.",
            "        Parameters:",
            "            status_update_rate: If \"auto\", Queue will send status estimations to all clients whenever a job is finished. Otherwise Queue will send status at regular intervals set by this parameter as the number of seconds.",
            "            api_open: If True, the REST routes of the backend will be open, allowing requests made directly to those endpoints to skip the queue.",
            "            max_size: The maximum number of events the queue will store at any given moment. If the queue is full, new events will not be added and a user will receive a message saying that the queue is full. If None, the queue size will be unlimited.",
            "            concurrency_count: Deprecated. Set the concurrency_limit directly on event listeners e.g. btn.click(fn, ..., concurrency_limit=10) or gr.Interface(concurrency_limit=10). If necessary, the total number of workers can be configured via `max_threads` in launch().",
            "            default_concurrency_limit: The default value of `concurrency_limit` to use for event listeners that don't specify a value. Can be set by environment variable GRADIO_DEFAULT_CONCURRENCY_LIMIT. Defaults to 1 if not set otherwise.",
            "        Example: (Blocks)",
            "            with gr.Blocks() as demo:",
            "                button = gr.Button(label=\"Generate Image\")",
            "                button.click(fn=image_generator, inputs=gr.Textbox(), outputs=gr.Image())",
            "            demo.queue(max_size=10)",
            "            demo.launch()",
            "        Example: (Interface)",
            "            demo = gr.Interface(image_generator, gr.Textbox(), gr.Image())",
            "            demo.queue(max_size=20)",
            "            demo.launch()",
            "        \"\"\"",
            "        if concurrency_count:",
            "            raise DeprecationWarning(",
            "                \"concurrency_count has been deprecated. Set the concurrency_limit directly on event listeners e.g. btn.click(fn, ..., concurrency_limit=10) or gr.Interface(concurrency_limit=10). If necessary, the total number of workers can be configured via `max_threads` in launch().\"",
            "            )",
            "        if api_open is not None:",
            "            self.api_open = api_open",
            "        if utils.is_zero_gpu_space():",
            "            max_size = 1 if max_size is None else max_size",
            "        self._queue = queueing.Queue(",
            "            live_updates=status_update_rate == \"auto\",",
            "            concurrency_count=self.max_threads,",
            "            update_intervals=status_update_rate if status_update_rate != \"auto\" else 1,",
            "            max_size=max_size,",
            "            block_fns=self.fns,",
            "            default_concurrency_limit=default_concurrency_limit,",
            "        )",
            "        self.config = self.get_config_file()",
            "        self.app = routes.App.create_app(self)",
            "        return self",
            "",
            "    def validate_queue_settings(self):",
            "        for dep in self.dependencies:",
            "            for i in dep[\"cancels\"]:",
            "                if not self.queue_enabled_for_fn(i):",
            "                    raise ValueError(",
            "                        \"Queue needs to be enabled! \"",
            "                        \"You may get this error by either 1) passing a function that uses the yield keyword \"",
            "                        \"into an interface without enabling the queue or 2) defining an event that cancels \"",
            "                        \"another event without enabling the queue. Both can be solved by calling .queue() \"",
            "                        \"before .launch()\"",
            "                    )",
            "            if dep[\"batch\"] and dep[\"queue\"] is False:",
            "                raise ValueError(\"In order to use batching, the queue must be enabled.\")",
            "",
            "    def launch(",
            "        self,",
            "        inline: bool | None = None,",
            "        inbrowser: bool = False,",
            "        share: bool | None = None,",
            "        debug: bool = False,",
            "        max_threads: int = 40,",
            "        auth: Callable | tuple[str, str] | list[tuple[str, str]] | None = None,",
            "        auth_message: str | None = None,",
            "        prevent_thread_lock: bool = False,",
            "        show_error: bool = False,",
            "        server_name: str | None = None,",
            "        server_port: int | None = None,",
            "        *,",
            "        height: int = 500,",
            "        width: int | str = \"100%\",",
            "        favicon_path: str | None = None,",
            "        ssl_keyfile: str | None = None,",
            "        ssl_certfile: str | None = None,",
            "        ssl_keyfile_password: str | None = None,",
            "        ssl_verify: bool = True,",
            "        quiet: bool = False,",
            "        show_api: bool = True,",
            "        allowed_paths: list[str] | None = None,",
            "        blocked_paths: list[str] | None = None,",
            "        root_path: str | None = None,",
            "        app_kwargs: dict[str, Any] | None = None,",
            "        state_session_capacity: int = 10000,",
            "        share_server_address: str | None = None,",
            "        share_server_protocol: Literal[\"http\", \"https\"] | None = None,",
            "        _frontend: bool = True,",
            "    ) -> tuple[FastAPI, str, str]:",
            "        \"\"\"",
            "        Launches a simple web server that serves the demo. Can also be used to create a",
            "        public link used by anyone to access the demo from their browser by setting share=True.",
            "",
            "        Parameters:",
            "            inline: whether to display in the interface inline in an iframe. Defaults to True in python notebooks; False otherwise.",
            "            inbrowser: whether to automatically launch the interface in a new tab on the default browser.",
            "            share: whether to create a publicly shareable link for the interface. Creates an SSH tunnel to make your UI accessible from anywhere. If not provided, it is set to False by default every time, except when running in Google Colab. When localhost is not accessible (e.g. Google Colab), setting share=False is not supported.",
            "            debug: if True, blocks the main thread from running. If running in Google Colab, this is needed to print the errors in the cell output.",
            "            auth: If provided, username and password (or list of username-password tuples) required to access interface. Can also provide function that takes username and password and returns True if valid login.",
            "            auth_message: If provided, HTML message provided on login page.",
            "            prevent_thread_lock: If True, the interface will block the main thread while the server is running.",
            "            show_error: If True, any errors in the interface will be displayed in an alert modal and printed in the browser console log",
            "            server_port: will start gradio app on this port (if available). Can be set by environment variable GRADIO_SERVER_PORT. If None, will search for an available port starting at 7860.",
            "            server_name: to make app accessible on local network, set this to \"0.0.0.0\". Can be set by environment variable GRADIO_SERVER_NAME. If None, will use \"127.0.0.1\".",
            "            max_threads: the maximum number of total threads that the Gradio app can generate in parallel. The default is inherited from the starlette library (currently 40).",
            "            width: The width in pixels of the iframe element containing the interface (used if inline=True)",
            "            height: The height in pixels of the iframe element containing the interface (used if inline=True)",
            "            favicon_path: If a path to a file (.png, .gif, or .ico) is provided, it will be used as the favicon for the web page.",
            "            ssl_keyfile: If a path to a file is provided, will use this as the private key file to create a local server running on https.",
            "            ssl_certfile: If a path to a file is provided, will use this as the signed certificate for https. Needs to be provided if ssl_keyfile is provided.",
            "            ssl_keyfile_password: If a password is provided, will use this with the ssl certificate for https.",
            "            ssl_verify: If False, skips certificate validation which allows self-signed certificates to be used.",
            "            quiet: If True, suppresses most print statements.",
            "            show_api: If True, shows the api docs in the footer of the app. Default True.",
            "            allowed_paths: List of complete filepaths or parent directories that gradio is allowed to serve (in addition to the directory containing the gradio python file). Must be absolute paths. Warning: if you provide directories, any files in these directories or their subdirectories are accessible to all users of your app.",
            "            blocked_paths: List of complete filepaths or parent directories that gradio is not allowed to serve (i.e. users of your app are not allowed to access). Must be absolute paths. Warning: takes precedence over `allowed_paths` and all other directories exposed by Gradio by default.",
            "            root_path: The root path (or \"mount point\") of the application, if it's not served from the root (\"/\") of the domain. Often used when the application is behind a reverse proxy that forwards requests to the application. For example, if the application is served at \"https://example.com/myapp\", the `root_path` should be set to \"/myapp\". Can be set by environment variable GRADIO_ROOT_PATH. Defaults to \"\".",
            "            app_kwargs: Additional keyword arguments to pass to the underlying FastAPI app as a dictionary of parameter keys and argument values. For example, `{\"docs_url\": \"/docs\"}`",
            "            state_session_capacity: The maximum number of sessions whose information to store in memory. If the number of sessions exceeds this number, the oldest sessions will be removed. Reduce capacity to reduce memory usage when using gradio.State or returning updated components from functions. Defaults to 10000.",
            "            share_server_address: Use this to specify a custom FRP server and port for sharing Gradio apps (only applies if share=True). If not provided, will use the default FRP server at https://gradio.live. See https://github.com/huggingface/frp for more information.",
            "            share_server_protocol: Use this to specify the protocol to use for the share links. Defaults to \"https\", unless a custom share_server_address is provided, in which case it defaults to \"http\". If you are using a custom share_server_address and want to use https, you must set this to \"https\".",
            "        Returns:",
            "            app: FastAPI app object that is running the demo",
            "            local_url: Locally accessible link to the demo",
            "            share_url: Publicly accessible link to the demo (if share=True, otherwise None)",
            "        Example: (Blocks)",
            "            import gradio as gr",
            "            def reverse(text):",
            "                return text[::-1]",
            "            with gr.Blocks() as demo:",
            "                button = gr.Button(value=\"Reverse\")",
            "                button.click(reverse, gr.Textbox(), gr.Textbox())",
            "            demo.launch(share=True, auth=(\"username\", \"password\"))",
            "        Example:  (Interface)",
            "            import gradio as gr",
            "            def reverse(text):",
            "                return text[::-1]",
            "            demo = gr.Interface(reverse, \"text\", \"text\")",
            "            demo.launch(share=True, auth=(\"username\", \"password\"))",
            "        \"\"\"",
            "        if self._is_running_in_reload_thread:",
            "            # We have already launched the demo",
            "            return None, None, None  # type: ignore",
            "",
            "        if not self.exited:",
            "            self.__exit__()",
            "",
            "        if (",
            "            auth",
            "            and not callable(auth)",
            "            and not isinstance(auth[0], tuple)",
            "            and not isinstance(auth[0], list)",
            "        ):",
            "            self.auth = [auth]",
            "        else:",
            "            self.auth = auth",
            "        self.auth_message = auth_message",
            "        self.show_error = show_error",
            "        self.height = height",
            "        self.width = width",
            "        self.favicon_path = favicon_path",
            "        self.ssl_verify = ssl_verify",
            "        self.state_session_capacity = state_session_capacity",
            "        if root_path is None:",
            "            self.root_path = os.environ.get(\"GRADIO_ROOT_PATH\", \"\")",
            "        else:",
            "            self.root_path = root_path",
            "",
            "        self.show_api = show_api",
            "",
            "        self.allowed_paths = allowed_paths or []",
            "        self.blocked_paths = blocked_paths or []",
            "",
            "        if not isinstance(self.allowed_paths, list):",
            "            raise ValueError(\"`allowed_paths` must be a list of directories.\")",
            "        if not isinstance(self.blocked_paths, list):",
            "            raise ValueError(\"`blocked_paths` must be a list of directories.\")",
            "",
            "        self.validate_queue_settings()",
            "",
            "        self.config = self.get_config_file()",
            "        self.max_threads = max_threads",
            "        self._queue.max_thread_count = max_threads",
            "",
            "        if self.is_running:",
            "            if not isinstance(self.local_url, str):",
            "                raise ValueError(f\"Invalid local_url: {self.local_url}\")",
            "            if not (quiet):",
            "                print(",
            "                    \"Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\\n----\"",
            "                )",
            "        else:",
            "            if wasm_utils.IS_WASM:",
            "                server_name = \"xxx\"",
            "                server_port = 99999",
            "                local_url = \"\"",
            "                server = None",
            "",
            "                # In the Wasm environment, we only need the app object",
            "                # which the frontend app will directly communicate with through the Worker API,",
            "                # and we don't need to start a server.",
            "                # So we just create the app object and register it here,",
            "                # and avoid using `networking.start_server` that would start a server that don't work in the Wasm env.",
            "                from gradio.routes import App",
            "",
            "                app = App.create_app(self, app_kwargs=app_kwargs)",
            "                wasm_utils.register_app(app)",
            "            else:",
            "                (",
            "                    server_name,",
            "                    server_port,",
            "                    local_url,",
            "                    app,",
            "                    server,",
            "                ) = networking.start_server(",
            "                    self,",
            "                    server_name,",
            "                    server_port,",
            "                    ssl_keyfile,",
            "                    ssl_certfile,",
            "                    ssl_keyfile_password,",
            "                    app_kwargs=app_kwargs,",
            "                )",
            "            self.server_name = server_name",
            "            self.local_url = local_url",
            "            self.server_port = server_port",
            "            self.server_app = (",
            "                self.app",
            "            ) = app  # server_app is included for backwards compatibility",
            "            self.server = server",
            "            self.is_running = True",
            "            self.is_colab = utils.colab_check()",
            "            self.is_kaggle = utils.kaggle_check()",
            "            self.share_server_address = share_server_address",
            "            self.share_server_protocol = share_server_protocol or (",
            "                \"http\" if share_server_address is not None else \"https\"",
            "            )",
            "",
            "            self.protocol = (",
            "                \"https\"",
            "                if self.local_url.startswith(\"https\") or self.is_colab",
            "                else \"http\"",
            "            )",
            "            if not wasm_utils.IS_WASM and not self.is_colab:",
            "                print(",
            "                    strings.en[\"RUNNING_LOCALLY_SEPARATED\"].format(",
            "                        self.protocol, self.server_name, self.server_port",
            "                    )",
            "                )",
            "",
            "            self._queue.set_server_app(self.server_app)",
            "",
            "            if not wasm_utils.IS_WASM:",
            "                # Cannot run async functions in background other than app's scope.",
            "                # Workaround by triggering the app endpoint",
            "                httpx.get(f\"{self.local_url}startup-events\", verify=ssl_verify)",
            "            else:",
            "                # NOTE: One benefit of the code above dispatching `startup_events()` via a self HTTP request is",
            "                # that `self._queue.start()` is called in another thread which is managed by the HTTP server, `uvicorn`",
            "                # so all the asyncio tasks created by the queue runs in an event loop in that thread and",
            "                # will be cancelled just by stopping the server.",
            "                # In contrast, in the Wasm env, we can't do that because `threading` is not supported and all async tasks will run in the same event loop, `pyodide.webloop.WebLoop` in the main thread.",
            "                # So we need to manually cancel them. See `self.close()`..",
            "                self.startup_events()",
            "",
            "        utils.launch_counter()",
            "        self.is_sagemaker = utils.sagemaker_check()",
            "        if share is None:",
            "            if self.is_colab:",
            "                if not quiet:",
            "                    print(",
            "                        \"Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\\n\"",
            "                    )",
            "                self.share = True",
            "            elif self.is_kaggle:",
            "                if not quiet:",
            "                    print(",
            "                        \"Kaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\\n\"",
            "                    )",
            "                self.share = True",
            "            elif self.is_sagemaker:",
            "                if not quiet:",
            "                    print(",
            "                        \"Sagemaker notebooks may require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\\n\"",
            "                    )",
            "                self.share = True",
            "            else:",
            "                self.share = False",
            "        else:",
            "            self.share = share",
            "",
            "        # If running in a colab or not able to access localhost,",
            "        # a shareable link must be created.",
            "        if (",
            "            _frontend",
            "            and not wasm_utils.IS_WASM",
            "            and not networking.url_ok(self.local_url)",
            "            and not self.share",
            "        ):",
            "            raise ValueError(",
            "                \"When localhost is not accessible, a shareable link must be created. Please set share=True or check your proxy settings to allow access to localhost.\"",
            "            )",
            "",
            "        if self.is_colab and not quiet:",
            "            if debug:",
            "                print(strings.en[\"COLAB_DEBUG_TRUE\"])",
            "            else:",
            "                print(strings.en[\"COLAB_DEBUG_FALSE\"])",
            "            if not self.share:",
            "                print(strings.en[\"COLAB_WARNING\"].format(self.server_port))",
            "",
            "        if self.share:",
            "            if self.space_id:",
            "                warnings.warn(",
            "                    \"Setting share=True is not supported on Hugging Face Spaces\"",
            "                )",
            "                self.share = False",
            "            if wasm_utils.IS_WASM:",
            "                warnings.warn(",
            "                    \"Setting share=True is not supported in the Wasm environment\"",
            "                )",
            "                self.share = False",
            "",
            "        if self.share:",
            "            try:",
            "                if self.share_url is None:",
            "                    share_url = networking.setup_tunnel(",
            "                        local_host=self.server_name,",
            "                        local_port=self.server_port,",
            "                        share_token=self.share_token,",
            "                        share_server_address=self.share_server_address,",
            "                    )",
            "                    parsed_url = urlparse(share_url)",
            "                    self.share_url = urlunparse(",
            "                        (self.share_server_protocol,) + parsed_url[1:]",
            "                    )",
            "                print(strings.en[\"SHARE_LINK_DISPLAY\"].format(self.share_url))",
            "                if not (quiet):",
            "                    print(strings.en[\"SHARE_LINK_MESSAGE\"])",
            "            except (RuntimeError, httpx.ConnectError):",
            "                if self.analytics_enabled:",
            "                    analytics.error_analytics(\"Not able to set up tunnel\")",
            "                self.share_url = None",
            "                self.share = False",
            "                if Path(BINARY_PATH).exists():",
            "                    print(strings.en[\"COULD_NOT_GET_SHARE_LINK\"])",
            "                else:",
            "                    print(",
            "                        strings.en[\"COULD_NOT_GET_SHARE_LINK_MISSING_FILE\"].format(",
            "                            BINARY_PATH,",
            "                            BINARY_URL,",
            "                            BINARY_FILENAME,",
            "                            BINARY_FOLDER,",
            "                        )",
            "                    )",
            "        else:",
            "            if not quiet and not wasm_utils.IS_WASM:",
            "                print(strings.en[\"PUBLIC_SHARE_TRUE\"])",
            "            self.share_url = None",
            "",
            "        if inbrowser and not wasm_utils.IS_WASM:",
            "            link = self.share_url if self.share and self.share_url else self.local_url",
            "            webbrowser.open(link)",
            "",
            "        # Check if running in a Python notebook in which case, display inline",
            "        if inline is None:",
            "            inline = utils.ipython_check()",
            "        if inline:",
            "            try:",
            "                from IPython.display import HTML, Javascript, display  # type: ignore",
            "",
            "                if self.share and self.share_url:",
            "                    while not networking.url_ok(self.share_url):",
            "                        time.sleep(0.25)",
            "                    artifact = HTML(",
            "                        f'<div><iframe src=\"{self.share_url}\" width=\"{self.width}\" height=\"{self.height}\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>'",
            "                    )",
            "",
            "                elif self.is_colab:",
            "                    # modified from /usr/local/lib/python3.7/dist-packages/google/colab/output/_util.py within Colab environment",
            "                    code = \"\"\"(async (port, path, width, height, cache, element) => {",
            "                        if (!google.colab.kernel.accessAllowed && !cache) {",
            "                            return;",
            "                        }",
            "                        element.appendChild(document.createTextNode(''));",
            "                        const url = await google.colab.kernel.proxyPort(port, {cache});",
            "",
            "                        const external_link = document.createElement('div');",
            "                        external_link.innerHTML = `",
            "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">",
            "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">",
            "                                    https://localhost:${port}${path}",
            "                                </a>",
            "                            </div>",
            "                        `;",
            "                        element.appendChild(external_link);",
            "",
            "                        const iframe = document.createElement('iframe');",
            "                        iframe.src = new URL(path, url).toString();",
            "                        iframe.height = height;",
            "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"",
            "                        iframe.width = width;",
            "                        iframe.style.border = 0;",
            "                        element.appendChild(iframe);",
            "                    })\"\"\" + \"({port}, {path}, {width}, {height}, {cache}, window.element)\".format(",
            "                        port=json.dumps(self.server_port),",
            "                        path=json.dumps(\"/\"),",
            "                        width=json.dumps(self.width),",
            "                        height=json.dumps(self.height),",
            "                        cache=json.dumps(False),",
            "                    )",
            "",
            "                    artifact = Javascript(code)",
            "                else:",
            "                    artifact = HTML(",
            "                        f'<div><iframe src=\"{self.local_url}\" width=\"{self.width}\" height=\"{self.height}\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>'",
            "                    )",
            "                self.artifact = artifact",
            "                display(artifact)",
            "            except ImportError:",
            "                pass",
            "",
            "        if getattr(self, \"analytics_enabled\", False):",
            "            data = {",
            "                \"launch_method\": \"browser\" if inbrowser else \"inline\",",
            "                \"is_google_colab\": self.is_colab,",
            "                \"is_sharing_on\": self.share,",
            "                \"share_url\": self.share_url,",
            "                \"enable_queue\": True,",
            "                \"server_name\": server_name,",
            "                \"server_port\": server_port,",
            "                \"is_space\": self.space_id is not None,",
            "                \"mode\": self.mode,",
            "            }",
            "            analytics.launched_analytics(self, data)",
            "",
            "        # Block main thread if debug==True",
            "        if debug or int(os.getenv(\"GRADIO_DEBUG\", \"0\")) == 1 and not wasm_utils.IS_WASM:",
            "            self.block_thread()",
            "        # Block main thread if running in a script to stop script from exiting",
            "        is_in_interactive_mode = bool(getattr(sys, \"ps1\", sys.flags.interactive))",
            "",
            "        if (",
            "            not prevent_thread_lock",
            "            and not is_in_interactive_mode",
            "            # In the Wasm env, we don't have to block the main thread because the server won't be shut down after the execution finishes.",
            "            # Moreover, we MUST NOT do it because there is only one thread in the Wasm env and blocking it will stop the subsequent code from running.",
            "            and not wasm_utils.IS_WASM",
            "        ):",
            "            self.block_thread()",
            "",
            "        return TupleNoPrint((self.server_app, self.local_url, self.share_url))  # type: ignore",
            "",
            "    def integrate(",
            "        self,",
            "        comet_ml=None,",
            "        wandb: ModuleType | None = None,",
            "        mlflow: ModuleType | None = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        A catch-all method for integrating with other libraries. This method should be run after launch()",
            "        Parameters:",
            "            comet_ml: If a comet_ml Experiment object is provided, will integrate with the experiment and appear on Comet dashboard",
            "            wandb: If the wandb module is provided, will integrate with it and appear on WandB dashboard",
            "            mlflow: If the mlflow module  is provided, will integrate with the experiment and appear on ML Flow dashboard",
            "        \"\"\"",
            "        analytics_integration = \"\"",
            "        if comet_ml is not None:",
            "            analytics_integration = \"CometML\"",
            "            comet_ml.log_other(\"Created from\", \"Gradio\")",
            "            if self.share_url is not None:",
            "                comet_ml.log_text(f\"gradio: {self.share_url}\")",
            "                comet_ml.end()",
            "            elif self.local_url:",
            "                comet_ml.log_text(f\"gradio: {self.local_url}\")",
            "                comet_ml.end()",
            "            else:",
            "                raise ValueError(\"Please run `launch()` first.\")",
            "        if wandb is not None:",
            "            analytics_integration = \"WandB\"",
            "            if self.share_url is not None:",
            "                wandb.log(",
            "                    {",
            "                        \"Gradio panel\": wandb.Html(",
            "                            '<iframe src=\"'",
            "                            + self.share_url",
            "                            + '\" width=\"'",
            "                            + str(self.width)",
            "                            + '\" height=\"'",
            "                            + str(self.height)",
            "                            + '\" frameBorder=\"0\"></iframe>'",
            "                        )",
            "                    }",
            "                )",
            "            else:",
            "                print(",
            "                    \"The WandB integration requires you to \"",
            "                    \"`launch(share=True)` first.\"",
            "                )",
            "        if mlflow is not None:",
            "            analytics_integration = \"MLFlow\"",
            "            if self.share_url is not None:",
            "                mlflow.log_param(\"Gradio Interface Share Link\", self.share_url)",
            "            else:",
            "                mlflow.log_param(\"Gradio Interface Local Link\", self.local_url)",
            "        if self.analytics_enabled and analytics_integration:",
            "            data = {\"integration\": analytics_integration}",
            "            analytics.integration_analytics(data)",
            "",
            "    def close(self, verbose: bool = True) -> None:",
            "        \"\"\"",
            "        Closes the Interface that was launched and frees the port.",
            "        \"\"\"",
            "        try:",
            "            if wasm_utils.IS_WASM:",
            "                # NOTE:",
            "                # Normally, queue-related async tasks (e.g. continuous events created by `gr.Blocks.load(..., every=interval)`, whose async tasks are started at the `/queue/data` endpoint function)",
            "                # are running in an event loop in the server thread,",
            "                # so they will be cancelled by `self.server.close()` below.",
            "                # However, in the Wasm env, we don't have the `server` and",
            "                # all async tasks are running in the same event loop, `pyodide.webloop.WebLoop` in the main thread,",
            "                # so we have to cancel them explicitly so that these tasks won't run after a new app is launched.",
            "                self._queue._cancel_asyncio_tasks()",
            "                self.server_app._cancel_asyncio_tasks()",
            "            self._queue.close()",
            "            if self.server:",
            "                self.server.close()",
            "            self.is_running = False",
            "            # So that the startup events (starting the queue)",
            "            # happen the next time the app is launched",
            "            self.app.startup_events_triggered = False",
            "            if verbose:",
            "                print(f\"Closing server running on port: {self.server_port}\")",
            "        except (AttributeError, OSError):  # can't close if not running",
            "            pass",
            "",
            "    def block_thread(",
            "        self,",
            "    ) -> None:",
            "        \"\"\"Block main thread until interrupted by user.\"\"\"",
            "        try:",
            "            while True:",
            "                time.sleep(0.1)",
            "        except (KeyboardInterrupt, OSError):",
            "            print(\"Keyboard interruption in main thread... closing server.\")",
            "            if self.server:",
            "                self.server.close()",
            "            for tunnel in CURRENT_TUNNELS:",
            "                tunnel.kill()",
            "",
            "    def attach_load_events(self):",
            "        \"\"\"Add a load event for every component whose initial value should be randomized.\"\"\"",
            "        if Context.root_block:",
            "            for component in Context.root_block.blocks.values():",
            "                if (",
            "                    isinstance(component, components.Component)",
            "                    and component.load_event_to_attach",
            "                ):",
            "                    load_fn, every = component.load_event_to_attach",
            "                    # Use set_event_trigger to avoid ambiguity between load class/instance method",
            "",
            "                    dep = self.set_event_trigger(",
            "                        [EventListenerMethod(self, \"load\")],",
            "                        load_fn,",
            "                        None,",
            "                        component,",
            "                        no_target=True,",
            "                        # If every is None, for sure skip the queue",
            "                        # else, let the enable_queue parameter take precedence",
            "                        # this will raise a nice error message is every is used",
            "                        # without queue",
            "                        queue=False if every is None else None,",
            "                        every=every,",
            "                    )[0]",
            "                    component.load_event = dep",
            "",
            "    def startup_events(self):",
            "        \"\"\"Events that should be run when the app containing this block starts up.\"\"\"",
            "        self._queue.start()",
            "        # So that processing can resume in case the queue was stopped",
            "        self._queue.stopped = False",
            "        self.create_limiter()",
            "",
            "    def queue_enabled_for_fn(self, fn_index: int):",
            "        return self.dependencies[fn_index][\"queue\"] is not False",
            "",
            "    def get_api_info(self):",
            "        \"\"\"",
            "        Gets the information needed to generate the API docs from a Blocks.",
            "        \"\"\"",
            "        config = self.config",
            "        api_info = {\"named_endpoints\": {}, \"unnamed_endpoints\": {}}",
            "",
            "        for dependency in config[\"dependencies\"]:",
            "            if (",
            "                not dependency[\"backend_fn\"]",
            "                or not dependency[\"show_api\"]",
            "                or dependency[\"api_name\"] is False",
            "            ):",
            "                continue",
            "",
            "            dependency_info = {\"parameters\": [], \"returns\": []}",
            "            skip_endpoint = False",
            "",
            "            inputs = dependency[\"inputs\"]",
            "            for i in inputs:",
            "                for component in config[\"components\"]:",
            "                    if component[\"id\"] == i:",
            "                        break",
            "                else:",
            "                    skip_endpoint = True  # if component not found, skip endpoint",
            "                    break",
            "                type = component[\"type\"]",
            "                if self.blocks[component[\"id\"]].skip_api:",
            "                    continue",
            "                label = component[\"props\"].get(\"label\", f\"parameter_{i}\")",
            "                comp = self.get_component(component[\"id\"])",
            "                if not isinstance(comp, components.Component):",
            "                    raise TypeError(f\"{comp!r} is not a Component\")",
            "                info = component[\"api_info\"]",
            "                example = comp.example_inputs()",
            "                python_type = client_utils.json_schema_to_python_type(info)",
            "                dependency_info[\"parameters\"].append(",
            "                    {",
            "                        \"label\": label,",
            "                        \"type\": info,",
            "                        \"python_type\": {",
            "                            \"type\": python_type,",
            "                            \"description\": info.get(\"description\", \"\"),",
            "                        },",
            "                        \"component\": type.capitalize(),",
            "                        \"example_input\": example,",
            "                    }",
            "                )",
            "",
            "            outputs = dependency[\"outputs\"]",
            "            for o in outputs:",
            "                for component in config[\"components\"]:",
            "                    if component[\"id\"] == o:",
            "                        break",
            "                else:",
            "                    skip_endpoint = True  # if component not found, skip endpoint",
            "                    break",
            "                type = component[\"type\"]",
            "                if self.blocks[component[\"id\"]].skip_api:",
            "                    continue",
            "                label = component[\"props\"].get(\"label\", f\"value_{o}\")",
            "                comp = self.get_component(component[\"id\"])",
            "                if not isinstance(comp, components.Component):",
            "                    raise TypeError(f\"{comp!r} is not a Component\")",
            "                info = component[\"api_info\"]",
            "                example = comp.example_inputs()",
            "                python_type = client_utils.json_schema_to_python_type(info)",
            "                dependency_info[\"returns\"].append(",
            "                    {",
            "                        \"label\": label,",
            "                        \"type\": info,",
            "                        \"python_type\": {",
            "                            \"type\": python_type,",
            "                            \"description\": info.get(\"description\", \"\"),",
            "                        },",
            "                        \"component\": type.capitalize(),",
            "                    }",
            "                )",
            "",
            "            if not skip_endpoint:",
            "                api_info[\"named_endpoints\"][",
            "                    f\"/{dependency['api_name']}\"",
            "                ] = dependency_info",
            "",
            "        return api_info"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "12": [],
            "122": [
                "Block",
                "__init__"
            ],
            "123": [
                "Block",
                "__init__"
            ],
            "124": [
                "Block",
                "__init__"
            ],
            "125": [
                "Block",
                "__init__"
            ],
            "126": [
                "Block",
                "__init__"
            ],
            "127": [
                "Block",
                "__init__"
            ],
            "1302": [
                "Blocks",
                "preprocess_data"
            ],
            "1329": [
                "Blocks",
                "preprocess_data"
            ],
            "1525": [
                "Blocks",
                "run_fn_batch"
            ],
            "1526": [
                "Blocks",
                "run_fn_batch"
            ],
            "1606": [
                "Blocks"
            ]
        },
        "addLocation": []
    },
    "gradio/processing_utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from gradio import wasm_utils"
            },
            "2": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " from gradio.data_classes import FileData, GradioModel, GradioRootModel"
            },
            "3": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from gradio.utils import abspath"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 23,
                "PatchRowcode": "+from gradio.utils import abspath, get_upload_folder, is_in_or_equal"
            },
            "5": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " with warnings.catch_warnings():"
            },
            "7": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 26,
                "PatchRowcode": "     warnings.simplefilter(\"ignore\")  # Ignore pydub warning if ffmpeg is not installed"
            },
            "8": {
                "beforePatchRowNumber": 241,
                "afterPatchRowNumber": 241,
                "PatchRowcode": "     block: Component,"
            },
            "9": {
                "beforePatchRowNumber": 242,
                "afterPatchRowNumber": 242,
                "PatchRowcode": "     postprocess: bool = False,"
            },
            "10": {
                "beforePatchRowNumber": 243,
                "afterPatchRowNumber": 243,
                "PatchRowcode": "     add_urls=False,"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 244,
                "PatchRowcode": "+    check_in_upload_folder=False,"
            },
            "12": {
                "beforePatchRowNumber": 244,
                "afterPatchRowNumber": 245,
                "PatchRowcode": " ) -> dict:"
            },
            "13": {
                "beforePatchRowNumber": 245,
                "afterPatchRowNumber": 246,
                "PatchRowcode": "     \"\"\"Move any files in `data` to cache and (optionally), adds URL prefixes (/file=...) needed to access the cached file."
            },
            "14": {
                "beforePatchRowNumber": 246,
                "afterPatchRowNumber": 247,
                "PatchRowcode": "     Also handles the case where the file is on an external Gradio app (/proxy=...)."
            },
            "15": {
                "beforePatchRowNumber": 252,
                "afterPatchRowNumber": 253,
                "PatchRowcode": "         block: The component whose data is being processed"
            },
            "16": {
                "beforePatchRowNumber": 253,
                "afterPatchRowNumber": 254,
                "PatchRowcode": "         postprocess: Whether its running from postprocessing"
            },
            "17": {
                "beforePatchRowNumber": 254,
                "afterPatchRowNumber": 255,
                "PatchRowcode": "         root_url: The root URL of the local server, if applicable"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 256,
                "PatchRowcode": "+        add_urls: Whether to add URLs to the payload"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 257,
                "PatchRowcode": "+        check_in_upload_folder: If True, instead of moving the file to cache, checks if the file is in already in cache (exception if not)."
            },
            "20": {
                "beforePatchRowNumber": 255,
                "afterPatchRowNumber": 258,
                "PatchRowcode": "     \"\"\""
            },
            "21": {
                "beforePatchRowNumber": 256,
                "afterPatchRowNumber": 259,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 257,
                "afterPatchRowNumber": 260,
                "PatchRowcode": "     def _move_to_cache(d: dict):"
            },
            "23": {
                "beforePatchRowNumber": 264,
                "afterPatchRowNumber": 267,
                "PatchRowcode": "             payload.path = payload.url"
            },
            "24": {
                "beforePatchRowNumber": 265,
                "afterPatchRowNumber": 268,
                "PatchRowcode": "         elif not block.proxy_url:"
            },
            "25": {
                "beforePatchRowNumber": 266,
                "afterPatchRowNumber": 269,
                "PatchRowcode": "             # If the file is on a remote server, do not move it to cache."
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 270,
                "PatchRowcode": "+            if check_in_upload_folder and not client_utils.is_http_url_like("
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 271,
                "PatchRowcode": "+                payload.path"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 272,
                "PatchRowcode": "+            ):"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 273,
                "PatchRowcode": "+                path = os.path.abspath(payload.path)"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 274,
                "PatchRowcode": "+                if not is_in_or_equal(path, get_upload_folder()):"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 275,
                "PatchRowcode": "+                    raise ValueError("
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 276,
                "PatchRowcode": "+                        f\"File {path} is not in the upload folder and cannot be accessed.\""
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 277,
                "PatchRowcode": "+                    )"
            },
            "34": {
                "beforePatchRowNumber": 267,
                "afterPatchRowNumber": 278,
                "PatchRowcode": "             temp_file_path = block.move_resource_to_block_cache(payload.path)"
            },
            "35": {
                "beforePatchRowNumber": 268,
                "afterPatchRowNumber": 279,
                "PatchRowcode": "             if temp_file_path is None:"
            },
            "36": {
                "beforePatchRowNumber": 269,
                "afterPatchRowNumber": 280,
                "PatchRowcode": "                 raise ValueError(\"Did not determine a file path for the resource.\")"
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import base64",
            "import hashlib",
            "import json",
            "import logging",
            "import os",
            "import shutil",
            "import subprocess",
            "import tempfile",
            "import warnings",
            "from io import BytesIO",
            "from pathlib import Path",
            "from typing import TYPE_CHECKING, Any, Literal",
            "",
            "import httpx",
            "import numpy as np",
            "from gradio_client import utils as client_utils",
            "from PIL import Image, ImageOps, PngImagePlugin",
            "",
            "from gradio import wasm_utils",
            "from gradio.data_classes import FileData, GradioModel, GradioRootModel",
            "from gradio.utils import abspath",
            "",
            "with warnings.catch_warnings():",
            "    warnings.simplefilter(\"ignore\")  # Ignore pydub warning if ffmpeg is not installed",
            "    from pydub import AudioSegment",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "if TYPE_CHECKING:",
            "    from gradio.components.base import Component",
            "",
            "#########################",
            "# GENERAL",
            "#########################",
            "",
            "",
            "def to_binary(x: str | dict) -> bytes:",
            "    \"\"\"Converts a base64 string or dictionary to a binary string that can be sent in a POST.\"\"\"",
            "    if isinstance(x, dict):",
            "        if x.get(\"data\"):",
            "            base64str = x[\"data\"]",
            "        else:",
            "            base64str = client_utils.encode_url_or_file_to_base64(x[\"path\"])",
            "    else:",
            "        base64str = x",
            "    return base64.b64decode(extract_base64_data(base64str))",
            "",
            "",
            "def extract_base64_data(x: str) -> str:",
            "    \"\"\"Just extracts the base64 data from a general base64 string.\"\"\"",
            "    return x.rsplit(\",\", 1)[-1]",
            "",
            "",
            "#########################",
            "# IMAGE PRE-PROCESSING",
            "#########################",
            "",
            "",
            "def encode_plot_to_base64(plt):",
            "    with BytesIO() as output_bytes:",
            "        plt.savefig(output_bytes, format=\"png\")",
            "        bytes_data = output_bytes.getvalue()",
            "    base64_str = str(base64.b64encode(bytes_data), \"utf-8\")",
            "    return \"data:image/png;base64,\" + base64_str",
            "",
            "",
            "def get_pil_metadata(pil_image):",
            "    # Copy any text-only metadata",
            "    metadata = PngImagePlugin.PngInfo()",
            "    for key, value in pil_image.info.items():",
            "        if isinstance(key, str) and isinstance(value, str):",
            "            metadata.add_text(key, value)",
            "",
            "    return metadata",
            "",
            "",
            "def encode_pil_to_bytes(pil_image, format=\"png\"):",
            "    with BytesIO() as output_bytes:",
            "        pil_image.save(output_bytes, format, pnginfo=get_pil_metadata(pil_image))",
            "        return output_bytes.getvalue()",
            "",
            "",
            "def encode_pil_to_base64(pil_image):",
            "    bytes_data = encode_pil_to_bytes(pil_image)",
            "    base64_str = str(base64.b64encode(bytes_data), \"utf-8\")",
            "    return \"data:image/png;base64,\" + base64_str",
            "",
            "",
            "def encode_array_to_base64(image_array):",
            "    with BytesIO() as output_bytes:",
            "        pil_image = Image.fromarray(_convert(image_array, np.uint8, force_copy=False))",
            "        pil_image.save(output_bytes, \"PNG\")",
            "        bytes_data = output_bytes.getvalue()",
            "    base64_str = str(base64.b64encode(bytes_data), \"utf-8\")",
            "    return \"data:image/png;base64,\" + base64_str",
            "",
            "",
            "def hash_file(file_path: str | Path, chunk_num_blocks: int = 128) -> str:",
            "    sha1 = hashlib.sha1()",
            "    with open(file_path, \"rb\") as f:",
            "        for chunk in iter(lambda: f.read(chunk_num_blocks * sha1.block_size), b\"\"):",
            "            sha1.update(chunk)",
            "    return sha1.hexdigest()",
            "",
            "",
            "def hash_url(url: str) -> str:",
            "    sha1 = hashlib.sha1()",
            "    sha1.update(url.encode(\"utf-8\"))",
            "    return sha1.hexdigest()",
            "",
            "",
            "def hash_bytes(bytes: bytes):",
            "    sha1 = hashlib.sha1()",
            "    sha1.update(bytes)",
            "    return sha1.hexdigest()",
            "",
            "",
            "def hash_base64(base64_encoding: str, chunk_num_blocks: int = 128) -> str:",
            "    sha1 = hashlib.sha1()",
            "    for i in range(0, len(base64_encoding), chunk_num_blocks * sha1.block_size):",
            "        data = base64_encoding[i : i + chunk_num_blocks * sha1.block_size]",
            "        sha1.update(data.encode(\"utf-8\"))",
            "    return sha1.hexdigest()",
            "",
            "",
            "def save_pil_to_cache(",
            "    img: Image.Image,",
            "    cache_dir: str,",
            "    name: str = \"image\",",
            "    format: Literal[\"png\", \"jpeg\"] = \"png\",",
            ") -> str:",
            "    bytes_data = encode_pil_to_bytes(img, format)",
            "    temp_dir = Path(cache_dir) / hash_bytes(bytes_data)",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "    filename = str((temp_dir / f\"{name}.{format}\").resolve())",
            "    (temp_dir / f\"{name}.{format}\").resolve().write_bytes(bytes_data)",
            "    return filename",
            "",
            "",
            "def save_img_array_to_cache(",
            "    arr: np.ndarray, cache_dir: str, format: Literal[\"png\", \"jpeg\"] = \"png\"",
            ") -> str:",
            "    pil_image = Image.fromarray(_convert(arr, np.uint8, force_copy=False))",
            "    return save_pil_to_cache(pil_image, cache_dir, format=format)",
            "",
            "",
            "def save_audio_to_cache(",
            "    data: np.ndarray, sample_rate: int, format: str, cache_dir: str",
            ") -> str:",
            "    temp_dir = Path(cache_dir) / hash_bytes(data.tobytes())",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "    filename = str((temp_dir / f\"audio.{format}\").resolve())",
            "    audio_to_file(sample_rate, data, filename, format=format)",
            "    return filename",
            "",
            "",
            "def save_bytes_to_cache(data: bytes, file_name: str, cache_dir: str) -> str:",
            "    path = Path(cache_dir) / hash_bytes(data)",
            "    path.mkdir(exist_ok=True, parents=True)",
            "    path = path / Path(file_name).name",
            "    path.write_bytes(data)",
            "    return str(path.resolve())",
            "",
            "",
            "def save_file_to_cache(file_path: str | Path, cache_dir: str) -> str:",
            "    \"\"\"Returns a temporary file path for a copy of the given file path if it does",
            "    not already exist. Otherwise returns the path to the existing temp file.\"\"\"",
            "    temp_dir = hash_file(file_path)",
            "    temp_dir = Path(cache_dir) / temp_dir",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "",
            "    name = client_utils.strip_invalid_filename_characters(Path(file_path).name)",
            "    full_temp_file_path = str(abspath(temp_dir / name))",
            "",
            "    if not Path(full_temp_file_path).exists():",
            "        shutil.copy2(file_path, full_temp_file_path)",
            "",
            "    return full_temp_file_path",
            "",
            "",
            "def save_url_to_cache(url: str, cache_dir: str) -> str:",
            "    \"\"\"Downloads a file and makes a temporary file path for a copy if does not already",
            "    exist. Otherwise returns the path to the existing temp file.\"\"\"",
            "    temp_dir = hash_url(url)",
            "    temp_dir = Path(cache_dir) / temp_dir",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "    name = client_utils.strip_invalid_filename_characters(Path(url).name)",
            "    full_temp_file_path = str(abspath(temp_dir / name))",
            "",
            "    if not Path(full_temp_file_path).exists():",
            "        with httpx.stream(\"GET\", url, follow_redirects=True) as r, open(",
            "            full_temp_file_path, \"wb\"",
            "        ) as f:",
            "            for chunk in r.iter_raw():",
            "                f.write(chunk)",
            "",
            "    return full_temp_file_path",
            "",
            "",
            "def save_base64_to_cache(",
            "    base64_encoding: str, cache_dir: str, file_name: str | None = None",
            ") -> str:",
            "    \"\"\"Converts a base64 encoding to a file and returns the path to the file if",
            "    the file doesn't already exist. Otherwise returns the path to the existing file.",
            "    \"\"\"",
            "    temp_dir = hash_base64(base64_encoding)",
            "    temp_dir = Path(cache_dir) / temp_dir",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "",
            "    guess_extension = client_utils.get_extension(base64_encoding)",
            "    if file_name:",
            "        file_name = client_utils.strip_invalid_filename_characters(file_name)",
            "    elif guess_extension:",
            "        file_name = f\"file.{guess_extension}\"",
            "    else:",
            "        file_name = \"file\"",
            "",
            "    full_temp_file_path = str(abspath(temp_dir / file_name))  # type: ignore",
            "",
            "    if not Path(full_temp_file_path).exists():",
            "        data, _ = client_utils.decode_base64_to_binary(base64_encoding)",
            "        with open(full_temp_file_path, \"wb\") as fb:",
            "            fb.write(data)",
            "",
            "    return full_temp_file_path",
            "",
            "",
            "def move_resource_to_block_cache(",
            "    url_or_file_path: str | Path | None, block: Component",
            ") -> str | None:",
            "    \"\"\"This method has been replaced by Block.move_resource_to_block_cache(), but is",
            "    left here for backwards compatibility for any custom components created in Gradio 4.2.0 or earlier.",
            "    \"\"\"",
            "    return block.move_resource_to_block_cache(url_or_file_path)",
            "",
            "",
            "def move_files_to_cache(",
            "    data: Any,",
            "    block: Component,",
            "    postprocess: bool = False,",
            "    add_urls=False,",
            ") -> dict:",
            "    \"\"\"Move any files in `data` to cache and (optionally), adds URL prefixes (/file=...) needed to access the cached file.",
            "    Also handles the case where the file is on an external Gradio app (/proxy=...).",
            "",
            "    Runs after .postprocess() and before .preprocess().",
            "",
            "    Args:",
            "        data: The input or output data for a component. Can be a dictionary or a dataclass",
            "        block: The component whose data is being processed",
            "        postprocess: Whether its running from postprocessing",
            "        root_url: The root URL of the local server, if applicable",
            "    \"\"\"",
            "",
            "    def _move_to_cache(d: dict):",
            "        payload = FileData(**d)",
            "        # If the gradio app developer is returning a URL from",
            "        # postprocess, it means the component can display a URL",
            "        # without it being served from the gradio server",
            "        # This makes it so that the URL is not downloaded and speeds up event processing",
            "        if payload.url and postprocess:",
            "            payload.path = payload.url",
            "        elif not block.proxy_url:",
            "            # If the file is on a remote server, do not move it to cache.",
            "            temp_file_path = block.move_resource_to_block_cache(payload.path)",
            "            if temp_file_path is None:",
            "                raise ValueError(\"Did not determine a file path for the resource.\")",
            "            payload.path = temp_file_path",
            "",
            "        if add_urls:",
            "            url_prefix = \"/stream/\" if payload.is_stream else \"/file=\"",
            "            if block.proxy_url:",
            "                proxy_url = block.proxy_url.rstrip(\"/\")",
            "                url = f\"/proxy={proxy_url}{url_prefix}{payload.path}\"",
            "            elif client_utils.is_http_url_like(payload.path) or payload.path.startswith(",
            "                f\"{url_prefix}\"",
            "            ):",
            "                url = payload.path",
            "            else:",
            "                url = f\"{url_prefix}{payload.path}\"",
            "            payload.url = url",
            "",
            "        return payload.model_dump()",
            "",
            "    if isinstance(data, (GradioRootModel, GradioModel)):",
            "        data = data.model_dump()",
            "",
            "    return client_utils.traverse(data, _move_to_cache, client_utils.is_file_obj)",
            "",
            "",
            "def add_root_url(data: dict, root_url: str, previous_root_url: str | None) -> dict:",
            "    def _add_root_url(file_dict: dict):",
            "        if not client_utils.is_http_url_like(file_dict[\"url\"]):",
            "            if previous_root_url and file_dict[\"url\"].startswith(previous_root_url):",
            "                file_dict[\"url\"] = file_dict[\"url\"][len(previous_root_url) :]",
            "            file_dict[\"url\"] = f'{root_url}{file_dict[\"url\"]}'",
            "        return file_dict",
            "",
            "    return client_utils.traverse(data, _add_root_url, client_utils.is_file_obj_with_url)",
            "",
            "",
            "def resize_and_crop(img, size, crop_type=\"center\"):",
            "    \"\"\"",
            "    Resize and crop an image to fit the specified size.",
            "    args:",
            "        size: `(width, height)` tuple. Pass `None` for either width or height",
            "        to only crop and resize the other.",
            "        crop_type: can be 'top', 'middle' or 'bottom', depending on this",
            "            value, the image will cropped getting the 'top/left', 'middle' or",
            "            'bottom/right' of the image to fit the size.",
            "    raises:",
            "        ValueError: if an invalid `crop_type` is provided.",
            "    \"\"\"",
            "    if crop_type == \"top\":",
            "        center = (0, 0)",
            "    elif crop_type == \"center\":",
            "        center = (0.5, 0.5)",
            "    else:",
            "        raise ValueError",
            "",
            "    resize = list(size)",
            "    if size[0] is None:",
            "        resize[0] = img.size[0]",
            "    if size[1] is None:",
            "        resize[1] = img.size[1]",
            "    return ImageOps.fit(img, resize, centering=center)  # type: ignore",
            "",
            "",
            "##################",
            "# Audio",
            "##################",
            "",
            "",
            "def audio_from_file(filename, crop_min=0, crop_max=100):",
            "    try:",
            "        audio = AudioSegment.from_file(filename)",
            "    except FileNotFoundError as e:",
            "        isfile = Path(filename).is_file()",
            "        msg = (",
            "            f\"Cannot load audio from file: `{'ffprobe' if isfile else filename}` not found.\"",
            "            + \" Please install `ffmpeg` in your system to use non-WAV audio file formats\"",
            "            \" and make sure `ffprobe` is in your PATH.\"",
            "            if isfile",
            "            else \"\"",
            "        )",
            "        raise RuntimeError(msg) from e",
            "    if crop_min != 0 or crop_max != 100:",
            "        audio_start = len(audio) * crop_min / 100",
            "        audio_end = len(audio) * crop_max / 100",
            "        audio = audio[audio_start:audio_end]",
            "    data = np.array(audio.get_array_of_samples())",
            "    if audio.channels > 1:",
            "        data = data.reshape(-1, audio.channels)",
            "    return audio.frame_rate, data",
            "",
            "",
            "def audio_to_file(sample_rate, data, filename, format=\"wav\"):",
            "    if format == \"wav\":",
            "        data = convert_to_16_bit_wav(data)",
            "    audio = AudioSegment(",
            "        data.tobytes(),",
            "        frame_rate=sample_rate,",
            "        sample_width=data.dtype.itemsize,",
            "        channels=(1 if len(data.shape) == 1 else data.shape[1]),",
            "    )",
            "    file = audio.export(filename, format=format)",
            "    file.close()  # type: ignore",
            "",
            "",
            "def convert_to_16_bit_wav(data):",
            "    # Based on: https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.write.html",
            "    warning = \"Trying to convert audio automatically from {} to 16-bit int format.\"",
            "    if data.dtype in [np.float64, np.float32, np.float16]:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data / np.abs(data).max()",
            "        data = data * 32767",
            "        data = data.astype(np.int16)",
            "    elif data.dtype == np.int32:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data / 65536",
            "        data = data.astype(np.int16)",
            "    elif data.dtype == np.int16:",
            "        pass",
            "    elif data.dtype == np.uint16:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data - 32768",
            "        data = data.astype(np.int16)",
            "    elif data.dtype == np.uint8:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data * 257 - 32768",
            "        data = data.astype(np.int16)",
            "    elif data.dtype == np.int8:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data * 256",
            "        data = data.astype(np.int16)",
            "    else:",
            "        raise ValueError(",
            "            \"Audio data cannot be converted automatically from \"",
            "            f\"{data.dtype} to 16-bit int format.\"",
            "        )",
            "    return data",
            "",
            "",
            "##################",
            "# OUTPUT",
            "##################",
            "",
            "",
            "def _convert(image, dtype, force_copy=False, uniform=False):",
            "    \"\"\"",
            "    Adapted from: https://github.com/scikit-image/scikit-image/blob/main/skimage/util/dtype.py#L510-L531",
            "",
            "    Convert an image to the requested data-type.",
            "    Warnings are issued in case of precision loss, or when negative values",
            "    are clipped during conversion to unsigned integer types (sign loss).",
            "    Floating point values are expected to be normalized and will be clipped",
            "    to the range [0.0, 1.0] or [-1.0, 1.0] when converting to unsigned or",
            "    signed integers respectively.",
            "    Numbers are not shifted to the negative side when converting from",
            "    unsigned to signed integer types. Negative values will be clipped when",
            "    converting to unsigned integers.",
            "    Parameters",
            "    ----------",
            "    image : ndarray",
            "        Input image.",
            "    dtype : dtype",
            "        Target data-type.",
            "    force_copy : bool, optional",
            "        Force a copy of the data, irrespective of its current dtype.",
            "    uniform : bool, optional",
            "        Uniformly quantize the floating point range to the integer range.",
            "        By default (uniform=False) floating point values are scaled and",
            "        rounded to the nearest integers, which minimizes back and forth",
            "        conversion errors.",
            "    .. versionchanged :: 0.15",
            "        ``_convert`` no longer warns about possible precision or sign",
            "        information loss. See discussions on these warnings at:",
            "        https://github.com/scikit-image/scikit-image/issues/2602",
            "        https://github.com/scikit-image/scikit-image/issues/543#issuecomment-208202228",
            "        https://github.com/scikit-image/scikit-image/pull/3575",
            "    References",
            "    ----------",
            "    .. [1] DirectX data conversion rules.",
            "           https://msdn.microsoft.com/en-us/library/windows/desktop/dd607323%28v=vs.85%29.aspx",
            "    .. [2] Data Conversions. In \"OpenGL ES 2.0 Specification v2.0.25\",",
            "           pp 7-8. Khronos Group, 2010.",
            "    .. [3] Proper treatment of pixels as integers. A.W. Paeth.",
            "           In \"Graphics Gems I\", pp 249-256. Morgan Kaufmann, 1990.",
            "    .. [4] Dirty Pixels. J. Blinn. In \"Jim Blinn's corner: Dirty Pixels\",",
            "           pp 47-57. Morgan Kaufmann, 1998.",
            "    \"\"\"",
            "    dtype_range = {",
            "        bool: (False, True),",
            "        np.bool_: (False, True),",
            "        np.bool8: (False, True),  # type: ignore",
            "        float: (-1, 1),",
            "        np.float_: (-1, 1),",
            "        np.float16: (-1, 1),",
            "        np.float32: (-1, 1),",
            "        np.float64: (-1, 1),",
            "    }",
            "",
            "    def _dtype_itemsize(itemsize, *dtypes):",
            "        \"\"\"Return first of `dtypes` with itemsize greater than `itemsize`",
            "        Parameters",
            "        ----------",
            "        itemsize: int",
            "            The data type object element size.",
            "        Other Parameters",
            "        ----------------",
            "        *dtypes:",
            "            Any Object accepted by `np.dtype` to be converted to a data",
            "            type object",
            "        Returns",
            "        -------",
            "        dtype: data type object",
            "            First of `dtypes` with itemsize greater than `itemsize`.",
            "        \"\"\"",
            "        return next(dt for dt in dtypes if np.dtype(dt).itemsize >= itemsize)",
            "",
            "    def _dtype_bits(kind, bits, itemsize=1):",
            "        \"\"\"Return dtype of `kind` that can store a `bits` wide unsigned int",
            "        Parameters:",
            "        kind: str",
            "            Data type kind.",
            "        bits: int",
            "            Desired number of bits.",
            "        itemsize: int",
            "            The data type object element size.",
            "        Returns",
            "        -------",
            "        dtype: data type object",
            "            Data type of `kind` that can store a `bits` wide unsigned int",
            "        \"\"\"",
            "",
            "        s = next(",
            "            i",
            "            for i in (itemsize,) + (2, 4, 8)",
            "            if bits < (i * 8) or (bits == (i * 8) and kind == \"u\")",
            "        )",
            "",
            "        return np.dtype(kind + str(s))",
            "",
            "    def _scale(a, n, m, copy=True):",
            "        \"\"\"Scale an array of unsigned/positive integers from `n` to `m` bits.",
            "        Numbers can be represented exactly only if `m` is a multiple of `n`.",
            "        Parameters",
            "        ----------",
            "        a : ndarray",
            "            Input image array.",
            "        n : int",
            "            Number of bits currently used to encode the values in `a`.",
            "        m : int",
            "            Desired number of bits to encode the values in `out`.",
            "        copy : bool, optional",
            "            If True, allocates and returns new array. Otherwise, modifies",
            "            `a` in place.",
            "        Returns",
            "        -------",
            "        out : array",
            "            Output image array. Has the same kind as `a`.",
            "        \"\"\"",
            "        kind = a.dtype.kind",
            "        if n > m and a.max() < 2**m:",
            "            return a.astype(_dtype_bits(kind, m))",
            "        elif n == m:",
            "            return a.copy() if copy else a",
            "        elif n > m:",
            "            # downscale with precision loss",
            "            if copy:",
            "                b = np.empty(a.shape, _dtype_bits(kind, m))",
            "                np.floor_divide(a, 2 ** (n - m), out=b, dtype=a.dtype, casting=\"unsafe\")",
            "                return b",
            "            else:",
            "                a //= 2 ** (n - m)",
            "                return a",
            "        elif m % n == 0:",
            "            # exact upscale to a multiple of `n` bits",
            "            if copy:",
            "                b = np.empty(a.shape, _dtype_bits(kind, m))",
            "                np.multiply(a, (2**m - 1) // (2**n - 1), out=b, dtype=b.dtype)",
            "                return b",
            "            else:",
            "                a = a.astype(_dtype_bits(kind, m, a.dtype.itemsize), copy=False)",
            "                a *= (2**m - 1) // (2**n - 1)",
            "                return a",
            "        else:",
            "            # upscale to a multiple of `n` bits,",
            "            # then downscale with precision loss",
            "            o = (m // n + 1) * n",
            "            if copy:",
            "                b = np.empty(a.shape, _dtype_bits(kind, o))",
            "                np.multiply(a, (2**o - 1) // (2**n - 1), out=b, dtype=b.dtype)",
            "                b //= 2 ** (o - m)",
            "                return b",
            "            else:",
            "                a = a.astype(_dtype_bits(kind, o, a.dtype.itemsize), copy=False)",
            "                a *= (2**o - 1) // (2**n - 1)",
            "                a //= 2 ** (o - m)",
            "                return a",
            "",
            "    image = np.asarray(image)",
            "    dtypeobj_in = image.dtype",
            "    dtypeobj_out = np.dtype(\"float64\") if dtype is np.floating else np.dtype(dtype)",
            "    dtype_in = dtypeobj_in.type",
            "    dtype_out = dtypeobj_out.type",
            "    kind_in = dtypeobj_in.kind",
            "    kind_out = dtypeobj_out.kind",
            "    itemsize_in = dtypeobj_in.itemsize",
            "    itemsize_out = dtypeobj_out.itemsize",
            "",
            "    # Below, we do an `issubdtype` check.  Its purpose is to find out",
            "    # whether we can get away without doing any image conversion.  This happens",
            "    # when:",
            "    #",
            "    # - the output and input dtypes are the same or",
            "    # - when the output is specified as a type, and the input dtype",
            "    #   is a subclass of that type (e.g. `np.floating` will allow",
            "    #   `float32` and `float64` arrays through)",
            "",
            "    if np.issubdtype(dtype_in, np.obj2sctype(dtype)):",
            "        if force_copy:",
            "            image = image.copy()",
            "        return image",
            "",
            "    if kind_in in \"ui\":",
            "        imin_in = np.iinfo(dtype_in).min",
            "        imax_in = np.iinfo(dtype_in).max",
            "    if kind_out in \"ui\":",
            "        imin_out = np.iinfo(dtype_out).min  # type: ignore",
            "        imax_out = np.iinfo(dtype_out).max  # type: ignore",
            "",
            "    # any -> binary",
            "    if kind_out == \"b\":",
            "        return image > dtype_in(dtype_range[dtype_in][1] / 2)",
            "",
            "    # binary -> any",
            "    if kind_in == \"b\":",
            "        result = image.astype(dtype_out)",
            "        if kind_out != \"f\":",
            "            result *= dtype_out(dtype_range[dtype_out][1])",
            "        return result",
            "",
            "    # float -> any",
            "    if kind_in == \"f\":",
            "        if kind_out == \"f\":",
            "            # float -> float",
            "            return image.astype(dtype_out)",
            "",
            "        if np.min(image) < -1.0 or np.max(image) > 1.0:",
            "            raise ValueError(\"Images of type float must be between -1 and 1.\")",
            "        # floating point -> integer",
            "        # use float type that can represent output integer type",
            "        computation_type = _dtype_itemsize(",
            "            itemsize_out, dtype_in, np.float32, np.float64",
            "        )",
            "",
            "        if not uniform:",
            "            if kind_out == \"u\":",
            "                image_out = np.multiply(image, imax_out, dtype=computation_type)  # type: ignore",
            "            else:",
            "                image_out = np.multiply(",
            "                    image,",
            "                    (imax_out - imin_out) / 2,  # type: ignore",
            "                    dtype=computation_type,",
            "                )",
            "                image_out -= 1.0 / 2.0",
            "            np.rint(image_out, out=image_out)",
            "            np.clip(image_out, imin_out, imax_out, out=image_out)  # type: ignore",
            "        elif kind_out == \"u\":",
            "            image_out = np.multiply(image, imax_out + 1, dtype=computation_type)  # type: ignore",
            "            np.clip(image_out, 0, imax_out, out=image_out)  # type: ignore",
            "        else:",
            "            image_out = np.multiply(",
            "                image,",
            "                (imax_out - imin_out + 1.0) / 2.0,  # type: ignore",
            "                dtype=computation_type,",
            "            )",
            "            np.floor(image_out, out=image_out)",
            "            np.clip(image_out, imin_out, imax_out, out=image_out)  # type: ignore",
            "        return image_out.astype(dtype_out)",
            "",
            "    # signed/unsigned int -> float",
            "    if kind_out == \"f\":",
            "        # use float type that can exactly represent input integers",
            "        computation_type = _dtype_itemsize(",
            "            itemsize_in, dtype_out, np.float32, np.float64",
            "        )",
            "",
            "        if kind_in == \"u\":",
            "            # using np.divide or np.multiply doesn't copy the data",
            "            # until the computation time",
            "            image = np.multiply(image, 1.0 / imax_in, dtype=computation_type)  # type: ignore",
            "            # DirectX uses this conversion also for signed ints",
            "            # if imin_in:",
            "            #     np.maximum(image, -1.0, out=image)",
            "        else:",
            "            image = np.add(image, 0.5, dtype=computation_type)",
            "            image *= 2 / (imax_in - imin_in)  # type: ignore",
            "",
            "        return np.asarray(image, dtype_out)",
            "",
            "    # unsigned int -> signed/unsigned int",
            "    if kind_in == \"u\":",
            "        if kind_out == \"i\":",
            "            # unsigned int -> signed int",
            "            image = _scale(image, 8 * itemsize_in, 8 * itemsize_out - 1)",
            "            return image.view(dtype_out)",
            "        else:",
            "            # unsigned int -> unsigned int",
            "            return _scale(image, 8 * itemsize_in, 8 * itemsize_out)",
            "",
            "    # signed int -> unsigned int",
            "    if kind_out == \"u\":",
            "        image = _scale(image, 8 * itemsize_in - 1, 8 * itemsize_out)",
            "        result = np.empty(image.shape, dtype_out)",
            "        np.maximum(image, 0, out=result, dtype=image.dtype, casting=\"unsafe\")",
            "        return result",
            "",
            "    # signed int -> signed int",
            "    if itemsize_in > itemsize_out:",
            "        return _scale(image, 8 * itemsize_in - 1, 8 * itemsize_out - 1)",
            "",
            "    image = image.astype(_dtype_bits(\"i\", itemsize_out * 8))",
            "    image -= imin_in  # type: ignore",
            "    image = _scale(image, 8 * itemsize_in, 8 * itemsize_out, copy=False)",
            "    image += imin_out  # type: ignore",
            "    return image.astype(dtype_out)",
            "",
            "",
            "def ffmpeg_installed() -> bool:",
            "    if wasm_utils.IS_WASM:",
            "        # TODO: Support ffmpeg in WASM",
            "        return False",
            "",
            "    return shutil.which(\"ffmpeg\") is not None",
            "",
            "",
            "def video_is_playable(video_filepath: str) -> bool:",
            "    \"\"\"Determines if a video is playable in the browser.",
            "",
            "    A video is playable if it has a playable container and codec.",
            "        .mp4 -> h264",
            "        .webm -> vp9",
            "        .ogg -> theora",
            "    \"\"\"",
            "    from ffmpy import FFprobe, FFRuntimeError",
            "",
            "    try:",
            "        container = Path(video_filepath).suffix.lower()",
            "        probe = FFprobe(",
            "            global_options=\"-show_format -show_streams -select_streams v -print_format json\",",
            "            inputs={video_filepath: None},",
            "        )",
            "        output = probe.run(stderr=subprocess.PIPE, stdout=subprocess.PIPE)",
            "        output = json.loads(output[0])",
            "        video_codec = output[\"streams\"][0][\"codec_name\"]",
            "        return (container, video_codec) in [",
            "            (\".mp4\", \"h264\"),",
            "            (\".ogg\", \"theora\"),",
            "            (\".webm\", \"vp9\"),",
            "        ]",
            "    # If anything goes wrong, assume the video can be played to not convert downstream",
            "    except (FFRuntimeError, IndexError, KeyError):",
            "        return True",
            "",
            "",
            "def convert_video_to_playable_mp4(video_path: str) -> str:",
            "    \"\"\"Convert the video to mp4. If something goes wrong return the original video.\"\"\"",
            "    from ffmpy import FFmpeg, FFRuntimeError",
            "",
            "    try:",
            "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:",
            "            output_path = Path(video_path).with_suffix(\".mp4\")",
            "            shutil.copy2(video_path, tmp_file.name)",
            "            # ffmpeg will automatically use h264 codec (playable in browser) when converting to mp4",
            "            ff = FFmpeg(",
            "                inputs={str(tmp_file.name): None},",
            "                outputs={str(output_path): None},",
            "                global_options=\"-y -loglevel quiet\",",
            "            )",
            "            ff.run()",
            "    except FFRuntimeError as e:",
            "        print(f\"Error converting video to browser-playable format {str(e)}\")",
            "        output_path = video_path",
            "    finally:",
            "        # Remove temp file",
            "        os.remove(tmp_file.name)  # type: ignore",
            "    return str(output_path)",
            "",
            "",
            "def get_video_length(video_path: str | Path):",
            "    if wasm_utils.IS_WASM:",
            "        raise wasm_utils.WasmUnsupportedError(",
            "            \"Video duration is not supported in the Wasm mode.\"",
            "        )",
            "    duration = subprocess.check_output(",
            "        [",
            "            \"ffprobe\",",
            "            \"-i\",",
            "            str(video_path),",
            "            \"-show_entries\",",
            "            \"format=duration\",",
            "            \"-v\",",
            "            \"quiet\",",
            "            \"-of\",",
            "            \"csv={}\".format(\"p=0\"),",
            "        ]",
            "    )",
            "    duration_str = duration.decode(\"utf-8\").strip()",
            "    duration_float = float(duration_str)",
            "",
            "    return duration_float"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import base64",
            "import hashlib",
            "import json",
            "import logging",
            "import os",
            "import shutil",
            "import subprocess",
            "import tempfile",
            "import warnings",
            "from io import BytesIO",
            "from pathlib import Path",
            "from typing import TYPE_CHECKING, Any, Literal",
            "",
            "import httpx",
            "import numpy as np",
            "from gradio_client import utils as client_utils",
            "from PIL import Image, ImageOps, PngImagePlugin",
            "",
            "from gradio import wasm_utils",
            "from gradio.data_classes import FileData, GradioModel, GradioRootModel",
            "from gradio.utils import abspath, get_upload_folder, is_in_or_equal",
            "",
            "with warnings.catch_warnings():",
            "    warnings.simplefilter(\"ignore\")  # Ignore pydub warning if ffmpeg is not installed",
            "    from pydub import AudioSegment",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "if TYPE_CHECKING:",
            "    from gradio.components.base import Component",
            "",
            "#########################",
            "# GENERAL",
            "#########################",
            "",
            "",
            "def to_binary(x: str | dict) -> bytes:",
            "    \"\"\"Converts a base64 string or dictionary to a binary string that can be sent in a POST.\"\"\"",
            "    if isinstance(x, dict):",
            "        if x.get(\"data\"):",
            "            base64str = x[\"data\"]",
            "        else:",
            "            base64str = client_utils.encode_url_or_file_to_base64(x[\"path\"])",
            "    else:",
            "        base64str = x",
            "    return base64.b64decode(extract_base64_data(base64str))",
            "",
            "",
            "def extract_base64_data(x: str) -> str:",
            "    \"\"\"Just extracts the base64 data from a general base64 string.\"\"\"",
            "    return x.rsplit(\",\", 1)[-1]",
            "",
            "",
            "#########################",
            "# IMAGE PRE-PROCESSING",
            "#########################",
            "",
            "",
            "def encode_plot_to_base64(plt):",
            "    with BytesIO() as output_bytes:",
            "        plt.savefig(output_bytes, format=\"png\")",
            "        bytes_data = output_bytes.getvalue()",
            "    base64_str = str(base64.b64encode(bytes_data), \"utf-8\")",
            "    return \"data:image/png;base64,\" + base64_str",
            "",
            "",
            "def get_pil_metadata(pil_image):",
            "    # Copy any text-only metadata",
            "    metadata = PngImagePlugin.PngInfo()",
            "    for key, value in pil_image.info.items():",
            "        if isinstance(key, str) and isinstance(value, str):",
            "            metadata.add_text(key, value)",
            "",
            "    return metadata",
            "",
            "",
            "def encode_pil_to_bytes(pil_image, format=\"png\"):",
            "    with BytesIO() as output_bytes:",
            "        pil_image.save(output_bytes, format, pnginfo=get_pil_metadata(pil_image))",
            "        return output_bytes.getvalue()",
            "",
            "",
            "def encode_pil_to_base64(pil_image):",
            "    bytes_data = encode_pil_to_bytes(pil_image)",
            "    base64_str = str(base64.b64encode(bytes_data), \"utf-8\")",
            "    return \"data:image/png;base64,\" + base64_str",
            "",
            "",
            "def encode_array_to_base64(image_array):",
            "    with BytesIO() as output_bytes:",
            "        pil_image = Image.fromarray(_convert(image_array, np.uint8, force_copy=False))",
            "        pil_image.save(output_bytes, \"PNG\")",
            "        bytes_data = output_bytes.getvalue()",
            "    base64_str = str(base64.b64encode(bytes_data), \"utf-8\")",
            "    return \"data:image/png;base64,\" + base64_str",
            "",
            "",
            "def hash_file(file_path: str | Path, chunk_num_blocks: int = 128) -> str:",
            "    sha1 = hashlib.sha1()",
            "    with open(file_path, \"rb\") as f:",
            "        for chunk in iter(lambda: f.read(chunk_num_blocks * sha1.block_size), b\"\"):",
            "            sha1.update(chunk)",
            "    return sha1.hexdigest()",
            "",
            "",
            "def hash_url(url: str) -> str:",
            "    sha1 = hashlib.sha1()",
            "    sha1.update(url.encode(\"utf-8\"))",
            "    return sha1.hexdigest()",
            "",
            "",
            "def hash_bytes(bytes: bytes):",
            "    sha1 = hashlib.sha1()",
            "    sha1.update(bytes)",
            "    return sha1.hexdigest()",
            "",
            "",
            "def hash_base64(base64_encoding: str, chunk_num_blocks: int = 128) -> str:",
            "    sha1 = hashlib.sha1()",
            "    for i in range(0, len(base64_encoding), chunk_num_blocks * sha1.block_size):",
            "        data = base64_encoding[i : i + chunk_num_blocks * sha1.block_size]",
            "        sha1.update(data.encode(\"utf-8\"))",
            "    return sha1.hexdigest()",
            "",
            "",
            "def save_pil_to_cache(",
            "    img: Image.Image,",
            "    cache_dir: str,",
            "    name: str = \"image\",",
            "    format: Literal[\"png\", \"jpeg\"] = \"png\",",
            ") -> str:",
            "    bytes_data = encode_pil_to_bytes(img, format)",
            "    temp_dir = Path(cache_dir) / hash_bytes(bytes_data)",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "    filename = str((temp_dir / f\"{name}.{format}\").resolve())",
            "    (temp_dir / f\"{name}.{format}\").resolve().write_bytes(bytes_data)",
            "    return filename",
            "",
            "",
            "def save_img_array_to_cache(",
            "    arr: np.ndarray, cache_dir: str, format: Literal[\"png\", \"jpeg\"] = \"png\"",
            ") -> str:",
            "    pil_image = Image.fromarray(_convert(arr, np.uint8, force_copy=False))",
            "    return save_pil_to_cache(pil_image, cache_dir, format=format)",
            "",
            "",
            "def save_audio_to_cache(",
            "    data: np.ndarray, sample_rate: int, format: str, cache_dir: str",
            ") -> str:",
            "    temp_dir = Path(cache_dir) / hash_bytes(data.tobytes())",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "    filename = str((temp_dir / f\"audio.{format}\").resolve())",
            "    audio_to_file(sample_rate, data, filename, format=format)",
            "    return filename",
            "",
            "",
            "def save_bytes_to_cache(data: bytes, file_name: str, cache_dir: str) -> str:",
            "    path = Path(cache_dir) / hash_bytes(data)",
            "    path.mkdir(exist_ok=True, parents=True)",
            "    path = path / Path(file_name).name",
            "    path.write_bytes(data)",
            "    return str(path.resolve())",
            "",
            "",
            "def save_file_to_cache(file_path: str | Path, cache_dir: str) -> str:",
            "    \"\"\"Returns a temporary file path for a copy of the given file path if it does",
            "    not already exist. Otherwise returns the path to the existing temp file.\"\"\"",
            "    temp_dir = hash_file(file_path)",
            "    temp_dir = Path(cache_dir) / temp_dir",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "",
            "    name = client_utils.strip_invalid_filename_characters(Path(file_path).name)",
            "    full_temp_file_path = str(abspath(temp_dir / name))",
            "",
            "    if not Path(full_temp_file_path).exists():",
            "        shutil.copy2(file_path, full_temp_file_path)",
            "",
            "    return full_temp_file_path",
            "",
            "",
            "def save_url_to_cache(url: str, cache_dir: str) -> str:",
            "    \"\"\"Downloads a file and makes a temporary file path for a copy if does not already",
            "    exist. Otherwise returns the path to the existing temp file.\"\"\"",
            "    temp_dir = hash_url(url)",
            "    temp_dir = Path(cache_dir) / temp_dir",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "    name = client_utils.strip_invalid_filename_characters(Path(url).name)",
            "    full_temp_file_path = str(abspath(temp_dir / name))",
            "",
            "    if not Path(full_temp_file_path).exists():",
            "        with httpx.stream(\"GET\", url, follow_redirects=True) as r, open(",
            "            full_temp_file_path, \"wb\"",
            "        ) as f:",
            "            for chunk in r.iter_raw():",
            "                f.write(chunk)",
            "",
            "    return full_temp_file_path",
            "",
            "",
            "def save_base64_to_cache(",
            "    base64_encoding: str, cache_dir: str, file_name: str | None = None",
            ") -> str:",
            "    \"\"\"Converts a base64 encoding to a file and returns the path to the file if",
            "    the file doesn't already exist. Otherwise returns the path to the existing file.",
            "    \"\"\"",
            "    temp_dir = hash_base64(base64_encoding)",
            "    temp_dir = Path(cache_dir) / temp_dir",
            "    temp_dir.mkdir(exist_ok=True, parents=True)",
            "",
            "    guess_extension = client_utils.get_extension(base64_encoding)",
            "    if file_name:",
            "        file_name = client_utils.strip_invalid_filename_characters(file_name)",
            "    elif guess_extension:",
            "        file_name = f\"file.{guess_extension}\"",
            "    else:",
            "        file_name = \"file\"",
            "",
            "    full_temp_file_path = str(abspath(temp_dir / file_name))  # type: ignore",
            "",
            "    if not Path(full_temp_file_path).exists():",
            "        data, _ = client_utils.decode_base64_to_binary(base64_encoding)",
            "        with open(full_temp_file_path, \"wb\") as fb:",
            "            fb.write(data)",
            "",
            "    return full_temp_file_path",
            "",
            "",
            "def move_resource_to_block_cache(",
            "    url_or_file_path: str | Path | None, block: Component",
            ") -> str | None:",
            "    \"\"\"This method has been replaced by Block.move_resource_to_block_cache(), but is",
            "    left here for backwards compatibility for any custom components created in Gradio 4.2.0 or earlier.",
            "    \"\"\"",
            "    return block.move_resource_to_block_cache(url_or_file_path)",
            "",
            "",
            "def move_files_to_cache(",
            "    data: Any,",
            "    block: Component,",
            "    postprocess: bool = False,",
            "    add_urls=False,",
            "    check_in_upload_folder=False,",
            ") -> dict:",
            "    \"\"\"Move any files in `data` to cache and (optionally), adds URL prefixes (/file=...) needed to access the cached file.",
            "    Also handles the case where the file is on an external Gradio app (/proxy=...).",
            "",
            "    Runs after .postprocess() and before .preprocess().",
            "",
            "    Args:",
            "        data: The input or output data for a component. Can be a dictionary or a dataclass",
            "        block: The component whose data is being processed",
            "        postprocess: Whether its running from postprocessing",
            "        root_url: The root URL of the local server, if applicable",
            "        add_urls: Whether to add URLs to the payload",
            "        check_in_upload_folder: If True, instead of moving the file to cache, checks if the file is in already in cache (exception if not).",
            "    \"\"\"",
            "",
            "    def _move_to_cache(d: dict):",
            "        payload = FileData(**d)",
            "        # If the gradio app developer is returning a URL from",
            "        # postprocess, it means the component can display a URL",
            "        # without it being served from the gradio server",
            "        # This makes it so that the URL is not downloaded and speeds up event processing",
            "        if payload.url and postprocess:",
            "            payload.path = payload.url",
            "        elif not block.proxy_url:",
            "            # If the file is on a remote server, do not move it to cache.",
            "            if check_in_upload_folder and not client_utils.is_http_url_like(",
            "                payload.path",
            "            ):",
            "                path = os.path.abspath(payload.path)",
            "                if not is_in_or_equal(path, get_upload_folder()):",
            "                    raise ValueError(",
            "                        f\"File {path} is not in the upload folder and cannot be accessed.\"",
            "                    )",
            "            temp_file_path = block.move_resource_to_block_cache(payload.path)",
            "            if temp_file_path is None:",
            "                raise ValueError(\"Did not determine a file path for the resource.\")",
            "            payload.path = temp_file_path",
            "",
            "        if add_urls:",
            "            url_prefix = \"/stream/\" if payload.is_stream else \"/file=\"",
            "            if block.proxy_url:",
            "                proxy_url = block.proxy_url.rstrip(\"/\")",
            "                url = f\"/proxy={proxy_url}{url_prefix}{payload.path}\"",
            "            elif client_utils.is_http_url_like(payload.path) or payload.path.startswith(",
            "                f\"{url_prefix}\"",
            "            ):",
            "                url = payload.path",
            "            else:",
            "                url = f\"{url_prefix}{payload.path}\"",
            "            payload.url = url",
            "",
            "        return payload.model_dump()",
            "",
            "    if isinstance(data, (GradioRootModel, GradioModel)):",
            "        data = data.model_dump()",
            "",
            "    return client_utils.traverse(data, _move_to_cache, client_utils.is_file_obj)",
            "",
            "",
            "def add_root_url(data: dict, root_url: str, previous_root_url: str | None) -> dict:",
            "    def _add_root_url(file_dict: dict):",
            "        if not client_utils.is_http_url_like(file_dict[\"url\"]):",
            "            if previous_root_url and file_dict[\"url\"].startswith(previous_root_url):",
            "                file_dict[\"url\"] = file_dict[\"url\"][len(previous_root_url) :]",
            "            file_dict[\"url\"] = f'{root_url}{file_dict[\"url\"]}'",
            "        return file_dict",
            "",
            "    return client_utils.traverse(data, _add_root_url, client_utils.is_file_obj_with_url)",
            "",
            "",
            "def resize_and_crop(img, size, crop_type=\"center\"):",
            "    \"\"\"",
            "    Resize and crop an image to fit the specified size.",
            "    args:",
            "        size: `(width, height)` tuple. Pass `None` for either width or height",
            "        to only crop and resize the other.",
            "        crop_type: can be 'top', 'middle' or 'bottom', depending on this",
            "            value, the image will cropped getting the 'top/left', 'middle' or",
            "            'bottom/right' of the image to fit the size.",
            "    raises:",
            "        ValueError: if an invalid `crop_type` is provided.",
            "    \"\"\"",
            "    if crop_type == \"top\":",
            "        center = (0, 0)",
            "    elif crop_type == \"center\":",
            "        center = (0.5, 0.5)",
            "    else:",
            "        raise ValueError",
            "",
            "    resize = list(size)",
            "    if size[0] is None:",
            "        resize[0] = img.size[0]",
            "    if size[1] is None:",
            "        resize[1] = img.size[1]",
            "    return ImageOps.fit(img, resize, centering=center)  # type: ignore",
            "",
            "",
            "##################",
            "# Audio",
            "##################",
            "",
            "",
            "def audio_from_file(filename, crop_min=0, crop_max=100):",
            "    try:",
            "        audio = AudioSegment.from_file(filename)",
            "    except FileNotFoundError as e:",
            "        isfile = Path(filename).is_file()",
            "        msg = (",
            "            f\"Cannot load audio from file: `{'ffprobe' if isfile else filename}` not found.\"",
            "            + \" Please install `ffmpeg` in your system to use non-WAV audio file formats\"",
            "            \" and make sure `ffprobe` is in your PATH.\"",
            "            if isfile",
            "            else \"\"",
            "        )",
            "        raise RuntimeError(msg) from e",
            "    if crop_min != 0 or crop_max != 100:",
            "        audio_start = len(audio) * crop_min / 100",
            "        audio_end = len(audio) * crop_max / 100",
            "        audio = audio[audio_start:audio_end]",
            "    data = np.array(audio.get_array_of_samples())",
            "    if audio.channels > 1:",
            "        data = data.reshape(-1, audio.channels)",
            "    return audio.frame_rate, data",
            "",
            "",
            "def audio_to_file(sample_rate, data, filename, format=\"wav\"):",
            "    if format == \"wav\":",
            "        data = convert_to_16_bit_wav(data)",
            "    audio = AudioSegment(",
            "        data.tobytes(),",
            "        frame_rate=sample_rate,",
            "        sample_width=data.dtype.itemsize,",
            "        channels=(1 if len(data.shape) == 1 else data.shape[1]),",
            "    )",
            "    file = audio.export(filename, format=format)",
            "    file.close()  # type: ignore",
            "",
            "",
            "def convert_to_16_bit_wav(data):",
            "    # Based on: https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.write.html",
            "    warning = \"Trying to convert audio automatically from {} to 16-bit int format.\"",
            "    if data.dtype in [np.float64, np.float32, np.float16]:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data / np.abs(data).max()",
            "        data = data * 32767",
            "        data = data.astype(np.int16)",
            "    elif data.dtype == np.int32:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data / 65536",
            "        data = data.astype(np.int16)",
            "    elif data.dtype == np.int16:",
            "        pass",
            "    elif data.dtype == np.uint16:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data - 32768",
            "        data = data.astype(np.int16)",
            "    elif data.dtype == np.uint8:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data * 257 - 32768",
            "        data = data.astype(np.int16)",
            "    elif data.dtype == np.int8:",
            "        warnings.warn(warning.format(data.dtype))",
            "        data = data * 256",
            "        data = data.astype(np.int16)",
            "    else:",
            "        raise ValueError(",
            "            \"Audio data cannot be converted automatically from \"",
            "            f\"{data.dtype} to 16-bit int format.\"",
            "        )",
            "    return data",
            "",
            "",
            "##################",
            "# OUTPUT",
            "##################",
            "",
            "",
            "def _convert(image, dtype, force_copy=False, uniform=False):",
            "    \"\"\"",
            "    Adapted from: https://github.com/scikit-image/scikit-image/blob/main/skimage/util/dtype.py#L510-L531",
            "",
            "    Convert an image to the requested data-type.",
            "    Warnings are issued in case of precision loss, or when negative values",
            "    are clipped during conversion to unsigned integer types (sign loss).",
            "    Floating point values are expected to be normalized and will be clipped",
            "    to the range [0.0, 1.0] or [-1.0, 1.0] when converting to unsigned or",
            "    signed integers respectively.",
            "    Numbers are not shifted to the negative side when converting from",
            "    unsigned to signed integer types. Negative values will be clipped when",
            "    converting to unsigned integers.",
            "    Parameters",
            "    ----------",
            "    image : ndarray",
            "        Input image.",
            "    dtype : dtype",
            "        Target data-type.",
            "    force_copy : bool, optional",
            "        Force a copy of the data, irrespective of its current dtype.",
            "    uniform : bool, optional",
            "        Uniformly quantize the floating point range to the integer range.",
            "        By default (uniform=False) floating point values are scaled and",
            "        rounded to the nearest integers, which minimizes back and forth",
            "        conversion errors.",
            "    .. versionchanged :: 0.15",
            "        ``_convert`` no longer warns about possible precision or sign",
            "        information loss. See discussions on these warnings at:",
            "        https://github.com/scikit-image/scikit-image/issues/2602",
            "        https://github.com/scikit-image/scikit-image/issues/543#issuecomment-208202228",
            "        https://github.com/scikit-image/scikit-image/pull/3575",
            "    References",
            "    ----------",
            "    .. [1] DirectX data conversion rules.",
            "           https://msdn.microsoft.com/en-us/library/windows/desktop/dd607323%28v=vs.85%29.aspx",
            "    .. [2] Data Conversions. In \"OpenGL ES 2.0 Specification v2.0.25\",",
            "           pp 7-8. Khronos Group, 2010.",
            "    .. [3] Proper treatment of pixels as integers. A.W. Paeth.",
            "           In \"Graphics Gems I\", pp 249-256. Morgan Kaufmann, 1990.",
            "    .. [4] Dirty Pixels. J. Blinn. In \"Jim Blinn's corner: Dirty Pixels\",",
            "           pp 47-57. Morgan Kaufmann, 1998.",
            "    \"\"\"",
            "    dtype_range = {",
            "        bool: (False, True),",
            "        np.bool_: (False, True),",
            "        np.bool8: (False, True),  # type: ignore",
            "        float: (-1, 1),",
            "        np.float_: (-1, 1),",
            "        np.float16: (-1, 1),",
            "        np.float32: (-1, 1),",
            "        np.float64: (-1, 1),",
            "    }",
            "",
            "    def _dtype_itemsize(itemsize, *dtypes):",
            "        \"\"\"Return first of `dtypes` with itemsize greater than `itemsize`",
            "        Parameters",
            "        ----------",
            "        itemsize: int",
            "            The data type object element size.",
            "        Other Parameters",
            "        ----------------",
            "        *dtypes:",
            "            Any Object accepted by `np.dtype` to be converted to a data",
            "            type object",
            "        Returns",
            "        -------",
            "        dtype: data type object",
            "            First of `dtypes` with itemsize greater than `itemsize`.",
            "        \"\"\"",
            "        return next(dt for dt in dtypes if np.dtype(dt).itemsize >= itemsize)",
            "",
            "    def _dtype_bits(kind, bits, itemsize=1):",
            "        \"\"\"Return dtype of `kind` that can store a `bits` wide unsigned int",
            "        Parameters:",
            "        kind: str",
            "            Data type kind.",
            "        bits: int",
            "            Desired number of bits.",
            "        itemsize: int",
            "            The data type object element size.",
            "        Returns",
            "        -------",
            "        dtype: data type object",
            "            Data type of `kind` that can store a `bits` wide unsigned int",
            "        \"\"\"",
            "",
            "        s = next(",
            "            i",
            "            for i in (itemsize,) + (2, 4, 8)",
            "            if bits < (i * 8) or (bits == (i * 8) and kind == \"u\")",
            "        )",
            "",
            "        return np.dtype(kind + str(s))",
            "",
            "    def _scale(a, n, m, copy=True):",
            "        \"\"\"Scale an array of unsigned/positive integers from `n` to `m` bits.",
            "        Numbers can be represented exactly only if `m` is a multiple of `n`.",
            "        Parameters",
            "        ----------",
            "        a : ndarray",
            "            Input image array.",
            "        n : int",
            "            Number of bits currently used to encode the values in `a`.",
            "        m : int",
            "            Desired number of bits to encode the values in `out`.",
            "        copy : bool, optional",
            "            If True, allocates and returns new array. Otherwise, modifies",
            "            `a` in place.",
            "        Returns",
            "        -------",
            "        out : array",
            "            Output image array. Has the same kind as `a`.",
            "        \"\"\"",
            "        kind = a.dtype.kind",
            "        if n > m and a.max() < 2**m:",
            "            return a.astype(_dtype_bits(kind, m))",
            "        elif n == m:",
            "            return a.copy() if copy else a",
            "        elif n > m:",
            "            # downscale with precision loss",
            "            if copy:",
            "                b = np.empty(a.shape, _dtype_bits(kind, m))",
            "                np.floor_divide(a, 2 ** (n - m), out=b, dtype=a.dtype, casting=\"unsafe\")",
            "                return b",
            "            else:",
            "                a //= 2 ** (n - m)",
            "                return a",
            "        elif m % n == 0:",
            "            # exact upscale to a multiple of `n` bits",
            "            if copy:",
            "                b = np.empty(a.shape, _dtype_bits(kind, m))",
            "                np.multiply(a, (2**m - 1) // (2**n - 1), out=b, dtype=b.dtype)",
            "                return b",
            "            else:",
            "                a = a.astype(_dtype_bits(kind, m, a.dtype.itemsize), copy=False)",
            "                a *= (2**m - 1) // (2**n - 1)",
            "                return a",
            "        else:",
            "            # upscale to a multiple of `n` bits,",
            "            # then downscale with precision loss",
            "            o = (m // n + 1) * n",
            "            if copy:",
            "                b = np.empty(a.shape, _dtype_bits(kind, o))",
            "                np.multiply(a, (2**o - 1) // (2**n - 1), out=b, dtype=b.dtype)",
            "                b //= 2 ** (o - m)",
            "                return b",
            "            else:",
            "                a = a.astype(_dtype_bits(kind, o, a.dtype.itemsize), copy=False)",
            "                a *= (2**o - 1) // (2**n - 1)",
            "                a //= 2 ** (o - m)",
            "                return a",
            "",
            "    image = np.asarray(image)",
            "    dtypeobj_in = image.dtype",
            "    dtypeobj_out = np.dtype(\"float64\") if dtype is np.floating else np.dtype(dtype)",
            "    dtype_in = dtypeobj_in.type",
            "    dtype_out = dtypeobj_out.type",
            "    kind_in = dtypeobj_in.kind",
            "    kind_out = dtypeobj_out.kind",
            "    itemsize_in = dtypeobj_in.itemsize",
            "    itemsize_out = dtypeobj_out.itemsize",
            "",
            "    # Below, we do an `issubdtype` check.  Its purpose is to find out",
            "    # whether we can get away without doing any image conversion.  This happens",
            "    # when:",
            "    #",
            "    # - the output and input dtypes are the same or",
            "    # - when the output is specified as a type, and the input dtype",
            "    #   is a subclass of that type (e.g. `np.floating` will allow",
            "    #   `float32` and `float64` arrays through)",
            "",
            "    if np.issubdtype(dtype_in, np.obj2sctype(dtype)):",
            "        if force_copy:",
            "            image = image.copy()",
            "        return image",
            "",
            "    if kind_in in \"ui\":",
            "        imin_in = np.iinfo(dtype_in).min",
            "        imax_in = np.iinfo(dtype_in).max",
            "    if kind_out in \"ui\":",
            "        imin_out = np.iinfo(dtype_out).min  # type: ignore",
            "        imax_out = np.iinfo(dtype_out).max  # type: ignore",
            "",
            "    # any -> binary",
            "    if kind_out == \"b\":",
            "        return image > dtype_in(dtype_range[dtype_in][1] / 2)",
            "",
            "    # binary -> any",
            "    if kind_in == \"b\":",
            "        result = image.astype(dtype_out)",
            "        if kind_out != \"f\":",
            "            result *= dtype_out(dtype_range[dtype_out][1])",
            "        return result",
            "",
            "    # float -> any",
            "    if kind_in == \"f\":",
            "        if kind_out == \"f\":",
            "            # float -> float",
            "            return image.astype(dtype_out)",
            "",
            "        if np.min(image) < -1.0 or np.max(image) > 1.0:",
            "            raise ValueError(\"Images of type float must be between -1 and 1.\")",
            "        # floating point -> integer",
            "        # use float type that can represent output integer type",
            "        computation_type = _dtype_itemsize(",
            "            itemsize_out, dtype_in, np.float32, np.float64",
            "        )",
            "",
            "        if not uniform:",
            "            if kind_out == \"u\":",
            "                image_out = np.multiply(image, imax_out, dtype=computation_type)  # type: ignore",
            "            else:",
            "                image_out = np.multiply(",
            "                    image,",
            "                    (imax_out - imin_out) / 2,  # type: ignore",
            "                    dtype=computation_type,",
            "                )",
            "                image_out -= 1.0 / 2.0",
            "            np.rint(image_out, out=image_out)",
            "            np.clip(image_out, imin_out, imax_out, out=image_out)  # type: ignore",
            "        elif kind_out == \"u\":",
            "            image_out = np.multiply(image, imax_out + 1, dtype=computation_type)  # type: ignore",
            "            np.clip(image_out, 0, imax_out, out=image_out)  # type: ignore",
            "        else:",
            "            image_out = np.multiply(",
            "                image,",
            "                (imax_out - imin_out + 1.0) / 2.0,  # type: ignore",
            "                dtype=computation_type,",
            "            )",
            "            np.floor(image_out, out=image_out)",
            "            np.clip(image_out, imin_out, imax_out, out=image_out)  # type: ignore",
            "        return image_out.astype(dtype_out)",
            "",
            "    # signed/unsigned int -> float",
            "    if kind_out == \"f\":",
            "        # use float type that can exactly represent input integers",
            "        computation_type = _dtype_itemsize(",
            "            itemsize_in, dtype_out, np.float32, np.float64",
            "        )",
            "",
            "        if kind_in == \"u\":",
            "            # using np.divide or np.multiply doesn't copy the data",
            "            # until the computation time",
            "            image = np.multiply(image, 1.0 / imax_in, dtype=computation_type)  # type: ignore",
            "            # DirectX uses this conversion also for signed ints",
            "            # if imin_in:",
            "            #     np.maximum(image, -1.0, out=image)",
            "        else:",
            "            image = np.add(image, 0.5, dtype=computation_type)",
            "            image *= 2 / (imax_in - imin_in)  # type: ignore",
            "",
            "        return np.asarray(image, dtype_out)",
            "",
            "    # unsigned int -> signed/unsigned int",
            "    if kind_in == \"u\":",
            "        if kind_out == \"i\":",
            "            # unsigned int -> signed int",
            "            image = _scale(image, 8 * itemsize_in, 8 * itemsize_out - 1)",
            "            return image.view(dtype_out)",
            "        else:",
            "            # unsigned int -> unsigned int",
            "            return _scale(image, 8 * itemsize_in, 8 * itemsize_out)",
            "",
            "    # signed int -> unsigned int",
            "    if kind_out == \"u\":",
            "        image = _scale(image, 8 * itemsize_in - 1, 8 * itemsize_out)",
            "        result = np.empty(image.shape, dtype_out)",
            "        np.maximum(image, 0, out=result, dtype=image.dtype, casting=\"unsafe\")",
            "        return result",
            "",
            "    # signed int -> signed int",
            "    if itemsize_in > itemsize_out:",
            "        return _scale(image, 8 * itemsize_in - 1, 8 * itemsize_out - 1)",
            "",
            "    image = image.astype(_dtype_bits(\"i\", itemsize_out * 8))",
            "    image -= imin_in  # type: ignore",
            "    image = _scale(image, 8 * itemsize_in, 8 * itemsize_out, copy=False)",
            "    image += imin_out  # type: ignore",
            "    return image.astype(dtype_out)",
            "",
            "",
            "def ffmpeg_installed() -> bool:",
            "    if wasm_utils.IS_WASM:",
            "        # TODO: Support ffmpeg in WASM",
            "        return False",
            "",
            "    return shutil.which(\"ffmpeg\") is not None",
            "",
            "",
            "def video_is_playable(video_filepath: str) -> bool:",
            "    \"\"\"Determines if a video is playable in the browser.",
            "",
            "    A video is playable if it has a playable container and codec.",
            "        .mp4 -> h264",
            "        .webm -> vp9",
            "        .ogg -> theora",
            "    \"\"\"",
            "    from ffmpy import FFprobe, FFRuntimeError",
            "",
            "    try:",
            "        container = Path(video_filepath).suffix.lower()",
            "        probe = FFprobe(",
            "            global_options=\"-show_format -show_streams -select_streams v -print_format json\",",
            "            inputs={video_filepath: None},",
            "        )",
            "        output = probe.run(stderr=subprocess.PIPE, stdout=subprocess.PIPE)",
            "        output = json.loads(output[0])",
            "        video_codec = output[\"streams\"][0][\"codec_name\"]",
            "        return (container, video_codec) in [",
            "            (\".mp4\", \"h264\"),",
            "            (\".ogg\", \"theora\"),",
            "            (\".webm\", \"vp9\"),",
            "        ]",
            "    # If anything goes wrong, assume the video can be played to not convert downstream",
            "    except (FFRuntimeError, IndexError, KeyError):",
            "        return True",
            "",
            "",
            "def convert_video_to_playable_mp4(video_path: str) -> str:",
            "    \"\"\"Convert the video to mp4. If something goes wrong return the original video.\"\"\"",
            "    from ffmpy import FFmpeg, FFRuntimeError",
            "",
            "    try:",
            "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:",
            "            output_path = Path(video_path).with_suffix(\".mp4\")",
            "            shutil.copy2(video_path, tmp_file.name)",
            "            # ffmpeg will automatically use h264 codec (playable in browser) when converting to mp4",
            "            ff = FFmpeg(",
            "                inputs={str(tmp_file.name): None},",
            "                outputs={str(output_path): None},",
            "                global_options=\"-y -loglevel quiet\",",
            "            )",
            "            ff.run()",
            "    except FFRuntimeError as e:",
            "        print(f\"Error converting video to browser-playable format {str(e)}\")",
            "        output_path = video_path",
            "    finally:",
            "        # Remove temp file",
            "        os.remove(tmp_file.name)  # type: ignore",
            "    return str(output_path)",
            "",
            "",
            "def get_video_length(video_path: str | Path):",
            "    if wasm_utils.IS_WASM:",
            "        raise wasm_utils.WasmUnsupportedError(",
            "            \"Video duration is not supported in the Wasm mode.\"",
            "        )",
            "    duration = subprocess.check_output(",
            "        [",
            "            \"ffprobe\",",
            "            \"-i\",",
            "            str(video_path),",
            "            \"-show_entries\",",
            "            \"format=duration\",",
            "            \"-v\",",
            "            \"quiet\",",
            "            \"-of\",",
            "            \"csv={}\".format(\"p=0\"),",
            "        ]",
            "    )",
            "    duration_str = duration.decode(\"utf-8\").strip()",
            "    duration_float = float(duration_str)",
            "",
            "    return duration_float"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "23": []
        },
        "addLocation": []
    },
    "gradio/routes.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " import os"
            },
            "1": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " import posixpath"
            },
            "2": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " import secrets"
            },
            "3": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import tempfile"
            },
            "4": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " import threading"
            },
            "5": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " import time"
            },
            "6": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " import traceback"
            },
            "7": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 66,
                "PatchRowcode": "     move_uploaded_files_to_cache,"
            },
            "8": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 67,
                "PatchRowcode": " )"
            },
            "9": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 68,
                "PatchRowcode": " from gradio.state_holder import StateHolder"
            },
            "10": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from gradio.utils import ("
            },
            "11": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    get_package_version,"
            },
            "12": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 69,
                "PatchRowcode": "+from gradio.utils import get_package_version, get_upload_folder"
            },
            "14": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 70,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 71,
                "PatchRowcode": " if TYPE_CHECKING:"
            },
            "16": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "     from gradio.blocks import Block"
            },
            "17": {
                "beforePatchRowNumber": 136,
                "afterPatchRowNumber": 133,
                "PatchRowcode": "         self.cookie_id = secrets.token_urlsafe(32)"
            },
            "18": {
                "beforePatchRowNumber": 137,
                "afterPatchRowNumber": 134,
                "PatchRowcode": "         self.queue_token = secrets.token_urlsafe(32)"
            },
            "19": {
                "beforePatchRowNumber": 138,
                "afterPatchRowNumber": 135,
                "PatchRowcode": "         self.startup_events_triggered = False"
            },
            "20": {
                "beforePatchRowNumber": 139,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.uploaded_file_dir = os.environ.get(\"GRADIO_TEMP_DIR\") or str("
            },
            "21": {
                "beforePatchRowNumber": 140,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            (Path(tempfile.gettempdir()) / \"gradio\").resolve()"
            },
            "22": {
                "beforePatchRowNumber": 141,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        )"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 136,
                "PatchRowcode": "+        self.uploaded_file_dir = get_upload_folder()"
            },
            "24": {
                "beforePatchRowNumber": 142,
                "afterPatchRowNumber": 137,
                "PatchRowcode": "         self.change_event: None | threading.Event = None"
            },
            "25": {
                "beforePatchRowNumber": 143,
                "afterPatchRowNumber": 138,
                "PatchRowcode": "         self._asyncio_tasks: list[asyncio.Task] = []"
            },
            "26": {
                "beforePatchRowNumber": 144,
                "afterPatchRowNumber": 139,
                "PatchRowcode": "         # Allow user to manually set `docs_url` and `redoc_url`"
            }
        },
        "frontPatchFile": [
            "\"\"\"Implements a FastAPI server to run the gradio interface. Note that some types in this",
            "module use the Optional/Union notation so that they work correctly with pydantic.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import asyncio",
            "import contextlib",
            "import sys",
            "",
            "if sys.version_info >= (3, 9):",
            "    from importlib.resources import files",
            "else:",
            "    from importlib_resources import files",
            "import inspect",
            "import json",
            "import mimetypes",
            "import os",
            "import posixpath",
            "import secrets",
            "import tempfile",
            "import threading",
            "import time",
            "import traceback",
            "from pathlib import Path",
            "from queue import Empty as EmptyQueue",
            "from typing import TYPE_CHECKING, Any, AsyncIterator, Dict, List, Optional, Type",
            "",
            "import fastapi",
            "import httpx",
            "import markupsafe",
            "import orjson",
            "from fastapi import BackgroundTasks, Depends, FastAPI, HTTPException, status",
            "from fastapi.middleware.cors import CORSMiddleware",
            "from fastapi.responses import (",
            "    FileResponse,",
            "    HTMLResponse,",
            "    JSONResponse,",
            "    PlainTextResponse,",
            ")",
            "from fastapi.security import OAuth2PasswordRequestForm",
            "from fastapi.templating import Jinja2Templates",
            "from gradio_client import utils as client_utils",
            "from gradio_client.documentation import document",
            "from gradio_client.utils import ServerMessage",
            "from jinja2.exceptions import TemplateNotFound",
            "from multipart.multipart import parse_options_header",
            "from starlette.background import BackgroundTask",
            "from starlette.responses import RedirectResponse, StreamingResponse",
            "",
            "import gradio",
            "from gradio import ranged_response, route_utils, utils, wasm_utils",
            "from gradio.context import Context",
            "from gradio.data_classes import ComponentServerBody, PredictBody, ResetBody",
            "from gradio.exceptions import Error",
            "from gradio.oauth import attach_oauth",
            "from gradio.processing_utils import add_root_url",
            "from gradio.queueing import Estimation",
            "from gradio.route_utils import (  # noqa: F401",
            "    FileUploadProgress,",
            "    FileUploadProgressNotQueuedError,",
            "    FileUploadProgressNotTrackedError,",
            "    GradioMultiPartParser,",
            "    GradioUploadFile,",
            "    MultiPartException,",
            "    Request,",
            "    compare_passwords_securely,",
            "    move_uploaded_files_to_cache,",
            ")",
            "from gradio.state_holder import StateHolder",
            "from gradio.utils import (",
            "    get_package_version,",
            ")",
            "",
            "if TYPE_CHECKING:",
            "    from gradio.blocks import Block",
            "",
            "",
            "mimetypes.init()",
            "",
            "STATIC_TEMPLATE_LIB = files(\"gradio\").joinpath(\"templates\").as_posix()  # type: ignore",
            "STATIC_PATH_LIB = files(\"gradio\").joinpath(\"templates\", \"frontend\", \"static\").as_posix()  # type: ignore",
            "BUILD_PATH_LIB = files(\"gradio\").joinpath(\"templates\", \"frontend\", \"assets\").as_posix()  # type: ignore",
            "VERSION = get_package_version()",
            "",
            "",
            "class ORJSONResponse(JSONResponse):",
            "    media_type = \"application/json\"",
            "",
            "    @staticmethod",
            "    def _render(content: Any) -> bytes:",
            "        return orjson.dumps(",
            "            content,",
            "            option=orjson.OPT_SERIALIZE_NUMPY | orjson.OPT_PASSTHROUGH_DATETIME,",
            "            default=str,",
            "        )",
            "",
            "    def render(self, content: Any) -> bytes:",
            "        return ORJSONResponse._render(content)",
            "",
            "    @staticmethod",
            "    def _render_str(content: Any) -> str:",
            "        return ORJSONResponse._render(content).decode(\"utf-8\")",
            "",
            "",
            "def toorjson(value):",
            "    return markupsafe.Markup(",
            "        ORJSONResponse._render_str(value)",
            "        .replace(\"<\", \"\\\\u003c\")",
            "        .replace(\">\", \"\\\\u003e\")",
            "        .replace(\"&\", \"\\\\u0026\")",
            "        .replace(\"'\", \"\\\\u0027\")",
            "    )",
            "",
            "",
            "templates = Jinja2Templates(directory=STATIC_TEMPLATE_LIB)",
            "templates.env.filters[\"toorjson\"] = toorjson",
            "",
            "client = httpx.AsyncClient()",
            "",
            "file_upload_statuses = FileUploadProgress()",
            "",
            "",
            "class App(FastAPI):",
            "    \"\"\"",
            "    FastAPI App Wrapper",
            "    \"\"\"",
            "",
            "    def __init__(self, **kwargs):",
            "        self.tokens = {}",
            "        self.auth = None",
            "        self.blocks: gradio.Blocks | None = None",
            "        self.state_holder = StateHolder()",
            "        self.iterators: dict[str, AsyncIterator] = {}",
            "        self.iterators_to_reset: set[str] = set()",
            "        self.lock = utils.safe_get_lock()",
            "        self.cookie_id = secrets.token_urlsafe(32)",
            "        self.queue_token = secrets.token_urlsafe(32)",
            "        self.startup_events_triggered = False",
            "        self.uploaded_file_dir = os.environ.get(\"GRADIO_TEMP_DIR\") or str(",
            "            (Path(tempfile.gettempdir()) / \"gradio\").resolve()",
            "        )",
            "        self.change_event: None | threading.Event = None",
            "        self._asyncio_tasks: list[asyncio.Task] = []",
            "        # Allow user to manually set `docs_url` and `redoc_url`",
            "        # when instantiating an App; when they're not set, disable docs and redoc.",
            "        kwargs.setdefault(\"docs_url\", None)",
            "        kwargs.setdefault(\"redoc_url\", None)",
            "        super().__init__(**kwargs)",
            "",
            "    def configure_app(self, blocks: gradio.Blocks) -> None:",
            "        auth = blocks.auth",
            "        if auth is not None:",
            "            if not callable(auth):",
            "                self.auth = {account[0]: account[1] for account in auth}",
            "            else:",
            "                self.auth = auth",
            "        else:",
            "            self.auth = None",
            "",
            "        self.blocks = blocks",
            "        self.cwd = os.getcwd()",
            "        self.favicon_path = blocks.favicon_path",
            "        self.tokens = {}",
            "        self.root_path = blocks.root_path",
            "        self.state_holder.set_blocks(blocks)",
            "",
            "    def get_blocks(self) -> gradio.Blocks:",
            "        if self.blocks is None:",
            "            raise ValueError(\"No Blocks has been configured for this app.\")",
            "        return self.blocks",
            "",
            "    def build_proxy_request(self, url_path):",
            "        url = httpx.URL(url_path)",
            "        assert self.blocks  # noqa: S101",
            "        # Don't proxy a URL unless it's a URL specifically loaded by the user using",
            "        # gr.load() to prevent SSRF or harvesting of HF tokens by malicious Spaces.",
            "        is_safe_url = any(",
            "            url.host == httpx.URL(root).host for root in self.blocks.proxy_urls",
            "        )",
            "        if not is_safe_url:",
            "            raise PermissionError(\"This URL cannot be proxied.\")",
            "        is_hf_url = url.host.endswith(\".hf.space\")",
            "        headers = {}",
            "        if Context.hf_token is not None and is_hf_url:",
            "            headers[\"Authorization\"] = f\"Bearer {Context.hf_token}\"",
            "        rp_req = client.build_request(\"GET\", url, headers=headers)",
            "        return rp_req",
            "",
            "    def _cancel_asyncio_tasks(self):",
            "        for task in self._asyncio_tasks:",
            "            task.cancel()",
            "        self._asyncio_tasks = []",
            "",
            "    @staticmethod",
            "    def create_app(",
            "        blocks: gradio.Blocks, app_kwargs: Dict[str, Any] | None = None",
            "    ) -> App:",
            "        app_kwargs = app_kwargs or {}",
            "        app_kwargs.setdefault(\"default_response_class\", ORJSONResponse)",
            "        app = App(**app_kwargs)",
            "        app.configure_app(blocks)",
            "",
            "        if not wasm_utils.IS_WASM:",
            "            app.add_middleware(",
            "                CORSMiddleware,",
            "                allow_origins=[\"*\"],",
            "                allow_methods=[\"*\"],",
            "                allow_headers=[\"*\"],",
            "            )",
            "",
            "        @app.get(\"/user\")",
            "        @app.get(\"/user/\")",
            "        def get_current_user(request: fastapi.Request) -> Optional[str]:",
            "            token = request.cookies.get(",
            "                f\"access-token-{app.cookie_id}\"",
            "            ) or request.cookies.get(f\"access-token-unsecure-{app.cookie_id}\")",
            "            return app.tokens.get(token)",
            "",
            "        @app.get(\"/login_check\")",
            "        @app.get(\"/login_check/\")",
            "        def login_check(user: str = Depends(get_current_user)):",
            "            if app.auth is None or user is not None:",
            "                return",
            "            raise HTTPException(",
            "                status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\"",
            "            )",
            "",
            "        @app.get(\"/token\")",
            "        @app.get(\"/token/\")",
            "        def get_token(request: fastapi.Request) -> dict:",
            "            token = request.cookies.get(f\"access-token-{app.cookie_id}\")",
            "            return {\"token\": token, \"user\": app.tokens.get(token)}",
            "",
            "        @app.get(\"/app_id\")",
            "        @app.get(\"/app_id/\")",
            "        def app_id(request: fastapi.Request) -> dict:  # noqa: ARG001",
            "            return {\"app_id\": app.get_blocks().app_id}",
            "",
            "        @app.get(\"/dev/reload\", dependencies=[Depends(login_check)])",
            "        async def notify_changes(",
            "            request: fastapi.Request,",
            "        ):",
            "            async def reload_checker(request: fastapi.Request):",
            "                heartbeat_rate = 15",
            "                check_rate = 0.05",
            "                last_heartbeat = time.perf_counter()",
            "",
            "                while True:",
            "                    if await request.is_disconnected():",
            "                        return",
            "",
            "                    if app.change_event and app.change_event.is_set():",
            "                        app.change_event.clear()",
            "                        yield \"\"\"data: CHANGE\\n\\n\"\"\"",
            "",
            "                    await asyncio.sleep(check_rate)",
            "                    if time.perf_counter() - last_heartbeat > heartbeat_rate:",
            "                        yield \"\"\"data: HEARTBEAT\\n\\n\"\"\"",
            "                        last_heartbeat = time.time()",
            "",
            "            return StreamingResponse(",
            "                reload_checker(request),",
            "                media_type=\"text/event-stream\",",
            "            )",
            "",
            "        @app.post(\"/login\")",
            "        @app.post(\"/login/\")",
            "        def login(form_data: OAuth2PasswordRequestForm = Depends()):",
            "            username, password = form_data.username.strip(), form_data.password",
            "            if app.auth is None:",
            "                return RedirectResponse(url=\"/\", status_code=status.HTTP_302_FOUND)",
            "            if (",
            "                not callable(app.auth)",
            "                and username in app.auth",
            "                and compare_passwords_securely(password, app.auth[username])  # type: ignore",
            "            ) or (callable(app.auth) and app.auth.__call__(username, password)):",
            "                token = secrets.token_urlsafe(16)",
            "                app.tokens[token] = username",
            "                response = JSONResponse(content={\"success\": True})",
            "                response.set_cookie(",
            "                    key=f\"access-token-{app.cookie_id}\",",
            "                    value=token,",
            "                    httponly=True,",
            "                    samesite=\"none\",",
            "                    secure=True,",
            "                )",
            "                response.set_cookie(",
            "                    key=f\"access-token-unsecure-{app.cookie_id}\",",
            "                    value=token,",
            "                    httponly=True,",
            "                )",
            "                return response",
            "            else:",
            "                raise HTTPException(status_code=400, detail=\"Incorrect credentials.\")",
            "",
            "        ###############",
            "        # OAuth Routes",
            "        ###############",
            "",
            "        # Define OAuth routes if the app expects it (i.e. a LoginButton is defined).",
            "        # It allows users to \"Sign in with HuggingFace\".",
            "        if app.blocks is not None and app.blocks.expects_oauth:",
            "            attach_oauth(app)",
            "",
            "        ###############",
            "        # Main Routes",
            "        ###############",
            "",
            "        @app.head(\"/\", response_class=HTMLResponse)",
            "        @app.get(\"/\", response_class=HTMLResponse)",
            "        def main(request: fastapi.Request, user: str = Depends(get_current_user)):",
            "            mimetypes.add_type(\"application/javascript\", \".js\")",
            "            blocks = app.get_blocks()",
            "            root = route_utils.get_root_url(",
            "                request=request, route_path=\"/\", root_path=app.root_path",
            "            )",
            "            if app.auth is None or user is not None:",
            "                config = app.get_blocks().config",
            "                config = route_utils.update_root_in_config(config, root)",
            "            else:",
            "                config = {",
            "                    \"auth_required\": True,",
            "                    \"auth_message\": blocks.auth_message,",
            "                    \"space_id\": app.get_blocks().space_id,",
            "                    \"root\": root,",
            "                }",
            "",
            "            try:",
            "                template = (",
            "                    \"frontend/share.html\" if blocks.share else \"frontend/index.html\"",
            "                )",
            "                return templates.TemplateResponse(",
            "                    template,",
            "                    {\"request\": request, \"config\": config},",
            "                )",
            "            except TemplateNotFound as err:",
            "                if blocks.share:",
            "                    raise ValueError(",
            "                        \"Did you install Gradio from source files? Share mode only \"",
            "                        \"works when Gradio is installed through the pip package.\"",
            "                    ) from err",
            "                else:",
            "                    raise ValueError(",
            "                        \"Did you install Gradio from source files? You need to build \"",
            "                        \"the frontend by running /scripts/build_frontend.sh\"",
            "                    ) from err",
            "",
            "        @app.get(\"/info/\", dependencies=[Depends(login_check)])",
            "        @app.get(\"/info\", dependencies=[Depends(login_check)])",
            "        def api_info():",
            "            return app.get_blocks().get_api_info()  # type: ignore",
            "",
            "        @app.get(\"/config/\", dependencies=[Depends(login_check)])",
            "        @app.get(\"/config\", dependencies=[Depends(login_check)])",
            "        def get_config(request: fastapi.Request):",
            "            config = app.get_blocks().config",
            "            root = route_utils.get_root_url(",
            "                request=request, route_path=\"/config\", root_path=app.root_path",
            "            )",
            "            config = route_utils.update_root_in_config(config, root)",
            "            return ORJSONResponse(content=config)",
            "",
            "        @app.get(\"/static/{path:path}\")",
            "        def static_resource(path: str):",
            "            static_file = safe_join(STATIC_PATH_LIB, path)",
            "            return FileResponse(static_file)",
            "",
            "        @app.get(\"/custom_component/{id}/{type}/{file_name}\")",
            "        def custom_component_path(id: str, type: str, file_name: str):",
            "            config = app.get_blocks().config",
            "            components = config[\"components\"]",
            "            location = next(",
            "                (item for item in components if item[\"component_class_id\"] == id), None",
            "            )",
            "",
            "            if location is None:",
            "                raise HTTPException(status_code=404, detail=\"Component not found.\")",
            "",
            "            component_instance = app.get_blocks().get_component(location[\"id\"])",
            "",
            "            module_name = component_instance.__class__.__module__",
            "            module_path = sys.modules[module_name].__file__",
            "",
            "            if module_path is None or component_instance is None:",
            "                raise HTTPException(status_code=404, detail=\"Component not found.\")",
            "",
            "            return FileResponse(",
            "                safe_join(",
            "                    str(Path(module_path).parent),",
            "                    f\"{component_instance.__class__.TEMPLATE_DIR}/{type}/{file_name}\",",
            "                )",
            "            )",
            "",
            "        @app.get(\"/assets/{path:path}\")",
            "        def build_resource(path: str):",
            "            build_file = safe_join(BUILD_PATH_LIB, path)",
            "            return FileResponse(build_file)",
            "",
            "        @app.get(\"/favicon.ico\")",
            "        async def favicon():",
            "            blocks = app.get_blocks()",
            "            if blocks.favicon_path is None:",
            "                return static_resource(\"img/logo.svg\")",
            "            else:",
            "                return FileResponse(blocks.favicon_path)",
            "",
            "        @app.head(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])",
            "        @app.get(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])",
            "        async def reverse_proxy(url_path: str):",
            "            # Adapted from: https://github.com/tiangolo/fastapi/issues/1788",
            "            try:",
            "                rp_req = app.build_proxy_request(url_path)",
            "            except PermissionError as err:",
            "                raise HTTPException(status_code=400, detail=str(err)) from err",
            "            rp_resp = await client.send(rp_req, stream=True)",
            "            return StreamingResponse(",
            "                rp_resp.aiter_raw(),",
            "                status_code=rp_resp.status_code,",
            "                headers=rp_resp.headers,  # type: ignore",
            "                background=BackgroundTask(rp_resp.aclose),",
            "            )",
            "",
            "        @app.head(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])",
            "        @app.get(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])",
            "        async def file(path_or_url: str, request: fastapi.Request):",
            "            blocks = app.get_blocks()",
            "            if client_utils.is_http_url_like(path_or_url):",
            "                return RedirectResponse(",
            "                    url=path_or_url, status_code=status.HTTP_302_FOUND",
            "                )",
            "",
            "            if route_utils.starts_with_protocol(path_or_url):",
            "                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")",
            "",
            "            abs_path = utils.abspath(path_or_url)",
            "",
            "            in_blocklist = any(",
            "                utils.is_in_or_equal(abs_path, blocked_path)",
            "                for blocked_path in blocks.blocked_paths",
            "            )",
            "",
            "            is_dir = abs_path.is_dir()",
            "",
            "            if in_blocklist or is_dir:",
            "                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")",
            "",
            "            created_by_app = False",
            "            for temp_file_set in blocks.temp_file_sets:",
            "                if abs_path in temp_file_set:",
            "                    created_by_app = True",
            "                    break",
            "            in_allowlist = any(",
            "                utils.is_in_or_equal(abs_path, allowed_path)",
            "                for allowed_path in blocks.allowed_paths",
            "            )",
            "            was_uploaded = utils.is_in_or_equal(abs_path, app.uploaded_file_dir)",
            "            is_cached_example = utils.is_in_or_equal(",
            "                abs_path, utils.abspath(utils.get_cache_folder())",
            "            )",
            "",
            "            if not (",
            "                created_by_app or in_allowlist or was_uploaded or is_cached_example",
            "            ):",
            "                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")",
            "",
            "            if not abs_path.exists():",
            "                raise HTTPException(404, f\"File not found: {path_or_url}.\")",
            "",
            "            range_val = request.headers.get(\"Range\", \"\").strip()",
            "            if range_val.startswith(\"bytes=\") and \"-\" in range_val:",
            "                range_val = range_val[6:]",
            "                start, end = range_val.split(\"-\")",
            "                if start.isnumeric() and end.isnumeric():",
            "                    start = int(start)",
            "                    end = int(end)",
            "                    response = ranged_response.RangedFileResponse(",
            "                        abs_path,",
            "                        ranged_response.OpenRange(start, end),",
            "                        dict(request.headers),",
            "                        stat_result=os.stat(abs_path),",
            "                    )",
            "                    return response",
            "",
            "            return FileResponse(abs_path, headers={\"Accept-Ranges\": \"bytes\"})",
            "",
            "        @app.get(",
            "            \"/stream/{session_hash}/{run}/{component_id}\",",
            "            dependencies=[Depends(login_check)],",
            "        )",
            "        async def stream(",
            "            session_hash: str,",
            "            run: int,",
            "            component_id: int,",
            "            request: fastapi.Request,  # noqa: ARG001",
            "        ):",
            "            stream: list = (",
            "                app.get_blocks()",
            "                .pending_streams[session_hash]",
            "                .get(run, {})",
            "                .get(component_id, None)",
            "            )",
            "            if stream is None:",
            "                raise HTTPException(404, \"Stream not found.\")",
            "",
            "            def stream_wrapper():",
            "                check_stream_rate = 0.01",
            "                max_wait_time = 120  # maximum wait between yields - assume generator thread has crashed otherwise.",
            "                wait_time = 0",
            "                while True:",
            "                    if len(stream) == 0:",
            "                        if wait_time > max_wait_time:",
            "                            return",
            "                        wait_time += check_stream_rate",
            "                        time.sleep(check_stream_rate)",
            "                        continue",
            "                    wait_time = 0",
            "                    next_stream = stream.pop(0)",
            "                    if next_stream is None:",
            "                        return",
            "                    yield next_stream",
            "",
            "            return StreamingResponse(stream_wrapper())",
            "",
            "        @app.get(\"/file/{path:path}\", dependencies=[Depends(login_check)])",
            "        async def file_deprecated(path: str, request: fastapi.Request):",
            "            return await file(path, request)",
            "",
            "        @app.post(\"/reset/\")",
            "        @app.post(\"/reset\")",
            "        async def reset_iterator(body: ResetBody):",
            "            if body.event_id not in app.iterators:",
            "                return {\"success\": False}",
            "            async with app.lock:",
            "                del app.iterators[body.event_id]",
            "                app.iterators_to_reset.add(body.event_id)",
            "                await app.get_blocks()._queue.clean_events(event_id=body.event_id)",
            "            return {\"success\": True}",
            "",
            "        # had to use '/run' endpoint for Colab compatibility, '/api' supported for backwards compatibility",
            "        @app.post(\"/run/{api_name}\", dependencies=[Depends(login_check)])",
            "        @app.post(\"/run/{api_name}/\", dependencies=[Depends(login_check)])",
            "        @app.post(\"/api/{api_name}\", dependencies=[Depends(login_check)])",
            "        @app.post(\"/api/{api_name}/\", dependencies=[Depends(login_check)])",
            "        async def predict(",
            "            api_name: str,",
            "            body: PredictBody,",
            "            request: fastapi.Request,",
            "            username: str = Depends(get_current_user),",
            "        ):",
            "            fn_index_inferred = route_utils.infer_fn_index(",
            "                app=app, api_name=api_name, body=body",
            "            )",
            "",
            "            if not app.get_blocks().api_open and app.get_blocks().queue_enabled_for_fn(",
            "                fn_index_inferred",
            "            ):",
            "                raise HTTPException(",
            "                    detail=\"This API endpoint does not accept direct HTTP POST requests. Please join the queue to use this API.\",",
            "                    status_code=status.HTTP_404_NOT_FOUND,",
            "                )",
            "",
            "            gr_request = route_utils.compile_gr_request(",
            "                app,",
            "                body,",
            "                fn_index_inferred=fn_index_inferred,",
            "                username=username,",
            "                request=request,",
            "            )",
            "",
            "            try:",
            "                output = await route_utils.call_process_api(",
            "                    app=app,",
            "                    body=body,",
            "                    gr_request=gr_request,",
            "                    fn_index_inferred=fn_index_inferred,",
            "                )",
            "            except BaseException as error:",
            "                show_error = app.get_blocks().show_error or isinstance(error, Error)",
            "                traceback.print_exc()",
            "                return JSONResponse(",
            "                    content={\"error\": str(error) if show_error else None},",
            "                    status_code=500,",
            "                )",
            "            root_path = route_utils.get_root_url(",
            "                request=request, route_path=f\"/api/{api_name}\", root_path=app.root_path",
            "            )",
            "            output = add_root_url(output, root_path, None)",
            "            return output",
            "",
            "        @app.get(\"/queue/data\", dependencies=[Depends(login_check)])",
            "        async def queue_data(",
            "            request: fastapi.Request,",
            "            session_hash: str,",
            "        ):",
            "            blocks = app.get_blocks()",
            "            root_path = route_utils.get_root_url(",
            "                request=request, route_path=\"/queue/data\", root_path=app.root_path",
            "            )",
            "",
            "            async def sse_stream(request: fastapi.Request):",
            "                try:",
            "                    last_heartbeat = time.perf_counter()",
            "                    while True:",
            "                        if await request.is_disconnected():",
            "                            await blocks._queue.clean_events(session_hash=session_hash)",
            "                            return",
            "",
            "                        if (",
            "                            session_hash",
            "                            not in blocks._queue.pending_messages_per_session",
            "                        ):",
            "                            raise HTTPException(",
            "                                status_code=status.HTTP_404_NOT_FOUND,",
            "                                detail=\"Session not found.\",",
            "                            )",
            "",
            "                        heartbeat_rate = 15",
            "                        check_rate = 0.05",
            "                        message = None",
            "                        try:",
            "                            messages = blocks._queue.pending_messages_per_session[",
            "                                session_hash",
            "                            ]",
            "                            message = messages.get_nowait()",
            "                        except EmptyQueue:",
            "                            await asyncio.sleep(check_rate)",
            "                            if time.perf_counter() - last_heartbeat > heartbeat_rate:",
            "                                # Fix this",
            "                                message = {",
            "                                    \"msg\": ServerMessage.heartbeat,",
            "                                }",
            "                                # Need to reset last_heartbeat with perf_counter",
            "                                # otherwise only a single hearbeat msg will be sent",
            "                                # and then the stream will retry leading to infinite queue \ud83d\ude2c",
            "                                last_heartbeat = time.perf_counter()",
            "",
            "                        if blocks._queue.stopped:",
            "                            message = {",
            "                                \"msg\": \"unexpected_error\",",
            "                                \"message\": \"Server stopped unexpectedly.\",",
            "                                \"success\": False,",
            "                            }",
            "                        if message:",
            "                            add_root_url(message, root_path, None)",
            "                            yield f\"data: {json.dumps(message)}\\n\\n\"",
            "                            if message[\"msg\"] == ServerMessage.process_completed:",
            "                                blocks._queue.pending_event_ids_session[",
            "                                    session_hash",
            "                                ].remove(message[\"event_id\"])",
            "                                if message[\"msg\"] == ServerMessage.server_stopped or (",
            "                                    message[\"msg\"] == ServerMessage.process_completed",
            "                                    and (",
            "                                        len(",
            "                                            blocks._queue.pending_event_ids_session[",
            "                                                session_hash",
            "                                            ]",
            "                                        )",
            "                                        == 0",
            "                                    )",
            "                                ):",
            "                                    return",
            "                except BaseException as e:",
            "                    message = {",
            "                        \"msg\": \"unexpected_error\",",
            "                        \"success\": False,",
            "                        \"message\": str(e),",
            "                    }",
            "                    yield f\"data: {json.dumps(message)}\\n\\n\"",
            "                    if isinstance(e, asyncio.CancelledError):",
            "                        del blocks._queue.pending_messages_per_session[session_hash]",
            "                        await blocks._queue.clean_events(session_hash=session_hash)",
            "                    raise e",
            "",
            "            return StreamingResponse(",
            "                sse_stream(request),",
            "                media_type=\"text/event-stream\",",
            "            )",
            "",
            "        @app.post(\"/queue/join\", dependencies=[Depends(login_check)])",
            "        async def queue_join(",
            "            body: PredictBody,",
            "            request: fastapi.Request,",
            "            username: str = Depends(get_current_user),",
            "        ):",
            "            blocks = app.get_blocks()",
            "",
            "            if blocks._queue.server_app is None:",
            "                blocks._queue.set_server_app(app)",
            "",
            "            if blocks._queue.stopped:",
            "                raise HTTPException(",
            "                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,",
            "                    detail=\"Queue is stopped.\",",
            "                )",
            "",
            "            success, event_id = await blocks._queue.push(body, request, username)",
            "            if not success:",
            "                status_code = (",
            "                    status.HTTP_503_SERVICE_UNAVAILABLE",
            "                    if \"Queue is full.\" in event_id",
            "                    else status.HTTP_400_BAD_REQUEST",
            "                )",
            "                raise HTTPException(status_code=status_code, detail=event_id)",
            "            return {\"event_id\": event_id}",
            "",
            "        @app.post(\"/component_server\", dependencies=[Depends(login_check)])",
            "        @app.post(\"/component_server/\", dependencies=[Depends(login_check)])",
            "        def component_server(body: ComponentServerBody):",
            "            state = app.state_holder[body.session_hash]",
            "            component_id = body.component_id",
            "            block: Block",
            "            if component_id in state:",
            "                block = state[component_id]",
            "            else:",
            "                block = app.get_blocks().blocks[component_id]",
            "            fn = getattr(block, body.fn_name, None)",
            "            if fn is None or not getattr(fn, \"_is_server_fn\", False):",
            "                raise HTTPException(",
            "                    status_code=status.HTTP_404_NOT_FOUND,",
            "                    detail=\"Function not found.\",",
            "                )",
            "            return fn(body.data)",
            "",
            "        @app.get(",
            "            \"/queue/status\",",
            "            dependencies=[Depends(login_check)],",
            "            response_model=Estimation,",
            "        )",
            "        async def get_queue_status():",
            "            return app.get_blocks()._queue.get_status()",
            "",
            "        @app.get(\"/upload_progress\")",
            "        def get_upload_progress(upload_id: str, request: fastapi.Request):",
            "            async def sse_stream(request: fastapi.Request):",
            "                last_heartbeat = time.perf_counter()",
            "                is_done = False",
            "                while True:",
            "                    if await request.is_disconnected():",
            "                        file_upload_statuses.stop_tracking(upload_id)",
            "                        return",
            "                    if is_done:",
            "                        file_upload_statuses.stop_tracking(upload_id)",
            "                        return",
            "",
            "                    heartbeat_rate = 15",
            "                    check_rate = 0.05",
            "                    try:",
            "                        if file_upload_statuses.is_done(upload_id):",
            "                            message = {\"msg\": \"done\"}",
            "                            is_done = True",
            "                        else:",
            "                            update = file_upload_statuses.pop(upload_id)",
            "                            message = {",
            "                                \"msg\": \"update\",",
            "                                \"orig_name\": update.filename,",
            "                                \"chunk_size\": update.chunk_size,",
            "                            }",
            "                        yield f\"data: {json.dumps(message)}\\n\\n\"",
            "                    except FileUploadProgressNotTrackedError:",
            "                        return",
            "                    except FileUploadProgressNotQueuedError:",
            "                        await asyncio.sleep(check_rate)",
            "                        if time.perf_counter() - last_heartbeat > heartbeat_rate:",
            "                            message = {\"msg\": \"heartbeat\"}",
            "                            yield f\"data: {json.dumps(message)}\\n\\n\"",
            "                            last_heartbeat = time.perf_counter()",
            "",
            "            return StreamingResponse(",
            "                sse_stream(request),",
            "                media_type=\"text/event-stream\",",
            "            )",
            "",
            "        @app.post(\"/upload\", dependencies=[Depends(login_check)])",
            "        async def upload_file(",
            "            request: fastapi.Request,",
            "            bg_tasks: BackgroundTasks,",
            "            upload_id: Optional[str] = None,",
            "        ):",
            "            content_type_header = request.headers.get(\"Content-Type\")",
            "            content_type: bytes",
            "            content_type, _ = parse_options_header(content_type_header or \"\")",
            "            if content_type != b\"multipart/form-data\":",
            "                raise HTTPException(status_code=400, detail=\"Invalid content type.\")",
            "",
            "            try:",
            "                if upload_id:",
            "                    file_upload_statuses.track(upload_id)",
            "                multipart_parser = GradioMultiPartParser(",
            "                    request.headers,",
            "                    request.stream(),",
            "                    max_files=1000,",
            "                    max_fields=1000,",
            "                    upload_id=upload_id if upload_id else None,",
            "                    upload_progress=file_upload_statuses if upload_id else None,",
            "                )",
            "                form = await multipart_parser.parse()",
            "            except MultiPartException as exc:",
            "                raise HTTPException(status_code=400, detail=exc.message) from exc",
            "",
            "            output_files = []",
            "            files_to_copy = []",
            "            locations: list[str] = []",
            "            for temp_file in form.getlist(\"files\"):",
            "                if not isinstance(temp_file, GradioUploadFile):",
            "                    raise TypeError(\"File is not an instance of GradioUploadFile\")",
            "                if temp_file.filename:",
            "                    file_name = Path(temp_file.filename).name",
            "                    name = client_utils.strip_invalid_filename_characters(file_name)",
            "                else:",
            "                    name = f\"tmp{secrets.token_hex(5)}\"",
            "                directory = Path(app.uploaded_file_dir) / temp_file.sha.hexdigest()",
            "                directory.mkdir(exist_ok=True, parents=True)",
            "                dest = (directory / name).resolve()",
            "                temp_file.file.close()",
            "                # we need to move the temp file to the cache directory",
            "                # but that's possibly blocking and we're in an async function",
            "                # so we try to rename (this is what shutil.move tries first)",
            "                # which should be super fast.",
            "                # if that fails, we move in the background.",
            "                try:",
            "                    os.rename(temp_file.file.name, dest)",
            "                except OSError:",
            "                    files_to_copy.append(temp_file.file.name)",
            "                    locations.append(str(dest))",
            "                output_files.append(dest)",
            "            if files_to_copy:",
            "                bg_tasks.add_task(",
            "                    move_uploaded_files_to_cache, files_to_copy, locations",
            "                )",
            "            return output_files",
            "",
            "        @app.on_event(\"startup\")",
            "        @app.get(\"/startup-events\")",
            "        async def startup_events():",
            "            if not app.startup_events_triggered:",
            "                app.get_blocks().startup_events()",
            "                app.startup_events_triggered = True",
            "                return True",
            "            return False",
            "",
            "        @app.get(\"/theme.css\", response_class=PlainTextResponse)",
            "        def theme_css():",
            "            return PlainTextResponse(app.get_blocks().theme_css, media_type=\"text/css\")",
            "",
            "        @app.get(\"/robots.txt\", response_class=PlainTextResponse)",
            "        def robots_txt():",
            "            if app.get_blocks().share:",
            "                return \"User-agent: *\\nDisallow: /\"",
            "            else:",
            "                return \"User-agent: *\\nDisallow: \"",
            "",
            "        return app",
            "",
            "",
            "########",
            "# Helper functions",
            "########",
            "",
            "",
            "def safe_join(directory: str, path: str) -> str:",
            "    \"\"\"Safely path to a base directory to avoid escaping the base directory.",
            "    Borrowed from: werkzeug.security.safe_join\"\"\"",
            "    _os_alt_seps: List[str] = [",
            "        sep for sep in [os.path.sep, os.path.altsep] if sep is not None and sep != \"/\"",
            "    ]",
            "",
            "    if path == \"\":",
            "        raise HTTPException(400)",
            "",
            "    filename = posixpath.normpath(path)",
            "    fullpath = os.path.join(directory, filename)",
            "    if (",
            "        any(sep in filename for sep in _os_alt_seps)",
            "        or os.path.isabs(filename)",
            "        or filename == \"..\"",
            "        or filename.startswith(\"../\")",
            "        or os.path.isdir(fullpath)",
            "    ):",
            "        raise HTTPException(403)",
            "",
            "    if not os.path.exists(fullpath):",
            "        raise HTTPException(404, \"File not found\")",
            "",
            "    return fullpath",
            "",
            "",
            "def get_types(cls_set: List[Type]):",
            "    docset = []",
            "    types = []",
            "    for cls in cls_set:",
            "        doc = inspect.getdoc(cls) or \"\"",
            "        doc_lines = doc.split(\"\\n\")",
            "        for line in doc_lines:",
            "            if \"value (\" in line:",
            "                types.append(line.split(\"value (\")[1].split(\")\")[0])",
            "        docset.append(doc_lines[1].split(\":\")[-1])",
            "    return docset, types",
            "",
            "",
            "@document()",
            "def mount_gradio_app(",
            "    app: fastapi.FastAPI,",
            "    blocks: gradio.Blocks,",
            "    path: str,",
            "    app_kwargs: dict[str, Any] | None = None,",
            ") -> fastapi.FastAPI:",
            "    \"\"\"Mount a gradio.Blocks to an existing FastAPI application.",
            "",
            "    Parameters:",
            "        app: The parent FastAPI application.",
            "        blocks: The blocks object we want to mount to the parent app.",
            "        path: The path at which the gradio application will be mounted.",
            "        app_kwargs: Additional keyword arguments to pass to the underlying FastAPI app as a dictionary of parameter keys and argument values. For example, `{\"docs_url\": \"/docs\"}`",
            "    Example:",
            "        from fastapi import FastAPI",
            "        import gradio as gr",
            "        app = FastAPI()",
            "        @app.get(\"/\")",
            "        def read_main():",
            "            return {\"message\": \"This is your main app\"}",
            "        io = gr.Interface(lambda x: \"Hello, \" + x + \"!\", \"textbox\", \"textbox\")",
            "        app = gr.mount_gradio_app(app, io, path=\"/gradio\")",
            "        # Then run `uvicorn run:app` from the terminal and navigate to http://localhost:8000/gradio.",
            "    \"\"\"",
            "    blocks.dev_mode = False",
            "    blocks.config = blocks.get_config_file()",
            "    blocks.validate_queue_settings()",
            "    gradio_app = App.create_app(blocks, app_kwargs=app_kwargs)",
            "",
            "    old_lifespan = app.router.lifespan_context",
            "",
            "    @contextlib.asynccontextmanager",
            "    async def new_lifespan(app: FastAPI):",
            "        async with old_lifespan(",
            "            app",
            "        ):  # Instert the startup events inside the FastAPI context manager",
            "            gradio_app.get_blocks().startup_events()",
            "            yield",
            "",
            "    app.router.lifespan_context = new_lifespan",
            "",
            "    app.mount(path, gradio_app)",
            "    return app"
        ],
        "afterPatchFile": [
            "\"\"\"Implements a FastAPI server to run the gradio interface. Note that some types in this",
            "module use the Optional/Union notation so that they work correctly with pydantic.\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import asyncio",
            "import contextlib",
            "import sys",
            "",
            "if sys.version_info >= (3, 9):",
            "    from importlib.resources import files",
            "else:",
            "    from importlib_resources import files",
            "import inspect",
            "import json",
            "import mimetypes",
            "import os",
            "import posixpath",
            "import secrets",
            "import threading",
            "import time",
            "import traceback",
            "from pathlib import Path",
            "from queue import Empty as EmptyQueue",
            "from typing import TYPE_CHECKING, Any, AsyncIterator, Dict, List, Optional, Type",
            "",
            "import fastapi",
            "import httpx",
            "import markupsafe",
            "import orjson",
            "from fastapi import BackgroundTasks, Depends, FastAPI, HTTPException, status",
            "from fastapi.middleware.cors import CORSMiddleware",
            "from fastapi.responses import (",
            "    FileResponse,",
            "    HTMLResponse,",
            "    JSONResponse,",
            "    PlainTextResponse,",
            ")",
            "from fastapi.security import OAuth2PasswordRequestForm",
            "from fastapi.templating import Jinja2Templates",
            "from gradio_client import utils as client_utils",
            "from gradio_client.documentation import document",
            "from gradio_client.utils import ServerMessage",
            "from jinja2.exceptions import TemplateNotFound",
            "from multipart.multipart import parse_options_header",
            "from starlette.background import BackgroundTask",
            "from starlette.responses import RedirectResponse, StreamingResponse",
            "",
            "import gradio",
            "from gradio import ranged_response, route_utils, utils, wasm_utils",
            "from gradio.context import Context",
            "from gradio.data_classes import ComponentServerBody, PredictBody, ResetBody",
            "from gradio.exceptions import Error",
            "from gradio.oauth import attach_oauth",
            "from gradio.processing_utils import add_root_url",
            "from gradio.queueing import Estimation",
            "from gradio.route_utils import (  # noqa: F401",
            "    FileUploadProgress,",
            "    FileUploadProgressNotQueuedError,",
            "    FileUploadProgressNotTrackedError,",
            "    GradioMultiPartParser,",
            "    GradioUploadFile,",
            "    MultiPartException,",
            "    Request,",
            "    compare_passwords_securely,",
            "    move_uploaded_files_to_cache,",
            ")",
            "from gradio.state_holder import StateHolder",
            "from gradio.utils import get_package_version, get_upload_folder",
            "",
            "if TYPE_CHECKING:",
            "    from gradio.blocks import Block",
            "",
            "",
            "mimetypes.init()",
            "",
            "STATIC_TEMPLATE_LIB = files(\"gradio\").joinpath(\"templates\").as_posix()  # type: ignore",
            "STATIC_PATH_LIB = files(\"gradio\").joinpath(\"templates\", \"frontend\", \"static\").as_posix()  # type: ignore",
            "BUILD_PATH_LIB = files(\"gradio\").joinpath(\"templates\", \"frontend\", \"assets\").as_posix()  # type: ignore",
            "VERSION = get_package_version()",
            "",
            "",
            "class ORJSONResponse(JSONResponse):",
            "    media_type = \"application/json\"",
            "",
            "    @staticmethod",
            "    def _render(content: Any) -> bytes:",
            "        return orjson.dumps(",
            "            content,",
            "            option=orjson.OPT_SERIALIZE_NUMPY | orjson.OPT_PASSTHROUGH_DATETIME,",
            "            default=str,",
            "        )",
            "",
            "    def render(self, content: Any) -> bytes:",
            "        return ORJSONResponse._render(content)",
            "",
            "    @staticmethod",
            "    def _render_str(content: Any) -> str:",
            "        return ORJSONResponse._render(content).decode(\"utf-8\")",
            "",
            "",
            "def toorjson(value):",
            "    return markupsafe.Markup(",
            "        ORJSONResponse._render_str(value)",
            "        .replace(\"<\", \"\\\\u003c\")",
            "        .replace(\">\", \"\\\\u003e\")",
            "        .replace(\"&\", \"\\\\u0026\")",
            "        .replace(\"'\", \"\\\\u0027\")",
            "    )",
            "",
            "",
            "templates = Jinja2Templates(directory=STATIC_TEMPLATE_LIB)",
            "templates.env.filters[\"toorjson\"] = toorjson",
            "",
            "client = httpx.AsyncClient()",
            "",
            "file_upload_statuses = FileUploadProgress()",
            "",
            "",
            "class App(FastAPI):",
            "    \"\"\"",
            "    FastAPI App Wrapper",
            "    \"\"\"",
            "",
            "    def __init__(self, **kwargs):",
            "        self.tokens = {}",
            "        self.auth = None",
            "        self.blocks: gradio.Blocks | None = None",
            "        self.state_holder = StateHolder()",
            "        self.iterators: dict[str, AsyncIterator] = {}",
            "        self.iterators_to_reset: set[str] = set()",
            "        self.lock = utils.safe_get_lock()",
            "        self.cookie_id = secrets.token_urlsafe(32)",
            "        self.queue_token = secrets.token_urlsafe(32)",
            "        self.startup_events_triggered = False",
            "        self.uploaded_file_dir = get_upload_folder()",
            "        self.change_event: None | threading.Event = None",
            "        self._asyncio_tasks: list[asyncio.Task] = []",
            "        # Allow user to manually set `docs_url` and `redoc_url`",
            "        # when instantiating an App; when they're not set, disable docs and redoc.",
            "        kwargs.setdefault(\"docs_url\", None)",
            "        kwargs.setdefault(\"redoc_url\", None)",
            "        super().__init__(**kwargs)",
            "",
            "    def configure_app(self, blocks: gradio.Blocks) -> None:",
            "        auth = blocks.auth",
            "        if auth is not None:",
            "            if not callable(auth):",
            "                self.auth = {account[0]: account[1] for account in auth}",
            "            else:",
            "                self.auth = auth",
            "        else:",
            "            self.auth = None",
            "",
            "        self.blocks = blocks",
            "        self.cwd = os.getcwd()",
            "        self.favicon_path = blocks.favicon_path",
            "        self.tokens = {}",
            "        self.root_path = blocks.root_path",
            "        self.state_holder.set_blocks(blocks)",
            "",
            "    def get_blocks(self) -> gradio.Blocks:",
            "        if self.blocks is None:",
            "            raise ValueError(\"No Blocks has been configured for this app.\")",
            "        return self.blocks",
            "",
            "    def build_proxy_request(self, url_path):",
            "        url = httpx.URL(url_path)",
            "        assert self.blocks  # noqa: S101",
            "        # Don't proxy a URL unless it's a URL specifically loaded by the user using",
            "        # gr.load() to prevent SSRF or harvesting of HF tokens by malicious Spaces.",
            "        is_safe_url = any(",
            "            url.host == httpx.URL(root).host for root in self.blocks.proxy_urls",
            "        )",
            "        if not is_safe_url:",
            "            raise PermissionError(\"This URL cannot be proxied.\")",
            "        is_hf_url = url.host.endswith(\".hf.space\")",
            "        headers = {}",
            "        if Context.hf_token is not None and is_hf_url:",
            "            headers[\"Authorization\"] = f\"Bearer {Context.hf_token}\"",
            "        rp_req = client.build_request(\"GET\", url, headers=headers)",
            "        return rp_req",
            "",
            "    def _cancel_asyncio_tasks(self):",
            "        for task in self._asyncio_tasks:",
            "            task.cancel()",
            "        self._asyncio_tasks = []",
            "",
            "    @staticmethod",
            "    def create_app(",
            "        blocks: gradio.Blocks, app_kwargs: Dict[str, Any] | None = None",
            "    ) -> App:",
            "        app_kwargs = app_kwargs or {}",
            "        app_kwargs.setdefault(\"default_response_class\", ORJSONResponse)",
            "        app = App(**app_kwargs)",
            "        app.configure_app(blocks)",
            "",
            "        if not wasm_utils.IS_WASM:",
            "            app.add_middleware(",
            "                CORSMiddleware,",
            "                allow_origins=[\"*\"],",
            "                allow_methods=[\"*\"],",
            "                allow_headers=[\"*\"],",
            "            )",
            "",
            "        @app.get(\"/user\")",
            "        @app.get(\"/user/\")",
            "        def get_current_user(request: fastapi.Request) -> Optional[str]:",
            "            token = request.cookies.get(",
            "                f\"access-token-{app.cookie_id}\"",
            "            ) or request.cookies.get(f\"access-token-unsecure-{app.cookie_id}\")",
            "            return app.tokens.get(token)",
            "",
            "        @app.get(\"/login_check\")",
            "        @app.get(\"/login_check/\")",
            "        def login_check(user: str = Depends(get_current_user)):",
            "            if app.auth is None or user is not None:",
            "                return",
            "            raise HTTPException(",
            "                status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\"",
            "            )",
            "",
            "        @app.get(\"/token\")",
            "        @app.get(\"/token/\")",
            "        def get_token(request: fastapi.Request) -> dict:",
            "            token = request.cookies.get(f\"access-token-{app.cookie_id}\")",
            "            return {\"token\": token, \"user\": app.tokens.get(token)}",
            "",
            "        @app.get(\"/app_id\")",
            "        @app.get(\"/app_id/\")",
            "        def app_id(request: fastapi.Request) -> dict:  # noqa: ARG001",
            "            return {\"app_id\": app.get_blocks().app_id}",
            "",
            "        @app.get(\"/dev/reload\", dependencies=[Depends(login_check)])",
            "        async def notify_changes(",
            "            request: fastapi.Request,",
            "        ):",
            "            async def reload_checker(request: fastapi.Request):",
            "                heartbeat_rate = 15",
            "                check_rate = 0.05",
            "                last_heartbeat = time.perf_counter()",
            "",
            "                while True:",
            "                    if await request.is_disconnected():",
            "                        return",
            "",
            "                    if app.change_event and app.change_event.is_set():",
            "                        app.change_event.clear()",
            "                        yield \"\"\"data: CHANGE\\n\\n\"\"\"",
            "",
            "                    await asyncio.sleep(check_rate)",
            "                    if time.perf_counter() - last_heartbeat > heartbeat_rate:",
            "                        yield \"\"\"data: HEARTBEAT\\n\\n\"\"\"",
            "                        last_heartbeat = time.time()",
            "",
            "            return StreamingResponse(",
            "                reload_checker(request),",
            "                media_type=\"text/event-stream\",",
            "            )",
            "",
            "        @app.post(\"/login\")",
            "        @app.post(\"/login/\")",
            "        def login(form_data: OAuth2PasswordRequestForm = Depends()):",
            "            username, password = form_data.username.strip(), form_data.password",
            "            if app.auth is None:",
            "                return RedirectResponse(url=\"/\", status_code=status.HTTP_302_FOUND)",
            "            if (",
            "                not callable(app.auth)",
            "                and username in app.auth",
            "                and compare_passwords_securely(password, app.auth[username])  # type: ignore",
            "            ) or (callable(app.auth) and app.auth.__call__(username, password)):",
            "                token = secrets.token_urlsafe(16)",
            "                app.tokens[token] = username",
            "                response = JSONResponse(content={\"success\": True})",
            "                response.set_cookie(",
            "                    key=f\"access-token-{app.cookie_id}\",",
            "                    value=token,",
            "                    httponly=True,",
            "                    samesite=\"none\",",
            "                    secure=True,",
            "                )",
            "                response.set_cookie(",
            "                    key=f\"access-token-unsecure-{app.cookie_id}\",",
            "                    value=token,",
            "                    httponly=True,",
            "                )",
            "                return response",
            "            else:",
            "                raise HTTPException(status_code=400, detail=\"Incorrect credentials.\")",
            "",
            "        ###############",
            "        # OAuth Routes",
            "        ###############",
            "",
            "        # Define OAuth routes if the app expects it (i.e. a LoginButton is defined).",
            "        # It allows users to \"Sign in with HuggingFace\".",
            "        if app.blocks is not None and app.blocks.expects_oauth:",
            "            attach_oauth(app)",
            "",
            "        ###############",
            "        # Main Routes",
            "        ###############",
            "",
            "        @app.head(\"/\", response_class=HTMLResponse)",
            "        @app.get(\"/\", response_class=HTMLResponse)",
            "        def main(request: fastapi.Request, user: str = Depends(get_current_user)):",
            "            mimetypes.add_type(\"application/javascript\", \".js\")",
            "            blocks = app.get_blocks()",
            "            root = route_utils.get_root_url(",
            "                request=request, route_path=\"/\", root_path=app.root_path",
            "            )",
            "            if app.auth is None or user is not None:",
            "                config = app.get_blocks().config",
            "                config = route_utils.update_root_in_config(config, root)",
            "            else:",
            "                config = {",
            "                    \"auth_required\": True,",
            "                    \"auth_message\": blocks.auth_message,",
            "                    \"space_id\": app.get_blocks().space_id,",
            "                    \"root\": root,",
            "                }",
            "",
            "            try:",
            "                template = (",
            "                    \"frontend/share.html\" if blocks.share else \"frontend/index.html\"",
            "                )",
            "                return templates.TemplateResponse(",
            "                    template,",
            "                    {\"request\": request, \"config\": config},",
            "                )",
            "            except TemplateNotFound as err:",
            "                if blocks.share:",
            "                    raise ValueError(",
            "                        \"Did you install Gradio from source files? Share mode only \"",
            "                        \"works when Gradio is installed through the pip package.\"",
            "                    ) from err",
            "                else:",
            "                    raise ValueError(",
            "                        \"Did you install Gradio from source files? You need to build \"",
            "                        \"the frontend by running /scripts/build_frontend.sh\"",
            "                    ) from err",
            "",
            "        @app.get(\"/info/\", dependencies=[Depends(login_check)])",
            "        @app.get(\"/info\", dependencies=[Depends(login_check)])",
            "        def api_info():",
            "            return app.get_blocks().get_api_info()  # type: ignore",
            "",
            "        @app.get(\"/config/\", dependencies=[Depends(login_check)])",
            "        @app.get(\"/config\", dependencies=[Depends(login_check)])",
            "        def get_config(request: fastapi.Request):",
            "            config = app.get_blocks().config",
            "            root = route_utils.get_root_url(",
            "                request=request, route_path=\"/config\", root_path=app.root_path",
            "            )",
            "            config = route_utils.update_root_in_config(config, root)",
            "            return ORJSONResponse(content=config)",
            "",
            "        @app.get(\"/static/{path:path}\")",
            "        def static_resource(path: str):",
            "            static_file = safe_join(STATIC_PATH_LIB, path)",
            "            return FileResponse(static_file)",
            "",
            "        @app.get(\"/custom_component/{id}/{type}/{file_name}\")",
            "        def custom_component_path(id: str, type: str, file_name: str):",
            "            config = app.get_blocks().config",
            "            components = config[\"components\"]",
            "            location = next(",
            "                (item for item in components if item[\"component_class_id\"] == id), None",
            "            )",
            "",
            "            if location is None:",
            "                raise HTTPException(status_code=404, detail=\"Component not found.\")",
            "",
            "            component_instance = app.get_blocks().get_component(location[\"id\"])",
            "",
            "            module_name = component_instance.__class__.__module__",
            "            module_path = sys.modules[module_name].__file__",
            "",
            "            if module_path is None or component_instance is None:",
            "                raise HTTPException(status_code=404, detail=\"Component not found.\")",
            "",
            "            return FileResponse(",
            "                safe_join(",
            "                    str(Path(module_path).parent),",
            "                    f\"{component_instance.__class__.TEMPLATE_DIR}/{type}/{file_name}\",",
            "                )",
            "            )",
            "",
            "        @app.get(\"/assets/{path:path}\")",
            "        def build_resource(path: str):",
            "            build_file = safe_join(BUILD_PATH_LIB, path)",
            "            return FileResponse(build_file)",
            "",
            "        @app.get(\"/favicon.ico\")",
            "        async def favicon():",
            "            blocks = app.get_blocks()",
            "            if blocks.favicon_path is None:",
            "                return static_resource(\"img/logo.svg\")",
            "            else:",
            "                return FileResponse(blocks.favicon_path)",
            "",
            "        @app.head(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])",
            "        @app.get(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])",
            "        async def reverse_proxy(url_path: str):",
            "            # Adapted from: https://github.com/tiangolo/fastapi/issues/1788",
            "            try:",
            "                rp_req = app.build_proxy_request(url_path)",
            "            except PermissionError as err:",
            "                raise HTTPException(status_code=400, detail=str(err)) from err",
            "            rp_resp = await client.send(rp_req, stream=True)",
            "            return StreamingResponse(",
            "                rp_resp.aiter_raw(),",
            "                status_code=rp_resp.status_code,",
            "                headers=rp_resp.headers,  # type: ignore",
            "                background=BackgroundTask(rp_resp.aclose),",
            "            )",
            "",
            "        @app.head(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])",
            "        @app.get(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])",
            "        async def file(path_or_url: str, request: fastapi.Request):",
            "            blocks = app.get_blocks()",
            "            if client_utils.is_http_url_like(path_or_url):",
            "                return RedirectResponse(",
            "                    url=path_or_url, status_code=status.HTTP_302_FOUND",
            "                )",
            "",
            "            if route_utils.starts_with_protocol(path_or_url):",
            "                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")",
            "",
            "            abs_path = utils.abspath(path_or_url)",
            "",
            "            in_blocklist = any(",
            "                utils.is_in_or_equal(abs_path, blocked_path)",
            "                for blocked_path in blocks.blocked_paths",
            "            )",
            "",
            "            is_dir = abs_path.is_dir()",
            "",
            "            if in_blocklist or is_dir:",
            "                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")",
            "",
            "            created_by_app = False",
            "            for temp_file_set in blocks.temp_file_sets:",
            "                if abs_path in temp_file_set:",
            "                    created_by_app = True",
            "                    break",
            "            in_allowlist = any(",
            "                utils.is_in_or_equal(abs_path, allowed_path)",
            "                for allowed_path in blocks.allowed_paths",
            "            )",
            "            was_uploaded = utils.is_in_or_equal(abs_path, app.uploaded_file_dir)",
            "            is_cached_example = utils.is_in_or_equal(",
            "                abs_path, utils.abspath(utils.get_cache_folder())",
            "            )",
            "",
            "            if not (",
            "                created_by_app or in_allowlist or was_uploaded or is_cached_example",
            "            ):",
            "                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")",
            "",
            "            if not abs_path.exists():",
            "                raise HTTPException(404, f\"File not found: {path_or_url}.\")",
            "",
            "            range_val = request.headers.get(\"Range\", \"\").strip()",
            "            if range_val.startswith(\"bytes=\") and \"-\" in range_val:",
            "                range_val = range_val[6:]",
            "                start, end = range_val.split(\"-\")",
            "                if start.isnumeric() and end.isnumeric():",
            "                    start = int(start)",
            "                    end = int(end)",
            "                    response = ranged_response.RangedFileResponse(",
            "                        abs_path,",
            "                        ranged_response.OpenRange(start, end),",
            "                        dict(request.headers),",
            "                        stat_result=os.stat(abs_path),",
            "                    )",
            "                    return response",
            "",
            "            return FileResponse(abs_path, headers={\"Accept-Ranges\": \"bytes\"})",
            "",
            "        @app.get(",
            "            \"/stream/{session_hash}/{run}/{component_id}\",",
            "            dependencies=[Depends(login_check)],",
            "        )",
            "        async def stream(",
            "            session_hash: str,",
            "            run: int,",
            "            component_id: int,",
            "            request: fastapi.Request,  # noqa: ARG001",
            "        ):",
            "            stream: list = (",
            "                app.get_blocks()",
            "                .pending_streams[session_hash]",
            "                .get(run, {})",
            "                .get(component_id, None)",
            "            )",
            "            if stream is None:",
            "                raise HTTPException(404, \"Stream not found.\")",
            "",
            "            def stream_wrapper():",
            "                check_stream_rate = 0.01",
            "                max_wait_time = 120  # maximum wait between yields - assume generator thread has crashed otherwise.",
            "                wait_time = 0",
            "                while True:",
            "                    if len(stream) == 0:",
            "                        if wait_time > max_wait_time:",
            "                            return",
            "                        wait_time += check_stream_rate",
            "                        time.sleep(check_stream_rate)",
            "                        continue",
            "                    wait_time = 0",
            "                    next_stream = stream.pop(0)",
            "                    if next_stream is None:",
            "                        return",
            "                    yield next_stream",
            "",
            "            return StreamingResponse(stream_wrapper())",
            "",
            "        @app.get(\"/file/{path:path}\", dependencies=[Depends(login_check)])",
            "        async def file_deprecated(path: str, request: fastapi.Request):",
            "            return await file(path, request)",
            "",
            "        @app.post(\"/reset/\")",
            "        @app.post(\"/reset\")",
            "        async def reset_iterator(body: ResetBody):",
            "            if body.event_id not in app.iterators:",
            "                return {\"success\": False}",
            "            async with app.lock:",
            "                del app.iterators[body.event_id]",
            "                app.iterators_to_reset.add(body.event_id)",
            "                await app.get_blocks()._queue.clean_events(event_id=body.event_id)",
            "            return {\"success\": True}",
            "",
            "        # had to use '/run' endpoint for Colab compatibility, '/api' supported for backwards compatibility",
            "        @app.post(\"/run/{api_name}\", dependencies=[Depends(login_check)])",
            "        @app.post(\"/run/{api_name}/\", dependencies=[Depends(login_check)])",
            "        @app.post(\"/api/{api_name}\", dependencies=[Depends(login_check)])",
            "        @app.post(\"/api/{api_name}/\", dependencies=[Depends(login_check)])",
            "        async def predict(",
            "            api_name: str,",
            "            body: PredictBody,",
            "            request: fastapi.Request,",
            "            username: str = Depends(get_current_user),",
            "        ):",
            "            fn_index_inferred = route_utils.infer_fn_index(",
            "                app=app, api_name=api_name, body=body",
            "            )",
            "",
            "            if not app.get_blocks().api_open and app.get_blocks().queue_enabled_for_fn(",
            "                fn_index_inferred",
            "            ):",
            "                raise HTTPException(",
            "                    detail=\"This API endpoint does not accept direct HTTP POST requests. Please join the queue to use this API.\",",
            "                    status_code=status.HTTP_404_NOT_FOUND,",
            "                )",
            "",
            "            gr_request = route_utils.compile_gr_request(",
            "                app,",
            "                body,",
            "                fn_index_inferred=fn_index_inferred,",
            "                username=username,",
            "                request=request,",
            "            )",
            "",
            "            try:",
            "                output = await route_utils.call_process_api(",
            "                    app=app,",
            "                    body=body,",
            "                    gr_request=gr_request,",
            "                    fn_index_inferred=fn_index_inferred,",
            "                )",
            "            except BaseException as error:",
            "                show_error = app.get_blocks().show_error or isinstance(error, Error)",
            "                traceback.print_exc()",
            "                return JSONResponse(",
            "                    content={\"error\": str(error) if show_error else None},",
            "                    status_code=500,",
            "                )",
            "            root_path = route_utils.get_root_url(",
            "                request=request, route_path=f\"/api/{api_name}\", root_path=app.root_path",
            "            )",
            "            output = add_root_url(output, root_path, None)",
            "            return output",
            "",
            "        @app.get(\"/queue/data\", dependencies=[Depends(login_check)])",
            "        async def queue_data(",
            "            request: fastapi.Request,",
            "            session_hash: str,",
            "        ):",
            "            blocks = app.get_blocks()",
            "            root_path = route_utils.get_root_url(",
            "                request=request, route_path=\"/queue/data\", root_path=app.root_path",
            "            )",
            "",
            "            async def sse_stream(request: fastapi.Request):",
            "                try:",
            "                    last_heartbeat = time.perf_counter()",
            "                    while True:",
            "                        if await request.is_disconnected():",
            "                            await blocks._queue.clean_events(session_hash=session_hash)",
            "                            return",
            "",
            "                        if (",
            "                            session_hash",
            "                            not in blocks._queue.pending_messages_per_session",
            "                        ):",
            "                            raise HTTPException(",
            "                                status_code=status.HTTP_404_NOT_FOUND,",
            "                                detail=\"Session not found.\",",
            "                            )",
            "",
            "                        heartbeat_rate = 15",
            "                        check_rate = 0.05",
            "                        message = None",
            "                        try:",
            "                            messages = blocks._queue.pending_messages_per_session[",
            "                                session_hash",
            "                            ]",
            "                            message = messages.get_nowait()",
            "                        except EmptyQueue:",
            "                            await asyncio.sleep(check_rate)",
            "                            if time.perf_counter() - last_heartbeat > heartbeat_rate:",
            "                                # Fix this",
            "                                message = {",
            "                                    \"msg\": ServerMessage.heartbeat,",
            "                                }",
            "                                # Need to reset last_heartbeat with perf_counter",
            "                                # otherwise only a single hearbeat msg will be sent",
            "                                # and then the stream will retry leading to infinite queue \ud83d\ude2c",
            "                                last_heartbeat = time.perf_counter()",
            "",
            "                        if blocks._queue.stopped:",
            "                            message = {",
            "                                \"msg\": \"unexpected_error\",",
            "                                \"message\": \"Server stopped unexpectedly.\",",
            "                                \"success\": False,",
            "                            }",
            "                        if message:",
            "                            add_root_url(message, root_path, None)",
            "                            yield f\"data: {json.dumps(message)}\\n\\n\"",
            "                            if message[\"msg\"] == ServerMessage.process_completed:",
            "                                blocks._queue.pending_event_ids_session[",
            "                                    session_hash",
            "                                ].remove(message[\"event_id\"])",
            "                                if message[\"msg\"] == ServerMessage.server_stopped or (",
            "                                    message[\"msg\"] == ServerMessage.process_completed",
            "                                    and (",
            "                                        len(",
            "                                            blocks._queue.pending_event_ids_session[",
            "                                                session_hash",
            "                                            ]",
            "                                        )",
            "                                        == 0",
            "                                    )",
            "                                ):",
            "                                    return",
            "                except BaseException as e:",
            "                    message = {",
            "                        \"msg\": \"unexpected_error\",",
            "                        \"success\": False,",
            "                        \"message\": str(e),",
            "                    }",
            "                    yield f\"data: {json.dumps(message)}\\n\\n\"",
            "                    if isinstance(e, asyncio.CancelledError):",
            "                        del blocks._queue.pending_messages_per_session[session_hash]",
            "                        await blocks._queue.clean_events(session_hash=session_hash)",
            "                    raise e",
            "",
            "            return StreamingResponse(",
            "                sse_stream(request),",
            "                media_type=\"text/event-stream\",",
            "            )",
            "",
            "        @app.post(\"/queue/join\", dependencies=[Depends(login_check)])",
            "        async def queue_join(",
            "            body: PredictBody,",
            "            request: fastapi.Request,",
            "            username: str = Depends(get_current_user),",
            "        ):",
            "            blocks = app.get_blocks()",
            "",
            "            if blocks._queue.server_app is None:",
            "                blocks._queue.set_server_app(app)",
            "",
            "            if blocks._queue.stopped:",
            "                raise HTTPException(",
            "                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,",
            "                    detail=\"Queue is stopped.\",",
            "                )",
            "",
            "            success, event_id = await blocks._queue.push(body, request, username)",
            "            if not success:",
            "                status_code = (",
            "                    status.HTTP_503_SERVICE_UNAVAILABLE",
            "                    if \"Queue is full.\" in event_id",
            "                    else status.HTTP_400_BAD_REQUEST",
            "                )",
            "                raise HTTPException(status_code=status_code, detail=event_id)",
            "            return {\"event_id\": event_id}",
            "",
            "        @app.post(\"/component_server\", dependencies=[Depends(login_check)])",
            "        @app.post(\"/component_server/\", dependencies=[Depends(login_check)])",
            "        def component_server(body: ComponentServerBody):",
            "            state = app.state_holder[body.session_hash]",
            "            component_id = body.component_id",
            "            block: Block",
            "            if component_id in state:",
            "                block = state[component_id]",
            "            else:",
            "                block = app.get_blocks().blocks[component_id]",
            "            fn = getattr(block, body.fn_name, None)",
            "            if fn is None or not getattr(fn, \"_is_server_fn\", False):",
            "                raise HTTPException(",
            "                    status_code=status.HTTP_404_NOT_FOUND,",
            "                    detail=\"Function not found.\",",
            "                )",
            "            return fn(body.data)",
            "",
            "        @app.get(",
            "            \"/queue/status\",",
            "            dependencies=[Depends(login_check)],",
            "            response_model=Estimation,",
            "        )",
            "        async def get_queue_status():",
            "            return app.get_blocks()._queue.get_status()",
            "",
            "        @app.get(\"/upload_progress\")",
            "        def get_upload_progress(upload_id: str, request: fastapi.Request):",
            "            async def sse_stream(request: fastapi.Request):",
            "                last_heartbeat = time.perf_counter()",
            "                is_done = False",
            "                while True:",
            "                    if await request.is_disconnected():",
            "                        file_upload_statuses.stop_tracking(upload_id)",
            "                        return",
            "                    if is_done:",
            "                        file_upload_statuses.stop_tracking(upload_id)",
            "                        return",
            "",
            "                    heartbeat_rate = 15",
            "                    check_rate = 0.05",
            "                    try:",
            "                        if file_upload_statuses.is_done(upload_id):",
            "                            message = {\"msg\": \"done\"}",
            "                            is_done = True",
            "                        else:",
            "                            update = file_upload_statuses.pop(upload_id)",
            "                            message = {",
            "                                \"msg\": \"update\",",
            "                                \"orig_name\": update.filename,",
            "                                \"chunk_size\": update.chunk_size,",
            "                            }",
            "                        yield f\"data: {json.dumps(message)}\\n\\n\"",
            "                    except FileUploadProgressNotTrackedError:",
            "                        return",
            "                    except FileUploadProgressNotQueuedError:",
            "                        await asyncio.sleep(check_rate)",
            "                        if time.perf_counter() - last_heartbeat > heartbeat_rate:",
            "                            message = {\"msg\": \"heartbeat\"}",
            "                            yield f\"data: {json.dumps(message)}\\n\\n\"",
            "                            last_heartbeat = time.perf_counter()",
            "",
            "            return StreamingResponse(",
            "                sse_stream(request),",
            "                media_type=\"text/event-stream\",",
            "            )",
            "",
            "        @app.post(\"/upload\", dependencies=[Depends(login_check)])",
            "        async def upload_file(",
            "            request: fastapi.Request,",
            "            bg_tasks: BackgroundTasks,",
            "            upload_id: Optional[str] = None,",
            "        ):",
            "            content_type_header = request.headers.get(\"Content-Type\")",
            "            content_type: bytes",
            "            content_type, _ = parse_options_header(content_type_header or \"\")",
            "            if content_type != b\"multipart/form-data\":",
            "                raise HTTPException(status_code=400, detail=\"Invalid content type.\")",
            "",
            "            try:",
            "                if upload_id:",
            "                    file_upload_statuses.track(upload_id)",
            "                multipart_parser = GradioMultiPartParser(",
            "                    request.headers,",
            "                    request.stream(),",
            "                    max_files=1000,",
            "                    max_fields=1000,",
            "                    upload_id=upload_id if upload_id else None,",
            "                    upload_progress=file_upload_statuses if upload_id else None,",
            "                )",
            "                form = await multipart_parser.parse()",
            "            except MultiPartException as exc:",
            "                raise HTTPException(status_code=400, detail=exc.message) from exc",
            "",
            "            output_files = []",
            "            files_to_copy = []",
            "            locations: list[str] = []",
            "            for temp_file in form.getlist(\"files\"):",
            "                if not isinstance(temp_file, GradioUploadFile):",
            "                    raise TypeError(\"File is not an instance of GradioUploadFile\")",
            "                if temp_file.filename:",
            "                    file_name = Path(temp_file.filename).name",
            "                    name = client_utils.strip_invalid_filename_characters(file_name)",
            "                else:",
            "                    name = f\"tmp{secrets.token_hex(5)}\"",
            "                directory = Path(app.uploaded_file_dir) / temp_file.sha.hexdigest()",
            "                directory.mkdir(exist_ok=True, parents=True)",
            "                dest = (directory / name).resolve()",
            "                temp_file.file.close()",
            "                # we need to move the temp file to the cache directory",
            "                # but that's possibly blocking and we're in an async function",
            "                # so we try to rename (this is what shutil.move tries first)",
            "                # which should be super fast.",
            "                # if that fails, we move in the background.",
            "                try:",
            "                    os.rename(temp_file.file.name, dest)",
            "                except OSError:",
            "                    files_to_copy.append(temp_file.file.name)",
            "                    locations.append(str(dest))",
            "                output_files.append(dest)",
            "            if files_to_copy:",
            "                bg_tasks.add_task(",
            "                    move_uploaded_files_to_cache, files_to_copy, locations",
            "                )",
            "            return output_files",
            "",
            "        @app.on_event(\"startup\")",
            "        @app.get(\"/startup-events\")",
            "        async def startup_events():",
            "            if not app.startup_events_triggered:",
            "                app.get_blocks().startup_events()",
            "                app.startup_events_triggered = True",
            "                return True",
            "            return False",
            "",
            "        @app.get(\"/theme.css\", response_class=PlainTextResponse)",
            "        def theme_css():",
            "            return PlainTextResponse(app.get_blocks().theme_css, media_type=\"text/css\")",
            "",
            "        @app.get(\"/robots.txt\", response_class=PlainTextResponse)",
            "        def robots_txt():",
            "            if app.get_blocks().share:",
            "                return \"User-agent: *\\nDisallow: /\"",
            "            else:",
            "                return \"User-agent: *\\nDisallow: \"",
            "",
            "        return app",
            "",
            "",
            "########",
            "# Helper functions",
            "########",
            "",
            "",
            "def safe_join(directory: str, path: str) -> str:",
            "    \"\"\"Safely path to a base directory to avoid escaping the base directory.",
            "    Borrowed from: werkzeug.security.safe_join\"\"\"",
            "    _os_alt_seps: List[str] = [",
            "        sep for sep in [os.path.sep, os.path.altsep] if sep is not None and sep != \"/\"",
            "    ]",
            "",
            "    if path == \"\":",
            "        raise HTTPException(400)",
            "",
            "    filename = posixpath.normpath(path)",
            "    fullpath = os.path.join(directory, filename)",
            "    if (",
            "        any(sep in filename for sep in _os_alt_seps)",
            "        or os.path.isabs(filename)",
            "        or filename == \"..\"",
            "        or filename.startswith(\"../\")",
            "        or os.path.isdir(fullpath)",
            "    ):",
            "        raise HTTPException(403)",
            "",
            "    if not os.path.exists(fullpath):",
            "        raise HTTPException(404, \"File not found\")",
            "",
            "    return fullpath",
            "",
            "",
            "def get_types(cls_set: List[Type]):",
            "    docset = []",
            "    types = []",
            "    for cls in cls_set:",
            "        doc = inspect.getdoc(cls) or \"\"",
            "        doc_lines = doc.split(\"\\n\")",
            "        for line in doc_lines:",
            "            if \"value (\" in line:",
            "                types.append(line.split(\"value (\")[1].split(\")\")[0])",
            "        docset.append(doc_lines[1].split(\":\")[-1])",
            "    return docset, types",
            "",
            "",
            "@document()",
            "def mount_gradio_app(",
            "    app: fastapi.FastAPI,",
            "    blocks: gradio.Blocks,",
            "    path: str,",
            "    app_kwargs: dict[str, Any] | None = None,",
            ") -> fastapi.FastAPI:",
            "    \"\"\"Mount a gradio.Blocks to an existing FastAPI application.",
            "",
            "    Parameters:",
            "        app: The parent FastAPI application.",
            "        blocks: The blocks object we want to mount to the parent app.",
            "        path: The path at which the gradio application will be mounted.",
            "        app_kwargs: Additional keyword arguments to pass to the underlying FastAPI app as a dictionary of parameter keys and argument values. For example, `{\"docs_url\": \"/docs\"}`",
            "    Example:",
            "        from fastapi import FastAPI",
            "        import gradio as gr",
            "        app = FastAPI()",
            "        @app.get(\"/\")",
            "        def read_main():",
            "            return {\"message\": \"This is your main app\"}",
            "        io = gr.Interface(lambda x: \"Hello, \" + x + \"!\", \"textbox\", \"textbox\")",
            "        app = gr.mount_gradio_app(app, io, path=\"/gradio\")",
            "        # Then run `uvicorn run:app` from the terminal and navigate to http://localhost:8000/gradio.",
            "    \"\"\"",
            "    blocks.dev_mode = False",
            "    blocks.config = blocks.get_config_file()",
            "    blocks.validate_queue_settings()",
            "    gradio_app = App.create_app(blocks, app_kwargs=app_kwargs)",
            "",
            "    old_lifespan = app.router.lifespan_context",
            "",
            "    @contextlib.asynccontextmanager",
            "    async def new_lifespan(app: FastAPI):",
            "        async with old_lifespan(",
            "            app",
            "        ):  # Instert the startup events inside the FastAPI context manager",
            "            gradio_app.get_blocks().startup_events()",
            "            yield",
            "",
            "    app.router.lifespan_context = new_lifespan",
            "",
            "    app.mount(path, gradio_app)",
            "    return app"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "20": [],
            "70": [],
            "71": [],
            "72": [],
            "139": [
                "App",
                "__init__"
            ],
            "140": [
                "App",
                "__init__"
            ],
            "141": [
                "App",
                "__init__"
            ]
        },
        "addLocation": []
    },
    "gradio/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " import os"
            },
            "1": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " import pkgutil"
            },
            "2": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " import re"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+import tempfile"
            },
            "4": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " import threading"
            },
            "5": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " import time"
            },
            "6": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " import traceback"
            },
            "7": {
                "beforePatchRowNumber": 1082,
                "afterPatchRowNumber": 1083,
                "PatchRowcode": "         return edits"
            },
            "8": {
                "beforePatchRowNumber": 1083,
                "afterPatchRowNumber": 1084,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 1084,
                "afterPatchRowNumber": 1085,
                "PatchRowcode": "     return compare_objects(old, new)"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1086,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1087,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1088,
                "PatchRowcode": "+def get_upload_folder() -> str:"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1089,
                "PatchRowcode": "+    return os.environ.get(\"GRADIO_TEMP_DIR\") or str("
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1090,
                "PatchRowcode": "+        (Path(tempfile.gettempdir()) / \"gradio\").resolve()"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1091,
                "PatchRowcode": "+    )"
            }
        },
        "frontPatchFile": [
            "\"\"\" Handy utility functions. \"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import asyncio",
            "import copy",
            "import dataclasses",
            "import functools",
            "import importlib",
            "import inspect",
            "import json",
            "import json.decoder",
            "import os",
            "import pkgutil",
            "import re",
            "import threading",
            "import time",
            "import traceback",
            "import typing",
            "import urllib.parse",
            "import warnings",
            "from abc import ABC, abstractmethod",
            "from collections import OrderedDict",
            "from contextlib import contextmanager",
            "from io import BytesIO",
            "from numbers import Number",
            "from pathlib import Path",
            "from types import AsyncGeneratorType, GeneratorType",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Callable,",
            "    Generic,",
            "    Iterable,",
            "    Iterator,",
            "    Optional,",
            "    TypeVar,",
            ")",
            "",
            "import anyio",
            "import httpx",
            "from typing_extensions import ParamSpec",
            "",
            "import gradio",
            "from gradio.context import Context",
            "from gradio.strings import en",
            "",
            "if TYPE_CHECKING:  # Only import for type checking (is False at runtime).",
            "    from gradio.blocks import BlockContext, Blocks",
            "    from gradio.components import Component",
            "    from gradio.routes import App, Request",
            "",
            "JSON_PATH = os.path.join(os.path.dirname(gradio.__file__), \"launches.json\")",
            "",
            "P = ParamSpec(\"P\")",
            "T = TypeVar(\"T\")",
            "",
            "",
            "def get_package_version() -> str:",
            "    try:",
            "        package_json_data = (",
            "            pkgutil.get_data(__name__, \"package.json\").decode(\"utf-8\").strip()  # type: ignore",
            "        )",
            "        package_data = json.loads(package_json_data)",
            "        version = package_data.get(\"version\", \"\")",
            "        return version",
            "    except Exception:",
            "        return \"\"",
            "",
            "",
            "def safe_get_lock() -> asyncio.Lock:",
            "    \"\"\"Get asyncio.Lock() without fear of getting an Exception.",
            "",
            "    Needed because in reload mode we import the Blocks object outside",
            "    the main thread.",
            "    \"\"\"",
            "    try:",
            "        asyncio.get_event_loop()",
            "        return asyncio.Lock()",
            "    except RuntimeError:",
            "        return None  # type: ignore",
            "",
            "",
            "class BaseReloader(ABC):",
            "    @property",
            "    @abstractmethod",
            "    def running_app(self) -> App:",
            "        pass",
            "",
            "    def queue_changed(self, demo: Blocks):",
            "        return (",
            "            hasattr(self.running_app.blocks, \"_queue\") and not hasattr(demo, \"_queue\")",
            "        ) or (",
            "            not hasattr(self.running_app.blocks, \"_queue\") and hasattr(demo, \"_queue\")",
            "        )",
            "",
            "    def swap_blocks(self, demo: Blocks):",
            "        assert self.running_app.blocks  # noqa: S101",
            "        # Copy over the blocks to get new components and events but",
            "        # not a new queue",
            "        self.running_app.blocks._queue.block_fns = demo.fns",
            "        demo._queue = self.running_app.blocks._queue",
            "        self.running_app.blocks = demo",
            "        demo._queue.reload()",
            "",
            "",
            "class SourceFileReloader(BaseReloader):",
            "    def __init__(",
            "        self,",
            "        app: App,",
            "        watch_dirs: list[str],",
            "        watch_module_name: str,",
            "        stop_event: threading.Event,",
            "        change_event: threading.Event,",
            "        demo_name: str = \"demo\",",
            "    ) -> None:",
            "        super().__init__()",
            "        self.app = app",
            "        self.watch_dirs = watch_dirs",
            "        self.watch_module_name = watch_module_name",
            "        self.stop_event = stop_event",
            "        self.change_event = change_event",
            "        self.demo_name = demo_name",
            "",
            "    @property",
            "    def running_app(self) -> App:",
            "        return self.app",
            "",
            "    def should_watch(self) -> bool:",
            "        return not self.stop_event.is_set()",
            "",
            "    def stop(self) -> None:",
            "        self.stop_event.set()",
            "",
            "    def alert_change(self):",
            "        self.change_event.set()",
            "",
            "    def swap_blocks(self, demo: Blocks):",
            "        super().swap_blocks(demo)",
            "        self.alert_change()",
            "",
            "",
            "def watchfn(reloader: SourceFileReloader):",
            "    \"\"\"Watch python files in a given module.",
            "",
            "    get_changes is taken from uvicorn's default file watcher.",
            "    \"\"\"",
            "",
            "    # The thread running watchfn will be the thread reloading",
            "    # the app. So we need to modify this thread_data attr here",
            "    # so that subsequent calls to reload don't launch the app",
            "    from gradio.cli.commands.reload import reload_thread",
            "",
            "    reload_thread.running_reload = True",
            "",
            "    def get_changes() -> Path | None:",
            "        for file in iter_py_files():",
            "            try:",
            "                mtime = file.stat().st_mtime",
            "            except OSError:  # pragma: nocover",
            "                continue",
            "",
            "            old_time = mtimes.get(file)",
            "            if old_time is None:",
            "                mtimes[file] = mtime",
            "                continue",
            "            elif mtime > old_time:",
            "                return file",
            "        return None",
            "",
            "    def iter_py_files() -> Iterator[Path]:",
            "        for reload_dir in reload_dirs:",
            "            for path in list(reload_dir.rglob(\"*.py\")):",
            "                yield path.resolve()",
            "            for path in list(reload_dir.rglob(\"*.css\")):",
            "                yield path.resolve()",
            "",
            "    module = None",
            "    reload_dirs = [Path(dir_) for dir_ in reloader.watch_dirs]",
            "    import sys",
            "",
            "    for dir_ in reload_dirs:",
            "        sys.path.insert(0, str(dir_))",
            "",
            "    mtimes = {}",
            "    while reloader.should_watch():",
            "        changed = get_changes()",
            "        if changed:",
            "            print(f\"Changes detected in: {changed}\")",
            "            # To simulate a fresh reload, delete all module references from sys.modules",
            "            # for the modules in the package the change came from.",
            "            dir_ = next(d for d in reload_dirs if is_in_or_equal(changed, d))",
            "            modules = list(sys.modules)",
            "            for k in modules:",
            "                v = sys.modules[k]",
            "                sourcefile = getattr(v, \"__file__\", None)",
            "                # Do not reload `reload.py` to keep thread data",
            "                if (",
            "                    sourcefile",
            "                    and dir_ == Path(inspect.getfile(gradio)).parent",
            "                    and sourcefile.endswith(\"reload.py\")",
            "                ):",
            "                    continue",
            "                if sourcefile and is_in_or_equal(sourcefile, dir_):",
            "                    del sys.modules[k]",
            "            try:",
            "                module = importlib.import_module(reloader.watch_module_name)",
            "                module = importlib.reload(module)",
            "            except Exception:",
            "                print(",
            "                    f\"Reloading {reloader.watch_module_name} failed with the following exception: \"",
            "                )",
            "                traceback.print_exc()",
            "                mtimes = {}",
            "                continue",
            "",
            "            demo = getattr(module, reloader.demo_name)",
            "            if reloader.queue_changed(demo):",
            "                print(",
            "                    \"Reloading failed. The new demo has a queue and the old one doesn't (or vice versa). \"",
            "                    \"Please launch your demo again\"",
            "                )",
            "            else:",
            "                reloader.swap_blocks(demo)",
            "            mtimes = {}",
            "        time.sleep(0.05)",
            "",
            "",
            "def colab_check() -> bool:",
            "    \"\"\"",
            "    Check if interface is launching from Google Colab",
            "    :return is_colab (bool): True or False",
            "    \"\"\"",
            "    is_colab = False",
            "    try:  # Check if running interactively using ipython.",
            "        from IPython.core.getipython import get_ipython",
            "",
            "        from_ipynb = get_ipython()",
            "        if \"google.colab\" in str(from_ipynb):",
            "            is_colab = True",
            "    except (ImportError, NameError):",
            "        pass",
            "    return is_colab",
            "",
            "",
            "def kaggle_check() -> bool:",
            "    return bool(",
            "        os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\") or os.environ.get(\"GFOOTBALL_DATA_DIR\")",
            "    )",
            "",
            "",
            "def sagemaker_check() -> bool:",
            "    try:",
            "        import boto3  # type: ignore",
            "",
            "        client = boto3.client(\"sts\")",
            "        response = client.get_caller_identity()",
            "        return \"sagemaker\" in response[\"Arn\"].lower()",
            "    except Exception:",
            "        return False",
            "",
            "",
            "def ipython_check() -> bool:",
            "    \"\"\"",
            "    Check if interface is launching from iPython (not colab)",
            "    :return is_ipython (bool): True or False",
            "    \"\"\"",
            "    is_ipython = False",
            "    try:  # Check if running interactively using ipython.",
            "        from IPython.core.getipython import get_ipython",
            "",
            "        if get_ipython() is not None:",
            "            is_ipython = True",
            "    except (ImportError, NameError):",
            "        pass",
            "    return is_ipython",
            "",
            "",
            "def get_space() -> str | None:",
            "    if os.getenv(\"SYSTEM\") == \"spaces\":",
            "        return os.getenv(\"SPACE_ID\")",
            "    return None",
            "",
            "",
            "def is_zero_gpu_space() -> bool:",
            "    return os.getenv(\"SPACES_ZERO_GPU\") == \"true\"",
            "",
            "",
            "def download_if_url(article: str) -> str:",
            "    try:",
            "        result = urllib.parse.urlparse(article)",
            "        is_url = all([result.scheme, result.netloc, result.path])",
            "        is_url = is_url and result.scheme in [\"http\", \"https\"]",
            "    except ValueError:",
            "        is_url = False",
            "",
            "    if not is_url:",
            "        return article",
            "",
            "    try:",
            "        response = httpx.get(article, timeout=3)",
            "        if response.status_code == httpx.codes.OK:  # pylint: disable=no-member",
            "            article = response.text",
            "    except (httpx.InvalidURL, httpx.RequestError):",
            "        pass",
            "",
            "    return article",
            "",
            "",
            "def launch_counter() -> None:",
            "    try:",
            "        if not os.path.exists(JSON_PATH):",
            "            launches = {\"launches\": 1}",
            "            with open(JSON_PATH, \"w+\") as j:",
            "                json.dump(launches, j)",
            "        else:",
            "            with open(JSON_PATH) as j:",
            "                launches = json.load(j)",
            "            launches[\"launches\"] += 1",
            "            if launches[\"launches\"] in [25, 50, 150, 500, 1000]:",
            "                print(en[\"BETA_INVITE\"])",
            "            with open(JSON_PATH, \"w\") as j:",
            "                j.write(json.dumps(launches))",
            "    except Exception:",
            "        pass",
            "",
            "",
            "def get_default_args(func: Callable) -> list[Any]:",
            "    signature = inspect.signature(func)",
            "    return [",
            "        v.default if v.default is not inspect.Parameter.empty else None",
            "        for v in signature.parameters.values()",
            "    ]",
            "",
            "",
            "def assert_configs_are_equivalent_besides_ids(",
            "    config1: dict, config2: dict, root_keys: tuple = (\"mode\",)",
            "):",
            "    \"\"\"Allows you to test if two different Blocks configs produce the same demo.",
            "",
            "    Parameters:",
            "    config1 (dict): nested dict with config from the first Blocks instance",
            "    config2 (dict): nested dict with config from the second Blocks instance",
            "    root_keys (Tuple): an interable consisting of which keys to test for equivalence at",
            "        the root level of the config. By default, only \"mode\" is tested,",
            "        so keys like \"version\" are ignored.",
            "    \"\"\"",
            "    config1 = copy.deepcopy(config1)",
            "    config2 = copy.deepcopy(config2)",
            "    config1 = json.loads(json.dumps(config1))  # convert tuples to lists",
            "    config2 = json.loads(json.dumps(config2))",
            "",
            "    for key in root_keys:",
            "        if config1[key] != config2[key]:",
            "            raise ValueError(f\"Configs have different: {key}\")",
            "",
            "    if len(config1[\"components\"]) != len(config2[\"components\"]):",
            "        raise ValueError(\"# of components are different\")",
            "",
            "    def assert_same_components(config1_id, config2_id):",
            "        c1 = list(filter(lambda c: c[\"id\"] == config1_id, config1[\"components\"]))",
            "        if len(c1) == 0:",
            "            raise ValueError(f\"Could not find component with id {config1_id}\")",
            "        c1 = c1[0]",
            "        c2 = list(filter(lambda c: c[\"id\"] == config2_id, config2[\"components\"]))",
            "        if len(c2) == 0:",
            "            raise ValueError(f\"Could not find component with id {config2_id}\")",
            "        c2 = c2[0]",
            "        c1 = copy.deepcopy(c1)",
            "        c1.pop(\"id\")",
            "        c2 = copy.deepcopy(c2)",
            "        c2.pop(\"id\")",
            "        if c1 != c2:",
            "            raise ValueError(f\"{c1} does not match {c2}\")",
            "",
            "    def same_children_recursive(children1, chidren2):",
            "        for child1, child2 in zip(children1, chidren2):",
            "            assert_same_components(child1[\"id\"], child2[\"id\"])",
            "            if \"children\" in child1 or \"children\" in child2:",
            "                same_children_recursive(child1[\"children\"], child2[\"children\"])",
            "",
            "    children1 = config1[\"layout\"][\"children\"]",
            "    children2 = config2[\"layout\"][\"children\"]",
            "    same_children_recursive(children1, children2)",
            "",
            "    for d1, d2 in zip(config1[\"dependencies\"], config2[\"dependencies\"]):",
            "        for t1, t2 in zip(d1.pop(\"targets\"), d2.pop(\"targets\")):",
            "            assert_same_components(t1[0], t2[0])",
            "        for i1, i2 in zip(d1.pop(\"inputs\"), d2.pop(\"inputs\")):",
            "            assert_same_components(i1, i2)",
            "        for o1, o2 in zip(d1.pop(\"outputs\"), d2.pop(\"outputs\")):",
            "            assert_same_components(o1, o2)",
            "",
            "        if d1 != d2:",
            "            raise ValueError(f\"{d1} does not match {d2}\")",
            "",
            "    return True",
            "",
            "",
            "def delete_none(_dict: dict, skip_value: bool = False) -> dict:",
            "    \"\"\"",
            "    Delete keys whose values are None from a dictionary",
            "    \"\"\"",
            "    for key, value in list(_dict.items()):",
            "        if skip_value and key == \"value\":",
            "            continue",
            "        elif value is None:",
            "            del _dict[key]",
            "    return _dict",
            "",
            "",
            "def resolve_singleton(_list: list[Any] | Any) -> Any:",
            "    if len(_list) == 1:",
            "        return _list[0]",
            "    else:",
            "        return _list",
            "",
            "",
            "def component_or_layout_class(cls_name: str) -> type[Component] | type[BlockContext]:",
            "    \"\"\"",
            "    Returns the component, template, or layout class with the given class name, or",
            "    raises a ValueError if not found.",
            "",
            "    Parameters:",
            "    cls_name (str): lower-case string class name of a component",
            "    Returns:",
            "    cls: the component class",
            "    \"\"\"",
            "    import gradio.blocks",
            "    import gradio.components",
            "    import gradio.layouts",
            "    import gradio.templates",
            "",
            "    components = [",
            "        (name, cls)",
            "        for name, cls in gradio.components.__dict__.items()",
            "        if isinstance(cls, type)",
            "    ]",
            "    templates = [",
            "        (name, cls)",
            "        for name, cls in gradio.templates.__dict__.items()",
            "        if isinstance(cls, type)",
            "    ]",
            "    layouts = [",
            "        (name, cls)",
            "        for name, cls in gradio.layouts.__dict__.items()",
            "        if isinstance(cls, type)",
            "    ]",
            "    for name, cls in components + templates + layouts:",
            "        if name.lower() == cls_name.replace(\"_\", \"\") and (",
            "            issubclass(cls, gradio.components.Component)",
            "            or issubclass(cls, gradio.blocks.BlockContext)",
            "        ):",
            "            return cls",
            "    raise ValueError(f\"No such component or layout: {cls_name}\")",
            "",
            "",
            "def run_coro_in_background(func: Callable, *args, **kwargs):",
            "    \"\"\"",
            "    Runs coroutines in background.",
            "",
            "    Warning, be careful to not use this function in other than FastAPI scope, because the event_loop has not started yet.",
            "    You can use it in any scope reached by FastAPI app.",
            "",
            "    correct scope examples: endpoints in routes, Blocks.process_api",
            "    incorrect scope examples: Blocks.launch",
            "",
            "    Use startup_events in routes.py if you need to run a coro in background in Blocks.launch().",
            "",
            "",
            "    Example:",
            "        utils.run_coro_in_background(fn, *args, **kwargs)",
            "",
            "    Args:",
            "        func:",
            "        *args:",
            "        **kwargs:",
            "",
            "    Returns:",
            "",
            "    \"\"\"",
            "    event_loop = asyncio.get_event_loop()",
            "    return event_loop.create_task(func(*args, **kwargs))",
            "",
            "",
            "def run_sync_iterator_async(iterator):",
            "    \"\"\"Helper for yielding StopAsyncIteration from sync iterators.\"\"\"",
            "    try:",
            "        return next(iterator)",
            "    except StopIteration:",
            "        # raise a ValueError here because co-routines can't raise StopIteration themselves",
            "        raise StopAsyncIteration() from None",
            "",
            "",
            "class SyncToAsyncIterator:",
            "    \"\"\"Treat a synchronous iterator as async one.\"\"\"",
            "",
            "    def __init__(self, iterator, limiter) -> None:",
            "        self.iterator = iterator",
            "        self.limiter = limiter",
            "",
            "    def __aiter__(self):",
            "        return self",
            "",
            "    async def __anext__(self):",
            "        return await anyio.to_thread.run_sync(",
            "            run_sync_iterator_async, self.iterator, limiter=self.limiter",
            "        )",
            "",
            "",
            "async def async_iteration(iterator):",
            "    # anext not introduced until 3.10 :(",
            "    return await iterator.__anext__()",
            "",
            "",
            "@contextmanager",
            "def set_directory(path: Path | str):",
            "    \"\"\"Context manager that sets the working directory to the given path.\"\"\"",
            "    origin = Path().absolute()",
            "    try:",
            "        os.chdir(path)",
            "        yield",
            "    finally:",
            "        os.chdir(origin)",
            "",
            "",
            "@contextmanager",
            "def no_raise_exception():",
            "    \"\"\"Context manager that suppresses exceptions.\"\"\"",
            "    try:",
            "        yield",
            "    except Exception:",
            "        pass",
            "",
            "",
            "def sanitize_value_for_csv(value: str | Number) -> str | Number:",
            "    \"\"\"",
            "    Sanitizes a value that is being written to a CSV file to prevent CSV injection attacks.",
            "    Reference: https://owasp.org/www-community/attacks/CSV_Injection",
            "    \"\"\"",
            "    if isinstance(value, Number):",
            "        return value",
            "    unsafe_prefixes = [\"=\", \"+\", \"-\", \"@\", \"\\t\", \"\\n\"]",
            "    unsafe_sequences = [\",=\", \",+\", \",-\", \",@\", \",\\t\", \",\\n\"]",
            "    if any(value.startswith(prefix) for prefix in unsafe_prefixes) or any(",
            "        sequence in value for sequence in unsafe_sequences",
            "    ):",
            "        value = f\"'{value}\"",
            "    return value",
            "",
            "",
            "def sanitize_list_for_csv(values: list[Any]) -> list[Any]:",
            "    \"\"\"",
            "    Sanitizes a list of values (or a list of list of values) that is being written to a",
            "    CSV file to prevent CSV injection attacks.",
            "    \"\"\"",
            "    sanitized_values = []",
            "    for value in values:",
            "        if isinstance(value, list):",
            "            sanitized_value = [sanitize_value_for_csv(v) for v in value]",
            "            sanitized_values.append(sanitized_value)",
            "        else:",
            "            sanitized_value = sanitize_value_for_csv(value)",
            "            sanitized_values.append(sanitized_value)",
            "    return sanitized_values",
            "",
            "",
            "def append_unique_suffix(name: str, list_of_names: list[str]):",
            "    \"\"\"Appends a numerical suffix to `name` so that it does not appear in `list_of_names`.\"\"\"",
            "    set_of_names: set[str] = set(list_of_names)  # for O(1) lookup",
            "    if name not in set_of_names:",
            "        return name",
            "    else:",
            "        suffix_counter = 1",
            "        new_name = f\"{name}_{suffix_counter}\"",
            "        while new_name in set_of_names:",
            "            suffix_counter += 1",
            "            new_name = f\"{name}_{suffix_counter}\"",
            "        return new_name",
            "",
            "",
            "def validate_url(possible_url: str) -> bool:",
            "    headers = {\"User-Agent\": \"gradio (https://gradio.app/; gradio-team@huggingface.co)\"}",
            "    try:",
            "        head_request = httpx.head(possible_url, headers=headers, follow_redirects=True)",
            "        # some URLs, such as AWS S3 presigned URLs, return a 405 or a 403 for HEAD requests",
            "        if head_request.status_code in (403, 405):",
            "            return httpx.get(",
            "                possible_url, headers=headers, follow_redirects=True",
            "            ).is_success",
            "        return head_request.is_success",
            "    except Exception:",
            "        return False",
            "",
            "",
            "def is_update(val):",
            "    return isinstance(val, dict) and \"update\" in val.get(\"__type__\", \"\")",
            "",
            "",
            "def get_continuous_fn(fn: Callable, every: float) -> Callable:",
            "    # For Wasm-compatibility, we need to use asyncio.sleep() instead of time.sleep(),",
            "    # so we need to make the function async.",
            "    async def continuous_coro(*args):",
            "        while True:",
            "            output = fn(*args)",
            "            if isinstance(output, GeneratorType):",
            "                for item in output:",
            "                    yield item",
            "            elif isinstance(output, AsyncGeneratorType):",
            "                async for item in output:",
            "                    yield item",
            "            elif inspect.isawaitable(output):",
            "                yield await output",
            "            else:",
            "                yield output",
            "            await asyncio.sleep(every)",
            "",
            "    return continuous_coro",
            "",
            "",
            "def function_wrapper(",
            "    f: Callable,",
            "    before_fn: Callable | None = None,",
            "    before_args: Iterable | None = None,",
            "    after_fn: Callable | None = None,",
            "    after_args: Iterable | None = None,",
            "):",
            "    before_args = [] if before_args is None else before_args",
            "    after_args = [] if after_args is None else after_args",
            "    if inspect.isasyncgenfunction(f):",
            "",
            "        @functools.wraps(f)",
            "        async def asyncgen_wrapper(*args, **kwargs):",
            "            iterator = f(*args, **kwargs)",
            "            while True:",
            "                if before_fn:",
            "                    before_fn(*before_args)",
            "                try:",
            "                    response = await iterator.__anext__()",
            "                except StopAsyncIteration:",
            "                    if after_fn:",
            "                        after_fn(*after_args)",
            "                    break",
            "                if after_fn:",
            "                    after_fn(*after_args)",
            "                yield response",
            "",
            "        return asyncgen_wrapper",
            "",
            "    elif asyncio.iscoroutinefunction(f):",
            "",
            "        @functools.wraps(f)",
            "        async def async_wrapper(*args, **kwargs):",
            "            if before_fn:",
            "                before_fn(*before_args)",
            "            response = await f(*args, **kwargs)",
            "            if after_fn:",
            "                after_fn(*after_args)",
            "            return response",
            "",
            "        return async_wrapper",
            "",
            "    elif inspect.isgeneratorfunction(f):",
            "",
            "        @functools.wraps(f)",
            "        def gen_wrapper(*args, **kwargs):",
            "            iterator = f(*args, **kwargs)",
            "            while True:",
            "                if before_fn:",
            "                    before_fn(*before_args)",
            "                try:",
            "                    response = next(iterator)",
            "                except StopIteration:",
            "                    if after_fn:",
            "                        after_fn(*after_args)",
            "                    break",
            "                if after_fn:",
            "                    after_fn(*after_args)",
            "                yield response",
            "",
            "        return gen_wrapper",
            "",
            "    else:",
            "",
            "        @functools.wraps(f)",
            "        def wrapper(*args, **kwargs):",
            "            if before_fn:",
            "                before_fn(*before_args)",
            "            response = f(*args, **kwargs)",
            "            if after_fn:",
            "                after_fn(*after_args)",
            "            return response",
            "",
            "        return wrapper",
            "",
            "",
            "def get_function_with_locals(",
            "    fn: Callable,",
            "    blocks: Blocks,",
            "    event_id: str | None,",
            "    in_event_listener: bool,",
            "    request: Request | None,",
            "):",
            "    def before_fn(blocks, event_id):",
            "        from gradio.context import LocalContext",
            "",
            "        LocalContext.blocks.set(blocks)",
            "        LocalContext.in_event_listener.set(in_event_listener)",
            "        LocalContext.event_id.set(event_id)",
            "        LocalContext.request.set(request)",
            "",
            "    def after_fn():",
            "        from gradio.context import LocalContext",
            "",
            "        LocalContext.in_event_listener.set(False)",
            "        LocalContext.request.set(None)",
            "",
            "    return function_wrapper(",
            "        fn,",
            "        before_fn=before_fn,",
            "        before_args=(blocks, event_id),",
            "        after_fn=after_fn,",
            "    )",
            "",
            "",
            "async def cancel_tasks(task_ids: set[str]):",
            "    matching_tasks = [",
            "        task for task in asyncio.all_tasks() if task.get_name() in task_ids",
            "    ]",
            "    for task in matching_tasks:",
            "        task.cancel()",
            "    await asyncio.gather(*matching_tasks, return_exceptions=True)",
            "",
            "",
            "def set_task_name(task, session_hash: str, fn_index: int, batch: bool):",
            "    if not batch:",
            "        task.set_name(f\"{session_hash}_{fn_index}\")",
            "",
            "",
            "def get_cancel_function(",
            "    dependencies: list[dict[str, Any]],",
            ") -> tuple[Callable, list[int]]:",
            "    fn_to_comp = {}",
            "    for dep in dependencies:",
            "        if Context.root_block:",
            "            fn_index = next(",
            "                i for i, d in enumerate(Context.root_block.dependencies) if d == dep",
            "            )",
            "            fn_to_comp[fn_index] = [",
            "                Context.root_block.blocks[o] for o in dep[\"outputs\"]",
            "            ]",
            "",
            "    async def cancel(session_hash: str) -> None:",
            "        task_ids = {f\"{session_hash}_{fn}\" for fn in fn_to_comp}",
            "        await cancel_tasks(task_ids)",
            "",
            "    return (",
            "        cancel,",
            "        list(fn_to_comp.keys()),",
            "    )",
            "",
            "",
            "def get_type_hints(fn):",
            "    # Importing gradio with the canonical abbreviation. Used in typing._eval_type.",
            "    import gradio as gr  # noqa: F401",
            "    from gradio import OAuthProfile, OAuthToken, Request  # noqa: F401",
            "",
            "    if inspect.isfunction(fn) or inspect.ismethod(fn):",
            "        pass",
            "    elif callable(fn):",
            "        fn = fn.__call__",
            "    else:",
            "        return {}",
            "",
            "    try:",
            "        return typing.get_type_hints(fn)",
            "    except TypeError:",
            "        # On Python 3.9 or earlier, get_type_hints throws a TypeError if the function",
            "        # has a type annotation that include \"|\". We resort to parsing the signature",
            "        # manually using inspect.signature.",
            "        type_hints = {}",
            "        sig = inspect.signature(fn)",
            "        for name, param in sig.parameters.items():",
            "            if param.annotation is inspect.Parameter.empty:",
            "                continue",
            "            if param.annotation == \"gr.OAuthProfile | None\":",
            "                # Special case: we want to inject the OAuthProfile value even on Python 3.9",
            "                type_hints[name] = Optional[OAuthProfile]",
            "            if param.annotation == \"gr.OAuthToken | None\":",
            "                # Special case: we want to inject the OAuthToken value even on Python 3.9",
            "                type_hints[name] = Optional[OAuthToken]",
            "            if \"|\" in str(param.annotation):",
            "                continue",
            "            # To convert the string annotation to a class, we use the",
            "            # internal typing._eval_type function. This is not ideal, but",
            "            # it's the only way to do it without eval-ing the string.",
            "            # Since the API is internal, it may change in the future.",
            "            try:",
            "                type_hints[name] = typing._eval_type(  # type: ignore",
            "                    typing.ForwardRef(param.annotation), globals(), locals()",
            "                )",
            "            except (NameError, TypeError):",
            "                pass",
            "        return type_hints",
            "",
            "",
            "def is_special_typed_parameter(name, parameter_types):",
            "    from gradio.helpers import EventData",
            "    from gradio.oauth import OAuthProfile, OAuthToken",
            "    from gradio.routes import Request",
            "",
            "    \"\"\"Checks if parameter has a type hint designating it as a gr.Request, gr.EventData, gr.OAuthProfile or gr.OAuthToken.\"\"\"",
            "    hint = parameter_types.get(name)",
            "    if not hint:",
            "        return False",
            "    is_request = hint == Request",
            "    is_oauth_arg = hint in (",
            "        OAuthProfile,",
            "        Optional[OAuthProfile],",
            "        OAuthToken,",
            "        Optional[OAuthToken],",
            "    )",
            "    is_event_data = inspect.isclass(hint) and issubclass(hint, EventData)",
            "    return is_request or is_event_data or is_oauth_arg",
            "",
            "",
            "def check_function_inputs_match(fn: Callable, inputs: list, inputs_as_dict: bool):",
            "    \"\"\"",
            "    Checks if the input component set matches the function",
            "    Returns: None if valid or if the function does not have a signature (e.g. is a built in),",
            "    or a string error message if mismatch",
            "    \"\"\"",
            "    try:",
            "        signature = inspect.signature(fn)",
            "    except ValueError:",
            "        return None",
            "    parameter_types = get_type_hints(fn)",
            "    min_args = 0",
            "    max_args = 0",
            "    infinity = -1",
            "    for name, param in signature.parameters.items():",
            "        has_default = param.default != param.empty",
            "        if param.kind in [param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD]:",
            "            if not is_special_typed_parameter(name, parameter_types):",
            "                if not has_default:",
            "                    min_args += 1",
            "                max_args += 1",
            "        elif param.kind == param.VAR_POSITIONAL:",
            "            max_args = infinity",
            "        elif param.kind == param.KEYWORD_ONLY and not has_default:",
            "            return f\"Keyword-only args must have default values for function {fn}\"",
            "    arg_count = 1 if inputs_as_dict else len(inputs)",
            "    if min_args == max_args and max_args != arg_count:",
            "        warnings.warn(",
            "            f\"Expected {max_args} arguments for function {fn}, received {arg_count}.\"",
            "        )",
            "    if arg_count < min_args:",
            "        warnings.warn(",
            "            f\"Expected at least {min_args} arguments for function {fn}, received {arg_count}.\"",
            "        )",
            "    if max_args != infinity and arg_count > max_args:",
            "        warnings.warn(",
            "            f\"Expected maximum {max_args} arguments for function {fn}, received {arg_count}.\"",
            "        )",
            "",
            "",
            "class TupleNoPrint(tuple):",
            "    # To remove printing function return in notebook",
            "    def __repr__(self):",
            "        return \"\"",
            "",
            "    def __str__(self):",
            "        return \"\"",
            "",
            "",
            "class MatplotlibBackendMananger:",
            "    def __enter__(self):",
            "        import matplotlib",
            "",
            "        self._original_backend = matplotlib.get_backend()",
            "        matplotlib.use(\"agg\")",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        import matplotlib",
            "",
            "        matplotlib.use(self._original_backend)",
            "",
            "",
            "def tex2svg(formula, *_args):",
            "    with MatplotlibBackendMananger():",
            "        import matplotlib.pyplot as plt",
            "",
            "        fontsize = 20",
            "        dpi = 300",
            "        plt.rc(\"mathtext\", fontset=\"cm\")",
            "        fig = plt.figure(figsize=(0.01, 0.01))",
            "        fig.text(0, 0, rf\"${formula}$\", fontsize=fontsize)",
            "        output = BytesIO()",
            "        fig.savefig(  # type: ignore",
            "            output,",
            "            dpi=dpi,",
            "            transparent=True,",
            "            format=\"svg\",",
            "            bbox_inches=\"tight\",",
            "            pad_inches=0.0,",
            "        )",
            "        plt.close(fig)",
            "        output.seek(0)",
            "        xml_code = output.read().decode(\"utf-8\")",
            "        svg_start = xml_code.index(\"<svg \")",
            "        svg_code = xml_code[svg_start:]",
            "        svg_code = re.sub(r\"<metadata>.*<\\/metadata>\", \"\", svg_code, flags=re.DOTALL)",
            "        svg_code = re.sub(r' width=\"[^\"]+\"', \"\", svg_code)",
            "        height_match = re.search(r'height=\"([\\d.]+)pt\"', svg_code)",
            "        if height_match:",
            "            height = float(height_match.group(1))",
            "            new_height = height / fontsize  # conversion from pt to em",
            "            svg_code = re.sub(",
            "                r'height=\"[\\d.]+pt\"', f'height=\"{new_height}em\"', svg_code",
            "            )",
            "        copy_code = f\"<span style='font-size: 0px'>{formula}</span>\"",
            "    return f\"{copy_code}{svg_code}\"",
            "",
            "",
            "def abspath(path: str | Path) -> Path:",
            "    \"\"\"Returns absolute path of a str or Path path, but does not resolve symlinks.\"\"\"",
            "    path = Path(path)",
            "",
            "    if path.is_absolute():",
            "        return path",
            "",
            "    # recursively check if there is a symlink within the path",
            "    is_symlink = path.is_symlink() or any(",
            "        parent.is_symlink() for parent in path.parents",
            "    )",
            "",
            "    if is_symlink or path == path.resolve():  # in case path couldn't be resolved",
            "        return Path.cwd() / path",
            "    else:",
            "        return path.resolve()",
            "",
            "",
            "def is_in_or_equal(path_1: str | Path, path_2: str | Path):",
            "    \"\"\"",
            "    True if path_1 is a descendant (i.e. located within) path_2 or if the paths are the",
            "    same, returns False otherwise.",
            "    Parameters:",
            "        path_1: str or Path (should be a file)",
            "        path_2: str or Path (can be a file or directory)",
            "    \"\"\"",
            "    path_1, path_2 = abspath(path_1), abspath(path_2)",
            "    try:",
            "        if \"..\" in str(path_1.relative_to(path_2)):  # prevent path traversal",
            "            return False",
            "    except ValueError:",
            "        return False",
            "    return True",
            "",
            "",
            "HTML_TAG_RE = re.compile(\"<.*?>\")",
            "",
            "",
            "def remove_html_tags(raw_html: str | None) -> str:",
            "    return re.sub(HTML_TAG_RE, \"\", raw_html or \"\")",
            "",
            "",
            "def find_user_stack_level() -> int:",
            "    \"\"\"",
            "    Find the first stack frame not inside Gradio.",
            "    \"\"\"",
            "    frame = inspect.currentframe()",
            "    n = 0",
            "    while frame:",
            "        fname = inspect.getfile(frame)",
            "        if \"/gradio/\" not in fname.replace(os.sep, \"/\"):",
            "            break",
            "        frame = frame.f_back",
            "        n += 1",
            "    return n",
            "",
            "",
            "class NamedString(str):",
            "    \"\"\"",
            "    Subclass of str that includes a .name attribute equal to the value of the string itself. This class is used when returning",
            "    a value from the `.preprocess()` methods of the File and UploadButton components. Before Gradio 4.0, these methods returned a file",
            "    object which was then converted to a string filepath using the `.name` attribute. In Gradio 4.0, these methods now return a str",
            "    filepath directly, but to maintain backwards compatibility, we use this class instead of a regular str.",
            "    \"\"\"",
            "",
            "    def __init__(self, *args):",
            "        super().__init__()",
            "        self.name = str(self) if args else \"\"",
            "",
            "",
            "def default_input_labels():",
            "    \"\"\"",
            "    A generator that provides default input labels for components when the user's function",
            "    does not have named parameters. The labels are of the form \"input 0\", \"input 1\", etc.",
            "    \"\"\"",
            "    n = 0",
            "    while True:",
            "        yield f\"input {n}\"",
            "        n += 1",
            "",
            "",
            "def get_extension_from_file_path_or_url(file_path_or_url: str) -> str:",
            "    \"\"\"",
            "    Returns the file extension (without the dot) from a file path or URL. If the file path or URL does not have a file extension, returns an empty string.",
            "    For example, \"https://example.com/avatar/xxxx.mp4?se=2023-11-16T06:51:23Z&sp=r\" would return \"mp4\".",
            "    \"\"\"",
            "    parsed_url = urllib.parse.urlparse(file_path_or_url)",
            "    file_extension = os.path.splitext(os.path.basename(parsed_url.path))[1]",
            "    return file_extension[1:] if file_extension else \"\"",
            "",
            "",
            "def convert_to_dict_if_dataclass(value):",
            "    if dataclasses.is_dataclass(value):",
            "        return dataclasses.asdict(value)",
            "    return value",
            "",
            "",
            "K = TypeVar(\"K\")",
            "V = TypeVar(\"V\")",
            "",
            "",
            "class LRUCache(OrderedDict, Generic[K, V]):",
            "    def __init__(self, max_size: int = 100):",
            "        super().__init__()",
            "        self.max_size: int = max_size",
            "",
            "    def __setitem__(self, key: K, value: V) -> None:",
            "        if key in self:",
            "            self.move_to_end(key)",
            "        elif len(self) >= self.max_size:",
            "            self.popitem(last=False)",
            "        super().__setitem__(key, value)",
            "",
            "",
            "def get_cache_folder() -> Path:",
            "    return Path(os.environ.get(\"GRADIO_EXAMPLES_CACHE\", \"gradio_cached_examples\"))",
            "",
            "",
            "def diff(old, new):",
            "    def compare_objects(obj1, obj2, path=None):",
            "        if path is None:",
            "            path = []",
            "        edits = []",
            "",
            "        if obj1 == obj2:",
            "            return edits",
            "",
            "        if type(obj1) != type(obj2):",
            "            edits.append((\"replace\", path, obj2))",
            "            return edits",
            "",
            "        if isinstance(obj1, str) and obj2.startswith(obj1):",
            "            edits.append((\"append\", path, obj2[len(obj1) :]))",
            "            return edits",
            "",
            "        if isinstance(obj1, list):",
            "            common_length = min(len(obj1), len(obj2))",
            "            for i in range(common_length):",
            "                edits.extend(compare_objects(obj1[i], obj2[i], path + [i]))",
            "            for i in range(common_length, len(obj1)):",
            "                edits.append((\"delete\", path + [i], None))",
            "            for i in range(common_length, len(obj2)):",
            "                edits.append((\"add\", path + [i], obj2[i]))",
            "            return edits",
            "",
            "        if isinstance(obj1, dict):",
            "            for key in obj1:",
            "                if key in obj2:",
            "                    edits.extend(compare_objects(obj1[key], obj2[key], path + [key]))",
            "                else:",
            "                    edits.append((\"delete\", path + [key], None))",
            "            for key in obj2:",
            "                if key not in obj1:",
            "                    edits.append((\"add\", path + [key], obj2[key]))",
            "            return edits",
            "",
            "        edits.append((\"replace\", path, obj2))",
            "        return edits",
            "",
            "    return compare_objects(old, new)"
        ],
        "afterPatchFile": [
            "\"\"\" Handy utility functions. \"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import asyncio",
            "import copy",
            "import dataclasses",
            "import functools",
            "import importlib",
            "import inspect",
            "import json",
            "import json.decoder",
            "import os",
            "import pkgutil",
            "import re",
            "import tempfile",
            "import threading",
            "import time",
            "import traceback",
            "import typing",
            "import urllib.parse",
            "import warnings",
            "from abc import ABC, abstractmethod",
            "from collections import OrderedDict",
            "from contextlib import contextmanager",
            "from io import BytesIO",
            "from numbers import Number",
            "from pathlib import Path",
            "from types import AsyncGeneratorType, GeneratorType",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Callable,",
            "    Generic,",
            "    Iterable,",
            "    Iterator,",
            "    Optional,",
            "    TypeVar,",
            ")",
            "",
            "import anyio",
            "import httpx",
            "from typing_extensions import ParamSpec",
            "",
            "import gradio",
            "from gradio.context import Context",
            "from gradio.strings import en",
            "",
            "if TYPE_CHECKING:  # Only import for type checking (is False at runtime).",
            "    from gradio.blocks import BlockContext, Blocks",
            "    from gradio.components import Component",
            "    from gradio.routes import App, Request",
            "",
            "JSON_PATH = os.path.join(os.path.dirname(gradio.__file__), \"launches.json\")",
            "",
            "P = ParamSpec(\"P\")",
            "T = TypeVar(\"T\")",
            "",
            "",
            "def get_package_version() -> str:",
            "    try:",
            "        package_json_data = (",
            "            pkgutil.get_data(__name__, \"package.json\").decode(\"utf-8\").strip()  # type: ignore",
            "        )",
            "        package_data = json.loads(package_json_data)",
            "        version = package_data.get(\"version\", \"\")",
            "        return version",
            "    except Exception:",
            "        return \"\"",
            "",
            "",
            "def safe_get_lock() -> asyncio.Lock:",
            "    \"\"\"Get asyncio.Lock() without fear of getting an Exception.",
            "",
            "    Needed because in reload mode we import the Blocks object outside",
            "    the main thread.",
            "    \"\"\"",
            "    try:",
            "        asyncio.get_event_loop()",
            "        return asyncio.Lock()",
            "    except RuntimeError:",
            "        return None  # type: ignore",
            "",
            "",
            "class BaseReloader(ABC):",
            "    @property",
            "    @abstractmethod",
            "    def running_app(self) -> App:",
            "        pass",
            "",
            "    def queue_changed(self, demo: Blocks):",
            "        return (",
            "            hasattr(self.running_app.blocks, \"_queue\") and not hasattr(demo, \"_queue\")",
            "        ) or (",
            "            not hasattr(self.running_app.blocks, \"_queue\") and hasattr(demo, \"_queue\")",
            "        )",
            "",
            "    def swap_blocks(self, demo: Blocks):",
            "        assert self.running_app.blocks  # noqa: S101",
            "        # Copy over the blocks to get new components and events but",
            "        # not a new queue",
            "        self.running_app.blocks._queue.block_fns = demo.fns",
            "        demo._queue = self.running_app.blocks._queue",
            "        self.running_app.blocks = demo",
            "        demo._queue.reload()",
            "",
            "",
            "class SourceFileReloader(BaseReloader):",
            "    def __init__(",
            "        self,",
            "        app: App,",
            "        watch_dirs: list[str],",
            "        watch_module_name: str,",
            "        stop_event: threading.Event,",
            "        change_event: threading.Event,",
            "        demo_name: str = \"demo\",",
            "    ) -> None:",
            "        super().__init__()",
            "        self.app = app",
            "        self.watch_dirs = watch_dirs",
            "        self.watch_module_name = watch_module_name",
            "        self.stop_event = stop_event",
            "        self.change_event = change_event",
            "        self.demo_name = demo_name",
            "",
            "    @property",
            "    def running_app(self) -> App:",
            "        return self.app",
            "",
            "    def should_watch(self) -> bool:",
            "        return not self.stop_event.is_set()",
            "",
            "    def stop(self) -> None:",
            "        self.stop_event.set()",
            "",
            "    def alert_change(self):",
            "        self.change_event.set()",
            "",
            "    def swap_blocks(self, demo: Blocks):",
            "        super().swap_blocks(demo)",
            "        self.alert_change()",
            "",
            "",
            "def watchfn(reloader: SourceFileReloader):",
            "    \"\"\"Watch python files in a given module.",
            "",
            "    get_changes is taken from uvicorn's default file watcher.",
            "    \"\"\"",
            "",
            "    # The thread running watchfn will be the thread reloading",
            "    # the app. So we need to modify this thread_data attr here",
            "    # so that subsequent calls to reload don't launch the app",
            "    from gradio.cli.commands.reload import reload_thread",
            "",
            "    reload_thread.running_reload = True",
            "",
            "    def get_changes() -> Path | None:",
            "        for file in iter_py_files():",
            "            try:",
            "                mtime = file.stat().st_mtime",
            "            except OSError:  # pragma: nocover",
            "                continue",
            "",
            "            old_time = mtimes.get(file)",
            "            if old_time is None:",
            "                mtimes[file] = mtime",
            "                continue",
            "            elif mtime > old_time:",
            "                return file",
            "        return None",
            "",
            "    def iter_py_files() -> Iterator[Path]:",
            "        for reload_dir in reload_dirs:",
            "            for path in list(reload_dir.rglob(\"*.py\")):",
            "                yield path.resolve()",
            "            for path in list(reload_dir.rglob(\"*.css\")):",
            "                yield path.resolve()",
            "",
            "    module = None",
            "    reload_dirs = [Path(dir_) for dir_ in reloader.watch_dirs]",
            "    import sys",
            "",
            "    for dir_ in reload_dirs:",
            "        sys.path.insert(0, str(dir_))",
            "",
            "    mtimes = {}",
            "    while reloader.should_watch():",
            "        changed = get_changes()",
            "        if changed:",
            "            print(f\"Changes detected in: {changed}\")",
            "            # To simulate a fresh reload, delete all module references from sys.modules",
            "            # for the modules in the package the change came from.",
            "            dir_ = next(d for d in reload_dirs if is_in_or_equal(changed, d))",
            "            modules = list(sys.modules)",
            "            for k in modules:",
            "                v = sys.modules[k]",
            "                sourcefile = getattr(v, \"__file__\", None)",
            "                # Do not reload `reload.py` to keep thread data",
            "                if (",
            "                    sourcefile",
            "                    and dir_ == Path(inspect.getfile(gradio)).parent",
            "                    and sourcefile.endswith(\"reload.py\")",
            "                ):",
            "                    continue",
            "                if sourcefile and is_in_or_equal(sourcefile, dir_):",
            "                    del sys.modules[k]",
            "            try:",
            "                module = importlib.import_module(reloader.watch_module_name)",
            "                module = importlib.reload(module)",
            "            except Exception:",
            "                print(",
            "                    f\"Reloading {reloader.watch_module_name} failed with the following exception: \"",
            "                )",
            "                traceback.print_exc()",
            "                mtimes = {}",
            "                continue",
            "",
            "            demo = getattr(module, reloader.demo_name)",
            "            if reloader.queue_changed(demo):",
            "                print(",
            "                    \"Reloading failed. The new demo has a queue and the old one doesn't (or vice versa). \"",
            "                    \"Please launch your demo again\"",
            "                )",
            "            else:",
            "                reloader.swap_blocks(demo)",
            "            mtimes = {}",
            "        time.sleep(0.05)",
            "",
            "",
            "def colab_check() -> bool:",
            "    \"\"\"",
            "    Check if interface is launching from Google Colab",
            "    :return is_colab (bool): True or False",
            "    \"\"\"",
            "    is_colab = False",
            "    try:  # Check if running interactively using ipython.",
            "        from IPython.core.getipython import get_ipython",
            "",
            "        from_ipynb = get_ipython()",
            "        if \"google.colab\" in str(from_ipynb):",
            "            is_colab = True",
            "    except (ImportError, NameError):",
            "        pass",
            "    return is_colab",
            "",
            "",
            "def kaggle_check() -> bool:",
            "    return bool(",
            "        os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\") or os.environ.get(\"GFOOTBALL_DATA_DIR\")",
            "    )",
            "",
            "",
            "def sagemaker_check() -> bool:",
            "    try:",
            "        import boto3  # type: ignore",
            "",
            "        client = boto3.client(\"sts\")",
            "        response = client.get_caller_identity()",
            "        return \"sagemaker\" in response[\"Arn\"].lower()",
            "    except Exception:",
            "        return False",
            "",
            "",
            "def ipython_check() -> bool:",
            "    \"\"\"",
            "    Check if interface is launching from iPython (not colab)",
            "    :return is_ipython (bool): True or False",
            "    \"\"\"",
            "    is_ipython = False",
            "    try:  # Check if running interactively using ipython.",
            "        from IPython.core.getipython import get_ipython",
            "",
            "        if get_ipython() is not None:",
            "            is_ipython = True",
            "    except (ImportError, NameError):",
            "        pass",
            "    return is_ipython",
            "",
            "",
            "def get_space() -> str | None:",
            "    if os.getenv(\"SYSTEM\") == \"spaces\":",
            "        return os.getenv(\"SPACE_ID\")",
            "    return None",
            "",
            "",
            "def is_zero_gpu_space() -> bool:",
            "    return os.getenv(\"SPACES_ZERO_GPU\") == \"true\"",
            "",
            "",
            "def download_if_url(article: str) -> str:",
            "    try:",
            "        result = urllib.parse.urlparse(article)",
            "        is_url = all([result.scheme, result.netloc, result.path])",
            "        is_url = is_url and result.scheme in [\"http\", \"https\"]",
            "    except ValueError:",
            "        is_url = False",
            "",
            "    if not is_url:",
            "        return article",
            "",
            "    try:",
            "        response = httpx.get(article, timeout=3)",
            "        if response.status_code == httpx.codes.OK:  # pylint: disable=no-member",
            "            article = response.text",
            "    except (httpx.InvalidURL, httpx.RequestError):",
            "        pass",
            "",
            "    return article",
            "",
            "",
            "def launch_counter() -> None:",
            "    try:",
            "        if not os.path.exists(JSON_PATH):",
            "            launches = {\"launches\": 1}",
            "            with open(JSON_PATH, \"w+\") as j:",
            "                json.dump(launches, j)",
            "        else:",
            "            with open(JSON_PATH) as j:",
            "                launches = json.load(j)",
            "            launches[\"launches\"] += 1",
            "            if launches[\"launches\"] in [25, 50, 150, 500, 1000]:",
            "                print(en[\"BETA_INVITE\"])",
            "            with open(JSON_PATH, \"w\") as j:",
            "                j.write(json.dumps(launches))",
            "    except Exception:",
            "        pass",
            "",
            "",
            "def get_default_args(func: Callable) -> list[Any]:",
            "    signature = inspect.signature(func)",
            "    return [",
            "        v.default if v.default is not inspect.Parameter.empty else None",
            "        for v in signature.parameters.values()",
            "    ]",
            "",
            "",
            "def assert_configs_are_equivalent_besides_ids(",
            "    config1: dict, config2: dict, root_keys: tuple = (\"mode\",)",
            "):",
            "    \"\"\"Allows you to test if two different Blocks configs produce the same demo.",
            "",
            "    Parameters:",
            "    config1 (dict): nested dict with config from the first Blocks instance",
            "    config2 (dict): nested dict with config from the second Blocks instance",
            "    root_keys (Tuple): an interable consisting of which keys to test for equivalence at",
            "        the root level of the config. By default, only \"mode\" is tested,",
            "        so keys like \"version\" are ignored.",
            "    \"\"\"",
            "    config1 = copy.deepcopy(config1)",
            "    config2 = copy.deepcopy(config2)",
            "    config1 = json.loads(json.dumps(config1))  # convert tuples to lists",
            "    config2 = json.loads(json.dumps(config2))",
            "",
            "    for key in root_keys:",
            "        if config1[key] != config2[key]:",
            "            raise ValueError(f\"Configs have different: {key}\")",
            "",
            "    if len(config1[\"components\"]) != len(config2[\"components\"]):",
            "        raise ValueError(\"# of components are different\")",
            "",
            "    def assert_same_components(config1_id, config2_id):",
            "        c1 = list(filter(lambda c: c[\"id\"] == config1_id, config1[\"components\"]))",
            "        if len(c1) == 0:",
            "            raise ValueError(f\"Could not find component with id {config1_id}\")",
            "        c1 = c1[0]",
            "        c2 = list(filter(lambda c: c[\"id\"] == config2_id, config2[\"components\"]))",
            "        if len(c2) == 0:",
            "            raise ValueError(f\"Could not find component with id {config2_id}\")",
            "        c2 = c2[0]",
            "        c1 = copy.deepcopy(c1)",
            "        c1.pop(\"id\")",
            "        c2 = copy.deepcopy(c2)",
            "        c2.pop(\"id\")",
            "        if c1 != c2:",
            "            raise ValueError(f\"{c1} does not match {c2}\")",
            "",
            "    def same_children_recursive(children1, chidren2):",
            "        for child1, child2 in zip(children1, chidren2):",
            "            assert_same_components(child1[\"id\"], child2[\"id\"])",
            "            if \"children\" in child1 or \"children\" in child2:",
            "                same_children_recursive(child1[\"children\"], child2[\"children\"])",
            "",
            "    children1 = config1[\"layout\"][\"children\"]",
            "    children2 = config2[\"layout\"][\"children\"]",
            "    same_children_recursive(children1, children2)",
            "",
            "    for d1, d2 in zip(config1[\"dependencies\"], config2[\"dependencies\"]):",
            "        for t1, t2 in zip(d1.pop(\"targets\"), d2.pop(\"targets\")):",
            "            assert_same_components(t1[0], t2[0])",
            "        for i1, i2 in zip(d1.pop(\"inputs\"), d2.pop(\"inputs\")):",
            "            assert_same_components(i1, i2)",
            "        for o1, o2 in zip(d1.pop(\"outputs\"), d2.pop(\"outputs\")):",
            "            assert_same_components(o1, o2)",
            "",
            "        if d1 != d2:",
            "            raise ValueError(f\"{d1} does not match {d2}\")",
            "",
            "    return True",
            "",
            "",
            "def delete_none(_dict: dict, skip_value: bool = False) -> dict:",
            "    \"\"\"",
            "    Delete keys whose values are None from a dictionary",
            "    \"\"\"",
            "    for key, value in list(_dict.items()):",
            "        if skip_value and key == \"value\":",
            "            continue",
            "        elif value is None:",
            "            del _dict[key]",
            "    return _dict",
            "",
            "",
            "def resolve_singleton(_list: list[Any] | Any) -> Any:",
            "    if len(_list) == 1:",
            "        return _list[0]",
            "    else:",
            "        return _list",
            "",
            "",
            "def component_or_layout_class(cls_name: str) -> type[Component] | type[BlockContext]:",
            "    \"\"\"",
            "    Returns the component, template, or layout class with the given class name, or",
            "    raises a ValueError if not found.",
            "",
            "    Parameters:",
            "    cls_name (str): lower-case string class name of a component",
            "    Returns:",
            "    cls: the component class",
            "    \"\"\"",
            "    import gradio.blocks",
            "    import gradio.components",
            "    import gradio.layouts",
            "    import gradio.templates",
            "",
            "    components = [",
            "        (name, cls)",
            "        for name, cls in gradio.components.__dict__.items()",
            "        if isinstance(cls, type)",
            "    ]",
            "    templates = [",
            "        (name, cls)",
            "        for name, cls in gradio.templates.__dict__.items()",
            "        if isinstance(cls, type)",
            "    ]",
            "    layouts = [",
            "        (name, cls)",
            "        for name, cls in gradio.layouts.__dict__.items()",
            "        if isinstance(cls, type)",
            "    ]",
            "    for name, cls in components + templates + layouts:",
            "        if name.lower() == cls_name.replace(\"_\", \"\") and (",
            "            issubclass(cls, gradio.components.Component)",
            "            or issubclass(cls, gradio.blocks.BlockContext)",
            "        ):",
            "            return cls",
            "    raise ValueError(f\"No such component or layout: {cls_name}\")",
            "",
            "",
            "def run_coro_in_background(func: Callable, *args, **kwargs):",
            "    \"\"\"",
            "    Runs coroutines in background.",
            "",
            "    Warning, be careful to not use this function in other than FastAPI scope, because the event_loop has not started yet.",
            "    You can use it in any scope reached by FastAPI app.",
            "",
            "    correct scope examples: endpoints in routes, Blocks.process_api",
            "    incorrect scope examples: Blocks.launch",
            "",
            "    Use startup_events in routes.py if you need to run a coro in background in Blocks.launch().",
            "",
            "",
            "    Example:",
            "        utils.run_coro_in_background(fn, *args, **kwargs)",
            "",
            "    Args:",
            "        func:",
            "        *args:",
            "        **kwargs:",
            "",
            "    Returns:",
            "",
            "    \"\"\"",
            "    event_loop = asyncio.get_event_loop()",
            "    return event_loop.create_task(func(*args, **kwargs))",
            "",
            "",
            "def run_sync_iterator_async(iterator):",
            "    \"\"\"Helper for yielding StopAsyncIteration from sync iterators.\"\"\"",
            "    try:",
            "        return next(iterator)",
            "    except StopIteration:",
            "        # raise a ValueError here because co-routines can't raise StopIteration themselves",
            "        raise StopAsyncIteration() from None",
            "",
            "",
            "class SyncToAsyncIterator:",
            "    \"\"\"Treat a synchronous iterator as async one.\"\"\"",
            "",
            "    def __init__(self, iterator, limiter) -> None:",
            "        self.iterator = iterator",
            "        self.limiter = limiter",
            "",
            "    def __aiter__(self):",
            "        return self",
            "",
            "    async def __anext__(self):",
            "        return await anyio.to_thread.run_sync(",
            "            run_sync_iterator_async, self.iterator, limiter=self.limiter",
            "        )",
            "",
            "",
            "async def async_iteration(iterator):",
            "    # anext not introduced until 3.10 :(",
            "    return await iterator.__anext__()",
            "",
            "",
            "@contextmanager",
            "def set_directory(path: Path | str):",
            "    \"\"\"Context manager that sets the working directory to the given path.\"\"\"",
            "    origin = Path().absolute()",
            "    try:",
            "        os.chdir(path)",
            "        yield",
            "    finally:",
            "        os.chdir(origin)",
            "",
            "",
            "@contextmanager",
            "def no_raise_exception():",
            "    \"\"\"Context manager that suppresses exceptions.\"\"\"",
            "    try:",
            "        yield",
            "    except Exception:",
            "        pass",
            "",
            "",
            "def sanitize_value_for_csv(value: str | Number) -> str | Number:",
            "    \"\"\"",
            "    Sanitizes a value that is being written to a CSV file to prevent CSV injection attacks.",
            "    Reference: https://owasp.org/www-community/attacks/CSV_Injection",
            "    \"\"\"",
            "    if isinstance(value, Number):",
            "        return value",
            "    unsafe_prefixes = [\"=\", \"+\", \"-\", \"@\", \"\\t\", \"\\n\"]",
            "    unsafe_sequences = [\",=\", \",+\", \",-\", \",@\", \",\\t\", \",\\n\"]",
            "    if any(value.startswith(prefix) for prefix in unsafe_prefixes) or any(",
            "        sequence in value for sequence in unsafe_sequences",
            "    ):",
            "        value = f\"'{value}\"",
            "    return value",
            "",
            "",
            "def sanitize_list_for_csv(values: list[Any]) -> list[Any]:",
            "    \"\"\"",
            "    Sanitizes a list of values (or a list of list of values) that is being written to a",
            "    CSV file to prevent CSV injection attacks.",
            "    \"\"\"",
            "    sanitized_values = []",
            "    for value in values:",
            "        if isinstance(value, list):",
            "            sanitized_value = [sanitize_value_for_csv(v) for v in value]",
            "            sanitized_values.append(sanitized_value)",
            "        else:",
            "            sanitized_value = sanitize_value_for_csv(value)",
            "            sanitized_values.append(sanitized_value)",
            "    return sanitized_values",
            "",
            "",
            "def append_unique_suffix(name: str, list_of_names: list[str]):",
            "    \"\"\"Appends a numerical suffix to `name` so that it does not appear in `list_of_names`.\"\"\"",
            "    set_of_names: set[str] = set(list_of_names)  # for O(1) lookup",
            "    if name not in set_of_names:",
            "        return name",
            "    else:",
            "        suffix_counter = 1",
            "        new_name = f\"{name}_{suffix_counter}\"",
            "        while new_name in set_of_names:",
            "            suffix_counter += 1",
            "            new_name = f\"{name}_{suffix_counter}\"",
            "        return new_name",
            "",
            "",
            "def validate_url(possible_url: str) -> bool:",
            "    headers = {\"User-Agent\": \"gradio (https://gradio.app/; gradio-team@huggingface.co)\"}",
            "    try:",
            "        head_request = httpx.head(possible_url, headers=headers, follow_redirects=True)",
            "        # some URLs, such as AWS S3 presigned URLs, return a 405 or a 403 for HEAD requests",
            "        if head_request.status_code in (403, 405):",
            "            return httpx.get(",
            "                possible_url, headers=headers, follow_redirects=True",
            "            ).is_success",
            "        return head_request.is_success",
            "    except Exception:",
            "        return False",
            "",
            "",
            "def is_update(val):",
            "    return isinstance(val, dict) and \"update\" in val.get(\"__type__\", \"\")",
            "",
            "",
            "def get_continuous_fn(fn: Callable, every: float) -> Callable:",
            "    # For Wasm-compatibility, we need to use asyncio.sleep() instead of time.sleep(),",
            "    # so we need to make the function async.",
            "    async def continuous_coro(*args):",
            "        while True:",
            "            output = fn(*args)",
            "            if isinstance(output, GeneratorType):",
            "                for item in output:",
            "                    yield item",
            "            elif isinstance(output, AsyncGeneratorType):",
            "                async for item in output:",
            "                    yield item",
            "            elif inspect.isawaitable(output):",
            "                yield await output",
            "            else:",
            "                yield output",
            "            await asyncio.sleep(every)",
            "",
            "    return continuous_coro",
            "",
            "",
            "def function_wrapper(",
            "    f: Callable,",
            "    before_fn: Callable | None = None,",
            "    before_args: Iterable | None = None,",
            "    after_fn: Callable | None = None,",
            "    after_args: Iterable | None = None,",
            "):",
            "    before_args = [] if before_args is None else before_args",
            "    after_args = [] if after_args is None else after_args",
            "    if inspect.isasyncgenfunction(f):",
            "",
            "        @functools.wraps(f)",
            "        async def asyncgen_wrapper(*args, **kwargs):",
            "            iterator = f(*args, **kwargs)",
            "            while True:",
            "                if before_fn:",
            "                    before_fn(*before_args)",
            "                try:",
            "                    response = await iterator.__anext__()",
            "                except StopAsyncIteration:",
            "                    if after_fn:",
            "                        after_fn(*after_args)",
            "                    break",
            "                if after_fn:",
            "                    after_fn(*after_args)",
            "                yield response",
            "",
            "        return asyncgen_wrapper",
            "",
            "    elif asyncio.iscoroutinefunction(f):",
            "",
            "        @functools.wraps(f)",
            "        async def async_wrapper(*args, **kwargs):",
            "            if before_fn:",
            "                before_fn(*before_args)",
            "            response = await f(*args, **kwargs)",
            "            if after_fn:",
            "                after_fn(*after_args)",
            "            return response",
            "",
            "        return async_wrapper",
            "",
            "    elif inspect.isgeneratorfunction(f):",
            "",
            "        @functools.wraps(f)",
            "        def gen_wrapper(*args, **kwargs):",
            "            iterator = f(*args, **kwargs)",
            "            while True:",
            "                if before_fn:",
            "                    before_fn(*before_args)",
            "                try:",
            "                    response = next(iterator)",
            "                except StopIteration:",
            "                    if after_fn:",
            "                        after_fn(*after_args)",
            "                    break",
            "                if after_fn:",
            "                    after_fn(*after_args)",
            "                yield response",
            "",
            "        return gen_wrapper",
            "",
            "    else:",
            "",
            "        @functools.wraps(f)",
            "        def wrapper(*args, **kwargs):",
            "            if before_fn:",
            "                before_fn(*before_args)",
            "            response = f(*args, **kwargs)",
            "            if after_fn:",
            "                after_fn(*after_args)",
            "            return response",
            "",
            "        return wrapper",
            "",
            "",
            "def get_function_with_locals(",
            "    fn: Callable,",
            "    blocks: Blocks,",
            "    event_id: str | None,",
            "    in_event_listener: bool,",
            "    request: Request | None,",
            "):",
            "    def before_fn(blocks, event_id):",
            "        from gradio.context import LocalContext",
            "",
            "        LocalContext.blocks.set(blocks)",
            "        LocalContext.in_event_listener.set(in_event_listener)",
            "        LocalContext.event_id.set(event_id)",
            "        LocalContext.request.set(request)",
            "",
            "    def after_fn():",
            "        from gradio.context import LocalContext",
            "",
            "        LocalContext.in_event_listener.set(False)",
            "        LocalContext.request.set(None)",
            "",
            "    return function_wrapper(",
            "        fn,",
            "        before_fn=before_fn,",
            "        before_args=(blocks, event_id),",
            "        after_fn=after_fn,",
            "    )",
            "",
            "",
            "async def cancel_tasks(task_ids: set[str]):",
            "    matching_tasks = [",
            "        task for task in asyncio.all_tasks() if task.get_name() in task_ids",
            "    ]",
            "    for task in matching_tasks:",
            "        task.cancel()",
            "    await asyncio.gather(*matching_tasks, return_exceptions=True)",
            "",
            "",
            "def set_task_name(task, session_hash: str, fn_index: int, batch: bool):",
            "    if not batch:",
            "        task.set_name(f\"{session_hash}_{fn_index}\")",
            "",
            "",
            "def get_cancel_function(",
            "    dependencies: list[dict[str, Any]],",
            ") -> tuple[Callable, list[int]]:",
            "    fn_to_comp = {}",
            "    for dep in dependencies:",
            "        if Context.root_block:",
            "            fn_index = next(",
            "                i for i, d in enumerate(Context.root_block.dependencies) if d == dep",
            "            )",
            "            fn_to_comp[fn_index] = [",
            "                Context.root_block.blocks[o] for o in dep[\"outputs\"]",
            "            ]",
            "",
            "    async def cancel(session_hash: str) -> None:",
            "        task_ids = {f\"{session_hash}_{fn}\" for fn in fn_to_comp}",
            "        await cancel_tasks(task_ids)",
            "",
            "    return (",
            "        cancel,",
            "        list(fn_to_comp.keys()),",
            "    )",
            "",
            "",
            "def get_type_hints(fn):",
            "    # Importing gradio with the canonical abbreviation. Used in typing._eval_type.",
            "    import gradio as gr  # noqa: F401",
            "    from gradio import OAuthProfile, OAuthToken, Request  # noqa: F401",
            "",
            "    if inspect.isfunction(fn) or inspect.ismethod(fn):",
            "        pass",
            "    elif callable(fn):",
            "        fn = fn.__call__",
            "    else:",
            "        return {}",
            "",
            "    try:",
            "        return typing.get_type_hints(fn)",
            "    except TypeError:",
            "        # On Python 3.9 or earlier, get_type_hints throws a TypeError if the function",
            "        # has a type annotation that include \"|\". We resort to parsing the signature",
            "        # manually using inspect.signature.",
            "        type_hints = {}",
            "        sig = inspect.signature(fn)",
            "        for name, param in sig.parameters.items():",
            "            if param.annotation is inspect.Parameter.empty:",
            "                continue",
            "            if param.annotation == \"gr.OAuthProfile | None\":",
            "                # Special case: we want to inject the OAuthProfile value even on Python 3.9",
            "                type_hints[name] = Optional[OAuthProfile]",
            "            if param.annotation == \"gr.OAuthToken | None\":",
            "                # Special case: we want to inject the OAuthToken value even on Python 3.9",
            "                type_hints[name] = Optional[OAuthToken]",
            "            if \"|\" in str(param.annotation):",
            "                continue",
            "            # To convert the string annotation to a class, we use the",
            "            # internal typing._eval_type function. This is not ideal, but",
            "            # it's the only way to do it without eval-ing the string.",
            "            # Since the API is internal, it may change in the future.",
            "            try:",
            "                type_hints[name] = typing._eval_type(  # type: ignore",
            "                    typing.ForwardRef(param.annotation), globals(), locals()",
            "                )",
            "            except (NameError, TypeError):",
            "                pass",
            "        return type_hints",
            "",
            "",
            "def is_special_typed_parameter(name, parameter_types):",
            "    from gradio.helpers import EventData",
            "    from gradio.oauth import OAuthProfile, OAuthToken",
            "    from gradio.routes import Request",
            "",
            "    \"\"\"Checks if parameter has a type hint designating it as a gr.Request, gr.EventData, gr.OAuthProfile or gr.OAuthToken.\"\"\"",
            "    hint = parameter_types.get(name)",
            "    if not hint:",
            "        return False",
            "    is_request = hint == Request",
            "    is_oauth_arg = hint in (",
            "        OAuthProfile,",
            "        Optional[OAuthProfile],",
            "        OAuthToken,",
            "        Optional[OAuthToken],",
            "    )",
            "    is_event_data = inspect.isclass(hint) and issubclass(hint, EventData)",
            "    return is_request or is_event_data or is_oauth_arg",
            "",
            "",
            "def check_function_inputs_match(fn: Callable, inputs: list, inputs_as_dict: bool):",
            "    \"\"\"",
            "    Checks if the input component set matches the function",
            "    Returns: None if valid or if the function does not have a signature (e.g. is a built in),",
            "    or a string error message if mismatch",
            "    \"\"\"",
            "    try:",
            "        signature = inspect.signature(fn)",
            "    except ValueError:",
            "        return None",
            "    parameter_types = get_type_hints(fn)",
            "    min_args = 0",
            "    max_args = 0",
            "    infinity = -1",
            "    for name, param in signature.parameters.items():",
            "        has_default = param.default != param.empty",
            "        if param.kind in [param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD]:",
            "            if not is_special_typed_parameter(name, parameter_types):",
            "                if not has_default:",
            "                    min_args += 1",
            "                max_args += 1",
            "        elif param.kind == param.VAR_POSITIONAL:",
            "            max_args = infinity",
            "        elif param.kind == param.KEYWORD_ONLY and not has_default:",
            "            return f\"Keyword-only args must have default values for function {fn}\"",
            "    arg_count = 1 if inputs_as_dict else len(inputs)",
            "    if min_args == max_args and max_args != arg_count:",
            "        warnings.warn(",
            "            f\"Expected {max_args} arguments for function {fn}, received {arg_count}.\"",
            "        )",
            "    if arg_count < min_args:",
            "        warnings.warn(",
            "            f\"Expected at least {min_args} arguments for function {fn}, received {arg_count}.\"",
            "        )",
            "    if max_args != infinity and arg_count > max_args:",
            "        warnings.warn(",
            "            f\"Expected maximum {max_args} arguments for function {fn}, received {arg_count}.\"",
            "        )",
            "",
            "",
            "class TupleNoPrint(tuple):",
            "    # To remove printing function return in notebook",
            "    def __repr__(self):",
            "        return \"\"",
            "",
            "    def __str__(self):",
            "        return \"\"",
            "",
            "",
            "class MatplotlibBackendMananger:",
            "    def __enter__(self):",
            "        import matplotlib",
            "",
            "        self._original_backend = matplotlib.get_backend()",
            "        matplotlib.use(\"agg\")",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        import matplotlib",
            "",
            "        matplotlib.use(self._original_backend)",
            "",
            "",
            "def tex2svg(formula, *_args):",
            "    with MatplotlibBackendMananger():",
            "        import matplotlib.pyplot as plt",
            "",
            "        fontsize = 20",
            "        dpi = 300",
            "        plt.rc(\"mathtext\", fontset=\"cm\")",
            "        fig = plt.figure(figsize=(0.01, 0.01))",
            "        fig.text(0, 0, rf\"${formula}$\", fontsize=fontsize)",
            "        output = BytesIO()",
            "        fig.savefig(  # type: ignore",
            "            output,",
            "            dpi=dpi,",
            "            transparent=True,",
            "            format=\"svg\",",
            "            bbox_inches=\"tight\",",
            "            pad_inches=0.0,",
            "        )",
            "        plt.close(fig)",
            "        output.seek(0)",
            "        xml_code = output.read().decode(\"utf-8\")",
            "        svg_start = xml_code.index(\"<svg \")",
            "        svg_code = xml_code[svg_start:]",
            "        svg_code = re.sub(r\"<metadata>.*<\\/metadata>\", \"\", svg_code, flags=re.DOTALL)",
            "        svg_code = re.sub(r' width=\"[^\"]+\"', \"\", svg_code)",
            "        height_match = re.search(r'height=\"([\\d.]+)pt\"', svg_code)",
            "        if height_match:",
            "            height = float(height_match.group(1))",
            "            new_height = height / fontsize  # conversion from pt to em",
            "            svg_code = re.sub(",
            "                r'height=\"[\\d.]+pt\"', f'height=\"{new_height}em\"', svg_code",
            "            )",
            "        copy_code = f\"<span style='font-size: 0px'>{formula}</span>\"",
            "    return f\"{copy_code}{svg_code}\"",
            "",
            "",
            "def abspath(path: str | Path) -> Path:",
            "    \"\"\"Returns absolute path of a str or Path path, but does not resolve symlinks.\"\"\"",
            "    path = Path(path)",
            "",
            "    if path.is_absolute():",
            "        return path",
            "",
            "    # recursively check if there is a symlink within the path",
            "    is_symlink = path.is_symlink() or any(",
            "        parent.is_symlink() for parent in path.parents",
            "    )",
            "",
            "    if is_symlink or path == path.resolve():  # in case path couldn't be resolved",
            "        return Path.cwd() / path",
            "    else:",
            "        return path.resolve()",
            "",
            "",
            "def is_in_or_equal(path_1: str | Path, path_2: str | Path):",
            "    \"\"\"",
            "    True if path_1 is a descendant (i.e. located within) path_2 or if the paths are the",
            "    same, returns False otherwise.",
            "    Parameters:",
            "        path_1: str or Path (should be a file)",
            "        path_2: str or Path (can be a file or directory)",
            "    \"\"\"",
            "    path_1, path_2 = abspath(path_1), abspath(path_2)",
            "    try:",
            "        if \"..\" in str(path_1.relative_to(path_2)):  # prevent path traversal",
            "            return False",
            "    except ValueError:",
            "        return False",
            "    return True",
            "",
            "",
            "HTML_TAG_RE = re.compile(\"<.*?>\")",
            "",
            "",
            "def remove_html_tags(raw_html: str | None) -> str:",
            "    return re.sub(HTML_TAG_RE, \"\", raw_html or \"\")",
            "",
            "",
            "def find_user_stack_level() -> int:",
            "    \"\"\"",
            "    Find the first stack frame not inside Gradio.",
            "    \"\"\"",
            "    frame = inspect.currentframe()",
            "    n = 0",
            "    while frame:",
            "        fname = inspect.getfile(frame)",
            "        if \"/gradio/\" not in fname.replace(os.sep, \"/\"):",
            "            break",
            "        frame = frame.f_back",
            "        n += 1",
            "    return n",
            "",
            "",
            "class NamedString(str):",
            "    \"\"\"",
            "    Subclass of str that includes a .name attribute equal to the value of the string itself. This class is used when returning",
            "    a value from the `.preprocess()` methods of the File and UploadButton components. Before Gradio 4.0, these methods returned a file",
            "    object which was then converted to a string filepath using the `.name` attribute. In Gradio 4.0, these methods now return a str",
            "    filepath directly, but to maintain backwards compatibility, we use this class instead of a regular str.",
            "    \"\"\"",
            "",
            "    def __init__(self, *args):",
            "        super().__init__()",
            "        self.name = str(self) if args else \"\"",
            "",
            "",
            "def default_input_labels():",
            "    \"\"\"",
            "    A generator that provides default input labels for components when the user's function",
            "    does not have named parameters. The labels are of the form \"input 0\", \"input 1\", etc.",
            "    \"\"\"",
            "    n = 0",
            "    while True:",
            "        yield f\"input {n}\"",
            "        n += 1",
            "",
            "",
            "def get_extension_from_file_path_or_url(file_path_or_url: str) -> str:",
            "    \"\"\"",
            "    Returns the file extension (without the dot) from a file path or URL. If the file path or URL does not have a file extension, returns an empty string.",
            "    For example, \"https://example.com/avatar/xxxx.mp4?se=2023-11-16T06:51:23Z&sp=r\" would return \"mp4\".",
            "    \"\"\"",
            "    parsed_url = urllib.parse.urlparse(file_path_or_url)",
            "    file_extension = os.path.splitext(os.path.basename(parsed_url.path))[1]",
            "    return file_extension[1:] if file_extension else \"\"",
            "",
            "",
            "def convert_to_dict_if_dataclass(value):",
            "    if dataclasses.is_dataclass(value):",
            "        return dataclasses.asdict(value)",
            "    return value",
            "",
            "",
            "K = TypeVar(\"K\")",
            "V = TypeVar(\"V\")",
            "",
            "",
            "class LRUCache(OrderedDict, Generic[K, V]):",
            "    def __init__(self, max_size: int = 100):",
            "        super().__init__()",
            "        self.max_size: int = max_size",
            "",
            "    def __setitem__(self, key: K, value: V) -> None:",
            "        if key in self:",
            "            self.move_to_end(key)",
            "        elif len(self) >= self.max_size:",
            "            self.popitem(last=False)",
            "        super().__setitem__(key, value)",
            "",
            "",
            "def get_cache_folder() -> Path:",
            "    return Path(os.environ.get(\"GRADIO_EXAMPLES_CACHE\", \"gradio_cached_examples\"))",
            "",
            "",
            "def diff(old, new):",
            "    def compare_objects(obj1, obj2, path=None):",
            "        if path is None:",
            "            path = []",
            "        edits = []",
            "",
            "        if obj1 == obj2:",
            "            return edits",
            "",
            "        if type(obj1) != type(obj2):",
            "            edits.append((\"replace\", path, obj2))",
            "            return edits",
            "",
            "        if isinstance(obj1, str) and obj2.startswith(obj1):",
            "            edits.append((\"append\", path, obj2[len(obj1) :]))",
            "            return edits",
            "",
            "        if isinstance(obj1, list):",
            "            common_length = min(len(obj1), len(obj2))",
            "            for i in range(common_length):",
            "                edits.extend(compare_objects(obj1[i], obj2[i], path + [i]))",
            "            for i in range(common_length, len(obj1)):",
            "                edits.append((\"delete\", path + [i], None))",
            "            for i in range(common_length, len(obj2)):",
            "                edits.append((\"add\", path + [i], obj2[i]))",
            "            return edits",
            "",
            "        if isinstance(obj1, dict):",
            "            for key in obj1:",
            "                if key in obj2:",
            "                    edits.extend(compare_objects(obj1[key], obj2[key], path + [key]))",
            "                else:",
            "                    edits.append((\"delete\", path + [key], None))",
            "            for key in obj2:",
            "                if key not in obj1:",
            "                    edits.append((\"add\", path + [key], obj2[key]))",
            "            return edits",
            "",
            "        edits.append((\"replace\", path, obj2))",
            "        return edits",
            "",
            "    return compare_objects(old, new)",
            "",
            "",
            "def get_upload_folder() -> str:",
            "    return os.environ.get(\"GRADIO_TEMP_DIR\") or str(",
            "        (Path(tempfile.gettempdir()) / \"gradio\").resolve()",
            "    )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "src.pyload.webui.app.blueprints.json_blueprint"
        ]
    }
}