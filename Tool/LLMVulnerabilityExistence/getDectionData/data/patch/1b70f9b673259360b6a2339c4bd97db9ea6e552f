{
    "onnx/backend/test/runner/__init__.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " import re"
            },
            "1": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " import shutil"
            },
            "2": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " import sys"
            },
            "3": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import tarfile"
            },
            "4": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " import tempfile"
            },
            "5": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " import time"
            },
            "6": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " import unittest"
            },
            "7": {
                "beforePatchRowNumber": 242,
                "afterPatchRowNumber": 241,
                "PatchRowcode": "             )"
            },
            "8": {
                "beforePatchRowNumber": 243,
                "afterPatchRowNumber": 242,
                "PatchRowcode": "             urlretrieve(model_test.url, download_file.name)"
            },
            "9": {
                "beforePatchRowNumber": 244,
                "afterPatchRowNumber": 243,
                "PatchRowcode": "             print(\"Done\")"
            },
            "10": {
                "beforePatchRowNumber": 245,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            with tarfile.open(download_file.name) as t:"
            },
            "11": {
                "beforePatchRowNumber": 246,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                t.extractall(models_dir)"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 244,
                "PatchRowcode": "+            onnx.utils._extract_model_safe(download_file.name, models_dir)"
            },
            "13": {
                "beforePatchRowNumber": 247,
                "afterPatchRowNumber": 245,
                "PatchRowcode": "         except Exception as e:"
            },
            "14": {
                "beforePatchRowNumber": 248,
                "afterPatchRowNumber": 246,
                "PatchRowcode": "             print(f\"Failed to prepare data for model {model_test.model_name}: {e}\")"
            },
            "15": {
                "beforePatchRowNumber": 249,
                "afterPatchRowNumber": 247,
                "PatchRowcode": "             raise"
            }
        },
        "frontPatchFile": [
            "# Copyright (c) ONNX Project Contributors",
            "#",
            "# SPDX-License-Identifier: Apache-2.0",
            "",
            "from __future__ import annotations",
            "",
            "import functools",
            "import glob",
            "import os",
            "import re",
            "import shutil",
            "import sys",
            "import tarfile",
            "import tempfile",
            "import time",
            "import unittest",
            "from collections import defaultdict",
            "from typing import Any, Callable, Iterable, Pattern, Sequence",
            "from urllib.request import urlretrieve",
            "",
            "import numpy as np",
            "",
            "import onnx",
            "import onnx.reference",
            "from onnx import ONNX_ML, ModelProto, NodeProto, TypeProto, ValueInfoProto, numpy_helper",
            "from onnx.backend.base import Backend",
            "from onnx.backend.test.case.test_case import TestCase",
            "from onnx.backend.test.loader import load_model_tests",
            "from onnx.backend.test.runner.item import TestItem",
            "",
            "",
            "class BackendIsNotSupposedToImplementIt(unittest.SkipTest):",
            "    pass",
            "",
            "",
            "def retry_execute(times: int) -> Callable[[Callable[..., Any]], Callable[..., Any]]:",
            "    assert times >= 1",
            "",
            "    def wrapper(func: Callable[..., Any]) -> Callable[..., Any]:",
            "        @functools.wraps(func)",
            "        def wrapped(*args: Any, **kwargs: Any) -> Any:",
            "            for i in range(1, times + 1):",
            "                try:",
            "                    return func(*args, **kwargs)",
            "                except Exception:  # noqa: PERF203",
            "                    print(f\"{i} times tried\")",
            "                    if i == times:",
            "                        raise",
            "                    time.sleep(5 * i)",
            "",
            "        return wrapped",
            "",
            "    return wrapper",
            "",
            "",
            "class Runner:",
            "    def __init__(",
            "        self,",
            "        backend: type[Backend],",
            "        parent_module: str | None = None,",
            "        test_kwargs: dict | None = None,",
            "    ) -> None:",
            "        self.backend = backend",
            "        self._parent_module = parent_module",
            "        self._include_patterns: set[Pattern[str]] = set()",
            "        self._exclude_patterns: set[Pattern[str]] = set()",
            "        self._xfail_patterns: set[Pattern[str]] = set()",
            "        self._test_kwargs: dict = test_kwargs or {}",
            "",
            "        # This is the source of the truth of all test functions.",
            "        # Properties `test_cases`, `test_suite` and `tests` will be",
            "        # derived from it.",
            "        # {category: {name: func}}",
            "        self._test_items: dict[str, dict[str, TestItem]] = defaultdict(dict)",
            "",
            "        for rt in load_model_tests(kind=\"node\"):",
            "            self._add_model_test(rt, \"Node\")",
            "",
            "        for rt in load_model_tests(kind=\"real\"):",
            "            self._add_model_test(rt, \"Real\")",
            "",
            "        for rt in load_model_tests(kind=\"simple\"):",
            "            self._add_model_test(rt, \"Simple\")",
            "",
            "        for ct in load_model_tests(kind=\"pytorch-converted\"):",
            "            self._add_model_test(ct, \"PyTorchConverted\")",
            "",
            "        for ot in load_model_tests(kind=\"pytorch-operator\"):",
            "            self._add_model_test(ot, \"PyTorchOperator\")",
            "",
            "    def _get_test_case(self, name: str) -> type[unittest.TestCase]:",
            "        test_case = type(str(name), (unittest.TestCase,), {})",
            "        if self._parent_module:",
            "            test_case.__module__ = self._parent_module",
            "        return test_case",
            "",
            "    def include(self, pattern: str) -> Runner:",
            "        self._include_patterns.add(re.compile(pattern))",
            "        return self",
            "",
            "    def exclude(self, pattern: str) -> Runner:",
            "        self._exclude_patterns.add(re.compile(pattern))",
            "        return self",
            "",
            "    def xfail(self, pattern: str) -> Runner:",
            "        self._xfail_patterns.add(re.compile(pattern))",
            "        return self",
            "",
            "    def enable_report(self) -> Runner:",
            "        import pytest",
            "",
            "        for category, items_map in self._test_items.items():",
            "            for item in items_map.values():",
            "                item.func = pytest.mark.onnx_coverage(item.proto, category)(item.func)",
            "        return self",
            "",
            "    @property",
            "    def _filtered_test_items(self) -> dict[str, dict[str, TestItem]]:",
            "        filtered: dict[str, dict[str, TestItem]] = {}",
            "        for category, items_map in self._test_items.items():",
            "            filtered[category] = {}",
            "            for name, item in items_map.items():",
            "                if self._include_patterns and (",
            "                    not any(include.search(name) for include in self._include_patterns)",
            "                ):",
            "                    item.func = unittest.skip(\"no matched include pattern\")(item.func)",
            "                for exclude in self._exclude_patterns:",
            "                    if exclude.search(name):",
            "                        item.func = unittest.skip(",
            "                            f'matched exclude pattern \"{exclude.pattern}\"'",
            "                        )(item.func)",
            "                for xfail in self._xfail_patterns:",
            "                    if xfail.search(name):",
            "                        item.func = unittest.expectedFailure(item.func)",
            "                filtered[category][name] = item",
            "        return filtered",
            "",
            "    @property",
            "    def test_cases(self) -> dict[str, type[unittest.TestCase]]:",
            "        \"\"\"List of test cases to be applied on the parent scope",
            "        Example usage:",
            "            globals().update(BackendTest(backend).test_cases)",
            "        \"\"\"",
            "        test_cases = {}",
            "        for category, items_map in self._filtered_test_items.items():",
            "            test_case_name = f\"OnnxBackend{category}Test\"",
            "            test_case = self._get_test_case(test_case_name)",
            "            for name, item in sorted(items_map.items()):",
            "                setattr(test_case, name, item.func)",
            "            test_cases[test_case_name] = test_case",
            "        return test_cases",
            "",
            "    @property",
            "    def test_suite(self) -> unittest.TestSuite:",
            "        \"\"\"TestSuite that can be run by TestRunner",
            "        Example usage:",
            "            unittest.TextTestRunner().run(BackendTest(backend).test_suite)",
            "        \"\"\"",
            "        suite = unittest.TestSuite()",
            "        for case in sorted(",
            "            self.test_cases.values(), key=lambda cl: cl.__class__.__name__",
            "        ):",
            "            suite.addTests(unittest.defaultTestLoader.loadTestsFromTestCase(case))",
            "        return suite",
            "",
            "    # For backward compatibility (we used to expose `.tests`)",
            "    @property",
            "    def tests(self) -> type[unittest.TestCase]:",
            "        \"\"\"One single unittest.TestCase that hosts all the test functions",
            "        Example usage:",
            "            onnx_backend_tests = BackendTest(backend).tests",
            "        \"\"\"",
            "        tests = self._get_test_case(\"OnnxBackendTest\")",
            "        for items_map in sorted(",
            "            self._filtered_test_items.values(), key=lambda cl: cl.__class__.__name__",
            "        ):",
            "            for name, item in sorted(items_map.items()):",
            "                setattr(tests, name, item.func)",
            "        return tests",
            "",
            "    @classmethod",
            "    def assert_similar_outputs(",
            "        cls,",
            "        ref_outputs: Sequence[Any],",
            "        outputs: Sequence[Any],",
            "        rtol: float,",
            "        atol: float,",
            "        model_dir: str | None = None,",
            "    ) -> None:",
            "        try:",
            "            np.testing.assert_equal(len(outputs), len(ref_outputs))",
            "        except TypeError as e:",
            "            raise TypeError(",
            "                f\"Unable to compare expected type {type(ref_outputs)} \"",
            "                f\"and runtime type {type(outputs)} (known test={model_dir or '?'!r})\"",
            "            ) from e",
            "        for i in range(len(outputs)):",
            "            if isinstance(outputs[i], (list, tuple)):",
            "                if not isinstance(ref_outputs[i], (list, tuple)):",
            "                    raise AssertionError(  # noqa: TRY004",
            "                        f\"Unexpected type {type(outputs[i])} for outputs[{i}]. Expected \"",
            "                        f\"type is {type(ref_outputs[i])} (known test={model_dir or '?'!r}).\"",
            "                    )",
            "                for j in range(len(outputs[i])):",
            "                    cls.assert_similar_outputs(",
            "                        ref_outputs[i][j],",
            "                        outputs[i][j],",
            "                        rtol,",
            "                        atol,",
            "                        model_dir=model_dir,",
            "                    )",
            "            else:",
            "                if not np.issubdtype(ref_outputs[i].dtype, np.number):",
            "                    if ref_outputs[i].tolist() != outputs[i].tolist():",
            "                        raise AssertionError(f\"{ref_outputs[i]} != {outputs[i]}\")",
            "                    continue",
            "                np.testing.assert_equal(outputs[i].dtype, ref_outputs[i].dtype)",
            "                if ref_outputs[i].dtype == object:  # type: ignore[attr-defined]",
            "                    np.testing.assert_array_equal(outputs[i], ref_outputs[i])",
            "                else:",
            "                    np.testing.assert_allclose(",
            "                        outputs[i], ref_outputs[i], rtol=rtol, atol=atol",
            "                    )",
            "",
            "    @classmethod",
            "    @retry_execute(3)",
            "    def download_model(",
            "        cls,",
            "        model_test: TestCase,",
            "        model_dir: str,",
            "        models_dir: str,",
            "    ) -> None:",
            "        # On Windows, NamedTemporaryFile can not be opened for a",
            "        # second time",
            "        del model_dir",
            "        download_file = tempfile.NamedTemporaryFile(delete=False)",
            "        try:",
            "            download_file.close()",
            "            assert model_test.url",
            "            print(",
            "                f\"Start downloading model {model_test.model_name} from {model_test.url}\"",
            "            )",
            "            urlretrieve(model_test.url, download_file.name)",
            "            print(\"Done\")",
            "            with tarfile.open(download_file.name) as t:",
            "                t.extractall(models_dir)",
            "        except Exception as e:",
            "            print(f\"Failed to prepare data for model {model_test.model_name}: {e}\")",
            "            raise",
            "        finally:",
            "            os.remove(download_file.name)",
            "",
            "    @classmethod",
            "    def prepare_model_data(cls, model_test: TestCase) -> str:",
            "        onnx_home = os.path.expanduser(",
            "            os.getenv(\"ONNX_HOME\", os.path.join(\"~\", \".onnx\"))",
            "        )",
            "        models_dir = os.getenv(\"ONNX_MODELS\", os.path.join(onnx_home, \"models\"))",
            "        model_dir: str = os.path.join(models_dir, model_test.model_name)",
            "        if not os.path.exists(os.path.join(model_dir, \"model.onnx\")):",
            "            if os.path.exists(model_dir):",
            "                bi = 0",
            "                while True:",
            "                    dest = f\"{model_dir}.old.{bi}\"",
            "                    if os.path.exists(dest):",
            "                        bi += 1",
            "                        continue",
            "                    shutil.move(model_dir, dest)",
            "                    break",
            "            os.makedirs(model_dir)",
            "",
            "            cls.download_model(",
            "                model_test=model_test, model_dir=model_dir, models_dir=models_dir",
            "            )",
            "        return model_dir",
            "",
            "    def _add_test(",
            "        self,",
            "        category: str,",
            "        test_name: str,",
            "        test_func: Callable[..., Any],",
            "        report_item: list[ModelProto | NodeProto | None],",
            "        devices: Iterable[str] = (\"CPU\", \"CUDA\"),",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        # We don't prepend the 'test_' prefix to improve greppability",
            "        if not test_name.startswith(\"test_\"):",
            "            raise ValueError(f\"Test name must start with test_: {test_name}\")",
            "",
            "        def add_device_test(device: str) -> None:",
            "            device_test_name = f\"{test_name}_{device.lower()}\"",
            "            if device_test_name in self._test_items[category]:",
            "                raise ValueError(",
            "                    f'Duplicated test name \"{device_test_name}\" in category \"{category}\"'",
            "                )",
            "",
            "            @unittest.skipIf(  # type: ignore",
            "                not self.backend.supports_device(device),",
            "                f\"Backend doesn't support device {device}\",",
            "            )",
            "            @functools.wraps(test_func)",
            "            def device_test_func(*args: Any, **device_test_kwarg: Any) -> Any:",
            "                try:",
            "                    merged_kwargs = {**kwargs, **device_test_kwarg}",
            "                    return test_func(*args, device, **merged_kwargs)",
            "                except BackendIsNotSupposedToImplementIt as e:",
            "                    # hacky verbose reporting",
            "                    if \"-v\" in sys.argv or \"--verbose\" in sys.argv:",
            "                        print(f\"Test {device_test_name} is effectively skipped: {e}\")",
            "",
            "            self._test_items[category][device_test_name] = TestItem(",
            "                device_test_func, report_item",
            "            )",
            "",
            "        for device in devices:",
            "            add_device_test(device)",
            "",
            "    @staticmethod",
            "    def generate_dummy_data(",
            "        x: ValueInfoProto, seed: int = 0, name: str = \"\", random: bool = False",
            "    ) -> np.ndarray:",
            "        \"\"\"Generates a random tensor based on the input definition.\"\"\"",
            "        if not x.type.tensor_type:",
            "            raise NotImplementedError(",
            "                f\"Input expected to have tensor type. \"",
            "                f\"Unable to generate random data for model {name!r} and input {x}.\"",
            "            )",
            "        if x.type.tensor_type.elem_type != 1:",
            "            raise NotImplementedError(",
            "                f\"Currently limited to float tensors. \"",
            "                f\"Unable to generate random data for model {name!r} and input {x}.\"",
            "            )",
            "        shape = tuple(",
            "            d.dim_value if d.HasField(\"dim_value\") else 1",
            "            for d in x.type.tensor_type.shape.dim",
            "        )",
            "        if random:",
            "            gen = np.random.default_rng(seed=seed)",
            "            return gen.random(shape, np.float32)",
            "        n = np.prod(shape)",
            "        return (np.arange(n).reshape(shape) / n).astype(np.float32)",
            "",
            "    def _add_model_test(self, model_test: TestCase, kind: str) -> None:",
            "        # model is loaded at runtime, note sometimes it could even",
            "        # never loaded if the test skipped",
            "        model_marker: list[ModelProto | NodeProto | None] = [None]",
            "",
            "        def run(test_self: Any, device: str, **kwargs) -> None:  # noqa: ARG001",
            "            if model_test.url is not None and model_test.url.startswith(",
            "                \"onnx/backend/test/data/light/\"",
            "            ):",
            "                # testing local files",
            "                model_pb_path = os.path.normpath(",
            "                    os.path.join(",
            "                        os.path.dirname(__file__),",
            "                        \"..\",",
            "                        \"..\",",
            "                        \"..\",",
            "                        \"..\",",
            "                        model_test.url,",
            "                    )",
            "                )",
            "                if not os.path.exists(model_pb_path):",
            "                    raise FileNotFoundError(f\"Unable to find model {model_pb_path!r}.\")",
            "                onnx_home = os.path.expanduser(",
            "                    os.getenv(\"ONNX_HOME\", os.path.join(\"~\", \".onnx\"))",
            "                )",
            "                models_dir = os.getenv(",
            "                    \"ONNX_MODELS\", os.path.join(onnx_home, \"models\", \"light\")",
            "                )",
            "                model_dir: str = os.path.join(models_dir, model_test.model_name)",
            "                if not os.path.exists(model_dir):",
            "                    os.makedirs(model_dir)",
            "                use_dummy = True",
            "            else:",
            "                if model_test.model_dir is None:",
            "                    model_dir = self.prepare_model_data(model_test)",
            "                else:",
            "                    model_dir = model_test.model_dir",
            "                model_pb_path = os.path.join(model_dir, \"model.onnx\")",
            "                use_dummy = False",
            "",
            "            if not ONNX_ML and \"ai_onnx_ml\" in model_dir:",
            "                return",
            "",
            "            model = onnx.load(model_pb_path)",
            "            model_marker[0] = model",
            "            if (",
            "                hasattr(self.backend, \"is_compatible\")",
            "                and callable(self.backend.is_compatible)",
            "                and not self.backend.is_compatible(model)",
            "            ):",
            "                raise unittest.SkipTest(\"Not compatible with backend\")",
            "",
            "            prepared_model = self.backend.prepare(model, device, **kwargs)",
            "            assert prepared_model is not None",
            "",
            "            if use_dummy:",
            "                # When the backend test goes through a test involving a",
            "                # model stored in onnx/backend/test/data/light,",
            "                # this function generates expected output coming from",
            "                # from ReferenceEvaluator run with random inputs.",
            "                # A couple of models include many Conv operators and the",
            "                # python implementation is slow (such as test_bvlc_alexnet).",
            "                with open(model_pb_path, \"rb\") as f:",
            "                    onx = onnx.load(f)",
            "",
            "                test_data_set = os.path.join(model_dir, \"test_data_set_0\")",
            "                if not os.path.exists(test_data_set):",
            "                    os.mkdir(test_data_set)",
            "                feeds = {}",
            "                inits = {i.name for i in onx.graph.initializer}",
            "                n_input = 0",
            "                inputs = []",
            "                for i in range(len(onx.graph.input)):",
            "                    if onx.graph.input[i].name in inits:",
            "                        continue",
            "                    name = os.path.join(test_data_set, f\"input_{n_input}.pb\")",
            "                    inputs.append(name)",
            "                    n_input += 1",
            "                    x = onx.graph.input[i]",
            "                    value = self.generate_dummy_data(",
            "                        x, seed=0, name=model_test.model_name, random=False",
            "                    )",
            "                    feeds[x.name] = value",
            "                    with open(name, \"wb\") as f:",
            "                        f.write(onnx.numpy_helper.from_array(value).SerializeToString())",
            "",
            "                # loads expected output if any available",
            "                prefix = os.path.splitext(model_pb_path)[0]",
            "                expected_outputs = []",
            "                for i in range(len(onx.graph.output)):",
            "                    name = f\"{prefix}_output_{i}.pb\"",
            "                    if os.path.exists(name):",
            "                        expected_outputs.append(name)",
            "                        continue",
            "                    expected_outputs = None",
            "                    break",
            "",
            "                if expected_outputs is None:",
            "                    ref = onnx.reference.ReferenceEvaluator(onx)",
            "                    outputs = ref.run(None, feeds)",
            "                    for i, o in enumerate(outputs):",
            "                        name = os.path.join(test_data_set, f\"output_{i}.pb\")",
            "                        with open(name, \"wb\") as f:",
            "                            f.write(onnx.numpy_helper.from_array(o).SerializeToString())",
            "                else:",
            "                    for i, o in enumerate(expected_outputs):",
            "                        name = os.path.join(test_data_set, f\"output_{i}.pb\")",
            "                        shutil.copy(o, name)",
            "            else:",
            "                # TODO after converting all npz files to protobuf, we can delete this.",
            "                for test_data_npz in glob.glob(",
            "                    os.path.join(model_dir, \"test_data_*.npz\")",
            "                ):",
            "                    test_data = np.load(test_data_npz, encoding=\"bytes\")",
            "                    inputs = list(test_data[\"inputs\"])",
            "                    outputs = list(prepared_model.run(inputs))",
            "                    ref_outputs = tuple(",
            "                        np.array(x) if not isinstance(x, (list, dict)) else x",
            "                        for f in test_data[\"outputs\"]",
            "                    )",
            "                    self.assert_similar_outputs(",
            "                        ref_outputs,",
            "                        outputs,",
            "                        rtol=kwargs.get(\"rtol\", model_test.rtol),",
            "                        atol=kwargs.get(\"atol\", model_test.atol),",
            "                        model_dir=model_dir,",
            "                    )",
            "",
            "            for test_data_dir in glob.glob(os.path.join(model_dir, \"test_data_set*\")):",
            "                inputs = []",
            "                inputs_num = len(glob.glob(os.path.join(test_data_dir, \"input_*.pb\")))",
            "                for i in range(inputs_num):",
            "                    input_file = os.path.join(test_data_dir, f\"input_{i}.pb\")",
            "                    self._load_proto(input_file, inputs, model.graph.input[i].type)",
            "                ref_outputs = []",
            "                ref_outputs_num = len(",
            "                    glob.glob(os.path.join(test_data_dir, \"output_*.pb\"))",
            "                )",
            "                for i in range(ref_outputs_num):",
            "                    output_file = os.path.join(test_data_dir, f\"output_{i}.pb\")",
            "                    self._load_proto(",
            "                        output_file, ref_outputs, model.graph.output[i].type",
            "                    )",
            "                outputs = list(prepared_model.run(inputs))",
            "                self.assert_similar_outputs(",
            "                    ref_outputs,",
            "                    outputs,",
            "                    rtol=kwargs.get(\"rtol\", model_test.rtol),",
            "                    atol=kwargs.get(\"atol\", model_test.atol),",
            "                    model_dir=model_dir,",
            "                )",
            "",
            "        if model_test.name in self._test_kwargs:",
            "            self._add_test(",
            "                kind + \"Model\",",
            "                model_test.name,",
            "                run,",
            "                model_marker,",
            "                **self._test_kwargs[model_test.name],",
            "            )",
            "        else:",
            "            self._add_test(kind + \"Model\", model_test.name, run, model_marker)",
            "",
            "    def _load_proto(",
            "        self,",
            "        proto_filename: str,",
            "        target_list: list[np.ndarray | list[Any]],",
            "        model_type_proto: TypeProto,",
            "    ) -> None:",
            "        with open(proto_filename, \"rb\") as f:",
            "            protobuf_content = f.read()",
            "            if model_type_proto.HasField(\"sequence_type\"):",
            "                sequence = onnx.SequenceProto()",
            "                sequence.ParseFromString(protobuf_content)",
            "                target_list.append(numpy_helper.to_list(sequence))",
            "            elif model_type_proto.HasField(\"tensor_type\"):",
            "                tensor = onnx.TensorProto()",
            "                tensor.ParseFromString(protobuf_content)",
            "                t = numpy_helper.to_array(tensor)",
            "                assert isinstance(t, np.ndarray)",
            "                target_list.append(t)",
            "            elif model_type_proto.HasField(\"optional_type\"):",
            "                optional = onnx.OptionalProto()",
            "                optional.ParseFromString(protobuf_content)",
            "                target_list.append(numpy_helper.to_optional(optional))  # type: ignore[arg-type]",
            "            else:",
            "                print(",
            "                    \"Loading proto of that specific type (Map/Sparse Tensor) is currently not supported\"",
            "                )"
        ],
        "afterPatchFile": [
            "# Copyright (c) ONNX Project Contributors",
            "#",
            "# SPDX-License-Identifier: Apache-2.0",
            "",
            "from __future__ import annotations",
            "",
            "import functools",
            "import glob",
            "import os",
            "import re",
            "import shutil",
            "import sys",
            "import tempfile",
            "import time",
            "import unittest",
            "from collections import defaultdict",
            "from typing import Any, Callable, Iterable, Pattern, Sequence",
            "from urllib.request import urlretrieve",
            "",
            "import numpy as np",
            "",
            "import onnx",
            "import onnx.reference",
            "from onnx import ONNX_ML, ModelProto, NodeProto, TypeProto, ValueInfoProto, numpy_helper",
            "from onnx.backend.base import Backend",
            "from onnx.backend.test.case.test_case import TestCase",
            "from onnx.backend.test.loader import load_model_tests",
            "from onnx.backend.test.runner.item import TestItem",
            "",
            "",
            "class BackendIsNotSupposedToImplementIt(unittest.SkipTest):",
            "    pass",
            "",
            "",
            "def retry_execute(times: int) -> Callable[[Callable[..., Any]], Callable[..., Any]]:",
            "    assert times >= 1",
            "",
            "    def wrapper(func: Callable[..., Any]) -> Callable[..., Any]:",
            "        @functools.wraps(func)",
            "        def wrapped(*args: Any, **kwargs: Any) -> Any:",
            "            for i in range(1, times + 1):",
            "                try:",
            "                    return func(*args, **kwargs)",
            "                except Exception:  # noqa: PERF203",
            "                    print(f\"{i} times tried\")",
            "                    if i == times:",
            "                        raise",
            "                    time.sleep(5 * i)",
            "",
            "        return wrapped",
            "",
            "    return wrapper",
            "",
            "",
            "class Runner:",
            "    def __init__(",
            "        self,",
            "        backend: type[Backend],",
            "        parent_module: str | None = None,",
            "        test_kwargs: dict | None = None,",
            "    ) -> None:",
            "        self.backend = backend",
            "        self._parent_module = parent_module",
            "        self._include_patterns: set[Pattern[str]] = set()",
            "        self._exclude_patterns: set[Pattern[str]] = set()",
            "        self._xfail_patterns: set[Pattern[str]] = set()",
            "        self._test_kwargs: dict = test_kwargs or {}",
            "",
            "        # This is the source of the truth of all test functions.",
            "        # Properties `test_cases`, `test_suite` and `tests` will be",
            "        # derived from it.",
            "        # {category: {name: func}}",
            "        self._test_items: dict[str, dict[str, TestItem]] = defaultdict(dict)",
            "",
            "        for rt in load_model_tests(kind=\"node\"):",
            "            self._add_model_test(rt, \"Node\")",
            "",
            "        for rt in load_model_tests(kind=\"real\"):",
            "            self._add_model_test(rt, \"Real\")",
            "",
            "        for rt in load_model_tests(kind=\"simple\"):",
            "            self._add_model_test(rt, \"Simple\")",
            "",
            "        for ct in load_model_tests(kind=\"pytorch-converted\"):",
            "            self._add_model_test(ct, \"PyTorchConverted\")",
            "",
            "        for ot in load_model_tests(kind=\"pytorch-operator\"):",
            "            self._add_model_test(ot, \"PyTorchOperator\")",
            "",
            "    def _get_test_case(self, name: str) -> type[unittest.TestCase]:",
            "        test_case = type(str(name), (unittest.TestCase,), {})",
            "        if self._parent_module:",
            "            test_case.__module__ = self._parent_module",
            "        return test_case",
            "",
            "    def include(self, pattern: str) -> Runner:",
            "        self._include_patterns.add(re.compile(pattern))",
            "        return self",
            "",
            "    def exclude(self, pattern: str) -> Runner:",
            "        self._exclude_patterns.add(re.compile(pattern))",
            "        return self",
            "",
            "    def xfail(self, pattern: str) -> Runner:",
            "        self._xfail_patterns.add(re.compile(pattern))",
            "        return self",
            "",
            "    def enable_report(self) -> Runner:",
            "        import pytest",
            "",
            "        for category, items_map in self._test_items.items():",
            "            for item in items_map.values():",
            "                item.func = pytest.mark.onnx_coverage(item.proto, category)(item.func)",
            "        return self",
            "",
            "    @property",
            "    def _filtered_test_items(self) -> dict[str, dict[str, TestItem]]:",
            "        filtered: dict[str, dict[str, TestItem]] = {}",
            "        for category, items_map in self._test_items.items():",
            "            filtered[category] = {}",
            "            for name, item in items_map.items():",
            "                if self._include_patterns and (",
            "                    not any(include.search(name) for include in self._include_patterns)",
            "                ):",
            "                    item.func = unittest.skip(\"no matched include pattern\")(item.func)",
            "                for exclude in self._exclude_patterns:",
            "                    if exclude.search(name):",
            "                        item.func = unittest.skip(",
            "                            f'matched exclude pattern \"{exclude.pattern}\"'",
            "                        )(item.func)",
            "                for xfail in self._xfail_patterns:",
            "                    if xfail.search(name):",
            "                        item.func = unittest.expectedFailure(item.func)",
            "                filtered[category][name] = item",
            "        return filtered",
            "",
            "    @property",
            "    def test_cases(self) -> dict[str, type[unittest.TestCase]]:",
            "        \"\"\"List of test cases to be applied on the parent scope",
            "        Example usage:",
            "            globals().update(BackendTest(backend).test_cases)",
            "        \"\"\"",
            "        test_cases = {}",
            "        for category, items_map in self._filtered_test_items.items():",
            "            test_case_name = f\"OnnxBackend{category}Test\"",
            "            test_case = self._get_test_case(test_case_name)",
            "            for name, item in sorted(items_map.items()):",
            "                setattr(test_case, name, item.func)",
            "            test_cases[test_case_name] = test_case",
            "        return test_cases",
            "",
            "    @property",
            "    def test_suite(self) -> unittest.TestSuite:",
            "        \"\"\"TestSuite that can be run by TestRunner",
            "        Example usage:",
            "            unittest.TextTestRunner().run(BackendTest(backend).test_suite)",
            "        \"\"\"",
            "        suite = unittest.TestSuite()",
            "        for case in sorted(",
            "            self.test_cases.values(), key=lambda cl: cl.__class__.__name__",
            "        ):",
            "            suite.addTests(unittest.defaultTestLoader.loadTestsFromTestCase(case))",
            "        return suite",
            "",
            "    # For backward compatibility (we used to expose `.tests`)",
            "    @property",
            "    def tests(self) -> type[unittest.TestCase]:",
            "        \"\"\"One single unittest.TestCase that hosts all the test functions",
            "        Example usage:",
            "            onnx_backend_tests = BackendTest(backend).tests",
            "        \"\"\"",
            "        tests = self._get_test_case(\"OnnxBackendTest\")",
            "        for items_map in sorted(",
            "            self._filtered_test_items.values(), key=lambda cl: cl.__class__.__name__",
            "        ):",
            "            for name, item in sorted(items_map.items()):",
            "                setattr(tests, name, item.func)",
            "        return tests",
            "",
            "    @classmethod",
            "    def assert_similar_outputs(",
            "        cls,",
            "        ref_outputs: Sequence[Any],",
            "        outputs: Sequence[Any],",
            "        rtol: float,",
            "        atol: float,",
            "        model_dir: str | None = None,",
            "    ) -> None:",
            "        try:",
            "            np.testing.assert_equal(len(outputs), len(ref_outputs))",
            "        except TypeError as e:",
            "            raise TypeError(",
            "                f\"Unable to compare expected type {type(ref_outputs)} \"",
            "                f\"and runtime type {type(outputs)} (known test={model_dir or '?'!r})\"",
            "            ) from e",
            "        for i in range(len(outputs)):",
            "            if isinstance(outputs[i], (list, tuple)):",
            "                if not isinstance(ref_outputs[i], (list, tuple)):",
            "                    raise AssertionError(  # noqa: TRY004",
            "                        f\"Unexpected type {type(outputs[i])} for outputs[{i}]. Expected \"",
            "                        f\"type is {type(ref_outputs[i])} (known test={model_dir or '?'!r}).\"",
            "                    )",
            "                for j in range(len(outputs[i])):",
            "                    cls.assert_similar_outputs(",
            "                        ref_outputs[i][j],",
            "                        outputs[i][j],",
            "                        rtol,",
            "                        atol,",
            "                        model_dir=model_dir,",
            "                    )",
            "            else:",
            "                if not np.issubdtype(ref_outputs[i].dtype, np.number):",
            "                    if ref_outputs[i].tolist() != outputs[i].tolist():",
            "                        raise AssertionError(f\"{ref_outputs[i]} != {outputs[i]}\")",
            "                    continue",
            "                np.testing.assert_equal(outputs[i].dtype, ref_outputs[i].dtype)",
            "                if ref_outputs[i].dtype == object:  # type: ignore[attr-defined]",
            "                    np.testing.assert_array_equal(outputs[i], ref_outputs[i])",
            "                else:",
            "                    np.testing.assert_allclose(",
            "                        outputs[i], ref_outputs[i], rtol=rtol, atol=atol",
            "                    )",
            "",
            "    @classmethod",
            "    @retry_execute(3)",
            "    def download_model(",
            "        cls,",
            "        model_test: TestCase,",
            "        model_dir: str,",
            "        models_dir: str,",
            "    ) -> None:",
            "        # On Windows, NamedTemporaryFile can not be opened for a",
            "        # second time",
            "        del model_dir",
            "        download_file = tempfile.NamedTemporaryFile(delete=False)",
            "        try:",
            "            download_file.close()",
            "            assert model_test.url",
            "            print(",
            "                f\"Start downloading model {model_test.model_name} from {model_test.url}\"",
            "            )",
            "            urlretrieve(model_test.url, download_file.name)",
            "            print(\"Done\")",
            "            onnx.utils._extract_model_safe(download_file.name, models_dir)",
            "        except Exception as e:",
            "            print(f\"Failed to prepare data for model {model_test.model_name}: {e}\")",
            "            raise",
            "        finally:",
            "            os.remove(download_file.name)",
            "",
            "    @classmethod",
            "    def prepare_model_data(cls, model_test: TestCase) -> str:",
            "        onnx_home = os.path.expanduser(",
            "            os.getenv(\"ONNX_HOME\", os.path.join(\"~\", \".onnx\"))",
            "        )",
            "        models_dir = os.getenv(\"ONNX_MODELS\", os.path.join(onnx_home, \"models\"))",
            "        model_dir: str = os.path.join(models_dir, model_test.model_name)",
            "        if not os.path.exists(os.path.join(model_dir, \"model.onnx\")):",
            "            if os.path.exists(model_dir):",
            "                bi = 0",
            "                while True:",
            "                    dest = f\"{model_dir}.old.{bi}\"",
            "                    if os.path.exists(dest):",
            "                        bi += 1",
            "                        continue",
            "                    shutil.move(model_dir, dest)",
            "                    break",
            "            os.makedirs(model_dir)",
            "",
            "            cls.download_model(",
            "                model_test=model_test, model_dir=model_dir, models_dir=models_dir",
            "            )",
            "        return model_dir",
            "",
            "    def _add_test(",
            "        self,",
            "        category: str,",
            "        test_name: str,",
            "        test_func: Callable[..., Any],",
            "        report_item: list[ModelProto | NodeProto | None],",
            "        devices: Iterable[str] = (\"CPU\", \"CUDA\"),",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        # We don't prepend the 'test_' prefix to improve greppability",
            "        if not test_name.startswith(\"test_\"):",
            "            raise ValueError(f\"Test name must start with test_: {test_name}\")",
            "",
            "        def add_device_test(device: str) -> None:",
            "            device_test_name = f\"{test_name}_{device.lower()}\"",
            "            if device_test_name in self._test_items[category]:",
            "                raise ValueError(",
            "                    f'Duplicated test name \"{device_test_name}\" in category \"{category}\"'",
            "                )",
            "",
            "            @unittest.skipIf(  # type: ignore",
            "                not self.backend.supports_device(device),",
            "                f\"Backend doesn't support device {device}\",",
            "            )",
            "            @functools.wraps(test_func)",
            "            def device_test_func(*args: Any, **device_test_kwarg: Any) -> Any:",
            "                try:",
            "                    merged_kwargs = {**kwargs, **device_test_kwarg}",
            "                    return test_func(*args, device, **merged_kwargs)",
            "                except BackendIsNotSupposedToImplementIt as e:",
            "                    # hacky verbose reporting",
            "                    if \"-v\" in sys.argv or \"--verbose\" in sys.argv:",
            "                        print(f\"Test {device_test_name} is effectively skipped: {e}\")",
            "",
            "            self._test_items[category][device_test_name] = TestItem(",
            "                device_test_func, report_item",
            "            )",
            "",
            "        for device in devices:",
            "            add_device_test(device)",
            "",
            "    @staticmethod",
            "    def generate_dummy_data(",
            "        x: ValueInfoProto, seed: int = 0, name: str = \"\", random: bool = False",
            "    ) -> np.ndarray:",
            "        \"\"\"Generates a random tensor based on the input definition.\"\"\"",
            "        if not x.type.tensor_type:",
            "            raise NotImplementedError(",
            "                f\"Input expected to have tensor type. \"",
            "                f\"Unable to generate random data for model {name!r} and input {x}.\"",
            "            )",
            "        if x.type.tensor_type.elem_type != 1:",
            "            raise NotImplementedError(",
            "                f\"Currently limited to float tensors. \"",
            "                f\"Unable to generate random data for model {name!r} and input {x}.\"",
            "            )",
            "        shape = tuple(",
            "            d.dim_value if d.HasField(\"dim_value\") else 1",
            "            for d in x.type.tensor_type.shape.dim",
            "        )",
            "        if random:",
            "            gen = np.random.default_rng(seed=seed)",
            "            return gen.random(shape, np.float32)",
            "        n = np.prod(shape)",
            "        return (np.arange(n).reshape(shape) / n).astype(np.float32)",
            "",
            "    def _add_model_test(self, model_test: TestCase, kind: str) -> None:",
            "        # model is loaded at runtime, note sometimes it could even",
            "        # never loaded if the test skipped",
            "        model_marker: list[ModelProto | NodeProto | None] = [None]",
            "",
            "        def run(test_self: Any, device: str, **kwargs) -> None:  # noqa: ARG001",
            "            if model_test.url is not None and model_test.url.startswith(",
            "                \"onnx/backend/test/data/light/\"",
            "            ):",
            "                # testing local files",
            "                model_pb_path = os.path.normpath(",
            "                    os.path.join(",
            "                        os.path.dirname(__file__),",
            "                        \"..\",",
            "                        \"..\",",
            "                        \"..\",",
            "                        \"..\",",
            "                        model_test.url,",
            "                    )",
            "                )",
            "                if not os.path.exists(model_pb_path):",
            "                    raise FileNotFoundError(f\"Unable to find model {model_pb_path!r}.\")",
            "                onnx_home = os.path.expanduser(",
            "                    os.getenv(\"ONNX_HOME\", os.path.join(\"~\", \".onnx\"))",
            "                )",
            "                models_dir = os.getenv(",
            "                    \"ONNX_MODELS\", os.path.join(onnx_home, \"models\", \"light\")",
            "                )",
            "                model_dir: str = os.path.join(models_dir, model_test.model_name)",
            "                if not os.path.exists(model_dir):",
            "                    os.makedirs(model_dir)",
            "                use_dummy = True",
            "            else:",
            "                if model_test.model_dir is None:",
            "                    model_dir = self.prepare_model_data(model_test)",
            "                else:",
            "                    model_dir = model_test.model_dir",
            "                model_pb_path = os.path.join(model_dir, \"model.onnx\")",
            "                use_dummy = False",
            "",
            "            if not ONNX_ML and \"ai_onnx_ml\" in model_dir:",
            "                return",
            "",
            "            model = onnx.load(model_pb_path)",
            "            model_marker[0] = model",
            "            if (",
            "                hasattr(self.backend, \"is_compatible\")",
            "                and callable(self.backend.is_compatible)",
            "                and not self.backend.is_compatible(model)",
            "            ):",
            "                raise unittest.SkipTest(\"Not compatible with backend\")",
            "",
            "            prepared_model = self.backend.prepare(model, device, **kwargs)",
            "            assert prepared_model is not None",
            "",
            "            if use_dummy:",
            "                # When the backend test goes through a test involving a",
            "                # model stored in onnx/backend/test/data/light,",
            "                # this function generates expected output coming from",
            "                # from ReferenceEvaluator run with random inputs.",
            "                # A couple of models include many Conv operators and the",
            "                # python implementation is slow (such as test_bvlc_alexnet).",
            "                with open(model_pb_path, \"rb\") as f:",
            "                    onx = onnx.load(f)",
            "",
            "                test_data_set = os.path.join(model_dir, \"test_data_set_0\")",
            "                if not os.path.exists(test_data_set):",
            "                    os.mkdir(test_data_set)",
            "                feeds = {}",
            "                inits = {i.name for i in onx.graph.initializer}",
            "                n_input = 0",
            "                inputs = []",
            "                for i in range(len(onx.graph.input)):",
            "                    if onx.graph.input[i].name in inits:",
            "                        continue",
            "                    name = os.path.join(test_data_set, f\"input_{n_input}.pb\")",
            "                    inputs.append(name)",
            "                    n_input += 1",
            "                    x = onx.graph.input[i]",
            "                    value = self.generate_dummy_data(",
            "                        x, seed=0, name=model_test.model_name, random=False",
            "                    )",
            "                    feeds[x.name] = value",
            "                    with open(name, \"wb\") as f:",
            "                        f.write(onnx.numpy_helper.from_array(value).SerializeToString())",
            "",
            "                # loads expected output if any available",
            "                prefix = os.path.splitext(model_pb_path)[0]",
            "                expected_outputs = []",
            "                for i in range(len(onx.graph.output)):",
            "                    name = f\"{prefix}_output_{i}.pb\"",
            "                    if os.path.exists(name):",
            "                        expected_outputs.append(name)",
            "                        continue",
            "                    expected_outputs = None",
            "                    break",
            "",
            "                if expected_outputs is None:",
            "                    ref = onnx.reference.ReferenceEvaluator(onx)",
            "                    outputs = ref.run(None, feeds)",
            "                    for i, o in enumerate(outputs):",
            "                        name = os.path.join(test_data_set, f\"output_{i}.pb\")",
            "                        with open(name, \"wb\") as f:",
            "                            f.write(onnx.numpy_helper.from_array(o).SerializeToString())",
            "                else:",
            "                    for i, o in enumerate(expected_outputs):",
            "                        name = os.path.join(test_data_set, f\"output_{i}.pb\")",
            "                        shutil.copy(o, name)",
            "            else:",
            "                # TODO after converting all npz files to protobuf, we can delete this.",
            "                for test_data_npz in glob.glob(",
            "                    os.path.join(model_dir, \"test_data_*.npz\")",
            "                ):",
            "                    test_data = np.load(test_data_npz, encoding=\"bytes\")",
            "                    inputs = list(test_data[\"inputs\"])",
            "                    outputs = list(prepared_model.run(inputs))",
            "                    ref_outputs = tuple(",
            "                        np.array(x) if not isinstance(x, (list, dict)) else x",
            "                        for f in test_data[\"outputs\"]",
            "                    )",
            "                    self.assert_similar_outputs(",
            "                        ref_outputs,",
            "                        outputs,",
            "                        rtol=kwargs.get(\"rtol\", model_test.rtol),",
            "                        atol=kwargs.get(\"atol\", model_test.atol),",
            "                        model_dir=model_dir,",
            "                    )",
            "",
            "            for test_data_dir in glob.glob(os.path.join(model_dir, \"test_data_set*\")):",
            "                inputs = []",
            "                inputs_num = len(glob.glob(os.path.join(test_data_dir, \"input_*.pb\")))",
            "                for i in range(inputs_num):",
            "                    input_file = os.path.join(test_data_dir, f\"input_{i}.pb\")",
            "                    self._load_proto(input_file, inputs, model.graph.input[i].type)",
            "                ref_outputs = []",
            "                ref_outputs_num = len(",
            "                    glob.glob(os.path.join(test_data_dir, \"output_*.pb\"))",
            "                )",
            "                for i in range(ref_outputs_num):",
            "                    output_file = os.path.join(test_data_dir, f\"output_{i}.pb\")",
            "                    self._load_proto(",
            "                        output_file, ref_outputs, model.graph.output[i].type",
            "                    )",
            "                outputs = list(prepared_model.run(inputs))",
            "                self.assert_similar_outputs(",
            "                    ref_outputs,",
            "                    outputs,",
            "                    rtol=kwargs.get(\"rtol\", model_test.rtol),",
            "                    atol=kwargs.get(\"atol\", model_test.atol),",
            "                    model_dir=model_dir,",
            "                )",
            "",
            "        if model_test.name in self._test_kwargs:",
            "            self._add_test(",
            "                kind + \"Model\",",
            "                model_test.name,",
            "                run,",
            "                model_marker,",
            "                **self._test_kwargs[model_test.name],",
            "            )",
            "        else:",
            "            self._add_test(kind + \"Model\", model_test.name, run, model_marker)",
            "",
            "    def _load_proto(",
            "        self,",
            "        proto_filename: str,",
            "        target_list: list[np.ndarray | list[Any]],",
            "        model_type_proto: TypeProto,",
            "    ) -> None:",
            "        with open(proto_filename, \"rb\") as f:",
            "            protobuf_content = f.read()",
            "            if model_type_proto.HasField(\"sequence_type\"):",
            "                sequence = onnx.SequenceProto()",
            "                sequence.ParseFromString(protobuf_content)",
            "                target_list.append(numpy_helper.to_list(sequence))",
            "            elif model_type_proto.HasField(\"tensor_type\"):",
            "                tensor = onnx.TensorProto()",
            "                tensor.ParseFromString(protobuf_content)",
            "                t = numpy_helper.to_array(tensor)",
            "                assert isinstance(t, np.ndarray)",
            "                target_list.append(t)",
            "            elif model_type_proto.HasField(\"optional_type\"):",
            "                optional = onnx.OptionalProto()",
            "                optional.ParseFromString(protobuf_content)",
            "                target_list.append(numpy_helper.to_optional(optional))  # type: ignore[arg-type]",
            "            else:",
            "                print(",
            "                    \"Loading proto of that specific type (Map/Sparse Tensor) is currently not supported\"",
            "                )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "13": [],
            "245": [
                "Runner",
                "download_model"
            ],
            "246": [
                "Runner",
                "download_model"
            ]
        },
        "addLocation": []
    },
    "onnx/hub.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " import json"
            },
            "1": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " import os"
            },
            "2": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " import sys"
            },
            "3": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import tarfile"
            },
            "4": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " from io import BytesIO"
            },
            "5": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " from os.path import join"
            },
            "6": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " from typing import IO, Any, Dict, List, cast"
            },
            "7": {
                "beforePatchRowNumber": 290,
                "afterPatchRowNumber": 289,
                "PatchRowcode": "     return onnx.load(cast(IO[bytes], BytesIO(model_bytes)))"
            },
            "8": {
                "beforePatchRowNumber": 291,
                "afterPatchRowNumber": 290,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 292,
                "afterPatchRowNumber": 291,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 293,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def _tar_members_filter(tar: tarfile.TarFile, base: str) -> list[tarfile.TarInfo]:"
            },
            "11": {
                "beforePatchRowNumber": 294,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    \"\"\"Check that the content of ``tar`` will be extracted safely"
            },
            "12": {
                "beforePatchRowNumber": 295,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "13": {
                "beforePatchRowNumber": 296,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    Args:"
            },
            "14": {
                "beforePatchRowNumber": 297,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        tar: The tarball file"
            },
            "15": {
                "beforePatchRowNumber": 298,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        base: The directory where the tarball will be extracted"
            },
            "16": {
                "beforePatchRowNumber": 299,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "17": {
                "beforePatchRowNumber": 300,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    Returns:"
            },
            "18": {
                "beforePatchRowNumber": 301,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        list of tarball members"
            },
            "19": {
                "beforePatchRowNumber": 302,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    \"\"\""
            },
            "20": {
                "beforePatchRowNumber": 303,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    result = []"
            },
            "21": {
                "beforePatchRowNumber": 304,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    for member in tar:"
            },
            "22": {
                "beforePatchRowNumber": 305,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        member_path = os.path.join(base, member.name)"
            },
            "23": {
                "beforePatchRowNumber": 306,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        abs_base = os.path.abspath(base)"
            },
            "24": {
                "beforePatchRowNumber": 307,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        abs_member = os.path.abspath(member_path)"
            },
            "25": {
                "beforePatchRowNumber": 308,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not abs_member.startswith(abs_base):"
            },
            "26": {
                "beforePatchRowNumber": 309,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            raise RuntimeError("
            },
            "27": {
                "beforePatchRowNumber": 310,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                f\"The tarball member {member_path} in downloading model contains \""
            },
            "28": {
                "beforePatchRowNumber": 311,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                f\"directory traversal sequence which may contain harmful payload.\""
            },
            "29": {
                "beforePatchRowNumber": 312,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "30": {
                "beforePatchRowNumber": 313,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        elif member.issym() or member.islnk():"
            },
            "31": {
                "beforePatchRowNumber": 314,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            raise RuntimeError("
            },
            "32": {
                "beforePatchRowNumber": 315,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                f\"The tarball member {member_path} in downloading model contains \""
            },
            "33": {
                "beforePatchRowNumber": 316,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                f\"symbolic links which may contain harmful payload.\""
            },
            "34": {
                "beforePatchRowNumber": 317,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "35": {
                "beforePatchRowNumber": 318,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        result.append(member)"
            },
            "36": {
                "beforePatchRowNumber": 319,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    return result"
            },
            "37": {
                "beforePatchRowNumber": 320,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "38": {
                "beforePatchRowNumber": 321,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "39": {
                "beforePatchRowNumber": 322,
                "afterPatchRowNumber": 292,
                "PatchRowcode": " def download_model_with_test_data("
            },
            "40": {
                "beforePatchRowNumber": 323,
                "afterPatchRowNumber": 293,
                "PatchRowcode": "     model: str,"
            },
            "41": {
                "beforePatchRowNumber": 324,
                "afterPatchRowNumber": 294,
                "PatchRowcode": "     repo: str = \"onnx/models:main\","
            },
            "42": {
                "beforePatchRowNumber": 393,
                "afterPatchRowNumber": 363,
                "PatchRowcode": "                 \"download the model from the model hub.\""
            },
            "43": {
                "beforePatchRowNumber": 394,
                "afterPatchRowNumber": 364,
                "PatchRowcode": "             )"
            },
            "44": {
                "beforePatchRowNumber": 395,
                "afterPatchRowNumber": 365,
                "PatchRowcode": " "
            },
            "45": {
                "beforePatchRowNumber": 396,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    with tarfile.open(local_model_with_data_path) as model_with_data_zipped:"
            },
            "46": {
                "beforePatchRowNumber": 397,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # FIXME: Avoid index manipulation with magic numbers"
            },
            "47": {
                "beforePatchRowNumber": 398,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        local_model_with_data_dir_path = local_model_with_data_path["
            },
            "48": {
                "beforePatchRowNumber": 399,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            0 : len(local_model_with_data_path) - 7"
            },
            "49": {
                "beforePatchRowNumber": 400,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        ]"
            },
            "50": {
                "beforePatchRowNumber": 401,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # Mitigate tarball directory traversal risks"
            },
            "51": {
                "beforePatchRowNumber": 402,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if hasattr(tarfile, \"data_filter\"):"
            },
            "52": {
                "beforePatchRowNumber": 403,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            model_with_data_zipped.extractall("
            },
            "53": {
                "beforePatchRowNumber": 404,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                path=local_model_with_data_dir_path, filter=\"data\""
            },
            "54": {
                "beforePatchRowNumber": 405,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "55": {
                "beforePatchRowNumber": 406,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        else:"
            },
            "56": {
                "beforePatchRowNumber": 407,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            model_with_data_zipped.extractall("
            },
            "57": {
                "beforePatchRowNumber": 408,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                path=local_model_with_data_dir_path,"
            },
            "58": {
                "beforePatchRowNumber": 409,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                members=_tar_members_filter("
            },
            "59": {
                "beforePatchRowNumber": 410,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    model_with_data_zipped, local_model_with_data_dir_path"
            },
            "60": {
                "beforePatchRowNumber": 411,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                ),"
            },
            "61": {
                "beforePatchRowNumber": 412,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 366,
                "PatchRowcode": "+    # FIXME: Avoid index manipulation with magic numbers,"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 367,
                "PatchRowcode": "+    # remove \".tar.gz\""
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 368,
                "PatchRowcode": "+    local_model_with_data_dir_path = local_model_with_data_path["
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 369,
                "PatchRowcode": "+        0 : len(local_model_with_data_path) - 7"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 370,
                "PatchRowcode": "+    ]"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 371,
                "PatchRowcode": "+    onnx.utils._extract_model_safe("
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 372,
                "PatchRowcode": "+        local_model_with_data_path, local_model_with_data_dir_path"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 373,
                "PatchRowcode": "+    )"
            },
            "70": {
                "beforePatchRowNumber": 413,
                "afterPatchRowNumber": 374,
                "PatchRowcode": "     model_with_data_path = ("
            },
            "71": {
                "beforePatchRowNumber": 414,
                "afterPatchRowNumber": 375,
                "PatchRowcode": "         local_model_with_data_dir_path"
            },
            "72": {
                "beforePatchRowNumber": 415,
                "afterPatchRowNumber": 376,
                "PatchRowcode": "         + \"/\""
            }
        },
        "frontPatchFile": [
            "# Copyright (c) ONNX Project Contributors",
            "#",
            "# SPDX-License-Identifier: Apache-2.0",
            "\"\"\"ONNX Model Hub",
            "",
            "This implements the python client for the ONNX model hub.",
            "\"\"\"",
            "from __future__ import annotations",
            "",
            "import hashlib",
            "import json",
            "import os",
            "import sys",
            "import tarfile",
            "from io import BytesIO",
            "from os.path import join",
            "from typing import IO, Any, Dict, List, cast",
            "from urllib.error import HTTPError",
            "from urllib.request import urlopen",
            "",
            "import onnx",
            "",
            "if \"ONNX_HOME\" in os.environ:",
            "    _ONNX_HUB_DIR = join(os.environ[\"ONNX_HOME\"], \"hub\")",
            "elif \"XDG_CACHE_HOME\" in os.environ:",
            "    _ONNX_HUB_DIR = join(os.environ[\"XDG_CACHE_HOME\"], \"onnx\", \"hub\")",
            "else:",
            "    _ONNX_HUB_DIR = join(os.path.expanduser(\"~\"), \".cache\", \"onnx\", \"hub\")",
            "",
            "",
            "class ModelInfo:",
            "    \"\"\"A class to represent a model's property and metadata in the ONNX Hub.",
            "    It extracts model name, path, sha, tags, etc. from the passed in raw_model_info dict.",
            "",
            "    Attributes:",
            "        model: The name of the model.",
            "        model_path: The path to the model, relative to the model zoo (https://github.com/onnx/models/) repo root.",
            "        metadata: Additional metadata of the model, such as the size of the model, IO ports, etc.",
            "        model_sha: The SHA256 digest of the model file.",
            "        tags: A set of tags associated with the model.",
            "        opset: The opset version of the model.",
            "    \"\"\"",
            "",
            "    def __init__(self, raw_model_info: dict[str, Any]) -> None:",
            "        \"\"\"Initializer.",
            "",
            "        Args:",
            "            raw_model_info: A JSON dict containing the model info.",
            "        \"\"\"",
            "        self.model = cast(str, raw_model_info[\"model\"])",
            "",
            "        self.model_path = cast(str, raw_model_info[\"model_path\"])",
            "        self.metadata: dict[str, Any] = cast(Dict[str, Any], raw_model_info[\"metadata\"])",
            "        self.model_sha: str | None = None",
            "        if \"model_sha\" in self.metadata:",
            "            self.model_sha = cast(str, self.metadata[\"model_sha\"])",
            "",
            "        self.tags: set[str] = set()",
            "        if \"tags\" in self.metadata:",
            "            self.tags = set(cast(List[str], self.metadata[\"tags\"]))",
            "",
            "        self.opset = cast(int, raw_model_info[\"opset_version\"])",
            "        self.raw_model_info: dict[str, Any] = raw_model_info",
            "",
            "    def __str__(self) -> str:",
            "        return f\"ModelInfo(model={self.model}, opset={self.opset}, path={self.model_path}, metadata={self.metadata})\"",
            "",
            "    def __repr__(self) -> str:",
            "        return self.__str__()",
            "",
            "",
            "def set_dir(new_dir: str) -> None:",
            "    \"\"\"Sets the current ONNX hub cache location.",
            "",
            "    Args:",
            "        new_dir: Location of new model hub cache.",
            "    \"\"\"",
            "    global _ONNX_HUB_DIR  # noqa: PLW0603",
            "    _ONNX_HUB_DIR = new_dir",
            "",
            "",
            "def get_dir() -> str:",
            "    \"\"\"Gets the current ONNX hub cache location.",
            "",
            "    Returns:",
            "        The location of the ONNX hub model cache.",
            "    \"\"\"",
            "    return _ONNX_HUB_DIR",
            "",
            "",
            "def _parse_repo_info(repo: str) -> tuple[str, str, str]:",
            "    \"\"\"Gets the repo owner, name and ref from a repo specification string.\"\"\"",
            "    repo_owner = repo.split(\":\")[0].split(\"/\")[0]",
            "    repo_name = repo.split(\":\")[0].split(\"/\")[1]",
            "    if \":\" in repo:",
            "        repo_ref = repo.split(\":\")[1]",
            "    else:",
            "        repo_ref = \"main\"",
            "    return repo_owner, repo_name, repo_ref",
            "",
            "",
            "def _verify_repo_ref(repo: str) -> bool:",
            "    \"\"\"Verifies whether the given model repo can be trusted.",
            "    A model repo can be trusted if it matches onnx/models:main.",
            "    \"\"\"",
            "    repo_owner, repo_name, repo_ref = _parse_repo_info(repo)",
            "    return (repo_owner == \"onnx\") and (repo_name == \"models\") and (repo_ref == \"main\")",
            "",
            "",
            "def _get_base_url(repo: str, lfs: bool = False) -> str:",
            "    \"\"\"Gets the base github url from a repo specification string.",
            "",
            "    Args:",
            "        repo: The location of the model repo in format",
            "            \"user/repo[:branch]\". If no branch is found will default to",
            "            \"main\".",
            "        lfs: Whether the url is for downloading lfs models.",
            "",
            "    Returns:",
            "        The base github url for downloading.",
            "    \"\"\"",
            "    repo_owner, repo_name, repo_ref = _parse_repo_info(repo)",
            "",
            "    if lfs:",
            "        return f\"https://media.githubusercontent.com/media/{repo_owner}/{repo_name}/{repo_ref}/\"",
            "    return f\"https://raw.githubusercontent.com/{repo_owner}/{repo_name}/{repo_ref}/\"",
            "",
            "",
            "def _download_file(url: str, file_name: str) -> None:",
            "    \"\"\"Downloads the file with specified file_name from the url.",
            "",
            "    Args:",
            "        url: A url of download link.",
            "        file_name: A specified file name for the downloaded file.",
            "    \"\"\"",
            "    chunk_size = 16384  # 1024 * 16",
            "    with urlopen(url) as response, open(file_name, \"wb\") as f:",
            "        # Loads processively with chuck_size for huge models",
            "        while True:",
            "            chunk = response.read(chunk_size)",
            "            if not chunk:",
            "                break",
            "            f.write(chunk)",
            "",
            "",
            "def list_models(",
            "    repo: str = \"onnx/models:main\",",
            "    model: str | None = None,",
            "    tags: list[str] | None = None,",
            ") -> list[ModelInfo]:",
            "    \"\"\"Gets the list of model info consistent with a given name and tags",
            "",
            "    Args:",
            "        repo: The location of the model repo in format",
            "            \"user/repo[:branch]\". If no branch is found will default to",
            "            \"main\"",
            "        model: The name of the model to search for. If `None`, will",
            "            return all models with matching tags.",
            "        tags: A list of tags to filter models by. If `None`, will return",
            "            all models with matching name.",
            "",
            "    Returns:",
            "        ``ModelInfo``s.",
            "    \"\"\"",
            "    base_url = _get_base_url(repo)",
            "    manifest_url = base_url + \"ONNX_HUB_MANIFEST.json\"",
            "    try:",
            "        with urlopen(manifest_url) as response:",
            "            manifest: list[ModelInfo] = [",
            "                ModelInfo(info) for info in json.load(cast(IO[str], response))",
            "            ]",
            "    except HTTPError as e:",
            "        raise AssertionError(f\"Could not find manifest at {manifest_url}\") from e",
            "",
            "    # Filter by model name first.",
            "    matching_models = (",
            "        manifest",
            "        if model is None",
            "        else [m for m in manifest if m.model.lower() == model.lower()]",
            "    )",
            "",
            "    # Filter by tags",
            "    if tags is None:",
            "        return matching_models",
            "",
            "    canonical_tags = {t.lower() for t in tags}",
            "    matching_info_list: list[ModelInfo] = []",
            "    for m in matching_models:",
            "        model_tags = {t.lower() for t in m.tags}",
            "        if len(canonical_tags.intersection(model_tags)) > 0:",
            "            matching_info_list.append(m)",
            "    return matching_info_list",
            "",
            "",
            "def get_model_info(",
            "    model: str, repo: str = \"onnx/models:main\", opset: int | None = None",
            ") -> ModelInfo:",
            "    \"\"\"Gets the model info matching the given name and opset.",
            "",
            "    Args:",
            "        model: The name of the onnx model in the manifest. This field is",
            "            case-sensitive",
            "        repo: The location of the model repo in format",
            "            \"user/repo[:branch]\". If no branch is found will default to",
            "            \"main\"",
            "        opset: The opset of the model to get. The default of `None` will",
            "            return the model with largest opset.",
            "",
            "    Returns:",
            "        ``ModelInfo``.",
            "    \"\"\"",
            "    matching_models = list_models(repo, model)",
            "    if not matching_models:",
            "        raise AssertionError(f\"No models found with name {model}\")",
            "",
            "    if opset is None:",
            "        selected_models = sorted(matching_models, key=lambda m: -m.opset)",
            "    else:",
            "        selected_models = [m for m in matching_models if m.opset == opset]",
            "        if not selected_models:",
            "            valid_opsets = [m.opset for m in matching_models]",
            "            raise AssertionError(",
            "                f\"{model} has no version with opset {opset}. Valid opsets: {valid_opsets}\"",
            "            )",
            "    return selected_models[0]",
            "",
            "",
            "def load(",
            "    model: str,",
            "    repo: str = \"onnx/models:main\",",
            "    opset: int | None = None,",
            "    force_reload: bool = False,",
            "    silent: bool = False,",
            ") -> onnx.ModelProto | None:",
            "    \"\"\"Downloads a model by name from the onnx model hub.",
            "",
            "    Args:",
            "        model: The name of the onnx model in the manifest. This field is",
            "            case-sensitive",
            "        repo: The location of the model repo in format",
            "            \"user/repo[:branch]\". If no branch is found will default to",
            "            \"main\"",
            "        opset: The opset of the model to download. The default of `None`",
            "            automatically chooses the largest opset",
            "        force_reload: Whether to force the model to re-download even if",
            "            its already found in the cache",
            "        silent: Whether to suppress the warning message if the repo is",
            "            not trusted.",
            "",
            "    Returns:",
            "        ModelProto or None",
            "    \"\"\"",
            "    selected_model = get_model_info(model, repo, opset)",
            "    local_model_path_arr = selected_model.model_path.split(\"/\")",
            "    if selected_model.model_sha is not None:",
            "        local_model_path_arr[-1] = (",
            "            f\"{selected_model.model_sha}_{local_model_path_arr[-1]}\"",
            "        )",
            "    local_model_path = join(_ONNX_HUB_DIR, os.sep.join(local_model_path_arr))",
            "",
            "    if force_reload or not os.path.exists(local_model_path):",
            "        if not _verify_repo_ref(repo) and not silent:",
            "            msg = f\"The model repo specification {repo} is not trusted and may contain security vulnerabilities. Only continue if you trust this repo.\"",
            "",
            "            print(msg, file=sys.stderr)",
            "            print(\"Continue?[y/n]\")",
            "            if input().lower() != \"y\":",
            "                return None",
            "",
            "        os.makedirs(os.path.dirname(local_model_path), exist_ok=True)",
            "        lfs_url = _get_base_url(repo, True)",
            "        print(f\"Downloading {model} to local path {local_model_path}\")",
            "        _download_file(lfs_url + selected_model.model_path, local_model_path)",
            "    else:",
            "        print(f\"Using cached {model} model from {local_model_path}\")",
            "",
            "    with open(local_model_path, \"rb\") as f:",
            "        model_bytes = f.read()",
            "",
            "    if selected_model.model_sha is not None:",
            "        downloaded_sha = hashlib.sha256(model_bytes).hexdigest()",
            "        if not downloaded_sha == selected_model.model_sha:",
            "            raise AssertionError(",
            "                f\"The cached model {selected_model.model} has SHA256 {downloaded_sha} \"",
            "                f\"while checksum should be {selected_model.model_sha}. \"",
            "                \"The model in the hub may have been updated. Use force_reload to \"",
            "                \"download the model from the model hub.\"",
            "            )",
            "",
            "    return onnx.load(cast(IO[bytes], BytesIO(model_bytes)))",
            "",
            "",
            "def _tar_members_filter(tar: tarfile.TarFile, base: str) -> list[tarfile.TarInfo]:",
            "    \"\"\"Check that the content of ``tar`` will be extracted safely",
            "",
            "    Args:",
            "        tar: The tarball file",
            "        base: The directory where the tarball will be extracted",
            "",
            "    Returns:",
            "        list of tarball members",
            "    \"\"\"",
            "    result = []",
            "    for member in tar:",
            "        member_path = os.path.join(base, member.name)",
            "        abs_base = os.path.abspath(base)",
            "        abs_member = os.path.abspath(member_path)",
            "        if not abs_member.startswith(abs_base):",
            "            raise RuntimeError(",
            "                f\"The tarball member {member_path} in downloading model contains \"",
            "                f\"directory traversal sequence which may contain harmful payload.\"",
            "            )",
            "        elif member.issym() or member.islnk():",
            "            raise RuntimeError(",
            "                f\"The tarball member {member_path} in downloading model contains \"",
            "                f\"symbolic links which may contain harmful payload.\"",
            "            )",
            "        result.append(member)",
            "    return result",
            "",
            "",
            "def download_model_with_test_data(",
            "    model: str,",
            "    repo: str = \"onnx/models:main\",",
            "    opset: int | None = None,",
            "    force_reload: bool = False,",
            "    silent: bool = False,",
            ") -> str | None:",
            "    \"\"\"Downloads a model along with test data by name from the onnx model hub and returns the directory to which the files have been extracted.",
            "    Users are responsible for making sure the model comes from a trusted source, and the data is safe to be extracted.",
            "",
            "    Args:",
            "        model: The name of the onnx model in the manifest. This field is",
            "            case-sensitive",
            "        repo: The location of the model repo in format",
            "            \"user/repo[:branch]\". If no branch is found will default to",
            "            \"main\"",
            "        opset: The opset of the model to download. The default of `None`",
            "            automatically chooses the largest opset",
            "        force_reload: Whether to force the model to re-download even if",
            "            its already found in the cache",
            "        silent: Whether to suppress the warning message if the repo is",
            "            not trusted.",
            "",
            "    Returns:",
            "        str or None",
            "    \"\"\"",
            "    selected_model = get_model_info(model, repo, opset)",
            "",
            "    local_model_with_data_path_arr = selected_model.metadata[",
            "        \"model_with_data_path\"",
            "    ].split(\"/\")",
            "",
            "    model_with_data_sha = selected_model.metadata[\"model_with_data_sha\"]",
            "",
            "    if model_with_data_sha is not None:",
            "        local_model_with_data_path_arr[-1] = (",
            "            f\"{model_with_data_sha}_{local_model_with_data_path_arr[-1]}\"",
            "        )",
            "    local_model_with_data_path = join(",
            "        _ONNX_HUB_DIR, os.sep.join(local_model_with_data_path_arr)",
            "    )",
            "",
            "    if force_reload or not os.path.exists(local_model_with_data_path):",
            "        if not _verify_repo_ref(repo) and not silent:",
            "            msg = f\"The model repo specification {repo} is not trusted and may contain security vulnerabilities. Only continue if you trust this repo.\"",
            "",
            "            print(msg, file=sys.stderr)",
            "            print(\"Continue?[y/n]\")",
            "            if input().lower() != \"y\":",
            "                return None",
            "",
            "        os.makedirs(os.path.dirname(local_model_with_data_path), exist_ok=True)",
            "        lfs_url = _get_base_url(repo, True)",
            "        print(f\"Downloading {model} to local path {local_model_with_data_path}\")",
            "        _download_file(",
            "            lfs_url + selected_model.metadata[\"model_with_data_path\"],",
            "            local_model_with_data_path,",
            "        )",
            "    else:",
            "        print(f\"Using cached {model} model from {local_model_with_data_path}\")",
            "",
            "    with open(local_model_with_data_path, \"rb\") as f:",
            "        model_with_data_bytes = f.read()",
            "",
            "    if model_with_data_sha is not None:",
            "        downloaded_sha = hashlib.sha256(model_with_data_bytes).hexdigest()",
            "        if not downloaded_sha == model_with_data_sha:",
            "            raise AssertionError(",
            "                f\"The cached model {selected_model.model} has SHA256 {downloaded_sha} \"",
            "                f\"while checksum should be {model_with_data_sha}. \"",
            "                \"The model in the hub may have been updated. Use force_reload to \"",
            "                \"download the model from the model hub.\"",
            "            )",
            "",
            "    with tarfile.open(local_model_with_data_path) as model_with_data_zipped:",
            "        # FIXME: Avoid index manipulation with magic numbers",
            "        local_model_with_data_dir_path = local_model_with_data_path[",
            "            0 : len(local_model_with_data_path) - 7",
            "        ]",
            "        # Mitigate tarball directory traversal risks",
            "        if hasattr(tarfile, \"data_filter\"):",
            "            model_with_data_zipped.extractall(",
            "                path=local_model_with_data_dir_path, filter=\"data\"",
            "            )",
            "        else:",
            "            model_with_data_zipped.extractall(",
            "                path=local_model_with_data_dir_path,",
            "                members=_tar_members_filter(",
            "                    model_with_data_zipped, local_model_with_data_dir_path",
            "                ),",
            "            )",
            "    model_with_data_path = (",
            "        local_model_with_data_dir_path",
            "        + \"/\"",
            "        + os.listdir(local_model_with_data_dir_path)[0]",
            "    )",
            "",
            "    return model_with_data_path",
            "",
            "",
            "def load_composite_model(",
            "    network_model: str,",
            "    preprocessing_model: str,",
            "    network_repo: str = \"onnx/models:main\",",
            "    preprocessing_repo: str = \"onnx/models:main\",",
            "    opset: int | None = None,",
            "    force_reload: bool = False,",
            "    silent: bool = False,",
            ") -> onnx.ModelProto | None:",
            "    \"\"\"Builds a composite model including data preprocessing by downloading a network and a preprocessing model",
            "    and combine it into a single model",
            "",
            "    Args:",
            "        network_model: The name of the onnx model in the manifest.",
            "        preprocessing_model: The name of the preprocessing model.",
            "        network_repo: The location of the model repo in format",
            "            \"user/repo[:branch]\". If no branch is found will default to",
            "            \"main\"",
            "        preprocessing_repo: The location of the proprocessing model repo in format",
            "            \"user/repo[:branch]\". If no branch is found will default to",
            "            \"main\"",
            "        opset: The opset of the model to download. The default of `None`",
            "            automatically chooses the largest opset",
            "        force_reload: Whether to force the model to re-download even if",
            "            its already found in the cache",
            "        silent: Whether to suppress the warning message if the repo is",
            "            not trusted.",
            "",
            "    Returns:",
            "        ModelProto or None",
            "    \"\"\"",
            "    preprocessing = load(",
            "        preprocessing_model, preprocessing_repo, opset, force_reload, silent",
            "    )",
            "    if preprocessing is None:",
            "        raise RuntimeError(",
            "            f\"Could not load the preprocessing model: {preprocessing_model}\"",
            "        )",
            "    network = load(network_model, network_repo, opset, force_reload, silent)",
            "    if network is None:",
            "        raise RuntimeError(f\"Could not load the network model: {network_model}\")",
            "",
            "    all_domains: set[str] = set()",
            "    domains_to_version_network: dict[str, int] = {}",
            "    domains_to_version_preprocessing: dict[str, int] = {}",
            "",
            "    for opset_import_entry in network.opset_import:",
            "        domain = (",
            "            \"ai.onnx\" if opset_import_entry.domain == \"\" else opset_import_entry.domain",
            "        )",
            "        all_domains.add(domain)",
            "        domains_to_version_network[domain] = opset_import_entry.version",
            "",
            "    for opset_import_entry in preprocessing.opset_import:",
            "        domain = (",
            "            \"ai.onnx\" if opset_import_entry.domain == \"\" else opset_import_entry.domain",
            "        )",
            "        all_domains.add(domain)",
            "        domains_to_version_preprocessing[domain] = opset_import_entry.version",
            "",
            "    preprocessing_opset_version = -1",
            "    network_opset_version = -1",
            "    for domain in all_domains:",
            "        if domain == \"ai.onnx\":",
            "            preprocessing_opset_version = domains_to_version_preprocessing[domain]",
            "            network_opset_version = domains_to_version_network[domain]",
            "        elif (",
            "            domain in domains_to_version_preprocessing",
            "            and domain in domains_to_version_network",
            "            and domains_to_version_preprocessing[domain]",
            "            != domains_to_version_preprocessing[domain]",
            "        ):",
            "            raise ValueError(",
            "                f\"Can not merge {preprocessing_model} and {network_model} because they contain \"",
            "                f\"different opset versions for domain {domain} ({domains_to_version_preprocessing[domain]}) \"",
            "                f\"and {domains_to_version_network[domain]}). Only the default domain can be \"",
            "                \"automatically converted to the highest version of the two.\"",
            "            )",
            "    if preprocessing_opset_version > network_opset_version:",
            "        network = onnx.version_converter.convert_version(",
            "            network, preprocessing_opset_version",
            "        )",
            "        network.ir_version = preprocessing.ir_version",
            "        onnx.checker.check_model(network)",
            "    elif network_opset_version > preprocessing_opset_version:",
            "        preprocessing = onnx.version_converter.convert_version(",
            "            preprocessing, network_opset_version",
            "        )",
            "        preprocessing.ir_version = network.ir_version",
            "        onnx.checker.check_model(preprocessing)",
            "",
            "    io_map = [",
            "        (out_entry.name, in_entry.name)",
            "        for out_entry, in_entry in zip(preprocessing.graph.output, network.graph.input)",
            "    ]",
            "",
            "    model_with_preprocessing = onnx.compose.merge_models(",
            "        preprocessing, network, io_map=io_map",
            "    )",
            "    return model_with_preprocessing"
        ],
        "afterPatchFile": [
            "# Copyright (c) ONNX Project Contributors",
            "#",
            "# SPDX-License-Identifier: Apache-2.0",
            "\"\"\"ONNX Model Hub",
            "",
            "This implements the python client for the ONNX model hub.",
            "\"\"\"",
            "from __future__ import annotations",
            "",
            "import hashlib",
            "import json",
            "import os",
            "import sys",
            "from io import BytesIO",
            "from os.path import join",
            "from typing import IO, Any, Dict, List, cast",
            "from urllib.error import HTTPError",
            "from urllib.request import urlopen",
            "",
            "import onnx",
            "",
            "if \"ONNX_HOME\" in os.environ:",
            "    _ONNX_HUB_DIR = join(os.environ[\"ONNX_HOME\"], \"hub\")",
            "elif \"XDG_CACHE_HOME\" in os.environ:",
            "    _ONNX_HUB_DIR = join(os.environ[\"XDG_CACHE_HOME\"], \"onnx\", \"hub\")",
            "else:",
            "    _ONNX_HUB_DIR = join(os.path.expanduser(\"~\"), \".cache\", \"onnx\", \"hub\")",
            "",
            "",
            "class ModelInfo:",
            "    \"\"\"A class to represent a model's property and metadata in the ONNX Hub.",
            "    It extracts model name, path, sha, tags, etc. from the passed in raw_model_info dict.",
            "",
            "    Attributes:",
            "        model: The name of the model.",
            "        model_path: The path to the model, relative to the model zoo (https://github.com/onnx/models/) repo root.",
            "        metadata: Additional metadata of the model, such as the size of the model, IO ports, etc.",
            "        model_sha: The SHA256 digest of the model file.",
            "        tags: A set of tags associated with the model.",
            "        opset: The opset version of the model.",
            "    \"\"\"",
            "",
            "    def __init__(self, raw_model_info: dict[str, Any]) -> None:",
            "        \"\"\"Initializer.",
            "",
            "        Args:",
            "            raw_model_info: A JSON dict containing the model info.",
            "        \"\"\"",
            "        self.model = cast(str, raw_model_info[\"model\"])",
            "",
            "        self.model_path = cast(str, raw_model_info[\"model_path\"])",
            "        self.metadata: dict[str, Any] = cast(Dict[str, Any], raw_model_info[\"metadata\"])",
            "        self.model_sha: str | None = None",
            "        if \"model_sha\" in self.metadata:",
            "            self.model_sha = cast(str, self.metadata[\"model_sha\"])",
            "",
            "        self.tags: set[str] = set()",
            "        if \"tags\" in self.metadata:",
            "            self.tags = set(cast(List[str], self.metadata[\"tags\"]))",
            "",
            "        self.opset = cast(int, raw_model_info[\"opset_version\"])",
            "        self.raw_model_info: dict[str, Any] = raw_model_info",
            "",
            "    def __str__(self) -> str:",
            "        return f\"ModelInfo(model={self.model}, opset={self.opset}, path={self.model_path}, metadata={self.metadata})\"",
            "",
            "    def __repr__(self) -> str:",
            "        return self.__str__()",
            "",
            "",
            "def set_dir(new_dir: str) -> None:",
            "    \"\"\"Sets the current ONNX hub cache location.",
            "",
            "    Args:",
            "        new_dir: Location of new model hub cache.",
            "    \"\"\"",
            "    global _ONNX_HUB_DIR  # noqa: PLW0603",
            "    _ONNX_HUB_DIR = new_dir",
            "",
            "",
            "def get_dir() -> str:",
            "    \"\"\"Gets the current ONNX hub cache location.",
            "",
            "    Returns:",
            "        The location of the ONNX hub model cache.",
            "    \"\"\"",
            "    return _ONNX_HUB_DIR",
            "",
            "",
            "def _parse_repo_info(repo: str) -> tuple[str, str, str]:",
            "    \"\"\"Gets the repo owner, name and ref from a repo specification string.\"\"\"",
            "    repo_owner = repo.split(\":\")[0].split(\"/\")[0]",
            "    repo_name = repo.split(\":\")[0].split(\"/\")[1]",
            "    if \":\" in repo:",
            "        repo_ref = repo.split(\":\")[1]",
            "    else:",
            "        repo_ref = \"main\"",
            "    return repo_owner, repo_name, repo_ref",
            "",
            "",
            "def _verify_repo_ref(repo: str) -> bool:",
            "    \"\"\"Verifies whether the given model repo can be trusted.",
            "    A model repo can be trusted if it matches onnx/models:main.",
            "    \"\"\"",
            "    repo_owner, repo_name, repo_ref = _parse_repo_info(repo)",
            "    return (repo_owner == \"onnx\") and (repo_name == \"models\") and (repo_ref == \"main\")",
            "",
            "",
            "def _get_base_url(repo: str, lfs: bool = False) -> str:",
            "    \"\"\"Gets the base github url from a repo specification string.",
            "",
            "    Args:",
            "        repo: The location of the model repo in format",
            "            \"user/repo[:branch]\". If no branch is found will default to",
            "            \"main\".",
            "        lfs: Whether the url is for downloading lfs models.",
            "",
            "    Returns:",
            "        The base github url for downloading.",
            "    \"\"\"",
            "    repo_owner, repo_name, repo_ref = _parse_repo_info(repo)",
            "",
            "    if lfs:",
            "        return f\"https://media.githubusercontent.com/media/{repo_owner}/{repo_name}/{repo_ref}/\"",
            "    return f\"https://raw.githubusercontent.com/{repo_owner}/{repo_name}/{repo_ref}/\"",
            "",
            "",
            "def _download_file(url: str, file_name: str) -> None:",
            "    \"\"\"Downloads the file with specified file_name from the url.",
            "",
            "    Args:",
            "        url: A url of download link.",
            "        file_name: A specified file name for the downloaded file.",
            "    \"\"\"",
            "    chunk_size = 16384  # 1024 * 16",
            "    with urlopen(url) as response, open(file_name, \"wb\") as f:",
            "        # Loads processively with chuck_size for huge models",
            "        while True:",
            "            chunk = response.read(chunk_size)",
            "            if not chunk:",
            "                break",
            "            f.write(chunk)",
            "",
            "",
            "def list_models(",
            "    repo: str = \"onnx/models:main\",",
            "    model: str | None = None,",
            "    tags: list[str] | None = None,",
            ") -> list[ModelInfo]:",
            "    \"\"\"Gets the list of model info consistent with a given name and tags",
            "",
            "    Args:",
            "        repo: The location of the model repo in format",
            "            \"user/repo[:branch]\". If no branch is found will default to",
            "            \"main\"",
            "        model: The name of the model to search for. If `None`, will",
            "            return all models with matching tags.",
            "        tags: A list of tags to filter models by. If `None`, will return",
            "            all models with matching name.",
            "",
            "    Returns:",
            "        ``ModelInfo``s.",
            "    \"\"\"",
            "    base_url = _get_base_url(repo)",
            "    manifest_url = base_url + \"ONNX_HUB_MANIFEST.json\"",
            "    try:",
            "        with urlopen(manifest_url) as response:",
            "            manifest: list[ModelInfo] = [",
            "                ModelInfo(info) for info in json.load(cast(IO[str], response))",
            "            ]",
            "    except HTTPError as e:",
            "        raise AssertionError(f\"Could not find manifest at {manifest_url}\") from e",
            "",
            "    # Filter by model name first.",
            "    matching_models = (",
            "        manifest",
            "        if model is None",
            "        else [m for m in manifest if m.model.lower() == model.lower()]",
            "    )",
            "",
            "    # Filter by tags",
            "    if tags is None:",
            "        return matching_models",
            "",
            "    canonical_tags = {t.lower() for t in tags}",
            "    matching_info_list: list[ModelInfo] = []",
            "    for m in matching_models:",
            "        model_tags = {t.lower() for t in m.tags}",
            "        if len(canonical_tags.intersection(model_tags)) > 0:",
            "            matching_info_list.append(m)",
            "    return matching_info_list",
            "",
            "",
            "def get_model_info(",
            "    model: str, repo: str = \"onnx/models:main\", opset: int | None = None",
            ") -> ModelInfo:",
            "    \"\"\"Gets the model info matching the given name and opset.",
            "",
            "    Args:",
            "        model: The name of the onnx model in the manifest. This field is",
            "            case-sensitive",
            "        repo: The location of the model repo in format",
            "            \"user/repo[:branch]\". If no branch is found will default to",
            "            \"main\"",
            "        opset: The opset of the model to get. The default of `None` will",
            "            return the model with largest opset.",
            "",
            "    Returns:",
            "        ``ModelInfo``.",
            "    \"\"\"",
            "    matching_models = list_models(repo, model)",
            "    if not matching_models:",
            "        raise AssertionError(f\"No models found with name {model}\")",
            "",
            "    if opset is None:",
            "        selected_models = sorted(matching_models, key=lambda m: -m.opset)",
            "    else:",
            "        selected_models = [m for m in matching_models if m.opset == opset]",
            "        if not selected_models:",
            "            valid_opsets = [m.opset for m in matching_models]",
            "            raise AssertionError(",
            "                f\"{model} has no version with opset {opset}. Valid opsets: {valid_opsets}\"",
            "            )",
            "    return selected_models[0]",
            "",
            "",
            "def load(",
            "    model: str,",
            "    repo: str = \"onnx/models:main\",",
            "    opset: int | None = None,",
            "    force_reload: bool = False,",
            "    silent: bool = False,",
            ") -> onnx.ModelProto | None:",
            "    \"\"\"Downloads a model by name from the onnx model hub.",
            "",
            "    Args:",
            "        model: The name of the onnx model in the manifest. This field is",
            "            case-sensitive",
            "        repo: The location of the model repo in format",
            "            \"user/repo[:branch]\". If no branch is found will default to",
            "            \"main\"",
            "        opset: The opset of the model to download. The default of `None`",
            "            automatically chooses the largest opset",
            "        force_reload: Whether to force the model to re-download even if",
            "            its already found in the cache",
            "        silent: Whether to suppress the warning message if the repo is",
            "            not trusted.",
            "",
            "    Returns:",
            "        ModelProto or None",
            "    \"\"\"",
            "    selected_model = get_model_info(model, repo, opset)",
            "    local_model_path_arr = selected_model.model_path.split(\"/\")",
            "    if selected_model.model_sha is not None:",
            "        local_model_path_arr[-1] = (",
            "            f\"{selected_model.model_sha}_{local_model_path_arr[-1]}\"",
            "        )",
            "    local_model_path = join(_ONNX_HUB_DIR, os.sep.join(local_model_path_arr))",
            "",
            "    if force_reload or not os.path.exists(local_model_path):",
            "        if not _verify_repo_ref(repo) and not silent:",
            "            msg = f\"The model repo specification {repo} is not trusted and may contain security vulnerabilities. Only continue if you trust this repo.\"",
            "",
            "            print(msg, file=sys.stderr)",
            "            print(\"Continue?[y/n]\")",
            "            if input().lower() != \"y\":",
            "                return None",
            "",
            "        os.makedirs(os.path.dirname(local_model_path), exist_ok=True)",
            "        lfs_url = _get_base_url(repo, True)",
            "        print(f\"Downloading {model} to local path {local_model_path}\")",
            "        _download_file(lfs_url + selected_model.model_path, local_model_path)",
            "    else:",
            "        print(f\"Using cached {model} model from {local_model_path}\")",
            "",
            "    with open(local_model_path, \"rb\") as f:",
            "        model_bytes = f.read()",
            "",
            "    if selected_model.model_sha is not None:",
            "        downloaded_sha = hashlib.sha256(model_bytes).hexdigest()",
            "        if not downloaded_sha == selected_model.model_sha:",
            "            raise AssertionError(",
            "                f\"The cached model {selected_model.model} has SHA256 {downloaded_sha} \"",
            "                f\"while checksum should be {selected_model.model_sha}. \"",
            "                \"The model in the hub may have been updated. Use force_reload to \"",
            "                \"download the model from the model hub.\"",
            "            )",
            "",
            "    return onnx.load(cast(IO[bytes], BytesIO(model_bytes)))",
            "",
            "",
            "def download_model_with_test_data(",
            "    model: str,",
            "    repo: str = \"onnx/models:main\",",
            "    opset: int | None = None,",
            "    force_reload: bool = False,",
            "    silent: bool = False,",
            ") -> str | None:",
            "    \"\"\"Downloads a model along with test data by name from the onnx model hub and returns the directory to which the files have been extracted.",
            "    Users are responsible for making sure the model comes from a trusted source, and the data is safe to be extracted.",
            "",
            "    Args:",
            "        model: The name of the onnx model in the manifest. This field is",
            "            case-sensitive",
            "        repo: The location of the model repo in format",
            "            \"user/repo[:branch]\". If no branch is found will default to",
            "            \"main\"",
            "        opset: The opset of the model to download. The default of `None`",
            "            automatically chooses the largest opset",
            "        force_reload: Whether to force the model to re-download even if",
            "            its already found in the cache",
            "        silent: Whether to suppress the warning message if the repo is",
            "            not trusted.",
            "",
            "    Returns:",
            "        str or None",
            "    \"\"\"",
            "    selected_model = get_model_info(model, repo, opset)",
            "",
            "    local_model_with_data_path_arr = selected_model.metadata[",
            "        \"model_with_data_path\"",
            "    ].split(\"/\")",
            "",
            "    model_with_data_sha = selected_model.metadata[\"model_with_data_sha\"]",
            "",
            "    if model_with_data_sha is not None:",
            "        local_model_with_data_path_arr[-1] = (",
            "            f\"{model_with_data_sha}_{local_model_with_data_path_arr[-1]}\"",
            "        )",
            "    local_model_with_data_path = join(",
            "        _ONNX_HUB_DIR, os.sep.join(local_model_with_data_path_arr)",
            "    )",
            "",
            "    if force_reload or not os.path.exists(local_model_with_data_path):",
            "        if not _verify_repo_ref(repo) and not silent:",
            "            msg = f\"The model repo specification {repo} is not trusted and may contain security vulnerabilities. Only continue if you trust this repo.\"",
            "",
            "            print(msg, file=sys.stderr)",
            "            print(\"Continue?[y/n]\")",
            "            if input().lower() != \"y\":",
            "                return None",
            "",
            "        os.makedirs(os.path.dirname(local_model_with_data_path), exist_ok=True)",
            "        lfs_url = _get_base_url(repo, True)",
            "        print(f\"Downloading {model} to local path {local_model_with_data_path}\")",
            "        _download_file(",
            "            lfs_url + selected_model.metadata[\"model_with_data_path\"],",
            "            local_model_with_data_path,",
            "        )",
            "    else:",
            "        print(f\"Using cached {model} model from {local_model_with_data_path}\")",
            "",
            "    with open(local_model_with_data_path, \"rb\") as f:",
            "        model_with_data_bytes = f.read()",
            "",
            "    if model_with_data_sha is not None:",
            "        downloaded_sha = hashlib.sha256(model_with_data_bytes).hexdigest()",
            "        if not downloaded_sha == model_with_data_sha:",
            "            raise AssertionError(",
            "                f\"The cached model {selected_model.model} has SHA256 {downloaded_sha} \"",
            "                f\"while checksum should be {model_with_data_sha}. \"",
            "                \"The model in the hub may have been updated. Use force_reload to \"",
            "                \"download the model from the model hub.\"",
            "            )",
            "",
            "    # FIXME: Avoid index manipulation with magic numbers,",
            "    # remove \".tar.gz\"",
            "    local_model_with_data_dir_path = local_model_with_data_path[",
            "        0 : len(local_model_with_data_path) - 7",
            "    ]",
            "    onnx.utils._extract_model_safe(",
            "        local_model_with_data_path, local_model_with_data_dir_path",
            "    )",
            "    model_with_data_path = (",
            "        local_model_with_data_dir_path",
            "        + \"/\"",
            "        + os.listdir(local_model_with_data_dir_path)[0]",
            "    )",
            "",
            "    return model_with_data_path",
            "",
            "",
            "def load_composite_model(",
            "    network_model: str,",
            "    preprocessing_model: str,",
            "    network_repo: str = \"onnx/models:main\",",
            "    preprocessing_repo: str = \"onnx/models:main\",",
            "    opset: int | None = None,",
            "    force_reload: bool = False,",
            "    silent: bool = False,",
            ") -> onnx.ModelProto | None:",
            "    \"\"\"Builds a composite model including data preprocessing by downloading a network and a preprocessing model",
            "    and combine it into a single model",
            "",
            "    Args:",
            "        network_model: The name of the onnx model in the manifest.",
            "        preprocessing_model: The name of the preprocessing model.",
            "        network_repo: The location of the model repo in format",
            "            \"user/repo[:branch]\". If no branch is found will default to",
            "            \"main\"",
            "        preprocessing_repo: The location of the proprocessing model repo in format",
            "            \"user/repo[:branch]\". If no branch is found will default to",
            "            \"main\"",
            "        opset: The opset of the model to download. The default of `None`",
            "            automatically chooses the largest opset",
            "        force_reload: Whether to force the model to re-download even if",
            "            its already found in the cache",
            "        silent: Whether to suppress the warning message if the repo is",
            "            not trusted.",
            "",
            "    Returns:",
            "        ModelProto or None",
            "    \"\"\"",
            "    preprocessing = load(",
            "        preprocessing_model, preprocessing_repo, opset, force_reload, silent",
            "    )",
            "    if preprocessing is None:",
            "        raise RuntimeError(",
            "            f\"Could not load the preprocessing model: {preprocessing_model}\"",
            "        )",
            "    network = load(network_model, network_repo, opset, force_reload, silent)",
            "    if network is None:",
            "        raise RuntimeError(f\"Could not load the network model: {network_model}\")",
            "",
            "    all_domains: set[str] = set()",
            "    domains_to_version_network: dict[str, int] = {}",
            "    domains_to_version_preprocessing: dict[str, int] = {}",
            "",
            "    for opset_import_entry in network.opset_import:",
            "        domain = (",
            "            \"ai.onnx\" if opset_import_entry.domain == \"\" else opset_import_entry.domain",
            "        )",
            "        all_domains.add(domain)",
            "        domains_to_version_network[domain] = opset_import_entry.version",
            "",
            "    for opset_import_entry in preprocessing.opset_import:",
            "        domain = (",
            "            \"ai.onnx\" if opset_import_entry.domain == \"\" else opset_import_entry.domain",
            "        )",
            "        all_domains.add(domain)",
            "        domains_to_version_preprocessing[domain] = opset_import_entry.version",
            "",
            "    preprocessing_opset_version = -1",
            "    network_opset_version = -1",
            "    for domain in all_domains:",
            "        if domain == \"ai.onnx\":",
            "            preprocessing_opset_version = domains_to_version_preprocessing[domain]",
            "            network_opset_version = domains_to_version_network[domain]",
            "        elif (",
            "            domain in domains_to_version_preprocessing",
            "            and domain in domains_to_version_network",
            "            and domains_to_version_preprocessing[domain]",
            "            != domains_to_version_preprocessing[domain]",
            "        ):",
            "            raise ValueError(",
            "                f\"Can not merge {preprocessing_model} and {network_model} because they contain \"",
            "                f\"different opset versions for domain {domain} ({domains_to_version_preprocessing[domain]}) \"",
            "                f\"and {domains_to_version_network[domain]}). Only the default domain can be \"",
            "                \"automatically converted to the highest version of the two.\"",
            "            )",
            "    if preprocessing_opset_version > network_opset_version:",
            "        network = onnx.version_converter.convert_version(",
            "            network, preprocessing_opset_version",
            "        )",
            "        network.ir_version = preprocessing.ir_version",
            "        onnx.checker.check_model(network)",
            "    elif network_opset_version > preprocessing_opset_version:",
            "        preprocessing = onnx.version_converter.convert_version(",
            "            preprocessing, network_opset_version",
            "        )",
            "        preprocessing.ir_version = network.ir_version",
            "        onnx.checker.check_model(preprocessing)",
            "",
            "    io_map = [",
            "        (out_entry.name, in_entry.name)",
            "        for out_entry, in_entry in zip(preprocessing.graph.output, network.graph.input)",
            "    ]",
            "",
            "    model_with_preprocessing = onnx.compose.merge_models(",
            "        preprocessing, network, io_map=io_map",
            "    )",
            "    return model_with_preprocessing"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "14": [],
            "293": [
                "_tar_members_filter"
            ],
            "294": [
                "_tar_members_filter"
            ],
            "295": [
                "_tar_members_filter"
            ],
            "296": [
                "_tar_members_filter"
            ],
            "297": [
                "_tar_members_filter"
            ],
            "298": [
                "_tar_members_filter"
            ],
            "299": [
                "_tar_members_filter"
            ],
            "300": [
                "_tar_members_filter"
            ],
            "301": [
                "_tar_members_filter"
            ],
            "302": [
                "_tar_members_filter"
            ],
            "303": [
                "_tar_members_filter"
            ],
            "304": [
                "_tar_members_filter"
            ],
            "305": [
                "_tar_members_filter"
            ],
            "306": [
                "_tar_members_filter"
            ],
            "307": [
                "_tar_members_filter"
            ],
            "308": [
                "_tar_members_filter"
            ],
            "309": [
                "_tar_members_filter"
            ],
            "310": [
                "_tar_members_filter"
            ],
            "311": [
                "_tar_members_filter"
            ],
            "312": [
                "_tar_members_filter"
            ],
            "313": [
                "_tar_members_filter"
            ],
            "314": [
                "_tar_members_filter"
            ],
            "315": [
                "_tar_members_filter"
            ],
            "316": [
                "_tar_members_filter"
            ],
            "317": [
                "_tar_members_filter"
            ],
            "318": [
                "_tar_members_filter"
            ],
            "319": [
                "_tar_members_filter"
            ],
            "320": [],
            "321": [],
            "396": [
                "download_model_with_test_data"
            ],
            "397": [
                "download_model_with_test_data"
            ],
            "398": [
                "download_model_with_test_data"
            ],
            "399": [
                "download_model_with_test_data"
            ],
            "400": [
                "download_model_with_test_data"
            ],
            "401": [
                "download_model_with_test_data"
            ],
            "402": [
                "download_model_with_test_data"
            ],
            "403": [
                "download_model_with_test_data"
            ],
            "404": [
                "download_model_with_test_data"
            ],
            "405": [
                "download_model_with_test_data"
            ],
            "406": [
                "download_model_with_test_data"
            ],
            "407": [
                "download_model_with_test_data"
            ],
            "408": [
                "download_model_with_test_data"
            ],
            "409": [
                "download_model_with_test_data"
            ],
            "410": [
                "download_model_with_test_data"
            ],
            "411": [
                "download_model_with_test_data"
            ],
            "412": [
                "download_model_with_test_data"
            ]
        },
        "addLocation": []
    },
    "onnx/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " from __future__ import annotations"
            },
            "1": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " import os"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 7,
                "PatchRowcode": "+import tarfile"
            },
            "4": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " import onnx.checker"
            },
            "6": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " import onnx.helper"
            },
            "7": {
                "beforePatchRowNumber": 232,
                "afterPatchRowNumber": 233,
                "PatchRowcode": "     onnx.save(extracted, output_path)"
            },
            "8": {
                "beforePatchRowNumber": 233,
                "afterPatchRowNumber": 234,
                "PatchRowcode": "     if check_model:"
            },
            "9": {
                "beforePatchRowNumber": 234,
                "afterPatchRowNumber": 235,
                "PatchRowcode": "         onnx.checker.check_model(output_path)"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 236,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 237,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 238,
                "PatchRowcode": "+def _tar_members_filter("
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 239,
                "PatchRowcode": "+    tar: tarfile.TarFile, base: str | os.PathLike"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 240,
                "PatchRowcode": "+) -> list[tarfile.TarInfo]:"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 241,
                "PatchRowcode": "+    \"\"\"Check that the content of ``tar`` will be extracted safely"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 242,
                "PatchRowcode": "+"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 243,
                "PatchRowcode": "+    Args:"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 244,
                "PatchRowcode": "+        tar: The tarball file"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 245,
                "PatchRowcode": "+        base: The directory where the tarball will be extracted"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 246,
                "PatchRowcode": "+"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 247,
                "PatchRowcode": "+    Returns:"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 248,
                "PatchRowcode": "+        list of tarball members"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 249,
                "PatchRowcode": "+    \"\"\""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 250,
                "PatchRowcode": "+    result = []"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 251,
                "PatchRowcode": "+    for member in tar:"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 252,
                "PatchRowcode": "+        member_path = os.path.join(base, member.name)"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 253,
                "PatchRowcode": "+        abs_base = os.path.abspath(base)"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 254,
                "PatchRowcode": "+        abs_member = os.path.abspath(member_path)"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 255,
                "PatchRowcode": "+        if not abs_member.startswith(abs_base):"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 256,
                "PatchRowcode": "+            raise RuntimeError("
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 257,
                "PatchRowcode": "+                f\"The tarball member {member_path} in downloading model contains \""
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 258,
                "PatchRowcode": "+                f\"directory traversal sequence which may contain harmful payload.\""
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 259,
                "PatchRowcode": "+            )"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 260,
                "PatchRowcode": "+        elif member.issym() or member.islnk():"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 261,
                "PatchRowcode": "+            raise RuntimeError("
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 262,
                "PatchRowcode": "+                f\"The tarball member {member_path} in downloading model contains \""
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 263,
                "PatchRowcode": "+                f\"symbolic links which may contain harmful payload.\""
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 264,
                "PatchRowcode": "+            )"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 265,
                "PatchRowcode": "+        result.append(member)"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 266,
                "PatchRowcode": "+    return result"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 267,
                "PatchRowcode": "+"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 268,
                "PatchRowcode": "+"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 269,
                "PatchRowcode": "+def _extract_model_safe("
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 270,
                "PatchRowcode": "+    model_tar_path: str | os.PathLike, local_model_with_data_dir_path: str | os.PathLike"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 271,
                "PatchRowcode": "+) -> None:"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 272,
                "PatchRowcode": "+    \"\"\"Safely extracts a tar file to a specified directory."
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 273,
                "PatchRowcode": "+"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 274,
                "PatchRowcode": "+    This function ensures that the extraction process mitigates against"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 275,
                "PatchRowcode": "+    directory traversal vulnerabilities by validating or sanitizing paths"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 276,
                "PatchRowcode": "+    within the tar file. It also provides compatibility for different versions"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 277,
                "PatchRowcode": "+    of the tarfile module by checking for the availability of certain attributes"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 278,
                "PatchRowcode": "+    or methods before invoking them."
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 279,
                "PatchRowcode": "+"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 280,
                "PatchRowcode": "+    Args:"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 281,
                "PatchRowcode": "+        model_tar_path: The path to the tar file to be extracted."
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 282,
                "PatchRowcode": "+        local_model_with_data_dir_path: The directory path where the tar file"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 283,
                "PatchRowcode": "+      contents will be extracted to."
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 284,
                "PatchRowcode": "+    \"\"\""
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 285,
                "PatchRowcode": "+    with tarfile.open(model_tar_path) as model_with_data_zipped:"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 286,
                "PatchRowcode": "+        # Mitigate tarball directory traversal risks"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 287,
                "PatchRowcode": "+        if hasattr(tarfile, \"data_filter\"):"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 288,
                "PatchRowcode": "+            model_with_data_zipped.extractall("
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 289,
                "PatchRowcode": "+                path=local_model_with_data_dir_path, filter=\"data\""
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 290,
                "PatchRowcode": "+            )"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 291,
                "PatchRowcode": "+        else:"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 292,
                "PatchRowcode": "+            model_with_data_zipped.extractall("
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 293,
                "PatchRowcode": "+                path=local_model_with_data_dir_path,"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 294,
                "PatchRowcode": "+                members=_tar_members_filter("
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 295,
                "PatchRowcode": "+                    model_with_data_zipped, local_model_with_data_dir_path"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 296,
                "PatchRowcode": "+                ),"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 297,
                "PatchRowcode": "+            )"
            }
        },
        "frontPatchFile": [
            "# Copyright (c) ONNX Project Contributors",
            "#",
            "# SPDX-License-Identifier: Apache-2.0",
            "from __future__ import annotations",
            "",
            "import os",
            "",
            "import onnx.checker",
            "import onnx.helper",
            "import onnx.shape_inference",
            "from onnx import FunctionProto, ModelProto, NodeProto, TensorProto, ValueInfoProto",
            "",
            "",
            "class Extractor:",
            "    def __init__(self, model: ModelProto) -> None:",
            "        self.model = onnx.shape_inference.infer_shapes(model)",
            "        self.graph = self.model.graph",
            "        self.wmap = self._build_name2obj_dict(self.graph.initializer)",
            "        self.vimap = self._build_name2obj_dict(self.graph.value_info)",
            "",
            "    @staticmethod",
            "    def _build_name2obj_dict(objs):  # type: ignore",
            "        return {obj.name: obj for obj in objs}",
            "",
            "    def _collect_new_io_core(self, original_io, io_names_to_extract):  # type: ignore",
            "        original_io_map = self._build_name2obj_dict(original_io)",
            "        original_io_names = set(original_io_map)",
            "        s_io_names_to_extract = set(io_names_to_extract)",
            "        io_names_to_keep = s_io_names_to_extract & original_io_names",
            "        new_io_names_to_add = s_io_names_to_extract - original_io_names",
            "",
            "        new_io_tensors = [original_io_map[name] for name in io_names_to_keep]",
            "        # activation become input or output",
            "        new_io_tensors.extend(self.vimap[name] for name in new_io_names_to_add)",
            "",
            "        # adjust sequence",
            "        new_io_tensors_map = self._build_name2obj_dict(new_io_tensors)",
            "        return [new_io_tensors_map[name] for name in io_names_to_extract]",
            "",
            "    def _collect_new_inputs(self, names: list[str]) -> list[ValueInfoProto]:",
            "        return self._collect_new_io_core(self.graph.input, names)  # type: ignore",
            "",
            "    def _collect_new_outputs(self, names: list[str]) -> list[ValueInfoProto]:",
            "        return self._collect_new_io_core(self.graph.output, names)  # type: ignore",
            "",
            "    def _dfs_search_reachable_nodes(",
            "        self,",
            "        node_output_name: str,",
            "        graph_input_names: set[str],",
            "        nodes: list[NodeProto],",
            "        reachable: set[int],",
            "        unreachable: set[int],",
            "    ) -> None:",
            "        \"\"\"Helper function to find nodes which are connected to an output",
            "",
            "        Arguments:",
            "            node_output_name (str): The name of the output",
            "            graph_input_names (set of string): The names of all inputs of the graph",
            "            nodes (list of nodes): The list of all nodes of the graph",
            "            reachable (set of int): The set of indexes to reachable nodes in `nodes`",
            "            unreachable (set of int): The set of indexes to unreachable nodes in `nodes`",
            "        \"\"\"",
            "        # finish search at inputs",
            "        if node_output_name in graph_input_names:",
            "            return",
            "",
            "        # find nodes connected to this output",
            "        nodes_to_search = [",
            "            index for index in unreachable if node_output_name in nodes[index].output",
            "        ]",
            "",
            "        # add nodes connected to this output to sets",
            "        for node_index in nodes_to_search:",
            "            reachable.add(node_index)",
            "            unreachable.remove(node_index)",
            "",
            "        # recurse on inputs",
            "        for node_index in nodes_to_search:",
            "            for name in nodes[node_index].input:",
            "                self._dfs_search_reachable_nodes(",
            "                    name, graph_input_names, nodes, reachable, unreachable",
            "                )",
            "",
            "    def _collect_reachable_nodes(",
            "        self,",
            "        input_names: list[str],",
            "        output_names: list[str],",
            "    ) -> list[NodeProto]:",
            "        _input_names = set(input_names)",
            "        nodes = list(self.graph.node)",
            "        reachable: set[int] = set()",
            "        unreachable: set[int] = set(range(len(nodes)))",
            "        for name in output_names:",
            "            self._dfs_search_reachable_nodes(",
            "                name, _input_names, nodes, reachable, unreachable",
            "            )",
            "        # needs to be topologically sorted",
            "        nodes = [nodes[node_index] for node_index in sorted(reachable)]",
            "        return nodes",
            "",
            "    def _collect_referred_local_functions(",
            "        self,",
            "        nodes,  # type: list[NodeProto]",
            "    ):  # type: (...) -> list[FunctionProto]",
            "        # a node in a model graph may refer a function.",
            "        # a function contains nodes, some of which may in turn refer a function.",
            "        # we need to find functions referred by graph nodes and",
            "        # by nodes used to define functions.",
            "        def find_referred_funcs(nodes, referred_local_functions):  # type: ignore",
            "            new_nodes = []  # type: list[NodeProto]",
            "            for node in nodes:",
            "                # check if the node is a function op",
            "                match_function = next(",
            "                    (",
            "                        f",
            "                        for f in self.model.functions",
            "                        if f.name == node.op_type and f.domain == node.domain",
            "                    ),",
            "                    None,",
            "                )",
            "                if match_function and match_function not in referred_local_functions:",
            "                    referred_local_functions.append(match_function)",
            "                    new_nodes.extend(match_function.node)",
            "",
            "            return new_nodes",
            "",
            "        referred_local_functions = []  # type: list[FunctionProto]",
            "        new_nodes = find_referred_funcs(nodes, referred_local_functions)",
            "        while new_nodes:",
            "            new_nodes = find_referred_funcs(new_nodes, referred_local_functions)",
            "",
            "        return referred_local_functions",
            "",
            "    def _collect_reachable_tensors(",
            "        self,",
            "        nodes: list[NodeProto],",
            "    ) -> tuple[list[TensorProto], list[ValueInfoProto]]:",
            "        all_tensors_names: set[str] = set()",
            "",
            "        for node in nodes:",
            "            all_tensors_names.update(node.input)",
            "            all_tensors_names.update(node.output)",
            "",
            "        initializer = [self.wmap[t] for t in self.wmap if t in all_tensors_names]",
            "        value_info = [self.vimap[t] for t in self.vimap if t in all_tensors_names]",
            "        len_sparse_initializer = len(self.graph.sparse_initializer)",
            "        if len_sparse_initializer != 0:",
            "            raise ValueError(",
            "                f\"len_sparse_initializer is {len_sparse_initializer}, it must be 0.\"",
            "            )",
            "        len_quantization_annotation = len(self.graph.quantization_annotation)",
            "        if len_quantization_annotation != 0:",
            "            raise ValueError(",
            "                f\"len_quantization_annotation is {len_quantization_annotation}, it must be 0.\"",
            "            )",
            "        return initializer, value_info",
            "",
            "    def _make_model(",
            "        self,",
            "        nodes: list[NodeProto],",
            "        inputs: list[ValueInfoProto],",
            "        outputs: list[ValueInfoProto],",
            "        initializer: list[TensorProto],",
            "        value_info: list[ValueInfoProto],",
            "        local_functions: list[FunctionProto],",
            "    ) -> ModelProto:",
            "        name = \"Extracted from {\" + self.graph.name + \"}\"",
            "        graph = onnx.helper.make_graph(",
            "            nodes, name, inputs, outputs, initializer=initializer, value_info=value_info",
            "        )",
            "",
            "        meta = {",
            "            \"ir_version\": self.model.ir_version,",
            "            \"opset_imports\": self.model.opset_import,",
            "            \"producer_name\": \"onnx.utils.extract_model\",",
            "            \"functions\": local_functions,",
            "        }",
            "        return onnx.helper.make_model(graph, **meta)",
            "",
            "    def extract_model(",
            "        self,",
            "        input_names: list[str],",
            "        output_names: list[str],",
            "    ) -> ModelProto:",
            "        inputs = self._collect_new_inputs(input_names)",
            "        outputs = self._collect_new_outputs(output_names)",
            "        nodes = self._collect_reachable_nodes(input_names, output_names)",
            "        initializer, value_info = self._collect_reachable_tensors(nodes)",
            "        local_functions = self._collect_referred_local_functions(nodes)",
            "        model = self._make_model(",
            "            nodes, inputs, outputs, initializer, value_info, local_functions",
            "        )",
            "",
            "        return model",
            "",
            "",
            "def extract_model(",
            "    input_path: str | os.PathLike,",
            "    output_path: str | os.PathLike,",
            "    input_names: list[str],",
            "    output_names: list[str],",
            "    check_model: bool = True,",
            ") -> None:",
            "    \"\"\"Extracts sub-model from an ONNX model.",
            "",
            "    The sub-model is defined by the names of the input and output tensors *exactly*.",
            "",
            "    Note: For control-flow operators, e.g. If and Loop, the _boundary of sub-model_,",
            "    which is defined by the input and output tensors, should not _cut through_ the",
            "    subgraph that is connected to the _main graph_ as attributes of these operators.",
            "",
            "    Arguments:",
            "        input_path (str | os.PathLike): The path to original ONNX model.",
            "        output_path (str | os.PathLike): The path to save the extracted ONNX model.",
            "        input_names (list of string): The names of the input tensors that to be extracted.",
            "        output_names (list of string): The names of the output tensors that to be extracted.",
            "        check_model (bool): Whether to run model checker on the extracted model.",
            "    \"\"\"",
            "    if not os.path.exists(input_path):",
            "        raise ValueError(f\"Invalid input model path: {input_path}\")",
            "    if not output_path:",
            "        raise ValueError(\"Output model path shall not be empty!\")",
            "    if not output_names:",
            "        raise ValueError(\"Output tensor names shall not be empty!\")",
            "",
            "    onnx.checker.check_model(input_path)",
            "    model = onnx.load(input_path)",
            "",
            "    e = Extractor(model)",
            "    extracted = e.extract_model(input_names, output_names)",
            "",
            "    onnx.save(extracted, output_path)",
            "    if check_model:",
            "        onnx.checker.check_model(output_path)"
        ],
        "afterPatchFile": [
            "# Copyright (c) ONNX Project Contributors",
            "#",
            "# SPDX-License-Identifier: Apache-2.0",
            "from __future__ import annotations",
            "",
            "import os",
            "import tarfile",
            "",
            "import onnx.checker",
            "import onnx.helper",
            "import onnx.shape_inference",
            "from onnx import FunctionProto, ModelProto, NodeProto, TensorProto, ValueInfoProto",
            "",
            "",
            "class Extractor:",
            "    def __init__(self, model: ModelProto) -> None:",
            "        self.model = onnx.shape_inference.infer_shapes(model)",
            "        self.graph = self.model.graph",
            "        self.wmap = self._build_name2obj_dict(self.graph.initializer)",
            "        self.vimap = self._build_name2obj_dict(self.graph.value_info)",
            "",
            "    @staticmethod",
            "    def _build_name2obj_dict(objs):  # type: ignore",
            "        return {obj.name: obj for obj in objs}",
            "",
            "    def _collect_new_io_core(self, original_io, io_names_to_extract):  # type: ignore",
            "        original_io_map = self._build_name2obj_dict(original_io)",
            "        original_io_names = set(original_io_map)",
            "        s_io_names_to_extract = set(io_names_to_extract)",
            "        io_names_to_keep = s_io_names_to_extract & original_io_names",
            "        new_io_names_to_add = s_io_names_to_extract - original_io_names",
            "",
            "        new_io_tensors = [original_io_map[name] for name in io_names_to_keep]",
            "        # activation become input or output",
            "        new_io_tensors.extend(self.vimap[name] for name in new_io_names_to_add)",
            "",
            "        # adjust sequence",
            "        new_io_tensors_map = self._build_name2obj_dict(new_io_tensors)",
            "        return [new_io_tensors_map[name] for name in io_names_to_extract]",
            "",
            "    def _collect_new_inputs(self, names: list[str]) -> list[ValueInfoProto]:",
            "        return self._collect_new_io_core(self.graph.input, names)  # type: ignore",
            "",
            "    def _collect_new_outputs(self, names: list[str]) -> list[ValueInfoProto]:",
            "        return self._collect_new_io_core(self.graph.output, names)  # type: ignore",
            "",
            "    def _dfs_search_reachable_nodes(",
            "        self,",
            "        node_output_name: str,",
            "        graph_input_names: set[str],",
            "        nodes: list[NodeProto],",
            "        reachable: set[int],",
            "        unreachable: set[int],",
            "    ) -> None:",
            "        \"\"\"Helper function to find nodes which are connected to an output",
            "",
            "        Arguments:",
            "            node_output_name (str): The name of the output",
            "            graph_input_names (set of string): The names of all inputs of the graph",
            "            nodes (list of nodes): The list of all nodes of the graph",
            "            reachable (set of int): The set of indexes to reachable nodes in `nodes`",
            "            unreachable (set of int): The set of indexes to unreachable nodes in `nodes`",
            "        \"\"\"",
            "        # finish search at inputs",
            "        if node_output_name in graph_input_names:",
            "            return",
            "",
            "        # find nodes connected to this output",
            "        nodes_to_search = [",
            "            index for index in unreachable if node_output_name in nodes[index].output",
            "        ]",
            "",
            "        # add nodes connected to this output to sets",
            "        for node_index in nodes_to_search:",
            "            reachable.add(node_index)",
            "            unreachable.remove(node_index)",
            "",
            "        # recurse on inputs",
            "        for node_index in nodes_to_search:",
            "            for name in nodes[node_index].input:",
            "                self._dfs_search_reachable_nodes(",
            "                    name, graph_input_names, nodes, reachable, unreachable",
            "                )",
            "",
            "    def _collect_reachable_nodes(",
            "        self,",
            "        input_names: list[str],",
            "        output_names: list[str],",
            "    ) -> list[NodeProto]:",
            "        _input_names = set(input_names)",
            "        nodes = list(self.graph.node)",
            "        reachable: set[int] = set()",
            "        unreachable: set[int] = set(range(len(nodes)))",
            "        for name in output_names:",
            "            self._dfs_search_reachable_nodes(",
            "                name, _input_names, nodes, reachable, unreachable",
            "            )",
            "        # needs to be topologically sorted",
            "        nodes = [nodes[node_index] for node_index in sorted(reachable)]",
            "        return nodes",
            "",
            "    def _collect_referred_local_functions(",
            "        self,",
            "        nodes,  # type: list[NodeProto]",
            "    ):  # type: (...) -> list[FunctionProto]",
            "        # a node in a model graph may refer a function.",
            "        # a function contains nodes, some of which may in turn refer a function.",
            "        # we need to find functions referred by graph nodes and",
            "        # by nodes used to define functions.",
            "        def find_referred_funcs(nodes, referred_local_functions):  # type: ignore",
            "            new_nodes = []  # type: list[NodeProto]",
            "            for node in nodes:",
            "                # check if the node is a function op",
            "                match_function = next(",
            "                    (",
            "                        f",
            "                        for f in self.model.functions",
            "                        if f.name == node.op_type and f.domain == node.domain",
            "                    ),",
            "                    None,",
            "                )",
            "                if match_function and match_function not in referred_local_functions:",
            "                    referred_local_functions.append(match_function)",
            "                    new_nodes.extend(match_function.node)",
            "",
            "            return new_nodes",
            "",
            "        referred_local_functions = []  # type: list[FunctionProto]",
            "        new_nodes = find_referred_funcs(nodes, referred_local_functions)",
            "        while new_nodes:",
            "            new_nodes = find_referred_funcs(new_nodes, referred_local_functions)",
            "",
            "        return referred_local_functions",
            "",
            "    def _collect_reachable_tensors(",
            "        self,",
            "        nodes: list[NodeProto],",
            "    ) -> tuple[list[TensorProto], list[ValueInfoProto]]:",
            "        all_tensors_names: set[str] = set()",
            "",
            "        for node in nodes:",
            "            all_tensors_names.update(node.input)",
            "            all_tensors_names.update(node.output)",
            "",
            "        initializer = [self.wmap[t] for t in self.wmap if t in all_tensors_names]",
            "        value_info = [self.vimap[t] for t in self.vimap if t in all_tensors_names]",
            "        len_sparse_initializer = len(self.graph.sparse_initializer)",
            "        if len_sparse_initializer != 0:",
            "            raise ValueError(",
            "                f\"len_sparse_initializer is {len_sparse_initializer}, it must be 0.\"",
            "            )",
            "        len_quantization_annotation = len(self.graph.quantization_annotation)",
            "        if len_quantization_annotation != 0:",
            "            raise ValueError(",
            "                f\"len_quantization_annotation is {len_quantization_annotation}, it must be 0.\"",
            "            )",
            "        return initializer, value_info",
            "",
            "    def _make_model(",
            "        self,",
            "        nodes: list[NodeProto],",
            "        inputs: list[ValueInfoProto],",
            "        outputs: list[ValueInfoProto],",
            "        initializer: list[TensorProto],",
            "        value_info: list[ValueInfoProto],",
            "        local_functions: list[FunctionProto],",
            "    ) -> ModelProto:",
            "        name = \"Extracted from {\" + self.graph.name + \"}\"",
            "        graph = onnx.helper.make_graph(",
            "            nodes, name, inputs, outputs, initializer=initializer, value_info=value_info",
            "        )",
            "",
            "        meta = {",
            "            \"ir_version\": self.model.ir_version,",
            "            \"opset_imports\": self.model.opset_import,",
            "            \"producer_name\": \"onnx.utils.extract_model\",",
            "            \"functions\": local_functions,",
            "        }",
            "        return onnx.helper.make_model(graph, **meta)",
            "",
            "    def extract_model(",
            "        self,",
            "        input_names: list[str],",
            "        output_names: list[str],",
            "    ) -> ModelProto:",
            "        inputs = self._collect_new_inputs(input_names)",
            "        outputs = self._collect_new_outputs(output_names)",
            "        nodes = self._collect_reachable_nodes(input_names, output_names)",
            "        initializer, value_info = self._collect_reachable_tensors(nodes)",
            "        local_functions = self._collect_referred_local_functions(nodes)",
            "        model = self._make_model(",
            "            nodes, inputs, outputs, initializer, value_info, local_functions",
            "        )",
            "",
            "        return model",
            "",
            "",
            "def extract_model(",
            "    input_path: str | os.PathLike,",
            "    output_path: str | os.PathLike,",
            "    input_names: list[str],",
            "    output_names: list[str],",
            "    check_model: bool = True,",
            ") -> None:",
            "    \"\"\"Extracts sub-model from an ONNX model.",
            "",
            "    The sub-model is defined by the names of the input and output tensors *exactly*.",
            "",
            "    Note: For control-flow operators, e.g. If and Loop, the _boundary of sub-model_,",
            "    which is defined by the input and output tensors, should not _cut through_ the",
            "    subgraph that is connected to the _main graph_ as attributes of these operators.",
            "",
            "    Arguments:",
            "        input_path (str | os.PathLike): The path to original ONNX model.",
            "        output_path (str | os.PathLike): The path to save the extracted ONNX model.",
            "        input_names (list of string): The names of the input tensors that to be extracted.",
            "        output_names (list of string): The names of the output tensors that to be extracted.",
            "        check_model (bool): Whether to run model checker on the extracted model.",
            "    \"\"\"",
            "    if not os.path.exists(input_path):",
            "        raise ValueError(f\"Invalid input model path: {input_path}\")",
            "    if not output_path:",
            "        raise ValueError(\"Output model path shall not be empty!\")",
            "    if not output_names:",
            "        raise ValueError(\"Output tensor names shall not be empty!\")",
            "",
            "    onnx.checker.check_model(input_path)",
            "    model = onnx.load(input_path)",
            "",
            "    e = Extractor(model)",
            "    extracted = e.extract_model(input_names, output_names)",
            "",
            "    onnx.save(extracted, output_path)",
            "    if check_model:",
            "        onnx.checker.check_model(output_path)",
            "",
            "",
            "def _tar_members_filter(",
            "    tar: tarfile.TarFile, base: str | os.PathLike",
            ") -> list[tarfile.TarInfo]:",
            "    \"\"\"Check that the content of ``tar`` will be extracted safely",
            "",
            "    Args:",
            "        tar: The tarball file",
            "        base: The directory where the tarball will be extracted",
            "",
            "    Returns:",
            "        list of tarball members",
            "    \"\"\"",
            "    result = []",
            "    for member in tar:",
            "        member_path = os.path.join(base, member.name)",
            "        abs_base = os.path.abspath(base)",
            "        abs_member = os.path.abspath(member_path)",
            "        if not abs_member.startswith(abs_base):",
            "            raise RuntimeError(",
            "                f\"The tarball member {member_path} in downloading model contains \"",
            "                f\"directory traversal sequence which may contain harmful payload.\"",
            "            )",
            "        elif member.issym() or member.islnk():",
            "            raise RuntimeError(",
            "                f\"The tarball member {member_path} in downloading model contains \"",
            "                f\"symbolic links which may contain harmful payload.\"",
            "            )",
            "        result.append(member)",
            "    return result",
            "",
            "",
            "def _extract_model_safe(",
            "    model_tar_path: str | os.PathLike, local_model_with_data_dir_path: str | os.PathLike",
            ") -> None:",
            "    \"\"\"Safely extracts a tar file to a specified directory.",
            "",
            "    This function ensures that the extraction process mitigates against",
            "    directory traversal vulnerabilities by validating or sanitizing paths",
            "    within the tar file. It also provides compatibility for different versions",
            "    of the tarfile module by checking for the availability of certain attributes",
            "    or methods before invoking them.",
            "",
            "    Args:",
            "        model_tar_path: The path to the tar file to be extracted.",
            "        local_model_with_data_dir_path: The directory path where the tar file",
            "      contents will be extracted to.",
            "    \"\"\"",
            "    with tarfile.open(model_tar_path) as model_with_data_zipped:",
            "        # Mitigate tarball directory traversal risks",
            "        if hasattr(tarfile, \"data_filter\"):",
            "            model_with_data_zipped.extractall(",
            "                path=local_model_with_data_dir_path, filter=\"data\"",
            "            )",
            "        else:",
            "            model_with_data_zipped.extractall(",
            "                path=local_model_with_data_dir_path,",
            "                members=_tar_members_filter(",
            "                    model_with_data_zipped, local_model_with_data_dir_path",
            "                ),",
            "            )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "modoboa.admin.views.identity"
        ]
    }
}