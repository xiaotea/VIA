{
    "synapse/app/generic_worker.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 266,
                "afterPatchRowNumber": 266,
                "PatchRowcode": "         super().__init__(hs)"
            },
            "1": {
                "beforePatchRowNumber": 267,
                "afterPatchRowNumber": 267,
                "PatchRowcode": "         self.hs = hs"
            },
            "2": {
                "beforePatchRowNumber": 268,
                "afterPatchRowNumber": 268,
                "PatchRowcode": "         self.is_mine_id = hs.is_mine_id"
            },
            "3": {
                "beforePatchRowNumber": 269,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.http_client = hs.get_simple_http_client()"
            },
            "4": {
                "beforePatchRowNumber": 270,
                "afterPatchRowNumber": 269,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 271,
                "afterPatchRowNumber": 270,
                "PatchRowcode": "         self._presence_enabled = hs.config.use_presence"
            },
            "6": {
                "beforePatchRowNumber": 272,
                "afterPatchRowNumber": 271,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "#!/usr/bin/env python",
            "# -*- coding: utf-8 -*-",
            "# Copyright 2016 OpenMarket Ltd",
            "# Copyright 2020 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import contextlib",
            "import logging",
            "import sys",
            "from typing import Dict, Iterable, Optional, Set",
            "",
            "from typing_extensions import ContextManager",
            "",
            "from twisted.internet import address, reactor",
            "",
            "import synapse",
            "import synapse.events",
            "from synapse.api.errors import HttpResponseException, RequestSendFailed, SynapseError",
            "from synapse.api.urls import (",
            "    CLIENT_API_PREFIX,",
            "    FEDERATION_PREFIX,",
            "    LEGACY_MEDIA_PREFIX,",
            "    MEDIA_PREFIX,",
            "    SERVER_KEY_V2_PREFIX,",
            ")",
            "from synapse.app import _base",
            "from synapse.config._base import ConfigError",
            "from synapse.config.homeserver import HomeServerConfig",
            "from synapse.config.logger import setup_logging",
            "from synapse.config.server import ListenerConfig",
            "from synapse.federation import send_queue",
            "from synapse.federation.transport.server import TransportLayerServer",
            "from synapse.handlers.presence import (",
            "    BasePresenceHandler,",
            "    PresenceState,",
            "    get_interested_parties,",
            ")",
            "from synapse.http.server import JsonResource, OptionsResource",
            "from synapse.http.servlet import RestServlet, parse_json_object_from_request",
            "from synapse.http.site import SynapseSite",
            "from synapse.logging.context import LoggingContext",
            "from synapse.metrics import METRICS_PREFIX, MetricsResource, RegistryProxy",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.replication.http import REPLICATION_PREFIX, ReplicationRestResource",
            "from synapse.replication.http.presence import (",
            "    ReplicationBumpPresenceActiveTime,",
            "    ReplicationPresenceSetState,",
            ")",
            "from synapse.replication.slave.storage._base import BaseSlavedStore",
            "from synapse.replication.slave.storage.account_data import SlavedAccountDataStore",
            "from synapse.replication.slave.storage.appservice import SlavedApplicationServiceStore",
            "from synapse.replication.slave.storage.client_ips import SlavedClientIpStore",
            "from synapse.replication.slave.storage.deviceinbox import SlavedDeviceInboxStore",
            "from synapse.replication.slave.storage.devices import SlavedDeviceStore",
            "from synapse.replication.slave.storage.directory import DirectoryStore",
            "from synapse.replication.slave.storage.events import SlavedEventStore",
            "from synapse.replication.slave.storage.filtering import SlavedFilteringStore",
            "from synapse.replication.slave.storage.groups import SlavedGroupServerStore",
            "from synapse.replication.slave.storage.keys import SlavedKeyStore",
            "from synapse.replication.slave.storage.presence import SlavedPresenceStore",
            "from synapse.replication.slave.storage.profile import SlavedProfileStore",
            "from synapse.replication.slave.storage.push_rule import SlavedPushRuleStore",
            "from synapse.replication.slave.storage.pushers import SlavedPusherStore",
            "from synapse.replication.slave.storage.receipts import SlavedReceiptsStore",
            "from synapse.replication.slave.storage.registration import SlavedRegistrationStore",
            "from synapse.replication.slave.storage.room import RoomStore",
            "from synapse.replication.slave.storage.transactions import SlavedTransactionStore",
            "from synapse.replication.tcp.client import ReplicationDataHandler",
            "from synapse.replication.tcp.commands import ClearUserSyncsCommand",
            "from synapse.replication.tcp.streams import (",
            "    AccountDataStream,",
            "    DeviceListsStream,",
            "    GroupServerStream,",
            "    PresenceStream,",
            "    PushersStream,",
            "    PushRulesStream,",
            "    ReceiptsStream,",
            "    TagAccountDataStream,",
            "    ToDeviceStream,",
            ")",
            "from synapse.rest.admin import register_servlets_for_media_repo",
            "from synapse.rest.client.v1 import events",
            "from synapse.rest.client.v1.initial_sync import InitialSyncRestServlet",
            "from synapse.rest.client.v1.login import LoginRestServlet",
            "from synapse.rest.client.v1.profile import (",
            "    ProfileAvatarURLRestServlet,",
            "    ProfileDisplaynameRestServlet,",
            "    ProfileRestServlet,",
            ")",
            "from synapse.rest.client.v1.push_rule import PushRuleRestServlet",
            "from synapse.rest.client.v1.room import (",
            "    JoinedRoomMemberListRestServlet,",
            "    JoinRoomAliasServlet,",
            "    PublicRoomListRestServlet,",
            "    RoomEventContextServlet,",
            "    RoomInitialSyncRestServlet,",
            "    RoomMemberListRestServlet,",
            "    RoomMembershipRestServlet,",
            "    RoomMessageListRestServlet,",
            "    RoomSendEventRestServlet,",
            "    RoomStateEventRestServlet,",
            "    RoomStateRestServlet,",
            "    RoomTypingRestServlet,",
            ")",
            "from synapse.rest.client.v1.voip import VoipRestServlet",
            "from synapse.rest.client.v2_alpha import groups, sync, user_directory",
            "from synapse.rest.client.v2_alpha._base import client_patterns",
            "from synapse.rest.client.v2_alpha.account import ThreepidRestServlet",
            "from synapse.rest.client.v2_alpha.account_data import (",
            "    AccountDataServlet,",
            "    RoomAccountDataServlet,",
            ")",
            "from synapse.rest.client.v2_alpha.keys import KeyChangesServlet, KeyQueryServlet",
            "from synapse.rest.client.v2_alpha.register import RegisterRestServlet",
            "from synapse.rest.client.versions import VersionsRestServlet",
            "from synapse.rest.health import HealthResource",
            "from synapse.rest.key.v2 import KeyApiV2Resource",
            "from synapse.server import HomeServer, cache_in_self",
            "from synapse.storage.databases.main.censor_events import CensorEventsStore",
            "from synapse.storage.databases.main.client_ips import ClientIpWorkerStore",
            "from synapse.storage.databases.main.media_repository import MediaRepositoryStore",
            "from synapse.storage.databases.main.metrics import ServerMetricsStore",
            "from synapse.storage.databases.main.monthly_active_users import (",
            "    MonthlyActiveUsersWorkerStore,",
            ")",
            "from synapse.storage.databases.main.presence import UserPresenceState",
            "from synapse.storage.databases.main.search import SearchWorkerStore",
            "from synapse.storage.databases.main.stats import StatsStore",
            "from synapse.storage.databases.main.transactions import TransactionWorkerStore",
            "from synapse.storage.databases.main.ui_auth import UIAuthWorkerStore",
            "from synapse.storage.databases.main.user_directory import UserDirectoryStore",
            "from synapse.types import ReadReceipt",
            "from synapse.util.async_helpers import Linearizer",
            "from synapse.util.httpresourcetree import create_resource_tree",
            "from synapse.util.manhole import manhole",
            "from synapse.util.versionstring import get_version_string",
            "",
            "logger = logging.getLogger(\"synapse.app.generic_worker\")",
            "",
            "",
            "class PresenceStatusStubServlet(RestServlet):",
            "    \"\"\"If presence is disabled this servlet can be used to stub out setting",
            "    presence status.",
            "    \"\"\"",
            "",
            "    PATTERNS = client_patterns(\"/presence/(?P<user_id>[^/]*)/status\")",
            "",
            "    def __init__(self, hs):",
            "        super().__init__()",
            "        self.auth = hs.get_auth()",
            "",
            "    async def on_GET(self, request, user_id):",
            "        await self.auth.get_user_by_req(request)",
            "        return 200, {\"presence\": \"offline\"}",
            "",
            "    async def on_PUT(self, request, user_id):",
            "        await self.auth.get_user_by_req(request)",
            "        return 200, {}",
            "",
            "",
            "class KeyUploadServlet(RestServlet):",
            "    \"\"\"An implementation of the `KeyUploadServlet` that responds to read only",
            "    requests, but otherwise proxies through to the master instance.",
            "    \"\"\"",
            "",
            "    PATTERNS = client_patterns(\"/keys/upload(/(?P<device_id>[^/]+))?$\")",
            "",
            "    def __init__(self, hs):",
            "        \"\"\"",
            "        Args:",
            "            hs (synapse.server.HomeServer): server",
            "        \"\"\"",
            "        super().__init__()",
            "        self.auth = hs.get_auth()",
            "        self.store = hs.get_datastore()",
            "        self.http_client = hs.get_simple_http_client()",
            "        self.main_uri = hs.config.worker_main_http_uri",
            "",
            "    async def on_POST(self, request, device_id):",
            "        requester = await self.auth.get_user_by_req(request, allow_guest=True)",
            "        user_id = requester.user.to_string()",
            "        body = parse_json_object_from_request(request)",
            "",
            "        if device_id is not None:",
            "            # passing the device_id here is deprecated; however, we allow it",
            "            # for now for compatibility with older clients.",
            "            if requester.device_id is not None and device_id != requester.device_id:",
            "                logger.warning(",
            "                    \"Client uploading keys for a different device \"",
            "                    \"(logged in as %s, uploading for %s)\",",
            "                    requester.device_id,",
            "                    device_id,",
            "                )",
            "        else:",
            "            device_id = requester.device_id",
            "",
            "        if device_id is None:",
            "            raise SynapseError(",
            "                400, \"To upload keys, you must pass device_id when authenticating\"",
            "            )",
            "",
            "        if body:",
            "            # They're actually trying to upload something, proxy to main synapse.",
            "",
            "            # Proxy headers from the original request, such as the auth headers",
            "            # (in case the access token is there) and the original IP /",
            "            # User-Agent of the request.",
            "            headers = {",
            "                header: request.requestHeaders.getRawHeaders(header, [])",
            "                for header in (b\"Authorization\", b\"User-Agent\")",
            "            }",
            "            # Add the previous hop the the X-Forwarded-For header.",
            "            x_forwarded_for = request.requestHeaders.getRawHeaders(",
            "                b\"X-Forwarded-For\", []",
            "            )",
            "            if isinstance(request.client, (address.IPv4Address, address.IPv6Address)):",
            "                previous_host = request.client.host.encode(\"ascii\")",
            "                # If the header exists, add to the comma-separated list of the first",
            "                # instance of the header. Otherwise, generate a new header.",
            "                if x_forwarded_for:",
            "                    x_forwarded_for = [",
            "                        x_forwarded_for[0] + b\", \" + previous_host",
            "                    ] + x_forwarded_for[1:]",
            "                else:",
            "                    x_forwarded_for = [previous_host]",
            "            headers[b\"X-Forwarded-For\"] = x_forwarded_for",
            "",
            "            try:",
            "                result = await self.http_client.post_json_get_json(",
            "                    self.main_uri + request.uri.decode(\"ascii\"), body, headers=headers",
            "                )",
            "            except HttpResponseException as e:",
            "                raise e.to_synapse_error() from e",
            "            except RequestSendFailed as e:",
            "                raise SynapseError(502, \"Failed to talk to master\") from e",
            "",
            "            return 200, result",
            "        else:",
            "            # Just interested in counts.",
            "            result = await self.store.count_e2e_one_time_keys(user_id, device_id)",
            "            return 200, {\"one_time_key_counts\": result}",
            "",
            "",
            "class _NullContextManager(ContextManager[None]):",
            "    \"\"\"A context manager which does nothing.\"\"\"",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        pass",
            "",
            "",
            "UPDATE_SYNCING_USERS_MS = 10 * 1000",
            "",
            "",
            "class GenericWorkerPresence(BasePresenceHandler):",
            "    def __init__(self, hs):",
            "        super().__init__(hs)",
            "        self.hs = hs",
            "        self.is_mine_id = hs.is_mine_id",
            "        self.http_client = hs.get_simple_http_client()",
            "",
            "        self._presence_enabled = hs.config.use_presence",
            "",
            "        # The number of ongoing syncs on this process, by user id.",
            "        # Empty if _presence_enabled is false.",
            "        self._user_to_num_current_syncs = {}  # type: Dict[str, int]",
            "",
            "        self.notifier = hs.get_notifier()",
            "        self.instance_id = hs.get_instance_id()",
            "",
            "        # user_id -> last_sync_ms. Lists the users that have stopped syncing",
            "        # but we haven't notified the master of that yet",
            "        self.users_going_offline = {}",
            "",
            "        self._bump_active_client = ReplicationBumpPresenceActiveTime.make_client(hs)",
            "        self._set_state_client = ReplicationPresenceSetState.make_client(hs)",
            "",
            "        self._send_stop_syncing_loop = self.clock.looping_call(",
            "            self.send_stop_syncing, UPDATE_SYNCING_USERS_MS",
            "        )",
            "",
            "        hs.get_reactor().addSystemEventTrigger(",
            "            \"before\",",
            "            \"shutdown\",",
            "            run_as_background_process,",
            "            \"generic_presence.on_shutdown\",",
            "            self._on_shutdown,",
            "        )",
            "",
            "    def _on_shutdown(self):",
            "        if self._presence_enabled:",
            "            self.hs.get_tcp_replication().send_command(",
            "                ClearUserSyncsCommand(self.instance_id)",
            "            )",
            "",
            "    def send_user_sync(self, user_id, is_syncing, last_sync_ms):",
            "        if self._presence_enabled:",
            "            self.hs.get_tcp_replication().send_user_sync(",
            "                self.instance_id, user_id, is_syncing, last_sync_ms",
            "            )",
            "",
            "    def mark_as_coming_online(self, user_id):",
            "        \"\"\"A user has started syncing. Send a UserSync to the master, unless they",
            "        had recently stopped syncing.",
            "",
            "        Args:",
            "            user_id (str)",
            "        \"\"\"",
            "        going_offline = self.users_going_offline.pop(user_id, None)",
            "        if not going_offline:",
            "            # Safe to skip because we haven't yet told the master they were offline",
            "            self.send_user_sync(user_id, True, self.clock.time_msec())",
            "",
            "    def mark_as_going_offline(self, user_id):",
            "        \"\"\"A user has stopped syncing. We wait before notifying the master as",
            "        its likely they'll come back soon. This allows us to avoid sending",
            "        a stopped syncing immediately followed by a started syncing notification",
            "        to the master",
            "",
            "        Args:",
            "            user_id (str)",
            "        \"\"\"",
            "        self.users_going_offline[user_id] = self.clock.time_msec()",
            "",
            "    def send_stop_syncing(self):",
            "        \"\"\"Check if there are any users who have stopped syncing a while ago",
            "        and haven't come back yet. If there are poke the master about them.",
            "        \"\"\"",
            "        now = self.clock.time_msec()",
            "        for user_id, last_sync_ms in list(self.users_going_offline.items()):",
            "            if now - last_sync_ms > UPDATE_SYNCING_USERS_MS:",
            "                self.users_going_offline.pop(user_id, None)",
            "                self.send_user_sync(user_id, False, last_sync_ms)",
            "",
            "    async def user_syncing(",
            "        self, user_id: str, affect_presence: bool",
            "    ) -> ContextManager[None]:",
            "        \"\"\"Record that a user is syncing.",
            "",
            "        Called by the sync and events servlets to record that a user has connected to",
            "        this worker and is waiting for some events.",
            "        \"\"\"",
            "        if not affect_presence or not self._presence_enabled:",
            "            return _NullContextManager()",
            "",
            "        curr_sync = self._user_to_num_current_syncs.get(user_id, 0)",
            "        self._user_to_num_current_syncs[user_id] = curr_sync + 1",
            "",
            "        # If we went from no in flight sync to some, notify replication",
            "        if self._user_to_num_current_syncs[user_id] == 1:",
            "            self.mark_as_coming_online(user_id)",
            "",
            "        def _end():",
            "            # We check that the user_id is in user_to_num_current_syncs because",
            "            # user_to_num_current_syncs may have been cleared if we are",
            "            # shutting down.",
            "            if user_id in self._user_to_num_current_syncs:",
            "                self._user_to_num_current_syncs[user_id] -= 1",
            "",
            "                # If we went from one in flight sync to non, notify replication",
            "                if self._user_to_num_current_syncs[user_id] == 0:",
            "                    self.mark_as_going_offline(user_id)",
            "",
            "        @contextlib.contextmanager",
            "        def _user_syncing():",
            "            try:",
            "                yield",
            "            finally:",
            "                _end()",
            "",
            "        return _user_syncing()",
            "",
            "    async def notify_from_replication(self, states, stream_id):",
            "        parties = await get_interested_parties(self.store, states)",
            "        room_ids_to_states, users_to_states = parties",
            "",
            "        self.notifier.on_new_event(",
            "            \"presence_key\",",
            "            stream_id,",
            "            rooms=room_ids_to_states.keys(),",
            "            users=users_to_states.keys(),",
            "        )",
            "",
            "    async def process_replication_rows(self, token, rows):",
            "        states = [",
            "            UserPresenceState(",
            "                row.user_id,",
            "                row.state,",
            "                row.last_active_ts,",
            "                row.last_federation_update_ts,",
            "                row.last_user_sync_ts,",
            "                row.status_msg,",
            "                row.currently_active,",
            "            )",
            "            for row in rows",
            "        ]",
            "",
            "        for state in states:",
            "            self.user_to_current_state[state.user_id] = state",
            "",
            "        stream_id = token",
            "        await self.notify_from_replication(states, stream_id)",
            "",
            "    def get_currently_syncing_users_for_replication(self) -> Iterable[str]:",
            "        return [",
            "            user_id",
            "            for user_id, count in self._user_to_num_current_syncs.items()",
            "            if count > 0",
            "        ]",
            "",
            "    async def set_state(self, target_user, state, ignore_status_msg=False):",
            "        \"\"\"Set the presence state of the user.",
            "        \"\"\"",
            "        presence = state[\"presence\"]",
            "",
            "        valid_presence = (",
            "            PresenceState.ONLINE,",
            "            PresenceState.UNAVAILABLE,",
            "            PresenceState.OFFLINE,",
            "        )",
            "        if presence not in valid_presence:",
            "            raise SynapseError(400, \"Invalid presence state\")",
            "",
            "        user_id = target_user.to_string()",
            "",
            "        # If presence is disabled, no-op",
            "        if not self.hs.config.use_presence:",
            "            return",
            "",
            "        # Proxy request to master",
            "        await self._set_state_client(",
            "            user_id=user_id, state=state, ignore_status_msg=ignore_status_msg",
            "        )",
            "",
            "    async def bump_presence_active_time(self, user):",
            "        \"\"\"We've seen the user do something that indicates they're interacting",
            "        with the app.",
            "        \"\"\"",
            "        # If presence is disabled, no-op",
            "        if not self.hs.config.use_presence:",
            "            return",
            "",
            "        # Proxy request to master",
            "        user_id = user.to_string()",
            "        await self._bump_active_client(user_id=user_id)",
            "",
            "",
            "class GenericWorkerSlavedStore(",
            "    # FIXME(#3714): We need to add UserDirectoryStore as we write directly",
            "    # rather than going via the correct worker.",
            "    UserDirectoryStore,",
            "    StatsStore,",
            "    UIAuthWorkerStore,",
            "    SlavedDeviceInboxStore,",
            "    SlavedDeviceStore,",
            "    SlavedReceiptsStore,",
            "    SlavedPushRuleStore,",
            "    SlavedGroupServerStore,",
            "    SlavedAccountDataStore,",
            "    SlavedPusherStore,",
            "    CensorEventsStore,",
            "    ClientIpWorkerStore,",
            "    SlavedEventStore,",
            "    SlavedKeyStore,",
            "    RoomStore,",
            "    DirectoryStore,",
            "    SlavedApplicationServiceStore,",
            "    SlavedRegistrationStore,",
            "    SlavedTransactionStore,",
            "    SlavedProfileStore,",
            "    SlavedClientIpStore,",
            "    SlavedPresenceStore,",
            "    SlavedFilteringStore,",
            "    MonthlyActiveUsersWorkerStore,",
            "    MediaRepositoryStore,",
            "    ServerMetricsStore,",
            "    SearchWorkerStore,",
            "    TransactionWorkerStore,",
            "    BaseSlavedStore,",
            "):",
            "    pass",
            "",
            "",
            "class GenericWorkerServer(HomeServer):",
            "    DATASTORE_CLASS = GenericWorkerSlavedStore",
            "",
            "    def _listen_http(self, listener_config: ListenerConfig):",
            "        port = listener_config.port",
            "        bind_addresses = listener_config.bind_addresses",
            "",
            "        assert listener_config.http_options is not None",
            "",
            "        site_tag = listener_config.http_options.tag",
            "        if site_tag is None:",
            "            site_tag = port",
            "",
            "        # We always include a health resource.",
            "        resources = {\"/health\": HealthResource()}",
            "",
            "        for res in listener_config.http_options.resources:",
            "            for name in res.names:",
            "                if name == \"metrics\":",
            "                    resources[METRICS_PREFIX] = MetricsResource(RegistryProxy)",
            "                elif name == \"client\":",
            "                    resource = JsonResource(self, canonical_json=False)",
            "",
            "                    PublicRoomListRestServlet(self).register(resource)",
            "                    RoomMemberListRestServlet(self).register(resource)",
            "                    JoinedRoomMemberListRestServlet(self).register(resource)",
            "                    RoomStateRestServlet(self).register(resource)",
            "                    RoomEventContextServlet(self).register(resource)",
            "                    RoomMessageListRestServlet(self).register(resource)",
            "                    RegisterRestServlet(self).register(resource)",
            "                    LoginRestServlet(self).register(resource)",
            "                    ThreepidRestServlet(self).register(resource)",
            "                    KeyQueryServlet(self).register(resource)",
            "                    KeyChangesServlet(self).register(resource)",
            "                    VoipRestServlet(self).register(resource)",
            "                    PushRuleRestServlet(self).register(resource)",
            "                    VersionsRestServlet(self).register(resource)",
            "                    RoomSendEventRestServlet(self).register(resource)",
            "                    RoomMembershipRestServlet(self).register(resource)",
            "                    RoomStateEventRestServlet(self).register(resource)",
            "                    JoinRoomAliasServlet(self).register(resource)",
            "                    ProfileAvatarURLRestServlet(self).register(resource)",
            "                    ProfileDisplaynameRestServlet(self).register(resource)",
            "                    ProfileRestServlet(self).register(resource)",
            "                    KeyUploadServlet(self).register(resource)",
            "                    AccountDataServlet(self).register(resource)",
            "                    RoomAccountDataServlet(self).register(resource)",
            "                    RoomTypingRestServlet(self).register(resource)",
            "",
            "                    sync.register_servlets(self, resource)",
            "                    events.register_servlets(self, resource)",
            "                    InitialSyncRestServlet(self).register(resource)",
            "                    RoomInitialSyncRestServlet(self).register(resource)",
            "",
            "                    user_directory.register_servlets(self, resource)",
            "",
            "                    # If presence is disabled, use the stub servlet that does",
            "                    # not allow sending presence",
            "                    if not self.config.use_presence:",
            "                        PresenceStatusStubServlet(self).register(resource)",
            "",
            "                    groups.register_servlets(self, resource)",
            "",
            "                    resources.update({CLIENT_API_PREFIX: resource})",
            "                elif name == \"federation\":",
            "                    resources.update({FEDERATION_PREFIX: TransportLayerServer(self)})",
            "                elif name == \"media\":",
            "                    if self.config.can_load_media_repo:",
            "                        media_repo = self.get_media_repository_resource()",
            "",
            "                        # We need to serve the admin servlets for media on the",
            "                        # worker.",
            "                        admin_resource = JsonResource(self, canonical_json=False)",
            "                        register_servlets_for_media_repo(self, admin_resource)",
            "",
            "                        resources.update(",
            "                            {",
            "                                MEDIA_PREFIX: media_repo,",
            "                                LEGACY_MEDIA_PREFIX: media_repo,",
            "                                \"/_synapse/admin\": admin_resource,",
            "                            }",
            "                        )",
            "                    else:",
            "                        logger.warning(",
            "                            \"A 'media' listener is configured but the media\"",
            "                            \" repository is disabled. Ignoring.\"",
            "                        )",
            "",
            "                if name == \"openid\" and \"federation\" not in res.names:",
            "                    # Only load the openid resource separately if federation resource",
            "                    # is not specified since federation resource includes openid",
            "                    # resource.",
            "                    resources.update(",
            "                        {",
            "                            FEDERATION_PREFIX: TransportLayerServer(",
            "                                self, servlet_groups=[\"openid\"]",
            "                            )",
            "                        }",
            "                    )",
            "",
            "                if name in [\"keys\", \"federation\"]:",
            "                    resources[SERVER_KEY_V2_PREFIX] = KeyApiV2Resource(self)",
            "",
            "                if name == \"replication\":",
            "                    resources[REPLICATION_PREFIX] = ReplicationRestResource(self)",
            "",
            "        root_resource = create_resource_tree(resources, OptionsResource())",
            "",
            "        _base.listen_tcp(",
            "            bind_addresses,",
            "            port,",
            "            SynapseSite(",
            "                \"synapse.access.http.%s\" % (site_tag,),",
            "                site_tag,",
            "                listener_config,",
            "                root_resource,",
            "                self.version_string,",
            "            ),",
            "            reactor=self.get_reactor(),",
            "        )",
            "",
            "        logger.info(\"Synapse worker now listening on port %d\", port)",
            "",
            "    def start_listening(self, listeners: Iterable[ListenerConfig]):",
            "        for listener in listeners:",
            "            if listener.type == \"http\":",
            "                self._listen_http(listener)",
            "            elif listener.type == \"manhole\":",
            "                _base.listen_tcp(",
            "                    listener.bind_addresses,",
            "                    listener.port,",
            "                    manhole(",
            "                        username=\"matrix\", password=\"rabbithole\", globals={\"hs\": self}",
            "                    ),",
            "                )",
            "            elif listener.type == \"metrics\":",
            "                if not self.get_config().enable_metrics:",
            "                    logger.warning(",
            "                        (",
            "                            \"Metrics listener configured, but \"",
            "                            \"enable_metrics is not True!\"",
            "                        )",
            "                    )",
            "                else:",
            "                    _base.listen_metrics(listener.bind_addresses, listener.port)",
            "            else:",
            "                logger.warning(\"Unsupported listener type: %s\", listener.type)",
            "",
            "        self.get_tcp_replication().start_replication(self)",
            "",
            "    async def remove_pusher(self, app_id, push_key, user_id):",
            "        self.get_tcp_replication().send_remove_pusher(app_id, push_key, user_id)",
            "",
            "    @cache_in_self",
            "    def get_replication_data_handler(self):",
            "        return GenericWorkerReplicationHandler(self)",
            "",
            "    @cache_in_self",
            "    def get_presence_handler(self):",
            "        return GenericWorkerPresence(self)",
            "",
            "",
            "class GenericWorkerReplicationHandler(ReplicationDataHandler):",
            "    def __init__(self, hs):",
            "        super().__init__(hs)",
            "",
            "        self.store = hs.get_datastore()",
            "        self.presence_handler = hs.get_presence_handler()  # type: GenericWorkerPresence",
            "        self.notifier = hs.get_notifier()",
            "",
            "        self.notify_pushers = hs.config.start_pushers",
            "        self.pusher_pool = hs.get_pusherpool()",
            "",
            "        self.send_handler = None  # type: Optional[FederationSenderHandler]",
            "        if hs.config.send_federation:",
            "            self.send_handler = FederationSenderHandler(hs)",
            "",
            "    async def on_rdata(self, stream_name, instance_name, token, rows):",
            "        await super().on_rdata(stream_name, instance_name, token, rows)",
            "        await self._process_and_notify(stream_name, instance_name, token, rows)",
            "",
            "    async def _process_and_notify(self, stream_name, instance_name, token, rows):",
            "        try:",
            "            if self.send_handler:",
            "                await self.send_handler.process_replication_rows(",
            "                    stream_name, token, rows",
            "                )",
            "",
            "            if stream_name == PushRulesStream.NAME:",
            "                self.notifier.on_new_event(",
            "                    \"push_rules_key\", token, users=[row.user_id for row in rows]",
            "                )",
            "            elif stream_name in (AccountDataStream.NAME, TagAccountDataStream.NAME):",
            "                self.notifier.on_new_event(",
            "                    \"account_data_key\", token, users=[row.user_id for row in rows]",
            "                )",
            "            elif stream_name == ReceiptsStream.NAME:",
            "                self.notifier.on_new_event(",
            "                    \"receipt_key\", token, rooms=[row.room_id for row in rows]",
            "                )",
            "                await self.pusher_pool.on_new_receipts(",
            "                    token, token, {row.room_id for row in rows}",
            "                )",
            "            elif stream_name == ToDeviceStream.NAME:",
            "                entities = [row.entity for row in rows if row.entity.startswith(\"@\")]",
            "                if entities:",
            "                    self.notifier.on_new_event(\"to_device_key\", token, users=entities)",
            "            elif stream_name == DeviceListsStream.NAME:",
            "                all_room_ids = set()  # type: Set[str]",
            "                for row in rows:",
            "                    if row.entity.startswith(\"@\"):",
            "                        room_ids = await self.store.get_rooms_for_user(row.entity)",
            "                        all_room_ids.update(room_ids)",
            "                self.notifier.on_new_event(\"device_list_key\", token, rooms=all_room_ids)",
            "            elif stream_name == PresenceStream.NAME:",
            "                await self.presence_handler.process_replication_rows(token, rows)",
            "            elif stream_name == GroupServerStream.NAME:",
            "                self.notifier.on_new_event(",
            "                    \"groups_key\", token, users=[row.user_id for row in rows]",
            "                )",
            "            elif stream_name == PushersStream.NAME:",
            "                for row in rows:",
            "                    if row.deleted:",
            "                        self.stop_pusher(row.user_id, row.app_id, row.pushkey)",
            "                    else:",
            "                        await self.start_pusher(row.user_id, row.app_id, row.pushkey)",
            "        except Exception:",
            "            logger.exception(\"Error processing replication\")",
            "",
            "    async def on_position(self, stream_name: str, instance_name: str, token: int):",
            "        await super().on_position(stream_name, instance_name, token)",
            "        # Also call on_rdata to ensure that stream positions are properly reset.",
            "        await self.on_rdata(stream_name, instance_name, token, [])",
            "",
            "    def stop_pusher(self, user_id, app_id, pushkey):",
            "        if not self.notify_pushers:",
            "            return",
            "",
            "        key = \"%s:%s\" % (app_id, pushkey)",
            "        pushers_for_user = self.pusher_pool.pushers.get(user_id, {})",
            "        pusher = pushers_for_user.pop(key, None)",
            "        if pusher is None:",
            "            return",
            "        logger.info(\"Stopping pusher %r / %r\", user_id, key)",
            "        pusher.on_stop()",
            "",
            "    async def start_pusher(self, user_id, app_id, pushkey):",
            "        if not self.notify_pushers:",
            "            return",
            "",
            "        key = \"%s:%s\" % (app_id, pushkey)",
            "        logger.info(\"Starting pusher %r / %r\", user_id, key)",
            "        return await self.pusher_pool.start_pusher_by_id(app_id, pushkey, user_id)",
            "",
            "    def on_remote_server_up(self, server: str):",
            "        \"\"\"Called when get a new REMOTE_SERVER_UP command.\"\"\"",
            "",
            "        # Let's wake up the transaction queue for the server in case we have",
            "        # pending stuff to send to it.",
            "        if self.send_handler:",
            "            self.send_handler.wake_destination(server)",
            "",
            "",
            "class FederationSenderHandler:",
            "    \"\"\"Processes the fedration replication stream",
            "",
            "    This class is only instantiate on the worker responsible for sending outbound",
            "    federation transactions. It receives rows from the replication stream and forwards",
            "    the appropriate entries to the FederationSender class.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: GenericWorkerServer):",
            "        self.store = hs.get_datastore()",
            "        self._is_mine_id = hs.is_mine_id",
            "        self.federation_sender = hs.get_federation_sender()",
            "        self._hs = hs",
            "",
            "        # Stores the latest position in the federation stream we've gotten up",
            "        # to. This is always set before we use it.",
            "        self.federation_position = None",
            "",
            "        self._fed_position_linearizer = Linearizer(name=\"_fed_position_linearizer\")",
            "",
            "    def on_start(self):",
            "        # There may be some events that are persisted but haven't been sent,",
            "        # so send them now.",
            "        self.federation_sender.notify_new_events(",
            "            self.store.get_room_max_stream_ordering()",
            "        )",
            "",
            "    def wake_destination(self, server: str):",
            "        self.federation_sender.wake_destination(server)",
            "",
            "    async def process_replication_rows(self, stream_name, token, rows):",
            "        # The federation stream contains things that we want to send out, e.g.",
            "        # presence, typing, etc.",
            "        if stream_name == \"federation\":",
            "            send_queue.process_rows_for_federation(self.federation_sender, rows)",
            "            await self.update_token(token)",
            "",
            "        # ... and when new receipts happen",
            "        elif stream_name == ReceiptsStream.NAME:",
            "            await self._on_new_receipts(rows)",
            "",
            "        # ... as well as device updates and messages",
            "        elif stream_name == DeviceListsStream.NAME:",
            "            # The entities are either user IDs (starting with '@') whose devices",
            "            # have changed, or remote servers that we need to tell about",
            "            # changes.",
            "            hosts = {row.entity for row in rows if not row.entity.startswith(\"@\")}",
            "            for host in hosts:",
            "                self.federation_sender.send_device_messages(host)",
            "",
            "        elif stream_name == ToDeviceStream.NAME:",
            "            # The to_device stream includes stuff to be pushed to both local",
            "            # clients and remote servers, so we ignore entities that start with",
            "            # '@' (since they'll be local users rather than destinations).",
            "            hosts = {row.entity for row in rows if not row.entity.startswith(\"@\")}",
            "            for host in hosts:",
            "                self.federation_sender.send_device_messages(host)",
            "",
            "    async def _on_new_receipts(self, rows):",
            "        \"\"\"",
            "        Args:",
            "            rows (Iterable[synapse.replication.tcp.streams.ReceiptsStream.ReceiptsStreamRow]):",
            "                new receipts to be processed",
            "        \"\"\"",
            "        for receipt in rows:",
            "            # we only want to send on receipts for our own users",
            "            if not self._is_mine_id(receipt.user_id):",
            "                continue",
            "            receipt_info = ReadReceipt(",
            "                receipt.room_id,",
            "                receipt.receipt_type,",
            "                receipt.user_id,",
            "                [receipt.event_id],",
            "                receipt.data,",
            "            )",
            "            await self.federation_sender.send_read_receipt(receipt_info)",
            "",
            "    async def update_token(self, token):",
            "        \"\"\"Update the record of where we have processed to in the federation stream.",
            "",
            "        Called after we have processed a an update received over replication. Sends",
            "        a FEDERATION_ACK back to the master, and stores the token that we have processed",
            "         in `federation_stream_position` so that we can restart where we left off.",
            "        \"\"\"",
            "        self.federation_position = token",
            "",
            "        # We save and send the ACK to master asynchronously, so we don't block",
            "        # processing on persistence. We don't need to do this operation for",
            "        # every single RDATA we receive, we just need to do it periodically.",
            "",
            "        if self._fed_position_linearizer.is_queued(None):",
            "            # There is already a task queued up to save and send the token, so",
            "            # no need to queue up another task.",
            "            return",
            "",
            "        run_as_background_process(\"_save_and_send_ack\", self._save_and_send_ack)",
            "",
            "    async def _save_and_send_ack(self):",
            "        \"\"\"Save the current federation position in the database and send an ACK",
            "        to master with where we're up to.",
            "        \"\"\"",
            "        try:",
            "            # We linearize here to ensure we don't have races updating the token",
            "            #",
            "            # XXX this appears to be redundant, since the ReplicationCommandHandler",
            "            # has a linearizer which ensures that we only process one line of",
            "            # replication data at a time. Should we remove it, or is it doing useful",
            "            # service for robustness? Or could we replace it with an assertion that",
            "            # we're not being re-entered?",
            "",
            "            with (await self._fed_position_linearizer.queue(None)):",
            "                # We persist and ack the same position, so we take a copy of it",
            "                # here as otherwise it can get modified from underneath us.",
            "                current_position = self.federation_position",
            "",
            "                await self.store.update_federation_out_pos(",
            "                    \"federation\", current_position",
            "                )",
            "",
            "                # We ACK this token over replication so that the master can drop",
            "                # its in memory queues",
            "                self._hs.get_tcp_replication().send_federation_ack(current_position)",
            "        except Exception:",
            "            logger.exception(\"Error updating federation stream position\")",
            "",
            "",
            "def start(config_options):",
            "    try:",
            "        config = HomeServerConfig.load_config(\"Synapse worker\", config_options)",
            "    except ConfigError as e:",
            "        sys.stderr.write(\"\\n\" + str(e) + \"\\n\")",
            "        sys.exit(1)",
            "",
            "    # For backwards compatibility let any of the old app names.",
            "    assert config.worker_app in (",
            "        \"synapse.app.appservice\",",
            "        \"synapse.app.client_reader\",",
            "        \"synapse.app.event_creator\",",
            "        \"synapse.app.federation_reader\",",
            "        \"synapse.app.federation_sender\",",
            "        \"synapse.app.frontend_proxy\",",
            "        \"synapse.app.generic_worker\",",
            "        \"synapse.app.media_repository\",",
            "        \"synapse.app.pusher\",",
            "        \"synapse.app.synchrotron\",",
            "        \"synapse.app.user_dir\",",
            "    )",
            "",
            "    if config.worker_app == \"synapse.app.appservice\":",
            "        if config.appservice.notify_appservices:",
            "            sys.stderr.write(",
            "                \"\\nThe appservices must be disabled in the main synapse process\"",
            "                \"\\nbefore they can be run in a separate worker.\"",
            "                \"\\nPlease add ``notify_appservices: false`` to the main config\"",
            "                \"\\n\"",
            "            )",
            "            sys.exit(1)",
            "",
            "        # Force the appservice to start since they will be disabled in the main config",
            "        config.appservice.notify_appservices = True",
            "    else:",
            "        # For other worker types we force this to off.",
            "        config.appservice.notify_appservices = False",
            "",
            "    if config.worker_app == \"synapse.app.pusher\":",
            "        if config.server.start_pushers:",
            "            sys.stderr.write(",
            "                \"\\nThe pushers must be disabled in the main synapse process\"",
            "                \"\\nbefore they can be run in a separate worker.\"",
            "                \"\\nPlease add ``start_pushers: false`` to the main config\"",
            "                \"\\n\"",
            "            )",
            "            sys.exit(1)",
            "",
            "        # Force the pushers to start since they will be disabled in the main config",
            "        config.server.start_pushers = True",
            "    else:",
            "        # For other worker types we force this to off.",
            "        config.server.start_pushers = False",
            "",
            "    if config.worker_app == \"synapse.app.user_dir\":",
            "        if config.server.update_user_directory:",
            "            sys.stderr.write(",
            "                \"\\nThe update_user_directory must be disabled in the main synapse process\"",
            "                \"\\nbefore they can be run in a separate worker.\"",
            "                \"\\nPlease add ``update_user_directory: false`` to the main config\"",
            "                \"\\n\"",
            "            )",
            "            sys.exit(1)",
            "",
            "        # Force the pushers to start since they will be disabled in the main config",
            "        config.server.update_user_directory = True",
            "    else:",
            "        # For other worker types we force this to off.",
            "        config.server.update_user_directory = False",
            "",
            "    if config.worker_app == \"synapse.app.federation_sender\":",
            "        if config.worker.send_federation:",
            "            sys.stderr.write(",
            "                \"\\nThe send_federation must be disabled in the main synapse process\"",
            "                \"\\nbefore they can be run in a separate worker.\"",
            "                \"\\nPlease add ``send_federation: false`` to the main config\"",
            "                \"\\n\"",
            "            )",
            "            sys.exit(1)",
            "",
            "        # Force the pushers to start since they will be disabled in the main config",
            "        config.worker.send_federation = True",
            "    else:",
            "        # For other worker types we force this to off.",
            "        config.worker.send_federation = False",
            "",
            "    synapse.events.USE_FROZEN_DICTS = config.use_frozen_dicts",
            "",
            "    hs = GenericWorkerServer(",
            "        config.server_name,",
            "        config=config,",
            "        version_string=\"Synapse/\" + get_version_string(synapse),",
            "    )",
            "",
            "    setup_logging(hs, config, use_worker_options=True)",
            "",
            "    hs.setup()",
            "",
            "    # Ensure the replication streamer is always started in case we write to any",
            "    # streams. Will no-op if no streams can be written to by this worker.",
            "    hs.get_replication_streamer()",
            "",
            "    reactor.addSystemEventTrigger(",
            "        \"before\", \"startup\", _base.start, hs, config.worker_listeners",
            "    )",
            "",
            "    _base.start_worker_reactor(\"synapse-generic-worker\", config)",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    with LoggingContext(\"main\"):",
            "        start(sys.argv[1:])"
        ],
        "afterPatchFile": [
            "#!/usr/bin/env python",
            "# -*- coding: utf-8 -*-",
            "# Copyright 2016 OpenMarket Ltd",
            "# Copyright 2020 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import contextlib",
            "import logging",
            "import sys",
            "from typing import Dict, Iterable, Optional, Set",
            "",
            "from typing_extensions import ContextManager",
            "",
            "from twisted.internet import address, reactor",
            "",
            "import synapse",
            "import synapse.events",
            "from synapse.api.errors import HttpResponseException, RequestSendFailed, SynapseError",
            "from synapse.api.urls import (",
            "    CLIENT_API_PREFIX,",
            "    FEDERATION_PREFIX,",
            "    LEGACY_MEDIA_PREFIX,",
            "    MEDIA_PREFIX,",
            "    SERVER_KEY_V2_PREFIX,",
            ")",
            "from synapse.app import _base",
            "from synapse.config._base import ConfigError",
            "from synapse.config.homeserver import HomeServerConfig",
            "from synapse.config.logger import setup_logging",
            "from synapse.config.server import ListenerConfig",
            "from synapse.federation import send_queue",
            "from synapse.federation.transport.server import TransportLayerServer",
            "from synapse.handlers.presence import (",
            "    BasePresenceHandler,",
            "    PresenceState,",
            "    get_interested_parties,",
            ")",
            "from synapse.http.server import JsonResource, OptionsResource",
            "from synapse.http.servlet import RestServlet, parse_json_object_from_request",
            "from synapse.http.site import SynapseSite",
            "from synapse.logging.context import LoggingContext",
            "from synapse.metrics import METRICS_PREFIX, MetricsResource, RegistryProxy",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.replication.http import REPLICATION_PREFIX, ReplicationRestResource",
            "from synapse.replication.http.presence import (",
            "    ReplicationBumpPresenceActiveTime,",
            "    ReplicationPresenceSetState,",
            ")",
            "from synapse.replication.slave.storage._base import BaseSlavedStore",
            "from synapse.replication.slave.storage.account_data import SlavedAccountDataStore",
            "from synapse.replication.slave.storage.appservice import SlavedApplicationServiceStore",
            "from synapse.replication.slave.storage.client_ips import SlavedClientIpStore",
            "from synapse.replication.slave.storage.deviceinbox import SlavedDeviceInboxStore",
            "from synapse.replication.slave.storage.devices import SlavedDeviceStore",
            "from synapse.replication.slave.storage.directory import DirectoryStore",
            "from synapse.replication.slave.storage.events import SlavedEventStore",
            "from synapse.replication.slave.storage.filtering import SlavedFilteringStore",
            "from synapse.replication.slave.storage.groups import SlavedGroupServerStore",
            "from synapse.replication.slave.storage.keys import SlavedKeyStore",
            "from synapse.replication.slave.storage.presence import SlavedPresenceStore",
            "from synapse.replication.slave.storage.profile import SlavedProfileStore",
            "from synapse.replication.slave.storage.push_rule import SlavedPushRuleStore",
            "from synapse.replication.slave.storage.pushers import SlavedPusherStore",
            "from synapse.replication.slave.storage.receipts import SlavedReceiptsStore",
            "from synapse.replication.slave.storage.registration import SlavedRegistrationStore",
            "from synapse.replication.slave.storage.room import RoomStore",
            "from synapse.replication.slave.storage.transactions import SlavedTransactionStore",
            "from synapse.replication.tcp.client import ReplicationDataHandler",
            "from synapse.replication.tcp.commands import ClearUserSyncsCommand",
            "from synapse.replication.tcp.streams import (",
            "    AccountDataStream,",
            "    DeviceListsStream,",
            "    GroupServerStream,",
            "    PresenceStream,",
            "    PushersStream,",
            "    PushRulesStream,",
            "    ReceiptsStream,",
            "    TagAccountDataStream,",
            "    ToDeviceStream,",
            ")",
            "from synapse.rest.admin import register_servlets_for_media_repo",
            "from synapse.rest.client.v1 import events",
            "from synapse.rest.client.v1.initial_sync import InitialSyncRestServlet",
            "from synapse.rest.client.v1.login import LoginRestServlet",
            "from synapse.rest.client.v1.profile import (",
            "    ProfileAvatarURLRestServlet,",
            "    ProfileDisplaynameRestServlet,",
            "    ProfileRestServlet,",
            ")",
            "from synapse.rest.client.v1.push_rule import PushRuleRestServlet",
            "from synapse.rest.client.v1.room import (",
            "    JoinedRoomMemberListRestServlet,",
            "    JoinRoomAliasServlet,",
            "    PublicRoomListRestServlet,",
            "    RoomEventContextServlet,",
            "    RoomInitialSyncRestServlet,",
            "    RoomMemberListRestServlet,",
            "    RoomMembershipRestServlet,",
            "    RoomMessageListRestServlet,",
            "    RoomSendEventRestServlet,",
            "    RoomStateEventRestServlet,",
            "    RoomStateRestServlet,",
            "    RoomTypingRestServlet,",
            ")",
            "from synapse.rest.client.v1.voip import VoipRestServlet",
            "from synapse.rest.client.v2_alpha import groups, sync, user_directory",
            "from synapse.rest.client.v2_alpha._base import client_patterns",
            "from synapse.rest.client.v2_alpha.account import ThreepidRestServlet",
            "from synapse.rest.client.v2_alpha.account_data import (",
            "    AccountDataServlet,",
            "    RoomAccountDataServlet,",
            ")",
            "from synapse.rest.client.v2_alpha.keys import KeyChangesServlet, KeyQueryServlet",
            "from synapse.rest.client.v2_alpha.register import RegisterRestServlet",
            "from synapse.rest.client.versions import VersionsRestServlet",
            "from synapse.rest.health import HealthResource",
            "from synapse.rest.key.v2 import KeyApiV2Resource",
            "from synapse.server import HomeServer, cache_in_self",
            "from synapse.storage.databases.main.censor_events import CensorEventsStore",
            "from synapse.storage.databases.main.client_ips import ClientIpWorkerStore",
            "from synapse.storage.databases.main.media_repository import MediaRepositoryStore",
            "from synapse.storage.databases.main.metrics import ServerMetricsStore",
            "from synapse.storage.databases.main.monthly_active_users import (",
            "    MonthlyActiveUsersWorkerStore,",
            ")",
            "from synapse.storage.databases.main.presence import UserPresenceState",
            "from synapse.storage.databases.main.search import SearchWorkerStore",
            "from synapse.storage.databases.main.stats import StatsStore",
            "from synapse.storage.databases.main.transactions import TransactionWorkerStore",
            "from synapse.storage.databases.main.ui_auth import UIAuthWorkerStore",
            "from synapse.storage.databases.main.user_directory import UserDirectoryStore",
            "from synapse.types import ReadReceipt",
            "from synapse.util.async_helpers import Linearizer",
            "from synapse.util.httpresourcetree import create_resource_tree",
            "from synapse.util.manhole import manhole",
            "from synapse.util.versionstring import get_version_string",
            "",
            "logger = logging.getLogger(\"synapse.app.generic_worker\")",
            "",
            "",
            "class PresenceStatusStubServlet(RestServlet):",
            "    \"\"\"If presence is disabled this servlet can be used to stub out setting",
            "    presence status.",
            "    \"\"\"",
            "",
            "    PATTERNS = client_patterns(\"/presence/(?P<user_id>[^/]*)/status\")",
            "",
            "    def __init__(self, hs):",
            "        super().__init__()",
            "        self.auth = hs.get_auth()",
            "",
            "    async def on_GET(self, request, user_id):",
            "        await self.auth.get_user_by_req(request)",
            "        return 200, {\"presence\": \"offline\"}",
            "",
            "    async def on_PUT(self, request, user_id):",
            "        await self.auth.get_user_by_req(request)",
            "        return 200, {}",
            "",
            "",
            "class KeyUploadServlet(RestServlet):",
            "    \"\"\"An implementation of the `KeyUploadServlet` that responds to read only",
            "    requests, but otherwise proxies through to the master instance.",
            "    \"\"\"",
            "",
            "    PATTERNS = client_patterns(\"/keys/upload(/(?P<device_id>[^/]+))?$\")",
            "",
            "    def __init__(self, hs):",
            "        \"\"\"",
            "        Args:",
            "            hs (synapse.server.HomeServer): server",
            "        \"\"\"",
            "        super().__init__()",
            "        self.auth = hs.get_auth()",
            "        self.store = hs.get_datastore()",
            "        self.http_client = hs.get_simple_http_client()",
            "        self.main_uri = hs.config.worker_main_http_uri",
            "",
            "    async def on_POST(self, request, device_id):",
            "        requester = await self.auth.get_user_by_req(request, allow_guest=True)",
            "        user_id = requester.user.to_string()",
            "        body = parse_json_object_from_request(request)",
            "",
            "        if device_id is not None:",
            "            # passing the device_id here is deprecated; however, we allow it",
            "            # for now for compatibility with older clients.",
            "            if requester.device_id is not None and device_id != requester.device_id:",
            "                logger.warning(",
            "                    \"Client uploading keys for a different device \"",
            "                    \"(logged in as %s, uploading for %s)\",",
            "                    requester.device_id,",
            "                    device_id,",
            "                )",
            "        else:",
            "            device_id = requester.device_id",
            "",
            "        if device_id is None:",
            "            raise SynapseError(",
            "                400, \"To upload keys, you must pass device_id when authenticating\"",
            "            )",
            "",
            "        if body:",
            "            # They're actually trying to upload something, proxy to main synapse.",
            "",
            "            # Proxy headers from the original request, such as the auth headers",
            "            # (in case the access token is there) and the original IP /",
            "            # User-Agent of the request.",
            "            headers = {",
            "                header: request.requestHeaders.getRawHeaders(header, [])",
            "                for header in (b\"Authorization\", b\"User-Agent\")",
            "            }",
            "            # Add the previous hop the the X-Forwarded-For header.",
            "            x_forwarded_for = request.requestHeaders.getRawHeaders(",
            "                b\"X-Forwarded-For\", []",
            "            )",
            "            if isinstance(request.client, (address.IPv4Address, address.IPv6Address)):",
            "                previous_host = request.client.host.encode(\"ascii\")",
            "                # If the header exists, add to the comma-separated list of the first",
            "                # instance of the header. Otherwise, generate a new header.",
            "                if x_forwarded_for:",
            "                    x_forwarded_for = [",
            "                        x_forwarded_for[0] + b\", \" + previous_host",
            "                    ] + x_forwarded_for[1:]",
            "                else:",
            "                    x_forwarded_for = [previous_host]",
            "            headers[b\"X-Forwarded-For\"] = x_forwarded_for",
            "",
            "            try:",
            "                result = await self.http_client.post_json_get_json(",
            "                    self.main_uri + request.uri.decode(\"ascii\"), body, headers=headers",
            "                )",
            "            except HttpResponseException as e:",
            "                raise e.to_synapse_error() from e",
            "            except RequestSendFailed as e:",
            "                raise SynapseError(502, \"Failed to talk to master\") from e",
            "",
            "            return 200, result",
            "        else:",
            "            # Just interested in counts.",
            "            result = await self.store.count_e2e_one_time_keys(user_id, device_id)",
            "            return 200, {\"one_time_key_counts\": result}",
            "",
            "",
            "class _NullContextManager(ContextManager[None]):",
            "    \"\"\"A context manager which does nothing.\"\"\"",
            "",
            "    def __exit__(self, exc_type, exc_val, exc_tb):",
            "        pass",
            "",
            "",
            "UPDATE_SYNCING_USERS_MS = 10 * 1000",
            "",
            "",
            "class GenericWorkerPresence(BasePresenceHandler):",
            "    def __init__(self, hs):",
            "        super().__init__(hs)",
            "        self.hs = hs",
            "        self.is_mine_id = hs.is_mine_id",
            "",
            "        self._presence_enabled = hs.config.use_presence",
            "",
            "        # The number of ongoing syncs on this process, by user id.",
            "        # Empty if _presence_enabled is false.",
            "        self._user_to_num_current_syncs = {}  # type: Dict[str, int]",
            "",
            "        self.notifier = hs.get_notifier()",
            "        self.instance_id = hs.get_instance_id()",
            "",
            "        # user_id -> last_sync_ms. Lists the users that have stopped syncing",
            "        # but we haven't notified the master of that yet",
            "        self.users_going_offline = {}",
            "",
            "        self._bump_active_client = ReplicationBumpPresenceActiveTime.make_client(hs)",
            "        self._set_state_client = ReplicationPresenceSetState.make_client(hs)",
            "",
            "        self._send_stop_syncing_loop = self.clock.looping_call(",
            "            self.send_stop_syncing, UPDATE_SYNCING_USERS_MS",
            "        )",
            "",
            "        hs.get_reactor().addSystemEventTrigger(",
            "            \"before\",",
            "            \"shutdown\",",
            "            run_as_background_process,",
            "            \"generic_presence.on_shutdown\",",
            "            self._on_shutdown,",
            "        )",
            "",
            "    def _on_shutdown(self):",
            "        if self._presence_enabled:",
            "            self.hs.get_tcp_replication().send_command(",
            "                ClearUserSyncsCommand(self.instance_id)",
            "            )",
            "",
            "    def send_user_sync(self, user_id, is_syncing, last_sync_ms):",
            "        if self._presence_enabled:",
            "            self.hs.get_tcp_replication().send_user_sync(",
            "                self.instance_id, user_id, is_syncing, last_sync_ms",
            "            )",
            "",
            "    def mark_as_coming_online(self, user_id):",
            "        \"\"\"A user has started syncing. Send a UserSync to the master, unless they",
            "        had recently stopped syncing.",
            "",
            "        Args:",
            "            user_id (str)",
            "        \"\"\"",
            "        going_offline = self.users_going_offline.pop(user_id, None)",
            "        if not going_offline:",
            "            # Safe to skip because we haven't yet told the master they were offline",
            "            self.send_user_sync(user_id, True, self.clock.time_msec())",
            "",
            "    def mark_as_going_offline(self, user_id):",
            "        \"\"\"A user has stopped syncing. We wait before notifying the master as",
            "        its likely they'll come back soon. This allows us to avoid sending",
            "        a stopped syncing immediately followed by a started syncing notification",
            "        to the master",
            "",
            "        Args:",
            "            user_id (str)",
            "        \"\"\"",
            "        self.users_going_offline[user_id] = self.clock.time_msec()",
            "",
            "    def send_stop_syncing(self):",
            "        \"\"\"Check if there are any users who have stopped syncing a while ago",
            "        and haven't come back yet. If there are poke the master about them.",
            "        \"\"\"",
            "        now = self.clock.time_msec()",
            "        for user_id, last_sync_ms in list(self.users_going_offline.items()):",
            "            if now - last_sync_ms > UPDATE_SYNCING_USERS_MS:",
            "                self.users_going_offline.pop(user_id, None)",
            "                self.send_user_sync(user_id, False, last_sync_ms)",
            "",
            "    async def user_syncing(",
            "        self, user_id: str, affect_presence: bool",
            "    ) -> ContextManager[None]:",
            "        \"\"\"Record that a user is syncing.",
            "",
            "        Called by the sync and events servlets to record that a user has connected to",
            "        this worker and is waiting for some events.",
            "        \"\"\"",
            "        if not affect_presence or not self._presence_enabled:",
            "            return _NullContextManager()",
            "",
            "        curr_sync = self._user_to_num_current_syncs.get(user_id, 0)",
            "        self._user_to_num_current_syncs[user_id] = curr_sync + 1",
            "",
            "        # If we went from no in flight sync to some, notify replication",
            "        if self._user_to_num_current_syncs[user_id] == 1:",
            "            self.mark_as_coming_online(user_id)",
            "",
            "        def _end():",
            "            # We check that the user_id is in user_to_num_current_syncs because",
            "            # user_to_num_current_syncs may have been cleared if we are",
            "            # shutting down.",
            "            if user_id in self._user_to_num_current_syncs:",
            "                self._user_to_num_current_syncs[user_id] -= 1",
            "",
            "                # If we went from one in flight sync to non, notify replication",
            "                if self._user_to_num_current_syncs[user_id] == 0:",
            "                    self.mark_as_going_offline(user_id)",
            "",
            "        @contextlib.contextmanager",
            "        def _user_syncing():",
            "            try:",
            "                yield",
            "            finally:",
            "                _end()",
            "",
            "        return _user_syncing()",
            "",
            "    async def notify_from_replication(self, states, stream_id):",
            "        parties = await get_interested_parties(self.store, states)",
            "        room_ids_to_states, users_to_states = parties",
            "",
            "        self.notifier.on_new_event(",
            "            \"presence_key\",",
            "            stream_id,",
            "            rooms=room_ids_to_states.keys(),",
            "            users=users_to_states.keys(),",
            "        )",
            "",
            "    async def process_replication_rows(self, token, rows):",
            "        states = [",
            "            UserPresenceState(",
            "                row.user_id,",
            "                row.state,",
            "                row.last_active_ts,",
            "                row.last_federation_update_ts,",
            "                row.last_user_sync_ts,",
            "                row.status_msg,",
            "                row.currently_active,",
            "            )",
            "            for row in rows",
            "        ]",
            "",
            "        for state in states:",
            "            self.user_to_current_state[state.user_id] = state",
            "",
            "        stream_id = token",
            "        await self.notify_from_replication(states, stream_id)",
            "",
            "    def get_currently_syncing_users_for_replication(self) -> Iterable[str]:",
            "        return [",
            "            user_id",
            "            for user_id, count in self._user_to_num_current_syncs.items()",
            "            if count > 0",
            "        ]",
            "",
            "    async def set_state(self, target_user, state, ignore_status_msg=False):",
            "        \"\"\"Set the presence state of the user.",
            "        \"\"\"",
            "        presence = state[\"presence\"]",
            "",
            "        valid_presence = (",
            "            PresenceState.ONLINE,",
            "            PresenceState.UNAVAILABLE,",
            "            PresenceState.OFFLINE,",
            "        )",
            "        if presence not in valid_presence:",
            "            raise SynapseError(400, \"Invalid presence state\")",
            "",
            "        user_id = target_user.to_string()",
            "",
            "        # If presence is disabled, no-op",
            "        if not self.hs.config.use_presence:",
            "            return",
            "",
            "        # Proxy request to master",
            "        await self._set_state_client(",
            "            user_id=user_id, state=state, ignore_status_msg=ignore_status_msg",
            "        )",
            "",
            "    async def bump_presence_active_time(self, user):",
            "        \"\"\"We've seen the user do something that indicates they're interacting",
            "        with the app.",
            "        \"\"\"",
            "        # If presence is disabled, no-op",
            "        if not self.hs.config.use_presence:",
            "            return",
            "",
            "        # Proxy request to master",
            "        user_id = user.to_string()",
            "        await self._bump_active_client(user_id=user_id)",
            "",
            "",
            "class GenericWorkerSlavedStore(",
            "    # FIXME(#3714): We need to add UserDirectoryStore as we write directly",
            "    # rather than going via the correct worker.",
            "    UserDirectoryStore,",
            "    StatsStore,",
            "    UIAuthWorkerStore,",
            "    SlavedDeviceInboxStore,",
            "    SlavedDeviceStore,",
            "    SlavedReceiptsStore,",
            "    SlavedPushRuleStore,",
            "    SlavedGroupServerStore,",
            "    SlavedAccountDataStore,",
            "    SlavedPusherStore,",
            "    CensorEventsStore,",
            "    ClientIpWorkerStore,",
            "    SlavedEventStore,",
            "    SlavedKeyStore,",
            "    RoomStore,",
            "    DirectoryStore,",
            "    SlavedApplicationServiceStore,",
            "    SlavedRegistrationStore,",
            "    SlavedTransactionStore,",
            "    SlavedProfileStore,",
            "    SlavedClientIpStore,",
            "    SlavedPresenceStore,",
            "    SlavedFilteringStore,",
            "    MonthlyActiveUsersWorkerStore,",
            "    MediaRepositoryStore,",
            "    ServerMetricsStore,",
            "    SearchWorkerStore,",
            "    TransactionWorkerStore,",
            "    BaseSlavedStore,",
            "):",
            "    pass",
            "",
            "",
            "class GenericWorkerServer(HomeServer):",
            "    DATASTORE_CLASS = GenericWorkerSlavedStore",
            "",
            "    def _listen_http(self, listener_config: ListenerConfig):",
            "        port = listener_config.port",
            "        bind_addresses = listener_config.bind_addresses",
            "",
            "        assert listener_config.http_options is not None",
            "",
            "        site_tag = listener_config.http_options.tag",
            "        if site_tag is None:",
            "            site_tag = port",
            "",
            "        # We always include a health resource.",
            "        resources = {\"/health\": HealthResource()}",
            "",
            "        for res in listener_config.http_options.resources:",
            "            for name in res.names:",
            "                if name == \"metrics\":",
            "                    resources[METRICS_PREFIX] = MetricsResource(RegistryProxy)",
            "                elif name == \"client\":",
            "                    resource = JsonResource(self, canonical_json=False)",
            "",
            "                    PublicRoomListRestServlet(self).register(resource)",
            "                    RoomMemberListRestServlet(self).register(resource)",
            "                    JoinedRoomMemberListRestServlet(self).register(resource)",
            "                    RoomStateRestServlet(self).register(resource)",
            "                    RoomEventContextServlet(self).register(resource)",
            "                    RoomMessageListRestServlet(self).register(resource)",
            "                    RegisterRestServlet(self).register(resource)",
            "                    LoginRestServlet(self).register(resource)",
            "                    ThreepidRestServlet(self).register(resource)",
            "                    KeyQueryServlet(self).register(resource)",
            "                    KeyChangesServlet(self).register(resource)",
            "                    VoipRestServlet(self).register(resource)",
            "                    PushRuleRestServlet(self).register(resource)",
            "                    VersionsRestServlet(self).register(resource)",
            "                    RoomSendEventRestServlet(self).register(resource)",
            "                    RoomMembershipRestServlet(self).register(resource)",
            "                    RoomStateEventRestServlet(self).register(resource)",
            "                    JoinRoomAliasServlet(self).register(resource)",
            "                    ProfileAvatarURLRestServlet(self).register(resource)",
            "                    ProfileDisplaynameRestServlet(self).register(resource)",
            "                    ProfileRestServlet(self).register(resource)",
            "                    KeyUploadServlet(self).register(resource)",
            "                    AccountDataServlet(self).register(resource)",
            "                    RoomAccountDataServlet(self).register(resource)",
            "                    RoomTypingRestServlet(self).register(resource)",
            "",
            "                    sync.register_servlets(self, resource)",
            "                    events.register_servlets(self, resource)",
            "                    InitialSyncRestServlet(self).register(resource)",
            "                    RoomInitialSyncRestServlet(self).register(resource)",
            "",
            "                    user_directory.register_servlets(self, resource)",
            "",
            "                    # If presence is disabled, use the stub servlet that does",
            "                    # not allow sending presence",
            "                    if not self.config.use_presence:",
            "                        PresenceStatusStubServlet(self).register(resource)",
            "",
            "                    groups.register_servlets(self, resource)",
            "",
            "                    resources.update({CLIENT_API_PREFIX: resource})",
            "                elif name == \"federation\":",
            "                    resources.update({FEDERATION_PREFIX: TransportLayerServer(self)})",
            "                elif name == \"media\":",
            "                    if self.config.can_load_media_repo:",
            "                        media_repo = self.get_media_repository_resource()",
            "",
            "                        # We need to serve the admin servlets for media on the",
            "                        # worker.",
            "                        admin_resource = JsonResource(self, canonical_json=False)",
            "                        register_servlets_for_media_repo(self, admin_resource)",
            "",
            "                        resources.update(",
            "                            {",
            "                                MEDIA_PREFIX: media_repo,",
            "                                LEGACY_MEDIA_PREFIX: media_repo,",
            "                                \"/_synapse/admin\": admin_resource,",
            "                            }",
            "                        )",
            "                    else:",
            "                        logger.warning(",
            "                            \"A 'media' listener is configured but the media\"",
            "                            \" repository is disabled. Ignoring.\"",
            "                        )",
            "",
            "                if name == \"openid\" and \"federation\" not in res.names:",
            "                    # Only load the openid resource separately if federation resource",
            "                    # is not specified since federation resource includes openid",
            "                    # resource.",
            "                    resources.update(",
            "                        {",
            "                            FEDERATION_PREFIX: TransportLayerServer(",
            "                                self, servlet_groups=[\"openid\"]",
            "                            )",
            "                        }",
            "                    )",
            "",
            "                if name in [\"keys\", \"federation\"]:",
            "                    resources[SERVER_KEY_V2_PREFIX] = KeyApiV2Resource(self)",
            "",
            "                if name == \"replication\":",
            "                    resources[REPLICATION_PREFIX] = ReplicationRestResource(self)",
            "",
            "        root_resource = create_resource_tree(resources, OptionsResource())",
            "",
            "        _base.listen_tcp(",
            "            bind_addresses,",
            "            port,",
            "            SynapseSite(",
            "                \"synapse.access.http.%s\" % (site_tag,),",
            "                site_tag,",
            "                listener_config,",
            "                root_resource,",
            "                self.version_string,",
            "            ),",
            "            reactor=self.get_reactor(),",
            "        )",
            "",
            "        logger.info(\"Synapse worker now listening on port %d\", port)",
            "",
            "    def start_listening(self, listeners: Iterable[ListenerConfig]):",
            "        for listener in listeners:",
            "            if listener.type == \"http\":",
            "                self._listen_http(listener)",
            "            elif listener.type == \"manhole\":",
            "                _base.listen_tcp(",
            "                    listener.bind_addresses,",
            "                    listener.port,",
            "                    manhole(",
            "                        username=\"matrix\", password=\"rabbithole\", globals={\"hs\": self}",
            "                    ),",
            "                )",
            "            elif listener.type == \"metrics\":",
            "                if not self.get_config().enable_metrics:",
            "                    logger.warning(",
            "                        (",
            "                            \"Metrics listener configured, but \"",
            "                            \"enable_metrics is not True!\"",
            "                        )",
            "                    )",
            "                else:",
            "                    _base.listen_metrics(listener.bind_addresses, listener.port)",
            "            else:",
            "                logger.warning(\"Unsupported listener type: %s\", listener.type)",
            "",
            "        self.get_tcp_replication().start_replication(self)",
            "",
            "    async def remove_pusher(self, app_id, push_key, user_id):",
            "        self.get_tcp_replication().send_remove_pusher(app_id, push_key, user_id)",
            "",
            "    @cache_in_self",
            "    def get_replication_data_handler(self):",
            "        return GenericWorkerReplicationHandler(self)",
            "",
            "    @cache_in_self",
            "    def get_presence_handler(self):",
            "        return GenericWorkerPresence(self)",
            "",
            "",
            "class GenericWorkerReplicationHandler(ReplicationDataHandler):",
            "    def __init__(self, hs):",
            "        super().__init__(hs)",
            "",
            "        self.store = hs.get_datastore()",
            "        self.presence_handler = hs.get_presence_handler()  # type: GenericWorkerPresence",
            "        self.notifier = hs.get_notifier()",
            "",
            "        self.notify_pushers = hs.config.start_pushers",
            "        self.pusher_pool = hs.get_pusherpool()",
            "",
            "        self.send_handler = None  # type: Optional[FederationSenderHandler]",
            "        if hs.config.send_federation:",
            "            self.send_handler = FederationSenderHandler(hs)",
            "",
            "    async def on_rdata(self, stream_name, instance_name, token, rows):",
            "        await super().on_rdata(stream_name, instance_name, token, rows)",
            "        await self._process_and_notify(stream_name, instance_name, token, rows)",
            "",
            "    async def _process_and_notify(self, stream_name, instance_name, token, rows):",
            "        try:",
            "            if self.send_handler:",
            "                await self.send_handler.process_replication_rows(",
            "                    stream_name, token, rows",
            "                )",
            "",
            "            if stream_name == PushRulesStream.NAME:",
            "                self.notifier.on_new_event(",
            "                    \"push_rules_key\", token, users=[row.user_id for row in rows]",
            "                )",
            "            elif stream_name in (AccountDataStream.NAME, TagAccountDataStream.NAME):",
            "                self.notifier.on_new_event(",
            "                    \"account_data_key\", token, users=[row.user_id for row in rows]",
            "                )",
            "            elif stream_name == ReceiptsStream.NAME:",
            "                self.notifier.on_new_event(",
            "                    \"receipt_key\", token, rooms=[row.room_id for row in rows]",
            "                )",
            "                await self.pusher_pool.on_new_receipts(",
            "                    token, token, {row.room_id for row in rows}",
            "                )",
            "            elif stream_name == ToDeviceStream.NAME:",
            "                entities = [row.entity for row in rows if row.entity.startswith(\"@\")]",
            "                if entities:",
            "                    self.notifier.on_new_event(\"to_device_key\", token, users=entities)",
            "            elif stream_name == DeviceListsStream.NAME:",
            "                all_room_ids = set()  # type: Set[str]",
            "                for row in rows:",
            "                    if row.entity.startswith(\"@\"):",
            "                        room_ids = await self.store.get_rooms_for_user(row.entity)",
            "                        all_room_ids.update(room_ids)",
            "                self.notifier.on_new_event(\"device_list_key\", token, rooms=all_room_ids)",
            "            elif stream_name == PresenceStream.NAME:",
            "                await self.presence_handler.process_replication_rows(token, rows)",
            "            elif stream_name == GroupServerStream.NAME:",
            "                self.notifier.on_new_event(",
            "                    \"groups_key\", token, users=[row.user_id for row in rows]",
            "                )",
            "            elif stream_name == PushersStream.NAME:",
            "                for row in rows:",
            "                    if row.deleted:",
            "                        self.stop_pusher(row.user_id, row.app_id, row.pushkey)",
            "                    else:",
            "                        await self.start_pusher(row.user_id, row.app_id, row.pushkey)",
            "        except Exception:",
            "            logger.exception(\"Error processing replication\")",
            "",
            "    async def on_position(self, stream_name: str, instance_name: str, token: int):",
            "        await super().on_position(stream_name, instance_name, token)",
            "        # Also call on_rdata to ensure that stream positions are properly reset.",
            "        await self.on_rdata(stream_name, instance_name, token, [])",
            "",
            "    def stop_pusher(self, user_id, app_id, pushkey):",
            "        if not self.notify_pushers:",
            "            return",
            "",
            "        key = \"%s:%s\" % (app_id, pushkey)",
            "        pushers_for_user = self.pusher_pool.pushers.get(user_id, {})",
            "        pusher = pushers_for_user.pop(key, None)",
            "        if pusher is None:",
            "            return",
            "        logger.info(\"Stopping pusher %r / %r\", user_id, key)",
            "        pusher.on_stop()",
            "",
            "    async def start_pusher(self, user_id, app_id, pushkey):",
            "        if not self.notify_pushers:",
            "            return",
            "",
            "        key = \"%s:%s\" % (app_id, pushkey)",
            "        logger.info(\"Starting pusher %r / %r\", user_id, key)",
            "        return await self.pusher_pool.start_pusher_by_id(app_id, pushkey, user_id)",
            "",
            "    def on_remote_server_up(self, server: str):",
            "        \"\"\"Called when get a new REMOTE_SERVER_UP command.\"\"\"",
            "",
            "        # Let's wake up the transaction queue for the server in case we have",
            "        # pending stuff to send to it.",
            "        if self.send_handler:",
            "            self.send_handler.wake_destination(server)",
            "",
            "",
            "class FederationSenderHandler:",
            "    \"\"\"Processes the fedration replication stream",
            "",
            "    This class is only instantiate on the worker responsible for sending outbound",
            "    federation transactions. It receives rows from the replication stream and forwards",
            "    the appropriate entries to the FederationSender class.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: GenericWorkerServer):",
            "        self.store = hs.get_datastore()",
            "        self._is_mine_id = hs.is_mine_id",
            "        self.federation_sender = hs.get_federation_sender()",
            "        self._hs = hs",
            "",
            "        # Stores the latest position in the federation stream we've gotten up",
            "        # to. This is always set before we use it.",
            "        self.federation_position = None",
            "",
            "        self._fed_position_linearizer = Linearizer(name=\"_fed_position_linearizer\")",
            "",
            "    def on_start(self):",
            "        # There may be some events that are persisted but haven't been sent,",
            "        # so send them now.",
            "        self.federation_sender.notify_new_events(",
            "            self.store.get_room_max_stream_ordering()",
            "        )",
            "",
            "    def wake_destination(self, server: str):",
            "        self.federation_sender.wake_destination(server)",
            "",
            "    async def process_replication_rows(self, stream_name, token, rows):",
            "        # The federation stream contains things that we want to send out, e.g.",
            "        # presence, typing, etc.",
            "        if stream_name == \"federation\":",
            "            send_queue.process_rows_for_federation(self.federation_sender, rows)",
            "            await self.update_token(token)",
            "",
            "        # ... and when new receipts happen",
            "        elif stream_name == ReceiptsStream.NAME:",
            "            await self._on_new_receipts(rows)",
            "",
            "        # ... as well as device updates and messages",
            "        elif stream_name == DeviceListsStream.NAME:",
            "            # The entities are either user IDs (starting with '@') whose devices",
            "            # have changed, or remote servers that we need to tell about",
            "            # changes.",
            "            hosts = {row.entity for row in rows if not row.entity.startswith(\"@\")}",
            "            for host in hosts:",
            "                self.federation_sender.send_device_messages(host)",
            "",
            "        elif stream_name == ToDeviceStream.NAME:",
            "            # The to_device stream includes stuff to be pushed to both local",
            "            # clients and remote servers, so we ignore entities that start with",
            "            # '@' (since they'll be local users rather than destinations).",
            "            hosts = {row.entity for row in rows if not row.entity.startswith(\"@\")}",
            "            for host in hosts:",
            "                self.federation_sender.send_device_messages(host)",
            "",
            "    async def _on_new_receipts(self, rows):",
            "        \"\"\"",
            "        Args:",
            "            rows (Iterable[synapse.replication.tcp.streams.ReceiptsStream.ReceiptsStreamRow]):",
            "                new receipts to be processed",
            "        \"\"\"",
            "        for receipt in rows:",
            "            # we only want to send on receipts for our own users",
            "            if not self._is_mine_id(receipt.user_id):",
            "                continue",
            "            receipt_info = ReadReceipt(",
            "                receipt.room_id,",
            "                receipt.receipt_type,",
            "                receipt.user_id,",
            "                [receipt.event_id],",
            "                receipt.data,",
            "            )",
            "            await self.federation_sender.send_read_receipt(receipt_info)",
            "",
            "    async def update_token(self, token):",
            "        \"\"\"Update the record of where we have processed to in the federation stream.",
            "",
            "        Called after we have processed a an update received over replication. Sends",
            "        a FEDERATION_ACK back to the master, and stores the token that we have processed",
            "         in `federation_stream_position` so that we can restart where we left off.",
            "        \"\"\"",
            "        self.federation_position = token",
            "",
            "        # We save and send the ACK to master asynchronously, so we don't block",
            "        # processing on persistence. We don't need to do this operation for",
            "        # every single RDATA we receive, we just need to do it periodically.",
            "",
            "        if self._fed_position_linearizer.is_queued(None):",
            "            # There is already a task queued up to save and send the token, so",
            "            # no need to queue up another task.",
            "            return",
            "",
            "        run_as_background_process(\"_save_and_send_ack\", self._save_and_send_ack)",
            "",
            "    async def _save_and_send_ack(self):",
            "        \"\"\"Save the current federation position in the database and send an ACK",
            "        to master with where we're up to.",
            "        \"\"\"",
            "        try:",
            "            # We linearize here to ensure we don't have races updating the token",
            "            #",
            "            # XXX this appears to be redundant, since the ReplicationCommandHandler",
            "            # has a linearizer which ensures that we only process one line of",
            "            # replication data at a time. Should we remove it, or is it doing useful",
            "            # service for robustness? Or could we replace it with an assertion that",
            "            # we're not being re-entered?",
            "",
            "            with (await self._fed_position_linearizer.queue(None)):",
            "                # We persist and ack the same position, so we take a copy of it",
            "                # here as otherwise it can get modified from underneath us.",
            "                current_position = self.federation_position",
            "",
            "                await self.store.update_federation_out_pos(",
            "                    \"federation\", current_position",
            "                )",
            "",
            "                # We ACK this token over replication so that the master can drop",
            "                # its in memory queues",
            "                self._hs.get_tcp_replication().send_federation_ack(current_position)",
            "        except Exception:",
            "            logger.exception(\"Error updating federation stream position\")",
            "",
            "",
            "def start(config_options):",
            "    try:",
            "        config = HomeServerConfig.load_config(\"Synapse worker\", config_options)",
            "    except ConfigError as e:",
            "        sys.stderr.write(\"\\n\" + str(e) + \"\\n\")",
            "        sys.exit(1)",
            "",
            "    # For backwards compatibility let any of the old app names.",
            "    assert config.worker_app in (",
            "        \"synapse.app.appservice\",",
            "        \"synapse.app.client_reader\",",
            "        \"synapse.app.event_creator\",",
            "        \"synapse.app.federation_reader\",",
            "        \"synapse.app.federation_sender\",",
            "        \"synapse.app.frontend_proxy\",",
            "        \"synapse.app.generic_worker\",",
            "        \"synapse.app.media_repository\",",
            "        \"synapse.app.pusher\",",
            "        \"synapse.app.synchrotron\",",
            "        \"synapse.app.user_dir\",",
            "    )",
            "",
            "    if config.worker_app == \"synapse.app.appservice\":",
            "        if config.appservice.notify_appservices:",
            "            sys.stderr.write(",
            "                \"\\nThe appservices must be disabled in the main synapse process\"",
            "                \"\\nbefore they can be run in a separate worker.\"",
            "                \"\\nPlease add ``notify_appservices: false`` to the main config\"",
            "                \"\\n\"",
            "            )",
            "            sys.exit(1)",
            "",
            "        # Force the appservice to start since they will be disabled in the main config",
            "        config.appservice.notify_appservices = True",
            "    else:",
            "        # For other worker types we force this to off.",
            "        config.appservice.notify_appservices = False",
            "",
            "    if config.worker_app == \"synapse.app.pusher\":",
            "        if config.server.start_pushers:",
            "            sys.stderr.write(",
            "                \"\\nThe pushers must be disabled in the main synapse process\"",
            "                \"\\nbefore they can be run in a separate worker.\"",
            "                \"\\nPlease add ``start_pushers: false`` to the main config\"",
            "                \"\\n\"",
            "            )",
            "            sys.exit(1)",
            "",
            "        # Force the pushers to start since they will be disabled in the main config",
            "        config.server.start_pushers = True",
            "    else:",
            "        # For other worker types we force this to off.",
            "        config.server.start_pushers = False",
            "",
            "    if config.worker_app == \"synapse.app.user_dir\":",
            "        if config.server.update_user_directory:",
            "            sys.stderr.write(",
            "                \"\\nThe update_user_directory must be disabled in the main synapse process\"",
            "                \"\\nbefore they can be run in a separate worker.\"",
            "                \"\\nPlease add ``update_user_directory: false`` to the main config\"",
            "                \"\\n\"",
            "            )",
            "            sys.exit(1)",
            "",
            "        # Force the pushers to start since they will be disabled in the main config",
            "        config.server.update_user_directory = True",
            "    else:",
            "        # For other worker types we force this to off.",
            "        config.server.update_user_directory = False",
            "",
            "    if config.worker_app == \"synapse.app.federation_sender\":",
            "        if config.worker.send_federation:",
            "            sys.stderr.write(",
            "                \"\\nThe send_federation must be disabled in the main synapse process\"",
            "                \"\\nbefore they can be run in a separate worker.\"",
            "                \"\\nPlease add ``send_federation: false`` to the main config\"",
            "                \"\\n\"",
            "            )",
            "            sys.exit(1)",
            "",
            "        # Force the pushers to start since they will be disabled in the main config",
            "        config.worker.send_federation = True",
            "    else:",
            "        # For other worker types we force this to off.",
            "        config.worker.send_federation = False",
            "",
            "    synapse.events.USE_FROZEN_DICTS = config.use_frozen_dicts",
            "",
            "    hs = GenericWorkerServer(",
            "        config.server_name,",
            "        config=config,",
            "        version_string=\"Synapse/\" + get_version_string(synapse),",
            "    )",
            "",
            "    setup_logging(hs, config, use_worker_options=True)",
            "",
            "    hs.setup()",
            "",
            "    # Ensure the replication streamer is always started in case we write to any",
            "    # streams. Will no-op if no streams can be written to by this worker.",
            "    hs.get_replication_streamer()",
            "",
            "    reactor.addSystemEventTrigger(",
            "        \"before\", \"startup\", _base.start, hs, config.worker_listeners",
            "    )",
            "",
            "    _base.start_worker_reactor(\"synapse-generic-worker\", config)",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    with LoggingContext(\"main\"):",
            "        start(sys.argv[1:])"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "269": [
                "GenericWorkerPresence",
                "__init__"
            ]
        },
        "addLocation": []
    },
    "synapse/config/federation.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 36,
                "PatchRowcode": "             for domain in federation_domain_whitelist:"
            },
            "1": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 37,
                "PatchRowcode": "                 self.federation_domain_whitelist[domain] = True"
            },
            "2": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 38,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.federation_ip_range_blacklist = config.get("
            },
            "4": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            \"federation_ip_range_blacklist\", []"
            },
            "5": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        )"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 39,
                "PatchRowcode": "+        ip_range_blacklist = config.get(\"ip_range_blacklist\", [])"
            },
            "7": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 40,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 41,
                "PatchRowcode": "         # Attempt to create an IPSet from the given ranges"
            },
            "9": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 42,
                "PatchRowcode": "         try:"
            },
            "10": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self.federation_ip_range_blacklist = IPSet("
            },
            "11": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                self.federation_ip_range_blacklist"
            },
            "12": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "13": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "14": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # Always blacklist 0.0.0.0, ::"
            },
            "15": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self.federation_ip_range_blacklist.update([\"0.0.0.0\", \"::\"])"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+            self.ip_range_blacklist = IPSet(ip_range_blacklist)"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+        except Exception as e:"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+            raise ConfigError(\"Invalid range(s) provided in ip_range_blacklist: %s\" % e)"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+        # Always blacklist 0.0.0.0, ::"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+        self.ip_range_blacklist.update([\"0.0.0.0\", \"::\"])"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+        # The federation_ip_range_blacklist is used for backwards-compatibility"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+        # and only applies to federation and identity servers. If it is not given,"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+        # default to ip_range_blacklist."
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+        federation_ip_range_blacklist = config.get("
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+            \"federation_ip_range_blacklist\", ip_range_blacklist"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 54,
                "PatchRowcode": "+        )"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+        try:"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 56,
                "PatchRowcode": "+            self.federation_ip_range_blacklist = IPSet(federation_ip_range_blacklist)"
            },
            "30": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 57,
                "PatchRowcode": "         except Exception as e:"
            },
            "31": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 58,
                "PatchRowcode": "             raise ConfigError("
            },
            "32": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 59,
                "PatchRowcode": "                 \"Invalid range(s) provided in federation_ip_range_blacklist: %s\" % e"
            },
            "33": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 60,
                "PatchRowcode": "             )"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 61,
                "PatchRowcode": "+        # Always blacklist 0.0.0.0, ::"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+        self.federation_ip_range_blacklist.update([\"0.0.0.0\", \"::\"])"
            },
            "36": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": 63,
                "PatchRowcode": " "
            },
            "37": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 64,
                "PatchRowcode": "         federation_metrics_domains = config.get(\"federation_metrics_domains\") or []"
            },
            "38": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 65,
                "PatchRowcode": "         validate_config("
            },
            "39": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 84,
                "PatchRowcode": "         #  - nyc.example.com"
            },
            "40": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 85,
                "PatchRowcode": "         #  - syd.example.com"
            },
            "41": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 86,
                "PatchRowcode": " "
            },
            "42": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # Prevent federation requests from being sent to the following"
            },
            "43": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # blacklist IP address CIDR ranges. If this option is not specified, or"
            },
            "44": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # specified with an empty list, no ip range blacklist will be enforced."
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+        # Prevent outgoing requests from being sent to the following blacklisted IP address"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+        # CIDR ranges. If this option is not specified, or specified with an empty list,"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 89,
                "PatchRowcode": "+        # no IP range blacklist will be enforced."
            },
            "48": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 90,
                "PatchRowcode": "         #"
            },
            "49": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # As of Synapse v1.4.0 this option also affects any outbound requests to identity"
            },
            "50": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # servers provided by user input."
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 91,
                "PatchRowcode": "+        # The blacklist applies to the outbound requests for federation, identity servers,"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 92,
                "PatchRowcode": "+        # push servers, and for checking key validitity for third-party invite events."
            },
            "53": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 93,
                "PatchRowcode": "         #"
            },
            "54": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 94,
                "PatchRowcode": "         # (0.0.0.0 and :: are always blacklisted, whether or not they are explicitly"
            },
            "55": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": 95,
                "PatchRowcode": "         # listed here, since they correspond to unroutable addresses.)"
            },
            "56": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": 96,
                "PatchRowcode": "         #"
            },
            "57": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        federation_ip_range_blacklist:"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+        # This option replaces federation_ip_range_blacklist in Synapse v1.24.0."
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+        #"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+        ip_range_blacklist:"
            },
            "61": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 100,
                "PatchRowcode": "           - '127.0.0.0/8'"
            },
            "62": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 101,
                "PatchRowcode": "           - '10.0.0.0/8'"
            },
            "63": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 102,
                "PatchRowcode": "           - '172.16.0.0/12'"
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2020 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "from typing import Optional",
            "",
            "from netaddr import IPSet",
            "",
            "from synapse.config._base import Config, ConfigError",
            "from synapse.config._util import validate_config",
            "",
            "",
            "class FederationConfig(Config):",
            "    section = \"federation\"",
            "",
            "    def read_config(self, config, **kwargs):",
            "        # FIXME: federation_domain_whitelist needs sytests",
            "        self.federation_domain_whitelist = None  # type: Optional[dict]",
            "        federation_domain_whitelist = config.get(\"federation_domain_whitelist\", None)",
            "",
            "        if federation_domain_whitelist is not None:",
            "            # turn the whitelist into a hash for speed of lookup",
            "            self.federation_domain_whitelist = {}",
            "",
            "            for domain in federation_domain_whitelist:",
            "                self.federation_domain_whitelist[domain] = True",
            "",
            "        self.federation_ip_range_blacklist = config.get(",
            "            \"federation_ip_range_blacklist\", []",
            "        )",
            "",
            "        # Attempt to create an IPSet from the given ranges",
            "        try:",
            "            self.federation_ip_range_blacklist = IPSet(",
            "                self.federation_ip_range_blacklist",
            "            )",
            "",
            "            # Always blacklist 0.0.0.0, ::",
            "            self.federation_ip_range_blacklist.update([\"0.0.0.0\", \"::\"])",
            "        except Exception as e:",
            "            raise ConfigError(",
            "                \"Invalid range(s) provided in federation_ip_range_blacklist: %s\" % e",
            "            )",
            "",
            "        federation_metrics_domains = config.get(\"federation_metrics_domains\") or []",
            "        validate_config(",
            "            _METRICS_FOR_DOMAINS_SCHEMA,",
            "            federation_metrics_domains,",
            "            (\"federation_metrics_domains\",),",
            "        )",
            "        self.federation_metrics_domains = set(federation_metrics_domains)",
            "",
            "    def generate_config_section(self, config_dir_path, server_name, **kwargs):",
            "        return \"\"\"\\",
            "        ## Federation ##",
            "",
            "        # Restrict federation to the following whitelist of domains.",
            "        # N.B. we recommend also firewalling your federation listener to limit",
            "        # inbound federation traffic as early as possible, rather than relying",
            "        # purely on this application-layer restriction.  If not specified, the",
            "        # default is to whitelist everything.",
            "        #",
            "        #federation_domain_whitelist:",
            "        #  - lon.example.com",
            "        #  - nyc.example.com",
            "        #  - syd.example.com",
            "",
            "        # Prevent federation requests from being sent to the following",
            "        # blacklist IP address CIDR ranges. If this option is not specified, or",
            "        # specified with an empty list, no ip range blacklist will be enforced.",
            "        #",
            "        # As of Synapse v1.4.0 this option also affects any outbound requests to identity",
            "        # servers provided by user input.",
            "        #",
            "        # (0.0.0.0 and :: are always blacklisted, whether or not they are explicitly",
            "        # listed here, since they correspond to unroutable addresses.)",
            "        #",
            "        federation_ip_range_blacklist:",
            "          - '127.0.0.0/8'",
            "          - '10.0.0.0/8'",
            "          - '172.16.0.0/12'",
            "          - '192.168.0.0/16'",
            "          - '100.64.0.0/10'",
            "          - '169.254.0.0/16'",
            "          - '::1/128'",
            "          - 'fe80::/64'",
            "          - 'fc00::/7'",
            "",
            "        # Report prometheus metrics on the age of PDUs being sent to and received from",
            "        # the following domains. This can be used to give an idea of \"delay\" on inbound",
            "        # and outbound federation, though be aware that any delay can be due to problems",
            "        # at either end or with the intermediate network.",
            "        #",
            "        # By default, no domains are monitored in this way.",
            "        #",
            "        #federation_metrics_domains:",
            "        #  - matrix.org",
            "        #  - example.com",
            "        \"\"\"",
            "",
            "",
            "_METRICS_FOR_DOMAINS_SCHEMA = {\"type\": \"array\", \"items\": {\"type\": \"string\"}}"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2020 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "from typing import Optional",
            "",
            "from netaddr import IPSet",
            "",
            "from synapse.config._base import Config, ConfigError",
            "from synapse.config._util import validate_config",
            "",
            "",
            "class FederationConfig(Config):",
            "    section = \"federation\"",
            "",
            "    def read_config(self, config, **kwargs):",
            "        # FIXME: federation_domain_whitelist needs sytests",
            "        self.federation_domain_whitelist = None  # type: Optional[dict]",
            "        federation_domain_whitelist = config.get(\"federation_domain_whitelist\", None)",
            "",
            "        if federation_domain_whitelist is not None:",
            "            # turn the whitelist into a hash for speed of lookup",
            "            self.federation_domain_whitelist = {}",
            "",
            "            for domain in federation_domain_whitelist:",
            "                self.federation_domain_whitelist[domain] = True",
            "",
            "        ip_range_blacklist = config.get(\"ip_range_blacklist\", [])",
            "",
            "        # Attempt to create an IPSet from the given ranges",
            "        try:",
            "            self.ip_range_blacklist = IPSet(ip_range_blacklist)",
            "        except Exception as e:",
            "            raise ConfigError(\"Invalid range(s) provided in ip_range_blacklist: %s\" % e)",
            "        # Always blacklist 0.0.0.0, ::",
            "        self.ip_range_blacklist.update([\"0.0.0.0\", \"::\"])",
            "",
            "        # The federation_ip_range_blacklist is used for backwards-compatibility",
            "        # and only applies to federation and identity servers. If it is not given,",
            "        # default to ip_range_blacklist.",
            "        federation_ip_range_blacklist = config.get(",
            "            \"federation_ip_range_blacklist\", ip_range_blacklist",
            "        )",
            "        try:",
            "            self.federation_ip_range_blacklist = IPSet(federation_ip_range_blacklist)",
            "        except Exception as e:",
            "            raise ConfigError(",
            "                \"Invalid range(s) provided in federation_ip_range_blacklist: %s\" % e",
            "            )",
            "        # Always blacklist 0.0.0.0, ::",
            "        self.federation_ip_range_blacklist.update([\"0.0.0.0\", \"::\"])",
            "",
            "        federation_metrics_domains = config.get(\"federation_metrics_domains\") or []",
            "        validate_config(",
            "            _METRICS_FOR_DOMAINS_SCHEMA,",
            "            federation_metrics_domains,",
            "            (\"federation_metrics_domains\",),",
            "        )",
            "        self.federation_metrics_domains = set(federation_metrics_domains)",
            "",
            "    def generate_config_section(self, config_dir_path, server_name, **kwargs):",
            "        return \"\"\"\\",
            "        ## Federation ##",
            "",
            "        # Restrict federation to the following whitelist of domains.",
            "        # N.B. we recommend also firewalling your federation listener to limit",
            "        # inbound federation traffic as early as possible, rather than relying",
            "        # purely on this application-layer restriction.  If not specified, the",
            "        # default is to whitelist everything.",
            "        #",
            "        #federation_domain_whitelist:",
            "        #  - lon.example.com",
            "        #  - nyc.example.com",
            "        #  - syd.example.com",
            "",
            "        # Prevent outgoing requests from being sent to the following blacklisted IP address",
            "        # CIDR ranges. If this option is not specified, or specified with an empty list,",
            "        # no IP range blacklist will be enforced.",
            "        #",
            "        # The blacklist applies to the outbound requests for federation, identity servers,",
            "        # push servers, and for checking key validitity for third-party invite events.",
            "        #",
            "        # (0.0.0.0 and :: are always blacklisted, whether or not they are explicitly",
            "        # listed here, since they correspond to unroutable addresses.)",
            "        #",
            "        # This option replaces federation_ip_range_blacklist in Synapse v1.24.0.",
            "        #",
            "        ip_range_blacklist:",
            "          - '127.0.0.0/8'",
            "          - '10.0.0.0/8'",
            "          - '172.16.0.0/12'",
            "          - '192.168.0.0/16'",
            "          - '100.64.0.0/10'",
            "          - '169.254.0.0/16'",
            "          - '::1/128'",
            "          - 'fe80::/64'",
            "          - 'fc00::/7'",
            "",
            "        # Report prometheus metrics on the age of PDUs being sent to and received from",
            "        # the following domains. This can be used to give an idea of \"delay\" on inbound",
            "        # and outbound federation, though be aware that any delay can be due to problems",
            "        # at either end or with the intermediate network.",
            "        #",
            "        # By default, no domains are monitored in this way.",
            "        #",
            "        #federation_metrics_domains:",
            "        #  - matrix.org",
            "        #  - example.com",
            "        \"\"\"",
            "",
            "",
            "_METRICS_FOR_DOMAINS_SCHEMA = {\"type\": \"array\", \"items\": {\"type\": \"string\"}}"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "39": [
                "FederationConfig",
                "read_config"
            ],
            "40": [
                "FederationConfig",
                "read_config"
            ],
            "41": [
                "FederationConfig",
                "read_config"
            ],
            "45": [
                "FederationConfig",
                "read_config"
            ],
            "46": [
                "FederationConfig",
                "read_config"
            ],
            "47": [
                "FederationConfig",
                "read_config"
            ],
            "48": [
                "FederationConfig",
                "read_config"
            ],
            "49": [
                "FederationConfig",
                "read_config"
            ],
            "50": [
                "FederationConfig",
                "read_config"
            ],
            "79": [
                "FederationConfig",
                "generate_config_section"
            ],
            "80": [
                "FederationConfig",
                "generate_config_section"
            ],
            "81": [
                "FederationConfig",
                "generate_config_section"
            ],
            "83": [
                "FederationConfig",
                "generate_config_section"
            ],
            "84": [
                "FederationConfig",
                "generate_config_section"
            ],
            "89": [
                "FederationConfig",
                "generate_config_section"
            ]
        },
        "addLocation": []
    },
    "synapse/crypto/keyring.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 578,
                "afterPatchRowNumber": 578,
                "PatchRowcode": "     def __init__(self, hs):"
            },
            "1": {
                "beforePatchRowNumber": 579,
                "afterPatchRowNumber": 579,
                "PatchRowcode": "         super().__init__(hs)"
            },
            "2": {
                "beforePatchRowNumber": 580,
                "afterPatchRowNumber": 580,
                "PatchRowcode": "         self.clock = hs.get_clock()"
            },
            "3": {
                "beforePatchRowNumber": 581,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.client = hs.get_http_client()"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 581,
                "PatchRowcode": "+        self.client = hs.get_federation_http_client()"
            },
            "5": {
                "beforePatchRowNumber": 582,
                "afterPatchRowNumber": 582,
                "PatchRowcode": "         self.key_servers = self.config.key_servers"
            },
            "6": {
                "beforePatchRowNumber": 583,
                "afterPatchRowNumber": 583,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 584,
                "afterPatchRowNumber": 584,
                "PatchRowcode": "     async def get_keys(self, keys_to_fetch):"
            },
            "8": {
                "beforePatchRowNumber": 748,
                "afterPatchRowNumber": 748,
                "PatchRowcode": "     def __init__(self, hs):"
            },
            "9": {
                "beforePatchRowNumber": 749,
                "afterPatchRowNumber": 749,
                "PatchRowcode": "         super().__init__(hs)"
            },
            "10": {
                "beforePatchRowNumber": 750,
                "afterPatchRowNumber": 750,
                "PatchRowcode": "         self.clock = hs.get_clock()"
            },
            "11": {
                "beforePatchRowNumber": 751,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.client = hs.get_http_client()"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 751,
                "PatchRowcode": "+        self.client = hs.get_federation_http_client()"
            },
            "13": {
                "beforePatchRowNumber": 752,
                "afterPatchRowNumber": 752,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 753,
                "afterPatchRowNumber": 753,
                "PatchRowcode": "     async def get_keys(self, keys_to_fetch):"
            },
            "15": {
                "beforePatchRowNumber": 754,
                "afterPatchRowNumber": 754,
                "PatchRowcode": "         \"\"\""
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2017, 2018 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import logging",
            "import urllib",
            "from collections import defaultdict",
            "",
            "import attr",
            "from signedjson.key import (",
            "    decode_verify_key_bytes,",
            "    encode_verify_key_base64,",
            "    is_signing_algorithm_supported,",
            ")",
            "from signedjson.sign import (",
            "    SignatureVerifyException,",
            "    encode_canonical_json,",
            "    signature_ids,",
            "    verify_signed_json,",
            ")",
            "from unpaddedbase64 import decode_base64",
            "",
            "from twisted.internet import defer",
            "",
            "from synapse.api.errors import (",
            "    Codes,",
            "    HttpResponseException,",
            "    RequestSendFailed,",
            "    SynapseError,",
            ")",
            "from synapse.logging.context import (",
            "    PreserveLoggingContext,",
            "    make_deferred_yieldable,",
            "    preserve_fn,",
            "    run_in_background,",
            ")",
            "from synapse.storage.keys import FetchKeyResult",
            "from synapse.util import unwrapFirstError",
            "from synapse.util.async_helpers import yieldable_gather_results",
            "from synapse.util.metrics import Measure",
            "from synapse.util.retryutils import NotRetryingDestination",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "@attr.s(slots=True, cmp=False)",
            "class VerifyJsonRequest:",
            "    \"\"\"",
            "    A request to verify a JSON object.",
            "",
            "    Attributes:",
            "        server_name(str): The name of the server to verify against.",
            "",
            "        key_ids(set[str]): The set of key_ids to that could be used to verify the",
            "            JSON object",
            "",
            "        json_object(dict): The JSON object to verify.",
            "",
            "        minimum_valid_until_ts (int): time at which we require the signing key to",
            "            be valid. (0 implies we don't care)",
            "",
            "        key_ready (Deferred[str, str, nacl.signing.VerifyKey]):",
            "            A deferred (server_name, key_id, verify_key) tuple that resolves when",
            "            a verify key has been fetched. The deferreds' callbacks are run with no",
            "            logcontext.",
            "",
            "            If we are unable to find a key which satisfies the request, the deferred",
            "            errbacks with an M_UNAUTHORIZED SynapseError.",
            "    \"\"\"",
            "",
            "    server_name = attr.ib()",
            "    json_object = attr.ib()",
            "    minimum_valid_until_ts = attr.ib()",
            "    request_name = attr.ib()",
            "    key_ids = attr.ib(init=False)",
            "    key_ready = attr.ib(default=attr.Factory(defer.Deferred))",
            "",
            "    def __attrs_post_init__(self):",
            "        self.key_ids = signature_ids(self.json_object, self.server_name)",
            "",
            "",
            "class KeyLookupError(ValueError):",
            "    pass",
            "",
            "",
            "class Keyring:",
            "    def __init__(self, hs, key_fetchers=None):",
            "        self.clock = hs.get_clock()",
            "",
            "        if key_fetchers is None:",
            "            key_fetchers = (",
            "                StoreKeyFetcher(hs),",
            "                PerspectivesKeyFetcher(hs),",
            "                ServerKeyFetcher(hs),",
            "            )",
            "        self._key_fetchers = key_fetchers",
            "",
            "        # map from server name to Deferred. Has an entry for each server with",
            "        # an ongoing key download; the Deferred completes once the download",
            "        # completes.",
            "        #",
            "        # These are regular, logcontext-agnostic Deferreds.",
            "        self.key_downloads = {}",
            "",
            "    def verify_json_for_server(",
            "        self, server_name, json_object, validity_time, request_name",
            "    ):",
            "        \"\"\"Verify that a JSON object has been signed by a given server",
            "",
            "        Args:",
            "            server_name (str): name of the server which must have signed this object",
            "",
            "            json_object (dict): object to be checked",
            "",
            "            validity_time (int): timestamp at which we require the signing key to",
            "                be valid. (0 implies we don't care)",
            "",
            "            request_name (str): an identifier for this json object (eg, an event id)",
            "                for logging.",
            "",
            "        Returns:",
            "            Deferred[None]: completes if the the object was correctly signed, otherwise",
            "                errbacks with an error",
            "        \"\"\"",
            "        req = VerifyJsonRequest(server_name, json_object, validity_time, request_name)",
            "        requests = (req,)",
            "        return make_deferred_yieldable(self._verify_objects(requests)[0])",
            "",
            "    def verify_json_objects_for_server(self, server_and_json):",
            "        \"\"\"Bulk verifies signatures of json objects, bulk fetching keys as",
            "        necessary.",
            "",
            "        Args:",
            "            server_and_json (iterable[Tuple[str, dict, int, str]):",
            "                Iterable of (server_name, json_object, validity_time, request_name)",
            "                tuples.",
            "",
            "                validity_time is a timestamp at which the signing key must be",
            "                valid.",
            "",
            "                request_name is an identifier for this json object (eg, an event id)",
            "                for logging.",
            "",
            "        Returns:",
            "            List<Deferred[None]>: for each input triplet, a deferred indicating success",
            "                or failure to verify each json object's signature for the given",
            "                server_name. The deferreds run their callbacks in the sentinel",
            "                logcontext.",
            "        \"\"\"",
            "        return self._verify_objects(",
            "            VerifyJsonRequest(server_name, json_object, validity_time, request_name)",
            "            for server_name, json_object, validity_time, request_name in server_and_json",
            "        )",
            "",
            "    def _verify_objects(self, verify_requests):",
            "        \"\"\"Does the work of verify_json_[objects_]for_server",
            "",
            "",
            "        Args:",
            "            verify_requests (iterable[VerifyJsonRequest]):",
            "                Iterable of verification requests.",
            "",
            "        Returns:",
            "            List<Deferred[None]>: for each input item, a deferred indicating success",
            "                or failure to verify each json object's signature for the given",
            "                server_name. The deferreds run their callbacks in the sentinel",
            "                logcontext.",
            "        \"\"\"",
            "        # a list of VerifyJsonRequests which are awaiting a key lookup",
            "        key_lookups = []",
            "        handle = preserve_fn(_handle_key_deferred)",
            "",
            "        def process(verify_request):",
            "            \"\"\"Process an entry in the request list",
            "",
            "            Adds a key request to key_lookups, and returns a deferred which",
            "            will complete or fail (in the sentinel context) when verification completes.",
            "            \"\"\"",
            "            if not verify_request.key_ids:",
            "                return defer.fail(",
            "                    SynapseError(",
            "                        400,",
            "                        \"Not signed by %s\" % (verify_request.server_name,),",
            "                        Codes.UNAUTHORIZED,",
            "                    )",
            "                )",
            "",
            "            logger.debug(",
            "                \"Verifying %s for %s with key_ids %s, min_validity %i\",",
            "                verify_request.request_name,",
            "                verify_request.server_name,",
            "                verify_request.key_ids,",
            "                verify_request.minimum_valid_until_ts,",
            "            )",
            "",
            "            # add the key request to the queue, but don't start it off yet.",
            "            key_lookups.append(verify_request)",
            "",
            "            # now run _handle_key_deferred, which will wait for the key request",
            "            # to complete and then do the verification.",
            "            #",
            "            # We want _handle_key_request to log to the right context, so we",
            "            # wrap it with preserve_fn (aka run_in_background)",
            "            return handle(verify_request)",
            "",
            "        results = [process(r) for r in verify_requests]",
            "",
            "        if key_lookups:",
            "            run_in_background(self._start_key_lookups, key_lookups)",
            "",
            "        return results",
            "",
            "    async def _start_key_lookups(self, verify_requests):",
            "        \"\"\"Sets off the key fetches for each verify request",
            "",
            "        Once each fetch completes, verify_request.key_ready will be resolved.",
            "",
            "        Args:",
            "            verify_requests (List[VerifyJsonRequest]):",
            "        \"\"\"",
            "",
            "        try:",
            "            # map from server name to a set of outstanding request ids",
            "            server_to_request_ids = {}",
            "",
            "            for verify_request in verify_requests:",
            "                server_name = verify_request.server_name",
            "                request_id = id(verify_request)",
            "                server_to_request_ids.setdefault(server_name, set()).add(request_id)",
            "",
            "            # Wait for any previous lookups to complete before proceeding.",
            "            await self.wait_for_previous_lookups(server_to_request_ids.keys())",
            "",
            "            # take out a lock on each of the servers by sticking a Deferred in",
            "            # key_downloads",
            "            for server_name in server_to_request_ids.keys():",
            "                self.key_downloads[server_name] = defer.Deferred()",
            "                logger.debug(\"Got key lookup lock on %s\", server_name)",
            "",
            "            # When we've finished fetching all the keys for a given server_name,",
            "            # drop the lock by resolving the deferred in key_downloads.",
            "            def drop_server_lock(server_name):",
            "                d = self.key_downloads.pop(server_name)",
            "                d.callback(None)",
            "",
            "            def lookup_done(res, verify_request):",
            "                server_name = verify_request.server_name",
            "                server_requests = server_to_request_ids[server_name]",
            "                server_requests.remove(id(verify_request))",
            "",
            "                # if there are no more requests for this server, we can drop the lock.",
            "                if not server_requests:",
            "                    logger.debug(\"Releasing key lookup lock on %s\", server_name)",
            "                    drop_server_lock(server_name)",
            "",
            "                return res",
            "",
            "            for verify_request in verify_requests:",
            "                verify_request.key_ready.addBoth(lookup_done, verify_request)",
            "",
            "            # Actually start fetching keys.",
            "            self._get_server_verify_keys(verify_requests)",
            "        except Exception:",
            "            logger.exception(\"Error starting key lookups\")",
            "",
            "    async def wait_for_previous_lookups(self, server_names) -> None:",
            "        \"\"\"Waits for any previous key lookups for the given servers to finish.",
            "",
            "        Args:",
            "            server_names (Iterable[str]): list of servers which we want to look up",
            "",
            "        Returns:",
            "            Resolves once all key lookups for the given servers have",
            "                completed. Follows the synapse rules of logcontext preservation.",
            "        \"\"\"",
            "        loop_count = 1",
            "        while True:",
            "            wait_on = [",
            "                (server_name, self.key_downloads[server_name])",
            "                for server_name in server_names",
            "                if server_name in self.key_downloads",
            "            ]",
            "            if not wait_on:",
            "                break",
            "            logger.info(",
            "                \"Waiting for existing lookups for %s to complete [loop %i]\",",
            "                [w[0] for w in wait_on],",
            "                loop_count,",
            "            )",
            "            with PreserveLoggingContext():",
            "                await defer.DeferredList((w[1] for w in wait_on))",
            "",
            "            loop_count += 1",
            "",
            "    def _get_server_verify_keys(self, verify_requests):",
            "        \"\"\"Tries to find at least one key for each verify request",
            "",
            "        For each verify_request, verify_request.key_ready is called back with",
            "        params (server_name, key_id, VerifyKey) if a key is found, or errbacked",
            "        with a SynapseError if none of the keys are found.",
            "",
            "        Args:",
            "            verify_requests (list[VerifyJsonRequest]): list of verify requests",
            "        \"\"\"",
            "",
            "        remaining_requests = {rq for rq in verify_requests if not rq.key_ready.called}",
            "",
            "        async def do_iterations():",
            "            try:",
            "                with Measure(self.clock, \"get_server_verify_keys\"):",
            "                    for f in self._key_fetchers:",
            "                        if not remaining_requests:",
            "                            return",
            "                        await self._attempt_key_fetches_with_fetcher(",
            "                            f, remaining_requests",
            "                        )",
            "",
            "                    # look for any requests which weren't satisfied",
            "                    while remaining_requests:",
            "                        verify_request = remaining_requests.pop()",
            "                        rq_str = (",
            "                            \"VerifyJsonRequest(server=%s, key_ids=%s, min_valid=%i)\"",
            "                            % (",
            "                                verify_request.server_name,",
            "                                verify_request.key_ids,",
            "                                verify_request.minimum_valid_until_ts,",
            "                            )",
            "                        )",
            "",
            "                        # If we run the errback immediately, it may cancel our",
            "                        # loggingcontext while we are still in it, so instead we",
            "                        # schedule it for the next time round the reactor.",
            "                        #",
            "                        # (this also ensures that we don't get a stack overflow if we",
            "                        # has a massive queue of lookups waiting for this server).",
            "                        self.clock.call_later(",
            "                            0,",
            "                            verify_request.key_ready.errback,",
            "                            SynapseError(",
            "                                401,",
            "                                \"Failed to find any key to satisfy %s\" % (rq_str,),",
            "                                Codes.UNAUTHORIZED,",
            "                            ),",
            "                        )",
            "            except Exception as err:",
            "                # we don't really expect to get here, because any errors should already",
            "                # have been caught and logged. But if we do, let's log the error and make",
            "                # sure that all of the deferreds are resolved.",
            "                logger.error(\"Unexpected error in _get_server_verify_keys: %s\", err)",
            "                with PreserveLoggingContext():",
            "                    for verify_request in remaining_requests:",
            "                        if not verify_request.key_ready.called:",
            "                            verify_request.key_ready.errback(err)",
            "",
            "        run_in_background(do_iterations)",
            "",
            "    async def _attempt_key_fetches_with_fetcher(self, fetcher, remaining_requests):",
            "        \"\"\"Use a key fetcher to attempt to satisfy some key requests",
            "",
            "        Args:",
            "            fetcher (KeyFetcher): fetcher to use to fetch the keys",
            "            remaining_requests (set[VerifyJsonRequest]): outstanding key requests.",
            "                Any successfully-completed requests will be removed from the list.",
            "        \"\"\"",
            "        # dict[str, dict[str, int]]: keys to fetch.",
            "        # server_name -> key_id -> min_valid_ts",
            "        missing_keys = defaultdict(dict)",
            "",
            "        for verify_request in remaining_requests:",
            "            # any completed requests should already have been removed",
            "            assert not verify_request.key_ready.called",
            "            keys_for_server = missing_keys[verify_request.server_name]",
            "",
            "            for key_id in verify_request.key_ids:",
            "                # If we have several requests for the same key, then we only need to",
            "                # request that key once, but we should do so with the greatest",
            "                # min_valid_until_ts of the requests, so that we can satisfy all of",
            "                # the requests.",
            "                keys_for_server[key_id] = max(",
            "                    keys_for_server.get(key_id, -1),",
            "                    verify_request.minimum_valid_until_ts,",
            "                )",
            "",
            "        results = await fetcher.get_keys(missing_keys)",
            "",
            "        completed = []",
            "        for verify_request in remaining_requests:",
            "            server_name = verify_request.server_name",
            "",
            "            # see if any of the keys we got this time are sufficient to",
            "            # complete this VerifyJsonRequest.",
            "            result_keys = results.get(server_name, {})",
            "            for key_id in verify_request.key_ids:",
            "                fetch_key_result = result_keys.get(key_id)",
            "                if not fetch_key_result:",
            "                    # we didn't get a result for this key",
            "                    continue",
            "",
            "                if (",
            "                    fetch_key_result.valid_until_ts",
            "                    < verify_request.minimum_valid_until_ts",
            "                ):",
            "                    # key was not valid at this point",
            "                    continue",
            "",
            "                # we have a valid key for this request. If we run the callback",
            "                # immediately, it may cancel our loggingcontext while we are still in",
            "                # it, so instead we schedule it for the next time round the reactor.",
            "                #",
            "                # (this also ensures that we don't get a stack overflow if we had",
            "                # a massive queue of lookups waiting for this server).",
            "                logger.debug(",
            "                    \"Found key %s:%s for %s\",",
            "                    server_name,",
            "                    key_id,",
            "                    verify_request.request_name,",
            "                )",
            "                self.clock.call_later(",
            "                    0,",
            "                    verify_request.key_ready.callback,",
            "                    (server_name, key_id, fetch_key_result.verify_key),",
            "                )",
            "                completed.append(verify_request)",
            "                break",
            "",
            "        remaining_requests.difference_update(completed)",
            "",
            "",
            "class KeyFetcher:",
            "    async def get_keys(self, keys_to_fetch):",
            "        \"\"\"",
            "        Args:",
            "            keys_to_fetch (dict[str, dict[str, int]]):",
            "                the keys to be fetched. server_name -> key_id -> min_valid_ts",
            "",
            "        Returns:",
            "            Deferred[dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]]:",
            "                map from server_name -> key_id -> FetchKeyResult",
            "        \"\"\"",
            "        raise NotImplementedError",
            "",
            "",
            "class StoreKeyFetcher(KeyFetcher):",
            "    \"\"\"KeyFetcher impl which fetches keys from our data store\"\"\"",
            "",
            "    def __init__(self, hs):",
            "        self.store = hs.get_datastore()",
            "",
            "    async def get_keys(self, keys_to_fetch):",
            "        \"\"\"see KeyFetcher.get_keys\"\"\"",
            "",
            "        keys_to_fetch = (",
            "            (server_name, key_id)",
            "            for server_name, keys_for_server in keys_to_fetch.items()",
            "            for key_id in keys_for_server.keys()",
            "        )",
            "",
            "        res = await self.store.get_server_verify_keys(keys_to_fetch)",
            "        keys = {}",
            "        for (server_name, key_id), key in res.items():",
            "            keys.setdefault(server_name, {})[key_id] = key",
            "        return keys",
            "",
            "",
            "class BaseV2KeyFetcher:",
            "    def __init__(self, hs):",
            "        self.store = hs.get_datastore()",
            "        self.config = hs.get_config()",
            "",
            "    async def process_v2_response(self, from_server, response_json, time_added_ms):",
            "        \"\"\"Parse a 'Server Keys' structure from the result of a /key request",
            "",
            "        This is used to parse either the entirety of the response from",
            "        GET /_matrix/key/v2/server, or a single entry from the list returned by",
            "        POST /_matrix/key/v2/query.",
            "",
            "        Checks that each signature in the response that claims to come from the origin",
            "        server is valid, and that there is at least one such signature.",
            "",
            "        Stores the json in server_keys_json so that it can be used for future responses",
            "        to /_matrix/key/v2/query.",
            "",
            "        Args:",
            "            from_server (str): the name of the server producing this result: either",
            "                the origin server for a /_matrix/key/v2/server request, or the notary",
            "                for a /_matrix/key/v2/query.",
            "",
            "            response_json (dict): the json-decoded Server Keys response object",
            "",
            "            time_added_ms (int): the timestamp to record in server_keys_json",
            "",
            "        Returns:",
            "            Deferred[dict[str, FetchKeyResult]]: map from key_id to result object",
            "        \"\"\"",
            "        ts_valid_until_ms = response_json[\"valid_until_ts\"]",
            "",
            "        # start by extracting the keys from the response, since they may be required",
            "        # to validate the signature on the response.",
            "        verify_keys = {}",
            "        for key_id, key_data in response_json[\"verify_keys\"].items():",
            "            if is_signing_algorithm_supported(key_id):",
            "                key_base64 = key_data[\"key\"]",
            "                key_bytes = decode_base64(key_base64)",
            "                verify_key = decode_verify_key_bytes(key_id, key_bytes)",
            "                verify_keys[key_id] = FetchKeyResult(",
            "                    verify_key=verify_key, valid_until_ts=ts_valid_until_ms",
            "                )",
            "",
            "        server_name = response_json[\"server_name\"]",
            "        verified = False",
            "        for key_id in response_json[\"signatures\"].get(server_name, {}):",
            "            key = verify_keys.get(key_id)",
            "            if not key:",
            "                # the key may not be present in verify_keys if:",
            "                #  * we got the key from the notary server, and:",
            "                #  * the key belongs to the notary server, and:",
            "                #  * the notary server is using a different key to sign notary",
            "                #    responses.",
            "                continue",
            "",
            "            verify_signed_json(response_json, server_name, key.verify_key)",
            "            verified = True",
            "            break",
            "",
            "        if not verified:",
            "            raise KeyLookupError(",
            "                \"Key response for %s is not signed by the origin server\"",
            "                % (server_name,)",
            "            )",
            "",
            "        for key_id, key_data in response_json[\"old_verify_keys\"].items():",
            "            if is_signing_algorithm_supported(key_id):",
            "                key_base64 = key_data[\"key\"]",
            "                key_bytes = decode_base64(key_base64)",
            "                verify_key = decode_verify_key_bytes(key_id, key_bytes)",
            "                verify_keys[key_id] = FetchKeyResult(",
            "                    verify_key=verify_key, valid_until_ts=key_data[\"expired_ts\"]",
            "                )",
            "",
            "        key_json_bytes = encode_canonical_json(response_json)",
            "",
            "        await make_deferred_yieldable(",
            "            defer.gatherResults(",
            "                [",
            "                    run_in_background(",
            "                        self.store.store_server_keys_json,",
            "                        server_name=server_name,",
            "                        key_id=key_id,",
            "                        from_server=from_server,",
            "                        ts_now_ms=time_added_ms,",
            "                        ts_expires_ms=ts_valid_until_ms,",
            "                        key_json_bytes=key_json_bytes,",
            "                    )",
            "                    for key_id in verify_keys",
            "                ],",
            "                consumeErrors=True,",
            "            ).addErrback(unwrapFirstError)",
            "        )",
            "",
            "        return verify_keys",
            "",
            "",
            "class PerspectivesKeyFetcher(BaseV2KeyFetcher):",
            "    \"\"\"KeyFetcher impl which fetches keys from the \"perspectives\" servers\"\"\"",
            "",
            "    def __init__(self, hs):",
            "        super().__init__(hs)",
            "        self.clock = hs.get_clock()",
            "        self.client = hs.get_http_client()",
            "        self.key_servers = self.config.key_servers",
            "",
            "    async def get_keys(self, keys_to_fetch):",
            "        \"\"\"see KeyFetcher.get_keys\"\"\"",
            "",
            "        async def get_key(key_server):",
            "            try:",
            "                result = await self.get_server_verify_key_v2_indirect(",
            "                    keys_to_fetch, key_server",
            "                )",
            "                return result",
            "            except KeyLookupError as e:",
            "                logger.warning(",
            "                    \"Key lookup failed from %r: %s\", key_server.server_name, e",
            "                )",
            "            except Exception as e:",
            "                logger.exception(",
            "                    \"Unable to get key from %r: %s %s\",",
            "                    key_server.server_name,",
            "                    type(e).__name__,",
            "                    str(e),",
            "                )",
            "",
            "            return {}",
            "",
            "        results = await make_deferred_yieldable(",
            "            defer.gatherResults(",
            "                [run_in_background(get_key, server) for server in self.key_servers],",
            "                consumeErrors=True,",
            "            ).addErrback(unwrapFirstError)",
            "        )",
            "",
            "        union_of_keys = {}",
            "        for result in results:",
            "            for server_name, keys in result.items():",
            "                union_of_keys.setdefault(server_name, {}).update(keys)",
            "",
            "        return union_of_keys",
            "",
            "    async def get_server_verify_key_v2_indirect(self, keys_to_fetch, key_server):",
            "        \"\"\"",
            "        Args:",
            "            keys_to_fetch (dict[str, dict[str, int]]):",
            "                the keys to be fetched. server_name -> key_id -> min_valid_ts",
            "",
            "            key_server (synapse.config.key.TrustedKeyServer): notary server to query for",
            "                the keys",
            "",
            "        Returns:",
            "            dict[str, dict[str, synapse.storage.keys.FetchKeyResult]]: map",
            "                from server_name -> key_id -> FetchKeyResult",
            "",
            "        Raises:",
            "            KeyLookupError if there was an error processing the entire response from",
            "                the server",
            "        \"\"\"",
            "        perspective_name = key_server.server_name",
            "        logger.info(",
            "            \"Requesting keys %s from notary server %s\",",
            "            keys_to_fetch.items(),",
            "            perspective_name,",
            "        )",
            "",
            "        try:",
            "            query_response = await self.client.post_json(",
            "                destination=perspective_name,",
            "                path=\"/_matrix/key/v2/query\",",
            "                data={",
            "                    \"server_keys\": {",
            "                        server_name: {",
            "                            key_id: {\"minimum_valid_until_ts\": min_valid_ts}",
            "                            for key_id, min_valid_ts in server_keys.items()",
            "                        }",
            "                        for server_name, server_keys in keys_to_fetch.items()",
            "                    }",
            "                },",
            "            )",
            "        except (NotRetryingDestination, RequestSendFailed) as e:",
            "            # these both have str() representations which we can't really improve upon",
            "            raise KeyLookupError(str(e))",
            "        except HttpResponseException as e:",
            "            raise KeyLookupError(\"Remote server returned an error: %s\" % (e,))",
            "",
            "        keys = {}",
            "        added_keys = []",
            "",
            "        time_now_ms = self.clock.time_msec()",
            "",
            "        for response in query_response[\"server_keys\"]:",
            "            # do this first, so that we can give useful errors thereafter",
            "            server_name = response.get(\"server_name\")",
            "            if not isinstance(server_name, str):",
            "                raise KeyLookupError(",
            "                    \"Malformed response from key notary server %s: invalid server_name\"",
            "                    % (perspective_name,)",
            "                )",
            "",
            "            try:",
            "                self._validate_perspectives_response(key_server, response)",
            "",
            "                processed_response = await self.process_v2_response(",
            "                    perspective_name, response, time_added_ms=time_now_ms",
            "                )",
            "            except KeyLookupError as e:",
            "                logger.warning(",
            "                    \"Error processing response from key notary server %s for origin \"",
            "                    \"server %s: %s\",",
            "                    perspective_name,",
            "                    server_name,",
            "                    e,",
            "                )",
            "                # we continue to process the rest of the response",
            "                continue",
            "",
            "            added_keys.extend(",
            "                (server_name, key_id, key) for key_id, key in processed_response.items()",
            "            )",
            "            keys.setdefault(server_name, {}).update(processed_response)",
            "",
            "        await self.store.store_server_verify_keys(",
            "            perspective_name, time_now_ms, added_keys",
            "        )",
            "",
            "        return keys",
            "",
            "    def _validate_perspectives_response(self, key_server, response):",
            "        \"\"\"Optionally check the signature on the result of a /key/query request",
            "",
            "        Args:",
            "            key_server (synapse.config.key.TrustedKeyServer): the notary server that",
            "                produced this result",
            "",
            "            response (dict): the json-decoded Server Keys response object",
            "        \"\"\"",
            "        perspective_name = key_server.server_name",
            "        perspective_keys = key_server.verify_keys",
            "",
            "        if perspective_keys is None:",
            "            # signature checking is disabled on this server",
            "            return",
            "",
            "        if (",
            "            \"signatures\" not in response",
            "            or perspective_name not in response[\"signatures\"]",
            "        ):",
            "            raise KeyLookupError(\"Response not signed by the notary server\")",
            "",
            "        verified = False",
            "        for key_id in response[\"signatures\"][perspective_name]:",
            "            if key_id in perspective_keys:",
            "                verify_signed_json(response, perspective_name, perspective_keys[key_id])",
            "                verified = True",
            "",
            "        if not verified:",
            "            raise KeyLookupError(",
            "                \"Response not signed with a known key: signed with: %r, known keys: %r\"",
            "                % (",
            "                    list(response[\"signatures\"][perspective_name].keys()),",
            "                    list(perspective_keys.keys()),",
            "                )",
            "            )",
            "",
            "",
            "class ServerKeyFetcher(BaseV2KeyFetcher):",
            "    \"\"\"KeyFetcher impl which fetches keys from the origin servers\"\"\"",
            "",
            "    def __init__(self, hs):",
            "        super().__init__(hs)",
            "        self.clock = hs.get_clock()",
            "        self.client = hs.get_http_client()",
            "",
            "    async def get_keys(self, keys_to_fetch):",
            "        \"\"\"",
            "        Args:",
            "            keys_to_fetch (dict[str, iterable[str]]):",
            "                the keys to be fetched. server_name -> key_ids",
            "",
            "        Returns:",
            "            dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]:",
            "                map from server_name -> key_id -> FetchKeyResult",
            "        \"\"\"",
            "",
            "        results = {}",
            "",
            "        async def get_key(key_to_fetch_item):",
            "            server_name, key_ids = key_to_fetch_item",
            "            try:",
            "                keys = await self.get_server_verify_key_v2_direct(server_name, key_ids)",
            "                results[server_name] = keys",
            "            except KeyLookupError as e:",
            "                logger.warning(",
            "                    \"Error looking up keys %s from %s: %s\", key_ids, server_name, e",
            "                )",
            "            except Exception:",
            "                logger.exception(\"Error getting keys %s from %s\", key_ids, server_name)",
            "",
            "        await yieldable_gather_results(get_key, keys_to_fetch.items())",
            "        return results",
            "",
            "    async def get_server_verify_key_v2_direct(self, server_name, key_ids):",
            "        \"\"\"",
            "",
            "        Args:",
            "            server_name (str):",
            "            key_ids (iterable[str]):",
            "",
            "        Returns:",
            "            dict[str, FetchKeyResult]: map from key ID to lookup result",
            "",
            "        Raises:",
            "            KeyLookupError if there was a problem making the lookup",
            "        \"\"\"",
            "        keys = {}  # type: dict[str, FetchKeyResult]",
            "",
            "        for requested_key_id in key_ids:",
            "            # we may have found this key as a side-effect of asking for another.",
            "            if requested_key_id in keys:",
            "                continue",
            "",
            "            time_now_ms = self.clock.time_msec()",
            "            try:",
            "                response = await self.client.get_json(",
            "                    destination=server_name,",
            "                    path=\"/_matrix/key/v2/server/\"",
            "                    + urllib.parse.quote(requested_key_id),",
            "                    ignore_backoff=True,",
            "                    # we only give the remote server 10s to respond. It should be an",
            "                    # easy request to handle, so if it doesn't reply within 10s, it's",
            "                    # probably not going to.",
            "                    #",
            "                    # Furthermore, when we are acting as a notary server, we cannot",
            "                    # wait all day for all of the origin servers, as the requesting",
            "                    # server will otherwise time out before we can respond.",
            "                    #",
            "                    # (Note that get_json may make 4 attempts, so this can still take",
            "                    # almost 45 seconds to fetch the headers, plus up to another 60s to",
            "                    # read the response).",
            "                    timeout=10000,",
            "                )",
            "            except (NotRetryingDestination, RequestSendFailed) as e:",
            "                # these both have str() representations which we can't really improve",
            "                # upon",
            "                raise KeyLookupError(str(e))",
            "            except HttpResponseException as e:",
            "                raise KeyLookupError(\"Remote server returned an error: %s\" % (e,))",
            "",
            "            if response[\"server_name\"] != server_name:",
            "                raise KeyLookupError(",
            "                    \"Expected a response for server %r not %r\"",
            "                    % (server_name, response[\"server_name\"])",
            "                )",
            "",
            "            response_keys = await self.process_v2_response(",
            "                from_server=server_name,",
            "                response_json=response,",
            "                time_added_ms=time_now_ms,",
            "            )",
            "            await self.store.store_server_verify_keys(",
            "                server_name,",
            "                time_now_ms,",
            "                ((server_name, key_id, key) for key_id, key in response_keys.items()),",
            "            )",
            "            keys.update(response_keys)",
            "",
            "        return keys",
            "",
            "",
            "async def _handle_key_deferred(verify_request) -> None:",
            "    \"\"\"Waits for the key to become available, and then performs a verification",
            "",
            "    Args:",
            "        verify_request (VerifyJsonRequest):",
            "",
            "    Raises:",
            "        SynapseError if there was a problem performing the verification",
            "    \"\"\"",
            "    server_name = verify_request.server_name",
            "    with PreserveLoggingContext():",
            "        _, key_id, verify_key = await verify_request.key_ready",
            "",
            "    json_object = verify_request.json_object",
            "",
            "    try:",
            "        verify_signed_json(json_object, server_name, verify_key)",
            "    except SignatureVerifyException as e:",
            "        logger.debug(",
            "            \"Error verifying signature for %s:%s:%s with key %s: %s\",",
            "            server_name,",
            "            verify_key.alg,",
            "            verify_key.version,",
            "            encode_verify_key_base64(verify_key),",
            "            str(e),",
            "        )",
            "        raise SynapseError(",
            "            401,",
            "            \"Invalid signature for server %s with key %s:%s: %s\"",
            "            % (server_name, verify_key.alg, verify_key.version, str(e)),",
            "            Codes.UNAUTHORIZED,",
            "        )"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2017, 2018 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import logging",
            "import urllib",
            "from collections import defaultdict",
            "",
            "import attr",
            "from signedjson.key import (",
            "    decode_verify_key_bytes,",
            "    encode_verify_key_base64,",
            "    is_signing_algorithm_supported,",
            ")",
            "from signedjson.sign import (",
            "    SignatureVerifyException,",
            "    encode_canonical_json,",
            "    signature_ids,",
            "    verify_signed_json,",
            ")",
            "from unpaddedbase64 import decode_base64",
            "",
            "from twisted.internet import defer",
            "",
            "from synapse.api.errors import (",
            "    Codes,",
            "    HttpResponseException,",
            "    RequestSendFailed,",
            "    SynapseError,",
            ")",
            "from synapse.logging.context import (",
            "    PreserveLoggingContext,",
            "    make_deferred_yieldable,",
            "    preserve_fn,",
            "    run_in_background,",
            ")",
            "from synapse.storage.keys import FetchKeyResult",
            "from synapse.util import unwrapFirstError",
            "from synapse.util.async_helpers import yieldable_gather_results",
            "from synapse.util.metrics import Measure",
            "from synapse.util.retryutils import NotRetryingDestination",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "@attr.s(slots=True, cmp=False)",
            "class VerifyJsonRequest:",
            "    \"\"\"",
            "    A request to verify a JSON object.",
            "",
            "    Attributes:",
            "        server_name(str): The name of the server to verify against.",
            "",
            "        key_ids(set[str]): The set of key_ids to that could be used to verify the",
            "            JSON object",
            "",
            "        json_object(dict): The JSON object to verify.",
            "",
            "        minimum_valid_until_ts (int): time at which we require the signing key to",
            "            be valid. (0 implies we don't care)",
            "",
            "        key_ready (Deferred[str, str, nacl.signing.VerifyKey]):",
            "            A deferred (server_name, key_id, verify_key) tuple that resolves when",
            "            a verify key has been fetched. The deferreds' callbacks are run with no",
            "            logcontext.",
            "",
            "            If we are unable to find a key which satisfies the request, the deferred",
            "            errbacks with an M_UNAUTHORIZED SynapseError.",
            "    \"\"\"",
            "",
            "    server_name = attr.ib()",
            "    json_object = attr.ib()",
            "    minimum_valid_until_ts = attr.ib()",
            "    request_name = attr.ib()",
            "    key_ids = attr.ib(init=False)",
            "    key_ready = attr.ib(default=attr.Factory(defer.Deferred))",
            "",
            "    def __attrs_post_init__(self):",
            "        self.key_ids = signature_ids(self.json_object, self.server_name)",
            "",
            "",
            "class KeyLookupError(ValueError):",
            "    pass",
            "",
            "",
            "class Keyring:",
            "    def __init__(self, hs, key_fetchers=None):",
            "        self.clock = hs.get_clock()",
            "",
            "        if key_fetchers is None:",
            "            key_fetchers = (",
            "                StoreKeyFetcher(hs),",
            "                PerspectivesKeyFetcher(hs),",
            "                ServerKeyFetcher(hs),",
            "            )",
            "        self._key_fetchers = key_fetchers",
            "",
            "        # map from server name to Deferred. Has an entry for each server with",
            "        # an ongoing key download; the Deferred completes once the download",
            "        # completes.",
            "        #",
            "        # These are regular, logcontext-agnostic Deferreds.",
            "        self.key_downloads = {}",
            "",
            "    def verify_json_for_server(",
            "        self, server_name, json_object, validity_time, request_name",
            "    ):",
            "        \"\"\"Verify that a JSON object has been signed by a given server",
            "",
            "        Args:",
            "            server_name (str): name of the server which must have signed this object",
            "",
            "            json_object (dict): object to be checked",
            "",
            "            validity_time (int): timestamp at which we require the signing key to",
            "                be valid. (0 implies we don't care)",
            "",
            "            request_name (str): an identifier for this json object (eg, an event id)",
            "                for logging.",
            "",
            "        Returns:",
            "            Deferred[None]: completes if the the object was correctly signed, otherwise",
            "                errbacks with an error",
            "        \"\"\"",
            "        req = VerifyJsonRequest(server_name, json_object, validity_time, request_name)",
            "        requests = (req,)",
            "        return make_deferred_yieldable(self._verify_objects(requests)[0])",
            "",
            "    def verify_json_objects_for_server(self, server_and_json):",
            "        \"\"\"Bulk verifies signatures of json objects, bulk fetching keys as",
            "        necessary.",
            "",
            "        Args:",
            "            server_and_json (iterable[Tuple[str, dict, int, str]):",
            "                Iterable of (server_name, json_object, validity_time, request_name)",
            "                tuples.",
            "",
            "                validity_time is a timestamp at which the signing key must be",
            "                valid.",
            "",
            "                request_name is an identifier for this json object (eg, an event id)",
            "                for logging.",
            "",
            "        Returns:",
            "            List<Deferred[None]>: for each input triplet, a deferred indicating success",
            "                or failure to verify each json object's signature for the given",
            "                server_name. The deferreds run their callbacks in the sentinel",
            "                logcontext.",
            "        \"\"\"",
            "        return self._verify_objects(",
            "            VerifyJsonRequest(server_name, json_object, validity_time, request_name)",
            "            for server_name, json_object, validity_time, request_name in server_and_json",
            "        )",
            "",
            "    def _verify_objects(self, verify_requests):",
            "        \"\"\"Does the work of verify_json_[objects_]for_server",
            "",
            "",
            "        Args:",
            "            verify_requests (iterable[VerifyJsonRequest]):",
            "                Iterable of verification requests.",
            "",
            "        Returns:",
            "            List<Deferred[None]>: for each input item, a deferred indicating success",
            "                or failure to verify each json object's signature for the given",
            "                server_name. The deferreds run their callbacks in the sentinel",
            "                logcontext.",
            "        \"\"\"",
            "        # a list of VerifyJsonRequests which are awaiting a key lookup",
            "        key_lookups = []",
            "        handle = preserve_fn(_handle_key_deferred)",
            "",
            "        def process(verify_request):",
            "            \"\"\"Process an entry in the request list",
            "",
            "            Adds a key request to key_lookups, and returns a deferred which",
            "            will complete or fail (in the sentinel context) when verification completes.",
            "            \"\"\"",
            "            if not verify_request.key_ids:",
            "                return defer.fail(",
            "                    SynapseError(",
            "                        400,",
            "                        \"Not signed by %s\" % (verify_request.server_name,),",
            "                        Codes.UNAUTHORIZED,",
            "                    )",
            "                )",
            "",
            "            logger.debug(",
            "                \"Verifying %s for %s with key_ids %s, min_validity %i\",",
            "                verify_request.request_name,",
            "                verify_request.server_name,",
            "                verify_request.key_ids,",
            "                verify_request.minimum_valid_until_ts,",
            "            )",
            "",
            "            # add the key request to the queue, but don't start it off yet.",
            "            key_lookups.append(verify_request)",
            "",
            "            # now run _handle_key_deferred, which will wait for the key request",
            "            # to complete and then do the verification.",
            "            #",
            "            # We want _handle_key_request to log to the right context, so we",
            "            # wrap it with preserve_fn (aka run_in_background)",
            "            return handle(verify_request)",
            "",
            "        results = [process(r) for r in verify_requests]",
            "",
            "        if key_lookups:",
            "            run_in_background(self._start_key_lookups, key_lookups)",
            "",
            "        return results",
            "",
            "    async def _start_key_lookups(self, verify_requests):",
            "        \"\"\"Sets off the key fetches for each verify request",
            "",
            "        Once each fetch completes, verify_request.key_ready will be resolved.",
            "",
            "        Args:",
            "            verify_requests (List[VerifyJsonRequest]):",
            "        \"\"\"",
            "",
            "        try:",
            "            # map from server name to a set of outstanding request ids",
            "            server_to_request_ids = {}",
            "",
            "            for verify_request in verify_requests:",
            "                server_name = verify_request.server_name",
            "                request_id = id(verify_request)",
            "                server_to_request_ids.setdefault(server_name, set()).add(request_id)",
            "",
            "            # Wait for any previous lookups to complete before proceeding.",
            "            await self.wait_for_previous_lookups(server_to_request_ids.keys())",
            "",
            "            # take out a lock on each of the servers by sticking a Deferred in",
            "            # key_downloads",
            "            for server_name in server_to_request_ids.keys():",
            "                self.key_downloads[server_name] = defer.Deferred()",
            "                logger.debug(\"Got key lookup lock on %s\", server_name)",
            "",
            "            # When we've finished fetching all the keys for a given server_name,",
            "            # drop the lock by resolving the deferred in key_downloads.",
            "            def drop_server_lock(server_name):",
            "                d = self.key_downloads.pop(server_name)",
            "                d.callback(None)",
            "",
            "            def lookup_done(res, verify_request):",
            "                server_name = verify_request.server_name",
            "                server_requests = server_to_request_ids[server_name]",
            "                server_requests.remove(id(verify_request))",
            "",
            "                # if there are no more requests for this server, we can drop the lock.",
            "                if not server_requests:",
            "                    logger.debug(\"Releasing key lookup lock on %s\", server_name)",
            "                    drop_server_lock(server_name)",
            "",
            "                return res",
            "",
            "            for verify_request in verify_requests:",
            "                verify_request.key_ready.addBoth(lookup_done, verify_request)",
            "",
            "            # Actually start fetching keys.",
            "            self._get_server_verify_keys(verify_requests)",
            "        except Exception:",
            "            logger.exception(\"Error starting key lookups\")",
            "",
            "    async def wait_for_previous_lookups(self, server_names) -> None:",
            "        \"\"\"Waits for any previous key lookups for the given servers to finish.",
            "",
            "        Args:",
            "            server_names (Iterable[str]): list of servers which we want to look up",
            "",
            "        Returns:",
            "            Resolves once all key lookups for the given servers have",
            "                completed. Follows the synapse rules of logcontext preservation.",
            "        \"\"\"",
            "        loop_count = 1",
            "        while True:",
            "            wait_on = [",
            "                (server_name, self.key_downloads[server_name])",
            "                for server_name in server_names",
            "                if server_name in self.key_downloads",
            "            ]",
            "            if not wait_on:",
            "                break",
            "            logger.info(",
            "                \"Waiting for existing lookups for %s to complete [loop %i]\",",
            "                [w[0] for w in wait_on],",
            "                loop_count,",
            "            )",
            "            with PreserveLoggingContext():",
            "                await defer.DeferredList((w[1] for w in wait_on))",
            "",
            "            loop_count += 1",
            "",
            "    def _get_server_verify_keys(self, verify_requests):",
            "        \"\"\"Tries to find at least one key for each verify request",
            "",
            "        For each verify_request, verify_request.key_ready is called back with",
            "        params (server_name, key_id, VerifyKey) if a key is found, or errbacked",
            "        with a SynapseError if none of the keys are found.",
            "",
            "        Args:",
            "            verify_requests (list[VerifyJsonRequest]): list of verify requests",
            "        \"\"\"",
            "",
            "        remaining_requests = {rq for rq in verify_requests if not rq.key_ready.called}",
            "",
            "        async def do_iterations():",
            "            try:",
            "                with Measure(self.clock, \"get_server_verify_keys\"):",
            "                    for f in self._key_fetchers:",
            "                        if not remaining_requests:",
            "                            return",
            "                        await self._attempt_key_fetches_with_fetcher(",
            "                            f, remaining_requests",
            "                        )",
            "",
            "                    # look for any requests which weren't satisfied",
            "                    while remaining_requests:",
            "                        verify_request = remaining_requests.pop()",
            "                        rq_str = (",
            "                            \"VerifyJsonRequest(server=%s, key_ids=%s, min_valid=%i)\"",
            "                            % (",
            "                                verify_request.server_name,",
            "                                verify_request.key_ids,",
            "                                verify_request.minimum_valid_until_ts,",
            "                            )",
            "                        )",
            "",
            "                        # If we run the errback immediately, it may cancel our",
            "                        # loggingcontext while we are still in it, so instead we",
            "                        # schedule it for the next time round the reactor.",
            "                        #",
            "                        # (this also ensures that we don't get a stack overflow if we",
            "                        # has a massive queue of lookups waiting for this server).",
            "                        self.clock.call_later(",
            "                            0,",
            "                            verify_request.key_ready.errback,",
            "                            SynapseError(",
            "                                401,",
            "                                \"Failed to find any key to satisfy %s\" % (rq_str,),",
            "                                Codes.UNAUTHORIZED,",
            "                            ),",
            "                        )",
            "            except Exception as err:",
            "                # we don't really expect to get here, because any errors should already",
            "                # have been caught and logged. But if we do, let's log the error and make",
            "                # sure that all of the deferreds are resolved.",
            "                logger.error(\"Unexpected error in _get_server_verify_keys: %s\", err)",
            "                with PreserveLoggingContext():",
            "                    for verify_request in remaining_requests:",
            "                        if not verify_request.key_ready.called:",
            "                            verify_request.key_ready.errback(err)",
            "",
            "        run_in_background(do_iterations)",
            "",
            "    async def _attempt_key_fetches_with_fetcher(self, fetcher, remaining_requests):",
            "        \"\"\"Use a key fetcher to attempt to satisfy some key requests",
            "",
            "        Args:",
            "            fetcher (KeyFetcher): fetcher to use to fetch the keys",
            "            remaining_requests (set[VerifyJsonRequest]): outstanding key requests.",
            "                Any successfully-completed requests will be removed from the list.",
            "        \"\"\"",
            "        # dict[str, dict[str, int]]: keys to fetch.",
            "        # server_name -> key_id -> min_valid_ts",
            "        missing_keys = defaultdict(dict)",
            "",
            "        for verify_request in remaining_requests:",
            "            # any completed requests should already have been removed",
            "            assert not verify_request.key_ready.called",
            "            keys_for_server = missing_keys[verify_request.server_name]",
            "",
            "            for key_id in verify_request.key_ids:",
            "                # If we have several requests for the same key, then we only need to",
            "                # request that key once, but we should do so with the greatest",
            "                # min_valid_until_ts of the requests, so that we can satisfy all of",
            "                # the requests.",
            "                keys_for_server[key_id] = max(",
            "                    keys_for_server.get(key_id, -1),",
            "                    verify_request.minimum_valid_until_ts,",
            "                )",
            "",
            "        results = await fetcher.get_keys(missing_keys)",
            "",
            "        completed = []",
            "        for verify_request in remaining_requests:",
            "            server_name = verify_request.server_name",
            "",
            "            # see if any of the keys we got this time are sufficient to",
            "            # complete this VerifyJsonRequest.",
            "            result_keys = results.get(server_name, {})",
            "            for key_id in verify_request.key_ids:",
            "                fetch_key_result = result_keys.get(key_id)",
            "                if not fetch_key_result:",
            "                    # we didn't get a result for this key",
            "                    continue",
            "",
            "                if (",
            "                    fetch_key_result.valid_until_ts",
            "                    < verify_request.minimum_valid_until_ts",
            "                ):",
            "                    # key was not valid at this point",
            "                    continue",
            "",
            "                # we have a valid key for this request. If we run the callback",
            "                # immediately, it may cancel our loggingcontext while we are still in",
            "                # it, so instead we schedule it for the next time round the reactor.",
            "                #",
            "                # (this also ensures that we don't get a stack overflow if we had",
            "                # a massive queue of lookups waiting for this server).",
            "                logger.debug(",
            "                    \"Found key %s:%s for %s\",",
            "                    server_name,",
            "                    key_id,",
            "                    verify_request.request_name,",
            "                )",
            "                self.clock.call_later(",
            "                    0,",
            "                    verify_request.key_ready.callback,",
            "                    (server_name, key_id, fetch_key_result.verify_key),",
            "                )",
            "                completed.append(verify_request)",
            "                break",
            "",
            "        remaining_requests.difference_update(completed)",
            "",
            "",
            "class KeyFetcher:",
            "    async def get_keys(self, keys_to_fetch):",
            "        \"\"\"",
            "        Args:",
            "            keys_to_fetch (dict[str, dict[str, int]]):",
            "                the keys to be fetched. server_name -> key_id -> min_valid_ts",
            "",
            "        Returns:",
            "            Deferred[dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]]:",
            "                map from server_name -> key_id -> FetchKeyResult",
            "        \"\"\"",
            "        raise NotImplementedError",
            "",
            "",
            "class StoreKeyFetcher(KeyFetcher):",
            "    \"\"\"KeyFetcher impl which fetches keys from our data store\"\"\"",
            "",
            "    def __init__(self, hs):",
            "        self.store = hs.get_datastore()",
            "",
            "    async def get_keys(self, keys_to_fetch):",
            "        \"\"\"see KeyFetcher.get_keys\"\"\"",
            "",
            "        keys_to_fetch = (",
            "            (server_name, key_id)",
            "            for server_name, keys_for_server in keys_to_fetch.items()",
            "            for key_id in keys_for_server.keys()",
            "        )",
            "",
            "        res = await self.store.get_server_verify_keys(keys_to_fetch)",
            "        keys = {}",
            "        for (server_name, key_id), key in res.items():",
            "            keys.setdefault(server_name, {})[key_id] = key",
            "        return keys",
            "",
            "",
            "class BaseV2KeyFetcher:",
            "    def __init__(self, hs):",
            "        self.store = hs.get_datastore()",
            "        self.config = hs.get_config()",
            "",
            "    async def process_v2_response(self, from_server, response_json, time_added_ms):",
            "        \"\"\"Parse a 'Server Keys' structure from the result of a /key request",
            "",
            "        This is used to parse either the entirety of the response from",
            "        GET /_matrix/key/v2/server, or a single entry from the list returned by",
            "        POST /_matrix/key/v2/query.",
            "",
            "        Checks that each signature in the response that claims to come from the origin",
            "        server is valid, and that there is at least one such signature.",
            "",
            "        Stores the json in server_keys_json so that it can be used for future responses",
            "        to /_matrix/key/v2/query.",
            "",
            "        Args:",
            "            from_server (str): the name of the server producing this result: either",
            "                the origin server for a /_matrix/key/v2/server request, or the notary",
            "                for a /_matrix/key/v2/query.",
            "",
            "            response_json (dict): the json-decoded Server Keys response object",
            "",
            "            time_added_ms (int): the timestamp to record in server_keys_json",
            "",
            "        Returns:",
            "            Deferred[dict[str, FetchKeyResult]]: map from key_id to result object",
            "        \"\"\"",
            "        ts_valid_until_ms = response_json[\"valid_until_ts\"]",
            "",
            "        # start by extracting the keys from the response, since they may be required",
            "        # to validate the signature on the response.",
            "        verify_keys = {}",
            "        for key_id, key_data in response_json[\"verify_keys\"].items():",
            "            if is_signing_algorithm_supported(key_id):",
            "                key_base64 = key_data[\"key\"]",
            "                key_bytes = decode_base64(key_base64)",
            "                verify_key = decode_verify_key_bytes(key_id, key_bytes)",
            "                verify_keys[key_id] = FetchKeyResult(",
            "                    verify_key=verify_key, valid_until_ts=ts_valid_until_ms",
            "                )",
            "",
            "        server_name = response_json[\"server_name\"]",
            "        verified = False",
            "        for key_id in response_json[\"signatures\"].get(server_name, {}):",
            "            key = verify_keys.get(key_id)",
            "            if not key:",
            "                # the key may not be present in verify_keys if:",
            "                #  * we got the key from the notary server, and:",
            "                #  * the key belongs to the notary server, and:",
            "                #  * the notary server is using a different key to sign notary",
            "                #    responses.",
            "                continue",
            "",
            "            verify_signed_json(response_json, server_name, key.verify_key)",
            "            verified = True",
            "            break",
            "",
            "        if not verified:",
            "            raise KeyLookupError(",
            "                \"Key response for %s is not signed by the origin server\"",
            "                % (server_name,)",
            "            )",
            "",
            "        for key_id, key_data in response_json[\"old_verify_keys\"].items():",
            "            if is_signing_algorithm_supported(key_id):",
            "                key_base64 = key_data[\"key\"]",
            "                key_bytes = decode_base64(key_base64)",
            "                verify_key = decode_verify_key_bytes(key_id, key_bytes)",
            "                verify_keys[key_id] = FetchKeyResult(",
            "                    verify_key=verify_key, valid_until_ts=key_data[\"expired_ts\"]",
            "                )",
            "",
            "        key_json_bytes = encode_canonical_json(response_json)",
            "",
            "        await make_deferred_yieldable(",
            "            defer.gatherResults(",
            "                [",
            "                    run_in_background(",
            "                        self.store.store_server_keys_json,",
            "                        server_name=server_name,",
            "                        key_id=key_id,",
            "                        from_server=from_server,",
            "                        ts_now_ms=time_added_ms,",
            "                        ts_expires_ms=ts_valid_until_ms,",
            "                        key_json_bytes=key_json_bytes,",
            "                    )",
            "                    for key_id in verify_keys",
            "                ],",
            "                consumeErrors=True,",
            "            ).addErrback(unwrapFirstError)",
            "        )",
            "",
            "        return verify_keys",
            "",
            "",
            "class PerspectivesKeyFetcher(BaseV2KeyFetcher):",
            "    \"\"\"KeyFetcher impl which fetches keys from the \"perspectives\" servers\"\"\"",
            "",
            "    def __init__(self, hs):",
            "        super().__init__(hs)",
            "        self.clock = hs.get_clock()",
            "        self.client = hs.get_federation_http_client()",
            "        self.key_servers = self.config.key_servers",
            "",
            "    async def get_keys(self, keys_to_fetch):",
            "        \"\"\"see KeyFetcher.get_keys\"\"\"",
            "",
            "        async def get_key(key_server):",
            "            try:",
            "                result = await self.get_server_verify_key_v2_indirect(",
            "                    keys_to_fetch, key_server",
            "                )",
            "                return result",
            "            except KeyLookupError as e:",
            "                logger.warning(",
            "                    \"Key lookup failed from %r: %s\", key_server.server_name, e",
            "                )",
            "            except Exception as e:",
            "                logger.exception(",
            "                    \"Unable to get key from %r: %s %s\",",
            "                    key_server.server_name,",
            "                    type(e).__name__,",
            "                    str(e),",
            "                )",
            "",
            "            return {}",
            "",
            "        results = await make_deferred_yieldable(",
            "            defer.gatherResults(",
            "                [run_in_background(get_key, server) for server in self.key_servers],",
            "                consumeErrors=True,",
            "            ).addErrback(unwrapFirstError)",
            "        )",
            "",
            "        union_of_keys = {}",
            "        for result in results:",
            "            for server_name, keys in result.items():",
            "                union_of_keys.setdefault(server_name, {}).update(keys)",
            "",
            "        return union_of_keys",
            "",
            "    async def get_server_verify_key_v2_indirect(self, keys_to_fetch, key_server):",
            "        \"\"\"",
            "        Args:",
            "            keys_to_fetch (dict[str, dict[str, int]]):",
            "                the keys to be fetched. server_name -> key_id -> min_valid_ts",
            "",
            "            key_server (synapse.config.key.TrustedKeyServer): notary server to query for",
            "                the keys",
            "",
            "        Returns:",
            "            dict[str, dict[str, synapse.storage.keys.FetchKeyResult]]: map",
            "                from server_name -> key_id -> FetchKeyResult",
            "",
            "        Raises:",
            "            KeyLookupError if there was an error processing the entire response from",
            "                the server",
            "        \"\"\"",
            "        perspective_name = key_server.server_name",
            "        logger.info(",
            "            \"Requesting keys %s from notary server %s\",",
            "            keys_to_fetch.items(),",
            "            perspective_name,",
            "        )",
            "",
            "        try:",
            "            query_response = await self.client.post_json(",
            "                destination=perspective_name,",
            "                path=\"/_matrix/key/v2/query\",",
            "                data={",
            "                    \"server_keys\": {",
            "                        server_name: {",
            "                            key_id: {\"minimum_valid_until_ts\": min_valid_ts}",
            "                            for key_id, min_valid_ts in server_keys.items()",
            "                        }",
            "                        for server_name, server_keys in keys_to_fetch.items()",
            "                    }",
            "                },",
            "            )",
            "        except (NotRetryingDestination, RequestSendFailed) as e:",
            "            # these both have str() representations which we can't really improve upon",
            "            raise KeyLookupError(str(e))",
            "        except HttpResponseException as e:",
            "            raise KeyLookupError(\"Remote server returned an error: %s\" % (e,))",
            "",
            "        keys = {}",
            "        added_keys = []",
            "",
            "        time_now_ms = self.clock.time_msec()",
            "",
            "        for response in query_response[\"server_keys\"]:",
            "            # do this first, so that we can give useful errors thereafter",
            "            server_name = response.get(\"server_name\")",
            "            if not isinstance(server_name, str):",
            "                raise KeyLookupError(",
            "                    \"Malformed response from key notary server %s: invalid server_name\"",
            "                    % (perspective_name,)",
            "                )",
            "",
            "            try:",
            "                self._validate_perspectives_response(key_server, response)",
            "",
            "                processed_response = await self.process_v2_response(",
            "                    perspective_name, response, time_added_ms=time_now_ms",
            "                )",
            "            except KeyLookupError as e:",
            "                logger.warning(",
            "                    \"Error processing response from key notary server %s for origin \"",
            "                    \"server %s: %s\",",
            "                    perspective_name,",
            "                    server_name,",
            "                    e,",
            "                )",
            "                # we continue to process the rest of the response",
            "                continue",
            "",
            "            added_keys.extend(",
            "                (server_name, key_id, key) for key_id, key in processed_response.items()",
            "            )",
            "            keys.setdefault(server_name, {}).update(processed_response)",
            "",
            "        await self.store.store_server_verify_keys(",
            "            perspective_name, time_now_ms, added_keys",
            "        )",
            "",
            "        return keys",
            "",
            "    def _validate_perspectives_response(self, key_server, response):",
            "        \"\"\"Optionally check the signature on the result of a /key/query request",
            "",
            "        Args:",
            "            key_server (synapse.config.key.TrustedKeyServer): the notary server that",
            "                produced this result",
            "",
            "            response (dict): the json-decoded Server Keys response object",
            "        \"\"\"",
            "        perspective_name = key_server.server_name",
            "        perspective_keys = key_server.verify_keys",
            "",
            "        if perspective_keys is None:",
            "            # signature checking is disabled on this server",
            "            return",
            "",
            "        if (",
            "            \"signatures\" not in response",
            "            or perspective_name not in response[\"signatures\"]",
            "        ):",
            "            raise KeyLookupError(\"Response not signed by the notary server\")",
            "",
            "        verified = False",
            "        for key_id in response[\"signatures\"][perspective_name]:",
            "            if key_id in perspective_keys:",
            "                verify_signed_json(response, perspective_name, perspective_keys[key_id])",
            "                verified = True",
            "",
            "        if not verified:",
            "            raise KeyLookupError(",
            "                \"Response not signed with a known key: signed with: %r, known keys: %r\"",
            "                % (",
            "                    list(response[\"signatures\"][perspective_name].keys()),",
            "                    list(perspective_keys.keys()),",
            "                )",
            "            )",
            "",
            "",
            "class ServerKeyFetcher(BaseV2KeyFetcher):",
            "    \"\"\"KeyFetcher impl which fetches keys from the origin servers\"\"\"",
            "",
            "    def __init__(self, hs):",
            "        super().__init__(hs)",
            "        self.clock = hs.get_clock()",
            "        self.client = hs.get_federation_http_client()",
            "",
            "    async def get_keys(self, keys_to_fetch):",
            "        \"\"\"",
            "        Args:",
            "            keys_to_fetch (dict[str, iterable[str]]):",
            "                the keys to be fetched. server_name -> key_ids",
            "",
            "        Returns:",
            "            dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]:",
            "                map from server_name -> key_id -> FetchKeyResult",
            "        \"\"\"",
            "",
            "        results = {}",
            "",
            "        async def get_key(key_to_fetch_item):",
            "            server_name, key_ids = key_to_fetch_item",
            "            try:",
            "                keys = await self.get_server_verify_key_v2_direct(server_name, key_ids)",
            "                results[server_name] = keys",
            "            except KeyLookupError as e:",
            "                logger.warning(",
            "                    \"Error looking up keys %s from %s: %s\", key_ids, server_name, e",
            "                )",
            "            except Exception:",
            "                logger.exception(\"Error getting keys %s from %s\", key_ids, server_name)",
            "",
            "        await yieldable_gather_results(get_key, keys_to_fetch.items())",
            "        return results",
            "",
            "    async def get_server_verify_key_v2_direct(self, server_name, key_ids):",
            "        \"\"\"",
            "",
            "        Args:",
            "            server_name (str):",
            "            key_ids (iterable[str]):",
            "",
            "        Returns:",
            "            dict[str, FetchKeyResult]: map from key ID to lookup result",
            "",
            "        Raises:",
            "            KeyLookupError if there was a problem making the lookup",
            "        \"\"\"",
            "        keys = {}  # type: dict[str, FetchKeyResult]",
            "",
            "        for requested_key_id in key_ids:",
            "            # we may have found this key as a side-effect of asking for another.",
            "            if requested_key_id in keys:",
            "                continue",
            "",
            "            time_now_ms = self.clock.time_msec()",
            "            try:",
            "                response = await self.client.get_json(",
            "                    destination=server_name,",
            "                    path=\"/_matrix/key/v2/server/\"",
            "                    + urllib.parse.quote(requested_key_id),",
            "                    ignore_backoff=True,",
            "                    # we only give the remote server 10s to respond. It should be an",
            "                    # easy request to handle, so if it doesn't reply within 10s, it's",
            "                    # probably not going to.",
            "                    #",
            "                    # Furthermore, when we are acting as a notary server, we cannot",
            "                    # wait all day for all of the origin servers, as the requesting",
            "                    # server will otherwise time out before we can respond.",
            "                    #",
            "                    # (Note that get_json may make 4 attempts, so this can still take",
            "                    # almost 45 seconds to fetch the headers, plus up to another 60s to",
            "                    # read the response).",
            "                    timeout=10000,",
            "                )",
            "            except (NotRetryingDestination, RequestSendFailed) as e:",
            "                # these both have str() representations which we can't really improve",
            "                # upon",
            "                raise KeyLookupError(str(e))",
            "            except HttpResponseException as e:",
            "                raise KeyLookupError(\"Remote server returned an error: %s\" % (e,))",
            "",
            "            if response[\"server_name\"] != server_name:",
            "                raise KeyLookupError(",
            "                    \"Expected a response for server %r not %r\"",
            "                    % (server_name, response[\"server_name\"])",
            "                )",
            "",
            "            response_keys = await self.process_v2_response(",
            "                from_server=server_name,",
            "                response_json=response,",
            "                time_added_ms=time_now_ms,",
            "            )",
            "            await self.store.store_server_verify_keys(",
            "                server_name,",
            "                time_now_ms,",
            "                ((server_name, key_id, key) for key_id, key in response_keys.items()),",
            "            )",
            "            keys.update(response_keys)",
            "",
            "        return keys",
            "",
            "",
            "async def _handle_key_deferred(verify_request) -> None:",
            "    \"\"\"Waits for the key to become available, and then performs a verification",
            "",
            "    Args:",
            "        verify_request (VerifyJsonRequest):",
            "",
            "    Raises:",
            "        SynapseError if there was a problem performing the verification",
            "    \"\"\"",
            "    server_name = verify_request.server_name",
            "    with PreserveLoggingContext():",
            "        _, key_id, verify_key = await verify_request.key_ready",
            "",
            "    json_object = verify_request.json_object",
            "",
            "    try:",
            "        verify_signed_json(json_object, server_name, verify_key)",
            "    except SignatureVerifyException as e:",
            "        logger.debug(",
            "            \"Error verifying signature for %s:%s:%s with key %s: %s\",",
            "            server_name,",
            "            verify_key.alg,",
            "            verify_key.version,",
            "            encode_verify_key_base64(verify_key),",
            "            str(e),",
            "        )",
            "        raise SynapseError(",
            "            401,",
            "            \"Invalid signature for server %s with key %s:%s: %s\"",
            "            % (server_name, verify_key.alg, verify_key.version, str(e)),",
            "            Codes.UNAUTHORIZED,",
            "        )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "581": [
                "PerspectivesKeyFetcher",
                "__init__"
            ],
            "751": [
                "ServerKeyFetcher",
                "__init__"
            ]
        },
        "addLocation": []
    },
    "synapse/federation/federation_server.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 845,
                "afterPatchRowNumber": 845,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 846,
                "afterPatchRowNumber": 846,
                "PatchRowcode": "     def __init__(self, hs: \"HomeServer\"):"
            },
            "2": {
                "beforePatchRowNumber": 847,
                "afterPatchRowNumber": 847,
                "PatchRowcode": "         self.config = hs.config"
            },
            "3": {
                "beforePatchRowNumber": 848,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.http_client = hs.get_simple_http_client()"
            },
            "4": {
                "beforePatchRowNumber": 849,
                "afterPatchRowNumber": 848,
                "PatchRowcode": "         self.clock = hs.get_clock()"
            },
            "5": {
                "beforePatchRowNumber": 850,
                "afterPatchRowNumber": 849,
                "PatchRowcode": "         self._instance_name = hs.get_instance_name()"
            },
            "6": {
                "beforePatchRowNumber": 851,
                "afterPatchRowNumber": 850,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "# Copyright 2019 Matrix.org Federation C.I.C",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Awaitable,",
            "    Callable,",
            "    Dict,",
            "    List,",
            "    Optional,",
            "    Tuple,",
            "    Union,",
            ")",
            "",
            "from prometheus_client import Counter, Gauge, Histogram",
            "",
            "from twisted.internet import defer",
            "from twisted.internet.abstract import isIPAddress",
            "from twisted.python import failure",
            "",
            "from synapse.api.constants import EventTypes, Membership",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    Codes,",
            "    FederationError,",
            "    IncompatibleRoomVersionError,",
            "    NotFoundError,",
            "    SynapseError,",
            "    UnsupportedRoomVersionError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS",
            "from synapse.events import EventBase",
            "from synapse.federation.federation_base import FederationBase, event_from_pdu_json",
            "from synapse.federation.persistence import TransactionActions",
            "from synapse.federation.units import Edu, Transaction",
            "from synapse.http.endpoint import parse_server_name",
            "from synapse.http.servlet import assert_params_in_dict",
            "from synapse.logging.context import (",
            "    make_deferred_yieldable,",
            "    nested_logging_context,",
            "    run_in_background,",
            ")",
            "from synapse.logging.opentracing import log_kv, start_active_span_from_edu, trace",
            "from synapse.logging.utils import log_function",
            "from synapse.replication.http.federation import (",
            "    ReplicationFederationSendEduRestServlet,",
            "    ReplicationGetQueryRestServlet,",
            ")",
            "from synapse.types import JsonDict, get_domain_from_id",
            "from synapse.util import glob_to_regex, json_decoder, unwrapFirstError",
            "from synapse.util.async_helpers import Linearizer, concurrently_execute",
            "from synapse.util.caches.response_cache import ResponseCache",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "# when processing incoming transactions, we try to handle multiple rooms in",
            "# parallel, up to this limit.",
            "TRANSACTION_CONCURRENCY_LIMIT = 10",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "received_pdus_counter = Counter(\"synapse_federation_server_received_pdus\", \"\")",
            "",
            "received_edus_counter = Counter(\"synapse_federation_server_received_edus\", \"\")",
            "",
            "received_queries_counter = Counter(",
            "    \"synapse_federation_server_received_queries\", \"\", [\"type\"]",
            ")",
            "",
            "pdu_process_time = Histogram(",
            "    \"synapse_federation_server_pdu_process_time\", \"Time taken to process an event\",",
            ")",
            "",
            "",
            "last_pdu_age_metric = Gauge(",
            "    \"synapse_federation_last_received_pdu_age\",",
            "    \"The age (in seconds) of the last PDU successfully received from the given domain\",",
            "    labelnames=(\"server_name\",),",
            ")",
            "",
            "",
            "class FederationServer(FederationBase):",
            "    def __init__(self, hs):",
            "        super().__init__(hs)",
            "",
            "        self.auth = hs.get_auth()",
            "        self.handler = hs.get_federation_handler()",
            "        self.state = hs.get_state_handler()",
            "",
            "        self.device_handler = hs.get_device_handler()",
            "",
            "        # Ensure the following handlers are loaded since they register callbacks",
            "        # with FederationHandlerRegistry.",
            "        hs.get_directory_handler()",
            "",
            "        self._federation_ratelimiter = hs.get_federation_ratelimiter()",
            "",
            "        self._server_linearizer = Linearizer(\"fed_server\")",
            "        self._transaction_linearizer = Linearizer(\"fed_txn_handler\")",
            "",
            "        # We cache results for transaction with the same ID",
            "        self._transaction_resp_cache = ResponseCache(",
            "            hs, \"fed_txn_handler\", timeout_ms=30000",
            "        )  # type: ResponseCache[Tuple[str, str]]",
            "",
            "        self.transaction_actions = TransactionActions(self.store)",
            "",
            "        self.registry = hs.get_federation_registry()",
            "",
            "        # We cache responses to state queries, as they take a while and often",
            "        # come in waves.",
            "        self._state_resp_cache = ResponseCache(",
            "            hs, \"state_resp\", timeout_ms=30000",
            "        )  # type: ResponseCache[Tuple[str, str]]",
            "        self._state_ids_resp_cache = ResponseCache(",
            "            hs, \"state_ids_resp\", timeout_ms=30000",
            "        )  # type: ResponseCache[Tuple[str, str]]",
            "",
            "        self._federation_metrics_domains = (",
            "            hs.get_config().federation.federation_metrics_domains",
            "        )",
            "",
            "    async def on_backfill_request(",
            "        self, origin: str, room_id: str, versions: List[str], limit: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        with (await self._server_linearizer.queue((origin, room_id))):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            pdus = await self.handler.on_backfill_request(",
            "                origin, room_id, versions, limit",
            "            )",
            "",
            "            res = self._transaction_from_pdus(pdus).get_dict()",
            "",
            "        return 200, res",
            "",
            "    async def on_incoming_transaction(",
            "        self, origin: str, transaction_data: JsonDict",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        # keep this as early as possible to make the calculated origin ts as",
            "        # accurate as possible.",
            "        request_time = self._clock.time_msec()",
            "",
            "        transaction = Transaction(**transaction_data)",
            "        transaction_id = transaction.transaction_id  # type: ignore",
            "",
            "        if not transaction_id:",
            "            raise Exception(\"Transaction missing transaction_id\")",
            "",
            "        logger.debug(\"[%s] Got transaction\", transaction_id)",
            "",
            "        # We wrap in a ResponseCache so that we de-duplicate retried",
            "        # transactions.",
            "        return await self._transaction_resp_cache.wrap(",
            "            (origin, transaction_id),",
            "            self._on_incoming_transaction_inner,",
            "            origin,",
            "            transaction,",
            "            request_time,",
            "        )",
            "",
            "    async def _on_incoming_transaction_inner(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        # Use a linearizer to ensure that transactions from a remote are",
            "        # processed in order.",
            "        with await self._transaction_linearizer.queue(origin):",
            "            # We rate limit here *after* we've queued up the incoming requests,",
            "            # so that we don't fill up the ratelimiter with blocked requests.",
            "            #",
            "            # This is important as the ratelimiter allows N concurrent requests",
            "            # at a time, and only starts ratelimiting if there are more requests",
            "            # than that being processed at a time. If we queued up requests in",
            "            # the linearizer/response cache *after* the ratelimiting then those",
            "            # queued up requests would count as part of the allowed limit of N",
            "            # concurrent requests.",
            "            with self._federation_ratelimiter.ratelimit(origin) as d:",
            "                await d",
            "",
            "                result = await self._handle_incoming_transaction(",
            "                    origin, transaction, request_time",
            "                )",
            "",
            "        return result",
            "",
            "    async def _handle_incoming_transaction(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        \"\"\" Process an incoming transaction and return the HTTP response",
            "",
            "        Args:",
            "            origin: the server making the request",
            "            transaction: incoming transaction",
            "            request_time: timestamp that the HTTP request arrived at",
            "",
            "        Returns:",
            "            HTTP response code and body",
            "        \"\"\"",
            "        response = await self.transaction_actions.have_responded(origin, transaction)",
            "",
            "        if response:",
            "            logger.debug(",
            "                \"[%s] We've already responded to this request\",",
            "                transaction.transaction_id,  # type: ignore",
            "            )",
            "            return response",
            "",
            "        logger.debug(\"[%s] Transaction is new\", transaction.transaction_id)  # type: ignore",
            "",
            "        # Reject if PDU count > 50 or EDU count > 100",
            "        if len(transaction.pdus) > 50 or (  # type: ignore",
            "            hasattr(transaction, \"edus\") and len(transaction.edus) > 100  # type: ignore",
            "        ):",
            "",
            "            logger.info(\"Transaction PDU or EDU count too large. Returning 400\")",
            "",
            "            response = {}",
            "            await self.transaction_actions.set_response(",
            "                origin, transaction, 400, response",
            "            )",
            "            return 400, response",
            "",
            "        # We process PDUs and EDUs in parallel. This is important as we don't",
            "        # want to block things like to device messages from reaching clients",
            "        # behind the potentially expensive handling of PDUs.",
            "        pdu_results, _ = await make_deferred_yieldable(",
            "            defer.gatherResults(",
            "                [",
            "                    run_in_background(",
            "                        self._handle_pdus_in_txn, origin, transaction, request_time",
            "                    ),",
            "                    run_in_background(self._handle_edus_in_txn, origin, transaction),",
            "                ],",
            "                consumeErrors=True,",
            "            ).addErrback(unwrapFirstError)",
            "        )",
            "",
            "        response = {\"pdus\": pdu_results}",
            "",
            "        logger.debug(\"Returning: %s\", str(response))",
            "",
            "        await self.transaction_actions.set_response(origin, transaction, 200, response)",
            "        return 200, response",
            "",
            "    async def _handle_pdus_in_txn(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Dict[str, dict]:",
            "        \"\"\"Process the PDUs in a received transaction.",
            "",
            "        Args:",
            "            origin: the server making the request",
            "            transaction: incoming transaction",
            "            request_time: timestamp that the HTTP request arrived at",
            "",
            "        Returns:",
            "            A map from event ID of a processed PDU to any errors we should",
            "            report back to the sending server.",
            "        \"\"\"",
            "",
            "        received_pdus_counter.inc(len(transaction.pdus))  # type: ignore",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "",
            "        pdus_by_room = {}  # type: Dict[str, List[EventBase]]",
            "",
            "        newest_pdu_ts = 0",
            "",
            "        for p in transaction.pdus:  # type: ignore",
            "            # FIXME (richardv): I don't think this works:",
            "            #  https://github.com/matrix-org/synapse/issues/8429",
            "            if \"unsigned\" in p:",
            "                unsigned = p[\"unsigned\"]",
            "                if \"age\" in unsigned:",
            "                    p[\"age\"] = unsigned[\"age\"]",
            "            if \"age\" in p:",
            "                p[\"age_ts\"] = request_time - int(p[\"age\"])",
            "                del p[\"age\"]",
            "",
            "            # We try and pull out an event ID so that if later checks fail we",
            "            # can log something sensible. We don't mandate an event ID here in",
            "            # case future event formats get rid of the key.",
            "            possible_event_id = p.get(\"event_id\", \"<Unknown>\")",
            "",
            "            # Now we get the room ID so that we can check that we know the",
            "            # version of the room.",
            "            room_id = p.get(\"room_id\")",
            "            if not room_id:",
            "                logger.info(",
            "                    \"Ignoring PDU as does not have a room_id. Event ID: %s\",",
            "                    possible_event_id,",
            "                )",
            "                continue",
            "",
            "            try:",
            "                room_version = await self.store.get_room_version(room_id)",
            "            except NotFoundError:",
            "                logger.info(\"Ignoring PDU for unknown room_id: %s\", room_id)",
            "                continue",
            "            except UnsupportedRoomVersionError as e:",
            "                # this can happen if support for a given room version is withdrawn,",
            "                # so that we still get events for said room.",
            "                logger.info(\"Ignoring PDU: %s\", e)",
            "                continue",
            "",
            "            event = event_from_pdu_json(p, room_version)",
            "            pdus_by_room.setdefault(room_id, []).append(event)",
            "",
            "            if event.origin_server_ts > newest_pdu_ts:",
            "                newest_pdu_ts = event.origin_server_ts",
            "",
            "        pdu_results = {}",
            "",
            "        # we can process different rooms in parallel (which is useful if they",
            "        # require callouts to other servers to fetch missing events), but",
            "        # impose a limit to avoid going too crazy with ram/cpu.",
            "",
            "        async def process_pdus_for_room(room_id: str):",
            "            logger.debug(\"Processing PDUs for %s\", room_id)",
            "            try:",
            "                await self.check_server_matches_acl(origin_host, room_id)",
            "            except AuthError as e:",
            "                logger.warning(\"Ignoring PDUs for room %s from banned server\", room_id)",
            "                for pdu in pdus_by_room[room_id]:",
            "                    event_id = pdu.event_id",
            "                    pdu_results[event_id] = e.error_dict()",
            "                return",
            "",
            "            for pdu in pdus_by_room[room_id]:",
            "                event_id = pdu.event_id",
            "                with pdu_process_time.time():",
            "                    with nested_logging_context(event_id):",
            "                        try:",
            "                            await self._handle_received_pdu(origin, pdu)",
            "                            pdu_results[event_id] = {}",
            "                        except FederationError as e:",
            "                            logger.warning(\"Error handling PDU %s: %s\", event_id, e)",
            "                            pdu_results[event_id] = {\"error\": str(e)}",
            "                        except Exception as e:",
            "                            f = failure.Failure()",
            "                            pdu_results[event_id] = {\"error\": str(e)}",
            "                            logger.error(",
            "                                \"Failed to handle PDU %s\",",
            "                                event_id,",
            "                                exc_info=(f.type, f.value, f.getTracebackObject()),",
            "                            )",
            "",
            "        await concurrently_execute(",
            "            process_pdus_for_room, pdus_by_room.keys(), TRANSACTION_CONCURRENCY_LIMIT",
            "        )",
            "",
            "        if newest_pdu_ts and origin in self._federation_metrics_domains:",
            "            newest_pdu_age = self._clock.time_msec() - newest_pdu_ts",
            "            last_pdu_age_metric.labels(server_name=origin).set(newest_pdu_age / 1000)",
            "",
            "        return pdu_results",
            "",
            "    async def _handle_edus_in_txn(self, origin: str, transaction: Transaction):",
            "        \"\"\"Process the EDUs in a received transaction.",
            "        \"\"\"",
            "",
            "        async def _process_edu(edu_dict):",
            "            received_edus_counter.inc()",
            "",
            "            edu = Edu(",
            "                origin=origin,",
            "                destination=self.server_name,",
            "                edu_type=edu_dict[\"edu_type\"],",
            "                content=edu_dict[\"content\"],",
            "            )",
            "            await self.registry.on_edu(edu.edu_type, origin, edu.content)",
            "",
            "        await concurrently_execute(",
            "            _process_edu,",
            "            getattr(transaction, \"edus\", []),",
            "            TRANSACTION_CONCURRENCY_LIMIT,",
            "        )",
            "",
            "    async def on_room_state_request(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        in_room = await self.auth.check_host_in_room(room_id, origin)",
            "        if not in_room:",
            "            raise AuthError(403, \"Host not in room.\")",
            "",
            "        # we grab the linearizer to protect ourselves from servers which hammer",
            "        # us. In theory we might already have the response to this query",
            "        # in the cache so we could return it without waiting for the linearizer",
            "        # - but that's non-trivial to get right, and anyway somewhat defeats",
            "        # the point of the linearizer.",
            "        with (await self._server_linearizer.queue((origin, room_id))):",
            "            resp = dict(",
            "                await self._state_resp_cache.wrap(",
            "                    (room_id, event_id),",
            "                    self._on_context_state_request_compute,",
            "                    room_id,",
            "                    event_id,",
            "                )",
            "            )",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "        resp[\"room_version\"] = room_version",
            "",
            "        return 200, resp",
            "",
            "    async def on_state_ids_request(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        if not event_id:",
            "            raise NotImplementedError(\"Specify an event\")",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        in_room = await self.auth.check_host_in_room(room_id, origin)",
            "        if not in_room:",
            "            raise AuthError(403, \"Host not in room.\")",
            "",
            "        resp = await self._state_ids_resp_cache.wrap(",
            "            (room_id, event_id), self._on_state_ids_request_compute, room_id, event_id,",
            "        )",
            "",
            "        return 200, resp",
            "",
            "    async def _on_state_ids_request_compute(self, room_id, event_id):",
            "        state_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)",
            "        auth_chain_ids = await self.store.get_auth_chain_ids(state_ids)",
            "        return {\"pdu_ids\": state_ids, \"auth_chain_ids\": auth_chain_ids}",
            "",
            "    async def _on_context_state_request_compute(",
            "        self, room_id: str, event_id: str",
            "    ) -> Dict[str, list]:",
            "        if event_id:",
            "            pdus = await self.handler.get_state_for_pdu(room_id, event_id)",
            "        else:",
            "            pdus = (await self.state.get_current_state(room_id)).values()",
            "",
            "        auth_chain = await self.store.get_auth_chain([pdu.event_id for pdu in pdus])",
            "",
            "        return {",
            "            \"pdus\": [pdu.get_pdu_json() for pdu in pdus],",
            "            \"auth_chain\": [pdu.get_pdu_json() for pdu in auth_chain],",
            "        }",
            "",
            "    async def on_pdu_request(",
            "        self, origin: str, event_id: str",
            "    ) -> Tuple[int, Union[JsonDict, str]]:",
            "        pdu = await self.handler.get_persisted_pdu(origin, event_id)",
            "",
            "        if pdu:",
            "            return 200, self._transaction_from_pdus([pdu]).get_dict()",
            "        else:",
            "            return 404, \"\"",
            "",
            "    async def on_query_request(",
            "        self, query_type: str, args: Dict[str, str]",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        received_queries_counter.labels(query_type).inc()",
            "        resp = await self.registry.on_query(query_type, args)",
            "        return 200, resp",
            "",
            "    async def on_make_join_request(",
            "        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]",
            "    ) -> Dict[str, Any]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "        if room_version not in supported_versions:",
            "            logger.warning(",
            "                \"Room version %s not in %s\", room_version, supported_versions",
            "            )",
            "            raise IncompatibleRoomVersionError(room_version=room_version)",
            "",
            "        pdu = await self.handler.on_make_join_request(origin, room_id, user_id)",
            "        time_now = self._clock.time_msec()",
            "        return {\"event\": pdu.get_pdu_json(time_now), \"room_version\": room_version}",
            "",
            "    async def on_invite_request(",
            "        self, origin: str, content: JsonDict, room_version_id: str",
            "    ) -> Dict[str, Any]:",
            "        room_version = KNOWN_ROOM_VERSIONS.get(room_version_id)",
            "        if not room_version:",
            "            raise SynapseError(",
            "                400,",
            "                \"Homeserver does not support this room version\",",
            "                Codes.UNSUPPORTED_ROOM_VERSION,",
            "            )",
            "",
            "        pdu = event_from_pdu_json(content, room_version)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, pdu.room_id)",
            "        pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "        ret_pdu = await self.handler.on_invite_request(origin, pdu, room_version)",
            "        time_now = self._clock.time_msec()",
            "        return {\"event\": ret_pdu.get_pdu_json(time_now)}",
            "",
            "    async def on_send_join_request(",
            "        self, origin: str, content: JsonDict",
            "    ) -> Dict[str, Any]:",
            "        logger.debug(\"on_send_join_request: content: %s\", content)",
            "",
            "        assert_params_in_dict(content, [\"room_id\"])",
            "        room_version = await self.store.get_room_version(content[\"room_id\"])",
            "        pdu = event_from_pdu_json(content, room_version)",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, pdu.room_id)",
            "",
            "        logger.debug(\"on_send_join_request: pdu sigs: %s\", pdu.signatures)",
            "",
            "        pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "",
            "        res_pdus = await self.handler.on_send_join_request(origin, pdu)",
            "        time_now = self._clock.time_msec()",
            "        return {",
            "            \"state\": [p.get_pdu_json(time_now) for p in res_pdus[\"state\"]],",
            "            \"auth_chain\": [p.get_pdu_json(time_now) for p in res_pdus[\"auth_chain\"]],",
            "        }",
            "",
            "    async def on_make_leave_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> Dict[str, Any]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "        pdu = await self.handler.on_make_leave_request(origin, room_id, user_id)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "",
            "        time_now = self._clock.time_msec()",
            "        return {\"event\": pdu.get_pdu_json(time_now), \"room_version\": room_version}",
            "",
            "    async def on_send_leave_request(self, origin: str, content: JsonDict) -> dict:",
            "        logger.debug(\"on_send_leave_request: content: %s\", content)",
            "",
            "        assert_params_in_dict(content, [\"room_id\"])",
            "        room_version = await self.store.get_room_version(content[\"room_id\"])",
            "        pdu = event_from_pdu_json(content, room_version)",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, pdu.room_id)",
            "",
            "        logger.debug(\"on_send_leave_request: pdu sigs: %s\", pdu.signatures)",
            "",
            "        pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "",
            "        await self.handler.on_send_leave_request(origin, pdu)",
            "        return {}",
            "",
            "    async def on_event_auth(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        with (await self._server_linearizer.queue((origin, room_id))):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            time_now = self._clock.time_msec()",
            "            auth_pdus = await self.handler.on_event_auth(event_id)",
            "            res = {\"auth_chain\": [a.get_pdu_json(time_now) for a in auth_pdus]}",
            "        return 200, res",
            "",
            "    @log_function",
            "    async def on_query_client_keys(",
            "        self, origin: str, content: Dict[str, str]",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        return await self.on_query_request(\"client_keys\", content)",
            "",
            "    async def on_query_user_devices(",
            "        self, origin: str, user_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        keys = await self.device_handler.on_federation_query_user_devices(user_id)",
            "        return 200, keys",
            "",
            "    @trace",
            "    async def on_claim_client_keys(",
            "        self, origin: str, content: JsonDict",
            "    ) -> Dict[str, Any]:",
            "        query = []",
            "        for user_id, device_keys in content.get(\"one_time_keys\", {}).items():",
            "            for device_id, algorithm in device_keys.items():",
            "                query.append((user_id, device_id, algorithm))",
            "",
            "        log_kv({\"message\": \"Claiming one time keys.\", \"user, device pairs\": query})",
            "        results = await self.store.claim_e2e_one_time_keys(query)",
            "",
            "        json_result = {}  # type: Dict[str, Dict[str, dict]]",
            "        for user_id, device_keys in results.items():",
            "            for device_id, keys in device_keys.items():",
            "                for key_id, json_str in keys.items():",
            "                    json_result.setdefault(user_id, {})[device_id] = {",
            "                        key_id: json_decoder.decode(json_str)",
            "                    }",
            "",
            "        logger.info(",
            "            \"Claimed one-time-keys: %s\",",
            "            \",\".join(",
            "                (",
            "                    \"%s for %s:%s\" % (key_id, user_id, device_id)",
            "                    for user_id, user_keys in json_result.items()",
            "                    for device_id, device_keys in user_keys.items()",
            "                    for key_id, _ in device_keys.items()",
            "                )",
            "            ),",
            "        )",
            "",
            "        return {\"one_time_keys\": json_result}",
            "",
            "    async def on_get_missing_events(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        earliest_events: List[str],",
            "        latest_events: List[str],",
            "        limit: int,",
            "    ) -> Dict[str, list]:",
            "        with (await self._server_linearizer.queue((origin, room_id))):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            logger.debug(",
            "                \"on_get_missing_events: earliest_events: %r, latest_events: %r,\"",
            "                \" limit: %d\",",
            "                earliest_events,",
            "                latest_events,",
            "                limit,",
            "            )",
            "",
            "            missing_events = await self.handler.on_get_missing_events(",
            "                origin, room_id, earliest_events, latest_events, limit",
            "            )",
            "",
            "            if len(missing_events) < 5:",
            "                logger.debug(",
            "                    \"Returning %d events: %r\", len(missing_events), missing_events",
            "                )",
            "            else:",
            "                logger.debug(\"Returning %d events\", len(missing_events))",
            "",
            "            time_now = self._clock.time_msec()",
            "",
            "        return {\"events\": [ev.get_pdu_json(time_now) for ev in missing_events]}",
            "",
            "    @log_function",
            "    async def on_openid_userinfo(self, token: str) -> Optional[str]:",
            "        ts_now_ms = self._clock.time_msec()",
            "        return await self.store.get_user_id_for_open_id_token(token, ts_now_ms)",
            "",
            "    def _transaction_from_pdus(self, pdu_list: List[EventBase]) -> Transaction:",
            "        \"\"\"Returns a new Transaction containing the given PDUs suitable for",
            "        transmission.",
            "        \"\"\"",
            "        time_now = self._clock.time_msec()",
            "        pdus = [p.get_pdu_json(time_now) for p in pdu_list]",
            "        return Transaction(",
            "            origin=self.server_name,",
            "            pdus=pdus,",
            "            origin_server_ts=int(time_now),",
            "            destination=None,",
            "        )",
            "",
            "    async def _handle_received_pdu(self, origin: str, pdu: EventBase) -> None:",
            "        \"\"\" Process a PDU received in a federation /send/ transaction.",
            "",
            "        If the event is invalid, then this method throws a FederationError.",
            "        (The error will then be logged and sent back to the sender (which",
            "        probably won't do anything with it), and other events in the",
            "        transaction will be processed as normal).",
            "",
            "        It is likely that we'll then receive other events which refer to",
            "        this rejected_event in their prev_events, etc.  When that happens,",
            "        we'll attempt to fetch the rejected event again, which will presumably",
            "        fail, so those second-generation events will also get rejected.",
            "",
            "        Eventually, we get to the point where there are more than 10 events",
            "        between any new events and the original rejected event. Since we",
            "        only try to backfill 10 events deep on received pdu, we then accept the",
            "        new event, possibly introducing a discontinuity in the DAG, with new",
            "        forward extremities, so normal service is approximately returned,",
            "        until we try to backfill across the discontinuity.",
            "",
            "        Args:",
            "            origin: server which sent the pdu",
            "            pdu: received pdu",
            "",
            "        Raises: FederationError if the signatures / hash do not match, or",
            "            if the event was unacceptable for any other reason (eg, too large,",
            "            too many prev_events, couldn't find the prev_events)",
            "        \"\"\"",
            "        # check that it's actually being sent from a valid destination to",
            "        # workaround bug #1753 in 0.18.5 and 0.18.6",
            "        if origin != get_domain_from_id(pdu.sender):",
            "            # We continue to accept join events from any server; this is",
            "            # necessary for the federation join dance to work correctly.",
            "            # (When we join over federation, the \"helper\" server is",
            "            # responsible for sending out the join event, rather than the",
            "            # origin. See bug #1893. This is also true for some third party",
            "            # invites).",
            "            if not (",
            "                pdu.type == \"m.room.member\"",
            "                and pdu.content",
            "                and pdu.content.get(\"membership\", None)",
            "                in (Membership.JOIN, Membership.INVITE)",
            "            ):",
            "                logger.info(",
            "                    \"Discarding PDU %s from invalid origin %s\", pdu.event_id, origin",
            "                )",
            "                return",
            "            else:",
            "                logger.info(\"Accepting join PDU %s from %s\", pdu.event_id, origin)",
            "",
            "        # We've already checked that we know the room version by this point",
            "        room_version = await self.store.get_room_version(pdu.room_id)",
            "",
            "        # Check signature.",
            "        try:",
            "            pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "        except SynapseError as e:",
            "            raise FederationError(\"ERROR\", e.code, e.msg, affected=pdu.event_id)",
            "",
            "        await self.handler.on_receive_pdu(origin, pdu, sent_to_us_directly=True)",
            "",
            "    def __str__(self):",
            "        return \"<ReplicationLayer(%s)>\" % self.server_name",
            "",
            "    async def exchange_third_party_invite(",
            "        self, sender_user_id: str, target_user_id: str, room_id: str, signed: Dict",
            "    ):",
            "        ret = await self.handler.exchange_third_party_invite(",
            "            sender_user_id, target_user_id, room_id, signed",
            "        )",
            "        return ret",
            "",
            "    async def on_exchange_third_party_invite_request(self, event_dict: Dict):",
            "        ret = await self.handler.on_exchange_third_party_invite_request(event_dict)",
            "        return ret",
            "",
            "    async def check_server_matches_acl(self, server_name: str, room_id: str):",
            "        \"\"\"Check if the given server is allowed by the server ACLs in the room",
            "",
            "        Args:",
            "            server_name: name of server, *without any port part*",
            "            room_id: ID of the room to check",
            "",
            "        Raises:",
            "            AuthError if the server does not match the ACL",
            "        \"\"\"",
            "        state_ids = await self.store.get_current_state_ids(room_id)",
            "        acl_event_id = state_ids.get((EventTypes.ServerACL, \"\"))",
            "",
            "        if not acl_event_id:",
            "            return",
            "",
            "        acl_event = await self.store.get_event(acl_event_id)",
            "        if server_matches_acl_event(server_name, acl_event):",
            "            return",
            "",
            "        raise AuthError(code=403, msg=\"Server is banned from room\")",
            "",
            "",
            "def server_matches_acl_event(server_name: str, acl_event: EventBase) -> bool:",
            "    \"\"\"Check if the given server is allowed by the ACL event",
            "",
            "    Args:",
            "        server_name: name of server, without any port part",
            "        acl_event: m.room.server_acl event",
            "",
            "    Returns:",
            "        True if this server is allowed by the ACLs",
            "    \"\"\"",
            "    logger.debug(\"Checking %s against acl %s\", server_name, acl_event.content)",
            "",
            "    # first of all, check if literal IPs are blocked, and if so, whether the",
            "    # server name is a literal IP",
            "    allow_ip_literals = acl_event.content.get(\"allow_ip_literals\", True)",
            "    if not isinstance(allow_ip_literals, bool):",
            "        logger.warning(\"Ignoring non-bool allow_ip_literals flag\")",
            "        allow_ip_literals = True",
            "    if not allow_ip_literals:",
            "        # check for ipv6 literals. These start with '['.",
            "        if server_name[0] == \"[\":",
            "            return False",
            "",
            "        # check for ipv4 literals. We can just lift the routine from twisted.",
            "        if isIPAddress(server_name):",
            "            return False",
            "",
            "    # next,  check the deny list",
            "    deny = acl_event.content.get(\"deny\", [])",
            "    if not isinstance(deny, (list, tuple)):",
            "        logger.warning(\"Ignoring non-list deny ACL %s\", deny)",
            "        deny = []",
            "    for e in deny:",
            "        if _acl_entry_matches(server_name, e):",
            "            # logger.info(\"%s matched deny rule %s\", server_name, e)",
            "            return False",
            "",
            "    # then the allow list.",
            "    allow = acl_event.content.get(\"allow\", [])",
            "    if not isinstance(allow, (list, tuple)):",
            "        logger.warning(\"Ignoring non-list allow ACL %s\", allow)",
            "        allow = []",
            "    for e in allow:",
            "        if _acl_entry_matches(server_name, e):",
            "            # logger.info(\"%s matched allow rule %s\", server_name, e)",
            "            return True",
            "",
            "    # everything else should be rejected.",
            "    # logger.info(\"%s fell through\", server_name)",
            "    return False",
            "",
            "",
            "def _acl_entry_matches(server_name: str, acl_entry: Any) -> bool:",
            "    if not isinstance(acl_entry, str):",
            "        logger.warning(",
            "            \"Ignoring non-str ACL entry '%s' (is %s)\", acl_entry, type(acl_entry)",
            "        )",
            "        return False",
            "    regex = glob_to_regex(acl_entry)",
            "    return bool(regex.match(server_name))",
            "",
            "",
            "class FederationHandlerRegistry:",
            "    \"\"\"Allows classes to register themselves as handlers for a given EDU or",
            "    query type for incoming federation traffic.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.config = hs.config",
            "        self.http_client = hs.get_simple_http_client()",
            "        self.clock = hs.get_clock()",
            "        self._instance_name = hs.get_instance_name()",
            "",
            "        # These are safe to load in monolith mode, but will explode if we try",
            "        # and use them. However we have guards before we use them to ensure that",
            "        # we don't route to ourselves, and in monolith mode that will always be",
            "        # the case.",
            "        self._get_query_client = ReplicationGetQueryRestServlet.make_client(hs)",
            "        self._send_edu = ReplicationFederationSendEduRestServlet.make_client(hs)",
            "",
            "        self.edu_handlers = (",
            "            {}",
            "        )  # type: Dict[str, Callable[[str, dict], Awaitable[None]]]",
            "        self.query_handlers = {}  # type: Dict[str, Callable[[dict], Awaitable[None]]]",
            "",
            "        # Map from type to instance name that we should route EDU handling to.",
            "        self._edu_type_to_instance = {}  # type: Dict[str, str]",
            "",
            "    def register_edu_handler(",
            "        self, edu_type: str, handler: Callable[[str, JsonDict], Awaitable[None]]",
            "    ):",
            "        \"\"\"Sets the handler callable that will be used to handle an incoming",
            "        federation EDU of the given type.",
            "",
            "        Args:",
            "            edu_type: The type of the incoming EDU to register handler for",
            "            handler: A callable invoked on incoming EDU",
            "                of the given type. The arguments are the origin server name and",
            "                the EDU contents.",
            "        \"\"\"",
            "        if edu_type in self.edu_handlers:",
            "            raise KeyError(\"Already have an EDU handler for %s\" % (edu_type,))",
            "",
            "        logger.info(\"Registering federation EDU handler for %r\", edu_type)",
            "",
            "        self.edu_handlers[edu_type] = handler",
            "",
            "    def register_query_handler(",
            "        self, query_type: str, handler: Callable[[dict], defer.Deferred]",
            "    ):",
            "        \"\"\"Sets the handler callable that will be used to handle an incoming",
            "        federation query of the given type.",
            "",
            "        Args:",
            "            query_type: Category name of the query, which should match",
            "                the string used by make_query.",
            "            handler: Invoked to handle",
            "                incoming queries of this type. The return will be yielded",
            "                on and the result used as the response to the query request.",
            "        \"\"\"",
            "        if query_type in self.query_handlers:",
            "            raise KeyError(\"Already have a Query handler for %s\" % (query_type,))",
            "",
            "        logger.info(\"Registering federation query handler for %r\", query_type)",
            "",
            "        self.query_handlers[query_type] = handler",
            "",
            "    def register_instance_for_edu(self, edu_type: str, instance_name: str):",
            "        \"\"\"Register that the EDU handler is on a different instance than master.",
            "        \"\"\"",
            "        self._edu_type_to_instance[edu_type] = instance_name",
            "",
            "    async def on_edu(self, edu_type: str, origin: str, content: dict):",
            "        if not self.config.use_presence and edu_type == \"m.presence\":",
            "            return",
            "",
            "        # Check if we have a handler on this instance",
            "        handler = self.edu_handlers.get(edu_type)",
            "        if handler:",
            "            with start_active_span_from_edu(content, \"handle_edu\"):",
            "                try:",
            "                    await handler(origin, content)",
            "                except SynapseError as e:",
            "                    logger.info(\"Failed to handle edu %r: %r\", edu_type, e)",
            "                except Exception:",
            "                    logger.exception(\"Failed to handle edu %r\", edu_type)",
            "            return",
            "",
            "        # Check if we can route it somewhere else that isn't us",
            "        route_to = self._edu_type_to_instance.get(edu_type, \"master\")",
            "        if route_to != self._instance_name:",
            "            try:",
            "                await self._send_edu(",
            "                    instance_name=route_to,",
            "                    edu_type=edu_type,",
            "                    origin=origin,",
            "                    content=content,",
            "                )",
            "            except SynapseError as e:",
            "                logger.info(\"Failed to handle edu %r: %r\", edu_type, e)",
            "            except Exception:",
            "                logger.exception(\"Failed to handle edu %r\", edu_type)",
            "            return",
            "",
            "        # Oh well, let's just log and move on.",
            "        logger.warning(\"No handler registered for EDU type %s\", edu_type)",
            "",
            "    async def on_query(self, query_type: str, args: dict):",
            "        handler = self.query_handlers.get(query_type)",
            "        if handler:",
            "            return await handler(args)",
            "",
            "        # Check if we can route it somewhere else that isn't us",
            "        if self._instance_name == \"master\":",
            "            return await self._get_query_client(query_type=query_type, args=args)",
            "",
            "        # Uh oh, no handler! Let's raise an exception so the request returns an",
            "        # error.",
            "        logger.warning(\"No handler registered for query type %s\", query_type)",
            "        raise NotFoundError(\"No handler for Query type '%s'\" % (query_type,))"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "# Copyright 2019 Matrix.org Federation C.I.C",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Awaitable,",
            "    Callable,",
            "    Dict,",
            "    List,",
            "    Optional,",
            "    Tuple,",
            "    Union,",
            ")",
            "",
            "from prometheus_client import Counter, Gauge, Histogram",
            "",
            "from twisted.internet import defer",
            "from twisted.internet.abstract import isIPAddress",
            "from twisted.python import failure",
            "",
            "from synapse.api.constants import EventTypes, Membership",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    Codes,",
            "    FederationError,",
            "    IncompatibleRoomVersionError,",
            "    NotFoundError,",
            "    SynapseError,",
            "    UnsupportedRoomVersionError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS",
            "from synapse.events import EventBase",
            "from synapse.federation.federation_base import FederationBase, event_from_pdu_json",
            "from synapse.federation.persistence import TransactionActions",
            "from synapse.federation.units import Edu, Transaction",
            "from synapse.http.endpoint import parse_server_name",
            "from synapse.http.servlet import assert_params_in_dict",
            "from synapse.logging.context import (",
            "    make_deferred_yieldable,",
            "    nested_logging_context,",
            "    run_in_background,",
            ")",
            "from synapse.logging.opentracing import log_kv, start_active_span_from_edu, trace",
            "from synapse.logging.utils import log_function",
            "from synapse.replication.http.federation import (",
            "    ReplicationFederationSendEduRestServlet,",
            "    ReplicationGetQueryRestServlet,",
            ")",
            "from synapse.types import JsonDict, get_domain_from_id",
            "from synapse.util import glob_to_regex, json_decoder, unwrapFirstError",
            "from synapse.util.async_helpers import Linearizer, concurrently_execute",
            "from synapse.util.caches.response_cache import ResponseCache",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "# when processing incoming transactions, we try to handle multiple rooms in",
            "# parallel, up to this limit.",
            "TRANSACTION_CONCURRENCY_LIMIT = 10",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "received_pdus_counter = Counter(\"synapse_federation_server_received_pdus\", \"\")",
            "",
            "received_edus_counter = Counter(\"synapse_federation_server_received_edus\", \"\")",
            "",
            "received_queries_counter = Counter(",
            "    \"synapse_federation_server_received_queries\", \"\", [\"type\"]",
            ")",
            "",
            "pdu_process_time = Histogram(",
            "    \"synapse_federation_server_pdu_process_time\", \"Time taken to process an event\",",
            ")",
            "",
            "",
            "last_pdu_age_metric = Gauge(",
            "    \"synapse_federation_last_received_pdu_age\",",
            "    \"The age (in seconds) of the last PDU successfully received from the given domain\",",
            "    labelnames=(\"server_name\",),",
            ")",
            "",
            "",
            "class FederationServer(FederationBase):",
            "    def __init__(self, hs):",
            "        super().__init__(hs)",
            "",
            "        self.auth = hs.get_auth()",
            "        self.handler = hs.get_federation_handler()",
            "        self.state = hs.get_state_handler()",
            "",
            "        self.device_handler = hs.get_device_handler()",
            "",
            "        # Ensure the following handlers are loaded since they register callbacks",
            "        # with FederationHandlerRegistry.",
            "        hs.get_directory_handler()",
            "",
            "        self._federation_ratelimiter = hs.get_federation_ratelimiter()",
            "",
            "        self._server_linearizer = Linearizer(\"fed_server\")",
            "        self._transaction_linearizer = Linearizer(\"fed_txn_handler\")",
            "",
            "        # We cache results for transaction with the same ID",
            "        self._transaction_resp_cache = ResponseCache(",
            "            hs, \"fed_txn_handler\", timeout_ms=30000",
            "        )  # type: ResponseCache[Tuple[str, str]]",
            "",
            "        self.transaction_actions = TransactionActions(self.store)",
            "",
            "        self.registry = hs.get_federation_registry()",
            "",
            "        # We cache responses to state queries, as they take a while and often",
            "        # come in waves.",
            "        self._state_resp_cache = ResponseCache(",
            "            hs, \"state_resp\", timeout_ms=30000",
            "        )  # type: ResponseCache[Tuple[str, str]]",
            "        self._state_ids_resp_cache = ResponseCache(",
            "            hs, \"state_ids_resp\", timeout_ms=30000",
            "        )  # type: ResponseCache[Tuple[str, str]]",
            "",
            "        self._federation_metrics_domains = (",
            "            hs.get_config().federation.federation_metrics_domains",
            "        )",
            "",
            "    async def on_backfill_request(",
            "        self, origin: str, room_id: str, versions: List[str], limit: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        with (await self._server_linearizer.queue((origin, room_id))):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            pdus = await self.handler.on_backfill_request(",
            "                origin, room_id, versions, limit",
            "            )",
            "",
            "            res = self._transaction_from_pdus(pdus).get_dict()",
            "",
            "        return 200, res",
            "",
            "    async def on_incoming_transaction(",
            "        self, origin: str, transaction_data: JsonDict",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        # keep this as early as possible to make the calculated origin ts as",
            "        # accurate as possible.",
            "        request_time = self._clock.time_msec()",
            "",
            "        transaction = Transaction(**transaction_data)",
            "        transaction_id = transaction.transaction_id  # type: ignore",
            "",
            "        if not transaction_id:",
            "            raise Exception(\"Transaction missing transaction_id\")",
            "",
            "        logger.debug(\"[%s] Got transaction\", transaction_id)",
            "",
            "        # We wrap in a ResponseCache so that we de-duplicate retried",
            "        # transactions.",
            "        return await self._transaction_resp_cache.wrap(",
            "            (origin, transaction_id),",
            "            self._on_incoming_transaction_inner,",
            "            origin,",
            "            transaction,",
            "            request_time,",
            "        )",
            "",
            "    async def _on_incoming_transaction_inner(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        # Use a linearizer to ensure that transactions from a remote are",
            "        # processed in order.",
            "        with await self._transaction_linearizer.queue(origin):",
            "            # We rate limit here *after* we've queued up the incoming requests,",
            "            # so that we don't fill up the ratelimiter with blocked requests.",
            "            #",
            "            # This is important as the ratelimiter allows N concurrent requests",
            "            # at a time, and only starts ratelimiting if there are more requests",
            "            # than that being processed at a time. If we queued up requests in",
            "            # the linearizer/response cache *after* the ratelimiting then those",
            "            # queued up requests would count as part of the allowed limit of N",
            "            # concurrent requests.",
            "            with self._federation_ratelimiter.ratelimit(origin) as d:",
            "                await d",
            "",
            "                result = await self._handle_incoming_transaction(",
            "                    origin, transaction, request_time",
            "                )",
            "",
            "        return result",
            "",
            "    async def _handle_incoming_transaction(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        \"\"\" Process an incoming transaction and return the HTTP response",
            "",
            "        Args:",
            "            origin: the server making the request",
            "            transaction: incoming transaction",
            "            request_time: timestamp that the HTTP request arrived at",
            "",
            "        Returns:",
            "            HTTP response code and body",
            "        \"\"\"",
            "        response = await self.transaction_actions.have_responded(origin, transaction)",
            "",
            "        if response:",
            "            logger.debug(",
            "                \"[%s] We've already responded to this request\",",
            "                transaction.transaction_id,  # type: ignore",
            "            )",
            "            return response",
            "",
            "        logger.debug(\"[%s] Transaction is new\", transaction.transaction_id)  # type: ignore",
            "",
            "        # Reject if PDU count > 50 or EDU count > 100",
            "        if len(transaction.pdus) > 50 or (  # type: ignore",
            "            hasattr(transaction, \"edus\") and len(transaction.edus) > 100  # type: ignore",
            "        ):",
            "",
            "            logger.info(\"Transaction PDU or EDU count too large. Returning 400\")",
            "",
            "            response = {}",
            "            await self.transaction_actions.set_response(",
            "                origin, transaction, 400, response",
            "            )",
            "            return 400, response",
            "",
            "        # We process PDUs and EDUs in parallel. This is important as we don't",
            "        # want to block things like to device messages from reaching clients",
            "        # behind the potentially expensive handling of PDUs.",
            "        pdu_results, _ = await make_deferred_yieldable(",
            "            defer.gatherResults(",
            "                [",
            "                    run_in_background(",
            "                        self._handle_pdus_in_txn, origin, transaction, request_time",
            "                    ),",
            "                    run_in_background(self._handle_edus_in_txn, origin, transaction),",
            "                ],",
            "                consumeErrors=True,",
            "            ).addErrback(unwrapFirstError)",
            "        )",
            "",
            "        response = {\"pdus\": pdu_results}",
            "",
            "        logger.debug(\"Returning: %s\", str(response))",
            "",
            "        await self.transaction_actions.set_response(origin, transaction, 200, response)",
            "        return 200, response",
            "",
            "    async def _handle_pdus_in_txn(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Dict[str, dict]:",
            "        \"\"\"Process the PDUs in a received transaction.",
            "",
            "        Args:",
            "            origin: the server making the request",
            "            transaction: incoming transaction",
            "            request_time: timestamp that the HTTP request arrived at",
            "",
            "        Returns:",
            "            A map from event ID of a processed PDU to any errors we should",
            "            report back to the sending server.",
            "        \"\"\"",
            "",
            "        received_pdus_counter.inc(len(transaction.pdus))  # type: ignore",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "",
            "        pdus_by_room = {}  # type: Dict[str, List[EventBase]]",
            "",
            "        newest_pdu_ts = 0",
            "",
            "        for p in transaction.pdus:  # type: ignore",
            "            # FIXME (richardv): I don't think this works:",
            "            #  https://github.com/matrix-org/synapse/issues/8429",
            "            if \"unsigned\" in p:",
            "                unsigned = p[\"unsigned\"]",
            "                if \"age\" in unsigned:",
            "                    p[\"age\"] = unsigned[\"age\"]",
            "            if \"age\" in p:",
            "                p[\"age_ts\"] = request_time - int(p[\"age\"])",
            "                del p[\"age\"]",
            "",
            "            # We try and pull out an event ID so that if later checks fail we",
            "            # can log something sensible. We don't mandate an event ID here in",
            "            # case future event formats get rid of the key.",
            "            possible_event_id = p.get(\"event_id\", \"<Unknown>\")",
            "",
            "            # Now we get the room ID so that we can check that we know the",
            "            # version of the room.",
            "            room_id = p.get(\"room_id\")",
            "            if not room_id:",
            "                logger.info(",
            "                    \"Ignoring PDU as does not have a room_id. Event ID: %s\",",
            "                    possible_event_id,",
            "                )",
            "                continue",
            "",
            "            try:",
            "                room_version = await self.store.get_room_version(room_id)",
            "            except NotFoundError:",
            "                logger.info(\"Ignoring PDU for unknown room_id: %s\", room_id)",
            "                continue",
            "            except UnsupportedRoomVersionError as e:",
            "                # this can happen if support for a given room version is withdrawn,",
            "                # so that we still get events for said room.",
            "                logger.info(\"Ignoring PDU: %s\", e)",
            "                continue",
            "",
            "            event = event_from_pdu_json(p, room_version)",
            "            pdus_by_room.setdefault(room_id, []).append(event)",
            "",
            "            if event.origin_server_ts > newest_pdu_ts:",
            "                newest_pdu_ts = event.origin_server_ts",
            "",
            "        pdu_results = {}",
            "",
            "        # we can process different rooms in parallel (which is useful if they",
            "        # require callouts to other servers to fetch missing events), but",
            "        # impose a limit to avoid going too crazy with ram/cpu.",
            "",
            "        async def process_pdus_for_room(room_id: str):",
            "            logger.debug(\"Processing PDUs for %s\", room_id)",
            "            try:",
            "                await self.check_server_matches_acl(origin_host, room_id)",
            "            except AuthError as e:",
            "                logger.warning(\"Ignoring PDUs for room %s from banned server\", room_id)",
            "                for pdu in pdus_by_room[room_id]:",
            "                    event_id = pdu.event_id",
            "                    pdu_results[event_id] = e.error_dict()",
            "                return",
            "",
            "            for pdu in pdus_by_room[room_id]:",
            "                event_id = pdu.event_id",
            "                with pdu_process_time.time():",
            "                    with nested_logging_context(event_id):",
            "                        try:",
            "                            await self._handle_received_pdu(origin, pdu)",
            "                            pdu_results[event_id] = {}",
            "                        except FederationError as e:",
            "                            logger.warning(\"Error handling PDU %s: %s\", event_id, e)",
            "                            pdu_results[event_id] = {\"error\": str(e)}",
            "                        except Exception as e:",
            "                            f = failure.Failure()",
            "                            pdu_results[event_id] = {\"error\": str(e)}",
            "                            logger.error(",
            "                                \"Failed to handle PDU %s\",",
            "                                event_id,",
            "                                exc_info=(f.type, f.value, f.getTracebackObject()),",
            "                            )",
            "",
            "        await concurrently_execute(",
            "            process_pdus_for_room, pdus_by_room.keys(), TRANSACTION_CONCURRENCY_LIMIT",
            "        )",
            "",
            "        if newest_pdu_ts and origin in self._federation_metrics_domains:",
            "            newest_pdu_age = self._clock.time_msec() - newest_pdu_ts",
            "            last_pdu_age_metric.labels(server_name=origin).set(newest_pdu_age / 1000)",
            "",
            "        return pdu_results",
            "",
            "    async def _handle_edus_in_txn(self, origin: str, transaction: Transaction):",
            "        \"\"\"Process the EDUs in a received transaction.",
            "        \"\"\"",
            "",
            "        async def _process_edu(edu_dict):",
            "            received_edus_counter.inc()",
            "",
            "            edu = Edu(",
            "                origin=origin,",
            "                destination=self.server_name,",
            "                edu_type=edu_dict[\"edu_type\"],",
            "                content=edu_dict[\"content\"],",
            "            )",
            "            await self.registry.on_edu(edu.edu_type, origin, edu.content)",
            "",
            "        await concurrently_execute(",
            "            _process_edu,",
            "            getattr(transaction, \"edus\", []),",
            "            TRANSACTION_CONCURRENCY_LIMIT,",
            "        )",
            "",
            "    async def on_room_state_request(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        in_room = await self.auth.check_host_in_room(room_id, origin)",
            "        if not in_room:",
            "            raise AuthError(403, \"Host not in room.\")",
            "",
            "        # we grab the linearizer to protect ourselves from servers which hammer",
            "        # us. In theory we might already have the response to this query",
            "        # in the cache so we could return it without waiting for the linearizer",
            "        # - but that's non-trivial to get right, and anyway somewhat defeats",
            "        # the point of the linearizer.",
            "        with (await self._server_linearizer.queue((origin, room_id))):",
            "            resp = dict(",
            "                await self._state_resp_cache.wrap(",
            "                    (room_id, event_id),",
            "                    self._on_context_state_request_compute,",
            "                    room_id,",
            "                    event_id,",
            "                )",
            "            )",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "        resp[\"room_version\"] = room_version",
            "",
            "        return 200, resp",
            "",
            "    async def on_state_ids_request(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        if not event_id:",
            "            raise NotImplementedError(\"Specify an event\")",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        in_room = await self.auth.check_host_in_room(room_id, origin)",
            "        if not in_room:",
            "            raise AuthError(403, \"Host not in room.\")",
            "",
            "        resp = await self._state_ids_resp_cache.wrap(",
            "            (room_id, event_id), self._on_state_ids_request_compute, room_id, event_id,",
            "        )",
            "",
            "        return 200, resp",
            "",
            "    async def _on_state_ids_request_compute(self, room_id, event_id):",
            "        state_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)",
            "        auth_chain_ids = await self.store.get_auth_chain_ids(state_ids)",
            "        return {\"pdu_ids\": state_ids, \"auth_chain_ids\": auth_chain_ids}",
            "",
            "    async def _on_context_state_request_compute(",
            "        self, room_id: str, event_id: str",
            "    ) -> Dict[str, list]:",
            "        if event_id:",
            "            pdus = await self.handler.get_state_for_pdu(room_id, event_id)",
            "        else:",
            "            pdus = (await self.state.get_current_state(room_id)).values()",
            "",
            "        auth_chain = await self.store.get_auth_chain([pdu.event_id for pdu in pdus])",
            "",
            "        return {",
            "            \"pdus\": [pdu.get_pdu_json() for pdu in pdus],",
            "            \"auth_chain\": [pdu.get_pdu_json() for pdu in auth_chain],",
            "        }",
            "",
            "    async def on_pdu_request(",
            "        self, origin: str, event_id: str",
            "    ) -> Tuple[int, Union[JsonDict, str]]:",
            "        pdu = await self.handler.get_persisted_pdu(origin, event_id)",
            "",
            "        if pdu:",
            "            return 200, self._transaction_from_pdus([pdu]).get_dict()",
            "        else:",
            "            return 404, \"\"",
            "",
            "    async def on_query_request(",
            "        self, query_type: str, args: Dict[str, str]",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        received_queries_counter.labels(query_type).inc()",
            "        resp = await self.registry.on_query(query_type, args)",
            "        return 200, resp",
            "",
            "    async def on_make_join_request(",
            "        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]",
            "    ) -> Dict[str, Any]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "        if room_version not in supported_versions:",
            "            logger.warning(",
            "                \"Room version %s not in %s\", room_version, supported_versions",
            "            )",
            "            raise IncompatibleRoomVersionError(room_version=room_version)",
            "",
            "        pdu = await self.handler.on_make_join_request(origin, room_id, user_id)",
            "        time_now = self._clock.time_msec()",
            "        return {\"event\": pdu.get_pdu_json(time_now), \"room_version\": room_version}",
            "",
            "    async def on_invite_request(",
            "        self, origin: str, content: JsonDict, room_version_id: str",
            "    ) -> Dict[str, Any]:",
            "        room_version = KNOWN_ROOM_VERSIONS.get(room_version_id)",
            "        if not room_version:",
            "            raise SynapseError(",
            "                400,",
            "                \"Homeserver does not support this room version\",",
            "                Codes.UNSUPPORTED_ROOM_VERSION,",
            "            )",
            "",
            "        pdu = event_from_pdu_json(content, room_version)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, pdu.room_id)",
            "        pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "        ret_pdu = await self.handler.on_invite_request(origin, pdu, room_version)",
            "        time_now = self._clock.time_msec()",
            "        return {\"event\": ret_pdu.get_pdu_json(time_now)}",
            "",
            "    async def on_send_join_request(",
            "        self, origin: str, content: JsonDict",
            "    ) -> Dict[str, Any]:",
            "        logger.debug(\"on_send_join_request: content: %s\", content)",
            "",
            "        assert_params_in_dict(content, [\"room_id\"])",
            "        room_version = await self.store.get_room_version(content[\"room_id\"])",
            "        pdu = event_from_pdu_json(content, room_version)",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, pdu.room_id)",
            "",
            "        logger.debug(\"on_send_join_request: pdu sigs: %s\", pdu.signatures)",
            "",
            "        pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "",
            "        res_pdus = await self.handler.on_send_join_request(origin, pdu)",
            "        time_now = self._clock.time_msec()",
            "        return {",
            "            \"state\": [p.get_pdu_json(time_now) for p in res_pdus[\"state\"]],",
            "            \"auth_chain\": [p.get_pdu_json(time_now) for p in res_pdus[\"auth_chain\"]],",
            "        }",
            "",
            "    async def on_make_leave_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> Dict[str, Any]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "        pdu = await self.handler.on_make_leave_request(origin, room_id, user_id)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "",
            "        time_now = self._clock.time_msec()",
            "        return {\"event\": pdu.get_pdu_json(time_now), \"room_version\": room_version}",
            "",
            "    async def on_send_leave_request(self, origin: str, content: JsonDict) -> dict:",
            "        logger.debug(\"on_send_leave_request: content: %s\", content)",
            "",
            "        assert_params_in_dict(content, [\"room_id\"])",
            "        room_version = await self.store.get_room_version(content[\"room_id\"])",
            "        pdu = event_from_pdu_json(content, room_version)",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, pdu.room_id)",
            "",
            "        logger.debug(\"on_send_leave_request: pdu sigs: %s\", pdu.signatures)",
            "",
            "        pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "",
            "        await self.handler.on_send_leave_request(origin, pdu)",
            "        return {}",
            "",
            "    async def on_event_auth(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        with (await self._server_linearizer.queue((origin, room_id))):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            time_now = self._clock.time_msec()",
            "            auth_pdus = await self.handler.on_event_auth(event_id)",
            "            res = {\"auth_chain\": [a.get_pdu_json(time_now) for a in auth_pdus]}",
            "        return 200, res",
            "",
            "    @log_function",
            "    async def on_query_client_keys(",
            "        self, origin: str, content: Dict[str, str]",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        return await self.on_query_request(\"client_keys\", content)",
            "",
            "    async def on_query_user_devices(",
            "        self, origin: str, user_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        keys = await self.device_handler.on_federation_query_user_devices(user_id)",
            "        return 200, keys",
            "",
            "    @trace",
            "    async def on_claim_client_keys(",
            "        self, origin: str, content: JsonDict",
            "    ) -> Dict[str, Any]:",
            "        query = []",
            "        for user_id, device_keys in content.get(\"one_time_keys\", {}).items():",
            "            for device_id, algorithm in device_keys.items():",
            "                query.append((user_id, device_id, algorithm))",
            "",
            "        log_kv({\"message\": \"Claiming one time keys.\", \"user, device pairs\": query})",
            "        results = await self.store.claim_e2e_one_time_keys(query)",
            "",
            "        json_result = {}  # type: Dict[str, Dict[str, dict]]",
            "        for user_id, device_keys in results.items():",
            "            for device_id, keys in device_keys.items():",
            "                for key_id, json_str in keys.items():",
            "                    json_result.setdefault(user_id, {})[device_id] = {",
            "                        key_id: json_decoder.decode(json_str)",
            "                    }",
            "",
            "        logger.info(",
            "            \"Claimed one-time-keys: %s\",",
            "            \",\".join(",
            "                (",
            "                    \"%s for %s:%s\" % (key_id, user_id, device_id)",
            "                    for user_id, user_keys in json_result.items()",
            "                    for device_id, device_keys in user_keys.items()",
            "                    for key_id, _ in device_keys.items()",
            "                )",
            "            ),",
            "        )",
            "",
            "        return {\"one_time_keys\": json_result}",
            "",
            "    async def on_get_missing_events(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        earliest_events: List[str],",
            "        latest_events: List[str],",
            "        limit: int,",
            "    ) -> Dict[str, list]:",
            "        with (await self._server_linearizer.queue((origin, room_id))):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            logger.debug(",
            "                \"on_get_missing_events: earliest_events: %r, latest_events: %r,\"",
            "                \" limit: %d\",",
            "                earliest_events,",
            "                latest_events,",
            "                limit,",
            "            )",
            "",
            "            missing_events = await self.handler.on_get_missing_events(",
            "                origin, room_id, earliest_events, latest_events, limit",
            "            )",
            "",
            "            if len(missing_events) < 5:",
            "                logger.debug(",
            "                    \"Returning %d events: %r\", len(missing_events), missing_events",
            "                )",
            "            else:",
            "                logger.debug(\"Returning %d events\", len(missing_events))",
            "",
            "            time_now = self._clock.time_msec()",
            "",
            "        return {\"events\": [ev.get_pdu_json(time_now) for ev in missing_events]}",
            "",
            "    @log_function",
            "    async def on_openid_userinfo(self, token: str) -> Optional[str]:",
            "        ts_now_ms = self._clock.time_msec()",
            "        return await self.store.get_user_id_for_open_id_token(token, ts_now_ms)",
            "",
            "    def _transaction_from_pdus(self, pdu_list: List[EventBase]) -> Transaction:",
            "        \"\"\"Returns a new Transaction containing the given PDUs suitable for",
            "        transmission.",
            "        \"\"\"",
            "        time_now = self._clock.time_msec()",
            "        pdus = [p.get_pdu_json(time_now) for p in pdu_list]",
            "        return Transaction(",
            "            origin=self.server_name,",
            "            pdus=pdus,",
            "            origin_server_ts=int(time_now),",
            "            destination=None,",
            "        )",
            "",
            "    async def _handle_received_pdu(self, origin: str, pdu: EventBase) -> None:",
            "        \"\"\" Process a PDU received in a federation /send/ transaction.",
            "",
            "        If the event is invalid, then this method throws a FederationError.",
            "        (The error will then be logged and sent back to the sender (which",
            "        probably won't do anything with it), and other events in the",
            "        transaction will be processed as normal).",
            "",
            "        It is likely that we'll then receive other events which refer to",
            "        this rejected_event in their prev_events, etc.  When that happens,",
            "        we'll attempt to fetch the rejected event again, which will presumably",
            "        fail, so those second-generation events will also get rejected.",
            "",
            "        Eventually, we get to the point where there are more than 10 events",
            "        between any new events and the original rejected event. Since we",
            "        only try to backfill 10 events deep on received pdu, we then accept the",
            "        new event, possibly introducing a discontinuity in the DAG, with new",
            "        forward extremities, so normal service is approximately returned,",
            "        until we try to backfill across the discontinuity.",
            "",
            "        Args:",
            "            origin: server which sent the pdu",
            "            pdu: received pdu",
            "",
            "        Raises: FederationError if the signatures / hash do not match, or",
            "            if the event was unacceptable for any other reason (eg, too large,",
            "            too many prev_events, couldn't find the prev_events)",
            "        \"\"\"",
            "        # check that it's actually being sent from a valid destination to",
            "        # workaround bug #1753 in 0.18.5 and 0.18.6",
            "        if origin != get_domain_from_id(pdu.sender):",
            "            # We continue to accept join events from any server; this is",
            "            # necessary for the federation join dance to work correctly.",
            "            # (When we join over federation, the \"helper\" server is",
            "            # responsible for sending out the join event, rather than the",
            "            # origin. See bug #1893. This is also true for some third party",
            "            # invites).",
            "            if not (",
            "                pdu.type == \"m.room.member\"",
            "                and pdu.content",
            "                and pdu.content.get(\"membership\", None)",
            "                in (Membership.JOIN, Membership.INVITE)",
            "            ):",
            "                logger.info(",
            "                    \"Discarding PDU %s from invalid origin %s\", pdu.event_id, origin",
            "                )",
            "                return",
            "            else:",
            "                logger.info(\"Accepting join PDU %s from %s\", pdu.event_id, origin)",
            "",
            "        # We've already checked that we know the room version by this point",
            "        room_version = await self.store.get_room_version(pdu.room_id)",
            "",
            "        # Check signature.",
            "        try:",
            "            pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "        except SynapseError as e:",
            "            raise FederationError(\"ERROR\", e.code, e.msg, affected=pdu.event_id)",
            "",
            "        await self.handler.on_receive_pdu(origin, pdu, sent_to_us_directly=True)",
            "",
            "    def __str__(self):",
            "        return \"<ReplicationLayer(%s)>\" % self.server_name",
            "",
            "    async def exchange_third_party_invite(",
            "        self, sender_user_id: str, target_user_id: str, room_id: str, signed: Dict",
            "    ):",
            "        ret = await self.handler.exchange_third_party_invite(",
            "            sender_user_id, target_user_id, room_id, signed",
            "        )",
            "        return ret",
            "",
            "    async def on_exchange_third_party_invite_request(self, event_dict: Dict):",
            "        ret = await self.handler.on_exchange_third_party_invite_request(event_dict)",
            "        return ret",
            "",
            "    async def check_server_matches_acl(self, server_name: str, room_id: str):",
            "        \"\"\"Check if the given server is allowed by the server ACLs in the room",
            "",
            "        Args:",
            "            server_name: name of server, *without any port part*",
            "            room_id: ID of the room to check",
            "",
            "        Raises:",
            "            AuthError if the server does not match the ACL",
            "        \"\"\"",
            "        state_ids = await self.store.get_current_state_ids(room_id)",
            "        acl_event_id = state_ids.get((EventTypes.ServerACL, \"\"))",
            "",
            "        if not acl_event_id:",
            "            return",
            "",
            "        acl_event = await self.store.get_event(acl_event_id)",
            "        if server_matches_acl_event(server_name, acl_event):",
            "            return",
            "",
            "        raise AuthError(code=403, msg=\"Server is banned from room\")",
            "",
            "",
            "def server_matches_acl_event(server_name: str, acl_event: EventBase) -> bool:",
            "    \"\"\"Check if the given server is allowed by the ACL event",
            "",
            "    Args:",
            "        server_name: name of server, without any port part",
            "        acl_event: m.room.server_acl event",
            "",
            "    Returns:",
            "        True if this server is allowed by the ACLs",
            "    \"\"\"",
            "    logger.debug(\"Checking %s against acl %s\", server_name, acl_event.content)",
            "",
            "    # first of all, check if literal IPs are blocked, and if so, whether the",
            "    # server name is a literal IP",
            "    allow_ip_literals = acl_event.content.get(\"allow_ip_literals\", True)",
            "    if not isinstance(allow_ip_literals, bool):",
            "        logger.warning(\"Ignoring non-bool allow_ip_literals flag\")",
            "        allow_ip_literals = True",
            "    if not allow_ip_literals:",
            "        # check for ipv6 literals. These start with '['.",
            "        if server_name[0] == \"[\":",
            "            return False",
            "",
            "        # check for ipv4 literals. We can just lift the routine from twisted.",
            "        if isIPAddress(server_name):",
            "            return False",
            "",
            "    # next,  check the deny list",
            "    deny = acl_event.content.get(\"deny\", [])",
            "    if not isinstance(deny, (list, tuple)):",
            "        logger.warning(\"Ignoring non-list deny ACL %s\", deny)",
            "        deny = []",
            "    for e in deny:",
            "        if _acl_entry_matches(server_name, e):",
            "            # logger.info(\"%s matched deny rule %s\", server_name, e)",
            "            return False",
            "",
            "    # then the allow list.",
            "    allow = acl_event.content.get(\"allow\", [])",
            "    if not isinstance(allow, (list, tuple)):",
            "        logger.warning(\"Ignoring non-list allow ACL %s\", allow)",
            "        allow = []",
            "    for e in allow:",
            "        if _acl_entry_matches(server_name, e):",
            "            # logger.info(\"%s matched allow rule %s\", server_name, e)",
            "            return True",
            "",
            "    # everything else should be rejected.",
            "    # logger.info(\"%s fell through\", server_name)",
            "    return False",
            "",
            "",
            "def _acl_entry_matches(server_name: str, acl_entry: Any) -> bool:",
            "    if not isinstance(acl_entry, str):",
            "        logger.warning(",
            "            \"Ignoring non-str ACL entry '%s' (is %s)\", acl_entry, type(acl_entry)",
            "        )",
            "        return False",
            "    regex = glob_to_regex(acl_entry)",
            "    return bool(regex.match(server_name))",
            "",
            "",
            "class FederationHandlerRegistry:",
            "    \"\"\"Allows classes to register themselves as handlers for a given EDU or",
            "    query type for incoming federation traffic.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.config = hs.config",
            "        self.clock = hs.get_clock()",
            "        self._instance_name = hs.get_instance_name()",
            "",
            "        # These are safe to load in monolith mode, but will explode if we try",
            "        # and use them. However we have guards before we use them to ensure that",
            "        # we don't route to ourselves, and in monolith mode that will always be",
            "        # the case.",
            "        self._get_query_client = ReplicationGetQueryRestServlet.make_client(hs)",
            "        self._send_edu = ReplicationFederationSendEduRestServlet.make_client(hs)",
            "",
            "        self.edu_handlers = (",
            "            {}",
            "        )  # type: Dict[str, Callable[[str, dict], Awaitable[None]]]",
            "        self.query_handlers = {}  # type: Dict[str, Callable[[dict], Awaitable[None]]]",
            "",
            "        # Map from type to instance name that we should route EDU handling to.",
            "        self._edu_type_to_instance = {}  # type: Dict[str, str]",
            "",
            "    def register_edu_handler(",
            "        self, edu_type: str, handler: Callable[[str, JsonDict], Awaitable[None]]",
            "    ):",
            "        \"\"\"Sets the handler callable that will be used to handle an incoming",
            "        federation EDU of the given type.",
            "",
            "        Args:",
            "            edu_type: The type of the incoming EDU to register handler for",
            "            handler: A callable invoked on incoming EDU",
            "                of the given type. The arguments are the origin server name and",
            "                the EDU contents.",
            "        \"\"\"",
            "        if edu_type in self.edu_handlers:",
            "            raise KeyError(\"Already have an EDU handler for %s\" % (edu_type,))",
            "",
            "        logger.info(\"Registering federation EDU handler for %r\", edu_type)",
            "",
            "        self.edu_handlers[edu_type] = handler",
            "",
            "    def register_query_handler(",
            "        self, query_type: str, handler: Callable[[dict], defer.Deferred]",
            "    ):",
            "        \"\"\"Sets the handler callable that will be used to handle an incoming",
            "        federation query of the given type.",
            "",
            "        Args:",
            "            query_type: Category name of the query, which should match",
            "                the string used by make_query.",
            "            handler: Invoked to handle",
            "                incoming queries of this type. The return will be yielded",
            "                on and the result used as the response to the query request.",
            "        \"\"\"",
            "        if query_type in self.query_handlers:",
            "            raise KeyError(\"Already have a Query handler for %s\" % (query_type,))",
            "",
            "        logger.info(\"Registering federation query handler for %r\", query_type)",
            "",
            "        self.query_handlers[query_type] = handler",
            "",
            "    def register_instance_for_edu(self, edu_type: str, instance_name: str):",
            "        \"\"\"Register that the EDU handler is on a different instance than master.",
            "        \"\"\"",
            "        self._edu_type_to_instance[edu_type] = instance_name",
            "",
            "    async def on_edu(self, edu_type: str, origin: str, content: dict):",
            "        if not self.config.use_presence and edu_type == \"m.presence\":",
            "            return",
            "",
            "        # Check if we have a handler on this instance",
            "        handler = self.edu_handlers.get(edu_type)",
            "        if handler:",
            "            with start_active_span_from_edu(content, \"handle_edu\"):",
            "                try:",
            "                    await handler(origin, content)",
            "                except SynapseError as e:",
            "                    logger.info(\"Failed to handle edu %r: %r\", edu_type, e)",
            "                except Exception:",
            "                    logger.exception(\"Failed to handle edu %r\", edu_type)",
            "            return",
            "",
            "        # Check if we can route it somewhere else that isn't us",
            "        route_to = self._edu_type_to_instance.get(edu_type, \"master\")",
            "        if route_to != self._instance_name:",
            "            try:",
            "                await self._send_edu(",
            "                    instance_name=route_to,",
            "                    edu_type=edu_type,",
            "                    origin=origin,",
            "                    content=content,",
            "                )",
            "            except SynapseError as e:",
            "                logger.info(\"Failed to handle edu %r: %r\", edu_type, e)",
            "            except Exception:",
            "                logger.exception(\"Failed to handle edu %r\", edu_type)",
            "            return",
            "",
            "        # Oh well, let's just log and move on.",
            "        logger.warning(\"No handler registered for EDU type %s\", edu_type)",
            "",
            "    async def on_query(self, query_type: str, args: dict):",
            "        handler = self.query_handlers.get(query_type)",
            "        if handler:",
            "            return await handler(args)",
            "",
            "        # Check if we can route it somewhere else that isn't us",
            "        if self._instance_name == \"master\":",
            "            return await self._get_query_client(query_type=query_type, args=args)",
            "",
            "        # Uh oh, no handler! Let's raise an exception so the request returns an",
            "        # error.",
            "        logger.warning(\"No handler registered for query type %s\", query_type)",
            "        raise NotFoundError(\"No handler for Query type '%s'\" % (query_type,))"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "848": [
                "FederationHandlerRegistry",
                "__init__"
            ]
        },
        "addLocation": []
    },
    "synapse/federation/transport/client.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 36,
                "PatchRowcode": "     def __init__(self, hs):"
            },
            "2": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 37,
                "PatchRowcode": "         self.server_name = hs.hostname"
            },
            "3": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.client = hs.get_http_client()"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 38,
                "PatchRowcode": "+        self.client = hs.get_federation_http_client()"
            },
            "5": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 39,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 40,
                "PatchRowcode": "     @log_function"
            },
            "7": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 41,
                "PatchRowcode": "     def get_room_state_ids(self, destination, room_id, event_id):"
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import logging",
            "import urllib",
            "from typing import Any, Dict, Optional",
            "",
            "from synapse.api.constants import Membership",
            "from synapse.api.errors import Codes, HttpResponseException, SynapseError",
            "from synapse.api.urls import (",
            "    FEDERATION_UNSTABLE_PREFIX,",
            "    FEDERATION_V1_PREFIX,",
            "    FEDERATION_V2_PREFIX,",
            ")",
            "from synapse.logging.utils import log_function",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class TransportLayerClient:",
            "    \"\"\"Sends federation HTTP requests to other servers\"\"\"",
            "",
            "    def __init__(self, hs):",
            "        self.server_name = hs.hostname",
            "        self.client = hs.get_http_client()",
            "",
            "    @log_function",
            "    def get_room_state_ids(self, destination, room_id, event_id):",
            "        \"\"\" Requests all state for a given room from the given server at the",
            "        given event. Returns the state's event_id's",
            "",
            "        Args:",
            "            destination (str): The host name of the remote homeserver we want",
            "                to get the state from.",
            "            context (str): The name of the context we want the state of",
            "            event_id (str): The event we want the context at.",
            "",
            "        Returns:",
            "            Awaitable: Results in a dict received from the remote homeserver.",
            "        \"\"\"",
            "        logger.debug(\"get_room_state_ids dest=%s, room=%s\", destination, room_id)",
            "",
            "        path = _create_v1_path(\"/state_ids/%s\", room_id)",
            "        return self.client.get_json(",
            "            destination,",
            "            path=path,",
            "            args={\"event_id\": event_id},",
            "            try_trailing_slash_on_400=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_event(self, destination, event_id, timeout=None):",
            "        \"\"\" Requests the pdu with give id and origin from the given server.",
            "",
            "        Args:",
            "            destination (str): The host name of the remote homeserver we want",
            "                to get the state from.",
            "            event_id (str): The id of the event being requested.",
            "            timeout (int): How long to try (in ms) the destination for before",
            "                giving up. None indicates no timeout.",
            "",
            "        Returns:",
            "            Awaitable: Results in a dict received from the remote homeserver.",
            "        \"\"\"",
            "        logger.debug(\"get_pdu dest=%s, event_id=%s\", destination, event_id)",
            "",
            "        path = _create_v1_path(\"/event/%s\", event_id)",
            "        return self.client.get_json(",
            "            destination, path=path, timeout=timeout, try_trailing_slash_on_400=True",
            "        )",
            "",
            "    @log_function",
            "    def backfill(self, destination, room_id, event_tuples, limit):",
            "        \"\"\" Requests `limit` previous PDUs in a given context before list of",
            "        PDUs.",
            "",
            "        Args:",
            "            dest (str)",
            "            room_id (str)",
            "            event_tuples (list)",
            "            limit (int)",
            "",
            "        Returns:",
            "            Awaitable: Results in a dict received from the remote homeserver.",
            "        \"\"\"",
            "        logger.debug(",
            "            \"backfill dest=%s, room_id=%s, event_tuples=%r, limit=%s\",",
            "            destination,",
            "            room_id,",
            "            event_tuples,",
            "            str(limit),",
            "        )",
            "",
            "        if not event_tuples:",
            "            # TODO: raise?",
            "            return",
            "",
            "        path = _create_v1_path(\"/backfill/%s\", room_id)",
            "",
            "        args = {\"v\": event_tuples, \"limit\": [str(limit)]}",
            "",
            "        return self.client.get_json(",
            "            destination, path=path, args=args, try_trailing_slash_on_400=True",
            "        )",
            "",
            "    @log_function",
            "    async def send_transaction(self, transaction, json_data_callback=None):",
            "        \"\"\" Sends the given Transaction to its destination",
            "",
            "        Args:",
            "            transaction (Transaction)",
            "",
            "        Returns:",
            "            Succeeds when we get a 2xx HTTP response. The result",
            "            will be the decoded JSON body.",
            "",
            "            Fails with ``HTTPRequestException`` if we get an HTTP response",
            "            code >= 300.",
            "",
            "            Fails with ``NotRetryingDestination`` if we are not yet ready",
            "            to retry this server.",
            "",
            "            Fails with ``FederationDeniedError`` if this destination",
            "            is not on our federation whitelist",
            "        \"\"\"",
            "        logger.debug(",
            "            \"send_data dest=%s, txid=%s\",",
            "            transaction.destination,",
            "            transaction.transaction_id,",
            "        )",
            "",
            "        if transaction.destination == self.server_name:",
            "            raise RuntimeError(\"Transport layer cannot send to itself!\")",
            "",
            "        # FIXME: This is only used by the tests. The actual json sent is",
            "        # generated by the json_data_callback.",
            "        json_data = transaction.get_dict()",
            "",
            "        path = _create_v1_path(\"/send/%s\", transaction.transaction_id)",
            "",
            "        response = await self.client.put_json(",
            "            transaction.destination,",
            "            path=path,",
            "            data=json_data,",
            "            json_data_callback=json_data_callback,",
            "            long_retries=True,",
            "            backoff_on_404=True,  # If we get a 404 the other side has gone",
            "            try_trailing_slash_on_400=True,",
            "        )",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def make_query(",
            "        self, destination, query_type, args, retry_on_dns_fail, ignore_backoff=False",
            "    ):",
            "        path = _create_v1_path(\"/query/%s\", query_type)",
            "",
            "        content = await self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args=args,",
            "            retry_on_dns_fail=retry_on_dns_fail,",
            "            timeout=10000,",
            "            ignore_backoff=ignore_backoff,",
            "        )",
            "",
            "        return content",
            "",
            "    @log_function",
            "    async def make_membership_event(",
            "        self, destination, room_id, user_id, membership, params",
            "    ):",
            "        \"\"\"Asks a remote server to build and sign us a membership event",
            "",
            "        Note that this does not append any events to any graphs.",
            "",
            "        Args:",
            "            destination (str): address of remote homeserver",
            "            room_id (str): room to join/leave",
            "            user_id (str): user to be joined/left",
            "            membership (str): one of join/leave",
            "            params (dict[str, str|Iterable[str]]): Query parameters to include in the",
            "                request.",
            "",
            "        Returns:",
            "            Succeeds when we get a 2xx HTTP response. The result",
            "            will be the decoded JSON body (ie, the new event).",
            "",
            "            Fails with ``HTTPRequestException`` if we get an HTTP response",
            "            code >= 300.",
            "",
            "            Fails with ``NotRetryingDestination`` if we are not yet ready",
            "            to retry this server.",
            "",
            "            Fails with ``FederationDeniedError`` if the remote destination",
            "            is not in our federation whitelist",
            "        \"\"\"",
            "        valid_memberships = {Membership.JOIN, Membership.LEAVE}",
            "        if membership not in valid_memberships:",
            "            raise RuntimeError(",
            "                \"make_membership_event called with membership='%s', must be one of %s\"",
            "                % (membership, \",\".join(valid_memberships))",
            "            )",
            "        path = _create_v1_path(\"/make_%s/%s/%s\", membership, room_id, user_id)",
            "",
            "        ignore_backoff = False",
            "        retry_on_dns_fail = False",
            "",
            "        if membership == Membership.LEAVE:",
            "            # we particularly want to do our best to send leave events. The",
            "            # problem is that if it fails, we won't retry it later, so if the",
            "            # remote server was just having a momentary blip, the room will be",
            "            # out of sync.",
            "            ignore_backoff = True",
            "            retry_on_dns_fail = True",
            "",
            "        content = await self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args=params,",
            "            retry_on_dns_fail=retry_on_dns_fail,",
            "            timeout=20000,",
            "            ignore_backoff=ignore_backoff,",
            "        )",
            "",
            "        return content",
            "",
            "    @log_function",
            "    async def send_join_v1(self, destination, room_id, event_id, content):",
            "        path = _create_v1_path(\"/send_join/%s/%s\", room_id, event_id)",
            "",
            "        response = await self.client.put_json(",
            "            destination=destination, path=path, data=content",
            "        )",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def send_join_v2(self, destination, room_id, event_id, content):",
            "        path = _create_v2_path(\"/send_join/%s/%s\", room_id, event_id)",
            "",
            "        response = await self.client.put_json(",
            "            destination=destination, path=path, data=content",
            "        )",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def send_leave_v1(self, destination, room_id, event_id, content):",
            "        path = _create_v1_path(\"/send_leave/%s/%s\", room_id, event_id)",
            "",
            "        response = await self.client.put_json(",
            "            destination=destination,",
            "            path=path,",
            "            data=content,",
            "            # we want to do our best to send this through. The problem is",
            "            # that if it fails, we won't retry it later, so if the remote",
            "            # server was just having a momentary blip, the room will be out of",
            "            # sync.",
            "            ignore_backoff=True,",
            "        )",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def send_leave_v2(self, destination, room_id, event_id, content):",
            "        path = _create_v2_path(\"/send_leave/%s/%s\", room_id, event_id)",
            "",
            "        response = await self.client.put_json(",
            "            destination=destination,",
            "            path=path,",
            "            data=content,",
            "            # we want to do our best to send this through. The problem is",
            "            # that if it fails, we won't retry it later, so if the remote",
            "            # server was just having a momentary blip, the room will be out of",
            "            # sync.",
            "            ignore_backoff=True,",
            "        )",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def send_invite_v1(self, destination, room_id, event_id, content):",
            "        path = _create_v1_path(\"/invite/%s/%s\", room_id, event_id)",
            "",
            "        response = await self.client.put_json(",
            "            destination=destination, path=path, data=content, ignore_backoff=True",
            "        )",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def send_invite_v2(self, destination, room_id, event_id, content):",
            "        path = _create_v2_path(\"/invite/%s/%s\", room_id, event_id)",
            "",
            "        response = await self.client.put_json(",
            "            destination=destination, path=path, data=content, ignore_backoff=True",
            "        )",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def get_public_rooms(",
            "        self,",
            "        remote_server: str,",
            "        limit: Optional[int] = None,",
            "        since_token: Optional[str] = None,",
            "        search_filter: Optional[Dict] = None,",
            "        include_all_networks: bool = False,",
            "        third_party_instance_id: Optional[str] = None,",
            "    ):",
            "        \"\"\"Get the list of public rooms from a remote homeserver",
            "",
            "        See synapse.federation.federation_client.FederationClient.get_public_rooms for",
            "        more information.",
            "        \"\"\"",
            "        if search_filter:",
            "            # this uses MSC2197 (Search Filtering over Federation)",
            "            path = _create_v1_path(\"/publicRooms\")",
            "",
            "            data = {",
            "                \"include_all_networks\": \"true\" if include_all_networks else \"false\"",
            "            }  # type: Dict[str, Any]",
            "            if third_party_instance_id:",
            "                data[\"third_party_instance_id\"] = third_party_instance_id",
            "            if limit:",
            "                data[\"limit\"] = str(limit)",
            "            if since_token:",
            "                data[\"since\"] = since_token",
            "",
            "            data[\"filter\"] = search_filter",
            "",
            "            try:",
            "                response = await self.client.post_json(",
            "                    destination=remote_server, path=path, data=data, ignore_backoff=True",
            "                )",
            "            except HttpResponseException as e:",
            "                if e.code == 403:",
            "                    raise SynapseError(",
            "                        403,",
            "                        \"You are not allowed to view the public rooms list of %s\"",
            "                        % (remote_server,),",
            "                        errcode=Codes.FORBIDDEN,",
            "                    )",
            "                raise",
            "        else:",
            "            path = _create_v1_path(\"/publicRooms\")",
            "",
            "            args = {",
            "                \"include_all_networks\": \"true\" if include_all_networks else \"false\"",
            "            }  # type: Dict[str, Any]",
            "            if third_party_instance_id:",
            "                args[\"third_party_instance_id\"] = (third_party_instance_id,)",
            "            if limit:",
            "                args[\"limit\"] = [str(limit)]",
            "            if since_token:",
            "                args[\"since\"] = [since_token]",
            "",
            "            try:",
            "                response = await self.client.get_json(",
            "                    destination=remote_server, path=path, args=args, ignore_backoff=True",
            "                )",
            "            except HttpResponseException as e:",
            "                if e.code == 403:",
            "                    raise SynapseError(",
            "                        403,",
            "                        \"You are not allowed to view the public rooms list of %s\"",
            "                        % (remote_server,),",
            "                        errcode=Codes.FORBIDDEN,",
            "                    )",
            "                raise",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def exchange_third_party_invite(self, destination, room_id, event_dict):",
            "        path = _create_v1_path(\"/exchange_third_party_invite/%s\", room_id)",
            "",
            "        response = await self.client.put_json(",
            "            destination=destination, path=path, data=event_dict",
            "        )",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def get_event_auth(self, destination, room_id, event_id):",
            "        path = _create_v1_path(\"/event_auth/%s/%s\", room_id, event_id)",
            "",
            "        content = await self.client.get_json(destination=destination, path=path)",
            "",
            "        return content",
            "",
            "    @log_function",
            "    async def query_client_keys(self, destination, query_content, timeout):",
            "        \"\"\"Query the device keys for a list of user ids hosted on a remote",
            "        server.",
            "",
            "        Request:",
            "            {",
            "              \"device_keys\": {",
            "                \"<user_id>\": [\"<device_id>\"]",
            "              }",
            "            }",
            "",
            "        Response:",
            "            {",
            "              \"device_keys\": {",
            "                \"<user_id>\": {",
            "                  \"<device_id>\": {...}",
            "                }",
            "              },",
            "              \"master_key\": {",
            "                \"<user_id>\": {...}",
            "                }",
            "              },",
            "              \"self_signing_key\": {",
            "                \"<user_id>\": {...}",
            "              }",
            "            }",
            "",
            "        Args:",
            "            destination(str): The server to query.",
            "            query_content(dict): The user ids to query.",
            "        Returns:",
            "            A dict containing device and cross-signing keys.",
            "        \"\"\"",
            "        path = _create_v1_path(\"/user/keys/query\")",
            "",
            "        content = await self.client.post_json(",
            "            destination=destination, path=path, data=query_content, timeout=timeout",
            "        )",
            "        return content",
            "",
            "    @log_function",
            "    async def query_user_devices(self, destination, user_id, timeout):",
            "        \"\"\"Query the devices for a user id hosted on a remote server.",
            "",
            "        Response:",
            "            {",
            "              \"stream_id\": \"...\",",
            "              \"devices\": [ { ... } ],",
            "              \"master_key\": {",
            "                \"user_id\": \"<user_id>\",",
            "                \"usage\": [...],",
            "                \"keys\": {...},",
            "                \"signatures\": {",
            "                  \"<user_id>\": {...}",
            "                }",
            "              },",
            "              \"self_signing_key\": {",
            "                \"user_id\": \"<user_id>\",",
            "                \"usage\": [...],",
            "                \"keys\": {...},",
            "                \"signatures\": {",
            "                  \"<user_id>\": {...}",
            "                }",
            "              }",
            "            }",
            "",
            "        Args:",
            "            destination(str): The server to query.",
            "            query_content(dict): The user ids to query.",
            "        Returns:",
            "            A dict containing device and cross-signing keys.",
            "        \"\"\"",
            "        path = _create_v1_path(\"/user/devices/%s\", user_id)",
            "",
            "        content = await self.client.get_json(",
            "            destination=destination, path=path, timeout=timeout",
            "        )",
            "        return content",
            "",
            "    @log_function",
            "    async def claim_client_keys(self, destination, query_content, timeout):",
            "        \"\"\"Claim one-time keys for a list of devices hosted on a remote server.",
            "",
            "        Request:",
            "            {",
            "              \"one_time_keys\": {",
            "                \"<user_id>\": {",
            "                  \"<device_id>\": \"<algorithm>\"",
            "                }",
            "              }",
            "            }",
            "",
            "        Response:",
            "            {",
            "              \"device_keys\": {",
            "                \"<user_id>\": {",
            "                  \"<device_id>\": {",
            "                    \"<algorithm>:<key_id>\": \"<key_base64>\"",
            "                  }",
            "                }",
            "              }",
            "            }",
            "",
            "        Args:",
            "            destination(str): The server to query.",
            "            query_content(dict): The user ids to query.",
            "        Returns:",
            "            A dict containing the one-time keys.",
            "        \"\"\"",
            "",
            "        path = _create_v1_path(\"/user/keys/claim\")",
            "",
            "        content = await self.client.post_json(",
            "            destination=destination, path=path, data=query_content, timeout=timeout",
            "        )",
            "        return content",
            "",
            "    @log_function",
            "    async def get_missing_events(",
            "        self,",
            "        destination,",
            "        room_id,",
            "        earliest_events,",
            "        latest_events,",
            "        limit,",
            "        min_depth,",
            "        timeout,",
            "    ):",
            "        path = _create_v1_path(\"/get_missing_events/%s\", room_id)",
            "",
            "        content = await self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            data={",
            "                \"limit\": int(limit),",
            "                \"min_depth\": int(min_depth),",
            "                \"earliest_events\": earliest_events,",
            "                \"latest_events\": latest_events,",
            "            },",
            "            timeout=timeout,",
            "        )",
            "",
            "        return content",
            "",
            "    @log_function",
            "    def get_group_profile(self, destination, group_id, requester_user_id):",
            "        \"\"\"Get a group profile",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/profile\", group_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def update_group_profile(self, destination, group_id, requester_user_id, content):",
            "        \"\"\"Update a remote group profile",
            "",
            "        Args:",
            "            destination (str)",
            "            group_id (str)",
            "            requester_user_id (str)",
            "            content (dict): The new profile of the group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/profile\", group_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_group_summary(self, destination, group_id, requester_user_id):",
            "        \"\"\"Get a group summary",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/summary\", group_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_rooms_in_group(self, destination, group_id, requester_user_id):",
            "        \"\"\"Get all rooms in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/rooms\", group_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    def add_room_to_group(",
            "        self, destination, group_id, requester_user_id, room_id, content",
            "    ):",
            "        \"\"\"Add a room to a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/room/%s\", group_id, room_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    def update_room_in_group(",
            "        self, destination, group_id, requester_user_id, room_id, config_key, content",
            "    ):",
            "        \"\"\"Update room in group",
            "        \"\"\"",
            "        path = _create_v1_path(",
            "            \"/groups/%s/room/%s/config/%s\", group_id, room_id, config_key",
            "        )",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    def remove_room_from_group(self, destination, group_id, requester_user_id, room_id):",
            "        \"\"\"Remove a room from a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/room/%s\", group_id, room_id)",
            "",
            "        return self.client.delete_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_users_in_group(self, destination, group_id, requester_user_id):",
            "        \"\"\"Get users in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/users\", group_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_invited_users_in_group(self, destination, group_id, requester_user_id):",
            "        \"\"\"Get users that have been invited to a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/invited_users\", group_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def accept_group_invite(self, destination, group_id, user_id, content):",
            "        \"\"\"Accept a group invite",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/users/%s/accept_invite\", group_id, user_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination, path=path, data=content, ignore_backoff=True",
            "        )",
            "",
            "    @log_function",
            "    def join_group(self, destination, group_id, user_id, content):",
            "        \"\"\"Attempts to join a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/users/%s/join\", group_id, user_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination, path=path, data=content, ignore_backoff=True",
            "        )",
            "",
            "    @log_function",
            "    def invite_to_group(",
            "        self, destination, group_id, user_id, requester_user_id, content",
            "    ):",
            "        \"\"\"Invite a user to a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/users/%s/invite\", group_id, user_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def invite_to_group_notification(self, destination, group_id, user_id, content):",
            "        \"\"\"Sent by group server to inform a user's server that they have been",
            "        invited.",
            "        \"\"\"",
            "",
            "        path = _create_v1_path(\"/groups/local/%s/users/%s/invite\", group_id, user_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination, path=path, data=content, ignore_backoff=True",
            "        )",
            "",
            "    @log_function",
            "    def remove_user_from_group(",
            "        self, destination, group_id, requester_user_id, user_id, content",
            "    ):",
            "        \"\"\"Remove a user from a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/users/%s/remove\", group_id, user_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def remove_user_from_group_notification(",
            "        self, destination, group_id, user_id, content",
            "    ):",
            "        \"\"\"Sent by group server to inform a user's server that they have been",
            "        kicked from the group.",
            "        \"\"\"",
            "",
            "        path = _create_v1_path(\"/groups/local/%s/users/%s/remove\", group_id, user_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination, path=path, data=content, ignore_backoff=True",
            "        )",
            "",
            "    @log_function",
            "    def renew_group_attestation(self, destination, group_id, user_id, content):",
            "        \"\"\"Sent by either a group server or a user's server to periodically update",
            "        the attestations",
            "        \"\"\"",
            "",
            "        path = _create_v1_path(\"/groups/%s/renew_attestation/%s\", group_id, user_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination, path=path, data=content, ignore_backoff=True",
            "        )",
            "",
            "    @log_function",
            "    def update_group_summary_room(",
            "        self, destination, group_id, user_id, room_id, category_id, content",
            "    ):",
            "        \"\"\"Update a room entry in a group summary",
            "        \"\"\"",
            "        if category_id:",
            "            path = _create_v1_path(",
            "                \"/groups/%s/summary/categories/%s/rooms/%s\",",
            "                group_id,",
            "                category_id,",
            "                room_id,",
            "            )",
            "        else:",
            "            path = _create_v1_path(\"/groups/%s/summary/rooms/%s\", group_id, room_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def delete_group_summary_room(",
            "        self, destination, group_id, user_id, room_id, category_id",
            "    ):",
            "        \"\"\"Delete a room entry in a group summary",
            "        \"\"\"",
            "        if category_id:",
            "            path = _create_v1_path(",
            "                \"/groups/%s/summary/categories/%s/rooms/%s\",",
            "                group_id,",
            "                category_id,",
            "                room_id,",
            "            )",
            "        else:",
            "            path = _create_v1_path(\"/groups/%s/summary/rooms/%s\", group_id, room_id)",
            "",
            "        return self.client.delete_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_group_categories(self, destination, group_id, requester_user_id):",
            "        \"\"\"Get all categories in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/categories\", group_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_group_category(self, destination, group_id, requester_user_id, category_id):",
            "        \"\"\"Get category info in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def update_group_category(",
            "        self, destination, group_id, requester_user_id, category_id, content",
            "    ):",
            "        \"\"\"Update a category in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def delete_group_category(",
            "        self, destination, group_id, requester_user_id, category_id",
            "    ):",
            "        \"\"\"Delete a category in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)",
            "",
            "        return self.client.delete_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_group_roles(self, destination, group_id, requester_user_id):",
            "        \"\"\"Get all roles in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/roles\", group_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_group_role(self, destination, group_id, requester_user_id, role_id):",
            "        \"\"\"Get a roles info",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def update_group_role(",
            "        self, destination, group_id, requester_user_id, role_id, content",
            "    ):",
            "        \"\"\"Update a role in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def delete_group_role(self, destination, group_id, requester_user_id, role_id):",
            "        \"\"\"Delete a role in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)",
            "",
            "        return self.client.delete_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def update_group_summary_user(",
            "        self, destination, group_id, requester_user_id, user_id, role_id, content",
            "    ):",
            "        \"\"\"Update a users entry in a group",
            "        \"\"\"",
            "        if role_id:",
            "            path = _create_v1_path(",
            "                \"/groups/%s/summary/roles/%s/users/%s\", group_id, role_id, user_id",
            "            )",
            "        else:",
            "            path = _create_v1_path(\"/groups/%s/summary/users/%s\", group_id, user_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def set_group_join_policy(self, destination, group_id, requester_user_id, content):",
            "        \"\"\"Sets the join policy for a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/settings/m.join_policy\", group_id)",
            "",
            "        return self.client.put_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def delete_group_summary_user(",
            "        self, destination, group_id, requester_user_id, user_id, role_id",
            "    ):",
            "        \"\"\"Delete a users entry in a group",
            "        \"\"\"",
            "        if role_id:",
            "            path = _create_v1_path(",
            "                \"/groups/%s/summary/roles/%s/users/%s\", group_id, role_id, user_id",
            "            )",
            "        else:",
            "            path = _create_v1_path(\"/groups/%s/summary/users/%s\", group_id, user_id)",
            "",
            "        return self.client.delete_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    def bulk_get_publicised_groups(self, destination, user_ids):",
            "        \"\"\"Get the groups a list of users are publicising",
            "        \"\"\"",
            "",
            "        path = _create_v1_path(\"/get_groups_publicised\")",
            "",
            "        content = {\"user_ids\": user_ids}",
            "",
            "        return self.client.post_json(",
            "            destination=destination, path=path, data=content, ignore_backoff=True",
            "        )",
            "",
            "    def get_room_complexity(self, destination, room_id):",
            "        \"\"\"",
            "        Args:",
            "            destination (str): The remote server",
            "            room_id (str): The room ID to ask about.",
            "        \"\"\"",
            "        path = _create_path(FEDERATION_UNSTABLE_PREFIX, \"/rooms/%s/complexity\", room_id)",
            "",
            "        return self.client.get_json(destination=destination, path=path)",
            "",
            "",
            "def _create_path(federation_prefix, path, *args):",
            "    \"\"\"",
            "    Ensures that all args are url encoded.",
            "    \"\"\"",
            "    return federation_prefix + path % tuple(urllib.parse.quote(arg, \"\") for arg in args)",
            "",
            "",
            "def _create_v1_path(path, *args):",
            "    \"\"\"Creates a path against V1 federation API from the path template and",
            "    args. Ensures that all args are url encoded.",
            "",
            "    Example:",
            "",
            "        _create_v1_path(\"/event/%s\", event_id)",
            "",
            "    Args:",
            "        path (str): String template for the path",
            "        args: ([str]): Args to insert into path. Each arg will be url encoded",
            "",
            "    Returns:",
            "        str",
            "    \"\"\"",
            "    return _create_path(FEDERATION_V1_PREFIX, path, *args)",
            "",
            "",
            "def _create_v2_path(path, *args):",
            "    \"\"\"Creates a path against V2 federation API from the path template and",
            "    args. Ensures that all args are url encoded.",
            "",
            "    Example:",
            "",
            "        _create_v2_path(\"/event/%s\", event_id)",
            "",
            "    Args:",
            "        path (str): String template for the path",
            "        args: ([str]): Args to insert into path. Each arg will be url encoded",
            "",
            "    Returns:",
            "        str",
            "    \"\"\"",
            "    return _create_path(FEDERATION_V2_PREFIX, path, *args)"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import logging",
            "import urllib",
            "from typing import Any, Dict, Optional",
            "",
            "from synapse.api.constants import Membership",
            "from synapse.api.errors import Codes, HttpResponseException, SynapseError",
            "from synapse.api.urls import (",
            "    FEDERATION_UNSTABLE_PREFIX,",
            "    FEDERATION_V1_PREFIX,",
            "    FEDERATION_V2_PREFIX,",
            ")",
            "from synapse.logging.utils import log_function",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class TransportLayerClient:",
            "    \"\"\"Sends federation HTTP requests to other servers\"\"\"",
            "",
            "    def __init__(self, hs):",
            "        self.server_name = hs.hostname",
            "        self.client = hs.get_federation_http_client()",
            "",
            "    @log_function",
            "    def get_room_state_ids(self, destination, room_id, event_id):",
            "        \"\"\" Requests all state for a given room from the given server at the",
            "        given event. Returns the state's event_id's",
            "",
            "        Args:",
            "            destination (str): The host name of the remote homeserver we want",
            "                to get the state from.",
            "            context (str): The name of the context we want the state of",
            "            event_id (str): The event we want the context at.",
            "",
            "        Returns:",
            "            Awaitable: Results in a dict received from the remote homeserver.",
            "        \"\"\"",
            "        logger.debug(\"get_room_state_ids dest=%s, room=%s\", destination, room_id)",
            "",
            "        path = _create_v1_path(\"/state_ids/%s\", room_id)",
            "        return self.client.get_json(",
            "            destination,",
            "            path=path,",
            "            args={\"event_id\": event_id},",
            "            try_trailing_slash_on_400=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_event(self, destination, event_id, timeout=None):",
            "        \"\"\" Requests the pdu with give id and origin from the given server.",
            "",
            "        Args:",
            "            destination (str): The host name of the remote homeserver we want",
            "                to get the state from.",
            "            event_id (str): The id of the event being requested.",
            "            timeout (int): How long to try (in ms) the destination for before",
            "                giving up. None indicates no timeout.",
            "",
            "        Returns:",
            "            Awaitable: Results in a dict received from the remote homeserver.",
            "        \"\"\"",
            "        logger.debug(\"get_pdu dest=%s, event_id=%s\", destination, event_id)",
            "",
            "        path = _create_v1_path(\"/event/%s\", event_id)",
            "        return self.client.get_json(",
            "            destination, path=path, timeout=timeout, try_trailing_slash_on_400=True",
            "        )",
            "",
            "    @log_function",
            "    def backfill(self, destination, room_id, event_tuples, limit):",
            "        \"\"\" Requests `limit` previous PDUs in a given context before list of",
            "        PDUs.",
            "",
            "        Args:",
            "            dest (str)",
            "            room_id (str)",
            "            event_tuples (list)",
            "            limit (int)",
            "",
            "        Returns:",
            "            Awaitable: Results in a dict received from the remote homeserver.",
            "        \"\"\"",
            "        logger.debug(",
            "            \"backfill dest=%s, room_id=%s, event_tuples=%r, limit=%s\",",
            "            destination,",
            "            room_id,",
            "            event_tuples,",
            "            str(limit),",
            "        )",
            "",
            "        if not event_tuples:",
            "            # TODO: raise?",
            "            return",
            "",
            "        path = _create_v1_path(\"/backfill/%s\", room_id)",
            "",
            "        args = {\"v\": event_tuples, \"limit\": [str(limit)]}",
            "",
            "        return self.client.get_json(",
            "            destination, path=path, args=args, try_trailing_slash_on_400=True",
            "        )",
            "",
            "    @log_function",
            "    async def send_transaction(self, transaction, json_data_callback=None):",
            "        \"\"\" Sends the given Transaction to its destination",
            "",
            "        Args:",
            "            transaction (Transaction)",
            "",
            "        Returns:",
            "            Succeeds when we get a 2xx HTTP response. The result",
            "            will be the decoded JSON body.",
            "",
            "            Fails with ``HTTPRequestException`` if we get an HTTP response",
            "            code >= 300.",
            "",
            "            Fails with ``NotRetryingDestination`` if we are not yet ready",
            "            to retry this server.",
            "",
            "            Fails with ``FederationDeniedError`` if this destination",
            "            is not on our federation whitelist",
            "        \"\"\"",
            "        logger.debug(",
            "            \"send_data dest=%s, txid=%s\",",
            "            transaction.destination,",
            "            transaction.transaction_id,",
            "        )",
            "",
            "        if transaction.destination == self.server_name:",
            "            raise RuntimeError(\"Transport layer cannot send to itself!\")",
            "",
            "        # FIXME: This is only used by the tests. The actual json sent is",
            "        # generated by the json_data_callback.",
            "        json_data = transaction.get_dict()",
            "",
            "        path = _create_v1_path(\"/send/%s\", transaction.transaction_id)",
            "",
            "        response = await self.client.put_json(",
            "            transaction.destination,",
            "            path=path,",
            "            data=json_data,",
            "            json_data_callback=json_data_callback,",
            "            long_retries=True,",
            "            backoff_on_404=True,  # If we get a 404 the other side has gone",
            "            try_trailing_slash_on_400=True,",
            "        )",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def make_query(",
            "        self, destination, query_type, args, retry_on_dns_fail, ignore_backoff=False",
            "    ):",
            "        path = _create_v1_path(\"/query/%s\", query_type)",
            "",
            "        content = await self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args=args,",
            "            retry_on_dns_fail=retry_on_dns_fail,",
            "            timeout=10000,",
            "            ignore_backoff=ignore_backoff,",
            "        )",
            "",
            "        return content",
            "",
            "    @log_function",
            "    async def make_membership_event(",
            "        self, destination, room_id, user_id, membership, params",
            "    ):",
            "        \"\"\"Asks a remote server to build and sign us a membership event",
            "",
            "        Note that this does not append any events to any graphs.",
            "",
            "        Args:",
            "            destination (str): address of remote homeserver",
            "            room_id (str): room to join/leave",
            "            user_id (str): user to be joined/left",
            "            membership (str): one of join/leave",
            "            params (dict[str, str|Iterable[str]]): Query parameters to include in the",
            "                request.",
            "",
            "        Returns:",
            "            Succeeds when we get a 2xx HTTP response. The result",
            "            will be the decoded JSON body (ie, the new event).",
            "",
            "            Fails with ``HTTPRequestException`` if we get an HTTP response",
            "            code >= 300.",
            "",
            "            Fails with ``NotRetryingDestination`` if we are not yet ready",
            "            to retry this server.",
            "",
            "            Fails with ``FederationDeniedError`` if the remote destination",
            "            is not in our federation whitelist",
            "        \"\"\"",
            "        valid_memberships = {Membership.JOIN, Membership.LEAVE}",
            "        if membership not in valid_memberships:",
            "            raise RuntimeError(",
            "                \"make_membership_event called with membership='%s', must be one of %s\"",
            "                % (membership, \",\".join(valid_memberships))",
            "            )",
            "        path = _create_v1_path(\"/make_%s/%s/%s\", membership, room_id, user_id)",
            "",
            "        ignore_backoff = False",
            "        retry_on_dns_fail = False",
            "",
            "        if membership == Membership.LEAVE:",
            "            # we particularly want to do our best to send leave events. The",
            "            # problem is that if it fails, we won't retry it later, so if the",
            "            # remote server was just having a momentary blip, the room will be",
            "            # out of sync.",
            "            ignore_backoff = True",
            "            retry_on_dns_fail = True",
            "",
            "        content = await self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args=params,",
            "            retry_on_dns_fail=retry_on_dns_fail,",
            "            timeout=20000,",
            "            ignore_backoff=ignore_backoff,",
            "        )",
            "",
            "        return content",
            "",
            "    @log_function",
            "    async def send_join_v1(self, destination, room_id, event_id, content):",
            "        path = _create_v1_path(\"/send_join/%s/%s\", room_id, event_id)",
            "",
            "        response = await self.client.put_json(",
            "            destination=destination, path=path, data=content",
            "        )",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def send_join_v2(self, destination, room_id, event_id, content):",
            "        path = _create_v2_path(\"/send_join/%s/%s\", room_id, event_id)",
            "",
            "        response = await self.client.put_json(",
            "            destination=destination, path=path, data=content",
            "        )",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def send_leave_v1(self, destination, room_id, event_id, content):",
            "        path = _create_v1_path(\"/send_leave/%s/%s\", room_id, event_id)",
            "",
            "        response = await self.client.put_json(",
            "            destination=destination,",
            "            path=path,",
            "            data=content,",
            "            # we want to do our best to send this through. The problem is",
            "            # that if it fails, we won't retry it later, so if the remote",
            "            # server was just having a momentary blip, the room will be out of",
            "            # sync.",
            "            ignore_backoff=True,",
            "        )",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def send_leave_v2(self, destination, room_id, event_id, content):",
            "        path = _create_v2_path(\"/send_leave/%s/%s\", room_id, event_id)",
            "",
            "        response = await self.client.put_json(",
            "            destination=destination,",
            "            path=path,",
            "            data=content,",
            "            # we want to do our best to send this through. The problem is",
            "            # that if it fails, we won't retry it later, so if the remote",
            "            # server was just having a momentary blip, the room will be out of",
            "            # sync.",
            "            ignore_backoff=True,",
            "        )",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def send_invite_v1(self, destination, room_id, event_id, content):",
            "        path = _create_v1_path(\"/invite/%s/%s\", room_id, event_id)",
            "",
            "        response = await self.client.put_json(",
            "            destination=destination, path=path, data=content, ignore_backoff=True",
            "        )",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def send_invite_v2(self, destination, room_id, event_id, content):",
            "        path = _create_v2_path(\"/invite/%s/%s\", room_id, event_id)",
            "",
            "        response = await self.client.put_json(",
            "            destination=destination, path=path, data=content, ignore_backoff=True",
            "        )",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def get_public_rooms(",
            "        self,",
            "        remote_server: str,",
            "        limit: Optional[int] = None,",
            "        since_token: Optional[str] = None,",
            "        search_filter: Optional[Dict] = None,",
            "        include_all_networks: bool = False,",
            "        third_party_instance_id: Optional[str] = None,",
            "    ):",
            "        \"\"\"Get the list of public rooms from a remote homeserver",
            "",
            "        See synapse.federation.federation_client.FederationClient.get_public_rooms for",
            "        more information.",
            "        \"\"\"",
            "        if search_filter:",
            "            # this uses MSC2197 (Search Filtering over Federation)",
            "            path = _create_v1_path(\"/publicRooms\")",
            "",
            "            data = {",
            "                \"include_all_networks\": \"true\" if include_all_networks else \"false\"",
            "            }  # type: Dict[str, Any]",
            "            if third_party_instance_id:",
            "                data[\"third_party_instance_id\"] = third_party_instance_id",
            "            if limit:",
            "                data[\"limit\"] = str(limit)",
            "            if since_token:",
            "                data[\"since\"] = since_token",
            "",
            "            data[\"filter\"] = search_filter",
            "",
            "            try:",
            "                response = await self.client.post_json(",
            "                    destination=remote_server, path=path, data=data, ignore_backoff=True",
            "                )",
            "            except HttpResponseException as e:",
            "                if e.code == 403:",
            "                    raise SynapseError(",
            "                        403,",
            "                        \"You are not allowed to view the public rooms list of %s\"",
            "                        % (remote_server,),",
            "                        errcode=Codes.FORBIDDEN,",
            "                    )",
            "                raise",
            "        else:",
            "            path = _create_v1_path(\"/publicRooms\")",
            "",
            "            args = {",
            "                \"include_all_networks\": \"true\" if include_all_networks else \"false\"",
            "            }  # type: Dict[str, Any]",
            "            if third_party_instance_id:",
            "                args[\"third_party_instance_id\"] = (third_party_instance_id,)",
            "            if limit:",
            "                args[\"limit\"] = [str(limit)]",
            "            if since_token:",
            "                args[\"since\"] = [since_token]",
            "",
            "            try:",
            "                response = await self.client.get_json(",
            "                    destination=remote_server, path=path, args=args, ignore_backoff=True",
            "                )",
            "            except HttpResponseException as e:",
            "                if e.code == 403:",
            "                    raise SynapseError(",
            "                        403,",
            "                        \"You are not allowed to view the public rooms list of %s\"",
            "                        % (remote_server,),",
            "                        errcode=Codes.FORBIDDEN,",
            "                    )",
            "                raise",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def exchange_third_party_invite(self, destination, room_id, event_dict):",
            "        path = _create_v1_path(\"/exchange_third_party_invite/%s\", room_id)",
            "",
            "        response = await self.client.put_json(",
            "            destination=destination, path=path, data=event_dict",
            "        )",
            "",
            "        return response",
            "",
            "    @log_function",
            "    async def get_event_auth(self, destination, room_id, event_id):",
            "        path = _create_v1_path(\"/event_auth/%s/%s\", room_id, event_id)",
            "",
            "        content = await self.client.get_json(destination=destination, path=path)",
            "",
            "        return content",
            "",
            "    @log_function",
            "    async def query_client_keys(self, destination, query_content, timeout):",
            "        \"\"\"Query the device keys for a list of user ids hosted on a remote",
            "        server.",
            "",
            "        Request:",
            "            {",
            "              \"device_keys\": {",
            "                \"<user_id>\": [\"<device_id>\"]",
            "              }",
            "            }",
            "",
            "        Response:",
            "            {",
            "              \"device_keys\": {",
            "                \"<user_id>\": {",
            "                  \"<device_id>\": {...}",
            "                }",
            "              },",
            "              \"master_key\": {",
            "                \"<user_id>\": {...}",
            "                }",
            "              },",
            "              \"self_signing_key\": {",
            "                \"<user_id>\": {...}",
            "              }",
            "            }",
            "",
            "        Args:",
            "            destination(str): The server to query.",
            "            query_content(dict): The user ids to query.",
            "        Returns:",
            "            A dict containing device and cross-signing keys.",
            "        \"\"\"",
            "        path = _create_v1_path(\"/user/keys/query\")",
            "",
            "        content = await self.client.post_json(",
            "            destination=destination, path=path, data=query_content, timeout=timeout",
            "        )",
            "        return content",
            "",
            "    @log_function",
            "    async def query_user_devices(self, destination, user_id, timeout):",
            "        \"\"\"Query the devices for a user id hosted on a remote server.",
            "",
            "        Response:",
            "            {",
            "              \"stream_id\": \"...\",",
            "              \"devices\": [ { ... } ],",
            "              \"master_key\": {",
            "                \"user_id\": \"<user_id>\",",
            "                \"usage\": [...],",
            "                \"keys\": {...},",
            "                \"signatures\": {",
            "                  \"<user_id>\": {...}",
            "                }",
            "              },",
            "              \"self_signing_key\": {",
            "                \"user_id\": \"<user_id>\",",
            "                \"usage\": [...],",
            "                \"keys\": {...},",
            "                \"signatures\": {",
            "                  \"<user_id>\": {...}",
            "                }",
            "              }",
            "            }",
            "",
            "        Args:",
            "            destination(str): The server to query.",
            "            query_content(dict): The user ids to query.",
            "        Returns:",
            "            A dict containing device and cross-signing keys.",
            "        \"\"\"",
            "        path = _create_v1_path(\"/user/devices/%s\", user_id)",
            "",
            "        content = await self.client.get_json(",
            "            destination=destination, path=path, timeout=timeout",
            "        )",
            "        return content",
            "",
            "    @log_function",
            "    async def claim_client_keys(self, destination, query_content, timeout):",
            "        \"\"\"Claim one-time keys for a list of devices hosted on a remote server.",
            "",
            "        Request:",
            "            {",
            "              \"one_time_keys\": {",
            "                \"<user_id>\": {",
            "                  \"<device_id>\": \"<algorithm>\"",
            "                }",
            "              }",
            "            }",
            "",
            "        Response:",
            "            {",
            "              \"device_keys\": {",
            "                \"<user_id>\": {",
            "                  \"<device_id>\": {",
            "                    \"<algorithm>:<key_id>\": \"<key_base64>\"",
            "                  }",
            "                }",
            "              }",
            "            }",
            "",
            "        Args:",
            "            destination(str): The server to query.",
            "            query_content(dict): The user ids to query.",
            "        Returns:",
            "            A dict containing the one-time keys.",
            "        \"\"\"",
            "",
            "        path = _create_v1_path(\"/user/keys/claim\")",
            "",
            "        content = await self.client.post_json(",
            "            destination=destination, path=path, data=query_content, timeout=timeout",
            "        )",
            "        return content",
            "",
            "    @log_function",
            "    async def get_missing_events(",
            "        self,",
            "        destination,",
            "        room_id,",
            "        earliest_events,",
            "        latest_events,",
            "        limit,",
            "        min_depth,",
            "        timeout,",
            "    ):",
            "        path = _create_v1_path(\"/get_missing_events/%s\", room_id)",
            "",
            "        content = await self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            data={",
            "                \"limit\": int(limit),",
            "                \"min_depth\": int(min_depth),",
            "                \"earliest_events\": earliest_events,",
            "                \"latest_events\": latest_events,",
            "            },",
            "            timeout=timeout,",
            "        )",
            "",
            "        return content",
            "",
            "    @log_function",
            "    def get_group_profile(self, destination, group_id, requester_user_id):",
            "        \"\"\"Get a group profile",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/profile\", group_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def update_group_profile(self, destination, group_id, requester_user_id, content):",
            "        \"\"\"Update a remote group profile",
            "",
            "        Args:",
            "            destination (str)",
            "            group_id (str)",
            "            requester_user_id (str)",
            "            content (dict): The new profile of the group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/profile\", group_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_group_summary(self, destination, group_id, requester_user_id):",
            "        \"\"\"Get a group summary",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/summary\", group_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_rooms_in_group(self, destination, group_id, requester_user_id):",
            "        \"\"\"Get all rooms in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/rooms\", group_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    def add_room_to_group(",
            "        self, destination, group_id, requester_user_id, room_id, content",
            "    ):",
            "        \"\"\"Add a room to a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/room/%s\", group_id, room_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    def update_room_in_group(",
            "        self, destination, group_id, requester_user_id, room_id, config_key, content",
            "    ):",
            "        \"\"\"Update room in group",
            "        \"\"\"",
            "        path = _create_v1_path(",
            "            \"/groups/%s/room/%s/config/%s\", group_id, room_id, config_key",
            "        )",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    def remove_room_from_group(self, destination, group_id, requester_user_id, room_id):",
            "        \"\"\"Remove a room from a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/room/%s\", group_id, room_id)",
            "",
            "        return self.client.delete_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_users_in_group(self, destination, group_id, requester_user_id):",
            "        \"\"\"Get users in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/users\", group_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_invited_users_in_group(self, destination, group_id, requester_user_id):",
            "        \"\"\"Get users that have been invited to a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/invited_users\", group_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def accept_group_invite(self, destination, group_id, user_id, content):",
            "        \"\"\"Accept a group invite",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/users/%s/accept_invite\", group_id, user_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination, path=path, data=content, ignore_backoff=True",
            "        )",
            "",
            "    @log_function",
            "    def join_group(self, destination, group_id, user_id, content):",
            "        \"\"\"Attempts to join a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/users/%s/join\", group_id, user_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination, path=path, data=content, ignore_backoff=True",
            "        )",
            "",
            "    @log_function",
            "    def invite_to_group(",
            "        self, destination, group_id, user_id, requester_user_id, content",
            "    ):",
            "        \"\"\"Invite a user to a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/users/%s/invite\", group_id, user_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def invite_to_group_notification(self, destination, group_id, user_id, content):",
            "        \"\"\"Sent by group server to inform a user's server that they have been",
            "        invited.",
            "        \"\"\"",
            "",
            "        path = _create_v1_path(\"/groups/local/%s/users/%s/invite\", group_id, user_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination, path=path, data=content, ignore_backoff=True",
            "        )",
            "",
            "    @log_function",
            "    def remove_user_from_group(",
            "        self, destination, group_id, requester_user_id, user_id, content",
            "    ):",
            "        \"\"\"Remove a user from a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/users/%s/remove\", group_id, user_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def remove_user_from_group_notification(",
            "        self, destination, group_id, user_id, content",
            "    ):",
            "        \"\"\"Sent by group server to inform a user's server that they have been",
            "        kicked from the group.",
            "        \"\"\"",
            "",
            "        path = _create_v1_path(\"/groups/local/%s/users/%s/remove\", group_id, user_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination, path=path, data=content, ignore_backoff=True",
            "        )",
            "",
            "    @log_function",
            "    def renew_group_attestation(self, destination, group_id, user_id, content):",
            "        \"\"\"Sent by either a group server or a user's server to periodically update",
            "        the attestations",
            "        \"\"\"",
            "",
            "        path = _create_v1_path(\"/groups/%s/renew_attestation/%s\", group_id, user_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination, path=path, data=content, ignore_backoff=True",
            "        )",
            "",
            "    @log_function",
            "    def update_group_summary_room(",
            "        self, destination, group_id, user_id, room_id, category_id, content",
            "    ):",
            "        \"\"\"Update a room entry in a group summary",
            "        \"\"\"",
            "        if category_id:",
            "            path = _create_v1_path(",
            "                \"/groups/%s/summary/categories/%s/rooms/%s\",",
            "                group_id,",
            "                category_id,",
            "                room_id,",
            "            )",
            "        else:",
            "            path = _create_v1_path(\"/groups/%s/summary/rooms/%s\", group_id, room_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def delete_group_summary_room(",
            "        self, destination, group_id, user_id, room_id, category_id",
            "    ):",
            "        \"\"\"Delete a room entry in a group summary",
            "        \"\"\"",
            "        if category_id:",
            "            path = _create_v1_path(",
            "                \"/groups/%s/summary/categories/%s/rooms/%s\",",
            "                group_id,",
            "                category_id,",
            "                room_id,",
            "            )",
            "        else:",
            "            path = _create_v1_path(\"/groups/%s/summary/rooms/%s\", group_id, room_id)",
            "",
            "        return self.client.delete_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_group_categories(self, destination, group_id, requester_user_id):",
            "        \"\"\"Get all categories in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/categories\", group_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_group_category(self, destination, group_id, requester_user_id, category_id):",
            "        \"\"\"Get category info in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def update_group_category(",
            "        self, destination, group_id, requester_user_id, category_id, content",
            "    ):",
            "        \"\"\"Update a category in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def delete_group_category(",
            "        self, destination, group_id, requester_user_id, category_id",
            "    ):",
            "        \"\"\"Delete a category in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)",
            "",
            "        return self.client.delete_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_group_roles(self, destination, group_id, requester_user_id):",
            "        \"\"\"Get all roles in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/roles\", group_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def get_group_role(self, destination, group_id, requester_user_id, role_id):",
            "        \"\"\"Get a roles info",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)",
            "",
            "        return self.client.get_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def update_group_role(",
            "        self, destination, group_id, requester_user_id, role_id, content",
            "    ):",
            "        \"\"\"Update a role in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def delete_group_role(self, destination, group_id, requester_user_id, role_id):",
            "        \"\"\"Delete a role in a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)",
            "",
            "        return self.client.delete_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def update_group_summary_user(",
            "        self, destination, group_id, requester_user_id, user_id, role_id, content",
            "    ):",
            "        \"\"\"Update a users entry in a group",
            "        \"\"\"",
            "        if role_id:",
            "            path = _create_v1_path(",
            "                \"/groups/%s/summary/roles/%s/users/%s\", group_id, role_id, user_id",
            "            )",
            "        else:",
            "            path = _create_v1_path(\"/groups/%s/summary/users/%s\", group_id, user_id)",
            "",
            "        return self.client.post_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def set_group_join_policy(self, destination, group_id, requester_user_id, content):",
            "        \"\"\"Sets the join policy for a group",
            "        \"\"\"",
            "        path = _create_v1_path(\"/groups/%s/settings/m.join_policy\", group_id)",
            "",
            "        return self.client.put_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            data=content,",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    @log_function",
            "    def delete_group_summary_user(",
            "        self, destination, group_id, requester_user_id, user_id, role_id",
            "    ):",
            "        \"\"\"Delete a users entry in a group",
            "        \"\"\"",
            "        if role_id:",
            "            path = _create_v1_path(",
            "                \"/groups/%s/summary/roles/%s/users/%s\", group_id, role_id, user_id",
            "            )",
            "        else:",
            "            path = _create_v1_path(\"/groups/%s/summary/users/%s\", group_id, user_id)",
            "",
            "        return self.client.delete_json(",
            "            destination=destination,",
            "            path=path,",
            "            args={\"requester_user_id\": requester_user_id},",
            "            ignore_backoff=True,",
            "        )",
            "",
            "    def bulk_get_publicised_groups(self, destination, user_ids):",
            "        \"\"\"Get the groups a list of users are publicising",
            "        \"\"\"",
            "",
            "        path = _create_v1_path(\"/get_groups_publicised\")",
            "",
            "        content = {\"user_ids\": user_ids}",
            "",
            "        return self.client.post_json(",
            "            destination=destination, path=path, data=content, ignore_backoff=True",
            "        )",
            "",
            "    def get_room_complexity(self, destination, room_id):",
            "        \"\"\"",
            "        Args:",
            "            destination (str): The remote server",
            "            room_id (str): The room ID to ask about.",
            "        \"\"\"",
            "        path = _create_path(FEDERATION_UNSTABLE_PREFIX, \"/rooms/%s/complexity\", room_id)",
            "",
            "        return self.client.get_json(destination=destination, path=path)",
            "",
            "",
            "def _create_path(federation_prefix, path, *args):",
            "    \"\"\"",
            "    Ensures that all args are url encoded.",
            "    \"\"\"",
            "    return federation_prefix + path % tuple(urllib.parse.quote(arg, \"\") for arg in args)",
            "",
            "",
            "def _create_v1_path(path, *args):",
            "    \"\"\"Creates a path against V1 federation API from the path template and",
            "    args. Ensures that all args are url encoded.",
            "",
            "    Example:",
            "",
            "        _create_v1_path(\"/event/%s\", event_id)",
            "",
            "    Args:",
            "        path (str): String template for the path",
            "        args: ([str]): Args to insert into path. Each arg will be url encoded",
            "",
            "    Returns:",
            "        str",
            "    \"\"\"",
            "    return _create_path(FEDERATION_V1_PREFIX, path, *args)",
            "",
            "",
            "def _create_v2_path(path, *args):",
            "    \"\"\"Creates a path against V2 federation API from the path template and",
            "    args. Ensures that all args are url encoded.",
            "",
            "    Example:",
            "",
            "        _create_v2_path(\"/event/%s\", event_id)",
            "",
            "    Args:",
            "        path (str): String template for the path",
            "        args: ([str]): Args to insert into path. Each arg will be url encoded",
            "",
            "    Returns:",
            "        str",
            "    \"\"\"",
            "    return _create_path(FEDERATION_V2_PREFIX, path, *args)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "38": [
                "TransportLayerClient",
                "__init__"
            ]
        },
        "addLocation": []
    },
    "synapse/handlers/federation.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 140,
                "afterPatchRowNumber": 140,
                "PatchRowcode": "         self._message_handler = hs.get_message_handler()"
            },
            "1": {
                "beforePatchRowNumber": 141,
                "afterPatchRowNumber": 141,
                "PatchRowcode": "         self._server_notices_mxid = hs.config.server_notices_mxid"
            },
            "2": {
                "beforePatchRowNumber": 142,
                "afterPatchRowNumber": 142,
                "PatchRowcode": "         self.config = hs.config"
            },
            "3": {
                "beforePatchRowNumber": 143,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.http_client = hs.get_simple_http_client()"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 143,
                "PatchRowcode": "+        self.http_client = hs.get_proxied_blacklisted_http_client()"
            },
            "5": {
                "beforePatchRowNumber": 144,
                "afterPatchRowNumber": 144,
                "PatchRowcode": "         self._instance_name = hs.get_instance_name()"
            },
            "6": {
                "beforePatchRowNumber": 145,
                "afterPatchRowNumber": 145,
                "PatchRowcode": "         self._replication = hs.get_replication_data_handler()"
            },
            "7": {
                "beforePatchRowNumber": 146,
                "afterPatchRowNumber": 146,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2017-2018 New Vector Ltd",
            "# Copyright 2019 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "\"\"\"Contains handlers for federation events.\"\"\"",
            "",
            "import itertools",
            "import logging",
            "from collections.abc import Container",
            "from http import HTTPStatus",
            "from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union",
            "",
            "import attr",
            "from signedjson.key import decode_verify_key_bytes",
            "from signedjson.sign import verify_signed_json",
            "from unpaddedbase64 import decode_base64",
            "",
            "from twisted.internet import defer",
            "",
            "from synapse import event_auth",
            "from synapse.api.constants import (",
            "    EventTypes,",
            "    Membership,",
            "    RejectedReason,",
            "    RoomEncryptionAlgorithms,",
            ")",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    CodeMessageException,",
            "    Codes,",
            "    FederationDeniedError,",
            "    FederationError,",
            "    HttpResponseException,",
            "    NotFoundError,",
            "    RequestSendFailed,",
            "    SynapseError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion, RoomVersions",
            "from synapse.crypto.event_signing import compute_event_signature",
            "from synapse.event_auth import auth_types_for_event",
            "from synapse.events import EventBase",
            "from synapse.events.snapshot import EventContext",
            "from synapse.events.validator import EventValidator",
            "from synapse.handlers._base import BaseHandler",
            "from synapse.http.servlet import assert_params_in_dict",
            "from synapse.logging.context import (",
            "    make_deferred_yieldable,",
            "    nested_logging_context,",
            "    preserve_fn,",
            "    run_in_background,",
            ")",
            "from synapse.logging.utils import log_function",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.replication.http.devices import ReplicationUserDevicesResyncRestServlet",
            "from synapse.replication.http.federation import (",
            "    ReplicationCleanRoomRestServlet,",
            "    ReplicationFederationSendEventsRestServlet,",
            "    ReplicationStoreRoomOnOutlierMembershipRestServlet,",
            ")",
            "from synapse.state import StateResolutionStore",
            "from synapse.storage.databases.main.events_worker import EventRedactBehaviour",
            "from synapse.types import (",
            "    JsonDict,",
            "    MutableStateMap,",
            "    PersistedEventPosition,",
            "    RoomStreamToken,",
            "    StateMap,",
            "    UserID,",
            "    get_domain_from_id,",
            ")",
            "from synapse.util.async_helpers import Linearizer, concurrently_execute",
            "from synapse.util.retryutils import NotRetryingDestination",
            "from synapse.util.stringutils import shortstr",
            "from synapse.visibility import filter_events_for_server",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "@attr.s(slots=True)",
            "class _NewEventInfo:",
            "    \"\"\"Holds information about a received event, ready for passing to _handle_new_events",
            "",
            "    Attributes:",
            "        event: the received event",
            "",
            "        state: the state at that event",
            "",
            "        auth_events: the auth_event map for that event",
            "    \"\"\"",
            "",
            "    event = attr.ib(type=EventBase)",
            "    state = attr.ib(type=Optional[Sequence[EventBase]], default=None)",
            "    auth_events = attr.ib(type=Optional[MutableStateMap[EventBase]], default=None)",
            "",
            "",
            "class FederationHandler(BaseHandler):",
            "    \"\"\"Handles events that originated from federation.",
            "        Responsible for:",
            "        a) handling received Pdus before handing them on as Events to the rest",
            "        of the homeserver (including auth and state conflict resolutions)",
            "        b) converting events that were produced by local clients that may need",
            "        to be sent to remote homeservers.",
            "        c) doing the necessary dances to invite remote users and join remote",
            "        rooms.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__(hs)",
            "",
            "        self.hs = hs",
            "",
            "        self.store = hs.get_datastore()",
            "        self.storage = hs.get_storage()",
            "        self.state_store = self.storage.state",
            "        self.federation_client = hs.get_federation_client()",
            "        self.state_handler = hs.get_state_handler()",
            "        self._state_resolution_handler = hs.get_state_resolution_handler()",
            "        self.server_name = hs.hostname",
            "        self.keyring = hs.get_keyring()",
            "        self.action_generator = hs.get_action_generator()",
            "        self.is_mine_id = hs.is_mine_id",
            "        self.spam_checker = hs.get_spam_checker()",
            "        self.event_creation_handler = hs.get_event_creation_handler()",
            "        self._message_handler = hs.get_message_handler()",
            "        self._server_notices_mxid = hs.config.server_notices_mxid",
            "        self.config = hs.config",
            "        self.http_client = hs.get_simple_http_client()",
            "        self._instance_name = hs.get_instance_name()",
            "        self._replication = hs.get_replication_data_handler()",
            "",
            "        self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)",
            "        self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(",
            "            hs",
            "        )",
            "",
            "        if hs.config.worker_app:",
            "            self._user_device_resync = ReplicationUserDevicesResyncRestServlet.make_client(",
            "                hs",
            "            )",
            "            self._maybe_store_room_on_outlier_membership = ReplicationStoreRoomOnOutlierMembershipRestServlet.make_client(",
            "                hs",
            "            )",
            "        else:",
            "            self._device_list_updater = hs.get_device_handler().device_list_updater",
            "            self._maybe_store_room_on_outlier_membership = (",
            "                self.store.maybe_store_room_on_outlier_membership",
            "            )",
            "",
            "        # When joining a room we need to queue any events for that room up.",
            "        # For each room, a list of (pdu, origin) tuples.",
            "        self.room_queues = {}  # type: Dict[str, List[Tuple[EventBase, str]]]",
            "        self._room_pdu_linearizer = Linearizer(\"fed_room_pdu\")",
            "",
            "        self.third_party_event_rules = hs.get_third_party_event_rules()",
            "",
            "        self._ephemeral_messages_enabled = hs.config.enable_ephemeral_messages",
            "",
            "    async def on_receive_pdu(self, origin, pdu, sent_to_us_directly=False) -> None:",
            "        \"\"\" Process a PDU received via a federation /send/ transaction, or",
            "        via backfill of missing prev_events",
            "",
            "        Args:",
            "            origin (str): server which initiated the /send/ transaction. Will",
            "                be used to fetch missing events or state.",
            "            pdu (FrozenEvent): received PDU",
            "            sent_to_us_directly (bool): True if this event was pushed to us; False if",
            "                we pulled it as the result of a missing prev_event.",
            "        \"\"\"",
            "",
            "        room_id = pdu.room_id",
            "        event_id = pdu.event_id",
            "",
            "        logger.info(\"handling received PDU: %s\", pdu)",
            "",
            "        # We reprocess pdus when we have seen them only as outliers",
            "        existing = await self.store.get_event(",
            "            event_id, allow_none=True, allow_rejected=True",
            "        )",
            "",
            "        # FIXME: Currently we fetch an event again when we already have it",
            "        # if it has been marked as an outlier.",
            "",
            "        already_seen = existing and (",
            "            not existing.internal_metadata.is_outlier()",
            "            or pdu.internal_metadata.is_outlier()",
            "        )",
            "        if already_seen:",
            "            logger.debug(\"[%s %s]: Already seen pdu\", room_id, event_id)",
            "            return",
            "",
            "        # do some initial sanity-checking of the event. In particular, make",
            "        # sure it doesn't have hundreds of prev_events or auth_events, which",
            "        # could cause a huge state resolution or cascade of event fetches.",
            "        try:",
            "            self._sanity_check_event(pdu)",
            "        except SynapseError as err:",
            "            logger.warning(",
            "                \"[%s %s] Received event failed sanity checks\", room_id, event_id",
            "            )",
            "            raise FederationError(\"ERROR\", err.code, err.msg, affected=pdu.event_id)",
            "",
            "        # If we are currently in the process of joining this room, then we",
            "        # queue up events for later processing.",
            "        if room_id in self.room_queues:",
            "            logger.info(",
            "                \"[%s %s] Queuing PDU from %s for now: join in progress\",",
            "                room_id,",
            "                event_id,",
            "                origin,",
            "            )",
            "            self.room_queues[room_id].append((pdu, origin))",
            "            return",
            "",
            "        # If we're not in the room just ditch the event entirely. This is",
            "        # probably an old server that has come back and thinks we're still in",
            "        # the room (or we've been rejoined to the room by a state reset).",
            "        #",
            "        # Note that if we were never in the room then we would have already",
            "        # dropped the event, since we wouldn't know the room version.",
            "        is_in_room = await self.auth.check_host_in_room(room_id, self.server_name)",
            "        if not is_in_room:",
            "            logger.info(",
            "                \"[%s %s] Ignoring PDU from %s as we're not in the room\",",
            "                room_id,",
            "                event_id,",
            "                origin,",
            "            )",
            "            return None",
            "",
            "        state = None",
            "",
            "        # Get missing pdus if necessary.",
            "        if not pdu.internal_metadata.is_outlier():",
            "            # We only backfill backwards to the min depth.",
            "            min_depth = await self.get_min_depth_for_context(pdu.room_id)",
            "",
            "            logger.debug(\"[%s %s] min_depth: %d\", room_id, event_id, min_depth)",
            "",
            "            prevs = set(pdu.prev_event_ids())",
            "            seen = await self.store.have_events_in_timeline(prevs)",
            "",
            "            if min_depth is not None and pdu.depth < min_depth:",
            "                # This is so that we don't notify the user about this",
            "                # message, to work around the fact that some events will",
            "                # reference really really old events we really don't want to",
            "                # send to the clients.",
            "                pdu.internal_metadata.outlier = True",
            "            elif min_depth is not None and pdu.depth > min_depth:",
            "                missing_prevs = prevs - seen",
            "                if sent_to_us_directly and missing_prevs:",
            "                    # If we're missing stuff, ensure we only fetch stuff one",
            "                    # at a time.",
            "                    logger.info(",
            "                        \"[%s %s] Acquiring room lock to fetch %d missing prev_events: %s\",",
            "                        room_id,",
            "                        event_id,",
            "                        len(missing_prevs),",
            "                        shortstr(missing_prevs),",
            "                    )",
            "                    with (await self._room_pdu_linearizer.queue(pdu.room_id)):",
            "                        logger.info(",
            "                            \"[%s %s] Acquired room lock to fetch %d missing prev_events\",",
            "                            room_id,",
            "                            event_id,",
            "                            len(missing_prevs),",
            "                        )",
            "",
            "                        try:",
            "                            await self._get_missing_events_for_pdu(",
            "                                origin, pdu, prevs, min_depth",
            "                            )",
            "                        except Exception as e:",
            "                            raise Exception(",
            "                                \"Error fetching missing prev_events for %s: %s\"",
            "                                % (event_id, e)",
            "                            ) from e",
            "",
            "                        # Update the set of things we've seen after trying to",
            "                        # fetch the missing stuff",
            "                        seen = await self.store.have_events_in_timeline(prevs)",
            "",
            "                        if not prevs - seen:",
            "                            logger.info(",
            "                                \"[%s %s] Found all missing prev_events\",",
            "                                room_id,",
            "                                event_id,",
            "                            )",
            "",
            "            if prevs - seen:",
            "                # We've still not been able to get all of the prev_events for this event.",
            "                #",
            "                # In this case, we need to fall back to asking another server in the",
            "                # federation for the state at this event. That's ok provided we then",
            "                # resolve the state against other bits of the DAG before using it (which",
            "                # will ensure that you can't just take over a room by sending an event,",
            "                # withholding its prev_events, and declaring yourself to be an admin in",
            "                # the subsequent state request).",
            "                #",
            "                # Now, if we're pulling this event as a missing prev_event, then clearly",
            "                # this event is not going to become the only forward-extremity and we are",
            "                # guaranteed to resolve its state against our existing forward",
            "                # extremities, so that should be fine.",
            "                #",
            "                # On the other hand, if this event was pushed to us, it is possible for",
            "                # it to become the only forward-extremity in the room, and we would then",
            "                # trust its state to be the state for the whole room. This is very bad.",
            "                # Further, if the event was pushed to us, there is no excuse for us not to",
            "                # have all the prev_events. We therefore reject any such events.",
            "                #",
            "                # XXX this really feels like it could/should be merged with the above,",
            "                # but there is an interaction with min_depth that I'm not really",
            "                # following.",
            "",
            "                if sent_to_us_directly:",
            "                    logger.warning(",
            "                        \"[%s %s] Rejecting: failed to fetch %d prev events: %s\",",
            "                        room_id,",
            "                        event_id,",
            "                        len(prevs - seen),",
            "                        shortstr(prevs - seen),",
            "                    )",
            "                    raise FederationError(",
            "                        \"ERROR\",",
            "                        403,",
            "                        (",
            "                            \"Your server isn't divulging details about prev_events \"",
            "                            \"referenced in this event.\"",
            "                        ),",
            "                        affected=pdu.event_id,",
            "                    )",
            "",
            "                logger.info(",
            "                    \"Event %s is missing prev_events: calculating state for a \"",
            "                    \"backwards extremity\",",
            "                    event_id,",
            "                )",
            "",
            "                # Calculate the state after each of the previous events, and",
            "                # resolve them to find the correct state at the current event.",
            "                event_map = {event_id: pdu}",
            "                try:",
            "                    # Get the state of the events we know about",
            "                    ours = await self.state_store.get_state_groups_ids(room_id, seen)",
            "",
            "                    # state_maps is a list of mappings from (type, state_key) to event_id",
            "                    state_maps = list(ours.values())  # type: List[StateMap[str]]",
            "",
            "                    # we don't need this any more, let's delete it.",
            "                    del ours",
            "",
            "                    # Ask the remote server for the states we don't",
            "                    # know about",
            "                    for p in prevs - seen:",
            "                        logger.info(",
            "                            \"Requesting state at missing prev_event %s\", event_id,",
            "                        )",
            "",
            "                        with nested_logging_context(p):",
            "                            # note that if any of the missing prevs share missing state or",
            "                            # auth events, the requests to fetch those events are deduped",
            "                            # by the get_pdu_cache in federation_client.",
            "                            (remote_state, _,) = await self._get_state_for_room(",
            "                                origin, room_id, p, include_event_in_state=True",
            "                            )",
            "",
            "                            remote_state_map = {",
            "                                (x.type, x.state_key): x.event_id for x in remote_state",
            "                            }",
            "                            state_maps.append(remote_state_map)",
            "",
            "                            for x in remote_state:",
            "                                event_map[x.event_id] = x",
            "",
            "                    room_version = await self.store.get_room_version_id(room_id)",
            "                    state_map = await self._state_resolution_handler.resolve_events_with_store(",
            "                        room_id,",
            "                        room_version,",
            "                        state_maps,",
            "                        event_map,",
            "                        state_res_store=StateResolutionStore(self.store),",
            "                    )",
            "",
            "                    # We need to give _process_received_pdu the actual state events",
            "                    # rather than event ids, so generate that now.",
            "",
            "                    # First though we need to fetch all the events that are in",
            "                    # state_map, so we can build up the state below.",
            "                    evs = await self.store.get_events(",
            "                        list(state_map.values()),",
            "                        get_prev_content=False,",
            "                        redact_behaviour=EventRedactBehaviour.AS_IS,",
            "                    )",
            "                    event_map.update(evs)",
            "",
            "                    state = [event_map[e] for e in state_map.values()]",
            "                except Exception:",
            "                    logger.warning(",
            "                        \"[%s %s] Error attempting to resolve state at missing \"",
            "                        \"prev_events\",",
            "                        room_id,",
            "                        event_id,",
            "                        exc_info=True,",
            "                    )",
            "                    raise FederationError(",
            "                        \"ERROR\",",
            "                        403,",
            "                        \"We can't get valid state history.\",",
            "                        affected=event_id,",
            "                    )",
            "",
            "        await self._process_received_pdu(origin, pdu, state=state)",
            "",
            "    async def _get_missing_events_for_pdu(self, origin, pdu, prevs, min_depth):",
            "        \"\"\"",
            "        Args:",
            "            origin (str): Origin of the pdu. Will be called to get the missing events",
            "            pdu: received pdu",
            "            prevs (set(str)): List of event ids which we are missing",
            "            min_depth (int): Minimum depth of events to return.",
            "        \"\"\"",
            "",
            "        room_id = pdu.room_id",
            "        event_id = pdu.event_id",
            "",
            "        seen = await self.store.have_events_in_timeline(prevs)",
            "",
            "        if not prevs - seen:",
            "            return",
            "",
            "        latest_list = await self.store.get_latest_event_ids_in_room(room_id)",
            "",
            "        # We add the prev events that we have seen to the latest",
            "        # list to ensure the remote server doesn't give them to us",
            "        latest = set(latest_list)",
            "        latest |= seen",
            "",
            "        logger.info(",
            "            \"[%s %s]: Requesting missing events between %s and %s\",",
            "            room_id,",
            "            event_id,",
            "            shortstr(latest),",
            "            event_id,",
            "        )",
            "",
            "        # XXX: we set timeout to 10s to help workaround",
            "        # https://github.com/matrix-org/synapse/issues/1733.",
            "        # The reason is to avoid holding the linearizer lock",
            "        # whilst processing inbound /send transactions, causing",
            "        # FDs to stack up and block other inbound transactions",
            "        # which empirically can currently take up to 30 minutes.",
            "        #",
            "        # N.B. this explicitly disables retry attempts.",
            "        #",
            "        # N.B. this also increases our chances of falling back to",
            "        # fetching fresh state for the room if the missing event",
            "        # can't be found, which slightly reduces our security.",
            "        # it may also increase our DAG extremity count for the room,",
            "        # causing additional state resolution?  See #1760.",
            "        # However, fetching state doesn't hold the linearizer lock",
            "        # apparently.",
            "        #",
            "        # see https://github.com/matrix-org/synapse/pull/1744",
            "        #",
            "        # ----",
            "        #",
            "        # Update richvdh 2018/09/18: There are a number of problems with timing this",
            "        # request out aggressively on the client side:",
            "        #",
            "        # - it plays badly with the server-side rate-limiter, which starts tarpitting you",
            "        #   if you send too many requests at once, so you end up with the server carefully",
            "        #   working through the backlog of your requests, which you have already timed",
            "        #   out.",
            "        #",
            "        # - for this request in particular, we now (as of",
            "        #   https://github.com/matrix-org/synapse/pull/3456) reject any PDUs where the",
            "        #   server can't produce a plausible-looking set of prev_events - so we becone",
            "        #   much more likely to reject the event.",
            "        #",
            "        # - contrary to what it says above, we do *not* fall back to fetching fresh state",
            "        #   for the room if get_missing_events times out. Rather, we give up processing",
            "        #   the PDU whose prevs we are missing, which then makes it much more likely that",
            "        #   we'll end up back here for the *next* PDU in the list, which exacerbates the",
            "        #   problem.",
            "        #",
            "        # - the aggressive 10s timeout was introduced to deal with incoming federation",
            "        #   requests taking 8 hours to process. It's not entirely clear why that was going",
            "        #   on; certainly there were other issues causing traffic storms which are now",
            "        #   resolved, and I think in any case we may be more sensible about our locking",
            "        #   now. We're *certainly* more sensible about our logging.",
            "        #",
            "        # All that said: Let's try increasing the timeout to 60s and see what happens.",
            "",
            "        try:",
            "            missing_events = await self.federation_client.get_missing_events(",
            "                origin,",
            "                room_id,",
            "                earliest_events_ids=list(latest),",
            "                latest_events=[pdu],",
            "                limit=10,",
            "                min_depth=min_depth,",
            "                timeout=60000,",
            "            )",
            "        except (RequestSendFailed, HttpResponseException, NotRetryingDestination) as e:",
            "            # We failed to get the missing events, but since we need to handle",
            "            # the case of `get_missing_events` not returning the necessary",
            "            # events anyway, it is safe to simply log the error and continue.",
            "            logger.warning(",
            "                \"[%s %s]: Failed to get prev_events: %s\", room_id, event_id, e",
            "            )",
            "            return",
            "",
            "        logger.info(",
            "            \"[%s %s]: Got %d prev_events: %s\",",
            "            room_id,",
            "            event_id,",
            "            len(missing_events),",
            "            shortstr(missing_events),",
            "        )",
            "",
            "        # We want to sort these by depth so we process them and",
            "        # tell clients about them in order.",
            "        missing_events.sort(key=lambda x: x.depth)",
            "",
            "        for ev in missing_events:",
            "            logger.info(",
            "                \"[%s %s] Handling received prev_event %s\",",
            "                room_id,",
            "                event_id,",
            "                ev.event_id,",
            "            )",
            "            with nested_logging_context(ev.event_id):",
            "                try:",
            "                    await self.on_receive_pdu(origin, ev, sent_to_us_directly=False)",
            "                except FederationError as e:",
            "                    if e.code == 403:",
            "                        logger.warning(",
            "                            \"[%s %s] Received prev_event %s failed history check.\",",
            "                            room_id,",
            "                            event_id,",
            "                            ev.event_id,",
            "                        )",
            "                    else:",
            "                        raise",
            "",
            "    async def _get_state_for_room(",
            "        self,",
            "        destination: str,",
            "        room_id: str,",
            "        event_id: str,",
            "        include_event_in_state: bool = False,",
            "    ) -> Tuple[List[EventBase], List[EventBase]]:",
            "        \"\"\"Requests all of the room state at a given event from a remote homeserver.",
            "",
            "        Args:",
            "            destination: The remote homeserver to query for the state.",
            "            room_id: The id of the room we're interested in.",
            "            event_id: The id of the event we want the state at.",
            "            include_event_in_state: if true, the event itself will be included in the",
            "                returned state event list.",
            "",
            "        Returns:",
            "            A list of events in the state, possibly including the event itself, and",
            "            a list of events in the auth chain for the given event.",
            "        \"\"\"",
            "        (",
            "            state_event_ids,",
            "            auth_event_ids,",
            "        ) = await self.federation_client.get_room_state_ids(",
            "            destination, room_id, event_id=event_id",
            "        )",
            "",
            "        desired_events = set(state_event_ids + auth_event_ids)",
            "",
            "        if include_event_in_state:",
            "            desired_events.add(event_id)",
            "",
            "        event_map = await self._get_events_from_store_or_dest(",
            "            destination, room_id, desired_events",
            "        )",
            "",
            "        failed_to_fetch = desired_events - event_map.keys()",
            "        if failed_to_fetch:",
            "            logger.warning(",
            "                \"Failed to fetch missing state/auth events for %s %s\",",
            "                event_id,",
            "                failed_to_fetch,",
            "            )",
            "",
            "        remote_state = [",
            "            event_map[e_id] for e_id in state_event_ids if e_id in event_map",
            "        ]",
            "",
            "        if include_event_in_state:",
            "            remote_event = event_map.get(event_id)",
            "            if not remote_event:",
            "                raise Exception(\"Unable to get missing prev_event %s\" % (event_id,))",
            "            if remote_event.is_state() and remote_event.rejected_reason is None:",
            "                remote_state.append(remote_event)",
            "",
            "        auth_chain = [event_map[e_id] for e_id in auth_event_ids if e_id in event_map]",
            "        auth_chain.sort(key=lambda e: e.depth)",
            "",
            "        return remote_state, auth_chain",
            "",
            "    async def _get_events_from_store_or_dest(",
            "        self, destination: str, room_id: str, event_ids: Iterable[str]",
            "    ) -> Dict[str, EventBase]:",
            "        \"\"\"Fetch events from a remote destination, checking if we already have them.",
            "",
            "        Persists any events we don't already have as outliers.",
            "",
            "        If we fail to fetch any of the events, a warning will be logged, and the event",
            "        will be omitted from the result. Likewise, any events which turn out not to",
            "        be in the given room.",
            "",
            "        This function *does not* automatically get missing auth events of the",
            "        newly fetched events. Callers must include the full auth chain of",
            "        of the missing events in the `event_ids` argument, to ensure that any",
            "        missing auth events are correctly fetched.",
            "",
            "        Returns:",
            "            map from event_id to event",
            "        \"\"\"",
            "        fetched_events = await self.store.get_events(event_ids, allow_rejected=True)",
            "",
            "        missing_events = set(event_ids) - fetched_events.keys()",
            "",
            "        if missing_events:",
            "            logger.debug(",
            "                \"Fetching unknown state/auth events %s for room %s\",",
            "                missing_events,",
            "                room_id,",
            "            )",
            "",
            "            await self._get_events_and_persist(",
            "                destination=destination, room_id=room_id, events=missing_events",
            "            )",
            "",
            "            # we need to make sure we re-load from the database to get the rejected",
            "            # state correct.",
            "            fetched_events.update(",
            "                (await self.store.get_events(missing_events, allow_rejected=True))",
            "            )",
            "",
            "        # check for events which were in the wrong room.",
            "        #",
            "        # this can happen if a remote server claims that the state or",
            "        # auth_events at an event in room A are actually events in room B",
            "",
            "        bad_events = [",
            "            (event_id, event.room_id)",
            "            for event_id, event in fetched_events.items()",
            "            if event.room_id != room_id",
            "        ]",
            "",
            "        for bad_event_id, bad_room_id in bad_events:",
            "            # This is a bogus situation, but since we may only discover it a long time",
            "            # after it happened, we try our best to carry on, by just omitting the",
            "            # bad events from the returned auth/state set.",
            "            logger.warning(",
            "                \"Remote server %s claims event %s in room %s is an auth/state \"",
            "                \"event in room %s\",",
            "                destination,",
            "                bad_event_id,",
            "                bad_room_id,",
            "                room_id,",
            "            )",
            "",
            "            del fetched_events[bad_event_id]",
            "",
            "        return fetched_events",
            "",
            "    async def _process_received_pdu(",
            "        self, origin: str, event: EventBase, state: Optional[Iterable[EventBase]],",
            "    ):",
            "        \"\"\" Called when we have a new pdu. We need to do auth checks and put it",
            "        through the StateHandler.",
            "",
            "        Args:",
            "            origin: server sending the event",
            "",
            "            event: event to be persisted",
            "",
            "            state: Normally None, but if we are handling a gap in the graph",
            "                (ie, we are missing one or more prev_events), the resolved state at the",
            "                event",
            "        \"\"\"",
            "        room_id = event.room_id",
            "        event_id = event.event_id",
            "",
            "        logger.debug(\"[%s %s] Processing event: %s\", room_id, event_id, event)",
            "",
            "        try:",
            "            await self._handle_new_event(origin, event, state=state)",
            "        except AuthError as e:",
            "            raise FederationError(\"ERROR\", e.code, e.msg, affected=event.event_id)",
            "",
            "        # For encrypted messages we check that we know about the sending device,",
            "        # if we don't then we mark the device cache for that user as stale.",
            "        if event.type == EventTypes.Encrypted:",
            "            device_id = event.content.get(\"device_id\")",
            "            sender_key = event.content.get(\"sender_key\")",
            "",
            "            cached_devices = await self.store.get_cached_devices_for_user(event.sender)",
            "",
            "            resync = False  # Whether we should resync device lists.",
            "",
            "            device = None",
            "            if device_id is not None:",
            "                device = cached_devices.get(device_id)",
            "                if device is None:",
            "                    logger.info(",
            "                        \"Received event from remote device not in our cache: %s %s\",",
            "                        event.sender,",
            "                        device_id,",
            "                    )",
            "                    resync = True",
            "",
            "            # We also check if the `sender_key` matches what we expect.",
            "            if sender_key is not None:",
            "                # Figure out what sender key we're expecting. If we know the",
            "                # device and recognize the algorithm then we can work out the",
            "                # exact key to expect. Otherwise check it matches any key we",
            "                # have for that device.",
            "",
            "                current_keys = []  # type: Container[str]",
            "",
            "                if device:",
            "                    keys = device.get(\"keys\", {}).get(\"keys\", {})",
            "",
            "                    if (",
            "                        event.content.get(\"algorithm\")",
            "                        == RoomEncryptionAlgorithms.MEGOLM_V1_AES_SHA2",
            "                    ):",
            "                        # For this algorithm we expect a curve25519 key.",
            "                        key_name = \"curve25519:%s\" % (device_id,)",
            "                        current_keys = [keys.get(key_name)]",
            "                    else:",
            "                        # We don't know understand the algorithm, so we just",
            "                        # check it matches a key for the device.",
            "                        current_keys = keys.values()",
            "                elif device_id:",
            "                    # We don't have any keys for the device ID.",
            "                    pass",
            "                else:",
            "                    # The event didn't include a device ID, so we just look for",
            "                    # keys across all devices.",
            "                    current_keys = [",
            "                        key",
            "                        for device in cached_devices.values()",
            "                        for key in device.get(\"keys\", {}).get(\"keys\", {}).values()",
            "                    ]",
            "",
            "                # We now check that the sender key matches (one of) the expected",
            "                # keys.",
            "                if sender_key not in current_keys:",
            "                    logger.info(",
            "                        \"Received event from remote device with unexpected sender key: %s %s: %s\",",
            "                        event.sender,",
            "                        device_id or \"<no device_id>\",",
            "                        sender_key,",
            "                    )",
            "                    resync = True",
            "",
            "            if resync:",
            "                run_as_background_process(",
            "                    \"resync_device_due_to_pdu\", self._resync_device, event.sender",
            "                )",
            "",
            "    async def _resync_device(self, sender: str) -> None:",
            "        \"\"\"We have detected that the device list for the given user may be out",
            "        of sync, so we try and resync them.",
            "        \"\"\"",
            "",
            "        try:",
            "            await self.store.mark_remote_user_device_cache_as_stale(sender)",
            "",
            "            # Immediately attempt a resync in the background",
            "            if self.config.worker_app:",
            "                await self._user_device_resync(user_id=sender)",
            "            else:",
            "                await self._device_list_updater.user_device_resync(sender)",
            "        except Exception:",
            "            logger.exception(\"Failed to resync device for %s\", sender)",
            "",
            "    @log_function",
            "    async def backfill(self, dest, room_id, limit, extremities):",
            "        \"\"\" Trigger a backfill request to `dest` for the given `room_id`",
            "",
            "        This will attempt to get more events from the remote. If the other side",
            "        has no new events to offer, this will return an empty list.",
            "",
            "        As the events are received, we check their signatures, and also do some",
            "        sanity-checking on them. If any of the backfilled events are invalid,",
            "        this method throws a SynapseError.",
            "",
            "        TODO: make this more useful to distinguish failures of the remote",
            "        server from invalid events (there is probably no point in trying to",
            "        re-fetch invalid events from every other HS in the room.)",
            "        \"\"\"",
            "        if dest == self.server_name:",
            "            raise SynapseError(400, \"Can't backfill from self.\")",
            "",
            "        events = await self.federation_client.backfill(",
            "            dest, room_id, limit=limit, extremities=extremities",
            "        )",
            "",
            "        if not events:",
            "            return []",
            "",
            "        # ideally we'd sanity check the events here for excess prev_events etc,",
            "        # but it's hard to reject events at this point without completely",
            "        # breaking backfill in the same way that it is currently broken by",
            "        # events whose signature we cannot verify (#3121).",
            "        #",
            "        # So for now we accept the events anyway. #3124 tracks this.",
            "        #",
            "        # for ev in events:",
            "        #     self._sanity_check_event(ev)",
            "",
            "        # Don't bother processing events we already have.",
            "        seen_events = await self.store.have_events_in_timeline(",
            "            {e.event_id for e in events}",
            "        )",
            "",
            "        events = [e for e in events if e.event_id not in seen_events]",
            "",
            "        if not events:",
            "            return []",
            "",
            "        event_map = {e.event_id: e for e in events}",
            "",
            "        event_ids = {e.event_id for e in events}",
            "",
            "        # build a list of events whose prev_events weren't in the batch.",
            "        # (XXX: this will include events whose prev_events we already have; that doesn't",
            "        # sound right?)",
            "        edges = [ev.event_id for ev in events if set(ev.prev_event_ids()) - event_ids]",
            "",
            "        logger.info(\"backfill: Got %d events with %d edges\", len(events), len(edges))",
            "",
            "        # For each edge get the current state.",
            "",
            "        auth_events = {}",
            "        state_events = {}",
            "        events_to_state = {}",
            "        for e_id in edges:",
            "            state, auth = await self._get_state_for_room(",
            "                destination=dest,",
            "                room_id=room_id,",
            "                event_id=e_id,",
            "                include_event_in_state=False,",
            "            )",
            "            auth_events.update({a.event_id: a for a in auth})",
            "            auth_events.update({s.event_id: s for s in state})",
            "            state_events.update({s.event_id: s for s in state})",
            "            events_to_state[e_id] = state",
            "",
            "        required_auth = {",
            "            a_id",
            "            for event in events",
            "            + list(state_events.values())",
            "            + list(auth_events.values())",
            "            for a_id in event.auth_event_ids()",
            "        }",
            "        auth_events.update(",
            "            {e_id: event_map[e_id] for e_id in required_auth if e_id in event_map}",
            "        )",
            "",
            "        ev_infos = []",
            "",
            "        # Step 1: persist the events in the chunk we fetched state for (i.e.",
            "        # the backwards extremities), with custom auth events and state",
            "        for e_id in events_to_state:",
            "            # For paranoia we ensure that these events are marked as",
            "            # non-outliers",
            "            ev = event_map[e_id]",
            "            assert not ev.internal_metadata.is_outlier()",
            "",
            "            ev_infos.append(",
            "                _NewEventInfo(",
            "                    event=ev,",
            "                    state=events_to_state[e_id],",
            "                    auth_events={",
            "                        (",
            "                            auth_events[a_id].type,",
            "                            auth_events[a_id].state_key,",
            "                        ): auth_events[a_id]",
            "                        for a_id in ev.auth_event_ids()",
            "                        if a_id in auth_events",
            "                    },",
            "                )",
            "            )",
            "",
            "        if ev_infos:",
            "            await self._handle_new_events(dest, room_id, ev_infos, backfilled=True)",
            "",
            "        # Step 2: Persist the rest of the events in the chunk one by one",
            "        events.sort(key=lambda e: e.depth)",
            "",
            "        for event in events:",
            "            if event in events_to_state:",
            "                continue",
            "",
            "            # For paranoia we ensure that these events are marked as",
            "            # non-outliers",
            "            assert not event.internal_metadata.is_outlier()",
            "",
            "            # We store these one at a time since each event depends on the",
            "            # previous to work out the state.",
            "            # TODO: We can probably do something more clever here.",
            "            await self._handle_new_event(dest, event, backfilled=True)",
            "",
            "        return events",
            "",
            "    async def maybe_backfill(",
            "        self, room_id: str, current_depth: int, limit: int",
            "    ) -> bool:",
            "        \"\"\"Checks the database to see if we should backfill before paginating,",
            "        and if so do.",
            "",
            "        Args:",
            "            room_id",
            "            current_depth: The depth from which we're paginating from. This is",
            "                used to decide if we should backfill and what extremities to",
            "                use.",
            "            limit: The number of events that the pagination request will",
            "                return. This is used as part of the heuristic to decide if we",
            "                should back paginate.",
            "        \"\"\"",
            "        extremities = await self.store.get_oldest_events_with_depth_in_room(room_id)",
            "",
            "        if not extremities:",
            "            logger.debug(\"Not backfilling as no extremeties found.\")",
            "            return False",
            "",
            "        # We only want to paginate if we can actually see the events we'll get,",
            "        # as otherwise we'll just spend a lot of resources to get redacted",
            "        # events.",
            "        #",
            "        # We do this by filtering all the backwards extremities and seeing if",
            "        # any remain. Given we don't have the extremity events themselves, we",
            "        # need to actually check the events that reference them.",
            "        #",
            "        # *Note*: the spec wants us to keep backfilling until we reach the start",
            "        # of the room in case we are allowed to see some of the history. However",
            "        # in practice that causes more issues than its worth, as a) its",
            "        # relatively rare for there to be any visible history and b) even when",
            "        # there is its often sufficiently long ago that clients would stop",
            "        # attempting to paginate before backfill reached the visible history.",
            "        #",
            "        # TODO: If we do do a backfill then we should filter the backwards",
            "        #   extremities to only include those that point to visible portions of",
            "        #   history.",
            "        #",
            "        # TODO: Correctly handle the case where we are allowed to see the",
            "        #   forward event but not the backward extremity, e.g. in the case of",
            "        #   initial join of the server where we are allowed to see the join",
            "        #   event but not anything before it. This would require looking at the",
            "        #   state *before* the event, ignoring the special casing certain event",
            "        #   types have.",
            "",
            "        forward_events = await self.store.get_successor_events(list(extremities))",
            "",
            "        extremities_events = await self.store.get_events(",
            "            forward_events,",
            "            redact_behaviour=EventRedactBehaviour.AS_IS,",
            "            get_prev_content=False,",
            "        )",
            "",
            "        # We set `check_history_visibility_only` as we might otherwise get false",
            "        # positives from users having been erased.",
            "        filtered_extremities = await filter_events_for_server(",
            "            self.storage,",
            "            self.server_name,",
            "            list(extremities_events.values()),",
            "            redact=False,",
            "            check_history_visibility_only=True,",
            "        )",
            "",
            "        if not filtered_extremities:",
            "            return False",
            "",
            "        # Check if we reached a point where we should start backfilling.",
            "        sorted_extremeties_tuple = sorted(extremities.items(), key=lambda e: -int(e[1]))",
            "        max_depth = sorted_extremeties_tuple[0][1]",
            "",
            "        # If we're approaching an extremity we trigger a backfill, otherwise we",
            "        # no-op.",
            "        #",
            "        # We chose twice the limit here as then clients paginating backwards",
            "        # will send pagination requests that trigger backfill at least twice",
            "        # using the most recent extremity before it gets removed (see below). We",
            "        # chose more than one times the limit in case of failure, but choosing a",
            "        # much larger factor will result in triggering a backfill request much",
            "        # earlier than necessary.",
            "        if current_depth - 2 * limit > max_depth:",
            "            logger.debug(",
            "                \"Not backfilling as we don't need to. %d < %d - 2 * %d\",",
            "                max_depth,",
            "                current_depth,",
            "                limit,",
            "            )",
            "            return False",
            "",
            "        logger.debug(",
            "            \"room_id: %s, backfill: current_depth: %s, max_depth: %s, extrems: %s\",",
            "            room_id,",
            "            current_depth,",
            "            max_depth,",
            "            sorted_extremeties_tuple,",
            "        )",
            "",
            "        # We ignore extremities that have a greater depth than our current depth",
            "        # as:",
            "        #    1. we don't really care about getting events that have happened",
            "        #       before our current position; and",
            "        #    2. we have likely previously tried and failed to backfill from that",
            "        #       extremity, so to avoid getting \"stuck\" requesting the same",
            "        #       backfill repeatedly we drop those extremities.",
            "        filtered_sorted_extremeties_tuple = [",
            "            t for t in sorted_extremeties_tuple if int(t[1]) <= current_depth",
            "        ]",
            "",
            "        # However, we need to check that the filtered extremities are non-empty.",
            "        # If they are empty then either we can a) bail or b) still attempt to",
            "        # backill. We opt to try backfilling anyway just in case we do get",
            "        # relevant events.",
            "        if filtered_sorted_extremeties_tuple:",
            "            sorted_extremeties_tuple = filtered_sorted_extremeties_tuple",
            "",
            "        # We don't want to specify too many extremities as it causes the backfill",
            "        # request URI to be too long.",
            "        extremities = dict(sorted_extremeties_tuple[:5])",
            "",
            "        # Now we need to decide which hosts to hit first.",
            "",
            "        # First we try hosts that are already in the room",
            "        # TODO: HEURISTIC ALERT.",
            "",
            "        curr_state = await self.state_handler.get_current_state(room_id)",
            "",
            "        def get_domains_from_state(state):",
            "            \"\"\"Get joined domains from state",
            "",
            "            Args:",
            "                state (dict[tuple, FrozenEvent]): State map from type/state",
            "                    key to event.",
            "",
            "            Returns:",
            "                list[tuple[str, int]]: Returns a list of servers with the",
            "                lowest depth of their joins. Sorted by lowest depth first.",
            "            \"\"\"",
            "            joined_users = [",
            "                (state_key, int(event.depth))",
            "                for (e_type, state_key), event in state.items()",
            "                if e_type == EventTypes.Member and event.membership == Membership.JOIN",
            "            ]",
            "",
            "            joined_domains = {}  # type: Dict[str, int]",
            "            for u, d in joined_users:",
            "                try:",
            "                    dom = get_domain_from_id(u)",
            "                    old_d = joined_domains.get(dom)",
            "                    if old_d:",
            "                        joined_domains[dom] = min(d, old_d)",
            "                    else:",
            "                        joined_domains[dom] = d",
            "                except Exception:",
            "                    pass",
            "",
            "            return sorted(joined_domains.items(), key=lambda d: d[1])",
            "",
            "        curr_domains = get_domains_from_state(curr_state)",
            "",
            "        likely_domains = [",
            "            domain for domain, depth in curr_domains if domain != self.server_name",
            "        ]",
            "",
            "        async def try_backfill(domains):",
            "            # TODO: Should we try multiple of these at a time?",
            "            for dom in domains:",
            "                try:",
            "                    await self.backfill(",
            "                        dom, room_id, limit=100, extremities=extremities",
            "                    )",
            "                    # If this succeeded then we probably already have the",
            "                    # appropriate stuff.",
            "                    # TODO: We can probably do something more intelligent here.",
            "                    return True",
            "                except SynapseError as e:",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except HttpResponseException as e:",
            "                    if 400 <= e.code < 500:",
            "                        raise e.to_synapse_error()",
            "",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except CodeMessageException as e:",
            "                    if 400 <= e.code < 500:",
            "                        raise",
            "",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except NotRetryingDestination as e:",
            "                    logger.info(str(e))",
            "                    continue",
            "                except RequestSendFailed as e:",
            "                    logger.info(\"Failed to get backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except FederationDeniedError as e:",
            "                    logger.info(e)",
            "                    continue",
            "                except Exception as e:",
            "                    logger.exception(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "",
            "            return False",
            "",
            "        success = await try_backfill(likely_domains)",
            "        if success:",
            "            return True",
            "",
            "        # Huh, well *those* domains didn't work out. Lets try some domains",
            "        # from the time.",
            "",
            "        tried_domains = set(likely_domains)",
            "        tried_domains.add(self.server_name)",
            "",
            "        event_ids = list(extremities.keys())",
            "",
            "        logger.debug(\"calling resolve_state_groups in _maybe_backfill\")",
            "        resolve = preserve_fn(self.state_handler.resolve_state_groups_for_events)",
            "        states = await make_deferred_yieldable(",
            "            defer.gatherResults(",
            "                [resolve(room_id, [e]) for e in event_ids], consumeErrors=True",
            "            )",
            "        )",
            "",
            "        # dict[str, dict[tuple, str]], a map from event_id to state map of",
            "        # event_ids.",
            "        states = dict(zip(event_ids, [s.state for s in states]))",
            "",
            "        state_map = await self.store.get_events(",
            "            [e_id for ids in states.values() for e_id in ids.values()],",
            "            get_prev_content=False,",
            "        )",
            "        states = {",
            "            key: {",
            "                k: state_map[e_id]",
            "                for k, e_id in state_dict.items()",
            "                if e_id in state_map",
            "            }",
            "            for key, state_dict in states.items()",
            "        }",
            "",
            "        for e_id, _ in sorted_extremeties_tuple:",
            "            likely_domains = get_domains_from_state(states[e_id])",
            "",
            "            success = await try_backfill(",
            "                [dom for dom, _ in likely_domains if dom not in tried_domains]",
            "            )",
            "            if success:",
            "                return True",
            "",
            "            tried_domains.update(dom for dom, _ in likely_domains)",
            "",
            "        return False",
            "",
            "    async def _get_events_and_persist(",
            "        self, destination: str, room_id: str, events: Iterable[str]",
            "    ):",
            "        \"\"\"Fetch the given events from a server, and persist them as outliers.",
            "",
            "        This function *does not* recursively get missing auth events of the",
            "        newly fetched events. Callers must include in the `events` argument",
            "        any missing events from the auth chain.",
            "",
            "        Logs a warning if we can't find the given event.",
            "        \"\"\"",
            "",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        event_map = {}  # type: Dict[str, EventBase]",
            "",
            "        async def get_event(event_id: str):",
            "            with nested_logging_context(event_id):",
            "                try:",
            "                    event = await self.federation_client.get_pdu(",
            "                        [destination], event_id, room_version, outlier=True,",
            "                    )",
            "                    if event is None:",
            "                        logger.warning(",
            "                            \"Server %s didn't return event %s\", destination, event_id,",
            "                        )",
            "                        return",
            "",
            "                    event_map[event.event_id] = event",
            "",
            "                except Exception as e:",
            "                    logger.warning(",
            "                        \"Error fetching missing state/auth event %s: %s %s\",",
            "                        event_id,",
            "                        type(e),",
            "                        e,",
            "                    )",
            "",
            "        await concurrently_execute(get_event, events, 5)",
            "",
            "        # Make a map of auth events for each event. We do this after fetching",
            "        # all the events as some of the events' auth events will be in the list",
            "        # of requested events.",
            "",
            "        auth_events = [",
            "            aid",
            "            for event in event_map.values()",
            "            for aid in event.auth_event_ids()",
            "            if aid not in event_map",
            "        ]",
            "        persisted_events = await self.store.get_events(",
            "            auth_events, allow_rejected=True,",
            "        )",
            "",
            "        event_infos = []",
            "        for event in event_map.values():",
            "            auth = {}",
            "            for auth_event_id in event.auth_event_ids():",
            "                ae = persisted_events.get(auth_event_id) or event_map.get(auth_event_id)",
            "                if ae:",
            "                    auth[(ae.type, ae.state_key)] = ae",
            "                else:",
            "                    logger.info(\"Missing auth event %s\", auth_event_id)",
            "",
            "            event_infos.append(_NewEventInfo(event, None, auth))",
            "",
            "        await self._handle_new_events(",
            "            destination, room_id, event_infos,",
            "        )",
            "",
            "    def _sanity_check_event(self, ev):",
            "        \"\"\"",
            "        Do some early sanity checks of a received event",
            "",
            "        In particular, checks it doesn't have an excessive number of",
            "        prev_events or auth_events, which could cause a huge state resolution",
            "        or cascade of event fetches.",
            "",
            "        Args:",
            "            ev (synapse.events.EventBase): event to be checked",
            "",
            "        Returns: None",
            "",
            "        Raises:",
            "            SynapseError if the event does not pass muster",
            "        \"\"\"",
            "        if len(ev.prev_event_ids()) > 20:",
            "            logger.warning(",
            "                \"Rejecting event %s which has %i prev_events\",",
            "                ev.event_id,",
            "                len(ev.prev_event_ids()),",
            "            )",
            "            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many prev_events\")",
            "",
            "        if len(ev.auth_event_ids()) > 10:",
            "            logger.warning(",
            "                \"Rejecting event %s which has %i auth_events\",",
            "                ev.event_id,",
            "                len(ev.auth_event_ids()),",
            "            )",
            "            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many auth_events\")",
            "",
            "    async def send_invite(self, target_host, event):",
            "        \"\"\" Sends the invite to the remote server for signing.",
            "",
            "        Invites must be signed by the invitee's server before distribution.",
            "        \"\"\"",
            "        pdu = await self.federation_client.send_invite(",
            "            destination=target_host,",
            "            room_id=event.room_id,",
            "            event_id=event.event_id,",
            "            pdu=event,",
            "        )",
            "",
            "        return pdu",
            "",
            "    async def on_event_auth(self, event_id: str) -> List[EventBase]:",
            "        event = await self.store.get_event(event_id)",
            "        auth = await self.store.get_auth_chain(",
            "            list(event.auth_event_ids()), include_given=True",
            "        )",
            "        return list(auth)",
            "",
            "    async def do_invite_join(",
            "        self, target_hosts: Iterable[str], room_id: str, joinee: str, content: JsonDict",
            "    ) -> Tuple[str, int]:",
            "        \"\"\" Attempts to join the `joinee` to the room `room_id` via the",
            "        servers contained in `target_hosts`.",
            "",
            "        This first triggers a /make_join/ request that returns a partial",
            "        event that we can fill out and sign. This is then sent to the",
            "        remote server via /send_join/ which responds with the state at that",
            "        event and the auth_chains.",
            "",
            "        We suspend processing of any received events from this room until we",
            "        have finished processing the join.",
            "",
            "        Args:",
            "            target_hosts: List of servers to attempt to join the room with.",
            "",
            "            room_id: The ID of the room to join.",
            "",
            "            joinee: The User ID of the joining user.",
            "",
            "            content: The event content to use for the join event.",
            "        \"\"\"",
            "        # TODO: We should be able to call this on workers, but the upgrading of",
            "        # room stuff after join currently doesn't work on workers.",
            "        assert self.config.worker.worker_app is None",
            "",
            "        logger.debug(\"Joining %s to %s\", joinee, room_id)",
            "",
            "        origin, event, room_version_obj = await self._make_and_verify_event(",
            "            target_hosts,",
            "            room_id,",
            "            joinee,",
            "            \"join\",",
            "            content,",
            "            params={\"ver\": KNOWN_ROOM_VERSIONS},",
            "        )",
            "",
            "        # This shouldn't happen, because the RoomMemberHandler has a",
            "        # linearizer lock which only allows one operation per user per room",
            "        # at a time - so this is just paranoia.",
            "        assert room_id not in self.room_queues",
            "",
            "        self.room_queues[room_id] = []",
            "",
            "        await self._clean_room_for_join(room_id)",
            "",
            "        handled_events = set()",
            "",
            "        try:",
            "            # Try the host we successfully got a response to /make_join/",
            "            # request first.",
            "            host_list = list(target_hosts)",
            "            try:",
            "                host_list.remove(origin)",
            "                host_list.insert(0, origin)",
            "            except ValueError:",
            "                pass",
            "",
            "            ret = await self.federation_client.send_join(",
            "                host_list, event, room_version_obj",
            "            )",
            "",
            "            origin = ret[\"origin\"]",
            "            state = ret[\"state\"]",
            "            auth_chain = ret[\"auth_chain\"]",
            "            auth_chain.sort(key=lambda e: e.depth)",
            "",
            "            handled_events.update([s.event_id for s in state])",
            "            handled_events.update([a.event_id for a in auth_chain])",
            "            handled_events.add(event.event_id)",
            "",
            "            logger.debug(\"do_invite_join auth_chain: %s\", auth_chain)",
            "            logger.debug(\"do_invite_join state: %s\", state)",
            "",
            "            logger.debug(\"do_invite_join event: %s\", event)",
            "",
            "            # if this is the first time we've joined this room, it's time to add",
            "            # a row to `rooms` with the correct room version. If there's already a",
            "            # row there, we should override it, since it may have been populated",
            "            # based on an invite request which lied about the room version.",
            "            #",
            "            # federation_client.send_join has already checked that the room",
            "            # version in the received create event is the same as room_version_obj,",
            "            # so we can rely on it now.",
            "            #",
            "            await self.store.upsert_room_on_join(",
            "                room_id=room_id, room_version=room_version_obj,",
            "            )",
            "",
            "            max_stream_id = await self._persist_auth_tree(",
            "                origin, room_id, auth_chain, state, event, room_version_obj",
            "            )",
            "",
            "            # We wait here until this instance has seen the events come down",
            "            # replication (if we're using replication) as the below uses caches.",
            "            await self._replication.wait_for_stream_position(",
            "                self.config.worker.events_shard_config.get_instance(room_id),",
            "                \"events\",",
            "                max_stream_id,",
            "            )",
            "",
            "            # Check whether this room is the result of an upgrade of a room we already know",
            "            # about. If so, migrate over user information",
            "            predecessor = await self.store.get_room_predecessor(room_id)",
            "            if not predecessor or not isinstance(predecessor.get(\"room_id\"), str):",
            "                return event.event_id, max_stream_id",
            "            old_room_id = predecessor[\"room_id\"]",
            "            logger.debug(",
            "                \"Found predecessor for %s during remote join: %s\", room_id, old_room_id",
            "            )",
            "",
            "            # We retrieve the room member handler here as to not cause a cyclic dependency",
            "            member_handler = self.hs.get_room_member_handler()",
            "            await member_handler.transfer_room_state_on_room_upgrade(",
            "                old_room_id, room_id",
            "            )",
            "",
            "            logger.debug(\"Finished joining %s to %s\", joinee, room_id)",
            "            return event.event_id, max_stream_id",
            "        finally:",
            "            room_queue = self.room_queues[room_id]",
            "            del self.room_queues[room_id]",
            "",
            "            # we don't need to wait for the queued events to be processed -",
            "            # it's just a best-effort thing at this point. We do want to do",
            "            # them roughly in order, though, otherwise we'll end up making",
            "            # lots of requests for missing prev_events which we do actually",
            "            # have. Hence we fire off the background task, but don't wait for it.",
            "",
            "            run_in_background(self._handle_queued_pdus, room_queue)",
            "",
            "    async def _handle_queued_pdus(self, room_queue):",
            "        \"\"\"Process PDUs which got queued up while we were busy send_joining.",
            "",
            "        Args:",
            "            room_queue (list[FrozenEvent, str]): list of PDUs to be processed",
            "                and the servers that sent them",
            "        \"\"\"",
            "        for p, origin in room_queue:",
            "            try:",
            "                logger.info(",
            "                    \"Processing queued PDU %s which was received \"",
            "                    \"while we were joining %s\",",
            "                    p.event_id,",
            "                    p.room_id,",
            "                )",
            "                with nested_logging_context(p.event_id):",
            "                    await self.on_receive_pdu(origin, p, sent_to_us_directly=True)",
            "            except Exception as e:",
            "                logger.warning(",
            "                    \"Error handling queued PDU %s from %s: %s\", p.event_id, origin, e",
            "                )",
            "",
            "    async def on_make_join_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> EventBase:",
            "        \"\"\" We've received a /make_join/ request, so we create a partial",
            "        join event for the room and return that. We do *not* persist or",
            "        process it until the other server has signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: Room to create join event in",
            "            user_id: The user to create the join for",
            "        \"\"\"",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.info(",
            "                \"Got /make_join request for user %r from different origin %s, ignoring\",",
            "                user_id,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        # checking the room version will check that we've actually heard of the room",
            "        # (and return a 404 otherwise)",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "",
            "        # now check that we are *still* in the room",
            "        is_in_room = await self.auth.check_host_in_room(room_id, self.server_name)",
            "        if not is_in_room:",
            "            logger.info(",
            "                \"Got /make_join request for room %s we are no longer in\", room_id,",
            "            )",
            "            raise NotFoundError(\"Not an active room on this server\")",
            "",
            "        event_content = {\"membership\": Membership.JOIN}",
            "",
            "        builder = self.event_builder_factory.new(",
            "            room_version,",
            "            {",
            "                \"type\": EventTypes.Member,",
            "                \"content\": event_content,",
            "                \"room_id\": room_id,",
            "                \"sender\": user_id,",
            "                \"state_key\": user_id,",
            "            },",
            "        )",
            "",
            "        try:",
            "            event, context = await self.event_creation_handler.create_new_client_event(",
            "                builder=builder",
            "            )",
            "        except SynapseError as e:",
            "            logger.warning(\"Failed to create join to %s because %s\", room_id, e)",
            "            raise",
            "",
            "        # The remote hasn't signed it yet, obviously. We'll do the full checks",
            "        # when we get the event back in `on_send_join_request`",
            "        await self.auth.check_from_context(",
            "            room_version, event, context, do_sig_check=False",
            "        )",
            "",
            "        return event",
            "",
            "    async def on_send_join_request(self, origin, pdu):",
            "        \"\"\" We have received a join event for a room. Fully process it and",
            "        respond with the current state and auth chains.",
            "        \"\"\"",
            "        event = pdu",
            "",
            "        logger.debug(",
            "            \"on_send_join_request from %s: Got event: %s, signatures: %s\",",
            "            origin,",
            "            event.event_id,",
            "            event.signatures,",
            "        )",
            "",
            "        if get_domain_from_id(event.sender) != origin:",
            "            logger.info(",
            "                \"Got /send_join request for user %r from different origin %s\",",
            "                event.sender,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        event.internal_metadata.outlier = False",
            "        # Send this event on behalf of the origin server.",
            "        #",
            "        # The reasons we have the destination server rather than the origin",
            "        # server send it are slightly mysterious: the origin server should have",
            "        # all the necessary state once it gets the response to the send_join,",
            "        # so it could send the event itself if it wanted to. It may be that",
            "        # doing it this way reduces failure modes, or avoids certain attacks",
            "        # where a new server selectively tells a subset of the federation that",
            "        # it has joined.",
            "        #",
            "        # The fact is that, as of the current writing, Synapse doesn't send out",
            "        # the join event over federation after joining, and changing it now",
            "        # would introduce the danger of backwards-compatibility problems.",
            "        event.internal_metadata.send_on_behalf_of = origin",
            "",
            "        context = await self._handle_new_event(origin, event)",
            "",
            "        logger.debug(",
            "            \"on_send_join_request: After _handle_new_event: %s, sigs: %s\",",
            "            event.event_id,",
            "            event.signatures,",
            "        )",
            "",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "",
            "        state_ids = list(prev_state_ids.values())",
            "        auth_chain = await self.store.get_auth_chain(state_ids)",
            "",
            "        state = await self.store.get_events(list(prev_state_ids.values()))",
            "",
            "        return {\"state\": list(state.values()), \"auth_chain\": auth_chain}",
            "",
            "    async def on_invite_request(",
            "        self, origin: str, event: EventBase, room_version: RoomVersion",
            "    ):",
            "        \"\"\" We've got an invite event. Process and persist it. Sign it.",
            "",
            "        Respond with the now signed event.",
            "        \"\"\"",
            "        if event.state_key is None:",
            "            raise SynapseError(400, \"The invite event did not have a state key\")",
            "",
            "        is_blocked = await self.store.is_room_blocked(event.room_id)",
            "        if is_blocked:",
            "            raise SynapseError(403, \"This room has been blocked on this server\")",
            "",
            "        if self.hs.config.block_non_admin_invites:",
            "            raise SynapseError(403, \"This server does not accept room invites\")",
            "",
            "        if not self.spam_checker.user_may_invite(",
            "            event.sender, event.state_key, event.room_id",
            "        ):",
            "            raise SynapseError(",
            "                403, \"This user is not permitted to send invites to this server/user\"",
            "            )",
            "",
            "        membership = event.content.get(\"membership\")",
            "        if event.type != EventTypes.Member or membership != Membership.INVITE:",
            "            raise SynapseError(400, \"The event was not an m.room.member invite event\")",
            "",
            "        sender_domain = get_domain_from_id(event.sender)",
            "        if sender_domain != origin:",
            "            raise SynapseError(",
            "                400, \"The invite event was not from the server sending it\"",
            "            )",
            "",
            "        if not self.is_mine_id(event.state_key):",
            "            raise SynapseError(400, \"The invite event must be for this server\")",
            "",
            "        # block any attempts to invite the server notices mxid",
            "        if event.state_key == self._server_notices_mxid:",
            "            raise SynapseError(HTTPStatus.FORBIDDEN, \"Cannot invite this user\")",
            "",
            "        # keep a record of the room version, if we don't yet know it.",
            "        # (this may get overwritten if we later get a different room version in a",
            "        # join dance).",
            "        await self._maybe_store_room_on_outlier_membership(",
            "            room_id=event.room_id, room_version=room_version",
            "        )",
            "",
            "        event.internal_metadata.outlier = True",
            "        event.internal_metadata.out_of_band_membership = True",
            "",
            "        event.signatures.update(",
            "            compute_event_signature(",
            "                room_version,",
            "                event.get_pdu_json(),",
            "                self.hs.hostname,",
            "                self.hs.signing_key,",
            "            )",
            "        )",
            "",
            "        context = await self.state_handler.compute_event_context(event)",
            "        await self.persist_events_and_notify(event.room_id, [(event, context)])",
            "",
            "        return event",
            "",
            "    async def do_remotely_reject_invite(",
            "        self, target_hosts: Iterable[str], room_id: str, user_id: str, content: JsonDict",
            "    ) -> Tuple[EventBase, int]:",
            "        origin, event, room_version = await self._make_and_verify_event(",
            "            target_hosts, room_id, user_id, \"leave\", content=content",
            "        )",
            "        # Mark as outlier as we don't have any state for this event; we're not",
            "        # even in the room.",
            "        event.internal_metadata.outlier = True",
            "        event.internal_metadata.out_of_band_membership = True",
            "",
            "        # Try the host that we successfully called /make_leave/ on first for",
            "        # the /send_leave/ request.",
            "        host_list = list(target_hosts)",
            "        try:",
            "            host_list.remove(origin)",
            "            host_list.insert(0, origin)",
            "        except ValueError:",
            "            pass",
            "",
            "        await self.federation_client.send_leave(host_list, event)",
            "",
            "        context = await self.state_handler.compute_event_context(event)",
            "        stream_id = await self.persist_events_and_notify(",
            "            event.room_id, [(event, context)]",
            "        )",
            "",
            "        return event, stream_id",
            "",
            "    async def _make_and_verify_event(",
            "        self,",
            "        target_hosts: Iterable[str],",
            "        room_id: str,",
            "        user_id: str,",
            "        membership: str,",
            "        content: JsonDict = {},",
            "        params: Optional[Dict[str, Union[str, Iterable[str]]]] = None,",
            "    ) -> Tuple[str, EventBase, RoomVersion]:",
            "        (",
            "            origin,",
            "            event,",
            "            room_version,",
            "        ) = await self.federation_client.make_membership_event(",
            "            target_hosts, room_id, user_id, membership, content, params=params",
            "        )",
            "",
            "        logger.debug(\"Got response to make_%s: %s\", membership, event)",
            "",
            "        # We should assert some things.",
            "        # FIXME: Do this in a nicer way",
            "        assert event.type == EventTypes.Member",
            "        assert event.user_id == user_id",
            "        assert event.state_key == user_id",
            "        assert event.room_id == room_id",
            "        return origin, event, room_version",
            "",
            "    async def on_make_leave_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> EventBase:",
            "        \"\"\" We've received a /make_leave/ request, so we create a partial",
            "        leave event for the room and return that. We do *not* persist or",
            "        process it until the other server has signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: Room to create leave event in",
            "            user_id: The user to create the leave for",
            "        \"\"\"",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.info(",
            "                \"Got /make_leave request for user %r from different origin %s, ignoring\",",
            "                user_id,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "        builder = self.event_builder_factory.new(",
            "            room_version,",
            "            {",
            "                \"type\": EventTypes.Member,",
            "                \"content\": {\"membership\": Membership.LEAVE},",
            "                \"room_id\": room_id,",
            "                \"sender\": user_id,",
            "                \"state_key\": user_id,",
            "            },",
            "        )",
            "",
            "        event, context = await self.event_creation_handler.create_new_client_event(",
            "            builder=builder",
            "        )",
            "",
            "        try:",
            "            # The remote hasn't signed it yet, obviously. We'll do the full checks",
            "            # when we get the event back in `on_send_leave_request`",
            "            await self.auth.check_from_context(",
            "                room_version, event, context, do_sig_check=False",
            "            )",
            "        except AuthError as e:",
            "            logger.warning(\"Failed to create new leave %r because %s\", event, e)",
            "            raise e",
            "",
            "        return event",
            "",
            "    async def on_send_leave_request(self, origin, pdu):",
            "        \"\"\" We have received a leave event for a room. Fully process it.\"\"\"",
            "        event = pdu",
            "",
            "        logger.debug(",
            "            \"on_send_leave_request: Got event: %s, signatures: %s\",",
            "            event.event_id,",
            "            event.signatures,",
            "        )",
            "",
            "        if get_domain_from_id(event.sender) != origin:",
            "            logger.info(",
            "                \"Got /send_leave request for user %r from different origin %s\",",
            "                event.sender,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        event.internal_metadata.outlier = False",
            "",
            "        await self._handle_new_event(origin, event)",
            "",
            "        logger.debug(",
            "            \"on_send_leave_request: After _handle_new_event: %s, sigs: %s\",",
            "            event.event_id,",
            "            event.signatures,",
            "        )",
            "",
            "        return None",
            "",
            "    async def get_state_for_pdu(self, room_id: str, event_id: str) -> List[EventBase]:",
            "        \"\"\"Returns the state at the event. i.e. not including said event.",
            "        \"\"\"",
            "",
            "        event = await self.store.get_event(event_id, check_room_id=room_id)",
            "",
            "        state_groups = await self.state_store.get_state_groups(room_id, [event_id])",
            "",
            "        if state_groups:",
            "            _, state = list(state_groups.items()).pop()",
            "            results = {(e.type, e.state_key): e for e in state}",
            "",
            "            if event.is_state():",
            "                # Get previous state",
            "                if \"replaces_state\" in event.unsigned:",
            "                    prev_id = event.unsigned[\"replaces_state\"]",
            "                    if prev_id != event.event_id:",
            "                        prev_event = await self.store.get_event(prev_id)",
            "                        results[(event.type, event.state_key)] = prev_event",
            "                else:",
            "                    del results[(event.type, event.state_key)]",
            "",
            "            res = list(results.values())",
            "            return res",
            "        else:",
            "            return []",
            "",
            "    async def get_state_ids_for_pdu(self, room_id: str, event_id: str) -> List[str]:",
            "        \"\"\"Returns the state at the event. i.e. not including said event.",
            "        \"\"\"",
            "        event = await self.store.get_event(event_id, check_room_id=room_id)",
            "",
            "        state_groups = await self.state_store.get_state_groups_ids(room_id, [event_id])",
            "",
            "        if state_groups:",
            "            _, state = list(state_groups.items()).pop()",
            "            results = state",
            "",
            "            if event.is_state():",
            "                # Get previous state",
            "                if \"replaces_state\" in event.unsigned:",
            "                    prev_id = event.unsigned[\"replaces_state\"]",
            "                    if prev_id != event.event_id:",
            "                        results[(event.type, event.state_key)] = prev_id",
            "                else:",
            "                    results.pop((event.type, event.state_key), None)",
            "",
            "            return list(results.values())",
            "        else:",
            "            return []",
            "",
            "    @log_function",
            "    async def on_backfill_request(",
            "        self, origin: str, room_id: str, pdu_list: List[str], limit: int",
            "    ) -> List[EventBase]:",
            "        in_room = await self.auth.check_host_in_room(room_id, origin)",
            "        if not in_room:",
            "            raise AuthError(403, \"Host not in room.\")",
            "",
            "        # Synapse asks for 100 events per backfill request. Do not allow more.",
            "        limit = min(limit, 100)",
            "",
            "        events = await self.store.get_backfill_events(room_id, pdu_list, limit)",
            "",
            "        events = await filter_events_for_server(self.storage, origin, events)",
            "",
            "        return events",
            "",
            "    @log_function",
            "    async def get_persisted_pdu(",
            "        self, origin: str, event_id: str",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"Get an event from the database for the given server.",
            "",
            "        Args:",
            "            origin: hostname of server which is requesting the event; we",
            "               will check that the server is allowed to see it.",
            "            event_id: id of the event being requested",
            "",
            "        Returns:",
            "            None if we know nothing about the event; otherwise the (possibly-redacted) event.",
            "",
            "        Raises:",
            "            AuthError if the server is not currently in the room",
            "        \"\"\"",
            "        event = await self.store.get_event(",
            "            event_id, allow_none=True, allow_rejected=True",
            "        )",
            "",
            "        if event:",
            "            in_room = await self.auth.check_host_in_room(event.room_id, origin)",
            "            if not in_room:",
            "                raise AuthError(403, \"Host not in room.\")",
            "",
            "            events = await filter_events_for_server(self.storage, origin, [event])",
            "            event = events[0]",
            "            return event",
            "        else:",
            "            return None",
            "",
            "    async def get_min_depth_for_context(self, context):",
            "        return await self.store.get_min_depth(context)",
            "",
            "    async def _handle_new_event(",
            "        self, origin, event, state=None, auth_events=None, backfilled=False",
            "    ):",
            "        context = await self._prep_event(",
            "            origin, event, state=state, auth_events=auth_events, backfilled=backfilled",
            "        )",
            "",
            "        try:",
            "            if (",
            "                not event.internal_metadata.is_outlier()",
            "                and not backfilled",
            "                and not context.rejected",
            "            ):",
            "                await self.action_generator.handle_push_actions_for_event(",
            "                    event, context",
            "                )",
            "",
            "            await self.persist_events_and_notify(",
            "                event.room_id, [(event, context)], backfilled=backfilled",
            "            )",
            "        except Exception:",
            "            run_in_background(",
            "                self.store.remove_push_actions_from_staging, event.event_id",
            "            )",
            "            raise",
            "",
            "        return context",
            "",
            "    async def _handle_new_events(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        event_infos: Iterable[_NewEventInfo],",
            "        backfilled: bool = False,",
            "    ) -> None:",
            "        \"\"\"Creates the appropriate contexts and persists events. The events",
            "        should not depend on one another, e.g. this should be used to persist",
            "        a bunch of outliers, but not a chunk of individual events that depend",
            "        on each other for state calculations.",
            "",
            "        Notifies about the events where appropriate.",
            "        \"\"\"",
            "",
            "        async def prep(ev_info: _NewEventInfo):",
            "            event = ev_info.event",
            "            with nested_logging_context(suffix=event.event_id):",
            "                res = await self._prep_event(",
            "                    origin,",
            "                    event,",
            "                    state=ev_info.state,",
            "                    auth_events=ev_info.auth_events,",
            "                    backfilled=backfilled,",
            "                )",
            "            return res",
            "",
            "        contexts = await make_deferred_yieldable(",
            "            defer.gatherResults(",
            "                [run_in_background(prep, ev_info) for ev_info in event_infos],",
            "                consumeErrors=True,",
            "            )",
            "        )",
            "",
            "        await self.persist_events_and_notify(",
            "            room_id,",
            "            [",
            "                (ev_info.event, context)",
            "                for ev_info, context in zip(event_infos, contexts)",
            "            ],",
            "            backfilled=backfilled,",
            "        )",
            "",
            "    async def _persist_auth_tree(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        auth_events: List[EventBase],",
            "        state: List[EventBase],",
            "        event: EventBase,",
            "        room_version: RoomVersion,",
            "    ) -> int:",
            "        \"\"\"Checks the auth chain is valid (and passes auth checks) for the",
            "        state and event. Then persists the auth chain and state atomically.",
            "        Persists the event separately. Notifies about the persisted events",
            "        where appropriate.",
            "",
            "        Will attempt to fetch missing auth events.",
            "",
            "        Args:",
            "            origin: Where the events came from",
            "            room_id,",
            "            auth_events",
            "            state",
            "            event",
            "            room_version: The room version we expect this room to have, and",
            "                will raise if it doesn't match the version in the create event.",
            "        \"\"\"",
            "        events_to_context = {}",
            "        for e in itertools.chain(auth_events, state):",
            "            e.internal_metadata.outlier = True",
            "            ctx = await self.state_handler.compute_event_context(e)",
            "            events_to_context[e.event_id] = ctx",
            "",
            "        event_map = {",
            "            e.event_id: e for e in itertools.chain(auth_events, state, [event])",
            "        }",
            "",
            "        create_event = None",
            "        for e in auth_events:",
            "            if (e.type, e.state_key) == (EventTypes.Create, \"\"):",
            "                create_event = e",
            "                break",
            "",
            "        if create_event is None:",
            "            # If the state doesn't have a create event then the room is",
            "            # invalid, and it would fail auth checks anyway.",
            "            raise SynapseError(400, \"No create event in state\")",
            "",
            "        room_version_id = create_event.content.get(",
            "            \"room_version\", RoomVersions.V1.identifier",
            "        )",
            "",
            "        if room_version.identifier != room_version_id:",
            "            raise SynapseError(400, \"Room version mismatch\")",
            "",
            "        missing_auth_events = set()",
            "        for e in itertools.chain(auth_events, state, [event]):",
            "            for e_id in e.auth_event_ids():",
            "                if e_id not in event_map:",
            "                    missing_auth_events.add(e_id)",
            "",
            "        for e_id in missing_auth_events:",
            "            m_ev = await self.federation_client.get_pdu(",
            "                [origin], e_id, room_version=room_version, outlier=True, timeout=10000,",
            "            )",
            "            if m_ev and m_ev.event_id == e_id:",
            "                event_map[e_id] = m_ev",
            "            else:",
            "                logger.info(\"Failed to find auth event %r\", e_id)",
            "",
            "        for e in itertools.chain(auth_events, state, [event]):",
            "            auth_for_e = {",
            "                (event_map[e_id].type, event_map[e_id].state_key): event_map[e_id]",
            "                for e_id in e.auth_event_ids()",
            "                if e_id in event_map",
            "            }",
            "            if create_event:",
            "                auth_for_e[(EventTypes.Create, \"\")] = create_event",
            "",
            "            try:",
            "                event_auth.check(room_version, e, auth_events=auth_for_e)",
            "            except SynapseError as err:",
            "                # we may get SynapseErrors here as well as AuthErrors. For",
            "                # instance, there are a couple of (ancient) events in some",
            "                # rooms whose senders do not have the correct sigil; these",
            "                # cause SynapseErrors in auth.check. We don't want to give up",
            "                # the attempt to federate altogether in such cases.",
            "",
            "                logger.warning(\"Rejecting %s because %s\", e.event_id, err.msg)",
            "",
            "                if e == event:",
            "                    raise",
            "                events_to_context[e.event_id].rejected = RejectedReason.AUTH_ERROR",
            "",
            "        await self.persist_events_and_notify(",
            "            room_id,",
            "            [",
            "                (e, events_to_context[e.event_id])",
            "                for e in itertools.chain(auth_events, state)",
            "            ],",
            "        )",
            "",
            "        new_event_context = await self.state_handler.compute_event_context(",
            "            event, old_state=state",
            "        )",
            "",
            "        return await self.persist_events_and_notify(",
            "            room_id, [(event, new_event_context)]",
            "        )",
            "",
            "    async def _prep_event(",
            "        self,",
            "        origin: str,",
            "        event: EventBase,",
            "        state: Optional[Iterable[EventBase]],",
            "        auth_events: Optional[MutableStateMap[EventBase]],",
            "        backfilled: bool,",
            "    ) -> EventContext:",
            "        context = await self.state_handler.compute_event_context(event, old_state=state)",
            "",
            "        if not auth_events:",
            "            prev_state_ids = await context.get_prev_state_ids()",
            "            auth_events_ids = self.auth.compute_auth_events(",
            "                event, prev_state_ids, for_verification=True",
            "            )",
            "            auth_events_x = await self.store.get_events(auth_events_ids)",
            "            auth_events = {(e.type, e.state_key): e for e in auth_events_x.values()}",
            "",
            "        # This is a hack to fix some old rooms where the initial join event",
            "        # didn't reference the create event in its auth events.",
            "        if event.type == EventTypes.Member and not event.auth_event_ids():",
            "            if len(event.prev_event_ids()) == 1 and event.depth < 5:",
            "                c = await self.store.get_event(",
            "                    event.prev_event_ids()[0], allow_none=True",
            "                )",
            "                if c and c.type == EventTypes.Create:",
            "                    auth_events[(c.type, c.state_key)] = c",
            "",
            "        context = await self.do_auth(origin, event, context, auth_events=auth_events)",
            "",
            "        if not context.rejected:",
            "            await self._check_for_soft_fail(event, state, backfilled)",
            "",
            "        if event.type == EventTypes.GuestAccess and not context.rejected:",
            "            await self.maybe_kick_guest_users(event)",
            "",
            "        return context",
            "",
            "    async def _check_for_soft_fail(",
            "        self, event: EventBase, state: Optional[Iterable[EventBase]], backfilled: bool",
            "    ) -> None:",
            "        \"\"\"Checks if we should soft fail the event; if so, marks the event as",
            "        such.",
            "",
            "        Args:",
            "            event",
            "            state: The state at the event if we don't have all the event's prev events",
            "            backfilled: Whether the event is from backfill",
            "        \"\"\"",
            "        # For new (non-backfilled and non-outlier) events we check if the event",
            "        # passes auth based on the current state. If it doesn't then we",
            "        # \"soft-fail\" the event.",
            "        if backfilled or event.internal_metadata.is_outlier():",
            "            return",
            "",
            "        extrem_ids_list = await self.store.get_latest_event_ids_in_room(event.room_id)",
            "        extrem_ids = set(extrem_ids_list)",
            "        prev_event_ids = set(event.prev_event_ids())",
            "",
            "        if extrem_ids == prev_event_ids:",
            "            # If they're the same then the current state is the same as the",
            "            # state at the event, so no point rechecking auth for soft fail.",
            "            return",
            "",
            "        room_version = await self.store.get_room_version_id(event.room_id)",
            "        room_version_obj = KNOWN_ROOM_VERSIONS[room_version]",
            "",
            "        # Calculate the \"current state\".",
            "        if state is not None:",
            "            # If we're explicitly given the state then we won't have all the",
            "            # prev events, and so we have a gap in the graph. In this case",
            "            # we want to be a little careful as we might have been down for",
            "            # a while and have an incorrect view of the current state,",
            "            # however we still want to do checks as gaps are easy to",
            "            # maliciously manufacture.",
            "            #",
            "            # So we use a \"current state\" that is actually a state",
            "            # resolution across the current forward extremities and the",
            "            # given state at the event. This should correctly handle cases",
            "            # like bans, especially with state res v2.",
            "",
            "            state_sets_d = await self.state_store.get_state_groups(",
            "                event.room_id, extrem_ids",
            "            )",
            "            state_sets = list(state_sets_d.values())  # type: List[Iterable[EventBase]]",
            "            state_sets.append(state)",
            "            current_states = await self.state_handler.resolve_events(",
            "                room_version, state_sets, event",
            "            )",
            "            current_state_ids = {",
            "                k: e.event_id for k, e in current_states.items()",
            "            }  # type: StateMap[str]",
            "        else:",
            "            current_state_ids = await self.state_handler.get_current_state_ids(",
            "                event.room_id, latest_event_ids=extrem_ids",
            "            )",
            "",
            "        logger.debug(",
            "            \"Doing soft-fail check for %s: state %s\", event.event_id, current_state_ids,",
            "        )",
            "",
            "        # Now check if event pass auth against said current state",
            "        auth_types = auth_types_for_event(event)",
            "        current_state_ids_list = [",
            "            e for k, e in current_state_ids.items() if k in auth_types",
            "        ]",
            "",
            "        auth_events_map = await self.store.get_events(current_state_ids_list)",
            "        current_auth_events = {",
            "            (e.type, e.state_key): e for e in auth_events_map.values()",
            "        }",
            "",
            "        try:",
            "            event_auth.check(room_version_obj, event, auth_events=current_auth_events)",
            "        except AuthError as e:",
            "            logger.warning(\"Soft-failing %r because %s\", event, e)",
            "            event.internal_metadata.soft_failed = True",
            "",
            "    async def on_query_auth(",
            "        self, origin, event_id, room_id, remote_auth_chain, rejects, missing",
            "    ):",
            "        in_room = await self.auth.check_host_in_room(room_id, origin)",
            "        if not in_room:",
            "            raise AuthError(403, \"Host not in room.\")",
            "",
            "        event = await self.store.get_event(event_id, check_room_id=room_id)",
            "",
            "        # Just go through and process each event in `remote_auth_chain`. We",
            "        # don't want to fall into the trap of `missing` being wrong.",
            "        for e in remote_auth_chain:",
            "            try:",
            "                await self._handle_new_event(origin, e)",
            "            except AuthError:",
            "                pass",
            "",
            "        # Now get the current auth_chain for the event.",
            "        local_auth_chain = await self.store.get_auth_chain(",
            "            list(event.auth_event_ids()), include_given=True",
            "        )",
            "",
            "        # TODO: Check if we would now reject event_id. If so we need to tell",
            "        # everyone.",
            "",
            "        ret = await self.construct_auth_difference(local_auth_chain, remote_auth_chain)",
            "",
            "        logger.debug(\"on_query_auth returning: %s\", ret)",
            "",
            "        return ret",
            "",
            "    async def on_get_missing_events(",
            "        self, origin, room_id, earliest_events, latest_events, limit",
            "    ):",
            "        in_room = await self.auth.check_host_in_room(room_id, origin)",
            "        if not in_room:",
            "            raise AuthError(403, \"Host not in room.\")",
            "",
            "        # Only allow up to 20 events to be retrieved per request.",
            "        limit = min(limit, 20)",
            "",
            "        missing_events = await self.store.get_missing_events(",
            "            room_id=room_id,",
            "            earliest_events=earliest_events,",
            "            latest_events=latest_events,",
            "            limit=limit,",
            "        )",
            "",
            "        missing_events = await filter_events_for_server(",
            "            self.storage, origin, missing_events",
            "        )",
            "",
            "        return missing_events",
            "",
            "    async def do_auth(",
            "        self,",
            "        origin: str,",
            "        event: EventBase,",
            "        context: EventContext,",
            "        auth_events: MutableStateMap[EventBase],",
            "    ) -> EventContext:",
            "        \"\"\"",
            "",
            "        Args:",
            "            origin:",
            "            event:",
            "            context:",
            "            auth_events:",
            "                Map from (event_type, state_key) to event",
            "",
            "                Normally, our calculated auth_events based on the state of the room",
            "                at the event's position in the DAG, though occasionally (eg if the",
            "                event is an outlier), may be the auth events claimed by the remote",
            "                server.",
            "",
            "                Also NB that this function adds entries to it.",
            "        Returns:",
            "            updated context object",
            "        \"\"\"",
            "        room_version = await self.store.get_room_version_id(event.room_id)",
            "        room_version_obj = KNOWN_ROOM_VERSIONS[room_version]",
            "",
            "        try:",
            "            context = await self._update_auth_events_and_context_for_auth(",
            "                origin, event, context, auth_events",
            "            )",
            "        except Exception:",
            "            # We don't really mind if the above fails, so lets not fail",
            "            # processing if it does. However, it really shouldn't fail so",
            "            # let's still log as an exception since we'll still want to fix",
            "            # any bugs.",
            "            logger.exception(",
            "                \"Failed to double check auth events for %s with remote. \"",
            "                \"Ignoring failure and continuing processing of event.\",",
            "                event.event_id,",
            "            )",
            "",
            "        try:",
            "            event_auth.check(room_version_obj, event, auth_events=auth_events)",
            "        except AuthError as e:",
            "            logger.warning(\"Failed auth resolution for %r because %s\", event, e)",
            "            context.rejected = RejectedReason.AUTH_ERROR",
            "",
            "        return context",
            "",
            "    async def _update_auth_events_and_context_for_auth(",
            "        self,",
            "        origin: str,",
            "        event: EventBase,",
            "        context: EventContext,",
            "        auth_events: MutableStateMap[EventBase],",
            "    ) -> EventContext:",
            "        \"\"\"Helper for do_auth. See there for docs.",
            "",
            "        Checks whether a given event has the expected auth events. If it",
            "        doesn't then we talk to the remote server to compare state to see if",
            "        we can come to a consensus (e.g. if one server missed some valid",
            "        state).",
            "",
            "        This attempts to resolve any potential divergence of state between",
            "        servers, but is not essential and so failures should not block further",
            "        processing of the event.",
            "",
            "        Args:",
            "            origin:",
            "            event:",
            "            context:",
            "",
            "            auth_events:",
            "                Map from (event_type, state_key) to event",
            "",
            "                Normally, our calculated auth_events based on the state of the room",
            "                at the event's position in the DAG, though occasionally (eg if the",
            "                event is an outlier), may be the auth events claimed by the remote",
            "                server.",
            "",
            "                Also NB that this function adds entries to it.",
            "",
            "        Returns:",
            "            updated context",
            "        \"\"\"",
            "        event_auth_events = set(event.auth_event_ids())",
            "",
            "        # missing_auth is the set of the event's auth_events which we don't yet have",
            "        # in auth_events.",
            "        missing_auth = event_auth_events.difference(",
            "            e.event_id for e in auth_events.values()",
            "        )",
            "",
            "        # if we have missing events, we need to fetch those events from somewhere.",
            "        #",
            "        # we start by checking if they are in the store, and then try calling /event_auth/.",
            "        if missing_auth:",
            "            have_events = await self.store.have_seen_events(missing_auth)",
            "            logger.debug(\"Events %s are in the store\", have_events)",
            "            missing_auth.difference_update(have_events)",
            "",
            "        if missing_auth:",
            "            # If we don't have all the auth events, we need to get them.",
            "            logger.info(\"auth_events contains unknown events: %s\", missing_auth)",
            "            try:",
            "                try:",
            "                    remote_auth_chain = await self.federation_client.get_event_auth(",
            "                        origin, event.room_id, event.event_id",
            "                    )",
            "                except RequestSendFailed as e1:",
            "                    # The other side isn't around or doesn't implement the",
            "                    # endpoint, so lets just bail out.",
            "                    logger.info(\"Failed to get event auth from remote: %s\", e1)",
            "                    return context",
            "",
            "                seen_remotes = await self.store.have_seen_events(",
            "                    [e.event_id for e in remote_auth_chain]",
            "                )",
            "",
            "                for e in remote_auth_chain:",
            "                    if e.event_id in seen_remotes:",
            "                        continue",
            "",
            "                    if e.event_id == event.event_id:",
            "                        continue",
            "",
            "                    try:",
            "                        auth_ids = e.auth_event_ids()",
            "                        auth = {",
            "                            (e.type, e.state_key): e",
            "                            for e in remote_auth_chain",
            "                            if e.event_id in auth_ids or e.type == EventTypes.Create",
            "                        }",
            "                        e.internal_metadata.outlier = True",
            "",
            "                        logger.debug(",
            "                            \"do_auth %s missing_auth: %s\", event.event_id, e.event_id",
            "                        )",
            "                        await self._handle_new_event(origin, e, auth_events=auth)",
            "",
            "                        if e.event_id in event_auth_events:",
            "                            auth_events[(e.type, e.state_key)] = e",
            "                    except AuthError:",
            "                        pass",
            "",
            "            except Exception:",
            "                logger.exception(\"Failed to get auth chain\")",
            "",
            "        if event.internal_metadata.is_outlier():",
            "            # XXX: given that, for an outlier, we'll be working with the",
            "            # event's *claimed* auth events rather than those we calculated:",
            "            # (a) is there any point in this test, since different_auth below will",
            "            # obviously be empty",
            "            # (b) alternatively, why don't we do it earlier?",
            "            logger.info(\"Skipping auth_event fetch for outlier\")",
            "            return context",
            "",
            "        different_auth = event_auth_events.difference(",
            "            e.event_id for e in auth_events.values()",
            "        )",
            "",
            "        if not different_auth:",
            "            return context",
            "",
            "        logger.info(",
            "            \"auth_events refers to events which are not in our calculated auth \"",
            "            \"chain: %s\",",
            "            different_auth,",
            "        )",
            "",
            "        # XXX: currently this checks for redactions but I'm not convinced that is",
            "        # necessary?",
            "        different_events = await self.store.get_events_as_list(different_auth)",
            "",
            "        for d in different_events:",
            "            if d.room_id != event.room_id:",
            "                logger.warning(",
            "                    \"Event %s refers to auth_event %s which is in a different room\",",
            "                    event.event_id,",
            "                    d.event_id,",
            "                )",
            "",
            "                # don't attempt to resolve the claimed auth events against our own",
            "                # in this case: just use our own auth events.",
            "                #",
            "                # XXX: should we reject the event in this case? It feels like we should,",
            "                # but then shouldn't we also do so if we've failed to fetch any of the",
            "                # auth events?",
            "                return context",
            "",
            "        # now we state-resolve between our own idea of the auth events, and the remote's",
            "        # idea of them.",
            "",
            "        local_state = auth_events.values()",
            "        remote_auth_events = dict(auth_events)",
            "        remote_auth_events.update({(d.type, d.state_key): d for d in different_events})",
            "        remote_state = remote_auth_events.values()",
            "",
            "        room_version = await self.store.get_room_version_id(event.room_id)",
            "        new_state = await self.state_handler.resolve_events(",
            "            room_version, (local_state, remote_state), event",
            "        )",
            "",
            "        logger.info(",
            "            \"After state res: updating auth_events with new state %s\",",
            "            {",
            "                (d.type, d.state_key): d.event_id",
            "                for d in new_state.values()",
            "                if auth_events.get((d.type, d.state_key)) != d",
            "            },",
            "        )",
            "",
            "        auth_events.update(new_state)",
            "",
            "        context = await self._update_context_for_auth_events(",
            "            event, context, auth_events",
            "        )",
            "",
            "        return context",
            "",
            "    async def _update_context_for_auth_events(",
            "        self, event: EventBase, context: EventContext, auth_events: StateMap[EventBase]",
            "    ) -> EventContext:",
            "        \"\"\"Update the state_ids in an event context after auth event resolution,",
            "        storing the changes as a new state group.",
            "",
            "        Args:",
            "            event: The event we're handling the context for",
            "",
            "            context: initial event context",
            "",
            "            auth_events: Events to update in the event context.",
            "",
            "        Returns:",
            "            new event context",
            "        \"\"\"",
            "        # exclude the state key of the new event from the current_state in the context.",
            "        if event.is_state():",
            "            event_key = (event.type, event.state_key)  # type: Optional[Tuple[str, str]]",
            "        else:",
            "            event_key = None",
            "        state_updates = {",
            "            k: a.event_id for k, a in auth_events.items() if k != event_key",
            "        }",
            "",
            "        current_state_ids = await context.get_current_state_ids()",
            "        current_state_ids = dict(current_state_ids)  # type: ignore",
            "",
            "        current_state_ids.update(state_updates)",
            "",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "        prev_state_ids = dict(prev_state_ids)",
            "",
            "        prev_state_ids.update({k: a.event_id for k, a in auth_events.items()})",
            "",
            "        # create a new state group as a delta from the existing one.",
            "        prev_group = context.state_group",
            "        state_group = await self.state_store.store_state_group(",
            "            event.event_id,",
            "            event.room_id,",
            "            prev_group=prev_group,",
            "            delta_ids=state_updates,",
            "            current_state_ids=current_state_ids,",
            "        )",
            "",
            "        return EventContext.with_state(",
            "            state_group=state_group,",
            "            state_group_before_event=context.state_group_before_event,",
            "            current_state_ids=current_state_ids,",
            "            prev_state_ids=prev_state_ids,",
            "            prev_group=prev_group,",
            "            delta_ids=state_updates,",
            "        )",
            "",
            "    async def construct_auth_difference(",
            "        self, local_auth: Iterable[EventBase], remote_auth: Iterable[EventBase]",
            "    ) -> Dict:",
            "        \"\"\" Given a local and remote auth chain, find the differences. This",
            "        assumes that we have already processed all events in remote_auth",
            "",
            "        Params:",
            "            local_auth (list)",
            "            remote_auth (list)",
            "",
            "        Returns:",
            "            dict",
            "        \"\"\"",
            "",
            "        logger.debug(\"construct_auth_difference Start!\")",
            "",
            "        # TODO: Make sure we are OK with local_auth or remote_auth having more",
            "        # auth events in them than strictly necessary.",
            "",
            "        def sort_fun(ev):",
            "            return ev.depth, ev.event_id",
            "",
            "        logger.debug(\"construct_auth_difference after sort_fun!\")",
            "",
            "        # We find the differences by starting at the \"bottom\" of each list",
            "        # and iterating up on both lists. The lists are ordered by depth and",
            "        # then event_id, we iterate up both lists until we find the event ids",
            "        # don't match. Then we look at depth/event_id to see which side is",
            "        # missing that event, and iterate only up that list. Repeat.",
            "",
            "        remote_list = list(remote_auth)",
            "        remote_list.sort(key=sort_fun)",
            "",
            "        local_list = list(local_auth)",
            "        local_list.sort(key=sort_fun)",
            "",
            "        local_iter = iter(local_list)",
            "        remote_iter = iter(remote_list)",
            "",
            "        logger.debug(\"construct_auth_difference before get_next!\")",
            "",
            "        def get_next(it, opt=None):",
            "            try:",
            "                return next(it)",
            "            except Exception:",
            "                return opt",
            "",
            "        current_local = get_next(local_iter)",
            "        current_remote = get_next(remote_iter)",
            "",
            "        logger.debug(\"construct_auth_difference before while\")",
            "",
            "        missing_remotes = []",
            "        missing_locals = []",
            "        while current_local or current_remote:",
            "            if current_remote is None:",
            "                missing_locals.append(current_local)",
            "                current_local = get_next(local_iter)",
            "                continue",
            "",
            "            if current_local is None:",
            "                missing_remotes.append(current_remote)",
            "                current_remote = get_next(remote_iter)",
            "                continue",
            "",
            "            if current_local.event_id == current_remote.event_id:",
            "                current_local = get_next(local_iter)",
            "                current_remote = get_next(remote_iter)",
            "                continue",
            "",
            "            if current_local.depth < current_remote.depth:",
            "                missing_locals.append(current_local)",
            "                current_local = get_next(local_iter)",
            "                continue",
            "",
            "            if current_local.depth > current_remote.depth:",
            "                missing_remotes.append(current_remote)",
            "                current_remote = get_next(remote_iter)",
            "                continue",
            "",
            "            # They have the same depth, so we fall back to the event_id order",
            "            if current_local.event_id < current_remote.event_id:",
            "                missing_locals.append(current_local)",
            "                current_local = get_next(local_iter)",
            "",
            "            if current_local.event_id > current_remote.event_id:",
            "                missing_remotes.append(current_remote)",
            "                current_remote = get_next(remote_iter)",
            "                continue",
            "",
            "        logger.debug(\"construct_auth_difference after while\")",
            "",
            "        # missing locals should be sent to the server",
            "        # We should find why we are missing remotes, as they will have been",
            "        # rejected.",
            "",
            "        # Remove events from missing_remotes if they are referencing a missing",
            "        # remote. We only care about the \"root\" rejected ones.",
            "        missing_remote_ids = [e.event_id for e in missing_remotes]",
            "        base_remote_rejected = list(missing_remotes)",
            "        for e in missing_remotes:",
            "            for e_id in e.auth_event_ids():",
            "                if e_id in missing_remote_ids:",
            "                    try:",
            "                        base_remote_rejected.remove(e)",
            "                    except ValueError:",
            "                        pass",
            "",
            "        reason_map = {}",
            "",
            "        for e in base_remote_rejected:",
            "            reason = await self.store.get_rejection_reason(e.event_id)",
            "            if reason is None:",
            "                # TODO: e is not in the current state, so we should",
            "                # construct some proof of that.",
            "                continue",
            "",
            "            reason_map[e.event_id] = reason",
            "",
            "        logger.debug(\"construct_auth_difference returning\")",
            "",
            "        return {",
            "            \"auth_chain\": local_auth,",
            "            \"rejects\": {",
            "                e.event_id: {\"reason\": reason_map[e.event_id], \"proof\": None}",
            "                for e in base_remote_rejected",
            "            },",
            "            \"missing\": [e.event_id for e in missing_locals],",
            "        }",
            "",
            "    @log_function",
            "    async def exchange_third_party_invite(",
            "        self, sender_user_id, target_user_id, room_id, signed",
            "    ):",
            "        third_party_invite = {\"signed\": signed}",
            "",
            "        event_dict = {",
            "            \"type\": EventTypes.Member,",
            "            \"content\": {",
            "                \"membership\": Membership.INVITE,",
            "                \"third_party_invite\": third_party_invite,",
            "            },",
            "            \"room_id\": room_id,",
            "            \"sender\": sender_user_id,",
            "            \"state_key\": target_user_id,",
            "        }",
            "",
            "        if await self.auth.check_host_in_room(room_id, self.hs.hostname):",
            "            room_version = await self.store.get_room_version_id(room_id)",
            "            builder = self.event_builder_factory.new(room_version, event_dict)",
            "",
            "            EventValidator().validate_builder(builder)",
            "            event, context = await self.event_creation_handler.create_new_client_event(",
            "                builder=builder",
            "            )",
            "",
            "            event, context = await self.add_display_name_to_third_party_invite(",
            "                room_version, event_dict, event, context",
            "            )",
            "",
            "            EventValidator().validate_new(event, self.config)",
            "",
            "            # We need to tell the transaction queue to send this out, even",
            "            # though the sender isn't a local user.",
            "            event.internal_metadata.send_on_behalf_of = self.hs.hostname",
            "",
            "            try:",
            "                await self.auth.check_from_context(room_version, event, context)",
            "            except AuthError as e:",
            "                logger.warning(\"Denying new third party invite %r because %s\", event, e)",
            "                raise e",
            "",
            "            await self._check_signature(event, context)",
            "",
            "            # We retrieve the room member handler here as to not cause a cyclic dependency",
            "            member_handler = self.hs.get_room_member_handler()",
            "            await member_handler.send_membership_event(None, event, context)",
            "        else:",
            "            destinations = {x.split(\":\", 1)[-1] for x in (sender_user_id, room_id)}",
            "            await self.federation_client.forward_third_party_invite(",
            "                destinations, room_id, event_dict",
            "            )",
            "",
            "    async def on_exchange_third_party_invite_request(",
            "        self, event_dict: JsonDict",
            "    ) -> None:",
            "        \"\"\"Handle an exchange_third_party_invite request from a remote server",
            "",
            "        The remote server will call this when it wants to turn a 3pid invite",
            "        into a normal m.room.member invite.",
            "",
            "        Args:",
            "            event_dict: Dictionary containing the event body.",
            "",
            "        \"\"\"",
            "        assert_params_in_dict(event_dict, [\"room_id\"])",
            "        room_version = await self.store.get_room_version_id(event_dict[\"room_id\"])",
            "",
            "        # NB: event_dict has a particular specced format we might need to fudge",
            "        # if we change event formats too much.",
            "        builder = self.event_builder_factory.new(room_version, event_dict)",
            "",
            "        event, context = await self.event_creation_handler.create_new_client_event(",
            "            builder=builder",
            "        )",
            "        event, context = await self.add_display_name_to_third_party_invite(",
            "            room_version, event_dict, event, context",
            "        )",
            "",
            "        try:",
            "            await self.auth.check_from_context(room_version, event, context)",
            "        except AuthError as e:",
            "            logger.warning(\"Denying third party invite %r because %s\", event, e)",
            "            raise e",
            "        await self._check_signature(event, context)",
            "",
            "        # We need to tell the transaction queue to send this out, even",
            "        # though the sender isn't a local user.",
            "        event.internal_metadata.send_on_behalf_of = get_domain_from_id(event.sender)",
            "",
            "        # We retrieve the room member handler here as to not cause a cyclic dependency",
            "        member_handler = self.hs.get_room_member_handler()",
            "        await member_handler.send_membership_event(None, event, context)",
            "",
            "    async def add_display_name_to_third_party_invite(",
            "        self, room_version, event_dict, event, context",
            "    ):",
            "        key = (",
            "            EventTypes.ThirdPartyInvite,",
            "            event.content[\"third_party_invite\"][\"signed\"][\"token\"],",
            "        )",
            "        original_invite = None",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "        original_invite_id = prev_state_ids.get(key)",
            "        if original_invite_id:",
            "            original_invite = await self.store.get_event(",
            "                original_invite_id, allow_none=True",
            "            )",
            "        if original_invite:",
            "            # If the m.room.third_party_invite event's content is empty, it means the",
            "            # invite has been revoked. In this case, we don't have to raise an error here",
            "            # because the auth check will fail on the invite (because it's not able to",
            "            # fetch public keys from the m.room.third_party_invite event's content, which",
            "            # is empty).",
            "            display_name = original_invite.content.get(\"display_name\")",
            "            event_dict[\"content\"][\"third_party_invite\"][\"display_name\"] = display_name",
            "        else:",
            "            logger.info(",
            "                \"Could not find invite event for third_party_invite: %r\", event_dict",
            "            )",
            "            # We don't discard here as this is not the appropriate place to do",
            "            # auth checks. If we need the invite and don't have it then the",
            "            # auth check code will explode appropriately.",
            "",
            "        builder = self.event_builder_factory.new(room_version, event_dict)",
            "        EventValidator().validate_builder(builder)",
            "        event, context = await self.event_creation_handler.create_new_client_event(",
            "            builder=builder",
            "        )",
            "        EventValidator().validate_new(event, self.config)",
            "        return (event, context)",
            "",
            "    async def _check_signature(self, event, context):",
            "        \"\"\"",
            "        Checks that the signature in the event is consistent with its invite.",
            "",
            "        Args:",
            "            event (Event): The m.room.member event to check",
            "            context (EventContext):",
            "",
            "        Raises:",
            "            AuthError: if signature didn't match any keys, or key has been",
            "                revoked,",
            "            SynapseError: if a transient error meant a key couldn't be checked",
            "                for revocation.",
            "        \"\"\"",
            "        signed = event.content[\"third_party_invite\"][\"signed\"]",
            "        token = signed[\"token\"]",
            "",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "        invite_event_id = prev_state_ids.get((EventTypes.ThirdPartyInvite, token))",
            "",
            "        invite_event = None",
            "        if invite_event_id:",
            "            invite_event = await self.store.get_event(invite_event_id, allow_none=True)",
            "",
            "        if not invite_event:",
            "            raise AuthError(403, \"Could not find invite\")",
            "",
            "        logger.debug(\"Checking auth on event %r\", event.content)",
            "",
            "        last_exception = None  # type: Optional[Exception]",
            "",
            "        # for each public key in the 3pid invite event",
            "        for public_key_object in self.hs.get_auth().get_public_keys(invite_event):",
            "            try:",
            "                # for each sig on the third_party_invite block of the actual invite",
            "                for server, signature_block in signed[\"signatures\"].items():",
            "                    for key_name, encoded_signature in signature_block.items():",
            "                        if not key_name.startswith(\"ed25519:\"):",
            "                            continue",
            "",
            "                        logger.debug(",
            "                            \"Attempting to verify sig with key %s from %r \"",
            "                            \"against pubkey %r\",",
            "                            key_name,",
            "                            server,",
            "                            public_key_object,",
            "                        )",
            "",
            "                        try:",
            "                            public_key = public_key_object[\"public_key\"]",
            "                            verify_key = decode_verify_key_bytes(",
            "                                key_name, decode_base64(public_key)",
            "                            )",
            "                            verify_signed_json(signed, server, verify_key)",
            "                            logger.debug(",
            "                                \"Successfully verified sig with key %s from %r \"",
            "                                \"against pubkey %r\",",
            "                                key_name,",
            "                                server,",
            "                                public_key_object,",
            "                            )",
            "                        except Exception:",
            "                            logger.info(",
            "                                \"Failed to verify sig with key %s from %r \"",
            "                                \"against pubkey %r\",",
            "                                key_name,",
            "                                server,",
            "                                public_key_object,",
            "                            )",
            "                            raise",
            "                        try:",
            "                            if \"key_validity_url\" in public_key_object:",
            "                                await self._check_key_revocation(",
            "                                    public_key, public_key_object[\"key_validity_url\"]",
            "                                )",
            "                        except Exception:",
            "                            logger.info(",
            "                                \"Failed to query key_validity_url %s\",",
            "                                public_key_object[\"key_validity_url\"],",
            "                            )",
            "                            raise",
            "                        return",
            "            except Exception as e:",
            "                last_exception = e",
            "",
            "        if last_exception is None:",
            "            # we can only get here if get_public_keys() returned an empty list",
            "            # TODO: make this better",
            "            raise RuntimeError(\"no public key in invite event\")",
            "",
            "        raise last_exception",
            "",
            "    async def _check_key_revocation(self, public_key, url):",
            "        \"\"\"",
            "        Checks whether public_key has been revoked.",
            "",
            "        Args:",
            "            public_key (str): base-64 encoded public key.",
            "            url (str): Key revocation URL.",
            "",
            "        Raises:",
            "            AuthError: if they key has been revoked.",
            "            SynapseError: if a transient error meant a key couldn't be checked",
            "                for revocation.",
            "        \"\"\"",
            "        try:",
            "            response = await self.http_client.get_json(url, {\"public_key\": public_key})",
            "        except Exception:",
            "            raise SynapseError(502, \"Third party certificate could not be checked\")",
            "        if \"valid\" not in response or not response[\"valid\"]:",
            "            raise AuthError(403, \"Third party certificate was invalid\")",
            "",
            "    async def persist_events_and_notify(",
            "        self,",
            "        room_id: str,",
            "        event_and_contexts: Sequence[Tuple[EventBase, EventContext]],",
            "        backfilled: bool = False,",
            "    ) -> int:",
            "        \"\"\"Persists events and tells the notifier/pushers about them, if",
            "        necessary.",
            "",
            "        Args:",
            "            room_id: The room ID of events being persisted.",
            "            event_and_contexts: Sequence of events with their associated",
            "                context that should be persisted. All events must belong to",
            "                the same room.",
            "            backfilled: Whether these events are a result of",
            "                backfilling or not",
            "        \"\"\"",
            "        instance = self.config.worker.events_shard_config.get_instance(room_id)",
            "        if instance != self._instance_name:",
            "            result = await self._send_events(",
            "                instance_name=instance,",
            "                store=self.store,",
            "                room_id=room_id,",
            "                event_and_contexts=event_and_contexts,",
            "                backfilled=backfilled,",
            "            )",
            "            return result[\"max_stream_id\"]",
            "        else:",
            "            assert self.storage.persistence",
            "",
            "            # Note that this returns the events that were persisted, which may not be",
            "            # the same as were passed in if some were deduplicated due to transaction IDs.",
            "            events, max_stream_token = await self.storage.persistence.persist_events(",
            "                event_and_contexts, backfilled=backfilled",
            "            )",
            "",
            "            if self._ephemeral_messages_enabled:",
            "                for event in events:",
            "                    # If there's an expiry timestamp on the event, schedule its expiry.",
            "                    self._message_handler.maybe_schedule_expiry(event)",
            "",
            "            if not backfilled:  # Never notify for backfilled events",
            "                for event in events:",
            "                    await self._notify_persisted_event(event, max_stream_token)",
            "",
            "            return max_stream_token.stream",
            "",
            "    async def _notify_persisted_event(",
            "        self, event: EventBase, max_stream_token: RoomStreamToken",
            "    ) -> None:",
            "        \"\"\"Checks to see if notifier/pushers should be notified about the",
            "        event or not.",
            "",
            "        Args:",
            "            event:",
            "            max_stream_id: The max_stream_id returned by persist_events",
            "        \"\"\"",
            "",
            "        extra_users = []",
            "        if event.type == EventTypes.Member:",
            "            target_user_id = event.state_key",
            "",
            "            # We notify for memberships if its an invite for one of our",
            "            # users",
            "            if event.internal_metadata.is_outlier():",
            "                if event.membership != Membership.INVITE:",
            "                    if not self.is_mine_id(target_user_id):",
            "                        return",
            "",
            "            target_user = UserID.from_string(target_user_id)",
            "            extra_users.append(target_user)",
            "        elif event.internal_metadata.is_outlier():",
            "            return",
            "",
            "        # the event has been persisted so it should have a stream ordering.",
            "        assert event.internal_metadata.stream_ordering",
            "",
            "        event_pos = PersistedEventPosition(",
            "            self._instance_name, event.internal_metadata.stream_ordering",
            "        )",
            "        self.notifier.on_new_room_event(",
            "            event, event_pos, max_stream_token, extra_users=extra_users",
            "        )",
            "",
            "    async def _clean_room_for_join(self, room_id: str) -> None:",
            "        \"\"\"Called to clean up any data in DB for a given room, ready for the",
            "        server to join the room.",
            "",
            "        Args:",
            "            room_id",
            "        \"\"\"",
            "        if self.config.worker_app:",
            "            await self._clean_room_for_join_client(room_id)",
            "        else:",
            "            await self.store.clean_room_for_join(room_id)",
            "",
            "    async def get_room_complexity(",
            "        self, remote_room_hosts: List[str], room_id: str",
            "    ) -> Optional[dict]:",
            "        \"\"\"",
            "        Fetch the complexity of a remote room over federation.",
            "",
            "        Args:",
            "            remote_room_hosts (list[str]): The remote servers to ask.",
            "            room_id (str): The room ID to ask about.",
            "",
            "        Returns:",
            "            Dict contains the complexity",
            "            metric versions, while None means we could not fetch the complexity.",
            "        \"\"\"",
            "",
            "        for host in remote_room_hosts:",
            "            res = await self.federation_client.get_room_complexity(host, room_id)",
            "",
            "            # We got a result, return it.",
            "            if res:",
            "                return res",
            "",
            "        # We fell off the bottom, couldn't get the complexity from anyone. Oh",
            "        # well.",
            "        return None"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2017-2018 New Vector Ltd",
            "# Copyright 2019 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "\"\"\"Contains handlers for federation events.\"\"\"",
            "",
            "import itertools",
            "import logging",
            "from collections.abc import Container",
            "from http import HTTPStatus",
            "from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union",
            "",
            "import attr",
            "from signedjson.key import decode_verify_key_bytes",
            "from signedjson.sign import verify_signed_json",
            "from unpaddedbase64 import decode_base64",
            "",
            "from twisted.internet import defer",
            "",
            "from synapse import event_auth",
            "from synapse.api.constants import (",
            "    EventTypes,",
            "    Membership,",
            "    RejectedReason,",
            "    RoomEncryptionAlgorithms,",
            ")",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    CodeMessageException,",
            "    Codes,",
            "    FederationDeniedError,",
            "    FederationError,",
            "    HttpResponseException,",
            "    NotFoundError,",
            "    RequestSendFailed,",
            "    SynapseError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion, RoomVersions",
            "from synapse.crypto.event_signing import compute_event_signature",
            "from synapse.event_auth import auth_types_for_event",
            "from synapse.events import EventBase",
            "from synapse.events.snapshot import EventContext",
            "from synapse.events.validator import EventValidator",
            "from synapse.handlers._base import BaseHandler",
            "from synapse.http.servlet import assert_params_in_dict",
            "from synapse.logging.context import (",
            "    make_deferred_yieldable,",
            "    nested_logging_context,",
            "    preserve_fn,",
            "    run_in_background,",
            ")",
            "from synapse.logging.utils import log_function",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.replication.http.devices import ReplicationUserDevicesResyncRestServlet",
            "from synapse.replication.http.federation import (",
            "    ReplicationCleanRoomRestServlet,",
            "    ReplicationFederationSendEventsRestServlet,",
            "    ReplicationStoreRoomOnOutlierMembershipRestServlet,",
            ")",
            "from synapse.state import StateResolutionStore",
            "from synapse.storage.databases.main.events_worker import EventRedactBehaviour",
            "from synapse.types import (",
            "    JsonDict,",
            "    MutableStateMap,",
            "    PersistedEventPosition,",
            "    RoomStreamToken,",
            "    StateMap,",
            "    UserID,",
            "    get_domain_from_id,",
            ")",
            "from synapse.util.async_helpers import Linearizer, concurrently_execute",
            "from synapse.util.retryutils import NotRetryingDestination",
            "from synapse.util.stringutils import shortstr",
            "from synapse.visibility import filter_events_for_server",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "@attr.s(slots=True)",
            "class _NewEventInfo:",
            "    \"\"\"Holds information about a received event, ready for passing to _handle_new_events",
            "",
            "    Attributes:",
            "        event: the received event",
            "",
            "        state: the state at that event",
            "",
            "        auth_events: the auth_event map for that event",
            "    \"\"\"",
            "",
            "    event = attr.ib(type=EventBase)",
            "    state = attr.ib(type=Optional[Sequence[EventBase]], default=None)",
            "    auth_events = attr.ib(type=Optional[MutableStateMap[EventBase]], default=None)",
            "",
            "",
            "class FederationHandler(BaseHandler):",
            "    \"\"\"Handles events that originated from federation.",
            "        Responsible for:",
            "        a) handling received Pdus before handing them on as Events to the rest",
            "        of the homeserver (including auth and state conflict resolutions)",
            "        b) converting events that were produced by local clients that may need",
            "        to be sent to remote homeservers.",
            "        c) doing the necessary dances to invite remote users and join remote",
            "        rooms.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__(hs)",
            "",
            "        self.hs = hs",
            "",
            "        self.store = hs.get_datastore()",
            "        self.storage = hs.get_storage()",
            "        self.state_store = self.storage.state",
            "        self.federation_client = hs.get_federation_client()",
            "        self.state_handler = hs.get_state_handler()",
            "        self._state_resolution_handler = hs.get_state_resolution_handler()",
            "        self.server_name = hs.hostname",
            "        self.keyring = hs.get_keyring()",
            "        self.action_generator = hs.get_action_generator()",
            "        self.is_mine_id = hs.is_mine_id",
            "        self.spam_checker = hs.get_spam_checker()",
            "        self.event_creation_handler = hs.get_event_creation_handler()",
            "        self._message_handler = hs.get_message_handler()",
            "        self._server_notices_mxid = hs.config.server_notices_mxid",
            "        self.config = hs.config",
            "        self.http_client = hs.get_proxied_blacklisted_http_client()",
            "        self._instance_name = hs.get_instance_name()",
            "        self._replication = hs.get_replication_data_handler()",
            "",
            "        self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)",
            "        self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(",
            "            hs",
            "        )",
            "",
            "        if hs.config.worker_app:",
            "            self._user_device_resync = ReplicationUserDevicesResyncRestServlet.make_client(",
            "                hs",
            "            )",
            "            self._maybe_store_room_on_outlier_membership = ReplicationStoreRoomOnOutlierMembershipRestServlet.make_client(",
            "                hs",
            "            )",
            "        else:",
            "            self._device_list_updater = hs.get_device_handler().device_list_updater",
            "            self._maybe_store_room_on_outlier_membership = (",
            "                self.store.maybe_store_room_on_outlier_membership",
            "            )",
            "",
            "        # When joining a room we need to queue any events for that room up.",
            "        # For each room, a list of (pdu, origin) tuples.",
            "        self.room_queues = {}  # type: Dict[str, List[Tuple[EventBase, str]]]",
            "        self._room_pdu_linearizer = Linearizer(\"fed_room_pdu\")",
            "",
            "        self.third_party_event_rules = hs.get_third_party_event_rules()",
            "",
            "        self._ephemeral_messages_enabled = hs.config.enable_ephemeral_messages",
            "",
            "    async def on_receive_pdu(self, origin, pdu, sent_to_us_directly=False) -> None:",
            "        \"\"\" Process a PDU received via a federation /send/ transaction, or",
            "        via backfill of missing prev_events",
            "",
            "        Args:",
            "            origin (str): server which initiated the /send/ transaction. Will",
            "                be used to fetch missing events or state.",
            "            pdu (FrozenEvent): received PDU",
            "            sent_to_us_directly (bool): True if this event was pushed to us; False if",
            "                we pulled it as the result of a missing prev_event.",
            "        \"\"\"",
            "",
            "        room_id = pdu.room_id",
            "        event_id = pdu.event_id",
            "",
            "        logger.info(\"handling received PDU: %s\", pdu)",
            "",
            "        # We reprocess pdus when we have seen them only as outliers",
            "        existing = await self.store.get_event(",
            "            event_id, allow_none=True, allow_rejected=True",
            "        )",
            "",
            "        # FIXME: Currently we fetch an event again when we already have it",
            "        # if it has been marked as an outlier.",
            "",
            "        already_seen = existing and (",
            "            not existing.internal_metadata.is_outlier()",
            "            or pdu.internal_metadata.is_outlier()",
            "        )",
            "        if already_seen:",
            "            logger.debug(\"[%s %s]: Already seen pdu\", room_id, event_id)",
            "            return",
            "",
            "        # do some initial sanity-checking of the event. In particular, make",
            "        # sure it doesn't have hundreds of prev_events or auth_events, which",
            "        # could cause a huge state resolution or cascade of event fetches.",
            "        try:",
            "            self._sanity_check_event(pdu)",
            "        except SynapseError as err:",
            "            logger.warning(",
            "                \"[%s %s] Received event failed sanity checks\", room_id, event_id",
            "            )",
            "            raise FederationError(\"ERROR\", err.code, err.msg, affected=pdu.event_id)",
            "",
            "        # If we are currently in the process of joining this room, then we",
            "        # queue up events for later processing.",
            "        if room_id in self.room_queues:",
            "            logger.info(",
            "                \"[%s %s] Queuing PDU from %s for now: join in progress\",",
            "                room_id,",
            "                event_id,",
            "                origin,",
            "            )",
            "            self.room_queues[room_id].append((pdu, origin))",
            "            return",
            "",
            "        # If we're not in the room just ditch the event entirely. This is",
            "        # probably an old server that has come back and thinks we're still in",
            "        # the room (or we've been rejoined to the room by a state reset).",
            "        #",
            "        # Note that if we were never in the room then we would have already",
            "        # dropped the event, since we wouldn't know the room version.",
            "        is_in_room = await self.auth.check_host_in_room(room_id, self.server_name)",
            "        if not is_in_room:",
            "            logger.info(",
            "                \"[%s %s] Ignoring PDU from %s as we're not in the room\",",
            "                room_id,",
            "                event_id,",
            "                origin,",
            "            )",
            "            return None",
            "",
            "        state = None",
            "",
            "        # Get missing pdus if necessary.",
            "        if not pdu.internal_metadata.is_outlier():",
            "            # We only backfill backwards to the min depth.",
            "            min_depth = await self.get_min_depth_for_context(pdu.room_id)",
            "",
            "            logger.debug(\"[%s %s] min_depth: %d\", room_id, event_id, min_depth)",
            "",
            "            prevs = set(pdu.prev_event_ids())",
            "            seen = await self.store.have_events_in_timeline(prevs)",
            "",
            "            if min_depth is not None and pdu.depth < min_depth:",
            "                # This is so that we don't notify the user about this",
            "                # message, to work around the fact that some events will",
            "                # reference really really old events we really don't want to",
            "                # send to the clients.",
            "                pdu.internal_metadata.outlier = True",
            "            elif min_depth is not None and pdu.depth > min_depth:",
            "                missing_prevs = prevs - seen",
            "                if sent_to_us_directly and missing_prevs:",
            "                    # If we're missing stuff, ensure we only fetch stuff one",
            "                    # at a time.",
            "                    logger.info(",
            "                        \"[%s %s] Acquiring room lock to fetch %d missing prev_events: %s\",",
            "                        room_id,",
            "                        event_id,",
            "                        len(missing_prevs),",
            "                        shortstr(missing_prevs),",
            "                    )",
            "                    with (await self._room_pdu_linearizer.queue(pdu.room_id)):",
            "                        logger.info(",
            "                            \"[%s %s] Acquired room lock to fetch %d missing prev_events\",",
            "                            room_id,",
            "                            event_id,",
            "                            len(missing_prevs),",
            "                        )",
            "",
            "                        try:",
            "                            await self._get_missing_events_for_pdu(",
            "                                origin, pdu, prevs, min_depth",
            "                            )",
            "                        except Exception as e:",
            "                            raise Exception(",
            "                                \"Error fetching missing prev_events for %s: %s\"",
            "                                % (event_id, e)",
            "                            ) from e",
            "",
            "                        # Update the set of things we've seen after trying to",
            "                        # fetch the missing stuff",
            "                        seen = await self.store.have_events_in_timeline(prevs)",
            "",
            "                        if not prevs - seen:",
            "                            logger.info(",
            "                                \"[%s %s] Found all missing prev_events\",",
            "                                room_id,",
            "                                event_id,",
            "                            )",
            "",
            "            if prevs - seen:",
            "                # We've still not been able to get all of the prev_events for this event.",
            "                #",
            "                # In this case, we need to fall back to asking another server in the",
            "                # federation for the state at this event. That's ok provided we then",
            "                # resolve the state against other bits of the DAG before using it (which",
            "                # will ensure that you can't just take over a room by sending an event,",
            "                # withholding its prev_events, and declaring yourself to be an admin in",
            "                # the subsequent state request).",
            "                #",
            "                # Now, if we're pulling this event as a missing prev_event, then clearly",
            "                # this event is not going to become the only forward-extremity and we are",
            "                # guaranteed to resolve its state against our existing forward",
            "                # extremities, so that should be fine.",
            "                #",
            "                # On the other hand, if this event was pushed to us, it is possible for",
            "                # it to become the only forward-extremity in the room, and we would then",
            "                # trust its state to be the state for the whole room. This is very bad.",
            "                # Further, if the event was pushed to us, there is no excuse for us not to",
            "                # have all the prev_events. We therefore reject any such events.",
            "                #",
            "                # XXX this really feels like it could/should be merged with the above,",
            "                # but there is an interaction with min_depth that I'm not really",
            "                # following.",
            "",
            "                if sent_to_us_directly:",
            "                    logger.warning(",
            "                        \"[%s %s] Rejecting: failed to fetch %d prev events: %s\",",
            "                        room_id,",
            "                        event_id,",
            "                        len(prevs - seen),",
            "                        shortstr(prevs - seen),",
            "                    )",
            "                    raise FederationError(",
            "                        \"ERROR\",",
            "                        403,",
            "                        (",
            "                            \"Your server isn't divulging details about prev_events \"",
            "                            \"referenced in this event.\"",
            "                        ),",
            "                        affected=pdu.event_id,",
            "                    )",
            "",
            "                logger.info(",
            "                    \"Event %s is missing prev_events: calculating state for a \"",
            "                    \"backwards extremity\",",
            "                    event_id,",
            "                )",
            "",
            "                # Calculate the state after each of the previous events, and",
            "                # resolve them to find the correct state at the current event.",
            "                event_map = {event_id: pdu}",
            "                try:",
            "                    # Get the state of the events we know about",
            "                    ours = await self.state_store.get_state_groups_ids(room_id, seen)",
            "",
            "                    # state_maps is a list of mappings from (type, state_key) to event_id",
            "                    state_maps = list(ours.values())  # type: List[StateMap[str]]",
            "",
            "                    # we don't need this any more, let's delete it.",
            "                    del ours",
            "",
            "                    # Ask the remote server for the states we don't",
            "                    # know about",
            "                    for p in prevs - seen:",
            "                        logger.info(",
            "                            \"Requesting state at missing prev_event %s\", event_id,",
            "                        )",
            "",
            "                        with nested_logging_context(p):",
            "                            # note that if any of the missing prevs share missing state or",
            "                            # auth events, the requests to fetch those events are deduped",
            "                            # by the get_pdu_cache in federation_client.",
            "                            (remote_state, _,) = await self._get_state_for_room(",
            "                                origin, room_id, p, include_event_in_state=True",
            "                            )",
            "",
            "                            remote_state_map = {",
            "                                (x.type, x.state_key): x.event_id for x in remote_state",
            "                            }",
            "                            state_maps.append(remote_state_map)",
            "",
            "                            for x in remote_state:",
            "                                event_map[x.event_id] = x",
            "",
            "                    room_version = await self.store.get_room_version_id(room_id)",
            "                    state_map = await self._state_resolution_handler.resolve_events_with_store(",
            "                        room_id,",
            "                        room_version,",
            "                        state_maps,",
            "                        event_map,",
            "                        state_res_store=StateResolutionStore(self.store),",
            "                    )",
            "",
            "                    # We need to give _process_received_pdu the actual state events",
            "                    # rather than event ids, so generate that now.",
            "",
            "                    # First though we need to fetch all the events that are in",
            "                    # state_map, so we can build up the state below.",
            "                    evs = await self.store.get_events(",
            "                        list(state_map.values()),",
            "                        get_prev_content=False,",
            "                        redact_behaviour=EventRedactBehaviour.AS_IS,",
            "                    )",
            "                    event_map.update(evs)",
            "",
            "                    state = [event_map[e] for e in state_map.values()]",
            "                except Exception:",
            "                    logger.warning(",
            "                        \"[%s %s] Error attempting to resolve state at missing \"",
            "                        \"prev_events\",",
            "                        room_id,",
            "                        event_id,",
            "                        exc_info=True,",
            "                    )",
            "                    raise FederationError(",
            "                        \"ERROR\",",
            "                        403,",
            "                        \"We can't get valid state history.\",",
            "                        affected=event_id,",
            "                    )",
            "",
            "        await self._process_received_pdu(origin, pdu, state=state)",
            "",
            "    async def _get_missing_events_for_pdu(self, origin, pdu, prevs, min_depth):",
            "        \"\"\"",
            "        Args:",
            "            origin (str): Origin of the pdu. Will be called to get the missing events",
            "            pdu: received pdu",
            "            prevs (set(str)): List of event ids which we are missing",
            "            min_depth (int): Minimum depth of events to return.",
            "        \"\"\"",
            "",
            "        room_id = pdu.room_id",
            "        event_id = pdu.event_id",
            "",
            "        seen = await self.store.have_events_in_timeline(prevs)",
            "",
            "        if not prevs - seen:",
            "            return",
            "",
            "        latest_list = await self.store.get_latest_event_ids_in_room(room_id)",
            "",
            "        # We add the prev events that we have seen to the latest",
            "        # list to ensure the remote server doesn't give them to us",
            "        latest = set(latest_list)",
            "        latest |= seen",
            "",
            "        logger.info(",
            "            \"[%s %s]: Requesting missing events between %s and %s\",",
            "            room_id,",
            "            event_id,",
            "            shortstr(latest),",
            "            event_id,",
            "        )",
            "",
            "        # XXX: we set timeout to 10s to help workaround",
            "        # https://github.com/matrix-org/synapse/issues/1733.",
            "        # The reason is to avoid holding the linearizer lock",
            "        # whilst processing inbound /send transactions, causing",
            "        # FDs to stack up and block other inbound transactions",
            "        # which empirically can currently take up to 30 minutes.",
            "        #",
            "        # N.B. this explicitly disables retry attempts.",
            "        #",
            "        # N.B. this also increases our chances of falling back to",
            "        # fetching fresh state for the room if the missing event",
            "        # can't be found, which slightly reduces our security.",
            "        # it may also increase our DAG extremity count for the room,",
            "        # causing additional state resolution?  See #1760.",
            "        # However, fetching state doesn't hold the linearizer lock",
            "        # apparently.",
            "        #",
            "        # see https://github.com/matrix-org/synapse/pull/1744",
            "        #",
            "        # ----",
            "        #",
            "        # Update richvdh 2018/09/18: There are a number of problems with timing this",
            "        # request out aggressively on the client side:",
            "        #",
            "        # - it plays badly with the server-side rate-limiter, which starts tarpitting you",
            "        #   if you send too many requests at once, so you end up with the server carefully",
            "        #   working through the backlog of your requests, which you have already timed",
            "        #   out.",
            "        #",
            "        # - for this request in particular, we now (as of",
            "        #   https://github.com/matrix-org/synapse/pull/3456) reject any PDUs where the",
            "        #   server can't produce a plausible-looking set of prev_events - so we becone",
            "        #   much more likely to reject the event.",
            "        #",
            "        # - contrary to what it says above, we do *not* fall back to fetching fresh state",
            "        #   for the room if get_missing_events times out. Rather, we give up processing",
            "        #   the PDU whose prevs we are missing, which then makes it much more likely that",
            "        #   we'll end up back here for the *next* PDU in the list, which exacerbates the",
            "        #   problem.",
            "        #",
            "        # - the aggressive 10s timeout was introduced to deal with incoming federation",
            "        #   requests taking 8 hours to process. It's not entirely clear why that was going",
            "        #   on; certainly there were other issues causing traffic storms which are now",
            "        #   resolved, and I think in any case we may be more sensible about our locking",
            "        #   now. We're *certainly* more sensible about our logging.",
            "        #",
            "        # All that said: Let's try increasing the timeout to 60s and see what happens.",
            "",
            "        try:",
            "            missing_events = await self.federation_client.get_missing_events(",
            "                origin,",
            "                room_id,",
            "                earliest_events_ids=list(latest),",
            "                latest_events=[pdu],",
            "                limit=10,",
            "                min_depth=min_depth,",
            "                timeout=60000,",
            "            )",
            "        except (RequestSendFailed, HttpResponseException, NotRetryingDestination) as e:",
            "            # We failed to get the missing events, but since we need to handle",
            "            # the case of `get_missing_events` not returning the necessary",
            "            # events anyway, it is safe to simply log the error and continue.",
            "            logger.warning(",
            "                \"[%s %s]: Failed to get prev_events: %s\", room_id, event_id, e",
            "            )",
            "            return",
            "",
            "        logger.info(",
            "            \"[%s %s]: Got %d prev_events: %s\",",
            "            room_id,",
            "            event_id,",
            "            len(missing_events),",
            "            shortstr(missing_events),",
            "        )",
            "",
            "        # We want to sort these by depth so we process them and",
            "        # tell clients about them in order.",
            "        missing_events.sort(key=lambda x: x.depth)",
            "",
            "        for ev in missing_events:",
            "            logger.info(",
            "                \"[%s %s] Handling received prev_event %s\",",
            "                room_id,",
            "                event_id,",
            "                ev.event_id,",
            "            )",
            "            with nested_logging_context(ev.event_id):",
            "                try:",
            "                    await self.on_receive_pdu(origin, ev, sent_to_us_directly=False)",
            "                except FederationError as e:",
            "                    if e.code == 403:",
            "                        logger.warning(",
            "                            \"[%s %s] Received prev_event %s failed history check.\",",
            "                            room_id,",
            "                            event_id,",
            "                            ev.event_id,",
            "                        )",
            "                    else:",
            "                        raise",
            "",
            "    async def _get_state_for_room(",
            "        self,",
            "        destination: str,",
            "        room_id: str,",
            "        event_id: str,",
            "        include_event_in_state: bool = False,",
            "    ) -> Tuple[List[EventBase], List[EventBase]]:",
            "        \"\"\"Requests all of the room state at a given event from a remote homeserver.",
            "",
            "        Args:",
            "            destination: The remote homeserver to query for the state.",
            "            room_id: The id of the room we're interested in.",
            "            event_id: The id of the event we want the state at.",
            "            include_event_in_state: if true, the event itself will be included in the",
            "                returned state event list.",
            "",
            "        Returns:",
            "            A list of events in the state, possibly including the event itself, and",
            "            a list of events in the auth chain for the given event.",
            "        \"\"\"",
            "        (",
            "            state_event_ids,",
            "            auth_event_ids,",
            "        ) = await self.federation_client.get_room_state_ids(",
            "            destination, room_id, event_id=event_id",
            "        )",
            "",
            "        desired_events = set(state_event_ids + auth_event_ids)",
            "",
            "        if include_event_in_state:",
            "            desired_events.add(event_id)",
            "",
            "        event_map = await self._get_events_from_store_or_dest(",
            "            destination, room_id, desired_events",
            "        )",
            "",
            "        failed_to_fetch = desired_events - event_map.keys()",
            "        if failed_to_fetch:",
            "            logger.warning(",
            "                \"Failed to fetch missing state/auth events for %s %s\",",
            "                event_id,",
            "                failed_to_fetch,",
            "            )",
            "",
            "        remote_state = [",
            "            event_map[e_id] for e_id in state_event_ids if e_id in event_map",
            "        ]",
            "",
            "        if include_event_in_state:",
            "            remote_event = event_map.get(event_id)",
            "            if not remote_event:",
            "                raise Exception(\"Unable to get missing prev_event %s\" % (event_id,))",
            "            if remote_event.is_state() and remote_event.rejected_reason is None:",
            "                remote_state.append(remote_event)",
            "",
            "        auth_chain = [event_map[e_id] for e_id in auth_event_ids if e_id in event_map]",
            "        auth_chain.sort(key=lambda e: e.depth)",
            "",
            "        return remote_state, auth_chain",
            "",
            "    async def _get_events_from_store_or_dest(",
            "        self, destination: str, room_id: str, event_ids: Iterable[str]",
            "    ) -> Dict[str, EventBase]:",
            "        \"\"\"Fetch events from a remote destination, checking if we already have them.",
            "",
            "        Persists any events we don't already have as outliers.",
            "",
            "        If we fail to fetch any of the events, a warning will be logged, and the event",
            "        will be omitted from the result. Likewise, any events which turn out not to",
            "        be in the given room.",
            "",
            "        This function *does not* automatically get missing auth events of the",
            "        newly fetched events. Callers must include the full auth chain of",
            "        of the missing events in the `event_ids` argument, to ensure that any",
            "        missing auth events are correctly fetched.",
            "",
            "        Returns:",
            "            map from event_id to event",
            "        \"\"\"",
            "        fetched_events = await self.store.get_events(event_ids, allow_rejected=True)",
            "",
            "        missing_events = set(event_ids) - fetched_events.keys()",
            "",
            "        if missing_events:",
            "            logger.debug(",
            "                \"Fetching unknown state/auth events %s for room %s\",",
            "                missing_events,",
            "                room_id,",
            "            )",
            "",
            "            await self._get_events_and_persist(",
            "                destination=destination, room_id=room_id, events=missing_events",
            "            )",
            "",
            "            # we need to make sure we re-load from the database to get the rejected",
            "            # state correct.",
            "            fetched_events.update(",
            "                (await self.store.get_events(missing_events, allow_rejected=True))",
            "            )",
            "",
            "        # check for events which were in the wrong room.",
            "        #",
            "        # this can happen if a remote server claims that the state or",
            "        # auth_events at an event in room A are actually events in room B",
            "",
            "        bad_events = [",
            "            (event_id, event.room_id)",
            "            for event_id, event in fetched_events.items()",
            "            if event.room_id != room_id",
            "        ]",
            "",
            "        for bad_event_id, bad_room_id in bad_events:",
            "            # This is a bogus situation, but since we may only discover it a long time",
            "            # after it happened, we try our best to carry on, by just omitting the",
            "            # bad events from the returned auth/state set.",
            "            logger.warning(",
            "                \"Remote server %s claims event %s in room %s is an auth/state \"",
            "                \"event in room %s\",",
            "                destination,",
            "                bad_event_id,",
            "                bad_room_id,",
            "                room_id,",
            "            )",
            "",
            "            del fetched_events[bad_event_id]",
            "",
            "        return fetched_events",
            "",
            "    async def _process_received_pdu(",
            "        self, origin: str, event: EventBase, state: Optional[Iterable[EventBase]],",
            "    ):",
            "        \"\"\" Called when we have a new pdu. We need to do auth checks and put it",
            "        through the StateHandler.",
            "",
            "        Args:",
            "            origin: server sending the event",
            "",
            "            event: event to be persisted",
            "",
            "            state: Normally None, but if we are handling a gap in the graph",
            "                (ie, we are missing one or more prev_events), the resolved state at the",
            "                event",
            "        \"\"\"",
            "        room_id = event.room_id",
            "        event_id = event.event_id",
            "",
            "        logger.debug(\"[%s %s] Processing event: %s\", room_id, event_id, event)",
            "",
            "        try:",
            "            await self._handle_new_event(origin, event, state=state)",
            "        except AuthError as e:",
            "            raise FederationError(\"ERROR\", e.code, e.msg, affected=event.event_id)",
            "",
            "        # For encrypted messages we check that we know about the sending device,",
            "        # if we don't then we mark the device cache for that user as stale.",
            "        if event.type == EventTypes.Encrypted:",
            "            device_id = event.content.get(\"device_id\")",
            "            sender_key = event.content.get(\"sender_key\")",
            "",
            "            cached_devices = await self.store.get_cached_devices_for_user(event.sender)",
            "",
            "            resync = False  # Whether we should resync device lists.",
            "",
            "            device = None",
            "            if device_id is not None:",
            "                device = cached_devices.get(device_id)",
            "                if device is None:",
            "                    logger.info(",
            "                        \"Received event from remote device not in our cache: %s %s\",",
            "                        event.sender,",
            "                        device_id,",
            "                    )",
            "                    resync = True",
            "",
            "            # We also check if the `sender_key` matches what we expect.",
            "            if sender_key is not None:",
            "                # Figure out what sender key we're expecting. If we know the",
            "                # device and recognize the algorithm then we can work out the",
            "                # exact key to expect. Otherwise check it matches any key we",
            "                # have for that device.",
            "",
            "                current_keys = []  # type: Container[str]",
            "",
            "                if device:",
            "                    keys = device.get(\"keys\", {}).get(\"keys\", {})",
            "",
            "                    if (",
            "                        event.content.get(\"algorithm\")",
            "                        == RoomEncryptionAlgorithms.MEGOLM_V1_AES_SHA2",
            "                    ):",
            "                        # For this algorithm we expect a curve25519 key.",
            "                        key_name = \"curve25519:%s\" % (device_id,)",
            "                        current_keys = [keys.get(key_name)]",
            "                    else:",
            "                        # We don't know understand the algorithm, so we just",
            "                        # check it matches a key for the device.",
            "                        current_keys = keys.values()",
            "                elif device_id:",
            "                    # We don't have any keys for the device ID.",
            "                    pass",
            "                else:",
            "                    # The event didn't include a device ID, so we just look for",
            "                    # keys across all devices.",
            "                    current_keys = [",
            "                        key",
            "                        for device in cached_devices.values()",
            "                        for key in device.get(\"keys\", {}).get(\"keys\", {}).values()",
            "                    ]",
            "",
            "                # We now check that the sender key matches (one of) the expected",
            "                # keys.",
            "                if sender_key not in current_keys:",
            "                    logger.info(",
            "                        \"Received event from remote device with unexpected sender key: %s %s: %s\",",
            "                        event.sender,",
            "                        device_id or \"<no device_id>\",",
            "                        sender_key,",
            "                    )",
            "                    resync = True",
            "",
            "            if resync:",
            "                run_as_background_process(",
            "                    \"resync_device_due_to_pdu\", self._resync_device, event.sender",
            "                )",
            "",
            "    async def _resync_device(self, sender: str) -> None:",
            "        \"\"\"We have detected that the device list for the given user may be out",
            "        of sync, so we try and resync them.",
            "        \"\"\"",
            "",
            "        try:",
            "            await self.store.mark_remote_user_device_cache_as_stale(sender)",
            "",
            "            # Immediately attempt a resync in the background",
            "            if self.config.worker_app:",
            "                await self._user_device_resync(user_id=sender)",
            "            else:",
            "                await self._device_list_updater.user_device_resync(sender)",
            "        except Exception:",
            "            logger.exception(\"Failed to resync device for %s\", sender)",
            "",
            "    @log_function",
            "    async def backfill(self, dest, room_id, limit, extremities):",
            "        \"\"\" Trigger a backfill request to `dest` for the given `room_id`",
            "",
            "        This will attempt to get more events from the remote. If the other side",
            "        has no new events to offer, this will return an empty list.",
            "",
            "        As the events are received, we check their signatures, and also do some",
            "        sanity-checking on them. If any of the backfilled events are invalid,",
            "        this method throws a SynapseError.",
            "",
            "        TODO: make this more useful to distinguish failures of the remote",
            "        server from invalid events (there is probably no point in trying to",
            "        re-fetch invalid events from every other HS in the room.)",
            "        \"\"\"",
            "        if dest == self.server_name:",
            "            raise SynapseError(400, \"Can't backfill from self.\")",
            "",
            "        events = await self.federation_client.backfill(",
            "            dest, room_id, limit=limit, extremities=extremities",
            "        )",
            "",
            "        if not events:",
            "            return []",
            "",
            "        # ideally we'd sanity check the events here for excess prev_events etc,",
            "        # but it's hard to reject events at this point without completely",
            "        # breaking backfill in the same way that it is currently broken by",
            "        # events whose signature we cannot verify (#3121).",
            "        #",
            "        # So for now we accept the events anyway. #3124 tracks this.",
            "        #",
            "        # for ev in events:",
            "        #     self._sanity_check_event(ev)",
            "",
            "        # Don't bother processing events we already have.",
            "        seen_events = await self.store.have_events_in_timeline(",
            "            {e.event_id for e in events}",
            "        )",
            "",
            "        events = [e for e in events if e.event_id not in seen_events]",
            "",
            "        if not events:",
            "            return []",
            "",
            "        event_map = {e.event_id: e for e in events}",
            "",
            "        event_ids = {e.event_id for e in events}",
            "",
            "        # build a list of events whose prev_events weren't in the batch.",
            "        # (XXX: this will include events whose prev_events we already have; that doesn't",
            "        # sound right?)",
            "        edges = [ev.event_id for ev in events if set(ev.prev_event_ids()) - event_ids]",
            "",
            "        logger.info(\"backfill: Got %d events with %d edges\", len(events), len(edges))",
            "",
            "        # For each edge get the current state.",
            "",
            "        auth_events = {}",
            "        state_events = {}",
            "        events_to_state = {}",
            "        for e_id in edges:",
            "            state, auth = await self._get_state_for_room(",
            "                destination=dest,",
            "                room_id=room_id,",
            "                event_id=e_id,",
            "                include_event_in_state=False,",
            "            )",
            "            auth_events.update({a.event_id: a for a in auth})",
            "            auth_events.update({s.event_id: s for s in state})",
            "            state_events.update({s.event_id: s for s in state})",
            "            events_to_state[e_id] = state",
            "",
            "        required_auth = {",
            "            a_id",
            "            for event in events",
            "            + list(state_events.values())",
            "            + list(auth_events.values())",
            "            for a_id in event.auth_event_ids()",
            "        }",
            "        auth_events.update(",
            "            {e_id: event_map[e_id] for e_id in required_auth if e_id in event_map}",
            "        )",
            "",
            "        ev_infos = []",
            "",
            "        # Step 1: persist the events in the chunk we fetched state for (i.e.",
            "        # the backwards extremities), with custom auth events and state",
            "        for e_id in events_to_state:",
            "            # For paranoia we ensure that these events are marked as",
            "            # non-outliers",
            "            ev = event_map[e_id]",
            "            assert not ev.internal_metadata.is_outlier()",
            "",
            "            ev_infos.append(",
            "                _NewEventInfo(",
            "                    event=ev,",
            "                    state=events_to_state[e_id],",
            "                    auth_events={",
            "                        (",
            "                            auth_events[a_id].type,",
            "                            auth_events[a_id].state_key,",
            "                        ): auth_events[a_id]",
            "                        for a_id in ev.auth_event_ids()",
            "                        if a_id in auth_events",
            "                    },",
            "                )",
            "            )",
            "",
            "        if ev_infos:",
            "            await self._handle_new_events(dest, room_id, ev_infos, backfilled=True)",
            "",
            "        # Step 2: Persist the rest of the events in the chunk one by one",
            "        events.sort(key=lambda e: e.depth)",
            "",
            "        for event in events:",
            "            if event in events_to_state:",
            "                continue",
            "",
            "            # For paranoia we ensure that these events are marked as",
            "            # non-outliers",
            "            assert not event.internal_metadata.is_outlier()",
            "",
            "            # We store these one at a time since each event depends on the",
            "            # previous to work out the state.",
            "            # TODO: We can probably do something more clever here.",
            "            await self._handle_new_event(dest, event, backfilled=True)",
            "",
            "        return events",
            "",
            "    async def maybe_backfill(",
            "        self, room_id: str, current_depth: int, limit: int",
            "    ) -> bool:",
            "        \"\"\"Checks the database to see if we should backfill before paginating,",
            "        and if so do.",
            "",
            "        Args:",
            "            room_id",
            "            current_depth: The depth from which we're paginating from. This is",
            "                used to decide if we should backfill and what extremities to",
            "                use.",
            "            limit: The number of events that the pagination request will",
            "                return. This is used as part of the heuristic to decide if we",
            "                should back paginate.",
            "        \"\"\"",
            "        extremities = await self.store.get_oldest_events_with_depth_in_room(room_id)",
            "",
            "        if not extremities:",
            "            logger.debug(\"Not backfilling as no extremeties found.\")",
            "            return False",
            "",
            "        # We only want to paginate if we can actually see the events we'll get,",
            "        # as otherwise we'll just spend a lot of resources to get redacted",
            "        # events.",
            "        #",
            "        # We do this by filtering all the backwards extremities and seeing if",
            "        # any remain. Given we don't have the extremity events themselves, we",
            "        # need to actually check the events that reference them.",
            "        #",
            "        # *Note*: the spec wants us to keep backfilling until we reach the start",
            "        # of the room in case we are allowed to see some of the history. However",
            "        # in practice that causes more issues than its worth, as a) its",
            "        # relatively rare for there to be any visible history and b) even when",
            "        # there is its often sufficiently long ago that clients would stop",
            "        # attempting to paginate before backfill reached the visible history.",
            "        #",
            "        # TODO: If we do do a backfill then we should filter the backwards",
            "        #   extremities to only include those that point to visible portions of",
            "        #   history.",
            "        #",
            "        # TODO: Correctly handle the case where we are allowed to see the",
            "        #   forward event but not the backward extremity, e.g. in the case of",
            "        #   initial join of the server where we are allowed to see the join",
            "        #   event but not anything before it. This would require looking at the",
            "        #   state *before* the event, ignoring the special casing certain event",
            "        #   types have.",
            "",
            "        forward_events = await self.store.get_successor_events(list(extremities))",
            "",
            "        extremities_events = await self.store.get_events(",
            "            forward_events,",
            "            redact_behaviour=EventRedactBehaviour.AS_IS,",
            "            get_prev_content=False,",
            "        )",
            "",
            "        # We set `check_history_visibility_only` as we might otherwise get false",
            "        # positives from users having been erased.",
            "        filtered_extremities = await filter_events_for_server(",
            "            self.storage,",
            "            self.server_name,",
            "            list(extremities_events.values()),",
            "            redact=False,",
            "            check_history_visibility_only=True,",
            "        )",
            "",
            "        if not filtered_extremities:",
            "            return False",
            "",
            "        # Check if we reached a point where we should start backfilling.",
            "        sorted_extremeties_tuple = sorted(extremities.items(), key=lambda e: -int(e[1]))",
            "        max_depth = sorted_extremeties_tuple[0][1]",
            "",
            "        # If we're approaching an extremity we trigger a backfill, otherwise we",
            "        # no-op.",
            "        #",
            "        # We chose twice the limit here as then clients paginating backwards",
            "        # will send pagination requests that trigger backfill at least twice",
            "        # using the most recent extremity before it gets removed (see below). We",
            "        # chose more than one times the limit in case of failure, but choosing a",
            "        # much larger factor will result in triggering a backfill request much",
            "        # earlier than necessary.",
            "        if current_depth - 2 * limit > max_depth:",
            "            logger.debug(",
            "                \"Not backfilling as we don't need to. %d < %d - 2 * %d\",",
            "                max_depth,",
            "                current_depth,",
            "                limit,",
            "            )",
            "            return False",
            "",
            "        logger.debug(",
            "            \"room_id: %s, backfill: current_depth: %s, max_depth: %s, extrems: %s\",",
            "            room_id,",
            "            current_depth,",
            "            max_depth,",
            "            sorted_extremeties_tuple,",
            "        )",
            "",
            "        # We ignore extremities that have a greater depth than our current depth",
            "        # as:",
            "        #    1. we don't really care about getting events that have happened",
            "        #       before our current position; and",
            "        #    2. we have likely previously tried and failed to backfill from that",
            "        #       extremity, so to avoid getting \"stuck\" requesting the same",
            "        #       backfill repeatedly we drop those extremities.",
            "        filtered_sorted_extremeties_tuple = [",
            "            t for t in sorted_extremeties_tuple if int(t[1]) <= current_depth",
            "        ]",
            "",
            "        # However, we need to check that the filtered extremities are non-empty.",
            "        # If they are empty then either we can a) bail or b) still attempt to",
            "        # backill. We opt to try backfilling anyway just in case we do get",
            "        # relevant events.",
            "        if filtered_sorted_extremeties_tuple:",
            "            sorted_extremeties_tuple = filtered_sorted_extremeties_tuple",
            "",
            "        # We don't want to specify too many extremities as it causes the backfill",
            "        # request URI to be too long.",
            "        extremities = dict(sorted_extremeties_tuple[:5])",
            "",
            "        # Now we need to decide which hosts to hit first.",
            "",
            "        # First we try hosts that are already in the room",
            "        # TODO: HEURISTIC ALERT.",
            "",
            "        curr_state = await self.state_handler.get_current_state(room_id)",
            "",
            "        def get_domains_from_state(state):",
            "            \"\"\"Get joined domains from state",
            "",
            "            Args:",
            "                state (dict[tuple, FrozenEvent]): State map from type/state",
            "                    key to event.",
            "",
            "            Returns:",
            "                list[tuple[str, int]]: Returns a list of servers with the",
            "                lowest depth of their joins. Sorted by lowest depth first.",
            "            \"\"\"",
            "            joined_users = [",
            "                (state_key, int(event.depth))",
            "                for (e_type, state_key), event in state.items()",
            "                if e_type == EventTypes.Member and event.membership == Membership.JOIN",
            "            ]",
            "",
            "            joined_domains = {}  # type: Dict[str, int]",
            "            for u, d in joined_users:",
            "                try:",
            "                    dom = get_domain_from_id(u)",
            "                    old_d = joined_domains.get(dom)",
            "                    if old_d:",
            "                        joined_domains[dom] = min(d, old_d)",
            "                    else:",
            "                        joined_domains[dom] = d",
            "                except Exception:",
            "                    pass",
            "",
            "            return sorted(joined_domains.items(), key=lambda d: d[1])",
            "",
            "        curr_domains = get_domains_from_state(curr_state)",
            "",
            "        likely_domains = [",
            "            domain for domain, depth in curr_domains if domain != self.server_name",
            "        ]",
            "",
            "        async def try_backfill(domains):",
            "            # TODO: Should we try multiple of these at a time?",
            "            for dom in domains:",
            "                try:",
            "                    await self.backfill(",
            "                        dom, room_id, limit=100, extremities=extremities",
            "                    )",
            "                    # If this succeeded then we probably already have the",
            "                    # appropriate stuff.",
            "                    # TODO: We can probably do something more intelligent here.",
            "                    return True",
            "                except SynapseError as e:",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except HttpResponseException as e:",
            "                    if 400 <= e.code < 500:",
            "                        raise e.to_synapse_error()",
            "",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except CodeMessageException as e:",
            "                    if 400 <= e.code < 500:",
            "                        raise",
            "",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except NotRetryingDestination as e:",
            "                    logger.info(str(e))",
            "                    continue",
            "                except RequestSendFailed as e:",
            "                    logger.info(\"Failed to get backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except FederationDeniedError as e:",
            "                    logger.info(e)",
            "                    continue",
            "                except Exception as e:",
            "                    logger.exception(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "",
            "            return False",
            "",
            "        success = await try_backfill(likely_domains)",
            "        if success:",
            "            return True",
            "",
            "        # Huh, well *those* domains didn't work out. Lets try some domains",
            "        # from the time.",
            "",
            "        tried_domains = set(likely_domains)",
            "        tried_domains.add(self.server_name)",
            "",
            "        event_ids = list(extremities.keys())",
            "",
            "        logger.debug(\"calling resolve_state_groups in _maybe_backfill\")",
            "        resolve = preserve_fn(self.state_handler.resolve_state_groups_for_events)",
            "        states = await make_deferred_yieldable(",
            "            defer.gatherResults(",
            "                [resolve(room_id, [e]) for e in event_ids], consumeErrors=True",
            "            )",
            "        )",
            "",
            "        # dict[str, dict[tuple, str]], a map from event_id to state map of",
            "        # event_ids.",
            "        states = dict(zip(event_ids, [s.state for s in states]))",
            "",
            "        state_map = await self.store.get_events(",
            "            [e_id for ids in states.values() for e_id in ids.values()],",
            "            get_prev_content=False,",
            "        )",
            "        states = {",
            "            key: {",
            "                k: state_map[e_id]",
            "                for k, e_id in state_dict.items()",
            "                if e_id in state_map",
            "            }",
            "            for key, state_dict in states.items()",
            "        }",
            "",
            "        for e_id, _ in sorted_extremeties_tuple:",
            "            likely_domains = get_domains_from_state(states[e_id])",
            "",
            "            success = await try_backfill(",
            "                [dom for dom, _ in likely_domains if dom not in tried_domains]",
            "            )",
            "            if success:",
            "                return True",
            "",
            "            tried_domains.update(dom for dom, _ in likely_domains)",
            "",
            "        return False",
            "",
            "    async def _get_events_and_persist(",
            "        self, destination: str, room_id: str, events: Iterable[str]",
            "    ):",
            "        \"\"\"Fetch the given events from a server, and persist them as outliers.",
            "",
            "        This function *does not* recursively get missing auth events of the",
            "        newly fetched events. Callers must include in the `events` argument",
            "        any missing events from the auth chain.",
            "",
            "        Logs a warning if we can't find the given event.",
            "        \"\"\"",
            "",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        event_map = {}  # type: Dict[str, EventBase]",
            "",
            "        async def get_event(event_id: str):",
            "            with nested_logging_context(event_id):",
            "                try:",
            "                    event = await self.federation_client.get_pdu(",
            "                        [destination], event_id, room_version, outlier=True,",
            "                    )",
            "                    if event is None:",
            "                        logger.warning(",
            "                            \"Server %s didn't return event %s\", destination, event_id,",
            "                        )",
            "                        return",
            "",
            "                    event_map[event.event_id] = event",
            "",
            "                except Exception as e:",
            "                    logger.warning(",
            "                        \"Error fetching missing state/auth event %s: %s %s\",",
            "                        event_id,",
            "                        type(e),",
            "                        e,",
            "                    )",
            "",
            "        await concurrently_execute(get_event, events, 5)",
            "",
            "        # Make a map of auth events for each event. We do this after fetching",
            "        # all the events as some of the events' auth events will be in the list",
            "        # of requested events.",
            "",
            "        auth_events = [",
            "            aid",
            "            for event in event_map.values()",
            "            for aid in event.auth_event_ids()",
            "            if aid not in event_map",
            "        ]",
            "        persisted_events = await self.store.get_events(",
            "            auth_events, allow_rejected=True,",
            "        )",
            "",
            "        event_infos = []",
            "        for event in event_map.values():",
            "            auth = {}",
            "            for auth_event_id in event.auth_event_ids():",
            "                ae = persisted_events.get(auth_event_id) or event_map.get(auth_event_id)",
            "                if ae:",
            "                    auth[(ae.type, ae.state_key)] = ae",
            "                else:",
            "                    logger.info(\"Missing auth event %s\", auth_event_id)",
            "",
            "            event_infos.append(_NewEventInfo(event, None, auth))",
            "",
            "        await self._handle_new_events(",
            "            destination, room_id, event_infos,",
            "        )",
            "",
            "    def _sanity_check_event(self, ev):",
            "        \"\"\"",
            "        Do some early sanity checks of a received event",
            "",
            "        In particular, checks it doesn't have an excessive number of",
            "        prev_events or auth_events, which could cause a huge state resolution",
            "        or cascade of event fetches.",
            "",
            "        Args:",
            "            ev (synapse.events.EventBase): event to be checked",
            "",
            "        Returns: None",
            "",
            "        Raises:",
            "            SynapseError if the event does not pass muster",
            "        \"\"\"",
            "        if len(ev.prev_event_ids()) > 20:",
            "            logger.warning(",
            "                \"Rejecting event %s which has %i prev_events\",",
            "                ev.event_id,",
            "                len(ev.prev_event_ids()),",
            "            )",
            "            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many prev_events\")",
            "",
            "        if len(ev.auth_event_ids()) > 10:",
            "            logger.warning(",
            "                \"Rejecting event %s which has %i auth_events\",",
            "                ev.event_id,",
            "                len(ev.auth_event_ids()),",
            "            )",
            "            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many auth_events\")",
            "",
            "    async def send_invite(self, target_host, event):",
            "        \"\"\" Sends the invite to the remote server for signing.",
            "",
            "        Invites must be signed by the invitee's server before distribution.",
            "        \"\"\"",
            "        pdu = await self.federation_client.send_invite(",
            "            destination=target_host,",
            "            room_id=event.room_id,",
            "            event_id=event.event_id,",
            "            pdu=event,",
            "        )",
            "",
            "        return pdu",
            "",
            "    async def on_event_auth(self, event_id: str) -> List[EventBase]:",
            "        event = await self.store.get_event(event_id)",
            "        auth = await self.store.get_auth_chain(",
            "            list(event.auth_event_ids()), include_given=True",
            "        )",
            "        return list(auth)",
            "",
            "    async def do_invite_join(",
            "        self, target_hosts: Iterable[str], room_id: str, joinee: str, content: JsonDict",
            "    ) -> Tuple[str, int]:",
            "        \"\"\" Attempts to join the `joinee` to the room `room_id` via the",
            "        servers contained in `target_hosts`.",
            "",
            "        This first triggers a /make_join/ request that returns a partial",
            "        event that we can fill out and sign. This is then sent to the",
            "        remote server via /send_join/ which responds with the state at that",
            "        event and the auth_chains.",
            "",
            "        We suspend processing of any received events from this room until we",
            "        have finished processing the join.",
            "",
            "        Args:",
            "            target_hosts: List of servers to attempt to join the room with.",
            "",
            "            room_id: The ID of the room to join.",
            "",
            "            joinee: The User ID of the joining user.",
            "",
            "            content: The event content to use for the join event.",
            "        \"\"\"",
            "        # TODO: We should be able to call this on workers, but the upgrading of",
            "        # room stuff after join currently doesn't work on workers.",
            "        assert self.config.worker.worker_app is None",
            "",
            "        logger.debug(\"Joining %s to %s\", joinee, room_id)",
            "",
            "        origin, event, room_version_obj = await self._make_and_verify_event(",
            "            target_hosts,",
            "            room_id,",
            "            joinee,",
            "            \"join\",",
            "            content,",
            "            params={\"ver\": KNOWN_ROOM_VERSIONS},",
            "        )",
            "",
            "        # This shouldn't happen, because the RoomMemberHandler has a",
            "        # linearizer lock which only allows one operation per user per room",
            "        # at a time - so this is just paranoia.",
            "        assert room_id not in self.room_queues",
            "",
            "        self.room_queues[room_id] = []",
            "",
            "        await self._clean_room_for_join(room_id)",
            "",
            "        handled_events = set()",
            "",
            "        try:",
            "            # Try the host we successfully got a response to /make_join/",
            "            # request first.",
            "            host_list = list(target_hosts)",
            "            try:",
            "                host_list.remove(origin)",
            "                host_list.insert(0, origin)",
            "            except ValueError:",
            "                pass",
            "",
            "            ret = await self.federation_client.send_join(",
            "                host_list, event, room_version_obj",
            "            )",
            "",
            "            origin = ret[\"origin\"]",
            "            state = ret[\"state\"]",
            "            auth_chain = ret[\"auth_chain\"]",
            "            auth_chain.sort(key=lambda e: e.depth)",
            "",
            "            handled_events.update([s.event_id for s in state])",
            "            handled_events.update([a.event_id for a in auth_chain])",
            "            handled_events.add(event.event_id)",
            "",
            "            logger.debug(\"do_invite_join auth_chain: %s\", auth_chain)",
            "            logger.debug(\"do_invite_join state: %s\", state)",
            "",
            "            logger.debug(\"do_invite_join event: %s\", event)",
            "",
            "            # if this is the first time we've joined this room, it's time to add",
            "            # a row to `rooms` with the correct room version. If there's already a",
            "            # row there, we should override it, since it may have been populated",
            "            # based on an invite request which lied about the room version.",
            "            #",
            "            # federation_client.send_join has already checked that the room",
            "            # version in the received create event is the same as room_version_obj,",
            "            # so we can rely on it now.",
            "            #",
            "            await self.store.upsert_room_on_join(",
            "                room_id=room_id, room_version=room_version_obj,",
            "            )",
            "",
            "            max_stream_id = await self._persist_auth_tree(",
            "                origin, room_id, auth_chain, state, event, room_version_obj",
            "            )",
            "",
            "            # We wait here until this instance has seen the events come down",
            "            # replication (if we're using replication) as the below uses caches.",
            "            await self._replication.wait_for_stream_position(",
            "                self.config.worker.events_shard_config.get_instance(room_id),",
            "                \"events\",",
            "                max_stream_id,",
            "            )",
            "",
            "            # Check whether this room is the result of an upgrade of a room we already know",
            "            # about. If so, migrate over user information",
            "            predecessor = await self.store.get_room_predecessor(room_id)",
            "            if not predecessor or not isinstance(predecessor.get(\"room_id\"), str):",
            "                return event.event_id, max_stream_id",
            "            old_room_id = predecessor[\"room_id\"]",
            "            logger.debug(",
            "                \"Found predecessor for %s during remote join: %s\", room_id, old_room_id",
            "            )",
            "",
            "            # We retrieve the room member handler here as to not cause a cyclic dependency",
            "            member_handler = self.hs.get_room_member_handler()",
            "            await member_handler.transfer_room_state_on_room_upgrade(",
            "                old_room_id, room_id",
            "            )",
            "",
            "            logger.debug(\"Finished joining %s to %s\", joinee, room_id)",
            "            return event.event_id, max_stream_id",
            "        finally:",
            "            room_queue = self.room_queues[room_id]",
            "            del self.room_queues[room_id]",
            "",
            "            # we don't need to wait for the queued events to be processed -",
            "            # it's just a best-effort thing at this point. We do want to do",
            "            # them roughly in order, though, otherwise we'll end up making",
            "            # lots of requests for missing prev_events which we do actually",
            "            # have. Hence we fire off the background task, but don't wait for it.",
            "",
            "            run_in_background(self._handle_queued_pdus, room_queue)",
            "",
            "    async def _handle_queued_pdus(self, room_queue):",
            "        \"\"\"Process PDUs which got queued up while we were busy send_joining.",
            "",
            "        Args:",
            "            room_queue (list[FrozenEvent, str]): list of PDUs to be processed",
            "                and the servers that sent them",
            "        \"\"\"",
            "        for p, origin in room_queue:",
            "            try:",
            "                logger.info(",
            "                    \"Processing queued PDU %s which was received \"",
            "                    \"while we were joining %s\",",
            "                    p.event_id,",
            "                    p.room_id,",
            "                )",
            "                with nested_logging_context(p.event_id):",
            "                    await self.on_receive_pdu(origin, p, sent_to_us_directly=True)",
            "            except Exception as e:",
            "                logger.warning(",
            "                    \"Error handling queued PDU %s from %s: %s\", p.event_id, origin, e",
            "                )",
            "",
            "    async def on_make_join_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> EventBase:",
            "        \"\"\" We've received a /make_join/ request, so we create a partial",
            "        join event for the room and return that. We do *not* persist or",
            "        process it until the other server has signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: Room to create join event in",
            "            user_id: The user to create the join for",
            "        \"\"\"",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.info(",
            "                \"Got /make_join request for user %r from different origin %s, ignoring\",",
            "                user_id,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        # checking the room version will check that we've actually heard of the room",
            "        # (and return a 404 otherwise)",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "",
            "        # now check that we are *still* in the room",
            "        is_in_room = await self.auth.check_host_in_room(room_id, self.server_name)",
            "        if not is_in_room:",
            "            logger.info(",
            "                \"Got /make_join request for room %s we are no longer in\", room_id,",
            "            )",
            "            raise NotFoundError(\"Not an active room on this server\")",
            "",
            "        event_content = {\"membership\": Membership.JOIN}",
            "",
            "        builder = self.event_builder_factory.new(",
            "            room_version,",
            "            {",
            "                \"type\": EventTypes.Member,",
            "                \"content\": event_content,",
            "                \"room_id\": room_id,",
            "                \"sender\": user_id,",
            "                \"state_key\": user_id,",
            "            },",
            "        )",
            "",
            "        try:",
            "            event, context = await self.event_creation_handler.create_new_client_event(",
            "                builder=builder",
            "            )",
            "        except SynapseError as e:",
            "            logger.warning(\"Failed to create join to %s because %s\", room_id, e)",
            "            raise",
            "",
            "        # The remote hasn't signed it yet, obviously. We'll do the full checks",
            "        # when we get the event back in `on_send_join_request`",
            "        await self.auth.check_from_context(",
            "            room_version, event, context, do_sig_check=False",
            "        )",
            "",
            "        return event",
            "",
            "    async def on_send_join_request(self, origin, pdu):",
            "        \"\"\" We have received a join event for a room. Fully process it and",
            "        respond with the current state and auth chains.",
            "        \"\"\"",
            "        event = pdu",
            "",
            "        logger.debug(",
            "            \"on_send_join_request from %s: Got event: %s, signatures: %s\",",
            "            origin,",
            "            event.event_id,",
            "            event.signatures,",
            "        )",
            "",
            "        if get_domain_from_id(event.sender) != origin:",
            "            logger.info(",
            "                \"Got /send_join request for user %r from different origin %s\",",
            "                event.sender,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        event.internal_metadata.outlier = False",
            "        # Send this event on behalf of the origin server.",
            "        #",
            "        # The reasons we have the destination server rather than the origin",
            "        # server send it are slightly mysterious: the origin server should have",
            "        # all the necessary state once it gets the response to the send_join,",
            "        # so it could send the event itself if it wanted to. It may be that",
            "        # doing it this way reduces failure modes, or avoids certain attacks",
            "        # where a new server selectively tells a subset of the federation that",
            "        # it has joined.",
            "        #",
            "        # The fact is that, as of the current writing, Synapse doesn't send out",
            "        # the join event over federation after joining, and changing it now",
            "        # would introduce the danger of backwards-compatibility problems.",
            "        event.internal_metadata.send_on_behalf_of = origin",
            "",
            "        context = await self._handle_new_event(origin, event)",
            "",
            "        logger.debug(",
            "            \"on_send_join_request: After _handle_new_event: %s, sigs: %s\",",
            "            event.event_id,",
            "            event.signatures,",
            "        )",
            "",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "",
            "        state_ids = list(prev_state_ids.values())",
            "        auth_chain = await self.store.get_auth_chain(state_ids)",
            "",
            "        state = await self.store.get_events(list(prev_state_ids.values()))",
            "",
            "        return {\"state\": list(state.values()), \"auth_chain\": auth_chain}",
            "",
            "    async def on_invite_request(",
            "        self, origin: str, event: EventBase, room_version: RoomVersion",
            "    ):",
            "        \"\"\" We've got an invite event. Process and persist it. Sign it.",
            "",
            "        Respond with the now signed event.",
            "        \"\"\"",
            "        if event.state_key is None:",
            "            raise SynapseError(400, \"The invite event did not have a state key\")",
            "",
            "        is_blocked = await self.store.is_room_blocked(event.room_id)",
            "        if is_blocked:",
            "            raise SynapseError(403, \"This room has been blocked on this server\")",
            "",
            "        if self.hs.config.block_non_admin_invites:",
            "            raise SynapseError(403, \"This server does not accept room invites\")",
            "",
            "        if not self.spam_checker.user_may_invite(",
            "            event.sender, event.state_key, event.room_id",
            "        ):",
            "            raise SynapseError(",
            "                403, \"This user is not permitted to send invites to this server/user\"",
            "            )",
            "",
            "        membership = event.content.get(\"membership\")",
            "        if event.type != EventTypes.Member or membership != Membership.INVITE:",
            "            raise SynapseError(400, \"The event was not an m.room.member invite event\")",
            "",
            "        sender_domain = get_domain_from_id(event.sender)",
            "        if sender_domain != origin:",
            "            raise SynapseError(",
            "                400, \"The invite event was not from the server sending it\"",
            "            )",
            "",
            "        if not self.is_mine_id(event.state_key):",
            "            raise SynapseError(400, \"The invite event must be for this server\")",
            "",
            "        # block any attempts to invite the server notices mxid",
            "        if event.state_key == self._server_notices_mxid:",
            "            raise SynapseError(HTTPStatus.FORBIDDEN, \"Cannot invite this user\")",
            "",
            "        # keep a record of the room version, if we don't yet know it.",
            "        # (this may get overwritten if we later get a different room version in a",
            "        # join dance).",
            "        await self._maybe_store_room_on_outlier_membership(",
            "            room_id=event.room_id, room_version=room_version",
            "        )",
            "",
            "        event.internal_metadata.outlier = True",
            "        event.internal_metadata.out_of_band_membership = True",
            "",
            "        event.signatures.update(",
            "            compute_event_signature(",
            "                room_version,",
            "                event.get_pdu_json(),",
            "                self.hs.hostname,",
            "                self.hs.signing_key,",
            "            )",
            "        )",
            "",
            "        context = await self.state_handler.compute_event_context(event)",
            "        await self.persist_events_and_notify(event.room_id, [(event, context)])",
            "",
            "        return event",
            "",
            "    async def do_remotely_reject_invite(",
            "        self, target_hosts: Iterable[str], room_id: str, user_id: str, content: JsonDict",
            "    ) -> Tuple[EventBase, int]:",
            "        origin, event, room_version = await self._make_and_verify_event(",
            "            target_hosts, room_id, user_id, \"leave\", content=content",
            "        )",
            "        # Mark as outlier as we don't have any state for this event; we're not",
            "        # even in the room.",
            "        event.internal_metadata.outlier = True",
            "        event.internal_metadata.out_of_band_membership = True",
            "",
            "        # Try the host that we successfully called /make_leave/ on first for",
            "        # the /send_leave/ request.",
            "        host_list = list(target_hosts)",
            "        try:",
            "            host_list.remove(origin)",
            "            host_list.insert(0, origin)",
            "        except ValueError:",
            "            pass",
            "",
            "        await self.federation_client.send_leave(host_list, event)",
            "",
            "        context = await self.state_handler.compute_event_context(event)",
            "        stream_id = await self.persist_events_and_notify(",
            "            event.room_id, [(event, context)]",
            "        )",
            "",
            "        return event, stream_id",
            "",
            "    async def _make_and_verify_event(",
            "        self,",
            "        target_hosts: Iterable[str],",
            "        room_id: str,",
            "        user_id: str,",
            "        membership: str,",
            "        content: JsonDict = {},",
            "        params: Optional[Dict[str, Union[str, Iterable[str]]]] = None,",
            "    ) -> Tuple[str, EventBase, RoomVersion]:",
            "        (",
            "            origin,",
            "            event,",
            "            room_version,",
            "        ) = await self.federation_client.make_membership_event(",
            "            target_hosts, room_id, user_id, membership, content, params=params",
            "        )",
            "",
            "        logger.debug(\"Got response to make_%s: %s\", membership, event)",
            "",
            "        # We should assert some things.",
            "        # FIXME: Do this in a nicer way",
            "        assert event.type == EventTypes.Member",
            "        assert event.user_id == user_id",
            "        assert event.state_key == user_id",
            "        assert event.room_id == room_id",
            "        return origin, event, room_version",
            "",
            "    async def on_make_leave_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> EventBase:",
            "        \"\"\" We've received a /make_leave/ request, so we create a partial",
            "        leave event for the room and return that. We do *not* persist or",
            "        process it until the other server has signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: Room to create leave event in",
            "            user_id: The user to create the leave for",
            "        \"\"\"",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.info(",
            "                \"Got /make_leave request for user %r from different origin %s, ignoring\",",
            "                user_id,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "        builder = self.event_builder_factory.new(",
            "            room_version,",
            "            {",
            "                \"type\": EventTypes.Member,",
            "                \"content\": {\"membership\": Membership.LEAVE},",
            "                \"room_id\": room_id,",
            "                \"sender\": user_id,",
            "                \"state_key\": user_id,",
            "            },",
            "        )",
            "",
            "        event, context = await self.event_creation_handler.create_new_client_event(",
            "            builder=builder",
            "        )",
            "",
            "        try:",
            "            # The remote hasn't signed it yet, obviously. We'll do the full checks",
            "            # when we get the event back in `on_send_leave_request`",
            "            await self.auth.check_from_context(",
            "                room_version, event, context, do_sig_check=False",
            "            )",
            "        except AuthError as e:",
            "            logger.warning(\"Failed to create new leave %r because %s\", event, e)",
            "            raise e",
            "",
            "        return event",
            "",
            "    async def on_send_leave_request(self, origin, pdu):",
            "        \"\"\" We have received a leave event for a room. Fully process it.\"\"\"",
            "        event = pdu",
            "",
            "        logger.debug(",
            "            \"on_send_leave_request: Got event: %s, signatures: %s\",",
            "            event.event_id,",
            "            event.signatures,",
            "        )",
            "",
            "        if get_domain_from_id(event.sender) != origin:",
            "            logger.info(",
            "                \"Got /send_leave request for user %r from different origin %s\",",
            "                event.sender,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        event.internal_metadata.outlier = False",
            "",
            "        await self._handle_new_event(origin, event)",
            "",
            "        logger.debug(",
            "            \"on_send_leave_request: After _handle_new_event: %s, sigs: %s\",",
            "            event.event_id,",
            "            event.signatures,",
            "        )",
            "",
            "        return None",
            "",
            "    async def get_state_for_pdu(self, room_id: str, event_id: str) -> List[EventBase]:",
            "        \"\"\"Returns the state at the event. i.e. not including said event.",
            "        \"\"\"",
            "",
            "        event = await self.store.get_event(event_id, check_room_id=room_id)",
            "",
            "        state_groups = await self.state_store.get_state_groups(room_id, [event_id])",
            "",
            "        if state_groups:",
            "            _, state = list(state_groups.items()).pop()",
            "            results = {(e.type, e.state_key): e for e in state}",
            "",
            "            if event.is_state():",
            "                # Get previous state",
            "                if \"replaces_state\" in event.unsigned:",
            "                    prev_id = event.unsigned[\"replaces_state\"]",
            "                    if prev_id != event.event_id:",
            "                        prev_event = await self.store.get_event(prev_id)",
            "                        results[(event.type, event.state_key)] = prev_event",
            "                else:",
            "                    del results[(event.type, event.state_key)]",
            "",
            "            res = list(results.values())",
            "            return res",
            "        else:",
            "            return []",
            "",
            "    async def get_state_ids_for_pdu(self, room_id: str, event_id: str) -> List[str]:",
            "        \"\"\"Returns the state at the event. i.e. not including said event.",
            "        \"\"\"",
            "        event = await self.store.get_event(event_id, check_room_id=room_id)",
            "",
            "        state_groups = await self.state_store.get_state_groups_ids(room_id, [event_id])",
            "",
            "        if state_groups:",
            "            _, state = list(state_groups.items()).pop()",
            "            results = state",
            "",
            "            if event.is_state():",
            "                # Get previous state",
            "                if \"replaces_state\" in event.unsigned:",
            "                    prev_id = event.unsigned[\"replaces_state\"]",
            "                    if prev_id != event.event_id:",
            "                        results[(event.type, event.state_key)] = prev_id",
            "                else:",
            "                    results.pop((event.type, event.state_key), None)",
            "",
            "            return list(results.values())",
            "        else:",
            "            return []",
            "",
            "    @log_function",
            "    async def on_backfill_request(",
            "        self, origin: str, room_id: str, pdu_list: List[str], limit: int",
            "    ) -> List[EventBase]:",
            "        in_room = await self.auth.check_host_in_room(room_id, origin)",
            "        if not in_room:",
            "            raise AuthError(403, \"Host not in room.\")",
            "",
            "        # Synapse asks for 100 events per backfill request. Do not allow more.",
            "        limit = min(limit, 100)",
            "",
            "        events = await self.store.get_backfill_events(room_id, pdu_list, limit)",
            "",
            "        events = await filter_events_for_server(self.storage, origin, events)",
            "",
            "        return events",
            "",
            "    @log_function",
            "    async def get_persisted_pdu(",
            "        self, origin: str, event_id: str",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"Get an event from the database for the given server.",
            "",
            "        Args:",
            "            origin: hostname of server which is requesting the event; we",
            "               will check that the server is allowed to see it.",
            "            event_id: id of the event being requested",
            "",
            "        Returns:",
            "            None if we know nothing about the event; otherwise the (possibly-redacted) event.",
            "",
            "        Raises:",
            "            AuthError if the server is not currently in the room",
            "        \"\"\"",
            "        event = await self.store.get_event(",
            "            event_id, allow_none=True, allow_rejected=True",
            "        )",
            "",
            "        if event:",
            "            in_room = await self.auth.check_host_in_room(event.room_id, origin)",
            "            if not in_room:",
            "                raise AuthError(403, \"Host not in room.\")",
            "",
            "            events = await filter_events_for_server(self.storage, origin, [event])",
            "            event = events[0]",
            "            return event",
            "        else:",
            "            return None",
            "",
            "    async def get_min_depth_for_context(self, context):",
            "        return await self.store.get_min_depth(context)",
            "",
            "    async def _handle_new_event(",
            "        self, origin, event, state=None, auth_events=None, backfilled=False",
            "    ):",
            "        context = await self._prep_event(",
            "            origin, event, state=state, auth_events=auth_events, backfilled=backfilled",
            "        )",
            "",
            "        try:",
            "            if (",
            "                not event.internal_metadata.is_outlier()",
            "                and not backfilled",
            "                and not context.rejected",
            "            ):",
            "                await self.action_generator.handle_push_actions_for_event(",
            "                    event, context",
            "                )",
            "",
            "            await self.persist_events_and_notify(",
            "                event.room_id, [(event, context)], backfilled=backfilled",
            "            )",
            "        except Exception:",
            "            run_in_background(",
            "                self.store.remove_push_actions_from_staging, event.event_id",
            "            )",
            "            raise",
            "",
            "        return context",
            "",
            "    async def _handle_new_events(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        event_infos: Iterable[_NewEventInfo],",
            "        backfilled: bool = False,",
            "    ) -> None:",
            "        \"\"\"Creates the appropriate contexts and persists events. The events",
            "        should not depend on one another, e.g. this should be used to persist",
            "        a bunch of outliers, but not a chunk of individual events that depend",
            "        on each other for state calculations.",
            "",
            "        Notifies about the events where appropriate.",
            "        \"\"\"",
            "",
            "        async def prep(ev_info: _NewEventInfo):",
            "            event = ev_info.event",
            "            with nested_logging_context(suffix=event.event_id):",
            "                res = await self._prep_event(",
            "                    origin,",
            "                    event,",
            "                    state=ev_info.state,",
            "                    auth_events=ev_info.auth_events,",
            "                    backfilled=backfilled,",
            "                )",
            "            return res",
            "",
            "        contexts = await make_deferred_yieldable(",
            "            defer.gatherResults(",
            "                [run_in_background(prep, ev_info) for ev_info in event_infos],",
            "                consumeErrors=True,",
            "            )",
            "        )",
            "",
            "        await self.persist_events_and_notify(",
            "            room_id,",
            "            [",
            "                (ev_info.event, context)",
            "                for ev_info, context in zip(event_infos, contexts)",
            "            ],",
            "            backfilled=backfilled,",
            "        )",
            "",
            "    async def _persist_auth_tree(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        auth_events: List[EventBase],",
            "        state: List[EventBase],",
            "        event: EventBase,",
            "        room_version: RoomVersion,",
            "    ) -> int:",
            "        \"\"\"Checks the auth chain is valid (and passes auth checks) for the",
            "        state and event. Then persists the auth chain and state atomically.",
            "        Persists the event separately. Notifies about the persisted events",
            "        where appropriate.",
            "",
            "        Will attempt to fetch missing auth events.",
            "",
            "        Args:",
            "            origin: Where the events came from",
            "            room_id,",
            "            auth_events",
            "            state",
            "            event",
            "            room_version: The room version we expect this room to have, and",
            "                will raise if it doesn't match the version in the create event.",
            "        \"\"\"",
            "        events_to_context = {}",
            "        for e in itertools.chain(auth_events, state):",
            "            e.internal_metadata.outlier = True",
            "            ctx = await self.state_handler.compute_event_context(e)",
            "            events_to_context[e.event_id] = ctx",
            "",
            "        event_map = {",
            "            e.event_id: e for e in itertools.chain(auth_events, state, [event])",
            "        }",
            "",
            "        create_event = None",
            "        for e in auth_events:",
            "            if (e.type, e.state_key) == (EventTypes.Create, \"\"):",
            "                create_event = e",
            "                break",
            "",
            "        if create_event is None:",
            "            # If the state doesn't have a create event then the room is",
            "            # invalid, and it would fail auth checks anyway.",
            "            raise SynapseError(400, \"No create event in state\")",
            "",
            "        room_version_id = create_event.content.get(",
            "            \"room_version\", RoomVersions.V1.identifier",
            "        )",
            "",
            "        if room_version.identifier != room_version_id:",
            "            raise SynapseError(400, \"Room version mismatch\")",
            "",
            "        missing_auth_events = set()",
            "        for e in itertools.chain(auth_events, state, [event]):",
            "            for e_id in e.auth_event_ids():",
            "                if e_id not in event_map:",
            "                    missing_auth_events.add(e_id)",
            "",
            "        for e_id in missing_auth_events:",
            "            m_ev = await self.federation_client.get_pdu(",
            "                [origin], e_id, room_version=room_version, outlier=True, timeout=10000,",
            "            )",
            "            if m_ev and m_ev.event_id == e_id:",
            "                event_map[e_id] = m_ev",
            "            else:",
            "                logger.info(\"Failed to find auth event %r\", e_id)",
            "",
            "        for e in itertools.chain(auth_events, state, [event]):",
            "            auth_for_e = {",
            "                (event_map[e_id].type, event_map[e_id].state_key): event_map[e_id]",
            "                for e_id in e.auth_event_ids()",
            "                if e_id in event_map",
            "            }",
            "            if create_event:",
            "                auth_for_e[(EventTypes.Create, \"\")] = create_event",
            "",
            "            try:",
            "                event_auth.check(room_version, e, auth_events=auth_for_e)",
            "            except SynapseError as err:",
            "                # we may get SynapseErrors here as well as AuthErrors. For",
            "                # instance, there are a couple of (ancient) events in some",
            "                # rooms whose senders do not have the correct sigil; these",
            "                # cause SynapseErrors in auth.check. We don't want to give up",
            "                # the attempt to federate altogether in such cases.",
            "",
            "                logger.warning(\"Rejecting %s because %s\", e.event_id, err.msg)",
            "",
            "                if e == event:",
            "                    raise",
            "                events_to_context[e.event_id].rejected = RejectedReason.AUTH_ERROR",
            "",
            "        await self.persist_events_and_notify(",
            "            room_id,",
            "            [",
            "                (e, events_to_context[e.event_id])",
            "                for e in itertools.chain(auth_events, state)",
            "            ],",
            "        )",
            "",
            "        new_event_context = await self.state_handler.compute_event_context(",
            "            event, old_state=state",
            "        )",
            "",
            "        return await self.persist_events_and_notify(",
            "            room_id, [(event, new_event_context)]",
            "        )",
            "",
            "    async def _prep_event(",
            "        self,",
            "        origin: str,",
            "        event: EventBase,",
            "        state: Optional[Iterable[EventBase]],",
            "        auth_events: Optional[MutableStateMap[EventBase]],",
            "        backfilled: bool,",
            "    ) -> EventContext:",
            "        context = await self.state_handler.compute_event_context(event, old_state=state)",
            "",
            "        if not auth_events:",
            "            prev_state_ids = await context.get_prev_state_ids()",
            "            auth_events_ids = self.auth.compute_auth_events(",
            "                event, prev_state_ids, for_verification=True",
            "            )",
            "            auth_events_x = await self.store.get_events(auth_events_ids)",
            "            auth_events = {(e.type, e.state_key): e for e in auth_events_x.values()}",
            "",
            "        # This is a hack to fix some old rooms where the initial join event",
            "        # didn't reference the create event in its auth events.",
            "        if event.type == EventTypes.Member and not event.auth_event_ids():",
            "            if len(event.prev_event_ids()) == 1 and event.depth < 5:",
            "                c = await self.store.get_event(",
            "                    event.prev_event_ids()[0], allow_none=True",
            "                )",
            "                if c and c.type == EventTypes.Create:",
            "                    auth_events[(c.type, c.state_key)] = c",
            "",
            "        context = await self.do_auth(origin, event, context, auth_events=auth_events)",
            "",
            "        if not context.rejected:",
            "            await self._check_for_soft_fail(event, state, backfilled)",
            "",
            "        if event.type == EventTypes.GuestAccess and not context.rejected:",
            "            await self.maybe_kick_guest_users(event)",
            "",
            "        return context",
            "",
            "    async def _check_for_soft_fail(",
            "        self, event: EventBase, state: Optional[Iterable[EventBase]], backfilled: bool",
            "    ) -> None:",
            "        \"\"\"Checks if we should soft fail the event; if so, marks the event as",
            "        such.",
            "",
            "        Args:",
            "            event",
            "            state: The state at the event if we don't have all the event's prev events",
            "            backfilled: Whether the event is from backfill",
            "        \"\"\"",
            "        # For new (non-backfilled and non-outlier) events we check if the event",
            "        # passes auth based on the current state. If it doesn't then we",
            "        # \"soft-fail\" the event.",
            "        if backfilled or event.internal_metadata.is_outlier():",
            "            return",
            "",
            "        extrem_ids_list = await self.store.get_latest_event_ids_in_room(event.room_id)",
            "        extrem_ids = set(extrem_ids_list)",
            "        prev_event_ids = set(event.prev_event_ids())",
            "",
            "        if extrem_ids == prev_event_ids:",
            "            # If they're the same then the current state is the same as the",
            "            # state at the event, so no point rechecking auth for soft fail.",
            "            return",
            "",
            "        room_version = await self.store.get_room_version_id(event.room_id)",
            "        room_version_obj = KNOWN_ROOM_VERSIONS[room_version]",
            "",
            "        # Calculate the \"current state\".",
            "        if state is not None:",
            "            # If we're explicitly given the state then we won't have all the",
            "            # prev events, and so we have a gap in the graph. In this case",
            "            # we want to be a little careful as we might have been down for",
            "            # a while and have an incorrect view of the current state,",
            "            # however we still want to do checks as gaps are easy to",
            "            # maliciously manufacture.",
            "            #",
            "            # So we use a \"current state\" that is actually a state",
            "            # resolution across the current forward extremities and the",
            "            # given state at the event. This should correctly handle cases",
            "            # like bans, especially with state res v2.",
            "",
            "            state_sets_d = await self.state_store.get_state_groups(",
            "                event.room_id, extrem_ids",
            "            )",
            "            state_sets = list(state_sets_d.values())  # type: List[Iterable[EventBase]]",
            "            state_sets.append(state)",
            "            current_states = await self.state_handler.resolve_events(",
            "                room_version, state_sets, event",
            "            )",
            "            current_state_ids = {",
            "                k: e.event_id for k, e in current_states.items()",
            "            }  # type: StateMap[str]",
            "        else:",
            "            current_state_ids = await self.state_handler.get_current_state_ids(",
            "                event.room_id, latest_event_ids=extrem_ids",
            "            )",
            "",
            "        logger.debug(",
            "            \"Doing soft-fail check for %s: state %s\", event.event_id, current_state_ids,",
            "        )",
            "",
            "        # Now check if event pass auth against said current state",
            "        auth_types = auth_types_for_event(event)",
            "        current_state_ids_list = [",
            "            e for k, e in current_state_ids.items() if k in auth_types",
            "        ]",
            "",
            "        auth_events_map = await self.store.get_events(current_state_ids_list)",
            "        current_auth_events = {",
            "            (e.type, e.state_key): e for e in auth_events_map.values()",
            "        }",
            "",
            "        try:",
            "            event_auth.check(room_version_obj, event, auth_events=current_auth_events)",
            "        except AuthError as e:",
            "            logger.warning(\"Soft-failing %r because %s\", event, e)",
            "            event.internal_metadata.soft_failed = True",
            "",
            "    async def on_query_auth(",
            "        self, origin, event_id, room_id, remote_auth_chain, rejects, missing",
            "    ):",
            "        in_room = await self.auth.check_host_in_room(room_id, origin)",
            "        if not in_room:",
            "            raise AuthError(403, \"Host not in room.\")",
            "",
            "        event = await self.store.get_event(event_id, check_room_id=room_id)",
            "",
            "        # Just go through and process each event in `remote_auth_chain`. We",
            "        # don't want to fall into the trap of `missing` being wrong.",
            "        for e in remote_auth_chain:",
            "            try:",
            "                await self._handle_new_event(origin, e)",
            "            except AuthError:",
            "                pass",
            "",
            "        # Now get the current auth_chain for the event.",
            "        local_auth_chain = await self.store.get_auth_chain(",
            "            list(event.auth_event_ids()), include_given=True",
            "        )",
            "",
            "        # TODO: Check if we would now reject event_id. If so we need to tell",
            "        # everyone.",
            "",
            "        ret = await self.construct_auth_difference(local_auth_chain, remote_auth_chain)",
            "",
            "        logger.debug(\"on_query_auth returning: %s\", ret)",
            "",
            "        return ret",
            "",
            "    async def on_get_missing_events(",
            "        self, origin, room_id, earliest_events, latest_events, limit",
            "    ):",
            "        in_room = await self.auth.check_host_in_room(room_id, origin)",
            "        if not in_room:",
            "            raise AuthError(403, \"Host not in room.\")",
            "",
            "        # Only allow up to 20 events to be retrieved per request.",
            "        limit = min(limit, 20)",
            "",
            "        missing_events = await self.store.get_missing_events(",
            "            room_id=room_id,",
            "            earliest_events=earliest_events,",
            "            latest_events=latest_events,",
            "            limit=limit,",
            "        )",
            "",
            "        missing_events = await filter_events_for_server(",
            "            self.storage, origin, missing_events",
            "        )",
            "",
            "        return missing_events",
            "",
            "    async def do_auth(",
            "        self,",
            "        origin: str,",
            "        event: EventBase,",
            "        context: EventContext,",
            "        auth_events: MutableStateMap[EventBase],",
            "    ) -> EventContext:",
            "        \"\"\"",
            "",
            "        Args:",
            "            origin:",
            "            event:",
            "            context:",
            "            auth_events:",
            "                Map from (event_type, state_key) to event",
            "",
            "                Normally, our calculated auth_events based on the state of the room",
            "                at the event's position in the DAG, though occasionally (eg if the",
            "                event is an outlier), may be the auth events claimed by the remote",
            "                server.",
            "",
            "                Also NB that this function adds entries to it.",
            "        Returns:",
            "            updated context object",
            "        \"\"\"",
            "        room_version = await self.store.get_room_version_id(event.room_id)",
            "        room_version_obj = KNOWN_ROOM_VERSIONS[room_version]",
            "",
            "        try:",
            "            context = await self._update_auth_events_and_context_for_auth(",
            "                origin, event, context, auth_events",
            "            )",
            "        except Exception:",
            "            # We don't really mind if the above fails, so lets not fail",
            "            # processing if it does. However, it really shouldn't fail so",
            "            # let's still log as an exception since we'll still want to fix",
            "            # any bugs.",
            "            logger.exception(",
            "                \"Failed to double check auth events for %s with remote. \"",
            "                \"Ignoring failure and continuing processing of event.\",",
            "                event.event_id,",
            "            )",
            "",
            "        try:",
            "            event_auth.check(room_version_obj, event, auth_events=auth_events)",
            "        except AuthError as e:",
            "            logger.warning(\"Failed auth resolution for %r because %s\", event, e)",
            "            context.rejected = RejectedReason.AUTH_ERROR",
            "",
            "        return context",
            "",
            "    async def _update_auth_events_and_context_for_auth(",
            "        self,",
            "        origin: str,",
            "        event: EventBase,",
            "        context: EventContext,",
            "        auth_events: MutableStateMap[EventBase],",
            "    ) -> EventContext:",
            "        \"\"\"Helper for do_auth. See there for docs.",
            "",
            "        Checks whether a given event has the expected auth events. If it",
            "        doesn't then we talk to the remote server to compare state to see if",
            "        we can come to a consensus (e.g. if one server missed some valid",
            "        state).",
            "",
            "        This attempts to resolve any potential divergence of state between",
            "        servers, but is not essential and so failures should not block further",
            "        processing of the event.",
            "",
            "        Args:",
            "            origin:",
            "            event:",
            "            context:",
            "",
            "            auth_events:",
            "                Map from (event_type, state_key) to event",
            "",
            "                Normally, our calculated auth_events based on the state of the room",
            "                at the event's position in the DAG, though occasionally (eg if the",
            "                event is an outlier), may be the auth events claimed by the remote",
            "                server.",
            "",
            "                Also NB that this function adds entries to it.",
            "",
            "        Returns:",
            "            updated context",
            "        \"\"\"",
            "        event_auth_events = set(event.auth_event_ids())",
            "",
            "        # missing_auth is the set of the event's auth_events which we don't yet have",
            "        # in auth_events.",
            "        missing_auth = event_auth_events.difference(",
            "            e.event_id for e in auth_events.values()",
            "        )",
            "",
            "        # if we have missing events, we need to fetch those events from somewhere.",
            "        #",
            "        # we start by checking if they are in the store, and then try calling /event_auth/.",
            "        if missing_auth:",
            "            have_events = await self.store.have_seen_events(missing_auth)",
            "            logger.debug(\"Events %s are in the store\", have_events)",
            "            missing_auth.difference_update(have_events)",
            "",
            "        if missing_auth:",
            "            # If we don't have all the auth events, we need to get them.",
            "            logger.info(\"auth_events contains unknown events: %s\", missing_auth)",
            "            try:",
            "                try:",
            "                    remote_auth_chain = await self.federation_client.get_event_auth(",
            "                        origin, event.room_id, event.event_id",
            "                    )",
            "                except RequestSendFailed as e1:",
            "                    # The other side isn't around or doesn't implement the",
            "                    # endpoint, so lets just bail out.",
            "                    logger.info(\"Failed to get event auth from remote: %s\", e1)",
            "                    return context",
            "",
            "                seen_remotes = await self.store.have_seen_events(",
            "                    [e.event_id for e in remote_auth_chain]",
            "                )",
            "",
            "                for e in remote_auth_chain:",
            "                    if e.event_id in seen_remotes:",
            "                        continue",
            "",
            "                    if e.event_id == event.event_id:",
            "                        continue",
            "",
            "                    try:",
            "                        auth_ids = e.auth_event_ids()",
            "                        auth = {",
            "                            (e.type, e.state_key): e",
            "                            for e in remote_auth_chain",
            "                            if e.event_id in auth_ids or e.type == EventTypes.Create",
            "                        }",
            "                        e.internal_metadata.outlier = True",
            "",
            "                        logger.debug(",
            "                            \"do_auth %s missing_auth: %s\", event.event_id, e.event_id",
            "                        )",
            "                        await self._handle_new_event(origin, e, auth_events=auth)",
            "",
            "                        if e.event_id in event_auth_events:",
            "                            auth_events[(e.type, e.state_key)] = e",
            "                    except AuthError:",
            "                        pass",
            "",
            "            except Exception:",
            "                logger.exception(\"Failed to get auth chain\")",
            "",
            "        if event.internal_metadata.is_outlier():",
            "            # XXX: given that, for an outlier, we'll be working with the",
            "            # event's *claimed* auth events rather than those we calculated:",
            "            # (a) is there any point in this test, since different_auth below will",
            "            # obviously be empty",
            "            # (b) alternatively, why don't we do it earlier?",
            "            logger.info(\"Skipping auth_event fetch for outlier\")",
            "            return context",
            "",
            "        different_auth = event_auth_events.difference(",
            "            e.event_id for e in auth_events.values()",
            "        )",
            "",
            "        if not different_auth:",
            "            return context",
            "",
            "        logger.info(",
            "            \"auth_events refers to events which are not in our calculated auth \"",
            "            \"chain: %s\",",
            "            different_auth,",
            "        )",
            "",
            "        # XXX: currently this checks for redactions but I'm not convinced that is",
            "        # necessary?",
            "        different_events = await self.store.get_events_as_list(different_auth)",
            "",
            "        for d in different_events:",
            "            if d.room_id != event.room_id:",
            "                logger.warning(",
            "                    \"Event %s refers to auth_event %s which is in a different room\",",
            "                    event.event_id,",
            "                    d.event_id,",
            "                )",
            "",
            "                # don't attempt to resolve the claimed auth events against our own",
            "                # in this case: just use our own auth events.",
            "                #",
            "                # XXX: should we reject the event in this case? It feels like we should,",
            "                # but then shouldn't we also do so if we've failed to fetch any of the",
            "                # auth events?",
            "                return context",
            "",
            "        # now we state-resolve between our own idea of the auth events, and the remote's",
            "        # idea of them.",
            "",
            "        local_state = auth_events.values()",
            "        remote_auth_events = dict(auth_events)",
            "        remote_auth_events.update({(d.type, d.state_key): d for d in different_events})",
            "        remote_state = remote_auth_events.values()",
            "",
            "        room_version = await self.store.get_room_version_id(event.room_id)",
            "        new_state = await self.state_handler.resolve_events(",
            "            room_version, (local_state, remote_state), event",
            "        )",
            "",
            "        logger.info(",
            "            \"After state res: updating auth_events with new state %s\",",
            "            {",
            "                (d.type, d.state_key): d.event_id",
            "                for d in new_state.values()",
            "                if auth_events.get((d.type, d.state_key)) != d",
            "            },",
            "        )",
            "",
            "        auth_events.update(new_state)",
            "",
            "        context = await self._update_context_for_auth_events(",
            "            event, context, auth_events",
            "        )",
            "",
            "        return context",
            "",
            "    async def _update_context_for_auth_events(",
            "        self, event: EventBase, context: EventContext, auth_events: StateMap[EventBase]",
            "    ) -> EventContext:",
            "        \"\"\"Update the state_ids in an event context after auth event resolution,",
            "        storing the changes as a new state group.",
            "",
            "        Args:",
            "            event: The event we're handling the context for",
            "",
            "            context: initial event context",
            "",
            "            auth_events: Events to update in the event context.",
            "",
            "        Returns:",
            "            new event context",
            "        \"\"\"",
            "        # exclude the state key of the new event from the current_state in the context.",
            "        if event.is_state():",
            "            event_key = (event.type, event.state_key)  # type: Optional[Tuple[str, str]]",
            "        else:",
            "            event_key = None",
            "        state_updates = {",
            "            k: a.event_id for k, a in auth_events.items() if k != event_key",
            "        }",
            "",
            "        current_state_ids = await context.get_current_state_ids()",
            "        current_state_ids = dict(current_state_ids)  # type: ignore",
            "",
            "        current_state_ids.update(state_updates)",
            "",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "        prev_state_ids = dict(prev_state_ids)",
            "",
            "        prev_state_ids.update({k: a.event_id for k, a in auth_events.items()})",
            "",
            "        # create a new state group as a delta from the existing one.",
            "        prev_group = context.state_group",
            "        state_group = await self.state_store.store_state_group(",
            "            event.event_id,",
            "            event.room_id,",
            "            prev_group=prev_group,",
            "            delta_ids=state_updates,",
            "            current_state_ids=current_state_ids,",
            "        )",
            "",
            "        return EventContext.with_state(",
            "            state_group=state_group,",
            "            state_group_before_event=context.state_group_before_event,",
            "            current_state_ids=current_state_ids,",
            "            prev_state_ids=prev_state_ids,",
            "            prev_group=prev_group,",
            "            delta_ids=state_updates,",
            "        )",
            "",
            "    async def construct_auth_difference(",
            "        self, local_auth: Iterable[EventBase], remote_auth: Iterable[EventBase]",
            "    ) -> Dict:",
            "        \"\"\" Given a local and remote auth chain, find the differences. This",
            "        assumes that we have already processed all events in remote_auth",
            "",
            "        Params:",
            "            local_auth (list)",
            "            remote_auth (list)",
            "",
            "        Returns:",
            "            dict",
            "        \"\"\"",
            "",
            "        logger.debug(\"construct_auth_difference Start!\")",
            "",
            "        # TODO: Make sure we are OK with local_auth or remote_auth having more",
            "        # auth events in them than strictly necessary.",
            "",
            "        def sort_fun(ev):",
            "            return ev.depth, ev.event_id",
            "",
            "        logger.debug(\"construct_auth_difference after sort_fun!\")",
            "",
            "        # We find the differences by starting at the \"bottom\" of each list",
            "        # and iterating up on both lists. The lists are ordered by depth and",
            "        # then event_id, we iterate up both lists until we find the event ids",
            "        # don't match. Then we look at depth/event_id to see which side is",
            "        # missing that event, and iterate only up that list. Repeat.",
            "",
            "        remote_list = list(remote_auth)",
            "        remote_list.sort(key=sort_fun)",
            "",
            "        local_list = list(local_auth)",
            "        local_list.sort(key=sort_fun)",
            "",
            "        local_iter = iter(local_list)",
            "        remote_iter = iter(remote_list)",
            "",
            "        logger.debug(\"construct_auth_difference before get_next!\")",
            "",
            "        def get_next(it, opt=None):",
            "            try:",
            "                return next(it)",
            "            except Exception:",
            "                return opt",
            "",
            "        current_local = get_next(local_iter)",
            "        current_remote = get_next(remote_iter)",
            "",
            "        logger.debug(\"construct_auth_difference before while\")",
            "",
            "        missing_remotes = []",
            "        missing_locals = []",
            "        while current_local or current_remote:",
            "            if current_remote is None:",
            "                missing_locals.append(current_local)",
            "                current_local = get_next(local_iter)",
            "                continue",
            "",
            "            if current_local is None:",
            "                missing_remotes.append(current_remote)",
            "                current_remote = get_next(remote_iter)",
            "                continue",
            "",
            "            if current_local.event_id == current_remote.event_id:",
            "                current_local = get_next(local_iter)",
            "                current_remote = get_next(remote_iter)",
            "                continue",
            "",
            "            if current_local.depth < current_remote.depth:",
            "                missing_locals.append(current_local)",
            "                current_local = get_next(local_iter)",
            "                continue",
            "",
            "            if current_local.depth > current_remote.depth:",
            "                missing_remotes.append(current_remote)",
            "                current_remote = get_next(remote_iter)",
            "                continue",
            "",
            "            # They have the same depth, so we fall back to the event_id order",
            "            if current_local.event_id < current_remote.event_id:",
            "                missing_locals.append(current_local)",
            "                current_local = get_next(local_iter)",
            "",
            "            if current_local.event_id > current_remote.event_id:",
            "                missing_remotes.append(current_remote)",
            "                current_remote = get_next(remote_iter)",
            "                continue",
            "",
            "        logger.debug(\"construct_auth_difference after while\")",
            "",
            "        # missing locals should be sent to the server",
            "        # We should find why we are missing remotes, as they will have been",
            "        # rejected.",
            "",
            "        # Remove events from missing_remotes if they are referencing a missing",
            "        # remote. We only care about the \"root\" rejected ones.",
            "        missing_remote_ids = [e.event_id for e in missing_remotes]",
            "        base_remote_rejected = list(missing_remotes)",
            "        for e in missing_remotes:",
            "            for e_id in e.auth_event_ids():",
            "                if e_id in missing_remote_ids:",
            "                    try:",
            "                        base_remote_rejected.remove(e)",
            "                    except ValueError:",
            "                        pass",
            "",
            "        reason_map = {}",
            "",
            "        for e in base_remote_rejected:",
            "            reason = await self.store.get_rejection_reason(e.event_id)",
            "            if reason is None:",
            "                # TODO: e is not in the current state, so we should",
            "                # construct some proof of that.",
            "                continue",
            "",
            "            reason_map[e.event_id] = reason",
            "",
            "        logger.debug(\"construct_auth_difference returning\")",
            "",
            "        return {",
            "            \"auth_chain\": local_auth,",
            "            \"rejects\": {",
            "                e.event_id: {\"reason\": reason_map[e.event_id], \"proof\": None}",
            "                for e in base_remote_rejected",
            "            },",
            "            \"missing\": [e.event_id for e in missing_locals],",
            "        }",
            "",
            "    @log_function",
            "    async def exchange_third_party_invite(",
            "        self, sender_user_id, target_user_id, room_id, signed",
            "    ):",
            "        third_party_invite = {\"signed\": signed}",
            "",
            "        event_dict = {",
            "            \"type\": EventTypes.Member,",
            "            \"content\": {",
            "                \"membership\": Membership.INVITE,",
            "                \"third_party_invite\": third_party_invite,",
            "            },",
            "            \"room_id\": room_id,",
            "            \"sender\": sender_user_id,",
            "            \"state_key\": target_user_id,",
            "        }",
            "",
            "        if await self.auth.check_host_in_room(room_id, self.hs.hostname):",
            "            room_version = await self.store.get_room_version_id(room_id)",
            "            builder = self.event_builder_factory.new(room_version, event_dict)",
            "",
            "            EventValidator().validate_builder(builder)",
            "            event, context = await self.event_creation_handler.create_new_client_event(",
            "                builder=builder",
            "            )",
            "",
            "            event, context = await self.add_display_name_to_third_party_invite(",
            "                room_version, event_dict, event, context",
            "            )",
            "",
            "            EventValidator().validate_new(event, self.config)",
            "",
            "            # We need to tell the transaction queue to send this out, even",
            "            # though the sender isn't a local user.",
            "            event.internal_metadata.send_on_behalf_of = self.hs.hostname",
            "",
            "            try:",
            "                await self.auth.check_from_context(room_version, event, context)",
            "            except AuthError as e:",
            "                logger.warning(\"Denying new third party invite %r because %s\", event, e)",
            "                raise e",
            "",
            "            await self._check_signature(event, context)",
            "",
            "            # We retrieve the room member handler here as to not cause a cyclic dependency",
            "            member_handler = self.hs.get_room_member_handler()",
            "            await member_handler.send_membership_event(None, event, context)",
            "        else:",
            "            destinations = {x.split(\":\", 1)[-1] for x in (sender_user_id, room_id)}",
            "            await self.federation_client.forward_third_party_invite(",
            "                destinations, room_id, event_dict",
            "            )",
            "",
            "    async def on_exchange_third_party_invite_request(",
            "        self, event_dict: JsonDict",
            "    ) -> None:",
            "        \"\"\"Handle an exchange_third_party_invite request from a remote server",
            "",
            "        The remote server will call this when it wants to turn a 3pid invite",
            "        into a normal m.room.member invite.",
            "",
            "        Args:",
            "            event_dict: Dictionary containing the event body.",
            "",
            "        \"\"\"",
            "        assert_params_in_dict(event_dict, [\"room_id\"])",
            "        room_version = await self.store.get_room_version_id(event_dict[\"room_id\"])",
            "",
            "        # NB: event_dict has a particular specced format we might need to fudge",
            "        # if we change event formats too much.",
            "        builder = self.event_builder_factory.new(room_version, event_dict)",
            "",
            "        event, context = await self.event_creation_handler.create_new_client_event(",
            "            builder=builder",
            "        )",
            "        event, context = await self.add_display_name_to_third_party_invite(",
            "            room_version, event_dict, event, context",
            "        )",
            "",
            "        try:",
            "            await self.auth.check_from_context(room_version, event, context)",
            "        except AuthError as e:",
            "            logger.warning(\"Denying third party invite %r because %s\", event, e)",
            "            raise e",
            "        await self._check_signature(event, context)",
            "",
            "        # We need to tell the transaction queue to send this out, even",
            "        # though the sender isn't a local user.",
            "        event.internal_metadata.send_on_behalf_of = get_domain_from_id(event.sender)",
            "",
            "        # We retrieve the room member handler here as to not cause a cyclic dependency",
            "        member_handler = self.hs.get_room_member_handler()",
            "        await member_handler.send_membership_event(None, event, context)",
            "",
            "    async def add_display_name_to_third_party_invite(",
            "        self, room_version, event_dict, event, context",
            "    ):",
            "        key = (",
            "            EventTypes.ThirdPartyInvite,",
            "            event.content[\"third_party_invite\"][\"signed\"][\"token\"],",
            "        )",
            "        original_invite = None",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "        original_invite_id = prev_state_ids.get(key)",
            "        if original_invite_id:",
            "            original_invite = await self.store.get_event(",
            "                original_invite_id, allow_none=True",
            "            )",
            "        if original_invite:",
            "            # If the m.room.third_party_invite event's content is empty, it means the",
            "            # invite has been revoked. In this case, we don't have to raise an error here",
            "            # because the auth check will fail on the invite (because it's not able to",
            "            # fetch public keys from the m.room.third_party_invite event's content, which",
            "            # is empty).",
            "            display_name = original_invite.content.get(\"display_name\")",
            "            event_dict[\"content\"][\"third_party_invite\"][\"display_name\"] = display_name",
            "        else:",
            "            logger.info(",
            "                \"Could not find invite event for third_party_invite: %r\", event_dict",
            "            )",
            "            # We don't discard here as this is not the appropriate place to do",
            "            # auth checks. If we need the invite and don't have it then the",
            "            # auth check code will explode appropriately.",
            "",
            "        builder = self.event_builder_factory.new(room_version, event_dict)",
            "        EventValidator().validate_builder(builder)",
            "        event, context = await self.event_creation_handler.create_new_client_event(",
            "            builder=builder",
            "        )",
            "        EventValidator().validate_new(event, self.config)",
            "        return (event, context)",
            "",
            "    async def _check_signature(self, event, context):",
            "        \"\"\"",
            "        Checks that the signature in the event is consistent with its invite.",
            "",
            "        Args:",
            "            event (Event): The m.room.member event to check",
            "            context (EventContext):",
            "",
            "        Raises:",
            "            AuthError: if signature didn't match any keys, or key has been",
            "                revoked,",
            "            SynapseError: if a transient error meant a key couldn't be checked",
            "                for revocation.",
            "        \"\"\"",
            "        signed = event.content[\"third_party_invite\"][\"signed\"]",
            "        token = signed[\"token\"]",
            "",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "        invite_event_id = prev_state_ids.get((EventTypes.ThirdPartyInvite, token))",
            "",
            "        invite_event = None",
            "        if invite_event_id:",
            "            invite_event = await self.store.get_event(invite_event_id, allow_none=True)",
            "",
            "        if not invite_event:",
            "            raise AuthError(403, \"Could not find invite\")",
            "",
            "        logger.debug(\"Checking auth on event %r\", event.content)",
            "",
            "        last_exception = None  # type: Optional[Exception]",
            "",
            "        # for each public key in the 3pid invite event",
            "        for public_key_object in self.hs.get_auth().get_public_keys(invite_event):",
            "            try:",
            "                # for each sig on the third_party_invite block of the actual invite",
            "                for server, signature_block in signed[\"signatures\"].items():",
            "                    for key_name, encoded_signature in signature_block.items():",
            "                        if not key_name.startswith(\"ed25519:\"):",
            "                            continue",
            "",
            "                        logger.debug(",
            "                            \"Attempting to verify sig with key %s from %r \"",
            "                            \"against pubkey %r\",",
            "                            key_name,",
            "                            server,",
            "                            public_key_object,",
            "                        )",
            "",
            "                        try:",
            "                            public_key = public_key_object[\"public_key\"]",
            "                            verify_key = decode_verify_key_bytes(",
            "                                key_name, decode_base64(public_key)",
            "                            )",
            "                            verify_signed_json(signed, server, verify_key)",
            "                            logger.debug(",
            "                                \"Successfully verified sig with key %s from %r \"",
            "                                \"against pubkey %r\",",
            "                                key_name,",
            "                                server,",
            "                                public_key_object,",
            "                            )",
            "                        except Exception:",
            "                            logger.info(",
            "                                \"Failed to verify sig with key %s from %r \"",
            "                                \"against pubkey %r\",",
            "                                key_name,",
            "                                server,",
            "                                public_key_object,",
            "                            )",
            "                            raise",
            "                        try:",
            "                            if \"key_validity_url\" in public_key_object:",
            "                                await self._check_key_revocation(",
            "                                    public_key, public_key_object[\"key_validity_url\"]",
            "                                )",
            "                        except Exception:",
            "                            logger.info(",
            "                                \"Failed to query key_validity_url %s\",",
            "                                public_key_object[\"key_validity_url\"],",
            "                            )",
            "                            raise",
            "                        return",
            "            except Exception as e:",
            "                last_exception = e",
            "",
            "        if last_exception is None:",
            "            # we can only get here if get_public_keys() returned an empty list",
            "            # TODO: make this better",
            "            raise RuntimeError(\"no public key in invite event\")",
            "",
            "        raise last_exception",
            "",
            "    async def _check_key_revocation(self, public_key, url):",
            "        \"\"\"",
            "        Checks whether public_key has been revoked.",
            "",
            "        Args:",
            "            public_key (str): base-64 encoded public key.",
            "            url (str): Key revocation URL.",
            "",
            "        Raises:",
            "            AuthError: if they key has been revoked.",
            "            SynapseError: if a transient error meant a key couldn't be checked",
            "                for revocation.",
            "        \"\"\"",
            "        try:",
            "            response = await self.http_client.get_json(url, {\"public_key\": public_key})",
            "        except Exception:",
            "            raise SynapseError(502, \"Third party certificate could not be checked\")",
            "        if \"valid\" not in response or not response[\"valid\"]:",
            "            raise AuthError(403, \"Third party certificate was invalid\")",
            "",
            "    async def persist_events_and_notify(",
            "        self,",
            "        room_id: str,",
            "        event_and_contexts: Sequence[Tuple[EventBase, EventContext]],",
            "        backfilled: bool = False,",
            "    ) -> int:",
            "        \"\"\"Persists events and tells the notifier/pushers about them, if",
            "        necessary.",
            "",
            "        Args:",
            "            room_id: The room ID of events being persisted.",
            "            event_and_contexts: Sequence of events with their associated",
            "                context that should be persisted. All events must belong to",
            "                the same room.",
            "            backfilled: Whether these events are a result of",
            "                backfilling or not",
            "        \"\"\"",
            "        instance = self.config.worker.events_shard_config.get_instance(room_id)",
            "        if instance != self._instance_name:",
            "            result = await self._send_events(",
            "                instance_name=instance,",
            "                store=self.store,",
            "                room_id=room_id,",
            "                event_and_contexts=event_and_contexts,",
            "                backfilled=backfilled,",
            "            )",
            "            return result[\"max_stream_id\"]",
            "        else:",
            "            assert self.storage.persistence",
            "",
            "            # Note that this returns the events that were persisted, which may not be",
            "            # the same as were passed in if some were deduplicated due to transaction IDs.",
            "            events, max_stream_token = await self.storage.persistence.persist_events(",
            "                event_and_contexts, backfilled=backfilled",
            "            )",
            "",
            "            if self._ephemeral_messages_enabled:",
            "                for event in events:",
            "                    # If there's an expiry timestamp on the event, schedule its expiry.",
            "                    self._message_handler.maybe_schedule_expiry(event)",
            "",
            "            if not backfilled:  # Never notify for backfilled events",
            "                for event in events:",
            "                    await self._notify_persisted_event(event, max_stream_token)",
            "",
            "            return max_stream_token.stream",
            "",
            "    async def _notify_persisted_event(",
            "        self, event: EventBase, max_stream_token: RoomStreamToken",
            "    ) -> None:",
            "        \"\"\"Checks to see if notifier/pushers should be notified about the",
            "        event or not.",
            "",
            "        Args:",
            "            event:",
            "            max_stream_id: The max_stream_id returned by persist_events",
            "        \"\"\"",
            "",
            "        extra_users = []",
            "        if event.type == EventTypes.Member:",
            "            target_user_id = event.state_key",
            "",
            "            # We notify for memberships if its an invite for one of our",
            "            # users",
            "            if event.internal_metadata.is_outlier():",
            "                if event.membership != Membership.INVITE:",
            "                    if not self.is_mine_id(target_user_id):",
            "                        return",
            "",
            "            target_user = UserID.from_string(target_user_id)",
            "            extra_users.append(target_user)",
            "        elif event.internal_metadata.is_outlier():",
            "            return",
            "",
            "        # the event has been persisted so it should have a stream ordering.",
            "        assert event.internal_metadata.stream_ordering",
            "",
            "        event_pos = PersistedEventPosition(",
            "            self._instance_name, event.internal_metadata.stream_ordering",
            "        )",
            "        self.notifier.on_new_room_event(",
            "            event, event_pos, max_stream_token, extra_users=extra_users",
            "        )",
            "",
            "    async def _clean_room_for_join(self, room_id: str) -> None:",
            "        \"\"\"Called to clean up any data in DB for a given room, ready for the",
            "        server to join the room.",
            "",
            "        Args:",
            "            room_id",
            "        \"\"\"",
            "        if self.config.worker_app:",
            "            await self._clean_room_for_join_client(room_id)",
            "        else:",
            "            await self.store.clean_room_for_join(room_id)",
            "",
            "    async def get_room_complexity(",
            "        self, remote_room_hosts: List[str], room_id: str",
            "    ) -> Optional[dict]:",
            "        \"\"\"",
            "        Fetch the complexity of a remote room over federation.",
            "",
            "        Args:",
            "            remote_room_hosts (list[str]): The remote servers to ask.",
            "            room_id (str): The room ID to ask about.",
            "",
            "        Returns:",
            "            Dict contains the complexity",
            "            metric versions, while None means we could not fetch the complexity.",
            "        \"\"\"",
            "",
            "        for host in remote_room_hosts:",
            "            res = await self.federation_client.get_room_complexity(host, room_id)",
            "",
            "            # We got a result, return it.",
            "            if res:",
            "                return res",
            "",
            "        # We fell off the bottom, couldn't get the complexity from anyone. Oh",
            "        # well.",
            "        return None"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "143": [
                "FederationHandler",
                "__init__"
            ]
        },
        "addLocation": []
    },
    "synapse/handlers/identity.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 46,
                "PatchRowcode": "     def __init__(self, hs):"
            },
            "1": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 47,
                "PatchRowcode": "         super().__init__(hs)"
            },
            "2": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 48,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+        # An HTTP client for contacting trusted URLs."
            },
            "4": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 50,
                "PatchRowcode": "         self.http_client = SimpleHttpClient(hs)"
            },
            "5": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # We create a blacklisting instance of SimpleHttpClient for contacting identity"
            },
            "6": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # servers specified by clients"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+        # An HTTP client for contacting identity servers specified by clients."
            },
            "8": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 52,
                "PatchRowcode": "         self.blacklisting_http_client = SimpleHttpClient("
            },
            "9": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 53,
                "PatchRowcode": "             hs, ip_blacklist=hs.config.federation_ip_range_blacklist"
            },
            "10": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 54,
                "PatchRowcode": "         )"
            },
            "11": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.federation_http_client = hs.get_http_client()"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+        self.federation_http_client = hs.get_federation_http_client()"
            },
            "13": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 56,
                "PatchRowcode": "         self.hs = hs"
            },
            "14": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 57,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": 58,
                "PatchRowcode": "     async def threepid_from_creds("
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "# Copyright 2017 Vector Creations Ltd",
            "# Copyright 2018 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "\"\"\"Utilities for interacting with Identity Servers\"\"\"",
            "",
            "import logging",
            "import urllib.parse",
            "from typing import Awaitable, Callable, Dict, List, Optional, Tuple",
            "",
            "from synapse.api.errors import (",
            "    CodeMessageException,",
            "    Codes,",
            "    HttpResponseException,",
            "    SynapseError,",
            ")",
            "from synapse.config.emailconfig import ThreepidBehaviour",
            "from synapse.http import RequestTimedOutError",
            "from synapse.http.client import SimpleHttpClient",
            "from synapse.types import JsonDict, Requester",
            "from synapse.util import json_decoder",
            "from synapse.util.hash import sha256_and_url_safe_base64",
            "from synapse.util.stringutils import assert_valid_client_secret, random_string",
            "",
            "from ._base import BaseHandler",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "id_server_scheme = \"https://\"",
            "",
            "",
            "class IdentityHandler(BaseHandler):",
            "    def __init__(self, hs):",
            "        super().__init__(hs)",
            "",
            "        self.http_client = SimpleHttpClient(hs)",
            "        # We create a blacklisting instance of SimpleHttpClient for contacting identity",
            "        # servers specified by clients",
            "        self.blacklisting_http_client = SimpleHttpClient(",
            "            hs, ip_blacklist=hs.config.federation_ip_range_blacklist",
            "        )",
            "        self.federation_http_client = hs.get_http_client()",
            "        self.hs = hs",
            "",
            "    async def threepid_from_creds(",
            "        self, id_server: str, creds: Dict[str, str]",
            "    ) -> Optional[JsonDict]:",
            "        \"\"\"",
            "        Retrieve and validate a threepid identifier from a \"credentials\" dictionary against a",
            "        given identity server",
            "",
            "        Args:",
            "            id_server: The identity server to validate 3PIDs against. Must be a",
            "                complete URL including the protocol (http(s)://)",
            "            creds: Dictionary containing the following keys:",
            "                * client_secret|clientSecret: A unique secret str provided by the client",
            "                * sid: The ID of the validation session",
            "",
            "        Returns:",
            "            A dictionary consisting of response params to the /getValidated3pid",
            "            endpoint of the Identity Service API, or None if the threepid was not found",
            "        \"\"\"",
            "        client_secret = creds.get(\"client_secret\") or creds.get(\"clientSecret\")",
            "        if not client_secret:",
            "            raise SynapseError(",
            "                400, \"Missing param client_secret in creds\", errcode=Codes.MISSING_PARAM",
            "            )",
            "        assert_valid_client_secret(client_secret)",
            "",
            "        session_id = creds.get(\"sid\")",
            "        if not session_id:",
            "            raise SynapseError(",
            "                400, \"Missing param session_id in creds\", errcode=Codes.MISSING_PARAM",
            "            )",
            "",
            "        query_params = {\"sid\": session_id, \"client_secret\": client_secret}",
            "",
            "        url = id_server + \"/_matrix/identity/api/v1/3pid/getValidated3pid\"",
            "",
            "        try:",
            "            data = await self.http_client.get_json(url, query_params)",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "        except HttpResponseException as e:",
            "            logger.info(",
            "                \"%s returned %i for threepid validation for: %s\",",
            "                id_server,",
            "                e.code,",
            "                creds,",
            "            )",
            "            return None",
            "",
            "        # Old versions of Sydent return a 200 http code even on a failed validation",
            "        # check. Thus, in addition to the HttpResponseException check above (which",
            "        # checks for non-200 errors), we need to make sure validation_session isn't",
            "        # actually an error, identified by the absence of a \"medium\" key",
            "        # See https://github.com/matrix-org/sydent/issues/215 for details",
            "        if \"medium\" in data:",
            "            return data",
            "",
            "        logger.info(\"%s reported non-validated threepid: %s\", id_server, creds)",
            "        return None",
            "",
            "    async def bind_threepid(",
            "        self,",
            "        client_secret: str,",
            "        sid: str,",
            "        mxid: str,",
            "        id_server: str,",
            "        id_access_token: Optional[str] = None,",
            "        use_v2: bool = True,",
            "    ) -> JsonDict:",
            "        \"\"\"Bind a 3PID to an identity server",
            "",
            "        Args:",
            "            client_secret: A unique secret provided by the client",
            "            sid: The ID of the validation session",
            "            mxid: The MXID to bind the 3PID to",
            "            id_server: The domain of the identity server to query",
            "            id_access_token: The access token to authenticate to the identity",
            "                server with, if necessary. Required if use_v2 is true",
            "            use_v2: Whether to use v2 Identity Service API endpoints. Defaults to True",
            "",
            "        Returns:",
            "            The response from the identity server",
            "        \"\"\"",
            "        logger.debug(\"Proxying threepid bind request for %s to %s\", mxid, id_server)",
            "",
            "        # If an id_access_token is not supplied, force usage of v1",
            "        if id_access_token is None:",
            "            use_v2 = False",
            "",
            "        # Decide which API endpoint URLs to use",
            "        headers = {}",
            "        bind_data = {\"sid\": sid, \"client_secret\": client_secret, \"mxid\": mxid}",
            "        if use_v2:",
            "            bind_url = \"https://%s/_matrix/identity/v2/3pid/bind\" % (id_server,)",
            "            headers[\"Authorization\"] = create_id_access_token_header(id_access_token)  # type: ignore",
            "        else:",
            "            bind_url = \"https://%s/_matrix/identity/api/v1/3pid/bind\" % (id_server,)",
            "",
            "        try:",
            "            # Use the blacklisting http client as this call is only to identity servers",
            "            # provided by a client",
            "            data = await self.blacklisting_http_client.post_json_get_json(",
            "                bind_url, bind_data, headers=headers",
            "            )",
            "",
            "            # Remember where we bound the threepid",
            "            await self.store.add_user_bound_threepid(",
            "                user_id=mxid,",
            "                medium=data[\"medium\"],",
            "                address=data[\"address\"],",
            "                id_server=id_server,",
            "            )",
            "",
            "            return data",
            "        except HttpResponseException as e:",
            "            if e.code != 404 or not use_v2:",
            "                logger.error(\"3PID bind failed with Matrix error: %r\", e)",
            "                raise e.to_synapse_error()",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "        except CodeMessageException as e:",
            "            data = json_decoder.decode(e.msg)  # XXX WAT?",
            "            return data",
            "",
            "        logger.info(\"Got 404 when POSTing JSON %s, falling back to v1 URL\", bind_url)",
            "        res = await self.bind_threepid(",
            "            client_secret, sid, mxid, id_server, id_access_token, use_v2=False",
            "        )",
            "        return res",
            "",
            "    async def try_unbind_threepid(self, mxid: str, threepid: dict) -> bool:",
            "        \"\"\"Attempt to remove a 3PID from an identity server, or if one is not provided, all",
            "        identity servers we're aware the binding is present on",
            "",
            "        Args:",
            "            mxid: Matrix user ID of binding to be removed",
            "            threepid: Dict with medium & address of binding to be",
            "                removed, and an optional id_server.",
            "",
            "        Raises:",
            "            SynapseError: If we failed to contact the identity server",
            "",
            "        Returns:",
            "            True on success, otherwise False if the identity",
            "            server doesn't support unbinding (or no identity server found to",
            "            contact).",
            "        \"\"\"",
            "        if threepid.get(\"id_server\"):",
            "            id_servers = [threepid[\"id_server\"]]",
            "        else:",
            "            id_servers = await self.store.get_id_servers_user_bound(",
            "                user_id=mxid, medium=threepid[\"medium\"], address=threepid[\"address\"]",
            "            )",
            "",
            "        # We don't know where to unbind, so we don't have a choice but to return",
            "        if not id_servers:",
            "            return False",
            "",
            "        changed = True",
            "        for id_server in id_servers:",
            "            changed &= await self.try_unbind_threepid_with_id_server(",
            "                mxid, threepid, id_server",
            "            )",
            "",
            "        return changed",
            "",
            "    async def try_unbind_threepid_with_id_server(",
            "        self, mxid: str, threepid: dict, id_server: str",
            "    ) -> bool:",
            "        \"\"\"Removes a binding from an identity server",
            "",
            "        Args:",
            "            mxid: Matrix user ID of binding to be removed",
            "            threepid: Dict with medium & address of binding to be removed",
            "            id_server: Identity server to unbind from",
            "",
            "        Raises:",
            "            SynapseError: If we failed to contact the identity server",
            "",
            "        Returns:",
            "            True on success, otherwise False if the identity",
            "            server doesn't support unbinding",
            "        \"\"\"",
            "        url = \"https://%s/_matrix/identity/api/v1/3pid/unbind\" % (id_server,)",
            "        url_bytes = \"/_matrix/identity/api/v1/3pid/unbind\".encode(\"ascii\")",
            "",
            "        content = {",
            "            \"mxid\": mxid,",
            "            \"threepid\": {\"medium\": threepid[\"medium\"], \"address\": threepid[\"address\"]},",
            "        }",
            "",
            "        # we abuse the federation http client to sign the request, but we have to send it",
            "        # using the normal http client since we don't want the SRV lookup and want normal",
            "        # 'browser-like' HTTPS.",
            "        auth_headers = self.federation_http_client.build_auth_headers(",
            "            destination=None,",
            "            method=b\"POST\",",
            "            url_bytes=url_bytes,",
            "            content=content,",
            "            destination_is=id_server.encode(\"ascii\"),",
            "        )",
            "        headers = {b\"Authorization\": auth_headers}",
            "",
            "        try:",
            "            # Use the blacklisting http client as this call is only to identity servers",
            "            # provided by a client",
            "            await self.blacklisting_http_client.post_json_get_json(",
            "                url, content, headers",
            "            )",
            "            changed = True",
            "        except HttpResponseException as e:",
            "            changed = False",
            "            if e.code in (400, 404, 501):",
            "                # The remote server probably doesn't support unbinding (yet)",
            "                logger.warning(\"Received %d response while unbinding threepid\", e.code)",
            "            else:",
            "                logger.error(\"Failed to unbind threepid on identity server: %s\", e)",
            "                raise SynapseError(500, \"Failed to contact identity server\")",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "",
            "        await self.store.remove_user_bound_threepid(",
            "            user_id=mxid,",
            "            medium=threepid[\"medium\"],",
            "            address=threepid[\"address\"],",
            "            id_server=id_server,",
            "        )",
            "",
            "        return changed",
            "",
            "    async def send_threepid_validation(",
            "        self,",
            "        email_address: str,",
            "        client_secret: str,",
            "        send_attempt: int,",
            "        send_email_func: Callable[[str, str, str, str], Awaitable],",
            "        next_link: Optional[str] = None,",
            "    ) -> str:",
            "        \"\"\"Send a threepid validation email for password reset or",
            "        registration purposes",
            "",
            "        Args:",
            "            email_address: The user's email address",
            "            client_secret: The provided client secret",
            "            send_attempt: Which send attempt this is",
            "            send_email_func: A function that takes an email address, token,",
            "                             client_secret and session_id, sends an email",
            "                             and returns an Awaitable.",
            "            next_link: The URL to redirect the user to after validation",
            "",
            "        Returns:",
            "            The new session_id upon success",
            "",
            "        Raises:",
            "            SynapseError is an error occurred when sending the email",
            "        \"\"\"",
            "        # Check that this email/client_secret/send_attempt combo is new or",
            "        # greater than what we've seen previously",
            "        session = await self.store.get_threepid_validation_session(",
            "            \"email\", client_secret, address=email_address, validated=False",
            "        )",
            "",
            "        # Check to see if a session already exists and that it is not yet",
            "        # marked as validated",
            "        if session and session.get(\"validated_at\") is None:",
            "            session_id = session[\"session_id\"]",
            "            last_send_attempt = session[\"last_send_attempt\"]",
            "",
            "            # Check that the send_attempt is higher than previous attempts",
            "            if send_attempt <= last_send_attempt:",
            "                # If not, just return a success without sending an email",
            "                return session_id",
            "        else:",
            "            # An non-validated session does not exist yet.",
            "            # Generate a session id",
            "            session_id = random_string(16)",
            "",
            "        if next_link:",
            "            # Manipulate the next_link to add the sid, because the caller won't get",
            "            # it until we send a response, by which time we've sent the mail.",
            "            if \"?\" in next_link:",
            "                next_link += \"&\"",
            "            else:",
            "                next_link += \"?\"",
            "            next_link += \"sid=\" + urllib.parse.quote(session_id)",
            "",
            "        # Generate a new validation token",
            "        token = random_string(32)",
            "",
            "        # Send the mail with the link containing the token, client_secret",
            "        # and session_id",
            "        try:",
            "            await send_email_func(email_address, token, client_secret, session_id)",
            "        except Exception:",
            "            logger.exception(",
            "                \"Error sending threepid validation email to %s\", email_address",
            "            )",
            "            raise SynapseError(500, \"An error was encountered when sending the email\")",
            "",
            "        token_expires = (",
            "            self.hs.get_clock().time_msec()",
            "            + self.hs.config.email_validation_token_lifetime",
            "        )",
            "",
            "        await self.store.start_or_continue_validation_session(",
            "            \"email\",",
            "            email_address,",
            "            session_id,",
            "            client_secret,",
            "            send_attempt,",
            "            next_link,",
            "            token,",
            "            token_expires,",
            "        )",
            "",
            "        return session_id",
            "",
            "    async def requestEmailToken(",
            "        self,",
            "        id_server: str,",
            "        email: str,",
            "        client_secret: str,",
            "        send_attempt: int,",
            "        next_link: Optional[str] = None,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Request an external server send an email on our behalf for the purposes of threepid",
            "        validation.",
            "",
            "        Args:",
            "            id_server: The identity server to proxy to",
            "            email: The email to send the message to",
            "            client_secret: The unique client_secret sends by the user",
            "            send_attempt: Which attempt this is",
            "            next_link: A link to redirect the user to once they submit the token",
            "",
            "        Returns:",
            "            The json response body from the server",
            "        \"\"\"",
            "        params = {",
            "            \"email\": email,",
            "            \"client_secret\": client_secret,",
            "            \"send_attempt\": send_attempt,",
            "        }",
            "        if next_link:",
            "            params[\"next_link\"] = next_link",
            "",
            "        if self.hs.config.using_identity_server_from_trusted_list:",
            "            # Warn that a deprecated config option is in use",
            "            logger.warning(",
            "                'The config option \"trust_identity_server_for_password_resets\" '",
            "                'has been replaced by \"account_threepid_delegate\". '",
            "                \"Please consult the sample config at docs/sample_config.yaml for \"",
            "                \"details and update your config file.\"",
            "            )",
            "",
            "        try:",
            "            data = await self.http_client.post_json_get_json(",
            "                id_server + \"/_matrix/identity/api/v1/validate/email/requestToken\",",
            "                params,",
            "            )",
            "            return data",
            "        except HttpResponseException as e:",
            "            logger.info(\"Proxied requestToken failed: %r\", e)",
            "            raise e.to_synapse_error()",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "",
            "    async def requestMsisdnToken(",
            "        self,",
            "        id_server: str,",
            "        country: str,",
            "        phone_number: str,",
            "        client_secret: str,",
            "        send_attempt: int,",
            "        next_link: Optional[str] = None,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Request an external server send an SMS message on our behalf for the purposes of",
            "        threepid validation.",
            "        Args:",
            "            id_server: The identity server to proxy to",
            "            country: The country code of the phone number",
            "            phone_number: The number to send the message to",
            "            client_secret: The unique client_secret sends by the user",
            "            send_attempt: Which attempt this is",
            "            next_link: A link to redirect the user to once they submit the token",
            "",
            "        Returns:",
            "            The json response body from the server",
            "        \"\"\"",
            "        params = {",
            "            \"country\": country,",
            "            \"phone_number\": phone_number,",
            "            \"client_secret\": client_secret,",
            "            \"send_attempt\": send_attempt,",
            "        }",
            "        if next_link:",
            "            params[\"next_link\"] = next_link",
            "",
            "        if self.hs.config.using_identity_server_from_trusted_list:",
            "            # Warn that a deprecated config option is in use",
            "            logger.warning(",
            "                'The config option \"trust_identity_server_for_password_resets\" '",
            "                'has been replaced by \"account_threepid_delegate\". '",
            "                \"Please consult the sample config at docs/sample_config.yaml for \"",
            "                \"details and update your config file.\"",
            "            )",
            "",
            "        try:",
            "            data = await self.http_client.post_json_get_json(",
            "                id_server + \"/_matrix/identity/api/v1/validate/msisdn/requestToken\",",
            "                params,",
            "            )",
            "        except HttpResponseException as e:",
            "            logger.info(\"Proxied requestToken failed: %r\", e)",
            "            raise e.to_synapse_error()",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "",
            "        assert self.hs.config.public_baseurl",
            "",
            "        # we need to tell the client to send the token back to us, since it doesn't",
            "        # otherwise know where to send it, so add submit_url response parameter",
            "        # (see also MSC2078)",
            "        data[\"submit_url\"] = (",
            "            self.hs.config.public_baseurl",
            "            + \"_matrix/client/unstable/add_threepid/msisdn/submit_token\"",
            "        )",
            "        return data",
            "",
            "    async def validate_threepid_session(",
            "        self, client_secret: str, sid: str",
            "    ) -> Optional[JsonDict]:",
            "        \"\"\"Validates a threepid session with only the client secret and session ID",
            "        Tries validating against any configured account_threepid_delegates as well as locally.",
            "",
            "        Args:",
            "            client_secret: A secret provided by the client",
            "            sid: The ID of the session",
            "",
            "        Returns:",
            "            The json response if validation was successful, otherwise None",
            "        \"\"\"",
            "        # XXX: We shouldn't need to keep wrapping and unwrapping this value",
            "        threepid_creds = {\"client_secret\": client_secret, \"sid\": sid}",
            "",
            "        # We don't actually know which medium this 3PID is. Thus we first assume it's email,",
            "        # and if validation fails we try msisdn",
            "        validation_session = None",
            "",
            "        # Try to validate as email",
            "        if self.hs.config.threepid_behaviour_email == ThreepidBehaviour.REMOTE:",
            "            # Ask our delegated email identity server",
            "            validation_session = await self.threepid_from_creds(",
            "                self.hs.config.account_threepid_delegate_email, threepid_creds",
            "            )",
            "        elif self.hs.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:",
            "            # Get a validated session matching these details",
            "            validation_session = await self.store.get_threepid_validation_session(",
            "                \"email\", client_secret, sid=sid, validated=True",
            "            )",
            "",
            "        if validation_session:",
            "            return validation_session",
            "",
            "        # Try to validate as msisdn",
            "        if self.hs.config.account_threepid_delegate_msisdn:",
            "            # Ask our delegated msisdn identity server",
            "            validation_session = await self.threepid_from_creds(",
            "                self.hs.config.account_threepid_delegate_msisdn, threepid_creds",
            "            )",
            "",
            "        return validation_session",
            "",
            "    async def proxy_msisdn_submit_token(",
            "        self, id_server: str, client_secret: str, sid: str, token: str",
            "    ) -> JsonDict:",
            "        \"\"\"Proxy a POST submitToken request to an identity server for verification purposes",
            "",
            "        Args:",
            "            id_server: The identity server URL to contact",
            "            client_secret: Secret provided by the client",
            "            sid: The ID of the session",
            "            token: The verification token",
            "",
            "        Raises:",
            "            SynapseError: If we failed to contact the identity server",
            "",
            "        Returns:",
            "            The response dict from the identity server",
            "        \"\"\"",
            "        body = {\"client_secret\": client_secret, \"sid\": sid, \"token\": token}",
            "",
            "        try:",
            "            return await self.http_client.post_json_get_json(",
            "                id_server + \"/_matrix/identity/api/v1/validate/msisdn/submitToken\",",
            "                body,",
            "            )",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "        except HttpResponseException as e:",
            "            logger.warning(\"Error contacting msisdn account_threepid_delegate: %s\", e)",
            "            raise SynapseError(400, \"Error contacting the identity server\")",
            "",
            "    async def lookup_3pid(",
            "        self,",
            "        id_server: str,",
            "        medium: str,",
            "        address: str,",
            "        id_access_token: Optional[str] = None,",
            "    ) -> Optional[str]:",
            "        \"\"\"Looks up a 3pid in the passed identity server.",
            "",
            "        Args:",
            "            id_server: The server name (including port, if required)",
            "                of the identity server to use.",
            "            medium: The type of the third party identifier (e.g. \"email\").",
            "            address: The third party identifier (e.g. \"foo@example.com\").",
            "            id_access_token: The access token to authenticate to the identity",
            "                server with",
            "",
            "        Returns:",
            "            the matrix ID of the 3pid, or None if it is not recognized.",
            "        \"\"\"",
            "        if id_access_token is not None:",
            "            try:",
            "                results = await self._lookup_3pid_v2(",
            "                    id_server, id_access_token, medium, address",
            "                )",
            "                return results",
            "",
            "            except Exception as e:",
            "                # Catch HttpResponseExcept for a non-200 response code",
            "                # Check if this identity server does not know about v2 lookups",
            "                if isinstance(e, HttpResponseException) and e.code == 404:",
            "                    # This is an old identity server that does not yet support v2 lookups",
            "                    logger.warning(",
            "                        \"Attempted v2 lookup on v1 identity server %s. Falling \"",
            "                        \"back to v1\",",
            "                        id_server,",
            "                    )",
            "                else:",
            "                    logger.warning(\"Error when looking up hashing details: %s\", e)",
            "                    return None",
            "",
            "        return await self._lookup_3pid_v1(id_server, medium, address)",
            "",
            "    async def _lookup_3pid_v1(",
            "        self, id_server: str, medium: str, address: str",
            "    ) -> Optional[str]:",
            "        \"\"\"Looks up a 3pid in the passed identity server using v1 lookup.",
            "",
            "        Args:",
            "            id_server: The server name (including port, if required)",
            "                of the identity server to use.",
            "            medium: The type of the third party identifier (e.g. \"email\").",
            "            address: The third party identifier (e.g. \"foo@example.com\").",
            "",
            "        Returns:",
            "            the matrix ID of the 3pid, or None if it is not recognized.",
            "        \"\"\"",
            "        try:",
            "            data = await self.blacklisting_http_client.get_json(",
            "                \"%s%s/_matrix/identity/api/v1/lookup\" % (id_server_scheme, id_server),",
            "                {\"medium\": medium, \"address\": address},",
            "            )",
            "",
            "            if \"mxid\" in data:",
            "                # note: we used to verify the identity server's signature here, but no longer",
            "                # require or validate it. See the following for context:",
            "                # https://github.com/matrix-org/synapse/issues/5253#issuecomment-666246950",
            "                return data[\"mxid\"]",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "        except IOError as e:",
            "            logger.warning(\"Error from v1 identity server lookup: %s\" % (e,))",
            "",
            "        return None",
            "",
            "    async def _lookup_3pid_v2(",
            "        self, id_server: str, id_access_token: str, medium: str, address: str",
            "    ) -> Optional[str]:",
            "        \"\"\"Looks up a 3pid in the passed identity server using v2 lookup.",
            "",
            "        Args:",
            "            id_server: The server name (including port, if required)",
            "                of the identity server to use.",
            "            id_access_token: The access token to authenticate to the identity server with",
            "            medium: The type of the third party identifier (e.g. \"email\").",
            "            address: The third party identifier (e.g. \"foo@example.com\").",
            "",
            "        Returns:",
            "            the matrix ID of the 3pid, or None if it is not recognised.",
            "        \"\"\"",
            "        # Check what hashing details are supported by this identity server",
            "        try:",
            "            hash_details = await self.blacklisting_http_client.get_json(",
            "                \"%s%s/_matrix/identity/v2/hash_details\" % (id_server_scheme, id_server),",
            "                {\"access_token\": id_access_token},",
            "            )",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "",
            "        if not isinstance(hash_details, dict):",
            "            logger.warning(",
            "                \"Got non-dict object when checking hash details of %s%s: %s\",",
            "                id_server_scheme,",
            "                id_server,",
            "                hash_details,",
            "            )",
            "            raise SynapseError(",
            "                400,",
            "                \"Non-dict object from %s%s during v2 hash_details request: %s\"",
            "                % (id_server_scheme, id_server, hash_details),",
            "            )",
            "",
            "        # Extract information from hash_details",
            "        supported_lookup_algorithms = hash_details.get(\"algorithms\")",
            "        lookup_pepper = hash_details.get(\"lookup_pepper\")",
            "        if (",
            "            not supported_lookup_algorithms",
            "            or not isinstance(supported_lookup_algorithms, list)",
            "            or not lookup_pepper",
            "            or not isinstance(lookup_pepper, str)",
            "        ):",
            "            raise SynapseError(",
            "                400,",
            "                \"Invalid hash details received from identity server %s%s: %s\"",
            "                % (id_server_scheme, id_server, hash_details),",
            "            )",
            "",
            "        # Check if any of the supported lookup algorithms are present",
            "        if LookupAlgorithm.SHA256 in supported_lookup_algorithms:",
            "            # Perform a hashed lookup",
            "            lookup_algorithm = LookupAlgorithm.SHA256",
            "",
            "            # Hash address, medium and the pepper with sha256",
            "            to_hash = \"%s %s %s\" % (address, medium, lookup_pepper)",
            "            lookup_value = sha256_and_url_safe_base64(to_hash)",
            "",
            "        elif LookupAlgorithm.NONE in supported_lookup_algorithms:",
            "            # Perform a non-hashed lookup",
            "            lookup_algorithm = LookupAlgorithm.NONE",
            "",
            "            # Combine together plaintext address and medium",
            "            lookup_value = \"%s %s\" % (address, medium)",
            "",
            "        else:",
            "            logger.warning(",
            "                \"None of the provided lookup algorithms of %s are supported: %s\",",
            "                id_server,",
            "                supported_lookup_algorithms,",
            "            )",
            "            raise SynapseError(",
            "                400,",
            "                \"Provided identity server does not support any v2 lookup \"",
            "                \"algorithms that this homeserver supports.\",",
            "            )",
            "",
            "        # Authenticate with identity server given the access token from the client",
            "        headers = {\"Authorization\": create_id_access_token_header(id_access_token)}",
            "",
            "        try:",
            "            lookup_results = await self.blacklisting_http_client.post_json_get_json(",
            "                \"%s%s/_matrix/identity/v2/lookup\" % (id_server_scheme, id_server),",
            "                {",
            "                    \"addresses\": [lookup_value],",
            "                    \"algorithm\": lookup_algorithm,",
            "                    \"pepper\": lookup_pepper,",
            "                },",
            "                headers=headers,",
            "            )",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "        except Exception as e:",
            "            logger.warning(\"Error when performing a v2 3pid lookup: %s\", e)",
            "            raise SynapseError(",
            "                500, \"Unknown error occurred during identity server lookup\"",
            "            )",
            "",
            "        # Check for a mapping from what we looked up to an MXID",
            "        if \"mappings\" not in lookup_results or not isinstance(",
            "            lookup_results[\"mappings\"], dict",
            "        ):",
            "            logger.warning(\"No results from 3pid lookup\")",
            "            return None",
            "",
            "        # Return the MXID if it's available, or None otherwise",
            "        mxid = lookup_results[\"mappings\"].get(lookup_value)",
            "        return mxid",
            "",
            "    async def ask_id_server_for_third_party_invite(",
            "        self,",
            "        requester: Requester,",
            "        id_server: str,",
            "        medium: str,",
            "        address: str,",
            "        room_id: str,",
            "        inviter_user_id: str,",
            "        room_alias: str,",
            "        room_avatar_url: str,",
            "        room_join_rules: str,",
            "        room_name: str,",
            "        inviter_display_name: str,",
            "        inviter_avatar_url: str,",
            "        id_access_token: Optional[str] = None,",
            "    ) -> Tuple[str, List[Dict[str, str]], Dict[str, str], str]:",
            "        \"\"\"",
            "        Asks an identity server for a third party invite.",
            "",
            "        Args:",
            "            requester",
            "            id_server: hostname + optional port for the identity server.",
            "            medium: The literal string \"email\".",
            "            address: The third party address being invited.",
            "            room_id: The ID of the room to which the user is invited.",
            "            inviter_user_id: The user ID of the inviter.",
            "            room_alias: An alias for the room, for cosmetic notifications.",
            "            room_avatar_url: The URL of the room's avatar, for cosmetic",
            "                notifications.",
            "            room_join_rules: The join rules of the email (e.g. \"public\").",
            "            room_name: The m.room.name of the room.",
            "            inviter_display_name: The current display name of the",
            "                inviter.",
            "            inviter_avatar_url: The URL of the inviter's avatar.",
            "            id_access_token (str|None): The access token to authenticate to the identity",
            "                server with",
            "",
            "        Returns:",
            "            A tuple containing:",
            "                token: The token which must be signed to prove authenticity.",
            "                public_keys ([{\"public_key\": str, \"key_validity_url\": str}]):",
            "                    public_key is a base64-encoded ed25519 public key.",
            "                fallback_public_key: One element from public_keys.",
            "                display_name: A user-friendly name to represent the invited user.",
            "        \"\"\"",
            "        invite_config = {",
            "            \"medium\": medium,",
            "            \"address\": address,",
            "            \"room_id\": room_id,",
            "            \"room_alias\": room_alias,",
            "            \"room_avatar_url\": room_avatar_url,",
            "            \"room_join_rules\": room_join_rules,",
            "            \"room_name\": room_name,",
            "            \"sender\": inviter_user_id,",
            "            \"sender_display_name\": inviter_display_name,",
            "            \"sender_avatar_url\": inviter_avatar_url,",
            "        }",
            "",
            "        # Add the identity service access token to the JSON body and use the v2",
            "        # Identity Service endpoints if id_access_token is present",
            "        data = None",
            "        base_url = \"%s%s/_matrix/identity\" % (id_server_scheme, id_server)",
            "",
            "        if id_access_token:",
            "            key_validity_url = \"%s%s/_matrix/identity/v2/pubkey/isvalid\" % (",
            "                id_server_scheme,",
            "                id_server,",
            "            )",
            "",
            "            # Attempt a v2 lookup",
            "            url = base_url + \"/v2/store-invite\"",
            "            try:",
            "                data = await self.blacklisting_http_client.post_json_get_json(",
            "                    url,",
            "                    invite_config,",
            "                    {\"Authorization\": create_id_access_token_header(id_access_token)},",
            "                )",
            "            except RequestTimedOutError:",
            "                raise SynapseError(500, \"Timed out contacting identity server\")",
            "            except HttpResponseException as e:",
            "                if e.code != 404:",
            "                    logger.info(\"Failed to POST %s with JSON: %s\", url, e)",
            "                    raise e",
            "",
            "        if data is None:",
            "            key_validity_url = \"%s%s/_matrix/identity/api/v1/pubkey/isvalid\" % (",
            "                id_server_scheme,",
            "                id_server,",
            "            )",
            "            url = base_url + \"/api/v1/store-invite\"",
            "",
            "            try:",
            "                data = await self.blacklisting_http_client.post_json_get_json(",
            "                    url, invite_config",
            "                )",
            "            except RequestTimedOutError:",
            "                raise SynapseError(500, \"Timed out contacting identity server\")",
            "            except HttpResponseException as e:",
            "                logger.warning(",
            "                    \"Error trying to call /store-invite on %s%s: %s\",",
            "                    id_server_scheme,",
            "                    id_server,",
            "                    e,",
            "                )",
            "",
            "            if data is None:",
            "                # Some identity servers may only support application/x-www-form-urlencoded",
            "                # types. This is especially true with old instances of Sydent, see",
            "                # https://github.com/matrix-org/sydent/pull/170",
            "                try:",
            "                    data = await self.blacklisting_http_client.post_urlencoded_get_json(",
            "                        url, invite_config",
            "                    )",
            "                except HttpResponseException as e:",
            "                    logger.warning(",
            "                        \"Error calling /store-invite on %s%s with fallback \"",
            "                        \"encoding: %s\",",
            "                        id_server_scheme,",
            "                        id_server,",
            "                        e,",
            "                    )",
            "                    raise e",
            "",
            "        # TODO: Check for success",
            "        token = data[\"token\"]",
            "        public_keys = data.get(\"public_keys\", [])",
            "        if \"public_key\" in data:",
            "            fallback_public_key = {",
            "                \"public_key\": data[\"public_key\"],",
            "                \"key_validity_url\": key_validity_url,",
            "            }",
            "        else:",
            "            fallback_public_key = public_keys[0]",
            "",
            "        if not public_keys:",
            "            public_keys.append(fallback_public_key)",
            "        display_name = data[\"display_name\"]",
            "        return token, public_keys, fallback_public_key, display_name",
            "",
            "",
            "def create_id_access_token_header(id_access_token: str) -> List[str]:",
            "    \"\"\"Create an Authorization header for passing to SimpleHttpClient as the header value",
            "    of an HTTP request.",
            "",
            "    Args:",
            "        id_access_token: An identity server access token.",
            "",
            "    Returns:",
            "        The ascii-encoded bearer token encased in a list.",
            "    \"\"\"",
            "    # Prefix with Bearer",
            "    bearer_token = \"Bearer %s\" % id_access_token",
            "",
            "    # Encode headers to standard ascii",
            "    bearer_token.encode(\"ascii\")",
            "",
            "    # Return as a list as that's how SimpleHttpClient takes header values",
            "    return [bearer_token]",
            "",
            "",
            "class LookupAlgorithm:",
            "    \"\"\"",
            "    Supported hashing algorithms when performing a 3PID lookup.",
            "",
            "    SHA256 - Hashing an (address, medium, pepper) combo with sha256, then url-safe base64",
            "        encoding",
            "    NONE - Not performing any hashing. Simply sending an (address, medium) combo in plaintext",
            "    \"\"\"",
            "",
            "    SHA256 = \"sha256\"",
            "    NONE = \"none\""
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "# Copyright 2017 Vector Creations Ltd",
            "# Copyright 2018 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "\"\"\"Utilities for interacting with Identity Servers\"\"\"",
            "",
            "import logging",
            "import urllib.parse",
            "from typing import Awaitable, Callable, Dict, List, Optional, Tuple",
            "",
            "from synapse.api.errors import (",
            "    CodeMessageException,",
            "    Codes,",
            "    HttpResponseException,",
            "    SynapseError,",
            ")",
            "from synapse.config.emailconfig import ThreepidBehaviour",
            "from synapse.http import RequestTimedOutError",
            "from synapse.http.client import SimpleHttpClient",
            "from synapse.types import JsonDict, Requester",
            "from synapse.util import json_decoder",
            "from synapse.util.hash import sha256_and_url_safe_base64",
            "from synapse.util.stringutils import assert_valid_client_secret, random_string",
            "",
            "from ._base import BaseHandler",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "id_server_scheme = \"https://\"",
            "",
            "",
            "class IdentityHandler(BaseHandler):",
            "    def __init__(self, hs):",
            "        super().__init__(hs)",
            "",
            "        # An HTTP client for contacting trusted URLs.",
            "        self.http_client = SimpleHttpClient(hs)",
            "        # An HTTP client for contacting identity servers specified by clients.",
            "        self.blacklisting_http_client = SimpleHttpClient(",
            "            hs, ip_blacklist=hs.config.federation_ip_range_blacklist",
            "        )",
            "        self.federation_http_client = hs.get_federation_http_client()",
            "        self.hs = hs",
            "",
            "    async def threepid_from_creds(",
            "        self, id_server: str, creds: Dict[str, str]",
            "    ) -> Optional[JsonDict]:",
            "        \"\"\"",
            "        Retrieve and validate a threepid identifier from a \"credentials\" dictionary against a",
            "        given identity server",
            "",
            "        Args:",
            "            id_server: The identity server to validate 3PIDs against. Must be a",
            "                complete URL including the protocol (http(s)://)",
            "            creds: Dictionary containing the following keys:",
            "                * client_secret|clientSecret: A unique secret str provided by the client",
            "                * sid: The ID of the validation session",
            "",
            "        Returns:",
            "            A dictionary consisting of response params to the /getValidated3pid",
            "            endpoint of the Identity Service API, or None if the threepid was not found",
            "        \"\"\"",
            "        client_secret = creds.get(\"client_secret\") or creds.get(\"clientSecret\")",
            "        if not client_secret:",
            "            raise SynapseError(",
            "                400, \"Missing param client_secret in creds\", errcode=Codes.MISSING_PARAM",
            "            )",
            "        assert_valid_client_secret(client_secret)",
            "",
            "        session_id = creds.get(\"sid\")",
            "        if not session_id:",
            "            raise SynapseError(",
            "                400, \"Missing param session_id in creds\", errcode=Codes.MISSING_PARAM",
            "            )",
            "",
            "        query_params = {\"sid\": session_id, \"client_secret\": client_secret}",
            "",
            "        url = id_server + \"/_matrix/identity/api/v1/3pid/getValidated3pid\"",
            "",
            "        try:",
            "            data = await self.http_client.get_json(url, query_params)",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "        except HttpResponseException as e:",
            "            logger.info(",
            "                \"%s returned %i for threepid validation for: %s\",",
            "                id_server,",
            "                e.code,",
            "                creds,",
            "            )",
            "            return None",
            "",
            "        # Old versions of Sydent return a 200 http code even on a failed validation",
            "        # check. Thus, in addition to the HttpResponseException check above (which",
            "        # checks for non-200 errors), we need to make sure validation_session isn't",
            "        # actually an error, identified by the absence of a \"medium\" key",
            "        # See https://github.com/matrix-org/sydent/issues/215 for details",
            "        if \"medium\" in data:",
            "            return data",
            "",
            "        logger.info(\"%s reported non-validated threepid: %s\", id_server, creds)",
            "        return None",
            "",
            "    async def bind_threepid(",
            "        self,",
            "        client_secret: str,",
            "        sid: str,",
            "        mxid: str,",
            "        id_server: str,",
            "        id_access_token: Optional[str] = None,",
            "        use_v2: bool = True,",
            "    ) -> JsonDict:",
            "        \"\"\"Bind a 3PID to an identity server",
            "",
            "        Args:",
            "            client_secret: A unique secret provided by the client",
            "            sid: The ID of the validation session",
            "            mxid: The MXID to bind the 3PID to",
            "            id_server: The domain of the identity server to query",
            "            id_access_token: The access token to authenticate to the identity",
            "                server with, if necessary. Required if use_v2 is true",
            "            use_v2: Whether to use v2 Identity Service API endpoints. Defaults to True",
            "",
            "        Returns:",
            "            The response from the identity server",
            "        \"\"\"",
            "        logger.debug(\"Proxying threepid bind request for %s to %s\", mxid, id_server)",
            "",
            "        # If an id_access_token is not supplied, force usage of v1",
            "        if id_access_token is None:",
            "            use_v2 = False",
            "",
            "        # Decide which API endpoint URLs to use",
            "        headers = {}",
            "        bind_data = {\"sid\": sid, \"client_secret\": client_secret, \"mxid\": mxid}",
            "        if use_v2:",
            "            bind_url = \"https://%s/_matrix/identity/v2/3pid/bind\" % (id_server,)",
            "            headers[\"Authorization\"] = create_id_access_token_header(id_access_token)  # type: ignore",
            "        else:",
            "            bind_url = \"https://%s/_matrix/identity/api/v1/3pid/bind\" % (id_server,)",
            "",
            "        try:",
            "            # Use the blacklisting http client as this call is only to identity servers",
            "            # provided by a client",
            "            data = await self.blacklisting_http_client.post_json_get_json(",
            "                bind_url, bind_data, headers=headers",
            "            )",
            "",
            "            # Remember where we bound the threepid",
            "            await self.store.add_user_bound_threepid(",
            "                user_id=mxid,",
            "                medium=data[\"medium\"],",
            "                address=data[\"address\"],",
            "                id_server=id_server,",
            "            )",
            "",
            "            return data",
            "        except HttpResponseException as e:",
            "            if e.code != 404 or not use_v2:",
            "                logger.error(\"3PID bind failed with Matrix error: %r\", e)",
            "                raise e.to_synapse_error()",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "        except CodeMessageException as e:",
            "            data = json_decoder.decode(e.msg)  # XXX WAT?",
            "            return data",
            "",
            "        logger.info(\"Got 404 when POSTing JSON %s, falling back to v1 URL\", bind_url)",
            "        res = await self.bind_threepid(",
            "            client_secret, sid, mxid, id_server, id_access_token, use_v2=False",
            "        )",
            "        return res",
            "",
            "    async def try_unbind_threepid(self, mxid: str, threepid: dict) -> bool:",
            "        \"\"\"Attempt to remove a 3PID from an identity server, or if one is not provided, all",
            "        identity servers we're aware the binding is present on",
            "",
            "        Args:",
            "            mxid: Matrix user ID of binding to be removed",
            "            threepid: Dict with medium & address of binding to be",
            "                removed, and an optional id_server.",
            "",
            "        Raises:",
            "            SynapseError: If we failed to contact the identity server",
            "",
            "        Returns:",
            "            True on success, otherwise False if the identity",
            "            server doesn't support unbinding (or no identity server found to",
            "            contact).",
            "        \"\"\"",
            "        if threepid.get(\"id_server\"):",
            "            id_servers = [threepid[\"id_server\"]]",
            "        else:",
            "            id_servers = await self.store.get_id_servers_user_bound(",
            "                user_id=mxid, medium=threepid[\"medium\"], address=threepid[\"address\"]",
            "            )",
            "",
            "        # We don't know where to unbind, so we don't have a choice but to return",
            "        if not id_servers:",
            "            return False",
            "",
            "        changed = True",
            "        for id_server in id_servers:",
            "            changed &= await self.try_unbind_threepid_with_id_server(",
            "                mxid, threepid, id_server",
            "            )",
            "",
            "        return changed",
            "",
            "    async def try_unbind_threepid_with_id_server(",
            "        self, mxid: str, threepid: dict, id_server: str",
            "    ) -> bool:",
            "        \"\"\"Removes a binding from an identity server",
            "",
            "        Args:",
            "            mxid: Matrix user ID of binding to be removed",
            "            threepid: Dict with medium & address of binding to be removed",
            "            id_server: Identity server to unbind from",
            "",
            "        Raises:",
            "            SynapseError: If we failed to contact the identity server",
            "",
            "        Returns:",
            "            True on success, otherwise False if the identity",
            "            server doesn't support unbinding",
            "        \"\"\"",
            "        url = \"https://%s/_matrix/identity/api/v1/3pid/unbind\" % (id_server,)",
            "        url_bytes = \"/_matrix/identity/api/v1/3pid/unbind\".encode(\"ascii\")",
            "",
            "        content = {",
            "            \"mxid\": mxid,",
            "            \"threepid\": {\"medium\": threepid[\"medium\"], \"address\": threepid[\"address\"]},",
            "        }",
            "",
            "        # we abuse the federation http client to sign the request, but we have to send it",
            "        # using the normal http client since we don't want the SRV lookup and want normal",
            "        # 'browser-like' HTTPS.",
            "        auth_headers = self.federation_http_client.build_auth_headers(",
            "            destination=None,",
            "            method=b\"POST\",",
            "            url_bytes=url_bytes,",
            "            content=content,",
            "            destination_is=id_server.encode(\"ascii\"),",
            "        )",
            "        headers = {b\"Authorization\": auth_headers}",
            "",
            "        try:",
            "            # Use the blacklisting http client as this call is only to identity servers",
            "            # provided by a client",
            "            await self.blacklisting_http_client.post_json_get_json(",
            "                url, content, headers",
            "            )",
            "            changed = True",
            "        except HttpResponseException as e:",
            "            changed = False",
            "            if e.code in (400, 404, 501):",
            "                # The remote server probably doesn't support unbinding (yet)",
            "                logger.warning(\"Received %d response while unbinding threepid\", e.code)",
            "            else:",
            "                logger.error(\"Failed to unbind threepid on identity server: %s\", e)",
            "                raise SynapseError(500, \"Failed to contact identity server\")",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "",
            "        await self.store.remove_user_bound_threepid(",
            "            user_id=mxid,",
            "            medium=threepid[\"medium\"],",
            "            address=threepid[\"address\"],",
            "            id_server=id_server,",
            "        )",
            "",
            "        return changed",
            "",
            "    async def send_threepid_validation(",
            "        self,",
            "        email_address: str,",
            "        client_secret: str,",
            "        send_attempt: int,",
            "        send_email_func: Callable[[str, str, str, str], Awaitable],",
            "        next_link: Optional[str] = None,",
            "    ) -> str:",
            "        \"\"\"Send a threepid validation email for password reset or",
            "        registration purposes",
            "",
            "        Args:",
            "            email_address: The user's email address",
            "            client_secret: The provided client secret",
            "            send_attempt: Which send attempt this is",
            "            send_email_func: A function that takes an email address, token,",
            "                             client_secret and session_id, sends an email",
            "                             and returns an Awaitable.",
            "            next_link: The URL to redirect the user to after validation",
            "",
            "        Returns:",
            "            The new session_id upon success",
            "",
            "        Raises:",
            "            SynapseError is an error occurred when sending the email",
            "        \"\"\"",
            "        # Check that this email/client_secret/send_attempt combo is new or",
            "        # greater than what we've seen previously",
            "        session = await self.store.get_threepid_validation_session(",
            "            \"email\", client_secret, address=email_address, validated=False",
            "        )",
            "",
            "        # Check to see if a session already exists and that it is not yet",
            "        # marked as validated",
            "        if session and session.get(\"validated_at\") is None:",
            "            session_id = session[\"session_id\"]",
            "            last_send_attempt = session[\"last_send_attempt\"]",
            "",
            "            # Check that the send_attempt is higher than previous attempts",
            "            if send_attempt <= last_send_attempt:",
            "                # If not, just return a success without sending an email",
            "                return session_id",
            "        else:",
            "            # An non-validated session does not exist yet.",
            "            # Generate a session id",
            "            session_id = random_string(16)",
            "",
            "        if next_link:",
            "            # Manipulate the next_link to add the sid, because the caller won't get",
            "            # it until we send a response, by which time we've sent the mail.",
            "            if \"?\" in next_link:",
            "                next_link += \"&\"",
            "            else:",
            "                next_link += \"?\"",
            "            next_link += \"sid=\" + urllib.parse.quote(session_id)",
            "",
            "        # Generate a new validation token",
            "        token = random_string(32)",
            "",
            "        # Send the mail with the link containing the token, client_secret",
            "        # and session_id",
            "        try:",
            "            await send_email_func(email_address, token, client_secret, session_id)",
            "        except Exception:",
            "            logger.exception(",
            "                \"Error sending threepid validation email to %s\", email_address",
            "            )",
            "            raise SynapseError(500, \"An error was encountered when sending the email\")",
            "",
            "        token_expires = (",
            "            self.hs.get_clock().time_msec()",
            "            + self.hs.config.email_validation_token_lifetime",
            "        )",
            "",
            "        await self.store.start_or_continue_validation_session(",
            "            \"email\",",
            "            email_address,",
            "            session_id,",
            "            client_secret,",
            "            send_attempt,",
            "            next_link,",
            "            token,",
            "            token_expires,",
            "        )",
            "",
            "        return session_id",
            "",
            "    async def requestEmailToken(",
            "        self,",
            "        id_server: str,",
            "        email: str,",
            "        client_secret: str,",
            "        send_attempt: int,",
            "        next_link: Optional[str] = None,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Request an external server send an email on our behalf for the purposes of threepid",
            "        validation.",
            "",
            "        Args:",
            "            id_server: The identity server to proxy to",
            "            email: The email to send the message to",
            "            client_secret: The unique client_secret sends by the user",
            "            send_attempt: Which attempt this is",
            "            next_link: A link to redirect the user to once they submit the token",
            "",
            "        Returns:",
            "            The json response body from the server",
            "        \"\"\"",
            "        params = {",
            "            \"email\": email,",
            "            \"client_secret\": client_secret,",
            "            \"send_attempt\": send_attempt,",
            "        }",
            "        if next_link:",
            "            params[\"next_link\"] = next_link",
            "",
            "        if self.hs.config.using_identity_server_from_trusted_list:",
            "            # Warn that a deprecated config option is in use",
            "            logger.warning(",
            "                'The config option \"trust_identity_server_for_password_resets\" '",
            "                'has been replaced by \"account_threepid_delegate\". '",
            "                \"Please consult the sample config at docs/sample_config.yaml for \"",
            "                \"details and update your config file.\"",
            "            )",
            "",
            "        try:",
            "            data = await self.http_client.post_json_get_json(",
            "                id_server + \"/_matrix/identity/api/v1/validate/email/requestToken\",",
            "                params,",
            "            )",
            "            return data",
            "        except HttpResponseException as e:",
            "            logger.info(\"Proxied requestToken failed: %r\", e)",
            "            raise e.to_synapse_error()",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "",
            "    async def requestMsisdnToken(",
            "        self,",
            "        id_server: str,",
            "        country: str,",
            "        phone_number: str,",
            "        client_secret: str,",
            "        send_attempt: int,",
            "        next_link: Optional[str] = None,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Request an external server send an SMS message on our behalf for the purposes of",
            "        threepid validation.",
            "        Args:",
            "            id_server: The identity server to proxy to",
            "            country: The country code of the phone number",
            "            phone_number: The number to send the message to",
            "            client_secret: The unique client_secret sends by the user",
            "            send_attempt: Which attempt this is",
            "            next_link: A link to redirect the user to once they submit the token",
            "",
            "        Returns:",
            "            The json response body from the server",
            "        \"\"\"",
            "        params = {",
            "            \"country\": country,",
            "            \"phone_number\": phone_number,",
            "            \"client_secret\": client_secret,",
            "            \"send_attempt\": send_attempt,",
            "        }",
            "        if next_link:",
            "            params[\"next_link\"] = next_link",
            "",
            "        if self.hs.config.using_identity_server_from_trusted_list:",
            "            # Warn that a deprecated config option is in use",
            "            logger.warning(",
            "                'The config option \"trust_identity_server_for_password_resets\" '",
            "                'has been replaced by \"account_threepid_delegate\". '",
            "                \"Please consult the sample config at docs/sample_config.yaml for \"",
            "                \"details and update your config file.\"",
            "            )",
            "",
            "        try:",
            "            data = await self.http_client.post_json_get_json(",
            "                id_server + \"/_matrix/identity/api/v1/validate/msisdn/requestToken\",",
            "                params,",
            "            )",
            "        except HttpResponseException as e:",
            "            logger.info(\"Proxied requestToken failed: %r\", e)",
            "            raise e.to_synapse_error()",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "",
            "        assert self.hs.config.public_baseurl",
            "",
            "        # we need to tell the client to send the token back to us, since it doesn't",
            "        # otherwise know where to send it, so add submit_url response parameter",
            "        # (see also MSC2078)",
            "        data[\"submit_url\"] = (",
            "            self.hs.config.public_baseurl",
            "            + \"_matrix/client/unstable/add_threepid/msisdn/submit_token\"",
            "        )",
            "        return data",
            "",
            "    async def validate_threepid_session(",
            "        self, client_secret: str, sid: str",
            "    ) -> Optional[JsonDict]:",
            "        \"\"\"Validates a threepid session with only the client secret and session ID",
            "        Tries validating against any configured account_threepid_delegates as well as locally.",
            "",
            "        Args:",
            "            client_secret: A secret provided by the client",
            "            sid: The ID of the session",
            "",
            "        Returns:",
            "            The json response if validation was successful, otherwise None",
            "        \"\"\"",
            "        # XXX: We shouldn't need to keep wrapping and unwrapping this value",
            "        threepid_creds = {\"client_secret\": client_secret, \"sid\": sid}",
            "",
            "        # We don't actually know which medium this 3PID is. Thus we first assume it's email,",
            "        # and if validation fails we try msisdn",
            "        validation_session = None",
            "",
            "        # Try to validate as email",
            "        if self.hs.config.threepid_behaviour_email == ThreepidBehaviour.REMOTE:",
            "            # Ask our delegated email identity server",
            "            validation_session = await self.threepid_from_creds(",
            "                self.hs.config.account_threepid_delegate_email, threepid_creds",
            "            )",
            "        elif self.hs.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:",
            "            # Get a validated session matching these details",
            "            validation_session = await self.store.get_threepid_validation_session(",
            "                \"email\", client_secret, sid=sid, validated=True",
            "            )",
            "",
            "        if validation_session:",
            "            return validation_session",
            "",
            "        # Try to validate as msisdn",
            "        if self.hs.config.account_threepid_delegate_msisdn:",
            "            # Ask our delegated msisdn identity server",
            "            validation_session = await self.threepid_from_creds(",
            "                self.hs.config.account_threepid_delegate_msisdn, threepid_creds",
            "            )",
            "",
            "        return validation_session",
            "",
            "    async def proxy_msisdn_submit_token(",
            "        self, id_server: str, client_secret: str, sid: str, token: str",
            "    ) -> JsonDict:",
            "        \"\"\"Proxy a POST submitToken request to an identity server for verification purposes",
            "",
            "        Args:",
            "            id_server: The identity server URL to contact",
            "            client_secret: Secret provided by the client",
            "            sid: The ID of the session",
            "            token: The verification token",
            "",
            "        Raises:",
            "            SynapseError: If we failed to contact the identity server",
            "",
            "        Returns:",
            "            The response dict from the identity server",
            "        \"\"\"",
            "        body = {\"client_secret\": client_secret, \"sid\": sid, \"token\": token}",
            "",
            "        try:",
            "            return await self.http_client.post_json_get_json(",
            "                id_server + \"/_matrix/identity/api/v1/validate/msisdn/submitToken\",",
            "                body,",
            "            )",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "        except HttpResponseException as e:",
            "            logger.warning(\"Error contacting msisdn account_threepid_delegate: %s\", e)",
            "            raise SynapseError(400, \"Error contacting the identity server\")",
            "",
            "    async def lookup_3pid(",
            "        self,",
            "        id_server: str,",
            "        medium: str,",
            "        address: str,",
            "        id_access_token: Optional[str] = None,",
            "    ) -> Optional[str]:",
            "        \"\"\"Looks up a 3pid in the passed identity server.",
            "",
            "        Args:",
            "            id_server: The server name (including port, if required)",
            "                of the identity server to use.",
            "            medium: The type of the third party identifier (e.g. \"email\").",
            "            address: The third party identifier (e.g. \"foo@example.com\").",
            "            id_access_token: The access token to authenticate to the identity",
            "                server with",
            "",
            "        Returns:",
            "            the matrix ID of the 3pid, or None if it is not recognized.",
            "        \"\"\"",
            "        if id_access_token is not None:",
            "            try:",
            "                results = await self._lookup_3pid_v2(",
            "                    id_server, id_access_token, medium, address",
            "                )",
            "                return results",
            "",
            "            except Exception as e:",
            "                # Catch HttpResponseExcept for a non-200 response code",
            "                # Check if this identity server does not know about v2 lookups",
            "                if isinstance(e, HttpResponseException) and e.code == 404:",
            "                    # This is an old identity server that does not yet support v2 lookups",
            "                    logger.warning(",
            "                        \"Attempted v2 lookup on v1 identity server %s. Falling \"",
            "                        \"back to v1\",",
            "                        id_server,",
            "                    )",
            "                else:",
            "                    logger.warning(\"Error when looking up hashing details: %s\", e)",
            "                    return None",
            "",
            "        return await self._lookup_3pid_v1(id_server, medium, address)",
            "",
            "    async def _lookup_3pid_v1(",
            "        self, id_server: str, medium: str, address: str",
            "    ) -> Optional[str]:",
            "        \"\"\"Looks up a 3pid in the passed identity server using v1 lookup.",
            "",
            "        Args:",
            "            id_server: The server name (including port, if required)",
            "                of the identity server to use.",
            "            medium: The type of the third party identifier (e.g. \"email\").",
            "            address: The third party identifier (e.g. \"foo@example.com\").",
            "",
            "        Returns:",
            "            the matrix ID of the 3pid, or None if it is not recognized.",
            "        \"\"\"",
            "        try:",
            "            data = await self.blacklisting_http_client.get_json(",
            "                \"%s%s/_matrix/identity/api/v1/lookup\" % (id_server_scheme, id_server),",
            "                {\"medium\": medium, \"address\": address},",
            "            )",
            "",
            "            if \"mxid\" in data:",
            "                # note: we used to verify the identity server's signature here, but no longer",
            "                # require or validate it. See the following for context:",
            "                # https://github.com/matrix-org/synapse/issues/5253#issuecomment-666246950",
            "                return data[\"mxid\"]",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "        except IOError as e:",
            "            logger.warning(\"Error from v1 identity server lookup: %s\" % (e,))",
            "",
            "        return None",
            "",
            "    async def _lookup_3pid_v2(",
            "        self, id_server: str, id_access_token: str, medium: str, address: str",
            "    ) -> Optional[str]:",
            "        \"\"\"Looks up a 3pid in the passed identity server using v2 lookup.",
            "",
            "        Args:",
            "            id_server: The server name (including port, if required)",
            "                of the identity server to use.",
            "            id_access_token: The access token to authenticate to the identity server with",
            "            medium: The type of the third party identifier (e.g. \"email\").",
            "            address: The third party identifier (e.g. \"foo@example.com\").",
            "",
            "        Returns:",
            "            the matrix ID of the 3pid, or None if it is not recognised.",
            "        \"\"\"",
            "        # Check what hashing details are supported by this identity server",
            "        try:",
            "            hash_details = await self.blacklisting_http_client.get_json(",
            "                \"%s%s/_matrix/identity/v2/hash_details\" % (id_server_scheme, id_server),",
            "                {\"access_token\": id_access_token},",
            "            )",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "",
            "        if not isinstance(hash_details, dict):",
            "            logger.warning(",
            "                \"Got non-dict object when checking hash details of %s%s: %s\",",
            "                id_server_scheme,",
            "                id_server,",
            "                hash_details,",
            "            )",
            "            raise SynapseError(",
            "                400,",
            "                \"Non-dict object from %s%s during v2 hash_details request: %s\"",
            "                % (id_server_scheme, id_server, hash_details),",
            "            )",
            "",
            "        # Extract information from hash_details",
            "        supported_lookup_algorithms = hash_details.get(\"algorithms\")",
            "        lookup_pepper = hash_details.get(\"lookup_pepper\")",
            "        if (",
            "            not supported_lookup_algorithms",
            "            or not isinstance(supported_lookup_algorithms, list)",
            "            or not lookup_pepper",
            "            or not isinstance(lookup_pepper, str)",
            "        ):",
            "            raise SynapseError(",
            "                400,",
            "                \"Invalid hash details received from identity server %s%s: %s\"",
            "                % (id_server_scheme, id_server, hash_details),",
            "            )",
            "",
            "        # Check if any of the supported lookup algorithms are present",
            "        if LookupAlgorithm.SHA256 in supported_lookup_algorithms:",
            "            # Perform a hashed lookup",
            "            lookup_algorithm = LookupAlgorithm.SHA256",
            "",
            "            # Hash address, medium and the pepper with sha256",
            "            to_hash = \"%s %s %s\" % (address, medium, lookup_pepper)",
            "            lookup_value = sha256_and_url_safe_base64(to_hash)",
            "",
            "        elif LookupAlgorithm.NONE in supported_lookup_algorithms:",
            "            # Perform a non-hashed lookup",
            "            lookup_algorithm = LookupAlgorithm.NONE",
            "",
            "            # Combine together plaintext address and medium",
            "            lookup_value = \"%s %s\" % (address, medium)",
            "",
            "        else:",
            "            logger.warning(",
            "                \"None of the provided lookup algorithms of %s are supported: %s\",",
            "                id_server,",
            "                supported_lookup_algorithms,",
            "            )",
            "            raise SynapseError(",
            "                400,",
            "                \"Provided identity server does not support any v2 lookup \"",
            "                \"algorithms that this homeserver supports.\",",
            "            )",
            "",
            "        # Authenticate with identity server given the access token from the client",
            "        headers = {\"Authorization\": create_id_access_token_header(id_access_token)}",
            "",
            "        try:",
            "            lookup_results = await self.blacklisting_http_client.post_json_get_json(",
            "                \"%s%s/_matrix/identity/v2/lookup\" % (id_server_scheme, id_server),",
            "                {",
            "                    \"addresses\": [lookup_value],",
            "                    \"algorithm\": lookup_algorithm,",
            "                    \"pepper\": lookup_pepper,",
            "                },",
            "                headers=headers,",
            "            )",
            "        except RequestTimedOutError:",
            "            raise SynapseError(500, \"Timed out contacting identity server\")",
            "        except Exception as e:",
            "            logger.warning(\"Error when performing a v2 3pid lookup: %s\", e)",
            "            raise SynapseError(",
            "                500, \"Unknown error occurred during identity server lookup\"",
            "            )",
            "",
            "        # Check for a mapping from what we looked up to an MXID",
            "        if \"mappings\" not in lookup_results or not isinstance(",
            "            lookup_results[\"mappings\"], dict",
            "        ):",
            "            logger.warning(\"No results from 3pid lookup\")",
            "            return None",
            "",
            "        # Return the MXID if it's available, or None otherwise",
            "        mxid = lookup_results[\"mappings\"].get(lookup_value)",
            "        return mxid",
            "",
            "    async def ask_id_server_for_third_party_invite(",
            "        self,",
            "        requester: Requester,",
            "        id_server: str,",
            "        medium: str,",
            "        address: str,",
            "        room_id: str,",
            "        inviter_user_id: str,",
            "        room_alias: str,",
            "        room_avatar_url: str,",
            "        room_join_rules: str,",
            "        room_name: str,",
            "        inviter_display_name: str,",
            "        inviter_avatar_url: str,",
            "        id_access_token: Optional[str] = None,",
            "    ) -> Tuple[str, List[Dict[str, str]], Dict[str, str], str]:",
            "        \"\"\"",
            "        Asks an identity server for a third party invite.",
            "",
            "        Args:",
            "            requester",
            "            id_server: hostname + optional port for the identity server.",
            "            medium: The literal string \"email\".",
            "            address: The third party address being invited.",
            "            room_id: The ID of the room to which the user is invited.",
            "            inviter_user_id: The user ID of the inviter.",
            "            room_alias: An alias for the room, for cosmetic notifications.",
            "            room_avatar_url: The URL of the room's avatar, for cosmetic",
            "                notifications.",
            "            room_join_rules: The join rules of the email (e.g. \"public\").",
            "            room_name: The m.room.name of the room.",
            "            inviter_display_name: The current display name of the",
            "                inviter.",
            "            inviter_avatar_url: The URL of the inviter's avatar.",
            "            id_access_token (str|None): The access token to authenticate to the identity",
            "                server with",
            "",
            "        Returns:",
            "            A tuple containing:",
            "                token: The token which must be signed to prove authenticity.",
            "                public_keys ([{\"public_key\": str, \"key_validity_url\": str}]):",
            "                    public_key is a base64-encoded ed25519 public key.",
            "                fallback_public_key: One element from public_keys.",
            "                display_name: A user-friendly name to represent the invited user.",
            "        \"\"\"",
            "        invite_config = {",
            "            \"medium\": medium,",
            "            \"address\": address,",
            "            \"room_id\": room_id,",
            "            \"room_alias\": room_alias,",
            "            \"room_avatar_url\": room_avatar_url,",
            "            \"room_join_rules\": room_join_rules,",
            "            \"room_name\": room_name,",
            "            \"sender\": inviter_user_id,",
            "            \"sender_display_name\": inviter_display_name,",
            "            \"sender_avatar_url\": inviter_avatar_url,",
            "        }",
            "",
            "        # Add the identity service access token to the JSON body and use the v2",
            "        # Identity Service endpoints if id_access_token is present",
            "        data = None",
            "        base_url = \"%s%s/_matrix/identity\" % (id_server_scheme, id_server)",
            "",
            "        if id_access_token:",
            "            key_validity_url = \"%s%s/_matrix/identity/v2/pubkey/isvalid\" % (",
            "                id_server_scheme,",
            "                id_server,",
            "            )",
            "",
            "            # Attempt a v2 lookup",
            "            url = base_url + \"/v2/store-invite\"",
            "            try:",
            "                data = await self.blacklisting_http_client.post_json_get_json(",
            "                    url,",
            "                    invite_config,",
            "                    {\"Authorization\": create_id_access_token_header(id_access_token)},",
            "                )",
            "            except RequestTimedOutError:",
            "                raise SynapseError(500, \"Timed out contacting identity server\")",
            "            except HttpResponseException as e:",
            "                if e.code != 404:",
            "                    logger.info(\"Failed to POST %s with JSON: %s\", url, e)",
            "                    raise e",
            "",
            "        if data is None:",
            "            key_validity_url = \"%s%s/_matrix/identity/api/v1/pubkey/isvalid\" % (",
            "                id_server_scheme,",
            "                id_server,",
            "            )",
            "            url = base_url + \"/api/v1/store-invite\"",
            "",
            "            try:",
            "                data = await self.blacklisting_http_client.post_json_get_json(",
            "                    url, invite_config",
            "                )",
            "            except RequestTimedOutError:",
            "                raise SynapseError(500, \"Timed out contacting identity server\")",
            "            except HttpResponseException as e:",
            "                logger.warning(",
            "                    \"Error trying to call /store-invite on %s%s: %s\",",
            "                    id_server_scheme,",
            "                    id_server,",
            "                    e,",
            "                )",
            "",
            "            if data is None:",
            "                # Some identity servers may only support application/x-www-form-urlencoded",
            "                # types. This is especially true with old instances of Sydent, see",
            "                # https://github.com/matrix-org/sydent/pull/170",
            "                try:",
            "                    data = await self.blacklisting_http_client.post_urlencoded_get_json(",
            "                        url, invite_config",
            "                    )",
            "                except HttpResponseException as e:",
            "                    logger.warning(",
            "                        \"Error calling /store-invite on %s%s with fallback \"",
            "                        \"encoding: %s\",",
            "                        id_server_scheme,",
            "                        id_server,",
            "                        e,",
            "                    )",
            "                    raise e",
            "",
            "        # TODO: Check for success",
            "        token = data[\"token\"]",
            "        public_keys = data.get(\"public_keys\", [])",
            "        if \"public_key\" in data:",
            "            fallback_public_key = {",
            "                \"public_key\": data[\"public_key\"],",
            "                \"key_validity_url\": key_validity_url,",
            "            }",
            "        else:",
            "            fallback_public_key = public_keys[0]",
            "",
            "        if not public_keys:",
            "            public_keys.append(fallback_public_key)",
            "        display_name = data[\"display_name\"]",
            "        return token, public_keys, fallback_public_key, display_name",
            "",
            "",
            "def create_id_access_token_header(id_access_token: str) -> List[str]:",
            "    \"\"\"Create an Authorization header for passing to SimpleHttpClient as the header value",
            "    of an HTTP request.",
            "",
            "    Args:",
            "        id_access_token: An identity server access token.",
            "",
            "    Returns:",
            "        The ascii-encoded bearer token encased in a list.",
            "    \"\"\"",
            "    # Prefix with Bearer",
            "    bearer_token = \"Bearer %s\" % id_access_token",
            "",
            "    # Encode headers to standard ascii",
            "    bearer_token.encode(\"ascii\")",
            "",
            "    # Return as a list as that's how SimpleHttpClient takes header values",
            "    return [bearer_token]",
            "",
            "",
            "class LookupAlgorithm:",
            "    \"\"\"",
            "    Supported hashing algorithms when performing a 3PID lookup.",
            "",
            "    SHA256 - Hashing an (address, medium, pepper) combo with sha256, then url-safe base64",
            "        encoding",
            "    NONE - Not performing any hashing. Simply sending an (address, medium) combo in plaintext",
            "    \"\"\"",
            "",
            "    SHA256 = \"sha256\"",
            "    NONE = \"none\""
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "50": [
                "IdentityHandler",
                "__init__"
            ],
            "51": [
                "IdentityHandler",
                "__init__"
            ],
            "55": [
                "IdentityHandler",
                "__init__"
            ]
        },
        "addLocation": [
            "src.octoprint.server.api",
            "synapse.handlers.identity.IdentityHandler"
        ]
    },
    "synapse/http/client.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 125,
                "afterPatchRowNumber": 125,
                "PatchRowcode": "     return _scheduler"
            },
            "1": {
                "beforePatchRowNumber": 126,
                "afterPatchRowNumber": 126,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 127,
                "afterPatchRowNumber": 127,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 128,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-class IPBlacklistingResolver:"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 128,
                "PatchRowcode": "+class _IPBlacklistingResolver:"
            },
            "5": {
                "beforePatchRowNumber": 129,
                "afterPatchRowNumber": 129,
                "PatchRowcode": "     \"\"\""
            },
            "6": {
                "beforePatchRowNumber": 130,
                "afterPatchRowNumber": 130,
                "PatchRowcode": "     A proxy for reactor.nameResolver which only produces non-blacklisted IP"
            },
            "7": {
                "beforePatchRowNumber": 131,
                "afterPatchRowNumber": 131,
                "PatchRowcode": "     addresses, preventing DNS rebinding attacks on URL preview."
            },
            "8": {
                "beforePatchRowNumber": 199,
                "afterPatchRowNumber": 199,
                "PatchRowcode": "         return r"
            },
            "9": {
                "beforePatchRowNumber": 200,
                "afterPatchRowNumber": 200,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 201,
                "afterPatchRowNumber": 201,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 202,
                "PatchRowcode": "+@implementer(IReactorPluggableNameResolver)"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 203,
                "PatchRowcode": "+class BlacklistingReactorWrapper:"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 204,
                "PatchRowcode": "+    \"\"\""
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 205,
                "PatchRowcode": "+    A Reactor wrapper which will prevent DNS resolution to blacklisted IP"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 206,
                "PatchRowcode": "+    addresses, to prevent DNS rebinding."
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 207,
                "PatchRowcode": "+    \"\"\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 208,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 209,
                "PatchRowcode": "+    def __init__("
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 210,
                "PatchRowcode": "+        self,"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 211,
                "PatchRowcode": "+        reactor: IReactorPluggableNameResolver,"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 212,
                "PatchRowcode": "+        ip_whitelist: Optional[IPSet],"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 213,
                "PatchRowcode": "+        ip_blacklist: IPSet,"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 214,
                "PatchRowcode": "+    ):"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 215,
                "PatchRowcode": "+        self._reactor = reactor"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 216,
                "PatchRowcode": "+"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 217,
                "PatchRowcode": "+        # We need to use a DNS resolver which filters out blacklisted IP"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 218,
                "PatchRowcode": "+        # addresses, to prevent DNS rebinding."
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 219,
                "PatchRowcode": "+        self._nameResolver = _IPBlacklistingResolver("
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 220,
                "PatchRowcode": "+            self._reactor, ip_whitelist, ip_blacklist"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 221,
                "PatchRowcode": "+        )"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 222,
                "PatchRowcode": "+"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 223,
                "PatchRowcode": "+    def __getattr__(self, attr: str) -> Any:"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 224,
                "PatchRowcode": "+        # Passthrough to the real reactor except for the DNS resolver."
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 225,
                "PatchRowcode": "+        if attr == \"nameResolver\":"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 226,
                "PatchRowcode": "+            return self._nameResolver"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 227,
                "PatchRowcode": "+        else:"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 228,
                "PatchRowcode": "+            return getattr(self._reactor, attr)"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 229,
                "PatchRowcode": "+"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 230,
                "PatchRowcode": "+"
            },
            "40": {
                "beforePatchRowNumber": 202,
                "afterPatchRowNumber": 231,
                "PatchRowcode": " class BlacklistingAgentWrapper(Agent):"
            },
            "41": {
                "beforePatchRowNumber": 203,
                "afterPatchRowNumber": 232,
                "PatchRowcode": "     \"\"\""
            },
            "42": {
                "beforePatchRowNumber": 204,
                "afterPatchRowNumber": 233,
                "PatchRowcode": "     An Agent wrapper which will prevent access to IP addresses being accessed"
            },
            "43": {
                "beforePatchRowNumber": 292,
                "afterPatchRowNumber": 321,
                "PatchRowcode": "         self.user_agent = self.user_agent.encode(\"ascii\")"
            },
            "44": {
                "beforePatchRowNumber": 293,
                "afterPatchRowNumber": 322,
                "PatchRowcode": " "
            },
            "45": {
                "beforePatchRowNumber": 294,
                "afterPatchRowNumber": 323,
                "PatchRowcode": "         if self._ip_blacklist:"
            },
            "46": {
                "beforePatchRowNumber": 295,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            real_reactor = hs.get_reactor()"
            },
            "47": {
                "beforePatchRowNumber": 296,
                "afterPatchRowNumber": 324,
                "PatchRowcode": "             # If we have an IP blacklist, we need to use a DNS resolver which"
            },
            "48": {
                "beforePatchRowNumber": 297,
                "afterPatchRowNumber": 325,
                "PatchRowcode": "             # filters out blacklisted IP addresses, to prevent DNS rebinding."
            },
            "49": {
                "beforePatchRowNumber": 298,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            nameResolver = IPBlacklistingResolver("
            },
            "50": {
                "beforePatchRowNumber": 299,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                real_reactor, self._ip_whitelist, self._ip_blacklist"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 326,
                "PatchRowcode": "+            self.reactor = BlacklistingReactorWrapper("
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 327,
                "PatchRowcode": "+                hs.get_reactor(), self._ip_whitelist, self._ip_blacklist"
            },
            "53": {
                "beforePatchRowNumber": 300,
                "afterPatchRowNumber": 328,
                "PatchRowcode": "             )"
            },
            "54": {
                "beforePatchRowNumber": 301,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "55": {
                "beforePatchRowNumber": 302,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            @implementer(IReactorPluggableNameResolver)"
            },
            "56": {
                "beforePatchRowNumber": 303,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            class Reactor:"
            },
            "57": {
                "beforePatchRowNumber": 304,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                def __getattr__(_self, attr):"
            },
            "58": {
                "beforePatchRowNumber": 305,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    if attr == \"nameResolver\":"
            },
            "59": {
                "beforePatchRowNumber": 306,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        return nameResolver"
            },
            "60": {
                "beforePatchRowNumber": 307,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    else:"
            },
            "61": {
                "beforePatchRowNumber": 308,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        return getattr(real_reactor, attr)"
            },
            "62": {
                "beforePatchRowNumber": 309,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "63": {
                "beforePatchRowNumber": 310,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self.reactor = Reactor()"
            },
            "64": {
                "beforePatchRowNumber": 311,
                "afterPatchRowNumber": 329,
                "PatchRowcode": "         else:"
            },
            "65": {
                "beforePatchRowNumber": 312,
                "afterPatchRowNumber": 330,
                "PatchRowcode": "             self.reactor = hs.get_reactor()"
            },
            "66": {
                "beforePatchRowNumber": 313,
                "afterPatchRowNumber": 331,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "import urllib.parse",
            "from io import BytesIO",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    BinaryIO,",
            "    Dict,",
            "    Iterable,",
            "    List,",
            "    Mapping,",
            "    Optional,",
            "    Sequence,",
            "    Tuple,",
            "    Union,",
            ")",
            "",
            "import treq",
            "from canonicaljson import encode_canonical_json",
            "from netaddr import IPAddress, IPSet",
            "from prometheus_client import Counter",
            "from zope.interface import implementer, provider",
            "",
            "from OpenSSL import SSL",
            "from OpenSSL.SSL import VERIFY_NONE",
            "from twisted.internet import defer, error as twisted_error, protocol, ssl",
            "from twisted.internet.interfaces import (",
            "    IAddress,",
            "    IHostResolution,",
            "    IReactorPluggableNameResolver,",
            "    IResolutionReceiver,",
            ")",
            "from twisted.internet.task import Cooperator",
            "from twisted.python.failure import Failure",
            "from twisted.web._newclient import ResponseDone",
            "from twisted.web.client import (",
            "    Agent,",
            "    HTTPConnectionPool,",
            "    ResponseNeverReceived,",
            "    readBody,",
            ")",
            "from twisted.web.http import PotentialDataLoss",
            "from twisted.web.http_headers import Headers",
            "from twisted.web.iweb import IAgent, IBodyProducer, IResponse",
            "",
            "from synapse.api.errors import Codes, HttpResponseException, SynapseError",
            "from synapse.http import QuieterFileBodyProducer, RequestTimedOutError, redact_uri",
            "from synapse.http.proxyagent import ProxyAgent",
            "from synapse.logging.context import make_deferred_yieldable",
            "from synapse.logging.opentracing import set_tag, start_active_span, tags",
            "from synapse.util import json_decoder",
            "from synapse.util.async_helpers import timeout_deferred",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.app.homeserver import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "outgoing_requests_counter = Counter(\"synapse_http_client_requests\", \"\", [\"method\"])",
            "incoming_responses_counter = Counter(",
            "    \"synapse_http_client_responses\", \"\", [\"method\", \"code\"]",
            ")",
            "",
            "# the type of the headers list, to be passed to the t.w.h.Headers.",
            "# Actually we can mix str and bytes keys, but Mapping treats 'key' as invariant so",
            "# we simplify.",
            "RawHeaders = Union[Mapping[str, \"RawHeaderValue\"], Mapping[bytes, \"RawHeaderValue\"]]",
            "",
            "# the value actually has to be a List, but List is invariant so we can't specify that",
            "# the entries can either be Lists or bytes.",
            "RawHeaderValue = Sequence[Union[str, bytes]]",
            "",
            "# the type of the query params, to be passed into `urlencode`",
            "QueryParamValue = Union[str, bytes, Iterable[Union[str, bytes]]]",
            "QueryParams = Union[Mapping[str, QueryParamValue], Mapping[bytes, QueryParamValue]]",
            "",
            "",
            "def check_against_blacklist(",
            "    ip_address: IPAddress, ip_whitelist: Optional[IPSet], ip_blacklist: IPSet",
            ") -> bool:",
            "    \"\"\"",
            "    Compares an IP address to allowed and disallowed IP sets.",
            "",
            "    Args:",
            "        ip_address: The IP address to check",
            "        ip_whitelist: Allowed IP addresses.",
            "        ip_blacklist: Disallowed IP addresses.",
            "",
            "    Returns:",
            "        True if the IP address is in the blacklist and not in the whitelist.",
            "    \"\"\"",
            "    if ip_address in ip_blacklist:",
            "        if ip_whitelist is None or ip_address not in ip_whitelist:",
            "            return True",
            "    return False",
            "",
            "",
            "_EPSILON = 0.00000001",
            "",
            "",
            "def _make_scheduler(reactor):",
            "    \"\"\"Makes a schedular suitable for a Cooperator using the given reactor.",
            "",
            "    (This is effectively just a copy from `twisted.internet.task`)",
            "    \"\"\"",
            "",
            "    def _scheduler(x):",
            "        return reactor.callLater(_EPSILON, x)",
            "",
            "    return _scheduler",
            "",
            "",
            "class IPBlacklistingResolver:",
            "    \"\"\"",
            "    A proxy for reactor.nameResolver which only produces non-blacklisted IP",
            "    addresses, preventing DNS rebinding attacks on URL preview.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        reactor: IReactorPluggableNameResolver,",
            "        ip_whitelist: Optional[IPSet],",
            "        ip_blacklist: IPSet,",
            "    ):",
            "        \"\"\"",
            "        Args:",
            "            reactor: The twisted reactor.",
            "            ip_whitelist: IP addresses to allow.",
            "            ip_blacklist: IP addresses to disallow.",
            "        \"\"\"",
            "        self._reactor = reactor",
            "        self._ip_whitelist = ip_whitelist",
            "        self._ip_blacklist = ip_blacklist",
            "",
            "    def resolveHostName(",
            "        self, recv: IResolutionReceiver, hostname: str, portNumber: int = 0",
            "    ) -> IResolutionReceiver:",
            "",
            "        r = recv()",
            "        addresses = []  # type: List[IAddress]",
            "",
            "        def _callback() -> None:",
            "            r.resolutionBegan(None)",
            "",
            "            has_bad_ip = False",
            "            for i in addresses:",
            "                ip_address = IPAddress(i.host)",
            "",
            "                if check_against_blacklist(",
            "                    ip_address, self._ip_whitelist, self._ip_blacklist",
            "                ):",
            "                    logger.info(",
            "                        \"Dropped %s from DNS resolution to %s due to blacklist\"",
            "                        % (ip_address, hostname)",
            "                    )",
            "                    has_bad_ip = True",
            "",
            "            # if we have a blacklisted IP, we'd like to raise an error to block the",
            "            # request, but all we can really do from here is claim that there were no",
            "            # valid results.",
            "            if not has_bad_ip:",
            "                for i in addresses:",
            "                    r.addressResolved(i)",
            "            r.resolutionComplete()",
            "",
            "        @provider(IResolutionReceiver)",
            "        class EndpointReceiver:",
            "            @staticmethod",
            "            def resolutionBegan(resolutionInProgress: IHostResolution) -> None:",
            "                pass",
            "",
            "            @staticmethod",
            "            def addressResolved(address: IAddress) -> None:",
            "                addresses.append(address)",
            "",
            "            @staticmethod",
            "            def resolutionComplete() -> None:",
            "                _callback()",
            "",
            "        self._reactor.nameResolver.resolveHostName(",
            "            EndpointReceiver, hostname, portNumber=portNumber",
            "        )",
            "",
            "        return r",
            "",
            "",
            "class BlacklistingAgentWrapper(Agent):",
            "    \"\"\"",
            "    An Agent wrapper which will prevent access to IP addresses being accessed",
            "    directly (without an IP address lookup).",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        agent: IAgent,",
            "        ip_whitelist: Optional[IPSet] = None,",
            "        ip_blacklist: Optional[IPSet] = None,",
            "    ):",
            "        \"\"\"",
            "        Args:",
            "            agent: The Agent to wrap.",
            "            ip_whitelist: IP addresses to allow.",
            "            ip_blacklist: IP addresses to disallow.",
            "        \"\"\"",
            "        self._agent = agent",
            "        self._ip_whitelist = ip_whitelist",
            "        self._ip_blacklist = ip_blacklist",
            "",
            "    def request(",
            "        self,",
            "        method: bytes,",
            "        uri: bytes,",
            "        headers: Optional[Headers] = None,",
            "        bodyProducer: Optional[IBodyProducer] = None,",
            "    ) -> defer.Deferred:",
            "        h = urllib.parse.urlparse(uri.decode(\"ascii\"))",
            "",
            "        try:",
            "            ip_address = IPAddress(h.hostname)",
            "",
            "            if check_against_blacklist(",
            "                ip_address, self._ip_whitelist, self._ip_blacklist",
            "            ):",
            "                logger.info(\"Blocking access to %s due to blacklist\" % (ip_address,))",
            "                e = SynapseError(403, \"IP address blocked by IP blacklist entry\")",
            "                return defer.fail(Failure(e))",
            "        except Exception:",
            "            # Not an IP",
            "            pass",
            "",
            "        return self._agent.request(",
            "            method, uri, headers=headers, bodyProducer=bodyProducer",
            "        )",
            "",
            "",
            "class SimpleHttpClient:",
            "    \"\"\"",
            "    A simple, no-frills HTTP client with methods that wrap up common ways of",
            "    using HTTP in Matrix",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        hs: \"HomeServer\",",
            "        treq_args: Dict[str, Any] = {},",
            "        ip_whitelist: Optional[IPSet] = None,",
            "        ip_blacklist: Optional[IPSet] = None,",
            "        http_proxy: Optional[bytes] = None,",
            "        https_proxy: Optional[bytes] = None,",
            "    ):",
            "        \"\"\"",
            "        Args:",
            "            hs",
            "            treq_args: Extra keyword arguments to be given to treq.request.",
            "            ip_blacklist: The IP addresses that are blacklisted that",
            "                we may not request.",
            "            ip_whitelist: The whitelisted IP addresses, that we can",
            "               request if it were otherwise caught in a blacklist.",
            "            http_proxy: proxy server to use for http connections. host[:port]",
            "            https_proxy: proxy server to use for https connections. host[:port]",
            "        \"\"\"",
            "        self.hs = hs",
            "",
            "        self._ip_whitelist = ip_whitelist",
            "        self._ip_blacklist = ip_blacklist",
            "        self._extra_treq_args = treq_args",
            "",
            "        self.user_agent = hs.version_string",
            "        self.clock = hs.get_clock()",
            "        if hs.config.user_agent_suffix:",
            "            self.user_agent = \"%s %s\" % (self.user_agent, hs.config.user_agent_suffix)",
            "",
            "        # We use this for our body producers to ensure that they use the correct",
            "        # reactor.",
            "        self._cooperator = Cooperator(scheduler=_make_scheduler(hs.get_reactor()))",
            "",
            "        self.user_agent = self.user_agent.encode(\"ascii\")",
            "",
            "        if self._ip_blacklist:",
            "            real_reactor = hs.get_reactor()",
            "            # If we have an IP blacklist, we need to use a DNS resolver which",
            "            # filters out blacklisted IP addresses, to prevent DNS rebinding.",
            "            nameResolver = IPBlacklistingResolver(",
            "                real_reactor, self._ip_whitelist, self._ip_blacklist",
            "            )",
            "",
            "            @implementer(IReactorPluggableNameResolver)",
            "            class Reactor:",
            "                def __getattr__(_self, attr):",
            "                    if attr == \"nameResolver\":",
            "                        return nameResolver",
            "                    else:",
            "                        return getattr(real_reactor, attr)",
            "",
            "            self.reactor = Reactor()",
            "        else:",
            "            self.reactor = hs.get_reactor()",
            "",
            "        # the pusher makes lots of concurrent SSL connections to sygnal, and",
            "        # tends to do so in batches, so we need to allow the pool to keep",
            "        # lots of idle connections around.",
            "        pool = HTTPConnectionPool(self.reactor)",
            "        # XXX: The justification for using the cache factor here is that larger instances",
            "        # will need both more cache and more connections.",
            "        # Still, this should probably be a separate dial",
            "        pool.maxPersistentPerHost = max((100 * hs.config.caches.global_factor, 5))",
            "        pool.cachedConnectionTimeout = 2 * 60",
            "",
            "        self.agent = ProxyAgent(",
            "            self.reactor,",
            "            connectTimeout=15,",
            "            contextFactory=self.hs.get_http_client_context_factory(),",
            "            pool=pool,",
            "            http_proxy=http_proxy,",
            "            https_proxy=https_proxy,",
            "        )",
            "",
            "        if self._ip_blacklist:",
            "            # If we have an IP blacklist, we then install the blacklisting Agent",
            "            # which prevents direct access to IP addresses, that are not caught",
            "            # by the DNS resolution.",
            "            self.agent = BlacklistingAgentWrapper(",
            "                self.agent,",
            "                ip_whitelist=self._ip_whitelist,",
            "                ip_blacklist=self._ip_blacklist,",
            "            )",
            "",
            "    async def request(",
            "        self,",
            "        method: str,",
            "        uri: str,",
            "        data: Optional[bytes] = None,",
            "        headers: Optional[Headers] = None,",
            "    ) -> IResponse:",
            "        \"\"\"",
            "        Args:",
            "            method: HTTP method to use.",
            "            uri: URI to query.",
            "            data: Data to send in the request body, if applicable.",
            "            headers: Request headers.",
            "",
            "        Returns:",
            "            Response object, once the headers have been read.",
            "",
            "        Raises:",
            "            RequestTimedOutError if the request times out before the headers are read",
            "",
            "        \"\"\"",
            "        outgoing_requests_counter.labels(method).inc()",
            "",
            "        # log request but strip `access_token` (AS requests for example include this)",
            "        logger.debug(\"Sending request %s %s\", method, redact_uri(uri))",
            "",
            "        with start_active_span(",
            "            \"outgoing-client-request\",",
            "            tags={",
            "                tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,",
            "                tags.HTTP_METHOD: method,",
            "                tags.HTTP_URL: uri,",
            "            },",
            "            finish_on_close=True,",
            "        ):",
            "            try:",
            "                body_producer = None",
            "                if data is not None:",
            "                    body_producer = QuieterFileBodyProducer(",
            "                        BytesIO(data), cooperator=self._cooperator,",
            "                    )",
            "",
            "                request_deferred = treq.request(",
            "                    method,",
            "                    uri,",
            "                    agent=self.agent,",
            "                    data=body_producer,",
            "                    headers=headers,",
            "                    **self._extra_treq_args,",
            "                )  # type: defer.Deferred",
            "",
            "                # we use our own timeout mechanism rather than treq's as a workaround",
            "                # for https://twistedmatrix.com/trac/ticket/9534.",
            "                request_deferred = timeout_deferred(",
            "                    request_deferred, 60, self.hs.get_reactor(),",
            "                )",
            "",
            "                # turn timeouts into RequestTimedOutErrors",
            "                request_deferred.addErrback(_timeout_to_request_timed_out_error)",
            "",
            "                response = await make_deferred_yieldable(request_deferred)",
            "",
            "                incoming_responses_counter.labels(method, response.code).inc()",
            "                logger.info(",
            "                    \"Received response to %s %s: %s\",",
            "                    method,",
            "                    redact_uri(uri),",
            "                    response.code,",
            "                )",
            "                return response",
            "            except Exception as e:",
            "                incoming_responses_counter.labels(method, \"ERR\").inc()",
            "                logger.info(",
            "                    \"Error sending request to  %s %s: %s %s\",",
            "                    method,",
            "                    redact_uri(uri),",
            "                    type(e).__name__,",
            "                    e.args[0],",
            "                )",
            "                set_tag(tags.ERROR, True)",
            "                set_tag(\"error_reason\", e.args[0])",
            "                raise",
            "",
            "    async def post_urlencoded_get_json(",
            "        self,",
            "        uri: str,",
            "        args: Optional[Mapping[str, Union[str, List[str]]]] = None,",
            "        headers: Optional[RawHeaders] = None,",
            "    ) -> Any:",
            "        \"\"\"",
            "        Args:",
            "            uri: uri to query",
            "            args: parameters to be url-encoded in the body",
            "            headers: a map from header name to a list of values for that header",
            "",
            "        Returns:",
            "            parsed json",
            "",
            "        Raises:",
            "            RequestTimedOutError: if there is a timeout before the response headers",
            "               are received. Note there is currently no timeout on reading the response",
            "               body.",
            "",
            "            HttpResponseException: On a non-2xx HTTP response.",
            "",
            "            ValueError: if the response was not JSON",
            "        \"\"\"",
            "",
            "        # TODO: Do we ever want to log message contents?",
            "        logger.debug(\"post_urlencoded_get_json args: %s\", args)",
            "",
            "        query_bytes = encode_query_args(args)",
            "",
            "        actual_headers = {",
            "            b\"Content-Type\": [b\"application/x-www-form-urlencoded\"],",
            "            b\"User-Agent\": [self.user_agent],",
            "            b\"Accept\": [b\"application/json\"],",
            "        }",
            "        if headers:",
            "            actual_headers.update(headers)  # type: ignore",
            "",
            "        response = await self.request(",
            "            \"POST\", uri, headers=Headers(actual_headers), data=query_bytes",
            "        )",
            "",
            "        body = await make_deferred_yieldable(readBody(response))",
            "",
            "        if 200 <= response.code < 300:",
            "            return json_decoder.decode(body.decode(\"utf-8\"))",
            "        else:",
            "            raise HttpResponseException(",
            "                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body",
            "            )",
            "",
            "    async def post_json_get_json(",
            "        self, uri: str, post_json: Any, headers: Optional[RawHeaders] = None",
            "    ) -> Any:",
            "        \"\"\"",
            "",
            "        Args:",
            "            uri: URI to query.",
            "            post_json: request body, to be encoded as json",
            "            headers: a map from header name to a list of values for that header",
            "",
            "        Returns:",
            "            parsed json",
            "",
            "        Raises:",
            "            RequestTimedOutError: if there is a timeout before the response headers",
            "               are received. Note there is currently no timeout on reading the response",
            "               body.",
            "",
            "            HttpResponseException: On a non-2xx HTTP response.",
            "",
            "            ValueError: if the response was not JSON",
            "        \"\"\"",
            "        json_str = encode_canonical_json(post_json)",
            "",
            "        logger.debug(\"HTTP POST %s -> %s\", json_str, uri)",
            "",
            "        actual_headers = {",
            "            b\"Content-Type\": [b\"application/json\"],",
            "            b\"User-Agent\": [self.user_agent],",
            "            b\"Accept\": [b\"application/json\"],",
            "        }",
            "        if headers:",
            "            actual_headers.update(headers)  # type: ignore",
            "",
            "        response = await self.request(",
            "            \"POST\", uri, headers=Headers(actual_headers), data=json_str",
            "        )",
            "",
            "        body = await make_deferred_yieldable(readBody(response))",
            "",
            "        if 200 <= response.code < 300:",
            "            return json_decoder.decode(body.decode(\"utf-8\"))",
            "        else:",
            "            raise HttpResponseException(",
            "                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body",
            "            )",
            "",
            "    async def get_json(",
            "        self,",
            "        uri: str,",
            "        args: Optional[QueryParams] = None,",
            "        headers: Optional[RawHeaders] = None,",
            "    ) -> Any:",
            "        \"\"\"Gets some json from the given URI.",
            "",
            "        Args:",
            "            uri: The URI to request, not including query parameters",
            "            args: A dictionary used to create query string",
            "            headers: a map from header name to a list of values for that header",
            "        Returns:",
            "            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.",
            "        Raises:",
            "            RequestTimedOutError: if there is a timeout before the response headers",
            "               are received. Note there is currently no timeout on reading the response",
            "               body.",
            "",
            "            HttpResponseException On a non-2xx HTTP response.",
            "",
            "            ValueError: if the response was not JSON",
            "        \"\"\"",
            "        actual_headers = {b\"Accept\": [b\"application/json\"]}",
            "        if headers:",
            "            actual_headers.update(headers)  # type: ignore",
            "",
            "        body = await self.get_raw(uri, args, headers=headers)",
            "        return json_decoder.decode(body.decode(\"utf-8\"))",
            "",
            "    async def put_json(",
            "        self,",
            "        uri: str,",
            "        json_body: Any,",
            "        args: Optional[QueryParams] = None,",
            "        headers: RawHeaders = None,",
            "    ) -> Any:",
            "        \"\"\"Puts some json to the given URI.",
            "",
            "        Args:",
            "            uri: The URI to request, not including query parameters",
            "            json_body: The JSON to put in the HTTP body,",
            "            args: A dictionary used to create query strings",
            "            headers: a map from header name to a list of values for that header",
            "        Returns:",
            "            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.",
            "        Raises:",
            "             RequestTimedOutError: if there is a timeout before the response headers",
            "               are received. Note there is currently no timeout on reading the response",
            "               body.",
            "",
            "            HttpResponseException On a non-2xx HTTP response.",
            "",
            "            ValueError: if the response was not JSON",
            "        \"\"\"",
            "        if args:",
            "            query_str = urllib.parse.urlencode(args, True)",
            "            uri = \"%s?%s\" % (uri, query_str)",
            "",
            "        json_str = encode_canonical_json(json_body)",
            "",
            "        actual_headers = {",
            "            b\"Content-Type\": [b\"application/json\"],",
            "            b\"User-Agent\": [self.user_agent],",
            "            b\"Accept\": [b\"application/json\"],",
            "        }",
            "        if headers:",
            "            actual_headers.update(headers)  # type: ignore",
            "",
            "        response = await self.request(",
            "            \"PUT\", uri, headers=Headers(actual_headers), data=json_str",
            "        )",
            "",
            "        body = await make_deferred_yieldable(readBody(response))",
            "",
            "        if 200 <= response.code < 300:",
            "            return json_decoder.decode(body.decode(\"utf-8\"))",
            "        else:",
            "            raise HttpResponseException(",
            "                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body",
            "            )",
            "",
            "    async def get_raw(",
            "        self,",
            "        uri: str,",
            "        args: Optional[QueryParams] = None,",
            "        headers: Optional[RawHeaders] = None,",
            "    ) -> bytes:",
            "        \"\"\"Gets raw text from the given URI.",
            "",
            "        Args:",
            "            uri: The URI to request, not including query parameters",
            "            args: A dictionary used to create query strings",
            "            headers: a map from header name to a list of values for that header",
            "        Returns:",
            "            Succeeds when we get a 2xx HTTP response, with the",
            "            HTTP body as bytes.",
            "        Raises:",
            "            RequestTimedOutError: if there is a timeout before the response headers",
            "               are received. Note there is currently no timeout on reading the response",
            "               body.",
            "",
            "            HttpResponseException on a non-2xx HTTP response.",
            "        \"\"\"",
            "        if args:",
            "            query_str = urllib.parse.urlencode(args, True)",
            "            uri = \"%s?%s\" % (uri, query_str)",
            "",
            "        actual_headers = {b\"User-Agent\": [self.user_agent]}",
            "        if headers:",
            "            actual_headers.update(headers)  # type: ignore",
            "",
            "        response = await self.request(\"GET\", uri, headers=Headers(actual_headers))",
            "",
            "        body = await make_deferred_yieldable(readBody(response))",
            "",
            "        if 200 <= response.code < 300:",
            "            return body",
            "        else:",
            "            raise HttpResponseException(",
            "                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body",
            "            )",
            "",
            "    # XXX: FIXME: This is horribly copy-pasted from matrixfederationclient.",
            "    # The two should be factored out.",
            "",
            "    async def get_file(",
            "        self,",
            "        url: str,",
            "        output_stream: BinaryIO,",
            "        max_size: Optional[int] = None,",
            "        headers: Optional[RawHeaders] = None,",
            "    ) -> Tuple[int, Dict[bytes, List[bytes]], str, int]:",
            "        \"\"\"GETs a file from a given URL",
            "        Args:",
            "            url: The URL to GET",
            "            output_stream: File to write the response body to.",
            "            headers: A map from header name to a list of values for that header",
            "        Returns:",
            "            A tuple of the file length, dict of the response",
            "            headers, absolute URI of the response and HTTP response code.",
            "",
            "        Raises:",
            "            RequestTimedOutError: if there is a timeout before the response headers",
            "               are received. Note there is currently no timeout on reading the response",
            "               body.",
            "",
            "            SynapseError: if the response is not a 2xx, the remote file is too large, or",
            "               another exception happens during the download.",
            "        \"\"\"",
            "",
            "        actual_headers = {b\"User-Agent\": [self.user_agent]}",
            "        if headers:",
            "            actual_headers.update(headers)  # type: ignore",
            "",
            "        response = await self.request(\"GET\", url, headers=Headers(actual_headers))",
            "",
            "        resp_headers = dict(response.headers.getAllRawHeaders())",
            "",
            "        if (",
            "            b\"Content-Length\" in resp_headers",
            "            and max_size",
            "            and int(resp_headers[b\"Content-Length\"][0]) > max_size",
            "        ):",
            "            logger.warning(\"Requested URL is too large > %r bytes\" % (max_size,))",
            "            raise SynapseError(",
            "                502,",
            "                \"Requested file is too large > %r bytes\" % (max_size,),",
            "                Codes.TOO_LARGE,",
            "            )",
            "",
            "        if response.code > 299:",
            "            logger.warning(\"Got %d when downloading %s\" % (response.code, url))",
            "            raise SynapseError(502, \"Got error %d\" % (response.code,), Codes.UNKNOWN)",
            "",
            "        # TODO: if our Content-Type is HTML or something, just read the first",
            "        # N bytes into RAM rather than saving it all to disk only to read it",
            "        # straight back in again",
            "",
            "        try:",
            "            length = await make_deferred_yieldable(",
            "                readBodyToFile(response, output_stream, max_size)",
            "            )",
            "        except SynapseError:",
            "            # This can happen e.g. because the body is too large.",
            "            raise",
            "        except Exception as e:",
            "            raise SynapseError(502, (\"Failed to download remote body: %s\" % e)) from e",
            "",
            "        return (",
            "            length,",
            "            resp_headers,",
            "            response.request.absoluteURI.decode(\"ascii\"),",
            "            response.code,",
            "        )",
            "",
            "",
            "def _timeout_to_request_timed_out_error(f: Failure):",
            "    if f.check(twisted_error.TimeoutError, twisted_error.ConnectingCancelledError):",
            "        # The TCP connection has its own timeout (set by the 'connectTimeout' param",
            "        # on the Agent), which raises twisted_error.TimeoutError exception.",
            "        raise RequestTimedOutError(\"Timeout connecting to remote server\")",
            "    elif f.check(defer.TimeoutError, ResponseNeverReceived):",
            "        # this one means that we hit our overall timeout on the request",
            "        raise RequestTimedOutError(\"Timeout waiting for response from remote server\")",
            "",
            "    return f",
            "",
            "",
            "class _ReadBodyToFileProtocol(protocol.Protocol):",
            "    def __init__(",
            "        self, stream: BinaryIO, deferred: defer.Deferred, max_size: Optional[int]",
            "    ):",
            "        self.stream = stream",
            "        self.deferred = deferred",
            "        self.length = 0",
            "        self.max_size = max_size",
            "",
            "    def dataReceived(self, data: bytes) -> None:",
            "        self.stream.write(data)",
            "        self.length += len(data)",
            "        if self.max_size is not None and self.length >= self.max_size:",
            "            self.deferred.errback(",
            "                SynapseError(",
            "                    502,",
            "                    \"Requested file is too large > %r bytes\" % (self.max_size,),",
            "                    Codes.TOO_LARGE,",
            "                )",
            "            )",
            "            self.deferred = defer.Deferred()",
            "            self.transport.loseConnection()",
            "",
            "    def connectionLost(self, reason: Failure) -> None:",
            "        if reason.check(ResponseDone):",
            "            self.deferred.callback(self.length)",
            "        elif reason.check(PotentialDataLoss):",
            "            # stolen from https://github.com/twisted/treq/pull/49/files",
            "            # http://twistedmatrix.com/trac/ticket/4840",
            "            self.deferred.callback(self.length)",
            "        else:",
            "            self.deferred.errback(reason)",
            "",
            "",
            "def readBodyToFile(",
            "    response: IResponse, stream: BinaryIO, max_size: Optional[int]",
            ") -> defer.Deferred:",
            "    \"\"\"",
            "    Read a HTTP response body to a file-object. Optionally enforcing a maximum file size.",
            "",
            "    Args:",
            "        response: The HTTP response to read from.",
            "        stream: The file-object to write to.",
            "        max_size: The maximum file size to allow.",
            "",
            "    Returns:",
            "        A Deferred which resolves to the length of the read body.",
            "    \"\"\"",
            "",
            "    d = defer.Deferred()",
            "    response.deliverBody(_ReadBodyToFileProtocol(stream, d, max_size))",
            "    return d",
            "",
            "",
            "def encode_query_args(args: Optional[Mapping[str, Union[str, List[str]]]]) -> bytes:",
            "    \"\"\"",
            "    Encodes a map of query arguments to bytes which can be appended to a URL.",
            "",
            "    Args:",
            "        args: The query arguments, a mapping of string to string or list of strings.",
            "",
            "    Returns:",
            "        The query arguments encoded as bytes.",
            "    \"\"\"",
            "    if args is None:",
            "        return b\"\"",
            "",
            "    encoded_args = {}",
            "    for k, vs in args.items():",
            "        if isinstance(vs, str):",
            "            vs = [vs]",
            "        encoded_args[k] = [v.encode(\"utf8\") for v in vs]",
            "",
            "    query_str = urllib.parse.urlencode(encoded_args, True)",
            "",
            "    return query_str.encode(\"utf8\")",
            "",
            "",
            "class InsecureInterceptableContextFactory(ssl.ContextFactory):",
            "    \"\"\"",
            "    Factory for PyOpenSSL SSL contexts which accepts any certificate for any domain.",
            "",
            "    Do not use this since it allows an attacker to intercept your communications.",
            "    \"\"\"",
            "",
            "    def __init__(self):",
            "        self._context = SSL.Context(SSL.SSLv23_METHOD)",
            "        self._context.set_verify(VERIFY_NONE, lambda *_: None)",
            "",
            "    def getContext(self, hostname=None, port=None):",
            "        return self._context",
            "",
            "    def creatorForNetloc(self, hostname, port):",
            "        return self"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "import urllib.parse",
            "from io import BytesIO",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    BinaryIO,",
            "    Dict,",
            "    Iterable,",
            "    List,",
            "    Mapping,",
            "    Optional,",
            "    Sequence,",
            "    Tuple,",
            "    Union,",
            ")",
            "",
            "import treq",
            "from canonicaljson import encode_canonical_json",
            "from netaddr import IPAddress, IPSet",
            "from prometheus_client import Counter",
            "from zope.interface import implementer, provider",
            "",
            "from OpenSSL import SSL",
            "from OpenSSL.SSL import VERIFY_NONE",
            "from twisted.internet import defer, error as twisted_error, protocol, ssl",
            "from twisted.internet.interfaces import (",
            "    IAddress,",
            "    IHostResolution,",
            "    IReactorPluggableNameResolver,",
            "    IResolutionReceiver,",
            ")",
            "from twisted.internet.task import Cooperator",
            "from twisted.python.failure import Failure",
            "from twisted.web._newclient import ResponseDone",
            "from twisted.web.client import (",
            "    Agent,",
            "    HTTPConnectionPool,",
            "    ResponseNeverReceived,",
            "    readBody,",
            ")",
            "from twisted.web.http import PotentialDataLoss",
            "from twisted.web.http_headers import Headers",
            "from twisted.web.iweb import IAgent, IBodyProducer, IResponse",
            "",
            "from synapse.api.errors import Codes, HttpResponseException, SynapseError",
            "from synapse.http import QuieterFileBodyProducer, RequestTimedOutError, redact_uri",
            "from synapse.http.proxyagent import ProxyAgent",
            "from synapse.logging.context import make_deferred_yieldable",
            "from synapse.logging.opentracing import set_tag, start_active_span, tags",
            "from synapse.util import json_decoder",
            "from synapse.util.async_helpers import timeout_deferred",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.app.homeserver import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "outgoing_requests_counter = Counter(\"synapse_http_client_requests\", \"\", [\"method\"])",
            "incoming_responses_counter = Counter(",
            "    \"synapse_http_client_responses\", \"\", [\"method\", \"code\"]",
            ")",
            "",
            "# the type of the headers list, to be passed to the t.w.h.Headers.",
            "# Actually we can mix str and bytes keys, but Mapping treats 'key' as invariant so",
            "# we simplify.",
            "RawHeaders = Union[Mapping[str, \"RawHeaderValue\"], Mapping[bytes, \"RawHeaderValue\"]]",
            "",
            "# the value actually has to be a List, but List is invariant so we can't specify that",
            "# the entries can either be Lists or bytes.",
            "RawHeaderValue = Sequence[Union[str, bytes]]",
            "",
            "# the type of the query params, to be passed into `urlencode`",
            "QueryParamValue = Union[str, bytes, Iterable[Union[str, bytes]]]",
            "QueryParams = Union[Mapping[str, QueryParamValue], Mapping[bytes, QueryParamValue]]",
            "",
            "",
            "def check_against_blacklist(",
            "    ip_address: IPAddress, ip_whitelist: Optional[IPSet], ip_blacklist: IPSet",
            ") -> bool:",
            "    \"\"\"",
            "    Compares an IP address to allowed and disallowed IP sets.",
            "",
            "    Args:",
            "        ip_address: The IP address to check",
            "        ip_whitelist: Allowed IP addresses.",
            "        ip_blacklist: Disallowed IP addresses.",
            "",
            "    Returns:",
            "        True if the IP address is in the blacklist and not in the whitelist.",
            "    \"\"\"",
            "    if ip_address in ip_blacklist:",
            "        if ip_whitelist is None or ip_address not in ip_whitelist:",
            "            return True",
            "    return False",
            "",
            "",
            "_EPSILON = 0.00000001",
            "",
            "",
            "def _make_scheduler(reactor):",
            "    \"\"\"Makes a schedular suitable for a Cooperator using the given reactor.",
            "",
            "    (This is effectively just a copy from `twisted.internet.task`)",
            "    \"\"\"",
            "",
            "    def _scheduler(x):",
            "        return reactor.callLater(_EPSILON, x)",
            "",
            "    return _scheduler",
            "",
            "",
            "class _IPBlacklistingResolver:",
            "    \"\"\"",
            "    A proxy for reactor.nameResolver which only produces non-blacklisted IP",
            "    addresses, preventing DNS rebinding attacks on URL preview.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        reactor: IReactorPluggableNameResolver,",
            "        ip_whitelist: Optional[IPSet],",
            "        ip_blacklist: IPSet,",
            "    ):",
            "        \"\"\"",
            "        Args:",
            "            reactor: The twisted reactor.",
            "            ip_whitelist: IP addresses to allow.",
            "            ip_blacklist: IP addresses to disallow.",
            "        \"\"\"",
            "        self._reactor = reactor",
            "        self._ip_whitelist = ip_whitelist",
            "        self._ip_blacklist = ip_blacklist",
            "",
            "    def resolveHostName(",
            "        self, recv: IResolutionReceiver, hostname: str, portNumber: int = 0",
            "    ) -> IResolutionReceiver:",
            "",
            "        r = recv()",
            "        addresses = []  # type: List[IAddress]",
            "",
            "        def _callback() -> None:",
            "            r.resolutionBegan(None)",
            "",
            "            has_bad_ip = False",
            "            for i in addresses:",
            "                ip_address = IPAddress(i.host)",
            "",
            "                if check_against_blacklist(",
            "                    ip_address, self._ip_whitelist, self._ip_blacklist",
            "                ):",
            "                    logger.info(",
            "                        \"Dropped %s from DNS resolution to %s due to blacklist\"",
            "                        % (ip_address, hostname)",
            "                    )",
            "                    has_bad_ip = True",
            "",
            "            # if we have a blacklisted IP, we'd like to raise an error to block the",
            "            # request, but all we can really do from here is claim that there were no",
            "            # valid results.",
            "            if not has_bad_ip:",
            "                for i in addresses:",
            "                    r.addressResolved(i)",
            "            r.resolutionComplete()",
            "",
            "        @provider(IResolutionReceiver)",
            "        class EndpointReceiver:",
            "            @staticmethod",
            "            def resolutionBegan(resolutionInProgress: IHostResolution) -> None:",
            "                pass",
            "",
            "            @staticmethod",
            "            def addressResolved(address: IAddress) -> None:",
            "                addresses.append(address)",
            "",
            "            @staticmethod",
            "            def resolutionComplete() -> None:",
            "                _callback()",
            "",
            "        self._reactor.nameResolver.resolveHostName(",
            "            EndpointReceiver, hostname, portNumber=portNumber",
            "        )",
            "",
            "        return r",
            "",
            "",
            "@implementer(IReactorPluggableNameResolver)",
            "class BlacklistingReactorWrapper:",
            "    \"\"\"",
            "    A Reactor wrapper which will prevent DNS resolution to blacklisted IP",
            "    addresses, to prevent DNS rebinding.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        reactor: IReactorPluggableNameResolver,",
            "        ip_whitelist: Optional[IPSet],",
            "        ip_blacklist: IPSet,",
            "    ):",
            "        self._reactor = reactor",
            "",
            "        # We need to use a DNS resolver which filters out blacklisted IP",
            "        # addresses, to prevent DNS rebinding.",
            "        self._nameResolver = _IPBlacklistingResolver(",
            "            self._reactor, ip_whitelist, ip_blacklist",
            "        )",
            "",
            "    def __getattr__(self, attr: str) -> Any:",
            "        # Passthrough to the real reactor except for the DNS resolver.",
            "        if attr == \"nameResolver\":",
            "            return self._nameResolver",
            "        else:",
            "            return getattr(self._reactor, attr)",
            "",
            "",
            "class BlacklistingAgentWrapper(Agent):",
            "    \"\"\"",
            "    An Agent wrapper which will prevent access to IP addresses being accessed",
            "    directly (without an IP address lookup).",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        agent: IAgent,",
            "        ip_whitelist: Optional[IPSet] = None,",
            "        ip_blacklist: Optional[IPSet] = None,",
            "    ):",
            "        \"\"\"",
            "        Args:",
            "            agent: The Agent to wrap.",
            "            ip_whitelist: IP addresses to allow.",
            "            ip_blacklist: IP addresses to disallow.",
            "        \"\"\"",
            "        self._agent = agent",
            "        self._ip_whitelist = ip_whitelist",
            "        self._ip_blacklist = ip_blacklist",
            "",
            "    def request(",
            "        self,",
            "        method: bytes,",
            "        uri: bytes,",
            "        headers: Optional[Headers] = None,",
            "        bodyProducer: Optional[IBodyProducer] = None,",
            "    ) -> defer.Deferred:",
            "        h = urllib.parse.urlparse(uri.decode(\"ascii\"))",
            "",
            "        try:",
            "            ip_address = IPAddress(h.hostname)",
            "",
            "            if check_against_blacklist(",
            "                ip_address, self._ip_whitelist, self._ip_blacklist",
            "            ):",
            "                logger.info(\"Blocking access to %s due to blacklist\" % (ip_address,))",
            "                e = SynapseError(403, \"IP address blocked by IP blacklist entry\")",
            "                return defer.fail(Failure(e))",
            "        except Exception:",
            "            # Not an IP",
            "            pass",
            "",
            "        return self._agent.request(",
            "            method, uri, headers=headers, bodyProducer=bodyProducer",
            "        )",
            "",
            "",
            "class SimpleHttpClient:",
            "    \"\"\"",
            "    A simple, no-frills HTTP client with methods that wrap up common ways of",
            "    using HTTP in Matrix",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        hs: \"HomeServer\",",
            "        treq_args: Dict[str, Any] = {},",
            "        ip_whitelist: Optional[IPSet] = None,",
            "        ip_blacklist: Optional[IPSet] = None,",
            "        http_proxy: Optional[bytes] = None,",
            "        https_proxy: Optional[bytes] = None,",
            "    ):",
            "        \"\"\"",
            "        Args:",
            "            hs",
            "            treq_args: Extra keyword arguments to be given to treq.request.",
            "            ip_blacklist: The IP addresses that are blacklisted that",
            "                we may not request.",
            "            ip_whitelist: The whitelisted IP addresses, that we can",
            "               request if it were otherwise caught in a blacklist.",
            "            http_proxy: proxy server to use for http connections. host[:port]",
            "            https_proxy: proxy server to use for https connections. host[:port]",
            "        \"\"\"",
            "        self.hs = hs",
            "",
            "        self._ip_whitelist = ip_whitelist",
            "        self._ip_blacklist = ip_blacklist",
            "        self._extra_treq_args = treq_args",
            "",
            "        self.user_agent = hs.version_string",
            "        self.clock = hs.get_clock()",
            "        if hs.config.user_agent_suffix:",
            "            self.user_agent = \"%s %s\" % (self.user_agent, hs.config.user_agent_suffix)",
            "",
            "        # We use this for our body producers to ensure that they use the correct",
            "        # reactor.",
            "        self._cooperator = Cooperator(scheduler=_make_scheduler(hs.get_reactor()))",
            "",
            "        self.user_agent = self.user_agent.encode(\"ascii\")",
            "",
            "        if self._ip_blacklist:",
            "            # If we have an IP blacklist, we need to use a DNS resolver which",
            "            # filters out blacklisted IP addresses, to prevent DNS rebinding.",
            "            self.reactor = BlacklistingReactorWrapper(",
            "                hs.get_reactor(), self._ip_whitelist, self._ip_blacklist",
            "            )",
            "        else:",
            "            self.reactor = hs.get_reactor()",
            "",
            "        # the pusher makes lots of concurrent SSL connections to sygnal, and",
            "        # tends to do so in batches, so we need to allow the pool to keep",
            "        # lots of idle connections around.",
            "        pool = HTTPConnectionPool(self.reactor)",
            "        # XXX: The justification for using the cache factor here is that larger instances",
            "        # will need both more cache and more connections.",
            "        # Still, this should probably be a separate dial",
            "        pool.maxPersistentPerHost = max((100 * hs.config.caches.global_factor, 5))",
            "        pool.cachedConnectionTimeout = 2 * 60",
            "",
            "        self.agent = ProxyAgent(",
            "            self.reactor,",
            "            connectTimeout=15,",
            "            contextFactory=self.hs.get_http_client_context_factory(),",
            "            pool=pool,",
            "            http_proxy=http_proxy,",
            "            https_proxy=https_proxy,",
            "        )",
            "",
            "        if self._ip_blacklist:",
            "            # If we have an IP blacklist, we then install the blacklisting Agent",
            "            # which prevents direct access to IP addresses, that are not caught",
            "            # by the DNS resolution.",
            "            self.agent = BlacklistingAgentWrapper(",
            "                self.agent,",
            "                ip_whitelist=self._ip_whitelist,",
            "                ip_blacklist=self._ip_blacklist,",
            "            )",
            "",
            "    async def request(",
            "        self,",
            "        method: str,",
            "        uri: str,",
            "        data: Optional[bytes] = None,",
            "        headers: Optional[Headers] = None,",
            "    ) -> IResponse:",
            "        \"\"\"",
            "        Args:",
            "            method: HTTP method to use.",
            "            uri: URI to query.",
            "            data: Data to send in the request body, if applicable.",
            "            headers: Request headers.",
            "",
            "        Returns:",
            "            Response object, once the headers have been read.",
            "",
            "        Raises:",
            "            RequestTimedOutError if the request times out before the headers are read",
            "",
            "        \"\"\"",
            "        outgoing_requests_counter.labels(method).inc()",
            "",
            "        # log request but strip `access_token` (AS requests for example include this)",
            "        logger.debug(\"Sending request %s %s\", method, redact_uri(uri))",
            "",
            "        with start_active_span(",
            "            \"outgoing-client-request\",",
            "            tags={",
            "                tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,",
            "                tags.HTTP_METHOD: method,",
            "                tags.HTTP_URL: uri,",
            "            },",
            "            finish_on_close=True,",
            "        ):",
            "            try:",
            "                body_producer = None",
            "                if data is not None:",
            "                    body_producer = QuieterFileBodyProducer(",
            "                        BytesIO(data), cooperator=self._cooperator,",
            "                    )",
            "",
            "                request_deferred = treq.request(",
            "                    method,",
            "                    uri,",
            "                    agent=self.agent,",
            "                    data=body_producer,",
            "                    headers=headers,",
            "                    **self._extra_treq_args,",
            "                )  # type: defer.Deferred",
            "",
            "                # we use our own timeout mechanism rather than treq's as a workaround",
            "                # for https://twistedmatrix.com/trac/ticket/9534.",
            "                request_deferred = timeout_deferred(",
            "                    request_deferred, 60, self.hs.get_reactor(),",
            "                )",
            "",
            "                # turn timeouts into RequestTimedOutErrors",
            "                request_deferred.addErrback(_timeout_to_request_timed_out_error)",
            "",
            "                response = await make_deferred_yieldable(request_deferred)",
            "",
            "                incoming_responses_counter.labels(method, response.code).inc()",
            "                logger.info(",
            "                    \"Received response to %s %s: %s\",",
            "                    method,",
            "                    redact_uri(uri),",
            "                    response.code,",
            "                )",
            "                return response",
            "            except Exception as e:",
            "                incoming_responses_counter.labels(method, \"ERR\").inc()",
            "                logger.info(",
            "                    \"Error sending request to  %s %s: %s %s\",",
            "                    method,",
            "                    redact_uri(uri),",
            "                    type(e).__name__,",
            "                    e.args[0],",
            "                )",
            "                set_tag(tags.ERROR, True)",
            "                set_tag(\"error_reason\", e.args[0])",
            "                raise",
            "",
            "    async def post_urlencoded_get_json(",
            "        self,",
            "        uri: str,",
            "        args: Optional[Mapping[str, Union[str, List[str]]]] = None,",
            "        headers: Optional[RawHeaders] = None,",
            "    ) -> Any:",
            "        \"\"\"",
            "        Args:",
            "            uri: uri to query",
            "            args: parameters to be url-encoded in the body",
            "            headers: a map from header name to a list of values for that header",
            "",
            "        Returns:",
            "            parsed json",
            "",
            "        Raises:",
            "            RequestTimedOutError: if there is a timeout before the response headers",
            "               are received. Note there is currently no timeout on reading the response",
            "               body.",
            "",
            "            HttpResponseException: On a non-2xx HTTP response.",
            "",
            "            ValueError: if the response was not JSON",
            "        \"\"\"",
            "",
            "        # TODO: Do we ever want to log message contents?",
            "        logger.debug(\"post_urlencoded_get_json args: %s\", args)",
            "",
            "        query_bytes = encode_query_args(args)",
            "",
            "        actual_headers = {",
            "            b\"Content-Type\": [b\"application/x-www-form-urlencoded\"],",
            "            b\"User-Agent\": [self.user_agent],",
            "            b\"Accept\": [b\"application/json\"],",
            "        }",
            "        if headers:",
            "            actual_headers.update(headers)  # type: ignore",
            "",
            "        response = await self.request(",
            "            \"POST\", uri, headers=Headers(actual_headers), data=query_bytes",
            "        )",
            "",
            "        body = await make_deferred_yieldable(readBody(response))",
            "",
            "        if 200 <= response.code < 300:",
            "            return json_decoder.decode(body.decode(\"utf-8\"))",
            "        else:",
            "            raise HttpResponseException(",
            "                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body",
            "            )",
            "",
            "    async def post_json_get_json(",
            "        self, uri: str, post_json: Any, headers: Optional[RawHeaders] = None",
            "    ) -> Any:",
            "        \"\"\"",
            "",
            "        Args:",
            "            uri: URI to query.",
            "            post_json: request body, to be encoded as json",
            "            headers: a map from header name to a list of values for that header",
            "",
            "        Returns:",
            "            parsed json",
            "",
            "        Raises:",
            "            RequestTimedOutError: if there is a timeout before the response headers",
            "               are received. Note there is currently no timeout on reading the response",
            "               body.",
            "",
            "            HttpResponseException: On a non-2xx HTTP response.",
            "",
            "            ValueError: if the response was not JSON",
            "        \"\"\"",
            "        json_str = encode_canonical_json(post_json)",
            "",
            "        logger.debug(\"HTTP POST %s -> %s\", json_str, uri)",
            "",
            "        actual_headers = {",
            "            b\"Content-Type\": [b\"application/json\"],",
            "            b\"User-Agent\": [self.user_agent],",
            "            b\"Accept\": [b\"application/json\"],",
            "        }",
            "        if headers:",
            "            actual_headers.update(headers)  # type: ignore",
            "",
            "        response = await self.request(",
            "            \"POST\", uri, headers=Headers(actual_headers), data=json_str",
            "        )",
            "",
            "        body = await make_deferred_yieldable(readBody(response))",
            "",
            "        if 200 <= response.code < 300:",
            "            return json_decoder.decode(body.decode(\"utf-8\"))",
            "        else:",
            "            raise HttpResponseException(",
            "                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body",
            "            )",
            "",
            "    async def get_json(",
            "        self,",
            "        uri: str,",
            "        args: Optional[QueryParams] = None,",
            "        headers: Optional[RawHeaders] = None,",
            "    ) -> Any:",
            "        \"\"\"Gets some json from the given URI.",
            "",
            "        Args:",
            "            uri: The URI to request, not including query parameters",
            "            args: A dictionary used to create query string",
            "            headers: a map from header name to a list of values for that header",
            "        Returns:",
            "            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.",
            "        Raises:",
            "            RequestTimedOutError: if there is a timeout before the response headers",
            "               are received. Note there is currently no timeout on reading the response",
            "               body.",
            "",
            "            HttpResponseException On a non-2xx HTTP response.",
            "",
            "            ValueError: if the response was not JSON",
            "        \"\"\"",
            "        actual_headers = {b\"Accept\": [b\"application/json\"]}",
            "        if headers:",
            "            actual_headers.update(headers)  # type: ignore",
            "",
            "        body = await self.get_raw(uri, args, headers=headers)",
            "        return json_decoder.decode(body.decode(\"utf-8\"))",
            "",
            "    async def put_json(",
            "        self,",
            "        uri: str,",
            "        json_body: Any,",
            "        args: Optional[QueryParams] = None,",
            "        headers: RawHeaders = None,",
            "    ) -> Any:",
            "        \"\"\"Puts some json to the given URI.",
            "",
            "        Args:",
            "            uri: The URI to request, not including query parameters",
            "            json_body: The JSON to put in the HTTP body,",
            "            args: A dictionary used to create query strings",
            "            headers: a map from header name to a list of values for that header",
            "        Returns:",
            "            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.",
            "        Raises:",
            "             RequestTimedOutError: if there is a timeout before the response headers",
            "               are received. Note there is currently no timeout on reading the response",
            "               body.",
            "",
            "            HttpResponseException On a non-2xx HTTP response.",
            "",
            "            ValueError: if the response was not JSON",
            "        \"\"\"",
            "        if args:",
            "            query_str = urllib.parse.urlencode(args, True)",
            "            uri = \"%s?%s\" % (uri, query_str)",
            "",
            "        json_str = encode_canonical_json(json_body)",
            "",
            "        actual_headers = {",
            "            b\"Content-Type\": [b\"application/json\"],",
            "            b\"User-Agent\": [self.user_agent],",
            "            b\"Accept\": [b\"application/json\"],",
            "        }",
            "        if headers:",
            "            actual_headers.update(headers)  # type: ignore",
            "",
            "        response = await self.request(",
            "            \"PUT\", uri, headers=Headers(actual_headers), data=json_str",
            "        )",
            "",
            "        body = await make_deferred_yieldable(readBody(response))",
            "",
            "        if 200 <= response.code < 300:",
            "            return json_decoder.decode(body.decode(\"utf-8\"))",
            "        else:",
            "            raise HttpResponseException(",
            "                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body",
            "            )",
            "",
            "    async def get_raw(",
            "        self,",
            "        uri: str,",
            "        args: Optional[QueryParams] = None,",
            "        headers: Optional[RawHeaders] = None,",
            "    ) -> bytes:",
            "        \"\"\"Gets raw text from the given URI.",
            "",
            "        Args:",
            "            uri: The URI to request, not including query parameters",
            "            args: A dictionary used to create query strings",
            "            headers: a map from header name to a list of values for that header",
            "        Returns:",
            "            Succeeds when we get a 2xx HTTP response, with the",
            "            HTTP body as bytes.",
            "        Raises:",
            "            RequestTimedOutError: if there is a timeout before the response headers",
            "               are received. Note there is currently no timeout on reading the response",
            "               body.",
            "",
            "            HttpResponseException on a non-2xx HTTP response.",
            "        \"\"\"",
            "        if args:",
            "            query_str = urllib.parse.urlencode(args, True)",
            "            uri = \"%s?%s\" % (uri, query_str)",
            "",
            "        actual_headers = {b\"User-Agent\": [self.user_agent]}",
            "        if headers:",
            "            actual_headers.update(headers)  # type: ignore",
            "",
            "        response = await self.request(\"GET\", uri, headers=Headers(actual_headers))",
            "",
            "        body = await make_deferred_yieldable(readBody(response))",
            "",
            "        if 200 <= response.code < 300:",
            "            return body",
            "        else:",
            "            raise HttpResponseException(",
            "                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body",
            "            )",
            "",
            "    # XXX: FIXME: This is horribly copy-pasted from matrixfederationclient.",
            "    # The two should be factored out.",
            "",
            "    async def get_file(",
            "        self,",
            "        url: str,",
            "        output_stream: BinaryIO,",
            "        max_size: Optional[int] = None,",
            "        headers: Optional[RawHeaders] = None,",
            "    ) -> Tuple[int, Dict[bytes, List[bytes]], str, int]:",
            "        \"\"\"GETs a file from a given URL",
            "        Args:",
            "            url: The URL to GET",
            "            output_stream: File to write the response body to.",
            "            headers: A map from header name to a list of values for that header",
            "        Returns:",
            "            A tuple of the file length, dict of the response",
            "            headers, absolute URI of the response and HTTP response code.",
            "",
            "        Raises:",
            "            RequestTimedOutError: if there is a timeout before the response headers",
            "               are received. Note there is currently no timeout on reading the response",
            "               body.",
            "",
            "            SynapseError: if the response is not a 2xx, the remote file is too large, or",
            "               another exception happens during the download.",
            "        \"\"\"",
            "",
            "        actual_headers = {b\"User-Agent\": [self.user_agent]}",
            "        if headers:",
            "            actual_headers.update(headers)  # type: ignore",
            "",
            "        response = await self.request(\"GET\", url, headers=Headers(actual_headers))",
            "",
            "        resp_headers = dict(response.headers.getAllRawHeaders())",
            "",
            "        if (",
            "            b\"Content-Length\" in resp_headers",
            "            and max_size",
            "            and int(resp_headers[b\"Content-Length\"][0]) > max_size",
            "        ):",
            "            logger.warning(\"Requested URL is too large > %r bytes\" % (max_size,))",
            "            raise SynapseError(",
            "                502,",
            "                \"Requested file is too large > %r bytes\" % (max_size,),",
            "                Codes.TOO_LARGE,",
            "            )",
            "",
            "        if response.code > 299:",
            "            logger.warning(\"Got %d when downloading %s\" % (response.code, url))",
            "            raise SynapseError(502, \"Got error %d\" % (response.code,), Codes.UNKNOWN)",
            "",
            "        # TODO: if our Content-Type is HTML or something, just read the first",
            "        # N bytes into RAM rather than saving it all to disk only to read it",
            "        # straight back in again",
            "",
            "        try:",
            "            length = await make_deferred_yieldable(",
            "                readBodyToFile(response, output_stream, max_size)",
            "            )",
            "        except SynapseError:",
            "            # This can happen e.g. because the body is too large.",
            "            raise",
            "        except Exception as e:",
            "            raise SynapseError(502, (\"Failed to download remote body: %s\" % e)) from e",
            "",
            "        return (",
            "            length,",
            "            resp_headers,",
            "            response.request.absoluteURI.decode(\"ascii\"),",
            "            response.code,",
            "        )",
            "",
            "",
            "def _timeout_to_request_timed_out_error(f: Failure):",
            "    if f.check(twisted_error.TimeoutError, twisted_error.ConnectingCancelledError):",
            "        # The TCP connection has its own timeout (set by the 'connectTimeout' param",
            "        # on the Agent), which raises twisted_error.TimeoutError exception.",
            "        raise RequestTimedOutError(\"Timeout connecting to remote server\")",
            "    elif f.check(defer.TimeoutError, ResponseNeverReceived):",
            "        # this one means that we hit our overall timeout on the request",
            "        raise RequestTimedOutError(\"Timeout waiting for response from remote server\")",
            "",
            "    return f",
            "",
            "",
            "class _ReadBodyToFileProtocol(protocol.Protocol):",
            "    def __init__(",
            "        self, stream: BinaryIO, deferred: defer.Deferred, max_size: Optional[int]",
            "    ):",
            "        self.stream = stream",
            "        self.deferred = deferred",
            "        self.length = 0",
            "        self.max_size = max_size",
            "",
            "    def dataReceived(self, data: bytes) -> None:",
            "        self.stream.write(data)",
            "        self.length += len(data)",
            "        if self.max_size is not None and self.length >= self.max_size:",
            "            self.deferred.errback(",
            "                SynapseError(",
            "                    502,",
            "                    \"Requested file is too large > %r bytes\" % (self.max_size,),",
            "                    Codes.TOO_LARGE,",
            "                )",
            "            )",
            "            self.deferred = defer.Deferred()",
            "            self.transport.loseConnection()",
            "",
            "    def connectionLost(self, reason: Failure) -> None:",
            "        if reason.check(ResponseDone):",
            "            self.deferred.callback(self.length)",
            "        elif reason.check(PotentialDataLoss):",
            "            # stolen from https://github.com/twisted/treq/pull/49/files",
            "            # http://twistedmatrix.com/trac/ticket/4840",
            "            self.deferred.callback(self.length)",
            "        else:",
            "            self.deferred.errback(reason)",
            "",
            "",
            "def readBodyToFile(",
            "    response: IResponse, stream: BinaryIO, max_size: Optional[int]",
            ") -> defer.Deferred:",
            "    \"\"\"",
            "    Read a HTTP response body to a file-object. Optionally enforcing a maximum file size.",
            "",
            "    Args:",
            "        response: The HTTP response to read from.",
            "        stream: The file-object to write to.",
            "        max_size: The maximum file size to allow.",
            "",
            "    Returns:",
            "        A Deferred which resolves to the length of the read body.",
            "    \"\"\"",
            "",
            "    d = defer.Deferred()",
            "    response.deliverBody(_ReadBodyToFileProtocol(stream, d, max_size))",
            "    return d",
            "",
            "",
            "def encode_query_args(args: Optional[Mapping[str, Union[str, List[str]]]]) -> bytes:",
            "    \"\"\"",
            "    Encodes a map of query arguments to bytes which can be appended to a URL.",
            "",
            "    Args:",
            "        args: The query arguments, a mapping of string to string or list of strings.",
            "",
            "    Returns:",
            "        The query arguments encoded as bytes.",
            "    \"\"\"",
            "    if args is None:",
            "        return b\"\"",
            "",
            "    encoded_args = {}",
            "    for k, vs in args.items():",
            "        if isinstance(vs, str):",
            "            vs = [vs]",
            "        encoded_args[k] = [v.encode(\"utf8\") for v in vs]",
            "",
            "    query_str = urllib.parse.urlencode(encoded_args, True)",
            "",
            "    return query_str.encode(\"utf8\")",
            "",
            "",
            "class InsecureInterceptableContextFactory(ssl.ContextFactory):",
            "    \"\"\"",
            "    Factory for PyOpenSSL SSL contexts which accepts any certificate for any domain.",
            "",
            "    Do not use this since it allows an attacker to intercept your communications.",
            "    \"\"\"",
            "",
            "    def __init__(self):",
            "        self._context = SSL.Context(SSL.SSLv23_METHOD)",
            "        self._context.set_verify(VERIFY_NONE, lambda *_: None)",
            "",
            "    def getContext(self, hostname=None, port=None):",
            "        return self._context",
            "",
            "    def creatorForNetloc(self, hostname, port):",
            "        return self"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "128": [
                "IPBlacklistingResolver"
            ],
            "295": [
                "SimpleHttpClient",
                "__init__"
            ],
            "298": [
                "SimpleHttpClient",
                "__init__"
            ],
            "299": [
                "SimpleHttpClient",
                "__init__"
            ],
            "301": [
                "SimpleHttpClient",
                "__init__"
            ],
            "302": [
                "SimpleHttpClient",
                "__init__"
            ],
            "303": [
                "SimpleHttpClient",
                "__init__",
                "Reactor"
            ],
            "304": [
                "SimpleHttpClient",
                "__init__",
                "Reactor",
                "__getattr__"
            ],
            "305": [
                "SimpleHttpClient",
                "__init__",
                "Reactor",
                "__getattr__"
            ],
            "306": [
                "SimpleHttpClient",
                "__init__",
                "Reactor",
                "__getattr__"
            ],
            "307": [
                "SimpleHttpClient",
                "__init__",
                "Reactor",
                "__getattr__"
            ],
            "308": [
                "SimpleHttpClient",
                "__init__",
                "Reactor",
                "__getattr__"
            ],
            "309": [
                "SimpleHttpClient",
                "__init__"
            ],
            "310": [
                "SimpleHttpClient",
                "__init__"
            ]
        },
        "addLocation": []
    },
    "synapse/http/federation/matrix_federation_agent.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " import urllib.parse"
            },
            "1": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " from typing import List, Optional"
            },
            "2": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from netaddr import AddrFormatError, IPAddress"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 19,
                "PatchRowcode": "+from netaddr import AddrFormatError, IPAddress, IPSet"
            },
            "5": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " from zope.interface import implementer"
            },
            "6": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " from twisted.internet import defer"
            },
            "8": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " from twisted.web.iweb import IAgent, IAgentEndpointFactory, IBodyProducer"
            },
            "9": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " from synapse.crypto.context_factory import FederationPolicyForHTTPS"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+from synapse.http.client import BlacklistingAgentWrapper"
            },
            "12": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " from synapse.http.federation.srv_resolver import Server, SrvResolver"
            },
            "13": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " from synapse.http.federation.well_known_resolver import WellKnownResolver"
            },
            "14": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 37,
                "PatchRowcode": " from synapse.logging.context import make_deferred_yieldable, run_in_background"
            },
            "15": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 71,
                "PatchRowcode": "         reactor: IReactorCore,"
            },
            "16": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "         tls_client_options_factory: Optional[FederationPolicyForHTTPS],"
            },
            "17": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "         user_agent: bytes,"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+        ip_blacklist: IPSet,"
            },
            "19": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 75,
                "PatchRowcode": "         _srv_resolver: Optional[SrvResolver] = None,"
            },
            "20": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 76,
                "PatchRowcode": "         _well_known_resolver: Optional[WellKnownResolver] = None,"
            },
            "21": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 77,
                "PatchRowcode": "     ):"
            },
            "22": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 92,
                "PatchRowcode": "         self.user_agent = user_agent"
            },
            "23": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 93,
                "PatchRowcode": " "
            },
            "24": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 94,
                "PatchRowcode": "         if _well_known_resolver is None:"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+            # Note that the name resolver has already been wrapped in a"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+            # IPBlacklistingResolver by MatrixFederationHttpClient."
            },
            "27": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 97,
                "PatchRowcode": "             _well_known_resolver = WellKnownResolver("
            },
            "28": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": 98,
                "PatchRowcode": "                 self._reactor,"
            },
            "29": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                agent=Agent("
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+                agent=BlacklistingAgentWrapper("
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+                    Agent("
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+                        self._reactor,"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+                        pool=self._pool,"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+                        contextFactory=tls_client_options_factory,"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 104,
                "PatchRowcode": "+                    ),"
            },
            "36": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 105,
                "PatchRowcode": "                     self._reactor,"
            },
            "37": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    pool=self._pool,"
            },
            "38": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    contextFactory=tls_client_options_factory,"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+                    ip_blacklist=ip_blacklist,"
            },
            "40": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 107,
                "PatchRowcode": "                 ),"
            },
            "41": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": 108,
                "PatchRowcode": "                 user_agent=self.user_agent,"
            },
            "42": {
                "beforePatchRowNumber": 101,
                "afterPatchRowNumber": 109,
                "PatchRowcode": "             )"
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2019 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "import urllib.parse",
            "from typing import List, Optional",
            "",
            "from netaddr import AddrFormatError, IPAddress",
            "from zope.interface import implementer",
            "",
            "from twisted.internet import defer",
            "from twisted.internet.endpoints import HostnameEndpoint, wrapClientTLS",
            "from twisted.internet.interfaces import (",
            "    IProtocolFactory,",
            "    IReactorCore,",
            "    IStreamClientEndpoint,",
            ")",
            "from twisted.web.client import URI, Agent, HTTPConnectionPool",
            "from twisted.web.http_headers import Headers",
            "from twisted.web.iweb import IAgent, IAgentEndpointFactory, IBodyProducer",
            "",
            "from synapse.crypto.context_factory import FederationPolicyForHTTPS",
            "from synapse.http.federation.srv_resolver import Server, SrvResolver",
            "from synapse.http.federation.well_known_resolver import WellKnownResolver",
            "from synapse.logging.context import make_deferred_yieldable, run_in_background",
            "from synapse.util import Clock",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "@implementer(IAgent)",
            "class MatrixFederationAgent:",
            "    \"\"\"An Agent-like thing which provides a `request` method which correctly",
            "    handles resolving matrix server names when using matrix://. Handles standard",
            "    https URIs as normal.",
            "",
            "    Doesn't implement any retries. (Those are done in MatrixFederationHttpClient.)",
            "",
            "    Args:",
            "        reactor: twisted reactor to use for underlying requests",
            "",
            "        tls_client_options_factory:",
            "            factory to use for fetching client tls options, or none to disable TLS.",
            "",
            "        user_agent:",
            "            The user agent header to use for federation requests.",
            "",
            "        _srv_resolver:",
            "            SrvResolver implementation to use for looking up SRV records. None",
            "            to use a default implementation.",
            "",
            "        _well_known_resolver:",
            "            WellKnownResolver to use to perform well-known lookups. None to use a",
            "            default implementation.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        reactor: IReactorCore,",
            "        tls_client_options_factory: Optional[FederationPolicyForHTTPS],",
            "        user_agent: bytes,",
            "        _srv_resolver: Optional[SrvResolver] = None,",
            "        _well_known_resolver: Optional[WellKnownResolver] = None,",
            "    ):",
            "        self._reactor = reactor",
            "        self._clock = Clock(reactor)",
            "        self._pool = HTTPConnectionPool(reactor)",
            "        self._pool.retryAutomatically = False",
            "        self._pool.maxPersistentPerHost = 5",
            "        self._pool.cachedConnectionTimeout = 2 * 60",
            "",
            "        self._agent = Agent.usingEndpointFactory(",
            "            self._reactor,",
            "            MatrixHostnameEndpointFactory(",
            "                reactor, tls_client_options_factory, _srv_resolver",
            "            ),",
            "            pool=self._pool,",
            "        )",
            "        self.user_agent = user_agent",
            "",
            "        if _well_known_resolver is None:",
            "            _well_known_resolver = WellKnownResolver(",
            "                self._reactor,",
            "                agent=Agent(",
            "                    self._reactor,",
            "                    pool=self._pool,",
            "                    contextFactory=tls_client_options_factory,",
            "                ),",
            "                user_agent=self.user_agent,",
            "            )",
            "",
            "        self._well_known_resolver = _well_known_resolver",
            "",
            "    @defer.inlineCallbacks",
            "    def request(",
            "        self,",
            "        method: bytes,",
            "        uri: bytes,",
            "        headers: Optional[Headers] = None,",
            "        bodyProducer: Optional[IBodyProducer] = None,",
            "    ) -> defer.Deferred:",
            "        \"\"\"",
            "        Args:",
            "            method: HTTP method: GET/POST/etc",
            "            uri: Absolute URI to be retrieved",
            "            headers:",
            "                HTTP headers to send with the request, or None to send no extra headers.",
            "            bodyProducer:",
            "                An object which can generate bytes to make up the",
            "                body of this request (for example, the properly encoded contents of",
            "                a file for a file upload).  Or None if the request is to have",
            "                no body.",
            "        Returns:",
            "            Deferred[twisted.web.iweb.IResponse]:",
            "                fires when the header of the response has been received (regardless of the",
            "                response status code). Fails if there is any problem which prevents that",
            "                response from being received (including problems that prevent the request",
            "                from being sent).",
            "        \"\"\"",
            "        # We use urlparse as that will set `port` to None if there is no",
            "        # explicit port.",
            "        parsed_uri = urllib.parse.urlparse(uri)",
            "",
            "        # There must be a valid hostname.",
            "        assert parsed_uri.hostname",
            "",
            "        # If this is a matrix:// URI check if the server has delegated matrix",
            "        # traffic using well-known delegation.",
            "        #",
            "        # We have to do this here and not in the endpoint as we need to rewrite",
            "        # the host header with the delegated server name.",
            "        delegated_server = None",
            "        if (",
            "            parsed_uri.scheme == b\"matrix\"",
            "            and not _is_ip_literal(parsed_uri.hostname)",
            "            and not parsed_uri.port",
            "        ):",
            "            well_known_result = yield defer.ensureDeferred(",
            "                self._well_known_resolver.get_well_known(parsed_uri.hostname)",
            "            )",
            "            delegated_server = well_known_result.delegated_server",
            "",
            "        if delegated_server:",
            "            # Ok, the server has delegated matrix traffic to somewhere else, so",
            "            # lets rewrite the URL to replace the server with the delegated",
            "            # server name.",
            "            uri = urllib.parse.urlunparse(",
            "                (",
            "                    parsed_uri.scheme,",
            "                    delegated_server,",
            "                    parsed_uri.path,",
            "                    parsed_uri.params,",
            "                    parsed_uri.query,",
            "                    parsed_uri.fragment,",
            "                )",
            "            )",
            "            parsed_uri = urllib.parse.urlparse(uri)",
            "",
            "        # We need to make sure the host header is set to the netloc of the",
            "        # server and that a user-agent is provided.",
            "        if headers is None:",
            "            headers = Headers()",
            "        else:",
            "            headers = headers.copy()",
            "",
            "        if not headers.hasHeader(b\"host\"):",
            "            headers.addRawHeader(b\"host\", parsed_uri.netloc)",
            "        if not headers.hasHeader(b\"user-agent\"):",
            "            headers.addRawHeader(b\"user-agent\", self.user_agent)",
            "",
            "        res = yield make_deferred_yieldable(",
            "            self._agent.request(method, uri, headers, bodyProducer)",
            "        )",
            "",
            "        return res",
            "",
            "",
            "@implementer(IAgentEndpointFactory)",
            "class MatrixHostnameEndpointFactory:",
            "    \"\"\"Factory for MatrixHostnameEndpoint for parsing to an Agent.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        reactor: IReactorCore,",
            "        tls_client_options_factory: Optional[FederationPolicyForHTTPS],",
            "        srv_resolver: Optional[SrvResolver],",
            "    ):",
            "        self._reactor = reactor",
            "        self._tls_client_options_factory = tls_client_options_factory",
            "",
            "        if srv_resolver is None:",
            "            srv_resolver = SrvResolver()",
            "",
            "        self._srv_resolver = srv_resolver",
            "",
            "    def endpointForURI(self, parsed_uri):",
            "        return MatrixHostnameEndpoint(",
            "            self._reactor,",
            "            self._tls_client_options_factory,",
            "            self._srv_resolver,",
            "            parsed_uri,",
            "        )",
            "",
            "",
            "@implementer(IStreamClientEndpoint)",
            "class MatrixHostnameEndpoint:",
            "    \"\"\"An endpoint that resolves matrix:// URLs using Matrix server name",
            "    resolution (i.e. via SRV). Does not check for well-known delegation.",
            "",
            "    Args:",
            "        reactor: twisted reactor to use for underlying requests",
            "        tls_client_options_factory:",
            "            factory to use for fetching client tls options, or none to disable TLS.",
            "        srv_resolver: The SRV resolver to use",
            "        parsed_uri: The parsed URI that we're wanting to connect to.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        reactor: IReactorCore,",
            "        tls_client_options_factory: Optional[FederationPolicyForHTTPS],",
            "        srv_resolver: SrvResolver,",
            "        parsed_uri: URI,",
            "    ):",
            "        self._reactor = reactor",
            "",
            "        self._parsed_uri = parsed_uri",
            "",
            "        # set up the TLS connection params",
            "        #",
            "        # XXX disabling TLS is really only supported here for the benefit of the",
            "        # unit tests. We should make the UTs cope with TLS rather than having to make",
            "        # the code support the unit tests.",
            "",
            "        if tls_client_options_factory is None:",
            "            self._tls_options = None",
            "        else:",
            "            self._tls_options = tls_client_options_factory.get_options(",
            "                self._parsed_uri.host",
            "            )",
            "",
            "        self._srv_resolver = srv_resolver",
            "",
            "    def connect(self, protocol_factory: IProtocolFactory) -> defer.Deferred:",
            "        \"\"\"Implements IStreamClientEndpoint interface",
            "        \"\"\"",
            "",
            "        return run_in_background(self._do_connect, protocol_factory)",
            "",
            "    async def _do_connect(self, protocol_factory: IProtocolFactory) -> None:",
            "        first_exception = None",
            "",
            "        server_list = await self._resolve_server()",
            "",
            "        for server in server_list:",
            "            host = server.host",
            "            port = server.port",
            "",
            "            try:",
            "                logger.debug(\"Connecting to %s:%i\", host.decode(\"ascii\"), port)",
            "                endpoint = HostnameEndpoint(self._reactor, host, port)",
            "                if self._tls_options:",
            "                    endpoint = wrapClientTLS(self._tls_options, endpoint)",
            "                result = await make_deferred_yieldable(",
            "                    endpoint.connect(protocol_factory)",
            "                )",
            "",
            "                return result",
            "            except Exception as e:",
            "                logger.info(",
            "                    \"Failed to connect to %s:%i: %s\", host.decode(\"ascii\"), port, e",
            "                )",
            "                if not first_exception:",
            "                    first_exception = e",
            "",
            "        # We return the first failure because that's probably the most interesting.",
            "        if first_exception:",
            "            raise first_exception",
            "",
            "        # This shouldn't happen as we should always have at least one host/port",
            "        # to try and if that doesn't work then we'll have an exception.",
            "        raise Exception(\"Failed to resolve server %r\" % (self._parsed_uri.netloc,))",
            "",
            "    async def _resolve_server(self) -> List[Server]:",
            "        \"\"\"Resolves the server name to a list of hosts and ports to attempt to",
            "        connect to.",
            "        \"\"\"",
            "",
            "        if self._parsed_uri.scheme != b\"matrix\":",
            "            return [Server(host=self._parsed_uri.host, port=self._parsed_uri.port)]",
            "",
            "        # Note: We don't do well-known lookup as that needs to have happened",
            "        # before now, due to needing to rewrite the Host header of the HTTP",
            "        # request.",
            "",
            "        # We reparse the URI so that defaultPort is -1 rather than 80",
            "        parsed_uri = urllib.parse.urlparse(self._parsed_uri.toBytes())",
            "",
            "        host = parsed_uri.hostname",
            "        port = parsed_uri.port",
            "",
            "        # If there is an explicit port or the host is an IP address we bypass",
            "        # SRV lookups and just use the given host/port.",
            "        if port or _is_ip_literal(host):",
            "            return [Server(host, port or 8448)]",
            "",
            "        server_list = await self._srv_resolver.resolve_service(b\"_matrix._tcp.\" + host)",
            "",
            "        if server_list:",
            "            return server_list",
            "",
            "        # No SRV records, so we fallback to host and 8448",
            "        return [Server(host, 8448)]",
            "",
            "",
            "def _is_ip_literal(host: bytes) -> bool:",
            "    \"\"\"Test if the given host name is either an IPv4 or IPv6 literal.",
            "",
            "    Args:",
            "        host: The host name to check",
            "",
            "    Returns:",
            "        True if the hostname is an IP address literal.",
            "    \"\"\"",
            "",
            "    host_str = host.decode(\"ascii\")",
            "",
            "    try:",
            "        IPAddress(host_str)",
            "        return True",
            "    except AddrFormatError:",
            "        return False"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2019 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "import urllib.parse",
            "from typing import List, Optional",
            "",
            "from netaddr import AddrFormatError, IPAddress, IPSet",
            "from zope.interface import implementer",
            "",
            "from twisted.internet import defer",
            "from twisted.internet.endpoints import HostnameEndpoint, wrapClientTLS",
            "from twisted.internet.interfaces import (",
            "    IProtocolFactory,",
            "    IReactorCore,",
            "    IStreamClientEndpoint,",
            ")",
            "from twisted.web.client import URI, Agent, HTTPConnectionPool",
            "from twisted.web.http_headers import Headers",
            "from twisted.web.iweb import IAgent, IAgentEndpointFactory, IBodyProducer",
            "",
            "from synapse.crypto.context_factory import FederationPolicyForHTTPS",
            "from synapse.http.client import BlacklistingAgentWrapper",
            "from synapse.http.federation.srv_resolver import Server, SrvResolver",
            "from synapse.http.federation.well_known_resolver import WellKnownResolver",
            "from synapse.logging.context import make_deferred_yieldable, run_in_background",
            "from synapse.util import Clock",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "@implementer(IAgent)",
            "class MatrixFederationAgent:",
            "    \"\"\"An Agent-like thing which provides a `request` method which correctly",
            "    handles resolving matrix server names when using matrix://. Handles standard",
            "    https URIs as normal.",
            "",
            "    Doesn't implement any retries. (Those are done in MatrixFederationHttpClient.)",
            "",
            "    Args:",
            "        reactor: twisted reactor to use for underlying requests",
            "",
            "        tls_client_options_factory:",
            "            factory to use for fetching client tls options, or none to disable TLS.",
            "",
            "        user_agent:",
            "            The user agent header to use for federation requests.",
            "",
            "        _srv_resolver:",
            "            SrvResolver implementation to use for looking up SRV records. None",
            "            to use a default implementation.",
            "",
            "        _well_known_resolver:",
            "            WellKnownResolver to use to perform well-known lookups. None to use a",
            "            default implementation.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        reactor: IReactorCore,",
            "        tls_client_options_factory: Optional[FederationPolicyForHTTPS],",
            "        user_agent: bytes,",
            "        ip_blacklist: IPSet,",
            "        _srv_resolver: Optional[SrvResolver] = None,",
            "        _well_known_resolver: Optional[WellKnownResolver] = None,",
            "    ):",
            "        self._reactor = reactor",
            "        self._clock = Clock(reactor)",
            "        self._pool = HTTPConnectionPool(reactor)",
            "        self._pool.retryAutomatically = False",
            "        self._pool.maxPersistentPerHost = 5",
            "        self._pool.cachedConnectionTimeout = 2 * 60",
            "",
            "        self._agent = Agent.usingEndpointFactory(",
            "            self._reactor,",
            "            MatrixHostnameEndpointFactory(",
            "                reactor, tls_client_options_factory, _srv_resolver",
            "            ),",
            "            pool=self._pool,",
            "        )",
            "        self.user_agent = user_agent",
            "",
            "        if _well_known_resolver is None:",
            "            # Note that the name resolver has already been wrapped in a",
            "            # IPBlacklistingResolver by MatrixFederationHttpClient.",
            "            _well_known_resolver = WellKnownResolver(",
            "                self._reactor,",
            "                agent=BlacklistingAgentWrapper(",
            "                    Agent(",
            "                        self._reactor,",
            "                        pool=self._pool,",
            "                        contextFactory=tls_client_options_factory,",
            "                    ),",
            "                    self._reactor,",
            "                    ip_blacklist=ip_blacklist,",
            "                ),",
            "                user_agent=self.user_agent,",
            "            )",
            "",
            "        self._well_known_resolver = _well_known_resolver",
            "",
            "    @defer.inlineCallbacks",
            "    def request(",
            "        self,",
            "        method: bytes,",
            "        uri: bytes,",
            "        headers: Optional[Headers] = None,",
            "        bodyProducer: Optional[IBodyProducer] = None,",
            "    ) -> defer.Deferred:",
            "        \"\"\"",
            "        Args:",
            "            method: HTTP method: GET/POST/etc",
            "            uri: Absolute URI to be retrieved",
            "            headers:",
            "                HTTP headers to send with the request, or None to send no extra headers.",
            "            bodyProducer:",
            "                An object which can generate bytes to make up the",
            "                body of this request (for example, the properly encoded contents of",
            "                a file for a file upload).  Or None if the request is to have",
            "                no body.",
            "        Returns:",
            "            Deferred[twisted.web.iweb.IResponse]:",
            "                fires when the header of the response has been received (regardless of the",
            "                response status code). Fails if there is any problem which prevents that",
            "                response from being received (including problems that prevent the request",
            "                from being sent).",
            "        \"\"\"",
            "        # We use urlparse as that will set `port` to None if there is no",
            "        # explicit port.",
            "        parsed_uri = urllib.parse.urlparse(uri)",
            "",
            "        # There must be a valid hostname.",
            "        assert parsed_uri.hostname",
            "",
            "        # If this is a matrix:// URI check if the server has delegated matrix",
            "        # traffic using well-known delegation.",
            "        #",
            "        # We have to do this here and not in the endpoint as we need to rewrite",
            "        # the host header with the delegated server name.",
            "        delegated_server = None",
            "        if (",
            "            parsed_uri.scheme == b\"matrix\"",
            "            and not _is_ip_literal(parsed_uri.hostname)",
            "            and not parsed_uri.port",
            "        ):",
            "            well_known_result = yield defer.ensureDeferred(",
            "                self._well_known_resolver.get_well_known(parsed_uri.hostname)",
            "            )",
            "            delegated_server = well_known_result.delegated_server",
            "",
            "        if delegated_server:",
            "            # Ok, the server has delegated matrix traffic to somewhere else, so",
            "            # lets rewrite the URL to replace the server with the delegated",
            "            # server name.",
            "            uri = urllib.parse.urlunparse(",
            "                (",
            "                    parsed_uri.scheme,",
            "                    delegated_server,",
            "                    parsed_uri.path,",
            "                    parsed_uri.params,",
            "                    parsed_uri.query,",
            "                    parsed_uri.fragment,",
            "                )",
            "            )",
            "            parsed_uri = urllib.parse.urlparse(uri)",
            "",
            "        # We need to make sure the host header is set to the netloc of the",
            "        # server and that a user-agent is provided.",
            "        if headers is None:",
            "            headers = Headers()",
            "        else:",
            "            headers = headers.copy()",
            "",
            "        if not headers.hasHeader(b\"host\"):",
            "            headers.addRawHeader(b\"host\", parsed_uri.netloc)",
            "        if not headers.hasHeader(b\"user-agent\"):",
            "            headers.addRawHeader(b\"user-agent\", self.user_agent)",
            "",
            "        res = yield make_deferred_yieldable(",
            "            self._agent.request(method, uri, headers, bodyProducer)",
            "        )",
            "",
            "        return res",
            "",
            "",
            "@implementer(IAgentEndpointFactory)",
            "class MatrixHostnameEndpointFactory:",
            "    \"\"\"Factory for MatrixHostnameEndpoint for parsing to an Agent.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        reactor: IReactorCore,",
            "        tls_client_options_factory: Optional[FederationPolicyForHTTPS],",
            "        srv_resolver: Optional[SrvResolver],",
            "    ):",
            "        self._reactor = reactor",
            "        self._tls_client_options_factory = tls_client_options_factory",
            "",
            "        if srv_resolver is None:",
            "            srv_resolver = SrvResolver()",
            "",
            "        self._srv_resolver = srv_resolver",
            "",
            "    def endpointForURI(self, parsed_uri):",
            "        return MatrixHostnameEndpoint(",
            "            self._reactor,",
            "            self._tls_client_options_factory,",
            "            self._srv_resolver,",
            "            parsed_uri,",
            "        )",
            "",
            "",
            "@implementer(IStreamClientEndpoint)",
            "class MatrixHostnameEndpoint:",
            "    \"\"\"An endpoint that resolves matrix:// URLs using Matrix server name",
            "    resolution (i.e. via SRV). Does not check for well-known delegation.",
            "",
            "    Args:",
            "        reactor: twisted reactor to use for underlying requests",
            "        tls_client_options_factory:",
            "            factory to use for fetching client tls options, or none to disable TLS.",
            "        srv_resolver: The SRV resolver to use",
            "        parsed_uri: The parsed URI that we're wanting to connect to.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        reactor: IReactorCore,",
            "        tls_client_options_factory: Optional[FederationPolicyForHTTPS],",
            "        srv_resolver: SrvResolver,",
            "        parsed_uri: URI,",
            "    ):",
            "        self._reactor = reactor",
            "",
            "        self._parsed_uri = parsed_uri",
            "",
            "        # set up the TLS connection params",
            "        #",
            "        # XXX disabling TLS is really only supported here for the benefit of the",
            "        # unit tests. We should make the UTs cope with TLS rather than having to make",
            "        # the code support the unit tests.",
            "",
            "        if tls_client_options_factory is None:",
            "            self._tls_options = None",
            "        else:",
            "            self._tls_options = tls_client_options_factory.get_options(",
            "                self._parsed_uri.host",
            "            )",
            "",
            "        self._srv_resolver = srv_resolver",
            "",
            "    def connect(self, protocol_factory: IProtocolFactory) -> defer.Deferred:",
            "        \"\"\"Implements IStreamClientEndpoint interface",
            "        \"\"\"",
            "",
            "        return run_in_background(self._do_connect, protocol_factory)",
            "",
            "    async def _do_connect(self, protocol_factory: IProtocolFactory) -> None:",
            "        first_exception = None",
            "",
            "        server_list = await self._resolve_server()",
            "",
            "        for server in server_list:",
            "            host = server.host",
            "            port = server.port",
            "",
            "            try:",
            "                logger.debug(\"Connecting to %s:%i\", host.decode(\"ascii\"), port)",
            "                endpoint = HostnameEndpoint(self._reactor, host, port)",
            "                if self._tls_options:",
            "                    endpoint = wrapClientTLS(self._tls_options, endpoint)",
            "                result = await make_deferred_yieldable(",
            "                    endpoint.connect(protocol_factory)",
            "                )",
            "",
            "                return result",
            "            except Exception as e:",
            "                logger.info(",
            "                    \"Failed to connect to %s:%i: %s\", host.decode(\"ascii\"), port, e",
            "                )",
            "                if not first_exception:",
            "                    first_exception = e",
            "",
            "        # We return the first failure because that's probably the most interesting.",
            "        if first_exception:",
            "            raise first_exception",
            "",
            "        # This shouldn't happen as we should always have at least one host/port",
            "        # to try and if that doesn't work then we'll have an exception.",
            "        raise Exception(\"Failed to resolve server %r\" % (self._parsed_uri.netloc,))",
            "",
            "    async def _resolve_server(self) -> List[Server]:",
            "        \"\"\"Resolves the server name to a list of hosts and ports to attempt to",
            "        connect to.",
            "        \"\"\"",
            "",
            "        if self._parsed_uri.scheme != b\"matrix\":",
            "            return [Server(host=self._parsed_uri.host, port=self._parsed_uri.port)]",
            "",
            "        # Note: We don't do well-known lookup as that needs to have happened",
            "        # before now, due to needing to rewrite the Host header of the HTTP",
            "        # request.",
            "",
            "        # We reparse the URI so that defaultPort is -1 rather than 80",
            "        parsed_uri = urllib.parse.urlparse(self._parsed_uri.toBytes())",
            "",
            "        host = parsed_uri.hostname",
            "        port = parsed_uri.port",
            "",
            "        # If there is an explicit port or the host is an IP address we bypass",
            "        # SRV lookups and just use the given host/port.",
            "        if port or _is_ip_literal(host):",
            "            return [Server(host, port or 8448)]",
            "",
            "        server_list = await self._srv_resolver.resolve_service(b\"_matrix._tcp.\" + host)",
            "",
            "        if server_list:",
            "            return server_list",
            "",
            "        # No SRV records, so we fallback to host and 8448",
            "        return [Server(host, 8448)]",
            "",
            "",
            "def _is_ip_literal(host: bytes) -> bool:",
            "    \"\"\"Test if the given host name is either an IPv4 or IPv6 literal.",
            "",
            "    Args:",
            "        host: The host name to check",
            "",
            "    Returns:",
            "        True if the hostname is an IP address literal.",
            "    \"\"\"",
            "",
            "    host_str = host.decode(\"ascii\")",
            "",
            "    try:",
            "        IPAddress(host_str)",
            "        return True",
            "    except AddrFormatError:",
            "        return False"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "19": [],
            "95": [
                "MatrixFederationAgent",
                "__init__"
            ],
            "97": [
                "MatrixFederationAgent",
                "__init__"
            ],
            "98": [
                "MatrixFederationAgent",
                "__init__"
            ]
        },
        "addLocation": []
    },
    "synapse/http/matrixfederationclient.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from canonicaljson import encode_canonical_json"
            },
            "1": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " from prometheus_client import Counter"
            },
            "2": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from signedjson.sign import sign_json"
            },
            "3": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from zope.interface import implementer"
            },
            "4": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " from twisted.internet import defer"
            },
            "6": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " from twisted.internet.error import DNSLookupError"
            },
            "7": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from twisted.internet.interfaces import IReactorPluggableNameResolver, IReactorTime"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+from twisted.internet.interfaces import IReactorTime"
            },
            "9": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " from twisted.internet.task import _EPSILON, Cooperator"
            },
            "10": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " from twisted.web.http_headers import Headers"
            },
            "11": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " from twisted.web.iweb import IBodyProducer, IResponse"
            },
            "12": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": 44,
                "PatchRowcode": " from synapse.http import QuieterFileBodyProducer"
            },
            "13": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 45,
                "PatchRowcode": " from synapse.http.client import ("
            },
            "14": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 46,
                "PatchRowcode": "     BlacklistingAgentWrapper,"
            },
            "15": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    IPBlacklistingResolver,"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+    BlacklistingReactorWrapper,"
            },
            "17": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 48,
                "PatchRowcode": "     encode_query_args,"
            },
            "18": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 49,
                "PatchRowcode": "     readBodyToFile,"
            },
            "19": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 50,
                "PatchRowcode": " )"
            },
            "20": {
                "beforePatchRowNumber": 221,
                "afterPatchRowNumber": 220,
                "PatchRowcode": "         self.signing_key = hs.signing_key"
            },
            "21": {
                "beforePatchRowNumber": 222,
                "afterPatchRowNumber": 221,
                "PatchRowcode": "         self.server_name = hs.hostname"
            },
            "22": {
                "beforePatchRowNumber": 223,
                "afterPatchRowNumber": 222,
                "PatchRowcode": " "
            },
            "23": {
                "beforePatchRowNumber": 224,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        real_reactor = hs.get_reactor()"
            },
            "24": {
                "beforePatchRowNumber": 225,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "25": {
                "beforePatchRowNumber": 226,
                "afterPatchRowNumber": 223,
                "PatchRowcode": "         # We need to use a DNS resolver which filters out blacklisted IP"
            },
            "26": {
                "beforePatchRowNumber": 227,
                "afterPatchRowNumber": 224,
                "PatchRowcode": "         # addresses, to prevent DNS rebinding."
            },
            "27": {
                "beforePatchRowNumber": 228,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        nameResolver = IPBlacklistingResolver("
            },
            "28": {
                "beforePatchRowNumber": 229,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            real_reactor, None, hs.config.federation_ip_range_blacklist"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 225,
                "PatchRowcode": "+        self.reactor = BlacklistingReactorWrapper("
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 226,
                "PatchRowcode": "+            hs.get_reactor(), None, hs.config.federation_ip_range_blacklist"
            },
            "31": {
                "beforePatchRowNumber": 230,
                "afterPatchRowNumber": 227,
                "PatchRowcode": "         )"
            },
            "32": {
                "beforePatchRowNumber": 231,
                "afterPatchRowNumber": 228,
                "PatchRowcode": " "
            },
            "33": {
                "beforePatchRowNumber": 232,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        @implementer(IReactorPluggableNameResolver)"
            },
            "34": {
                "beforePatchRowNumber": 233,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        class Reactor:"
            },
            "35": {
                "beforePatchRowNumber": 234,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            def __getattr__(_self, attr):"
            },
            "36": {
                "beforePatchRowNumber": 235,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                if attr == \"nameResolver\":"
            },
            "37": {
                "beforePatchRowNumber": 236,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    return nameResolver"
            },
            "38": {
                "beforePatchRowNumber": 237,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                else:"
            },
            "39": {
                "beforePatchRowNumber": 238,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    return getattr(real_reactor, attr)"
            },
            "40": {
                "beforePatchRowNumber": 239,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "41": {
                "beforePatchRowNumber": 240,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.reactor = Reactor()"
            },
            "42": {
                "beforePatchRowNumber": 241,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "43": {
                "beforePatchRowNumber": 242,
                "afterPatchRowNumber": 229,
                "PatchRowcode": "         user_agent = hs.version_string"
            },
            "44": {
                "beforePatchRowNumber": 243,
                "afterPatchRowNumber": 230,
                "PatchRowcode": "         if hs.config.user_agent_suffix:"
            },
            "45": {
                "beforePatchRowNumber": 244,
                "afterPatchRowNumber": 231,
                "PatchRowcode": "             user_agent = \"%s %s\" % (user_agent, hs.config.user_agent_suffix)"
            },
            "46": {
                "beforePatchRowNumber": 245,
                "afterPatchRowNumber": 232,
                "PatchRowcode": "         user_agent = user_agent.encode(\"ascii\")"
            },
            "47": {
                "beforePatchRowNumber": 246,
                "afterPatchRowNumber": 233,
                "PatchRowcode": " "
            },
            "48": {
                "beforePatchRowNumber": 247,
                "afterPatchRowNumber": 234,
                "PatchRowcode": "         self.agent = MatrixFederationAgent("
            },
            "49": {
                "beforePatchRowNumber": 248,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self.reactor, tls_client_options_factory, user_agent"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 235,
                "PatchRowcode": "+            self.reactor,"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 236,
                "PatchRowcode": "+            tls_client_options_factory,"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 237,
                "PatchRowcode": "+            user_agent,"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 238,
                "PatchRowcode": "+            hs.config.federation_ip_range_blacklist,"
            },
            "54": {
                "beforePatchRowNumber": 249,
                "afterPatchRowNumber": 239,
                "PatchRowcode": "         )"
            },
            "55": {
                "beforePatchRowNumber": 250,
                "afterPatchRowNumber": 240,
                "PatchRowcode": " "
            },
            "56": {
                "beforePatchRowNumber": 251,
                "afterPatchRowNumber": 241,
                "PatchRowcode": "         # Use a BlacklistingAgentWrapper to prevent circumventing the IP"
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import cgi",
            "import logging",
            "import random",
            "import sys",
            "import urllib.parse",
            "from io import BytesIO",
            "from typing import Callable, Dict, List, Optional, Tuple, Union",
            "",
            "import attr",
            "import treq",
            "from canonicaljson import encode_canonical_json",
            "from prometheus_client import Counter",
            "from signedjson.sign import sign_json",
            "from zope.interface import implementer",
            "",
            "from twisted.internet import defer",
            "from twisted.internet.error import DNSLookupError",
            "from twisted.internet.interfaces import IReactorPluggableNameResolver, IReactorTime",
            "from twisted.internet.task import _EPSILON, Cooperator",
            "from twisted.web.http_headers import Headers",
            "from twisted.web.iweb import IBodyProducer, IResponse",
            "",
            "import synapse.metrics",
            "import synapse.util.retryutils",
            "from synapse.api.errors import (",
            "    FederationDeniedError,",
            "    HttpResponseException,",
            "    RequestSendFailed,",
            ")",
            "from synapse.http import QuieterFileBodyProducer",
            "from synapse.http.client import (",
            "    BlacklistingAgentWrapper,",
            "    IPBlacklistingResolver,",
            "    encode_query_args,",
            "    readBodyToFile,",
            ")",
            "from synapse.http.federation.matrix_federation_agent import MatrixFederationAgent",
            "from synapse.logging.context import make_deferred_yieldable",
            "from synapse.logging.opentracing import (",
            "    inject_active_span_byte_dict,",
            "    set_tag,",
            "    start_active_span,",
            "    tags,",
            ")",
            "from synapse.types import JsonDict",
            "from synapse.util import json_decoder",
            "from synapse.util.async_helpers import timeout_deferred",
            "from synapse.util.metrics import Measure",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "outgoing_requests_counter = Counter(",
            "    \"synapse_http_matrixfederationclient_requests\", \"\", [\"method\"]",
            ")",
            "incoming_responses_counter = Counter(",
            "    \"synapse_http_matrixfederationclient_responses\", \"\", [\"method\", \"code\"]",
            ")",
            "",
            "",
            "MAX_LONG_RETRIES = 10",
            "MAX_SHORT_RETRIES = 3",
            "MAXINT = sys.maxsize",
            "",
            "",
            "_next_id = 1",
            "",
            "",
            "QueryArgs = Dict[str, Union[str, List[str]]]",
            "",
            "",
            "@attr.s(slots=True, frozen=True)",
            "class MatrixFederationRequest:",
            "    method = attr.ib(type=str)",
            "    \"\"\"HTTP method",
            "    \"\"\"",
            "",
            "    path = attr.ib(type=str)",
            "    \"\"\"HTTP path",
            "    \"\"\"",
            "",
            "    destination = attr.ib(type=str)",
            "    \"\"\"The remote server to send the HTTP request to.",
            "    \"\"\"",
            "",
            "    json = attr.ib(default=None, type=Optional[JsonDict])",
            "    \"\"\"JSON to send in the body.",
            "    \"\"\"",
            "",
            "    json_callback = attr.ib(default=None, type=Optional[Callable[[], JsonDict]])",
            "    \"\"\"A callback to generate the JSON.",
            "    \"\"\"",
            "",
            "    query = attr.ib(default=None, type=Optional[dict])",
            "    \"\"\"Query arguments.",
            "    \"\"\"",
            "",
            "    txn_id = attr.ib(default=None, type=Optional[str])",
            "    \"\"\"Unique ID for this request (for logging)",
            "    \"\"\"",
            "",
            "    uri = attr.ib(init=False, type=bytes)",
            "    \"\"\"The URI of this request",
            "    \"\"\"",
            "",
            "    def __attrs_post_init__(self) -> None:",
            "        global _next_id",
            "        txn_id = \"%s-O-%s\" % (self.method, _next_id)",
            "        _next_id = (_next_id + 1) % (MAXINT - 1)",
            "",
            "        object.__setattr__(self, \"txn_id\", txn_id)",
            "",
            "        destination_bytes = self.destination.encode(\"ascii\")",
            "        path_bytes = self.path.encode(\"ascii\")",
            "        if self.query:",
            "            query_bytes = encode_query_args(self.query)",
            "        else:",
            "            query_bytes = b\"\"",
            "",
            "        # The object is frozen so we can pre-compute this.",
            "        uri = urllib.parse.urlunparse(",
            "            (b\"matrix\", destination_bytes, path_bytes, None, query_bytes, b\"\")",
            "        )",
            "        object.__setattr__(self, \"uri\", uri)",
            "",
            "    def get_json(self) -> Optional[JsonDict]:",
            "        if self.json_callback:",
            "            return self.json_callback()",
            "        return self.json",
            "",
            "",
            "async def _handle_json_response(",
            "    reactor: IReactorTime,",
            "    timeout_sec: float,",
            "    request: MatrixFederationRequest,",
            "    response: IResponse,",
            "    start_ms: int,",
            ") -> JsonDict:",
            "    \"\"\"",
            "    Reads the JSON body of a response, with a timeout",
            "",
            "    Args:",
            "        reactor: twisted reactor, for the timeout",
            "        timeout_sec: number of seconds to wait for response to complete",
            "        request: the request that triggered the response",
            "        response: response to the request",
            "        start_ms: Timestamp when request was made",
            "",
            "    Returns:",
            "        The parsed JSON response",
            "    \"\"\"",
            "    try:",
            "        check_content_type_is_json(response.headers)",
            "",
            "        # Use the custom JSON decoder (partially re-implements treq.json_content).",
            "        d = treq.text_content(response, encoding=\"utf-8\")",
            "        d.addCallback(json_decoder.decode)",
            "        d = timeout_deferred(d, timeout=timeout_sec, reactor=reactor)",
            "",
            "        body = await make_deferred_yieldable(d)",
            "    except defer.TimeoutError as e:",
            "        logger.warning(",
            "            \"{%s} [%s] Timed out reading response - %s %s\",",
            "            request.txn_id,",
            "            request.destination,",
            "            request.method,",
            "            request.uri.decode(\"ascii\"),",
            "        )",
            "        raise RequestSendFailed(e, can_retry=True) from e",
            "    except Exception as e:",
            "        logger.warning(",
            "            \"{%s} [%s] Error reading response %s %s: %s\",",
            "            request.txn_id,",
            "            request.destination,",
            "            request.method,",
            "            request.uri.decode(\"ascii\"),",
            "            e,",
            "        )",
            "        raise",
            "",
            "    time_taken_secs = reactor.seconds() - start_ms / 1000",
            "",
            "    logger.info(",
            "        \"{%s} [%s] Completed request: %d %s in %.2f secs - %s %s\",",
            "        request.txn_id,",
            "        request.destination,",
            "        response.code,",
            "        response.phrase.decode(\"ascii\", errors=\"replace\"),",
            "        time_taken_secs,",
            "        request.method,",
            "        request.uri.decode(\"ascii\"),",
            "    )",
            "    return body",
            "",
            "",
            "class MatrixFederationHttpClient:",
            "    \"\"\"HTTP client used to talk to other homeservers over the federation",
            "    protocol. Send client certificates and signs requests.",
            "",
            "    Attributes:",
            "        agent (twisted.web.client.Agent): The twisted Agent used to send the",
            "            requests.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs, tls_client_options_factory):",
            "        self.hs = hs",
            "        self.signing_key = hs.signing_key",
            "        self.server_name = hs.hostname",
            "",
            "        real_reactor = hs.get_reactor()",
            "",
            "        # We need to use a DNS resolver which filters out blacklisted IP",
            "        # addresses, to prevent DNS rebinding.",
            "        nameResolver = IPBlacklistingResolver(",
            "            real_reactor, None, hs.config.federation_ip_range_blacklist",
            "        )",
            "",
            "        @implementer(IReactorPluggableNameResolver)",
            "        class Reactor:",
            "            def __getattr__(_self, attr):",
            "                if attr == \"nameResolver\":",
            "                    return nameResolver",
            "                else:",
            "                    return getattr(real_reactor, attr)",
            "",
            "        self.reactor = Reactor()",
            "",
            "        user_agent = hs.version_string",
            "        if hs.config.user_agent_suffix:",
            "            user_agent = \"%s %s\" % (user_agent, hs.config.user_agent_suffix)",
            "        user_agent = user_agent.encode(\"ascii\")",
            "",
            "        self.agent = MatrixFederationAgent(",
            "            self.reactor, tls_client_options_factory, user_agent",
            "        )",
            "",
            "        # Use a BlacklistingAgentWrapper to prevent circumventing the IP",
            "        # blacklist via IP literals in server names",
            "        self.agent = BlacklistingAgentWrapper(",
            "            self.agent, ip_blacklist=hs.config.federation_ip_range_blacklist,",
            "        )",
            "",
            "        self.clock = hs.get_clock()",
            "        self._store = hs.get_datastore()",
            "        self.version_string_bytes = hs.version_string.encode(\"ascii\")",
            "        self.default_timeout = 60",
            "",
            "        def schedule(x):",
            "            self.reactor.callLater(_EPSILON, x)",
            "",
            "        self._cooperator = Cooperator(scheduler=schedule)",
            "",
            "    async def _send_request_with_optional_trailing_slash(",
            "        self,",
            "        request: MatrixFederationRequest,",
            "        try_trailing_slash_on_400: bool = False,",
            "        **send_request_args",
            "    ) -> IResponse:",
            "        \"\"\"Wrapper for _send_request which can optionally retry the request",
            "        upon receiving a combination of a 400 HTTP response code and a",
            "        'M_UNRECOGNIZED' errcode. This is a workaround for Synapse <= v0.99.3",
            "        due to #3622.",
            "",
            "        Args:",
            "            request: details of request to be sent",
            "            try_trailing_slash_on_400: Whether on receiving a 400",
            "                'M_UNRECOGNIZED' from the server to retry the request with a",
            "                trailing slash appended to the request path.",
            "            send_request_args: A dictionary of arguments to pass to `_send_request()`.",
            "",
            "        Raises:",
            "            HttpResponseException: If we get an HTTP response code >= 300",
            "                (except 429).",
            "",
            "        Returns:",
            "            Parsed JSON response body.",
            "        \"\"\"",
            "        try:",
            "            response = await self._send_request(request, **send_request_args)",
            "        except HttpResponseException as e:",
            "            # Received an HTTP error > 300. Check if it meets the requirements",
            "            # to retry with a trailing slash",
            "            if not try_trailing_slash_on_400:",
            "                raise",
            "",
            "            if e.code != 400 or e.to_synapse_error().errcode != \"M_UNRECOGNIZED\":",
            "                raise",
            "",
            "            # Retry with a trailing slash if we received a 400 with",
            "            # 'M_UNRECOGNIZED' which some endpoints can return when omitting a",
            "            # trailing slash on Synapse <= v0.99.3.",
            "            logger.info(\"Retrying request with trailing slash\")",
            "",
            "            # Request is frozen so we create a new instance",
            "            request = attr.evolve(request, path=request.path + \"/\")",
            "",
            "            response = await self._send_request(request, **send_request_args)",
            "",
            "        return response",
            "",
            "    async def _send_request(",
            "        self,",
            "        request: MatrixFederationRequest,",
            "        retry_on_dns_fail: bool = True,",
            "        timeout: Optional[int] = None,",
            "        long_retries: bool = False,",
            "        ignore_backoff: bool = False,",
            "        backoff_on_404: bool = False,",
            "    ) -> IResponse:",
            "        \"\"\"",
            "        Sends a request to the given server.",
            "",
            "        Args:",
            "            request: details of request to be sent",
            "",
            "            retry_on_dns_fail: true if the request should be retied on DNS failures",
            "",
            "            timeout: number of milliseconds to wait for the response headers",
            "                (including connecting to the server), *for each attempt*.",
            "                60s by default.",
            "",
            "            long_retries: whether to use the long retry algorithm.",
            "",
            "                The regular retry algorithm makes 4 attempts, with intervals",
            "                [0.5s, 1s, 2s].",
            "",
            "                The long retry algorithm makes 11 attempts, with intervals",
            "                [4s, 16s, 60s, 60s, ...]",
            "",
            "                Both algorithms add -20%/+40% jitter to the retry intervals.",
            "",
            "                Note that the above intervals are *in addition* to the time spent",
            "                waiting for the request to complete (up to `timeout` ms).",
            "",
            "                NB: the long retry algorithm takes over 20 minutes to complete, with",
            "                a default timeout of 60s!",
            "",
            "            ignore_backoff: true to ignore the historical backoff data",
            "                and try the request anyway.",
            "",
            "            backoff_on_404: Back off if we get a 404",
            "",
            "        Returns:",
            "            Resolves with the HTTP response object on success.",
            "",
            "        Raises:",
            "            HttpResponseException: If we get an HTTP response code >= 300",
            "                (except 429).",
            "            NotRetryingDestination: If we are not yet ready to retry this",
            "                server.",
            "            FederationDeniedError: If this destination  is not on our",
            "                federation whitelist",
            "            RequestSendFailed: If there were problems connecting to the",
            "                remote, due to e.g. DNS failures, connection timeouts etc.",
            "        \"\"\"",
            "        if timeout:",
            "            _sec_timeout = timeout / 1000",
            "        else:",
            "            _sec_timeout = self.default_timeout",
            "",
            "        if (",
            "            self.hs.config.federation_domain_whitelist is not None",
            "            and request.destination not in self.hs.config.federation_domain_whitelist",
            "        ):",
            "            raise FederationDeniedError(request.destination)",
            "",
            "        limiter = await synapse.util.retryutils.get_retry_limiter(",
            "            request.destination,",
            "            self.clock,",
            "            self._store,",
            "            backoff_on_404=backoff_on_404,",
            "            ignore_backoff=ignore_backoff,",
            "        )",
            "",
            "        method_bytes = request.method.encode(\"ascii\")",
            "        destination_bytes = request.destination.encode(\"ascii\")",
            "        path_bytes = request.path.encode(\"ascii\")",
            "        if request.query:",
            "            query_bytes = encode_query_args(request.query)",
            "        else:",
            "            query_bytes = b\"\"",
            "",
            "        scope = start_active_span(",
            "            \"outgoing-federation-request\",",
            "            tags={",
            "                tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,",
            "                tags.PEER_ADDRESS: request.destination,",
            "                tags.HTTP_METHOD: request.method,",
            "                tags.HTTP_URL: request.path,",
            "            },",
            "            finish_on_close=True,",
            "        )",
            "",
            "        # Inject the span into the headers",
            "        headers_dict = {}  # type: Dict[bytes, List[bytes]]",
            "        inject_active_span_byte_dict(headers_dict, request.destination)",
            "",
            "        headers_dict[b\"User-Agent\"] = [self.version_string_bytes]",
            "",
            "        with limiter, scope:",
            "            # XXX: Would be much nicer to retry only at the transaction-layer",
            "            # (once we have reliable transactions in place)",
            "            if long_retries:",
            "                retries_left = MAX_LONG_RETRIES",
            "            else:",
            "                retries_left = MAX_SHORT_RETRIES",
            "",
            "            url_bytes = request.uri",
            "            url_str = url_bytes.decode(\"ascii\")",
            "",
            "            url_to_sign_bytes = urllib.parse.urlunparse(",
            "                (b\"\", b\"\", path_bytes, None, query_bytes, b\"\")",
            "            )",
            "",
            "            while True:",
            "                try:",
            "                    json = request.get_json()",
            "                    if json:",
            "                        headers_dict[b\"Content-Type\"] = [b\"application/json\"]",
            "                        auth_headers = self.build_auth_headers(",
            "                            destination_bytes, method_bytes, url_to_sign_bytes, json",
            "                        )",
            "                        data = encode_canonical_json(json)",
            "                        producer = QuieterFileBodyProducer(",
            "                            BytesIO(data), cooperator=self._cooperator",
            "                        )  # type: Optional[IBodyProducer]",
            "                    else:",
            "                        producer = None",
            "                        auth_headers = self.build_auth_headers(",
            "                            destination_bytes, method_bytes, url_to_sign_bytes",
            "                        )",
            "",
            "                    headers_dict[b\"Authorization\"] = auth_headers",
            "",
            "                    logger.debug(",
            "                        \"{%s} [%s] Sending request: %s %s; timeout %fs\",",
            "                        request.txn_id,",
            "                        request.destination,",
            "                        request.method,",
            "                        url_str,",
            "                        _sec_timeout,",
            "                    )",
            "",
            "                    outgoing_requests_counter.labels(request.method).inc()",
            "",
            "                    try:",
            "                        with Measure(self.clock, \"outbound_request\"):",
            "                            # we don't want all the fancy cookie and redirect handling",
            "                            # that treq.request gives: just use the raw Agent.",
            "                            request_deferred = self.agent.request(",
            "                                method_bytes,",
            "                                url_bytes,",
            "                                headers=Headers(headers_dict),",
            "                                bodyProducer=producer,",
            "                            )",
            "",
            "                            request_deferred = timeout_deferred(",
            "                                request_deferred,",
            "                                timeout=_sec_timeout,",
            "                                reactor=self.reactor,",
            "                            )",
            "",
            "                            response = await request_deferred",
            "                    except DNSLookupError as e:",
            "                        raise RequestSendFailed(e, can_retry=retry_on_dns_fail) from e",
            "                    except Exception as e:",
            "                        raise RequestSendFailed(e, can_retry=True) from e",
            "",
            "                    incoming_responses_counter.labels(",
            "                        request.method, response.code",
            "                    ).inc()",
            "",
            "                    set_tag(tags.HTTP_STATUS_CODE, response.code)",
            "                    response_phrase = response.phrase.decode(\"ascii\", errors=\"replace\")",
            "",
            "                    if 200 <= response.code < 300:",
            "                        logger.debug(",
            "                            \"{%s} [%s] Got response headers: %d %s\",",
            "                            request.txn_id,",
            "                            request.destination,",
            "                            response.code,",
            "                            response_phrase,",
            "                        )",
            "                        pass",
            "                    else:",
            "                        logger.info(",
            "                            \"{%s} [%s] Got response headers: %d %s\",",
            "                            request.txn_id,",
            "                            request.destination,",
            "                            response.code,",
            "                            response_phrase,",
            "                        )",
            "                        # :'(",
            "                        # Update transactions table?",
            "                        d = treq.content(response)",
            "                        d = timeout_deferred(",
            "                            d, timeout=_sec_timeout, reactor=self.reactor",
            "                        )",
            "",
            "                        try:",
            "                            body = await make_deferred_yieldable(d)",
            "                        except Exception as e:",
            "                            # Eh, we're already going to raise an exception so lets",
            "                            # ignore if this fails.",
            "                            logger.warning(",
            "                                \"{%s} [%s] Failed to get error response: %s %s: %s\",",
            "                                request.txn_id,",
            "                                request.destination,",
            "                                request.method,",
            "                                url_str,",
            "                                _flatten_response_never_received(e),",
            "                            )",
            "                            body = None",
            "",
            "                        exc = HttpResponseException(",
            "                            response.code, response_phrase, body",
            "                        )",
            "",
            "                        # Retry if the error is a 429 (Too Many Requests),",
            "                        # otherwise just raise a standard HttpResponseException",
            "                        if response.code == 429:",
            "                            raise RequestSendFailed(exc, can_retry=True) from exc",
            "                        else:",
            "                            raise exc",
            "",
            "                    break",
            "                except RequestSendFailed as e:",
            "                    logger.info(",
            "                        \"{%s} [%s] Request failed: %s %s: %s\",",
            "                        request.txn_id,",
            "                        request.destination,",
            "                        request.method,",
            "                        url_str,",
            "                        _flatten_response_never_received(e.inner_exception),",
            "                    )",
            "",
            "                    if not e.can_retry:",
            "                        raise",
            "",
            "                    if retries_left and not timeout:",
            "                        if long_retries:",
            "                            delay = 4 ** (MAX_LONG_RETRIES + 1 - retries_left)",
            "                            delay = min(delay, 60)",
            "                            delay *= random.uniform(0.8, 1.4)",
            "                        else:",
            "                            delay = 0.5 * 2 ** (MAX_SHORT_RETRIES - retries_left)",
            "                            delay = min(delay, 2)",
            "                            delay *= random.uniform(0.8, 1.4)",
            "",
            "                        logger.debug(",
            "                            \"{%s} [%s] Waiting %ss before re-sending...\",",
            "                            request.txn_id,",
            "                            request.destination,",
            "                            delay,",
            "                        )",
            "",
            "                        await self.clock.sleep(delay)",
            "                        retries_left -= 1",
            "                    else:",
            "                        raise",
            "",
            "                except Exception as e:",
            "                    logger.warning(",
            "                        \"{%s} [%s] Request failed: %s %s: %s\",",
            "                        request.txn_id,",
            "                        request.destination,",
            "                        request.method,",
            "                        url_str,",
            "                        _flatten_response_never_received(e),",
            "                    )",
            "                    raise",
            "        return response",
            "",
            "    def build_auth_headers(",
            "        self,",
            "        destination: Optional[bytes],",
            "        method: bytes,",
            "        url_bytes: bytes,",
            "        content: Optional[JsonDict] = None,",
            "        destination_is: Optional[bytes] = None,",
            "    ) -> List[bytes]:",
            "        \"\"\"",
            "        Builds the Authorization headers for a federation request",
            "        Args:",
            "            destination: The destination homeserver of the request.",
            "                May be None if the destination is an identity server, in which case",
            "                destination_is must be non-None.",
            "            method: The HTTP method of the request",
            "            url_bytes: The URI path of the request",
            "            content: The body of the request",
            "            destination_is: As 'destination', but if the destination is an",
            "                identity server",
            "",
            "        Returns:",
            "            A list of headers to be added as \"Authorization:\" headers",
            "        \"\"\"",
            "        request = {",
            "            \"method\": method.decode(\"ascii\"),",
            "            \"uri\": url_bytes.decode(\"ascii\"),",
            "            \"origin\": self.server_name,",
            "        }",
            "",
            "        if destination is not None:",
            "            request[\"destination\"] = destination.decode(\"ascii\")",
            "",
            "        if destination_is is not None:",
            "            request[\"destination_is\"] = destination_is.decode(\"ascii\")",
            "",
            "        if content is not None:",
            "            request[\"content\"] = content",
            "",
            "        request = sign_json(request, self.server_name, self.signing_key)",
            "",
            "        auth_headers = []",
            "",
            "        for key, sig in request[\"signatures\"][self.server_name].items():",
            "            auth_headers.append(",
            "                (",
            "                    'X-Matrix origin=%s,key=\"%s\",sig=\"%s\"'",
            "                    % (self.server_name, key, sig)",
            "                ).encode(\"ascii\")",
            "            )",
            "        return auth_headers",
            "",
            "    async def put_json(",
            "        self,",
            "        destination: str,",
            "        path: str,",
            "        args: Optional[QueryArgs] = None,",
            "        data: Optional[JsonDict] = None,",
            "        json_data_callback: Optional[Callable[[], JsonDict]] = None,",
            "        long_retries: bool = False,",
            "        timeout: Optional[int] = None,",
            "        ignore_backoff: bool = False,",
            "        backoff_on_404: bool = False,",
            "        try_trailing_slash_on_400: bool = False,",
            "    ) -> Union[JsonDict, list]:",
            "        \"\"\" Sends the specified json data using PUT",
            "",
            "        Args:",
            "            destination: The remote server to send the HTTP request to.",
            "            path: The HTTP path.",
            "            args: query params",
            "            data: A dict containing the data that will be used as",
            "                the request body. This will be encoded as JSON.",
            "            json_data_callback: A callable returning the dict to",
            "                use as the request body.",
            "",
            "            long_retries: whether to use the long retry algorithm. See",
            "                docs on _send_request for details.",
            "",
            "            timeout: number of milliseconds to wait for the response.",
            "                self._default_timeout (60s) by default.",
            "",
            "                Note that we may make several attempts to send the request; this",
            "                timeout applies to the time spent waiting for response headers for",
            "                *each* attempt (including connection time) as well as the time spent",
            "                reading the response body after a 200 response.",
            "",
            "            ignore_backoff: true to ignore the historical backoff data",
            "                and try the request anyway.",
            "            backoff_on_404: True if we should count a 404 response as",
            "                a failure of the server (and should therefore back off future",
            "                requests).",
            "            try_trailing_slash_on_400: True if on a 400 M_UNRECOGNIZED",
            "                response we should try appending a trailing slash to the end",
            "                of the request. Workaround for #3622 in Synapse <= v0.99.3. This",
            "                will be attempted before backing off if backing off has been",
            "                enabled.",
            "",
            "        Returns:",
            "            Succeeds when we get a 2xx HTTP response. The",
            "            result will be the decoded JSON body.",
            "",
            "        Raises:",
            "            HttpResponseException: If we get an HTTP response code >= 300",
            "                (except 429).",
            "            NotRetryingDestination: If we are not yet ready to retry this",
            "                server.",
            "            FederationDeniedError: If this destination  is not on our",
            "                federation whitelist",
            "            RequestSendFailed: If there were problems connecting to the",
            "                remote, due to e.g. DNS failures, connection timeouts etc.",
            "        \"\"\"",
            "        request = MatrixFederationRequest(",
            "            method=\"PUT\",",
            "            destination=destination,",
            "            path=path,",
            "            query=args,",
            "            json_callback=json_data_callback,",
            "            json=data,",
            "        )",
            "",
            "        start_ms = self.clock.time_msec()",
            "",
            "        response = await self._send_request_with_optional_trailing_slash(",
            "            request,",
            "            try_trailing_slash_on_400,",
            "            backoff_on_404=backoff_on_404,",
            "            ignore_backoff=ignore_backoff,",
            "            long_retries=long_retries,",
            "            timeout=timeout,",
            "        )",
            "",
            "        if timeout is not None:",
            "            _sec_timeout = timeout / 1000",
            "        else:",
            "            _sec_timeout = self.default_timeout",
            "",
            "        body = await _handle_json_response(",
            "            self.reactor, _sec_timeout, request, response, start_ms",
            "        )",
            "",
            "        return body",
            "",
            "    async def post_json(",
            "        self,",
            "        destination: str,",
            "        path: str,",
            "        data: Optional[JsonDict] = None,",
            "        long_retries: bool = False,",
            "        timeout: Optional[int] = None,",
            "        ignore_backoff: bool = False,",
            "        args: Optional[QueryArgs] = None,",
            "    ) -> Union[JsonDict, list]:",
            "        \"\"\" Sends the specified json data using POST",
            "",
            "        Args:",
            "            destination: The remote server to send the HTTP request to.",
            "",
            "            path: The HTTP path.",
            "",
            "            data: A dict containing the data that will be used as",
            "                the request body. This will be encoded as JSON.",
            "",
            "            long_retries: whether to use the long retry algorithm. See",
            "                docs on _send_request for details.",
            "",
            "            timeout: number of milliseconds to wait for the response.",
            "                self._default_timeout (60s) by default.",
            "",
            "                Note that we may make several attempts to send the request; this",
            "                timeout applies to the time spent waiting for response headers for",
            "                *each* attempt (including connection time) as well as the time spent",
            "                reading the response body after a 200 response.",
            "",
            "            ignore_backoff: true to ignore the historical backoff data and",
            "                try the request anyway.",
            "",
            "            args: query params",
            "        Returns:",
            "            dict|list: Succeeds when we get a 2xx HTTP response. The",
            "            result will be the decoded JSON body.",
            "",
            "        Raises:",
            "            HttpResponseException: If we get an HTTP response code >= 300",
            "                (except 429).",
            "            NotRetryingDestination: If we are not yet ready to retry this",
            "                server.",
            "            FederationDeniedError: If this destination  is not on our",
            "                federation whitelist",
            "            RequestSendFailed: If there were problems connecting to the",
            "                remote, due to e.g. DNS failures, connection timeouts etc.",
            "        \"\"\"",
            "",
            "        request = MatrixFederationRequest(",
            "            method=\"POST\", destination=destination, path=path, query=args, json=data",
            "        )",
            "",
            "        start_ms = self.clock.time_msec()",
            "",
            "        response = await self._send_request(",
            "            request,",
            "            long_retries=long_retries,",
            "            timeout=timeout,",
            "            ignore_backoff=ignore_backoff,",
            "        )",
            "",
            "        if timeout:",
            "            _sec_timeout = timeout / 1000",
            "        else:",
            "            _sec_timeout = self.default_timeout",
            "",
            "        body = await _handle_json_response(",
            "            self.reactor, _sec_timeout, request, response, start_ms,",
            "        )",
            "        return body",
            "",
            "    async def get_json(",
            "        self,",
            "        destination: str,",
            "        path: str,",
            "        args: Optional[QueryArgs] = None,",
            "        retry_on_dns_fail: bool = True,",
            "        timeout: Optional[int] = None,",
            "        ignore_backoff: bool = False,",
            "        try_trailing_slash_on_400: bool = False,",
            "    ) -> Union[JsonDict, list]:",
            "        \"\"\" GETs some json from the given host homeserver and path",
            "",
            "        Args:",
            "            destination: The remote server to send the HTTP request to.",
            "",
            "            path: The HTTP path.",
            "",
            "            args: A dictionary used to create query strings, defaults to",
            "                None.",
            "",
            "            timeout: number of milliseconds to wait for the response.",
            "                self._default_timeout (60s) by default.",
            "",
            "                Note that we may make several attempts to send the request; this",
            "                timeout applies to the time spent waiting for response headers for",
            "                *each* attempt (including connection time) as well as the time spent",
            "                reading the response body after a 200 response.",
            "",
            "            ignore_backoff: true to ignore the historical backoff data",
            "                and try the request anyway.",
            "",
            "            try_trailing_slash_on_400: True if on a 400 M_UNRECOGNIZED",
            "                response we should try appending a trailing slash to the end of",
            "                the request. Workaround for #3622 in Synapse <= v0.99.3.",
            "        Returns:",
            "            Succeeds when we get a 2xx HTTP response. The",
            "            result will be the decoded JSON body.",
            "",
            "        Raises:",
            "            HttpResponseException: If we get an HTTP response code >= 300",
            "                (except 429).",
            "            NotRetryingDestination: If we are not yet ready to retry this",
            "                server.",
            "            FederationDeniedError: If this destination  is not on our",
            "                federation whitelist",
            "            RequestSendFailed: If there were problems connecting to the",
            "                remote, due to e.g. DNS failures, connection timeouts etc.",
            "        \"\"\"",
            "        request = MatrixFederationRequest(",
            "            method=\"GET\", destination=destination, path=path, query=args",
            "        )",
            "",
            "        start_ms = self.clock.time_msec()",
            "",
            "        response = await self._send_request_with_optional_trailing_slash(",
            "            request,",
            "            try_trailing_slash_on_400,",
            "            backoff_on_404=False,",
            "            ignore_backoff=ignore_backoff,",
            "            retry_on_dns_fail=retry_on_dns_fail,",
            "            timeout=timeout,",
            "        )",
            "",
            "        if timeout is not None:",
            "            _sec_timeout = timeout / 1000",
            "        else:",
            "            _sec_timeout = self.default_timeout",
            "",
            "        body = await _handle_json_response(",
            "            self.reactor, _sec_timeout, request, response, start_ms",
            "        )",
            "",
            "        return body",
            "",
            "    async def delete_json(",
            "        self,",
            "        destination: str,",
            "        path: str,",
            "        long_retries: bool = False,",
            "        timeout: Optional[int] = None,",
            "        ignore_backoff: bool = False,",
            "        args: Optional[QueryArgs] = None,",
            "    ) -> Union[JsonDict, list]:",
            "        \"\"\"Send a DELETE request to the remote expecting some json response",
            "",
            "        Args:",
            "            destination: The remote server to send the HTTP request to.",
            "            path: The HTTP path.",
            "",
            "            long_retries: whether to use the long retry algorithm. See",
            "                docs on _send_request for details.",
            "",
            "            timeout: number of milliseconds to wait for the response.",
            "                self._default_timeout (60s) by default.",
            "",
            "                Note that we may make several attempts to send the request; this",
            "                timeout applies to the time spent waiting for response headers for",
            "                *each* attempt (including connection time) as well as the time spent",
            "                reading the response body after a 200 response.",
            "",
            "            ignore_backoff: true to ignore the historical backoff data and",
            "                try the request anyway.",
            "",
            "            args: query params",
            "        Returns:",
            "            Succeeds when we get a 2xx HTTP response. The",
            "            result will be the decoded JSON body.",
            "",
            "        Raises:",
            "            HttpResponseException: If we get an HTTP response code >= 300",
            "                (except 429).",
            "            NotRetryingDestination: If we are not yet ready to retry this",
            "                server.",
            "            FederationDeniedError: If this destination  is not on our",
            "                federation whitelist",
            "            RequestSendFailed: If there were problems connecting to the",
            "                remote, due to e.g. DNS failures, connection timeouts etc.",
            "        \"\"\"",
            "        request = MatrixFederationRequest(",
            "            method=\"DELETE\", destination=destination, path=path, query=args",
            "        )",
            "",
            "        start_ms = self.clock.time_msec()",
            "",
            "        response = await self._send_request(",
            "            request,",
            "            long_retries=long_retries,",
            "            timeout=timeout,",
            "            ignore_backoff=ignore_backoff,",
            "        )",
            "",
            "        if timeout is not None:",
            "            _sec_timeout = timeout / 1000",
            "        else:",
            "            _sec_timeout = self.default_timeout",
            "",
            "        body = await _handle_json_response(",
            "            self.reactor, _sec_timeout, request, response, start_ms",
            "        )",
            "        return body",
            "",
            "    async def get_file(",
            "        self,",
            "        destination: str,",
            "        path: str,",
            "        output_stream,",
            "        args: Optional[QueryArgs] = None,",
            "        retry_on_dns_fail: bool = True,",
            "        max_size: Optional[int] = None,",
            "        ignore_backoff: bool = False,",
            "    ) -> Tuple[int, Dict[bytes, List[bytes]]]:",
            "        \"\"\"GETs a file from a given homeserver",
            "        Args:",
            "            destination: The remote server to send the HTTP request to.",
            "            path: The HTTP path to GET.",
            "            output_stream: File to write the response body to.",
            "            args: Optional dictionary used to create the query string.",
            "            ignore_backoff: true to ignore the historical backoff data",
            "                and try the request anyway.",
            "",
            "        Returns:",
            "            Resolves with an (int,dict) tuple of",
            "            the file length and a dict of the response headers.",
            "",
            "        Raises:",
            "            HttpResponseException: If we get an HTTP response code >= 300",
            "                (except 429).",
            "            NotRetryingDestination: If we are not yet ready to retry this",
            "                server.",
            "            FederationDeniedError: If this destination  is not on our",
            "                federation whitelist",
            "            RequestSendFailed: If there were problems connecting to the",
            "                remote, due to e.g. DNS failures, connection timeouts etc.",
            "        \"\"\"",
            "        request = MatrixFederationRequest(",
            "            method=\"GET\", destination=destination, path=path, query=args",
            "        )",
            "",
            "        response = await self._send_request(",
            "            request, retry_on_dns_fail=retry_on_dns_fail, ignore_backoff=ignore_backoff",
            "        )",
            "",
            "        headers = dict(response.headers.getAllRawHeaders())",
            "",
            "        try:",
            "            d = readBodyToFile(response, output_stream, max_size)",
            "            d.addTimeout(self.default_timeout, self.reactor)",
            "            length = await make_deferred_yieldable(d)",
            "        except Exception as e:",
            "            logger.warning(",
            "                \"{%s} [%s] Error reading response: %s\",",
            "                request.txn_id,",
            "                request.destination,",
            "                e,",
            "            )",
            "            raise",
            "        logger.info(",
            "            \"{%s} [%s] Completed: %d %s [%d bytes] %s %s\",",
            "            request.txn_id,",
            "            request.destination,",
            "            response.code,",
            "            response.phrase.decode(\"ascii\", errors=\"replace\"),",
            "            length,",
            "            request.method,",
            "            request.uri.decode(\"ascii\"),",
            "        )",
            "        return (length, headers)",
            "",
            "",
            "def _flatten_response_never_received(e):",
            "    if hasattr(e, \"reasons\"):",
            "        reasons = \", \".join(",
            "            _flatten_response_never_received(f.value) for f in e.reasons",
            "        )",
            "",
            "        return \"%s:[%s]\" % (type(e).__name__, reasons)",
            "    else:",
            "        return repr(e)",
            "",
            "",
            "def check_content_type_is_json(headers: Headers) -> None:",
            "    \"\"\"",
            "    Check that a set of HTTP headers have a Content-Type header, and that it",
            "    is application/json.",
            "",
            "    Args:",
            "        headers: headers to check",
            "",
            "    Raises:",
            "        RequestSendFailed: if the Content-Type header is missing or isn't JSON",
            "",
            "    \"\"\"",
            "    c_type = headers.getRawHeaders(b\"Content-Type\")",
            "    if c_type is None:",
            "        raise RequestSendFailed(",
            "            RuntimeError(\"No Content-Type header received from remote server\"),",
            "            can_retry=False,",
            "        )",
            "",
            "    c_type = c_type[0].decode(\"ascii\")  # only the first header",
            "    val, options = cgi.parse_header(c_type)",
            "    if val != \"application/json\":",
            "        raise RequestSendFailed(",
            "            RuntimeError(",
            "                \"Remote server sent Content-Type header of '%s', not 'application/json'\"",
            "                % c_type,",
            "            ),",
            "            can_retry=False,",
            "        )"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import cgi",
            "import logging",
            "import random",
            "import sys",
            "import urllib.parse",
            "from io import BytesIO",
            "from typing import Callable, Dict, List, Optional, Tuple, Union",
            "",
            "import attr",
            "import treq",
            "from canonicaljson import encode_canonical_json",
            "from prometheus_client import Counter",
            "from signedjson.sign import sign_json",
            "",
            "from twisted.internet import defer",
            "from twisted.internet.error import DNSLookupError",
            "from twisted.internet.interfaces import IReactorTime",
            "from twisted.internet.task import _EPSILON, Cooperator",
            "from twisted.web.http_headers import Headers",
            "from twisted.web.iweb import IBodyProducer, IResponse",
            "",
            "import synapse.metrics",
            "import synapse.util.retryutils",
            "from synapse.api.errors import (",
            "    FederationDeniedError,",
            "    HttpResponseException,",
            "    RequestSendFailed,",
            ")",
            "from synapse.http import QuieterFileBodyProducer",
            "from synapse.http.client import (",
            "    BlacklistingAgentWrapper,",
            "    BlacklistingReactorWrapper,",
            "    encode_query_args,",
            "    readBodyToFile,",
            ")",
            "from synapse.http.federation.matrix_federation_agent import MatrixFederationAgent",
            "from synapse.logging.context import make_deferred_yieldable",
            "from synapse.logging.opentracing import (",
            "    inject_active_span_byte_dict,",
            "    set_tag,",
            "    start_active_span,",
            "    tags,",
            ")",
            "from synapse.types import JsonDict",
            "from synapse.util import json_decoder",
            "from synapse.util.async_helpers import timeout_deferred",
            "from synapse.util.metrics import Measure",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "outgoing_requests_counter = Counter(",
            "    \"synapse_http_matrixfederationclient_requests\", \"\", [\"method\"]",
            ")",
            "incoming_responses_counter = Counter(",
            "    \"synapse_http_matrixfederationclient_responses\", \"\", [\"method\", \"code\"]",
            ")",
            "",
            "",
            "MAX_LONG_RETRIES = 10",
            "MAX_SHORT_RETRIES = 3",
            "MAXINT = sys.maxsize",
            "",
            "",
            "_next_id = 1",
            "",
            "",
            "QueryArgs = Dict[str, Union[str, List[str]]]",
            "",
            "",
            "@attr.s(slots=True, frozen=True)",
            "class MatrixFederationRequest:",
            "    method = attr.ib(type=str)",
            "    \"\"\"HTTP method",
            "    \"\"\"",
            "",
            "    path = attr.ib(type=str)",
            "    \"\"\"HTTP path",
            "    \"\"\"",
            "",
            "    destination = attr.ib(type=str)",
            "    \"\"\"The remote server to send the HTTP request to.",
            "    \"\"\"",
            "",
            "    json = attr.ib(default=None, type=Optional[JsonDict])",
            "    \"\"\"JSON to send in the body.",
            "    \"\"\"",
            "",
            "    json_callback = attr.ib(default=None, type=Optional[Callable[[], JsonDict]])",
            "    \"\"\"A callback to generate the JSON.",
            "    \"\"\"",
            "",
            "    query = attr.ib(default=None, type=Optional[dict])",
            "    \"\"\"Query arguments.",
            "    \"\"\"",
            "",
            "    txn_id = attr.ib(default=None, type=Optional[str])",
            "    \"\"\"Unique ID for this request (for logging)",
            "    \"\"\"",
            "",
            "    uri = attr.ib(init=False, type=bytes)",
            "    \"\"\"The URI of this request",
            "    \"\"\"",
            "",
            "    def __attrs_post_init__(self) -> None:",
            "        global _next_id",
            "        txn_id = \"%s-O-%s\" % (self.method, _next_id)",
            "        _next_id = (_next_id + 1) % (MAXINT - 1)",
            "",
            "        object.__setattr__(self, \"txn_id\", txn_id)",
            "",
            "        destination_bytes = self.destination.encode(\"ascii\")",
            "        path_bytes = self.path.encode(\"ascii\")",
            "        if self.query:",
            "            query_bytes = encode_query_args(self.query)",
            "        else:",
            "            query_bytes = b\"\"",
            "",
            "        # The object is frozen so we can pre-compute this.",
            "        uri = urllib.parse.urlunparse(",
            "            (b\"matrix\", destination_bytes, path_bytes, None, query_bytes, b\"\")",
            "        )",
            "        object.__setattr__(self, \"uri\", uri)",
            "",
            "    def get_json(self) -> Optional[JsonDict]:",
            "        if self.json_callback:",
            "            return self.json_callback()",
            "        return self.json",
            "",
            "",
            "async def _handle_json_response(",
            "    reactor: IReactorTime,",
            "    timeout_sec: float,",
            "    request: MatrixFederationRequest,",
            "    response: IResponse,",
            "    start_ms: int,",
            ") -> JsonDict:",
            "    \"\"\"",
            "    Reads the JSON body of a response, with a timeout",
            "",
            "    Args:",
            "        reactor: twisted reactor, for the timeout",
            "        timeout_sec: number of seconds to wait for response to complete",
            "        request: the request that triggered the response",
            "        response: response to the request",
            "        start_ms: Timestamp when request was made",
            "",
            "    Returns:",
            "        The parsed JSON response",
            "    \"\"\"",
            "    try:",
            "        check_content_type_is_json(response.headers)",
            "",
            "        # Use the custom JSON decoder (partially re-implements treq.json_content).",
            "        d = treq.text_content(response, encoding=\"utf-8\")",
            "        d.addCallback(json_decoder.decode)",
            "        d = timeout_deferred(d, timeout=timeout_sec, reactor=reactor)",
            "",
            "        body = await make_deferred_yieldable(d)",
            "    except defer.TimeoutError as e:",
            "        logger.warning(",
            "            \"{%s} [%s] Timed out reading response - %s %s\",",
            "            request.txn_id,",
            "            request.destination,",
            "            request.method,",
            "            request.uri.decode(\"ascii\"),",
            "        )",
            "        raise RequestSendFailed(e, can_retry=True) from e",
            "    except Exception as e:",
            "        logger.warning(",
            "            \"{%s} [%s] Error reading response %s %s: %s\",",
            "            request.txn_id,",
            "            request.destination,",
            "            request.method,",
            "            request.uri.decode(\"ascii\"),",
            "            e,",
            "        )",
            "        raise",
            "",
            "    time_taken_secs = reactor.seconds() - start_ms / 1000",
            "",
            "    logger.info(",
            "        \"{%s} [%s] Completed request: %d %s in %.2f secs - %s %s\",",
            "        request.txn_id,",
            "        request.destination,",
            "        response.code,",
            "        response.phrase.decode(\"ascii\", errors=\"replace\"),",
            "        time_taken_secs,",
            "        request.method,",
            "        request.uri.decode(\"ascii\"),",
            "    )",
            "    return body",
            "",
            "",
            "class MatrixFederationHttpClient:",
            "    \"\"\"HTTP client used to talk to other homeservers over the federation",
            "    protocol. Send client certificates and signs requests.",
            "",
            "    Attributes:",
            "        agent (twisted.web.client.Agent): The twisted Agent used to send the",
            "            requests.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs, tls_client_options_factory):",
            "        self.hs = hs",
            "        self.signing_key = hs.signing_key",
            "        self.server_name = hs.hostname",
            "",
            "        # We need to use a DNS resolver which filters out blacklisted IP",
            "        # addresses, to prevent DNS rebinding.",
            "        self.reactor = BlacklistingReactorWrapper(",
            "            hs.get_reactor(), None, hs.config.federation_ip_range_blacklist",
            "        )",
            "",
            "        user_agent = hs.version_string",
            "        if hs.config.user_agent_suffix:",
            "            user_agent = \"%s %s\" % (user_agent, hs.config.user_agent_suffix)",
            "        user_agent = user_agent.encode(\"ascii\")",
            "",
            "        self.agent = MatrixFederationAgent(",
            "            self.reactor,",
            "            tls_client_options_factory,",
            "            user_agent,",
            "            hs.config.federation_ip_range_blacklist,",
            "        )",
            "",
            "        # Use a BlacklistingAgentWrapper to prevent circumventing the IP",
            "        # blacklist via IP literals in server names",
            "        self.agent = BlacklistingAgentWrapper(",
            "            self.agent, ip_blacklist=hs.config.federation_ip_range_blacklist,",
            "        )",
            "",
            "        self.clock = hs.get_clock()",
            "        self._store = hs.get_datastore()",
            "        self.version_string_bytes = hs.version_string.encode(\"ascii\")",
            "        self.default_timeout = 60",
            "",
            "        def schedule(x):",
            "            self.reactor.callLater(_EPSILON, x)",
            "",
            "        self._cooperator = Cooperator(scheduler=schedule)",
            "",
            "    async def _send_request_with_optional_trailing_slash(",
            "        self,",
            "        request: MatrixFederationRequest,",
            "        try_trailing_slash_on_400: bool = False,",
            "        **send_request_args",
            "    ) -> IResponse:",
            "        \"\"\"Wrapper for _send_request which can optionally retry the request",
            "        upon receiving a combination of a 400 HTTP response code and a",
            "        'M_UNRECOGNIZED' errcode. This is a workaround for Synapse <= v0.99.3",
            "        due to #3622.",
            "",
            "        Args:",
            "            request: details of request to be sent",
            "            try_trailing_slash_on_400: Whether on receiving a 400",
            "                'M_UNRECOGNIZED' from the server to retry the request with a",
            "                trailing slash appended to the request path.",
            "            send_request_args: A dictionary of arguments to pass to `_send_request()`.",
            "",
            "        Raises:",
            "            HttpResponseException: If we get an HTTP response code >= 300",
            "                (except 429).",
            "",
            "        Returns:",
            "            Parsed JSON response body.",
            "        \"\"\"",
            "        try:",
            "            response = await self._send_request(request, **send_request_args)",
            "        except HttpResponseException as e:",
            "            # Received an HTTP error > 300. Check if it meets the requirements",
            "            # to retry with a trailing slash",
            "            if not try_trailing_slash_on_400:",
            "                raise",
            "",
            "            if e.code != 400 or e.to_synapse_error().errcode != \"M_UNRECOGNIZED\":",
            "                raise",
            "",
            "            # Retry with a trailing slash if we received a 400 with",
            "            # 'M_UNRECOGNIZED' which some endpoints can return when omitting a",
            "            # trailing slash on Synapse <= v0.99.3.",
            "            logger.info(\"Retrying request with trailing slash\")",
            "",
            "            # Request is frozen so we create a new instance",
            "            request = attr.evolve(request, path=request.path + \"/\")",
            "",
            "            response = await self._send_request(request, **send_request_args)",
            "",
            "        return response",
            "",
            "    async def _send_request(",
            "        self,",
            "        request: MatrixFederationRequest,",
            "        retry_on_dns_fail: bool = True,",
            "        timeout: Optional[int] = None,",
            "        long_retries: bool = False,",
            "        ignore_backoff: bool = False,",
            "        backoff_on_404: bool = False,",
            "    ) -> IResponse:",
            "        \"\"\"",
            "        Sends a request to the given server.",
            "",
            "        Args:",
            "            request: details of request to be sent",
            "",
            "            retry_on_dns_fail: true if the request should be retied on DNS failures",
            "",
            "            timeout: number of milliseconds to wait for the response headers",
            "                (including connecting to the server), *for each attempt*.",
            "                60s by default.",
            "",
            "            long_retries: whether to use the long retry algorithm.",
            "",
            "                The regular retry algorithm makes 4 attempts, with intervals",
            "                [0.5s, 1s, 2s].",
            "",
            "                The long retry algorithm makes 11 attempts, with intervals",
            "                [4s, 16s, 60s, 60s, ...]",
            "",
            "                Both algorithms add -20%/+40% jitter to the retry intervals.",
            "",
            "                Note that the above intervals are *in addition* to the time spent",
            "                waiting for the request to complete (up to `timeout` ms).",
            "",
            "                NB: the long retry algorithm takes over 20 minutes to complete, with",
            "                a default timeout of 60s!",
            "",
            "            ignore_backoff: true to ignore the historical backoff data",
            "                and try the request anyway.",
            "",
            "            backoff_on_404: Back off if we get a 404",
            "",
            "        Returns:",
            "            Resolves with the HTTP response object on success.",
            "",
            "        Raises:",
            "            HttpResponseException: If we get an HTTP response code >= 300",
            "                (except 429).",
            "            NotRetryingDestination: If we are not yet ready to retry this",
            "                server.",
            "            FederationDeniedError: If this destination  is not on our",
            "                federation whitelist",
            "            RequestSendFailed: If there were problems connecting to the",
            "                remote, due to e.g. DNS failures, connection timeouts etc.",
            "        \"\"\"",
            "        if timeout:",
            "            _sec_timeout = timeout / 1000",
            "        else:",
            "            _sec_timeout = self.default_timeout",
            "",
            "        if (",
            "            self.hs.config.federation_domain_whitelist is not None",
            "            and request.destination not in self.hs.config.federation_domain_whitelist",
            "        ):",
            "            raise FederationDeniedError(request.destination)",
            "",
            "        limiter = await synapse.util.retryutils.get_retry_limiter(",
            "            request.destination,",
            "            self.clock,",
            "            self._store,",
            "            backoff_on_404=backoff_on_404,",
            "            ignore_backoff=ignore_backoff,",
            "        )",
            "",
            "        method_bytes = request.method.encode(\"ascii\")",
            "        destination_bytes = request.destination.encode(\"ascii\")",
            "        path_bytes = request.path.encode(\"ascii\")",
            "        if request.query:",
            "            query_bytes = encode_query_args(request.query)",
            "        else:",
            "            query_bytes = b\"\"",
            "",
            "        scope = start_active_span(",
            "            \"outgoing-federation-request\",",
            "            tags={",
            "                tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,",
            "                tags.PEER_ADDRESS: request.destination,",
            "                tags.HTTP_METHOD: request.method,",
            "                tags.HTTP_URL: request.path,",
            "            },",
            "            finish_on_close=True,",
            "        )",
            "",
            "        # Inject the span into the headers",
            "        headers_dict = {}  # type: Dict[bytes, List[bytes]]",
            "        inject_active_span_byte_dict(headers_dict, request.destination)",
            "",
            "        headers_dict[b\"User-Agent\"] = [self.version_string_bytes]",
            "",
            "        with limiter, scope:",
            "            # XXX: Would be much nicer to retry only at the transaction-layer",
            "            # (once we have reliable transactions in place)",
            "            if long_retries:",
            "                retries_left = MAX_LONG_RETRIES",
            "            else:",
            "                retries_left = MAX_SHORT_RETRIES",
            "",
            "            url_bytes = request.uri",
            "            url_str = url_bytes.decode(\"ascii\")",
            "",
            "            url_to_sign_bytes = urllib.parse.urlunparse(",
            "                (b\"\", b\"\", path_bytes, None, query_bytes, b\"\")",
            "            )",
            "",
            "            while True:",
            "                try:",
            "                    json = request.get_json()",
            "                    if json:",
            "                        headers_dict[b\"Content-Type\"] = [b\"application/json\"]",
            "                        auth_headers = self.build_auth_headers(",
            "                            destination_bytes, method_bytes, url_to_sign_bytes, json",
            "                        )",
            "                        data = encode_canonical_json(json)",
            "                        producer = QuieterFileBodyProducer(",
            "                            BytesIO(data), cooperator=self._cooperator",
            "                        )  # type: Optional[IBodyProducer]",
            "                    else:",
            "                        producer = None",
            "                        auth_headers = self.build_auth_headers(",
            "                            destination_bytes, method_bytes, url_to_sign_bytes",
            "                        )",
            "",
            "                    headers_dict[b\"Authorization\"] = auth_headers",
            "",
            "                    logger.debug(",
            "                        \"{%s} [%s] Sending request: %s %s; timeout %fs\",",
            "                        request.txn_id,",
            "                        request.destination,",
            "                        request.method,",
            "                        url_str,",
            "                        _sec_timeout,",
            "                    )",
            "",
            "                    outgoing_requests_counter.labels(request.method).inc()",
            "",
            "                    try:",
            "                        with Measure(self.clock, \"outbound_request\"):",
            "                            # we don't want all the fancy cookie and redirect handling",
            "                            # that treq.request gives: just use the raw Agent.",
            "                            request_deferred = self.agent.request(",
            "                                method_bytes,",
            "                                url_bytes,",
            "                                headers=Headers(headers_dict),",
            "                                bodyProducer=producer,",
            "                            )",
            "",
            "                            request_deferred = timeout_deferred(",
            "                                request_deferred,",
            "                                timeout=_sec_timeout,",
            "                                reactor=self.reactor,",
            "                            )",
            "",
            "                            response = await request_deferred",
            "                    except DNSLookupError as e:",
            "                        raise RequestSendFailed(e, can_retry=retry_on_dns_fail) from e",
            "                    except Exception as e:",
            "                        raise RequestSendFailed(e, can_retry=True) from e",
            "",
            "                    incoming_responses_counter.labels(",
            "                        request.method, response.code",
            "                    ).inc()",
            "",
            "                    set_tag(tags.HTTP_STATUS_CODE, response.code)",
            "                    response_phrase = response.phrase.decode(\"ascii\", errors=\"replace\")",
            "",
            "                    if 200 <= response.code < 300:",
            "                        logger.debug(",
            "                            \"{%s} [%s] Got response headers: %d %s\",",
            "                            request.txn_id,",
            "                            request.destination,",
            "                            response.code,",
            "                            response_phrase,",
            "                        )",
            "                        pass",
            "                    else:",
            "                        logger.info(",
            "                            \"{%s} [%s] Got response headers: %d %s\",",
            "                            request.txn_id,",
            "                            request.destination,",
            "                            response.code,",
            "                            response_phrase,",
            "                        )",
            "                        # :'(",
            "                        # Update transactions table?",
            "                        d = treq.content(response)",
            "                        d = timeout_deferred(",
            "                            d, timeout=_sec_timeout, reactor=self.reactor",
            "                        )",
            "",
            "                        try:",
            "                            body = await make_deferred_yieldable(d)",
            "                        except Exception as e:",
            "                            # Eh, we're already going to raise an exception so lets",
            "                            # ignore if this fails.",
            "                            logger.warning(",
            "                                \"{%s} [%s] Failed to get error response: %s %s: %s\",",
            "                                request.txn_id,",
            "                                request.destination,",
            "                                request.method,",
            "                                url_str,",
            "                                _flatten_response_never_received(e),",
            "                            )",
            "                            body = None",
            "",
            "                        exc = HttpResponseException(",
            "                            response.code, response_phrase, body",
            "                        )",
            "",
            "                        # Retry if the error is a 429 (Too Many Requests),",
            "                        # otherwise just raise a standard HttpResponseException",
            "                        if response.code == 429:",
            "                            raise RequestSendFailed(exc, can_retry=True) from exc",
            "                        else:",
            "                            raise exc",
            "",
            "                    break",
            "                except RequestSendFailed as e:",
            "                    logger.info(",
            "                        \"{%s} [%s] Request failed: %s %s: %s\",",
            "                        request.txn_id,",
            "                        request.destination,",
            "                        request.method,",
            "                        url_str,",
            "                        _flatten_response_never_received(e.inner_exception),",
            "                    )",
            "",
            "                    if not e.can_retry:",
            "                        raise",
            "",
            "                    if retries_left and not timeout:",
            "                        if long_retries:",
            "                            delay = 4 ** (MAX_LONG_RETRIES + 1 - retries_left)",
            "                            delay = min(delay, 60)",
            "                            delay *= random.uniform(0.8, 1.4)",
            "                        else:",
            "                            delay = 0.5 * 2 ** (MAX_SHORT_RETRIES - retries_left)",
            "                            delay = min(delay, 2)",
            "                            delay *= random.uniform(0.8, 1.4)",
            "",
            "                        logger.debug(",
            "                            \"{%s} [%s] Waiting %ss before re-sending...\",",
            "                            request.txn_id,",
            "                            request.destination,",
            "                            delay,",
            "                        )",
            "",
            "                        await self.clock.sleep(delay)",
            "                        retries_left -= 1",
            "                    else:",
            "                        raise",
            "",
            "                except Exception as e:",
            "                    logger.warning(",
            "                        \"{%s} [%s] Request failed: %s %s: %s\",",
            "                        request.txn_id,",
            "                        request.destination,",
            "                        request.method,",
            "                        url_str,",
            "                        _flatten_response_never_received(e),",
            "                    )",
            "                    raise",
            "        return response",
            "",
            "    def build_auth_headers(",
            "        self,",
            "        destination: Optional[bytes],",
            "        method: bytes,",
            "        url_bytes: bytes,",
            "        content: Optional[JsonDict] = None,",
            "        destination_is: Optional[bytes] = None,",
            "    ) -> List[bytes]:",
            "        \"\"\"",
            "        Builds the Authorization headers for a federation request",
            "        Args:",
            "            destination: The destination homeserver of the request.",
            "                May be None if the destination is an identity server, in which case",
            "                destination_is must be non-None.",
            "            method: The HTTP method of the request",
            "            url_bytes: The URI path of the request",
            "            content: The body of the request",
            "            destination_is: As 'destination', but if the destination is an",
            "                identity server",
            "",
            "        Returns:",
            "            A list of headers to be added as \"Authorization:\" headers",
            "        \"\"\"",
            "        request = {",
            "            \"method\": method.decode(\"ascii\"),",
            "            \"uri\": url_bytes.decode(\"ascii\"),",
            "            \"origin\": self.server_name,",
            "        }",
            "",
            "        if destination is not None:",
            "            request[\"destination\"] = destination.decode(\"ascii\")",
            "",
            "        if destination_is is not None:",
            "            request[\"destination_is\"] = destination_is.decode(\"ascii\")",
            "",
            "        if content is not None:",
            "            request[\"content\"] = content",
            "",
            "        request = sign_json(request, self.server_name, self.signing_key)",
            "",
            "        auth_headers = []",
            "",
            "        for key, sig in request[\"signatures\"][self.server_name].items():",
            "            auth_headers.append(",
            "                (",
            "                    'X-Matrix origin=%s,key=\"%s\",sig=\"%s\"'",
            "                    % (self.server_name, key, sig)",
            "                ).encode(\"ascii\")",
            "            )",
            "        return auth_headers",
            "",
            "    async def put_json(",
            "        self,",
            "        destination: str,",
            "        path: str,",
            "        args: Optional[QueryArgs] = None,",
            "        data: Optional[JsonDict] = None,",
            "        json_data_callback: Optional[Callable[[], JsonDict]] = None,",
            "        long_retries: bool = False,",
            "        timeout: Optional[int] = None,",
            "        ignore_backoff: bool = False,",
            "        backoff_on_404: bool = False,",
            "        try_trailing_slash_on_400: bool = False,",
            "    ) -> Union[JsonDict, list]:",
            "        \"\"\" Sends the specified json data using PUT",
            "",
            "        Args:",
            "            destination: The remote server to send the HTTP request to.",
            "            path: The HTTP path.",
            "            args: query params",
            "            data: A dict containing the data that will be used as",
            "                the request body. This will be encoded as JSON.",
            "            json_data_callback: A callable returning the dict to",
            "                use as the request body.",
            "",
            "            long_retries: whether to use the long retry algorithm. See",
            "                docs on _send_request for details.",
            "",
            "            timeout: number of milliseconds to wait for the response.",
            "                self._default_timeout (60s) by default.",
            "",
            "                Note that we may make several attempts to send the request; this",
            "                timeout applies to the time spent waiting for response headers for",
            "                *each* attempt (including connection time) as well as the time spent",
            "                reading the response body after a 200 response.",
            "",
            "            ignore_backoff: true to ignore the historical backoff data",
            "                and try the request anyway.",
            "            backoff_on_404: True if we should count a 404 response as",
            "                a failure of the server (and should therefore back off future",
            "                requests).",
            "            try_trailing_slash_on_400: True if on a 400 M_UNRECOGNIZED",
            "                response we should try appending a trailing slash to the end",
            "                of the request. Workaround for #3622 in Synapse <= v0.99.3. This",
            "                will be attempted before backing off if backing off has been",
            "                enabled.",
            "",
            "        Returns:",
            "            Succeeds when we get a 2xx HTTP response. The",
            "            result will be the decoded JSON body.",
            "",
            "        Raises:",
            "            HttpResponseException: If we get an HTTP response code >= 300",
            "                (except 429).",
            "            NotRetryingDestination: If we are not yet ready to retry this",
            "                server.",
            "            FederationDeniedError: If this destination  is not on our",
            "                federation whitelist",
            "            RequestSendFailed: If there were problems connecting to the",
            "                remote, due to e.g. DNS failures, connection timeouts etc.",
            "        \"\"\"",
            "        request = MatrixFederationRequest(",
            "            method=\"PUT\",",
            "            destination=destination,",
            "            path=path,",
            "            query=args,",
            "            json_callback=json_data_callback,",
            "            json=data,",
            "        )",
            "",
            "        start_ms = self.clock.time_msec()",
            "",
            "        response = await self._send_request_with_optional_trailing_slash(",
            "            request,",
            "            try_trailing_slash_on_400,",
            "            backoff_on_404=backoff_on_404,",
            "            ignore_backoff=ignore_backoff,",
            "            long_retries=long_retries,",
            "            timeout=timeout,",
            "        )",
            "",
            "        if timeout is not None:",
            "            _sec_timeout = timeout / 1000",
            "        else:",
            "            _sec_timeout = self.default_timeout",
            "",
            "        body = await _handle_json_response(",
            "            self.reactor, _sec_timeout, request, response, start_ms",
            "        )",
            "",
            "        return body",
            "",
            "    async def post_json(",
            "        self,",
            "        destination: str,",
            "        path: str,",
            "        data: Optional[JsonDict] = None,",
            "        long_retries: bool = False,",
            "        timeout: Optional[int] = None,",
            "        ignore_backoff: bool = False,",
            "        args: Optional[QueryArgs] = None,",
            "    ) -> Union[JsonDict, list]:",
            "        \"\"\" Sends the specified json data using POST",
            "",
            "        Args:",
            "            destination: The remote server to send the HTTP request to.",
            "",
            "            path: The HTTP path.",
            "",
            "            data: A dict containing the data that will be used as",
            "                the request body. This will be encoded as JSON.",
            "",
            "            long_retries: whether to use the long retry algorithm. See",
            "                docs on _send_request for details.",
            "",
            "            timeout: number of milliseconds to wait for the response.",
            "                self._default_timeout (60s) by default.",
            "",
            "                Note that we may make several attempts to send the request; this",
            "                timeout applies to the time spent waiting for response headers for",
            "                *each* attempt (including connection time) as well as the time spent",
            "                reading the response body after a 200 response.",
            "",
            "            ignore_backoff: true to ignore the historical backoff data and",
            "                try the request anyway.",
            "",
            "            args: query params",
            "        Returns:",
            "            dict|list: Succeeds when we get a 2xx HTTP response. The",
            "            result will be the decoded JSON body.",
            "",
            "        Raises:",
            "            HttpResponseException: If we get an HTTP response code >= 300",
            "                (except 429).",
            "            NotRetryingDestination: If we are not yet ready to retry this",
            "                server.",
            "            FederationDeniedError: If this destination  is not on our",
            "                federation whitelist",
            "            RequestSendFailed: If there were problems connecting to the",
            "                remote, due to e.g. DNS failures, connection timeouts etc.",
            "        \"\"\"",
            "",
            "        request = MatrixFederationRequest(",
            "            method=\"POST\", destination=destination, path=path, query=args, json=data",
            "        )",
            "",
            "        start_ms = self.clock.time_msec()",
            "",
            "        response = await self._send_request(",
            "            request,",
            "            long_retries=long_retries,",
            "            timeout=timeout,",
            "            ignore_backoff=ignore_backoff,",
            "        )",
            "",
            "        if timeout:",
            "            _sec_timeout = timeout / 1000",
            "        else:",
            "            _sec_timeout = self.default_timeout",
            "",
            "        body = await _handle_json_response(",
            "            self.reactor, _sec_timeout, request, response, start_ms,",
            "        )",
            "        return body",
            "",
            "    async def get_json(",
            "        self,",
            "        destination: str,",
            "        path: str,",
            "        args: Optional[QueryArgs] = None,",
            "        retry_on_dns_fail: bool = True,",
            "        timeout: Optional[int] = None,",
            "        ignore_backoff: bool = False,",
            "        try_trailing_slash_on_400: bool = False,",
            "    ) -> Union[JsonDict, list]:",
            "        \"\"\" GETs some json from the given host homeserver and path",
            "",
            "        Args:",
            "            destination: The remote server to send the HTTP request to.",
            "",
            "            path: The HTTP path.",
            "",
            "            args: A dictionary used to create query strings, defaults to",
            "                None.",
            "",
            "            timeout: number of milliseconds to wait for the response.",
            "                self._default_timeout (60s) by default.",
            "",
            "                Note that we may make several attempts to send the request; this",
            "                timeout applies to the time spent waiting for response headers for",
            "                *each* attempt (including connection time) as well as the time spent",
            "                reading the response body after a 200 response.",
            "",
            "            ignore_backoff: true to ignore the historical backoff data",
            "                and try the request anyway.",
            "",
            "            try_trailing_slash_on_400: True if on a 400 M_UNRECOGNIZED",
            "                response we should try appending a trailing slash to the end of",
            "                the request. Workaround for #3622 in Synapse <= v0.99.3.",
            "        Returns:",
            "            Succeeds when we get a 2xx HTTP response. The",
            "            result will be the decoded JSON body.",
            "",
            "        Raises:",
            "            HttpResponseException: If we get an HTTP response code >= 300",
            "                (except 429).",
            "            NotRetryingDestination: If we are not yet ready to retry this",
            "                server.",
            "            FederationDeniedError: If this destination  is not on our",
            "                federation whitelist",
            "            RequestSendFailed: If there were problems connecting to the",
            "                remote, due to e.g. DNS failures, connection timeouts etc.",
            "        \"\"\"",
            "        request = MatrixFederationRequest(",
            "            method=\"GET\", destination=destination, path=path, query=args",
            "        )",
            "",
            "        start_ms = self.clock.time_msec()",
            "",
            "        response = await self._send_request_with_optional_trailing_slash(",
            "            request,",
            "            try_trailing_slash_on_400,",
            "            backoff_on_404=False,",
            "            ignore_backoff=ignore_backoff,",
            "            retry_on_dns_fail=retry_on_dns_fail,",
            "            timeout=timeout,",
            "        )",
            "",
            "        if timeout is not None:",
            "            _sec_timeout = timeout / 1000",
            "        else:",
            "            _sec_timeout = self.default_timeout",
            "",
            "        body = await _handle_json_response(",
            "            self.reactor, _sec_timeout, request, response, start_ms",
            "        )",
            "",
            "        return body",
            "",
            "    async def delete_json(",
            "        self,",
            "        destination: str,",
            "        path: str,",
            "        long_retries: bool = False,",
            "        timeout: Optional[int] = None,",
            "        ignore_backoff: bool = False,",
            "        args: Optional[QueryArgs] = None,",
            "    ) -> Union[JsonDict, list]:",
            "        \"\"\"Send a DELETE request to the remote expecting some json response",
            "",
            "        Args:",
            "            destination: The remote server to send the HTTP request to.",
            "            path: The HTTP path.",
            "",
            "            long_retries: whether to use the long retry algorithm. See",
            "                docs on _send_request for details.",
            "",
            "            timeout: number of milliseconds to wait for the response.",
            "                self._default_timeout (60s) by default.",
            "",
            "                Note that we may make several attempts to send the request; this",
            "                timeout applies to the time spent waiting for response headers for",
            "                *each* attempt (including connection time) as well as the time spent",
            "                reading the response body after a 200 response.",
            "",
            "            ignore_backoff: true to ignore the historical backoff data and",
            "                try the request anyway.",
            "",
            "            args: query params",
            "        Returns:",
            "            Succeeds when we get a 2xx HTTP response. The",
            "            result will be the decoded JSON body.",
            "",
            "        Raises:",
            "            HttpResponseException: If we get an HTTP response code >= 300",
            "                (except 429).",
            "            NotRetryingDestination: If we are not yet ready to retry this",
            "                server.",
            "            FederationDeniedError: If this destination  is not on our",
            "                federation whitelist",
            "            RequestSendFailed: If there were problems connecting to the",
            "                remote, due to e.g. DNS failures, connection timeouts etc.",
            "        \"\"\"",
            "        request = MatrixFederationRequest(",
            "            method=\"DELETE\", destination=destination, path=path, query=args",
            "        )",
            "",
            "        start_ms = self.clock.time_msec()",
            "",
            "        response = await self._send_request(",
            "            request,",
            "            long_retries=long_retries,",
            "            timeout=timeout,",
            "            ignore_backoff=ignore_backoff,",
            "        )",
            "",
            "        if timeout is not None:",
            "            _sec_timeout = timeout / 1000",
            "        else:",
            "            _sec_timeout = self.default_timeout",
            "",
            "        body = await _handle_json_response(",
            "            self.reactor, _sec_timeout, request, response, start_ms",
            "        )",
            "        return body",
            "",
            "    async def get_file(",
            "        self,",
            "        destination: str,",
            "        path: str,",
            "        output_stream,",
            "        args: Optional[QueryArgs] = None,",
            "        retry_on_dns_fail: bool = True,",
            "        max_size: Optional[int] = None,",
            "        ignore_backoff: bool = False,",
            "    ) -> Tuple[int, Dict[bytes, List[bytes]]]:",
            "        \"\"\"GETs a file from a given homeserver",
            "        Args:",
            "            destination: The remote server to send the HTTP request to.",
            "            path: The HTTP path to GET.",
            "            output_stream: File to write the response body to.",
            "            args: Optional dictionary used to create the query string.",
            "            ignore_backoff: true to ignore the historical backoff data",
            "                and try the request anyway.",
            "",
            "        Returns:",
            "            Resolves with an (int,dict) tuple of",
            "            the file length and a dict of the response headers.",
            "",
            "        Raises:",
            "            HttpResponseException: If we get an HTTP response code >= 300",
            "                (except 429).",
            "            NotRetryingDestination: If we are not yet ready to retry this",
            "                server.",
            "            FederationDeniedError: If this destination  is not on our",
            "                federation whitelist",
            "            RequestSendFailed: If there were problems connecting to the",
            "                remote, due to e.g. DNS failures, connection timeouts etc.",
            "        \"\"\"",
            "        request = MatrixFederationRequest(",
            "            method=\"GET\", destination=destination, path=path, query=args",
            "        )",
            "",
            "        response = await self._send_request(",
            "            request, retry_on_dns_fail=retry_on_dns_fail, ignore_backoff=ignore_backoff",
            "        )",
            "",
            "        headers = dict(response.headers.getAllRawHeaders())",
            "",
            "        try:",
            "            d = readBodyToFile(response, output_stream, max_size)",
            "            d.addTimeout(self.default_timeout, self.reactor)",
            "            length = await make_deferred_yieldable(d)",
            "        except Exception as e:",
            "            logger.warning(",
            "                \"{%s} [%s] Error reading response: %s\",",
            "                request.txn_id,",
            "                request.destination,",
            "                e,",
            "            )",
            "            raise",
            "        logger.info(",
            "            \"{%s} [%s] Completed: %d %s [%d bytes] %s %s\",",
            "            request.txn_id,",
            "            request.destination,",
            "            response.code,",
            "            response.phrase.decode(\"ascii\", errors=\"replace\"),",
            "            length,",
            "            request.method,",
            "            request.uri.decode(\"ascii\"),",
            "        )",
            "        return (length, headers)",
            "",
            "",
            "def _flatten_response_never_received(e):",
            "    if hasattr(e, \"reasons\"):",
            "        reasons = \", \".join(",
            "            _flatten_response_never_received(f.value) for f in e.reasons",
            "        )",
            "",
            "        return \"%s:[%s]\" % (type(e).__name__, reasons)",
            "    else:",
            "        return repr(e)",
            "",
            "",
            "def check_content_type_is_json(headers: Headers) -> None:",
            "    \"\"\"",
            "    Check that a set of HTTP headers have a Content-Type header, and that it",
            "    is application/json.",
            "",
            "    Args:",
            "        headers: headers to check",
            "",
            "    Raises:",
            "        RequestSendFailed: if the Content-Type header is missing or isn't JSON",
            "",
            "    \"\"\"",
            "    c_type = headers.getRawHeaders(b\"Content-Type\")",
            "    if c_type is None:",
            "        raise RequestSendFailed(",
            "            RuntimeError(\"No Content-Type header received from remote server\"),",
            "            can_retry=False,",
            "        )",
            "",
            "    c_type = c_type[0].decode(\"ascii\")  # only the first header",
            "    val, options = cgi.parse_header(c_type)",
            "    if val != \"application/json\":",
            "        raise RequestSendFailed(",
            "            RuntimeError(",
            "                \"Remote server sent Content-Type header of '%s', not 'application/json'\"",
            "                % c_type,",
            "            ),",
            "            can_retry=False,",
            "        )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "29": [],
            "33": [],
            "48": [],
            "224": [
                "MatrixFederationHttpClient",
                "__init__"
            ],
            "225": [
                "MatrixFederationHttpClient",
                "__init__"
            ],
            "228": [
                "MatrixFederationHttpClient",
                "__init__"
            ],
            "229": [
                "MatrixFederationHttpClient",
                "__init__"
            ],
            "232": [
                "MatrixFederationHttpClient",
                "__init__"
            ],
            "233": [
                "MatrixFederationHttpClient",
                "__init__",
                "Reactor"
            ],
            "234": [
                "MatrixFederationHttpClient",
                "__init__",
                "Reactor",
                "__getattr__"
            ],
            "235": [
                "MatrixFederationHttpClient",
                "__init__",
                "Reactor",
                "__getattr__"
            ],
            "236": [
                "MatrixFederationHttpClient",
                "__init__",
                "Reactor",
                "__getattr__"
            ],
            "237": [
                "MatrixFederationHttpClient",
                "__init__",
                "Reactor",
                "__getattr__"
            ],
            "238": [
                "MatrixFederationHttpClient",
                "__init__",
                "Reactor",
                "__getattr__"
            ],
            "239": [
                "MatrixFederationHttpClient",
                "__init__"
            ],
            "240": [
                "MatrixFederationHttpClient",
                "__init__"
            ],
            "241": [
                "MatrixFederationHttpClient",
                "__init__"
            ],
            "248": [
                "MatrixFederationHttpClient",
                "__init__"
            ]
        },
        "addLocation": []
    },
    "synapse/push/httppusher.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": 100,
                "PatchRowcode": "         if \"url\" not in self.data:"
            },
            "1": {
                "beforePatchRowNumber": 101,
                "afterPatchRowNumber": 101,
                "PatchRowcode": "             raise PusherConfigException(\"'url' required in data for HTTP pusher\")"
            },
            "2": {
                "beforePatchRowNumber": 102,
                "afterPatchRowNumber": 102,
                "PatchRowcode": "         self.url = self.data[\"url\"]"
            },
            "3": {
                "beforePatchRowNumber": 103,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.http_client = hs.get_proxied_http_client()"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+        self.http_client = hs.get_proxied_blacklisted_http_client()"
            },
            "5": {
                "beforePatchRowNumber": 104,
                "afterPatchRowNumber": 104,
                "PatchRowcode": "         self.data_minus_url = {}"
            },
            "6": {
                "beforePatchRowNumber": 105,
                "afterPatchRowNumber": 105,
                "PatchRowcode": "         self.data_minus_url.update(self.data)"
            },
            "7": {
                "beforePatchRowNumber": 106,
                "afterPatchRowNumber": 106,
                "PatchRowcode": "         del self.data_minus_url[\"url\"]"
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "# Copyright 2017 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "",
            "from prometheus_client import Counter",
            "",
            "from twisted.internet.error import AlreadyCalled, AlreadyCancelled",
            "",
            "from synapse.api.constants import EventTypes",
            "from synapse.logging import opentracing",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.push import PusherConfigException",
            "from synapse.types import RoomStreamToken",
            "",
            "from . import push_rule_evaluator, push_tools",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "http_push_processed_counter = Counter(",
            "    \"synapse_http_httppusher_http_pushes_processed\",",
            "    \"Number of push notifications successfully sent\",",
            ")",
            "",
            "http_push_failed_counter = Counter(",
            "    \"synapse_http_httppusher_http_pushes_failed\",",
            "    \"Number of push notifications which failed\",",
            ")",
            "",
            "http_badges_processed_counter = Counter(",
            "    \"synapse_http_httppusher_badge_updates_processed\",",
            "    \"Number of badge updates successfully sent\",",
            ")",
            "",
            "http_badges_failed_counter = Counter(",
            "    \"synapse_http_httppusher_badge_updates_failed\",",
            "    \"Number of badge updates which failed\",",
            ")",
            "",
            "",
            "class HttpPusher:",
            "    INITIAL_BACKOFF_SEC = 1  # in seconds because that's what Twisted takes",
            "    MAX_BACKOFF_SEC = 60 * 60",
            "",
            "    # This one's in ms because we compare it against the clock",
            "    GIVE_UP_AFTER_MS = 24 * 60 * 60 * 1000",
            "",
            "    def __init__(self, hs, pusherdict):",
            "        self.hs = hs",
            "        self.store = self.hs.get_datastore()",
            "        self.storage = self.hs.get_storage()",
            "        self.clock = self.hs.get_clock()",
            "        self.state_handler = self.hs.get_state_handler()",
            "        self.user_id = pusherdict[\"user_name\"]",
            "        self.app_id = pusherdict[\"app_id\"]",
            "        self.app_display_name = pusherdict[\"app_display_name\"]",
            "        self.device_display_name = pusherdict[\"device_display_name\"]",
            "        self.pushkey = pusherdict[\"pushkey\"]",
            "        self.pushkey_ts = pusherdict[\"ts\"]",
            "        self.data = pusherdict[\"data\"]",
            "        self.last_stream_ordering = pusherdict[\"last_stream_ordering\"]",
            "        self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC",
            "        self.failing_since = pusherdict[\"failing_since\"]",
            "        self.timed_call = None",
            "        self._is_processing = False",
            "        self._group_unread_count_by_room = hs.config.push_group_unread_count_by_room",
            "",
            "        # This is the highest stream ordering we know it's safe to process.",
            "        # When new events arrive, we'll be given a window of new events: we",
            "        # should honour this rather than just looking for anything higher",
            "        # because of potential out-of-order event serialisation. This starts",
            "        # off as None though as we don't know any better.",
            "        self.max_stream_ordering = None",
            "",
            "        if \"data\" not in pusherdict:",
            "            raise PusherConfigException(\"No 'data' key for HTTP pusher\")",
            "        self.data = pusherdict[\"data\"]",
            "",
            "        self.name = \"%s/%s/%s\" % (",
            "            pusherdict[\"user_name\"],",
            "            pusherdict[\"app_id\"],",
            "            pusherdict[\"pushkey\"],",
            "        )",
            "",
            "        if self.data is None:",
            "            raise PusherConfigException(\"data can not be null for HTTP pusher\")",
            "",
            "        if \"url\" not in self.data:",
            "            raise PusherConfigException(\"'url' required in data for HTTP pusher\")",
            "        self.url = self.data[\"url\"]",
            "        self.http_client = hs.get_proxied_http_client()",
            "        self.data_minus_url = {}",
            "        self.data_minus_url.update(self.data)",
            "        del self.data_minus_url[\"url\"]",
            "",
            "    def on_started(self, should_check_for_notifs):",
            "        \"\"\"Called when this pusher has been started.",
            "",
            "        Args:",
            "            should_check_for_notifs (bool): Whether we should immediately",
            "                check for push to send. Set to False only if it's known there",
            "                is nothing to send",
            "        \"\"\"",
            "        if should_check_for_notifs:",
            "            self._start_processing()",
            "",
            "    def on_new_notifications(self, max_token: RoomStreamToken):",
            "        # We just use the minimum stream ordering and ignore the vector clock",
            "        # component. This is safe to do as long as we *always* ignore the vector",
            "        # clock components.",
            "        max_stream_ordering = max_token.stream",
            "",
            "        self.max_stream_ordering = max(",
            "            max_stream_ordering, self.max_stream_ordering or 0",
            "        )",
            "        self._start_processing()",
            "",
            "    def on_new_receipts(self, min_stream_id, max_stream_id):",
            "        # Note that the min here shouldn't be relied upon to be accurate.",
            "",
            "        # We could check the receipts are actually m.read receipts here,",
            "        # but currently that's the only type of receipt anyway...",
            "        run_as_background_process(\"http_pusher.on_new_receipts\", self._update_badge)",
            "",
            "    async def _update_badge(self):",
            "        # XXX as per https://github.com/matrix-org/matrix-doc/issues/2627, this seems",
            "        # to be largely redundant. perhaps we can remove it.",
            "        badge = await push_tools.get_badge_count(",
            "            self.hs.get_datastore(),",
            "            self.user_id,",
            "            group_by_room=self._group_unread_count_by_room,",
            "        )",
            "        await self._send_badge(badge)",
            "",
            "    def on_timer(self):",
            "        self._start_processing()",
            "",
            "    def on_stop(self):",
            "        if self.timed_call:",
            "            try:",
            "                self.timed_call.cancel()",
            "            except (AlreadyCalled, AlreadyCancelled):",
            "                pass",
            "            self.timed_call = None",
            "",
            "    def _start_processing(self):",
            "        if self._is_processing:",
            "            return",
            "",
            "        run_as_background_process(\"httppush.process\", self._process)",
            "",
            "    async def _process(self):",
            "        # we should never get here if we are already processing",
            "        assert not self._is_processing",
            "",
            "        try:",
            "            self._is_processing = True",
            "            # if the max ordering changes while we're running _unsafe_process,",
            "            # call it again, and so on until we've caught up.",
            "            while True:",
            "                starting_max_ordering = self.max_stream_ordering",
            "                try:",
            "                    await self._unsafe_process()",
            "                except Exception:",
            "                    logger.exception(\"Exception processing notifs\")",
            "                if self.max_stream_ordering == starting_max_ordering:",
            "                    break",
            "        finally:",
            "            self._is_processing = False",
            "",
            "    async def _unsafe_process(self):",
            "        \"\"\"",
            "        Looks for unset notifications and dispatch them, in order",
            "        Never call this directly: use _process which will only allow this to",
            "        run once per pusher.",
            "        \"\"\"",
            "",
            "        fn = self.store.get_unread_push_actions_for_user_in_range_for_http",
            "        unprocessed = await fn(",
            "            self.user_id, self.last_stream_ordering, self.max_stream_ordering",
            "        )",
            "",
            "        logger.info(",
            "            \"Processing %i unprocessed push actions for %s starting at \"",
            "            \"stream_ordering %s\",",
            "            len(unprocessed),",
            "            self.name,",
            "            self.last_stream_ordering,",
            "        )",
            "",
            "        for push_action in unprocessed:",
            "            with opentracing.start_active_span(",
            "                \"http-push\",",
            "                tags={",
            "                    \"authenticated_entity\": self.user_id,",
            "                    \"event_id\": push_action[\"event_id\"],",
            "                    \"app_id\": self.app_id,",
            "                    \"app_display_name\": self.app_display_name,",
            "                },",
            "            ):",
            "                processed = await self._process_one(push_action)",
            "",
            "            if processed:",
            "                http_push_processed_counter.inc()",
            "                self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC",
            "                self.last_stream_ordering = push_action[\"stream_ordering\"]",
            "                pusher_still_exists = await self.store.update_pusher_last_stream_ordering_and_success(",
            "                    self.app_id,",
            "                    self.pushkey,",
            "                    self.user_id,",
            "                    self.last_stream_ordering,",
            "                    self.clock.time_msec(),",
            "                )",
            "                if not pusher_still_exists:",
            "                    # The pusher has been deleted while we were processing, so",
            "                    # lets just stop and return.",
            "                    self.on_stop()",
            "                    return",
            "",
            "                if self.failing_since:",
            "                    self.failing_since = None",
            "                    await self.store.update_pusher_failing_since(",
            "                        self.app_id, self.pushkey, self.user_id, self.failing_since",
            "                    )",
            "            else:",
            "                http_push_failed_counter.inc()",
            "                if not self.failing_since:",
            "                    self.failing_since = self.clock.time_msec()",
            "                    await self.store.update_pusher_failing_since(",
            "                        self.app_id, self.pushkey, self.user_id, self.failing_since",
            "                    )",
            "",
            "                if (",
            "                    self.failing_since",
            "                    and self.failing_since",
            "                    < self.clock.time_msec() - HttpPusher.GIVE_UP_AFTER_MS",
            "                ):",
            "                    # we really only give up so that if the URL gets",
            "                    # fixed, we don't suddenly deliver a load",
            "                    # of old notifications.",
            "                    logger.warning(",
            "                        \"Giving up on a notification to user %s, pushkey %s\",",
            "                        self.user_id,",
            "                        self.pushkey,",
            "                    )",
            "                    self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC",
            "                    self.last_stream_ordering = push_action[\"stream_ordering\"]",
            "                    pusher_still_exists = await self.store.update_pusher_last_stream_ordering(",
            "                        self.app_id,",
            "                        self.pushkey,",
            "                        self.user_id,",
            "                        self.last_stream_ordering,",
            "                    )",
            "                    if not pusher_still_exists:",
            "                        # The pusher has been deleted while we were processing, so",
            "                        # lets just stop and return.",
            "                        self.on_stop()",
            "                        return",
            "",
            "                    self.failing_since = None",
            "                    await self.store.update_pusher_failing_since(",
            "                        self.app_id, self.pushkey, self.user_id, self.failing_since",
            "                    )",
            "                else:",
            "                    logger.info(\"Push failed: delaying for %ds\", self.backoff_delay)",
            "                    self.timed_call = self.hs.get_reactor().callLater(",
            "                        self.backoff_delay, self.on_timer",
            "                    )",
            "                    self.backoff_delay = min(",
            "                        self.backoff_delay * 2, self.MAX_BACKOFF_SEC",
            "                    )",
            "                    break",
            "",
            "    async def _process_one(self, push_action):",
            "        if \"notify\" not in push_action[\"actions\"]:",
            "            return True",
            "",
            "        tweaks = push_rule_evaluator.tweaks_for_actions(push_action[\"actions\"])",
            "        badge = await push_tools.get_badge_count(",
            "            self.hs.get_datastore(),",
            "            self.user_id,",
            "            group_by_room=self._group_unread_count_by_room,",
            "        )",
            "",
            "        event = await self.store.get_event(push_action[\"event_id\"], allow_none=True)",
            "        if event is None:",
            "            return True  # It's been redacted",
            "        rejected = await self.dispatch_push(event, tweaks, badge)",
            "        if rejected is False:",
            "            return False",
            "",
            "        if isinstance(rejected, list) or isinstance(rejected, tuple):",
            "            for pk in rejected:",
            "                if pk != self.pushkey:",
            "                    # for sanity, we only remove the pushkey if it",
            "                    # was the one we actually sent...",
            "                    logger.warning(",
            "                        (\"Ignoring rejected pushkey %s because we didn't send it\"), pk,",
            "                    )",
            "                else:",
            "                    logger.info(\"Pushkey %s was rejected: removing\", pk)",
            "                    await self.hs.remove_pusher(self.app_id, pk, self.user_id)",
            "        return True",
            "",
            "    async def _build_notification_dict(self, event, tweaks, badge):",
            "        priority = \"low\"",
            "        if (",
            "            event.type == EventTypes.Encrypted",
            "            or tweaks.get(\"highlight\")",
            "            or tweaks.get(\"sound\")",
            "        ):",
            "            # HACK send our push as high priority only if it generates a sound, highlight",
            "            #  or may do so (i.e. is encrypted so has unknown effects).",
            "            priority = \"high\"",
            "",
            "        if self.data.get(\"format\") == \"event_id_only\":",
            "            d = {",
            "                \"notification\": {",
            "                    \"event_id\": event.event_id,",
            "                    \"room_id\": event.room_id,",
            "                    \"counts\": {\"unread\": badge},",
            "                    \"prio\": priority,",
            "                    \"devices\": [",
            "                        {",
            "                            \"app_id\": self.app_id,",
            "                            \"pushkey\": self.pushkey,",
            "                            \"pushkey_ts\": int(self.pushkey_ts / 1000),",
            "                            \"data\": self.data_minus_url,",
            "                        }",
            "                    ],",
            "                }",
            "            }",
            "            return d",
            "",
            "        ctx = await push_tools.get_context_for_event(",
            "            self.storage, self.state_handler, event, self.user_id",
            "        )",
            "",
            "        d = {",
            "            \"notification\": {",
            "                \"id\": event.event_id,  # deprecated: remove soon",
            "                \"event_id\": event.event_id,",
            "                \"room_id\": event.room_id,",
            "                \"type\": event.type,",
            "                \"sender\": event.user_id,",
            "                \"prio\": priority,",
            "                \"counts\": {",
            "                    \"unread\": badge,",
            "                    # 'missed_calls': 2",
            "                },",
            "                \"devices\": [",
            "                    {",
            "                        \"app_id\": self.app_id,",
            "                        \"pushkey\": self.pushkey,",
            "                        \"pushkey_ts\": int(self.pushkey_ts / 1000),",
            "                        \"data\": self.data_minus_url,",
            "                        \"tweaks\": tweaks,",
            "                    }",
            "                ],",
            "            }",
            "        }",
            "        if event.type == \"m.room.member\" and event.is_state():",
            "            d[\"notification\"][\"membership\"] = event.content[\"membership\"]",
            "            d[\"notification\"][\"user_is_target\"] = event.state_key == self.user_id",
            "        if self.hs.config.push_include_content and event.content:",
            "            d[\"notification\"][\"content\"] = event.content",
            "",
            "        # We no longer send aliases separately, instead, we send the human",
            "        # readable name of the room, which may be an alias.",
            "        if \"sender_display_name\" in ctx and len(ctx[\"sender_display_name\"]) > 0:",
            "            d[\"notification\"][\"sender_display_name\"] = ctx[\"sender_display_name\"]",
            "        if \"name\" in ctx and len(ctx[\"name\"]) > 0:",
            "            d[\"notification\"][\"room_name\"] = ctx[\"name\"]",
            "",
            "        return d",
            "",
            "    async def dispatch_push(self, event, tweaks, badge):",
            "        notification_dict = await self._build_notification_dict(event, tweaks, badge)",
            "        if not notification_dict:",
            "            return []",
            "        try:",
            "            resp = await self.http_client.post_json_get_json(",
            "                self.url, notification_dict",
            "            )",
            "        except Exception as e:",
            "            logger.warning(",
            "                \"Failed to push event %s to %s: %s %s\",",
            "                event.event_id,",
            "                self.name,",
            "                type(e),",
            "                e,",
            "            )",
            "            return False",
            "        rejected = []",
            "        if \"rejected\" in resp:",
            "            rejected = resp[\"rejected\"]",
            "        return rejected",
            "",
            "    async def _send_badge(self, badge):",
            "        \"\"\"",
            "        Args:",
            "            badge (int): number of unread messages",
            "        \"\"\"",
            "        logger.debug(\"Sending updated badge count %d to %s\", badge, self.name)",
            "        d = {",
            "            \"notification\": {",
            "                \"id\": \"\",",
            "                \"type\": None,",
            "                \"sender\": \"\",",
            "                \"counts\": {\"unread\": badge},",
            "                \"devices\": [",
            "                    {",
            "                        \"app_id\": self.app_id,",
            "                        \"pushkey\": self.pushkey,",
            "                        \"pushkey_ts\": int(self.pushkey_ts / 1000),",
            "                        \"data\": self.data_minus_url,",
            "                    }",
            "                ],",
            "            }",
            "        }",
            "        try:",
            "            await self.http_client.post_json_get_json(self.url, d)",
            "            http_badges_processed_counter.inc()",
            "        except Exception as e:",
            "            logger.warning(",
            "                \"Failed to send badge count to %s: %s %s\", self.name, type(e), e",
            "            )",
            "            http_badges_failed_counter.inc()"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "# Copyright 2017 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "",
            "from prometheus_client import Counter",
            "",
            "from twisted.internet.error import AlreadyCalled, AlreadyCancelled",
            "",
            "from synapse.api.constants import EventTypes",
            "from synapse.logging import opentracing",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.push import PusherConfigException",
            "from synapse.types import RoomStreamToken",
            "",
            "from . import push_rule_evaluator, push_tools",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "http_push_processed_counter = Counter(",
            "    \"synapse_http_httppusher_http_pushes_processed\",",
            "    \"Number of push notifications successfully sent\",",
            ")",
            "",
            "http_push_failed_counter = Counter(",
            "    \"synapse_http_httppusher_http_pushes_failed\",",
            "    \"Number of push notifications which failed\",",
            ")",
            "",
            "http_badges_processed_counter = Counter(",
            "    \"synapse_http_httppusher_badge_updates_processed\",",
            "    \"Number of badge updates successfully sent\",",
            ")",
            "",
            "http_badges_failed_counter = Counter(",
            "    \"synapse_http_httppusher_badge_updates_failed\",",
            "    \"Number of badge updates which failed\",",
            ")",
            "",
            "",
            "class HttpPusher:",
            "    INITIAL_BACKOFF_SEC = 1  # in seconds because that's what Twisted takes",
            "    MAX_BACKOFF_SEC = 60 * 60",
            "",
            "    # This one's in ms because we compare it against the clock",
            "    GIVE_UP_AFTER_MS = 24 * 60 * 60 * 1000",
            "",
            "    def __init__(self, hs, pusherdict):",
            "        self.hs = hs",
            "        self.store = self.hs.get_datastore()",
            "        self.storage = self.hs.get_storage()",
            "        self.clock = self.hs.get_clock()",
            "        self.state_handler = self.hs.get_state_handler()",
            "        self.user_id = pusherdict[\"user_name\"]",
            "        self.app_id = pusherdict[\"app_id\"]",
            "        self.app_display_name = pusherdict[\"app_display_name\"]",
            "        self.device_display_name = pusherdict[\"device_display_name\"]",
            "        self.pushkey = pusherdict[\"pushkey\"]",
            "        self.pushkey_ts = pusherdict[\"ts\"]",
            "        self.data = pusherdict[\"data\"]",
            "        self.last_stream_ordering = pusherdict[\"last_stream_ordering\"]",
            "        self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC",
            "        self.failing_since = pusherdict[\"failing_since\"]",
            "        self.timed_call = None",
            "        self._is_processing = False",
            "        self._group_unread_count_by_room = hs.config.push_group_unread_count_by_room",
            "",
            "        # This is the highest stream ordering we know it's safe to process.",
            "        # When new events arrive, we'll be given a window of new events: we",
            "        # should honour this rather than just looking for anything higher",
            "        # because of potential out-of-order event serialisation. This starts",
            "        # off as None though as we don't know any better.",
            "        self.max_stream_ordering = None",
            "",
            "        if \"data\" not in pusherdict:",
            "            raise PusherConfigException(\"No 'data' key for HTTP pusher\")",
            "        self.data = pusherdict[\"data\"]",
            "",
            "        self.name = \"%s/%s/%s\" % (",
            "            pusherdict[\"user_name\"],",
            "            pusherdict[\"app_id\"],",
            "            pusherdict[\"pushkey\"],",
            "        )",
            "",
            "        if self.data is None:",
            "            raise PusherConfigException(\"data can not be null for HTTP pusher\")",
            "",
            "        if \"url\" not in self.data:",
            "            raise PusherConfigException(\"'url' required in data for HTTP pusher\")",
            "        self.url = self.data[\"url\"]",
            "        self.http_client = hs.get_proxied_blacklisted_http_client()",
            "        self.data_minus_url = {}",
            "        self.data_minus_url.update(self.data)",
            "        del self.data_minus_url[\"url\"]",
            "",
            "    def on_started(self, should_check_for_notifs):",
            "        \"\"\"Called when this pusher has been started.",
            "",
            "        Args:",
            "            should_check_for_notifs (bool): Whether we should immediately",
            "                check for push to send. Set to False only if it's known there",
            "                is nothing to send",
            "        \"\"\"",
            "        if should_check_for_notifs:",
            "            self._start_processing()",
            "",
            "    def on_new_notifications(self, max_token: RoomStreamToken):",
            "        # We just use the minimum stream ordering and ignore the vector clock",
            "        # component. This is safe to do as long as we *always* ignore the vector",
            "        # clock components.",
            "        max_stream_ordering = max_token.stream",
            "",
            "        self.max_stream_ordering = max(",
            "            max_stream_ordering, self.max_stream_ordering or 0",
            "        )",
            "        self._start_processing()",
            "",
            "    def on_new_receipts(self, min_stream_id, max_stream_id):",
            "        # Note that the min here shouldn't be relied upon to be accurate.",
            "",
            "        # We could check the receipts are actually m.read receipts here,",
            "        # but currently that's the only type of receipt anyway...",
            "        run_as_background_process(\"http_pusher.on_new_receipts\", self._update_badge)",
            "",
            "    async def _update_badge(self):",
            "        # XXX as per https://github.com/matrix-org/matrix-doc/issues/2627, this seems",
            "        # to be largely redundant. perhaps we can remove it.",
            "        badge = await push_tools.get_badge_count(",
            "            self.hs.get_datastore(),",
            "            self.user_id,",
            "            group_by_room=self._group_unread_count_by_room,",
            "        )",
            "        await self._send_badge(badge)",
            "",
            "    def on_timer(self):",
            "        self._start_processing()",
            "",
            "    def on_stop(self):",
            "        if self.timed_call:",
            "            try:",
            "                self.timed_call.cancel()",
            "            except (AlreadyCalled, AlreadyCancelled):",
            "                pass",
            "            self.timed_call = None",
            "",
            "    def _start_processing(self):",
            "        if self._is_processing:",
            "            return",
            "",
            "        run_as_background_process(\"httppush.process\", self._process)",
            "",
            "    async def _process(self):",
            "        # we should never get here if we are already processing",
            "        assert not self._is_processing",
            "",
            "        try:",
            "            self._is_processing = True",
            "            # if the max ordering changes while we're running _unsafe_process,",
            "            # call it again, and so on until we've caught up.",
            "            while True:",
            "                starting_max_ordering = self.max_stream_ordering",
            "                try:",
            "                    await self._unsafe_process()",
            "                except Exception:",
            "                    logger.exception(\"Exception processing notifs\")",
            "                if self.max_stream_ordering == starting_max_ordering:",
            "                    break",
            "        finally:",
            "            self._is_processing = False",
            "",
            "    async def _unsafe_process(self):",
            "        \"\"\"",
            "        Looks for unset notifications and dispatch them, in order",
            "        Never call this directly: use _process which will only allow this to",
            "        run once per pusher.",
            "        \"\"\"",
            "",
            "        fn = self.store.get_unread_push_actions_for_user_in_range_for_http",
            "        unprocessed = await fn(",
            "            self.user_id, self.last_stream_ordering, self.max_stream_ordering",
            "        )",
            "",
            "        logger.info(",
            "            \"Processing %i unprocessed push actions for %s starting at \"",
            "            \"stream_ordering %s\",",
            "            len(unprocessed),",
            "            self.name,",
            "            self.last_stream_ordering,",
            "        )",
            "",
            "        for push_action in unprocessed:",
            "            with opentracing.start_active_span(",
            "                \"http-push\",",
            "                tags={",
            "                    \"authenticated_entity\": self.user_id,",
            "                    \"event_id\": push_action[\"event_id\"],",
            "                    \"app_id\": self.app_id,",
            "                    \"app_display_name\": self.app_display_name,",
            "                },",
            "            ):",
            "                processed = await self._process_one(push_action)",
            "",
            "            if processed:",
            "                http_push_processed_counter.inc()",
            "                self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC",
            "                self.last_stream_ordering = push_action[\"stream_ordering\"]",
            "                pusher_still_exists = await self.store.update_pusher_last_stream_ordering_and_success(",
            "                    self.app_id,",
            "                    self.pushkey,",
            "                    self.user_id,",
            "                    self.last_stream_ordering,",
            "                    self.clock.time_msec(),",
            "                )",
            "                if not pusher_still_exists:",
            "                    # The pusher has been deleted while we were processing, so",
            "                    # lets just stop and return.",
            "                    self.on_stop()",
            "                    return",
            "",
            "                if self.failing_since:",
            "                    self.failing_since = None",
            "                    await self.store.update_pusher_failing_since(",
            "                        self.app_id, self.pushkey, self.user_id, self.failing_since",
            "                    )",
            "            else:",
            "                http_push_failed_counter.inc()",
            "                if not self.failing_since:",
            "                    self.failing_since = self.clock.time_msec()",
            "                    await self.store.update_pusher_failing_since(",
            "                        self.app_id, self.pushkey, self.user_id, self.failing_since",
            "                    )",
            "",
            "                if (",
            "                    self.failing_since",
            "                    and self.failing_since",
            "                    < self.clock.time_msec() - HttpPusher.GIVE_UP_AFTER_MS",
            "                ):",
            "                    # we really only give up so that if the URL gets",
            "                    # fixed, we don't suddenly deliver a load",
            "                    # of old notifications.",
            "                    logger.warning(",
            "                        \"Giving up on a notification to user %s, pushkey %s\",",
            "                        self.user_id,",
            "                        self.pushkey,",
            "                    )",
            "                    self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC",
            "                    self.last_stream_ordering = push_action[\"stream_ordering\"]",
            "                    pusher_still_exists = await self.store.update_pusher_last_stream_ordering(",
            "                        self.app_id,",
            "                        self.pushkey,",
            "                        self.user_id,",
            "                        self.last_stream_ordering,",
            "                    )",
            "                    if not pusher_still_exists:",
            "                        # The pusher has been deleted while we were processing, so",
            "                        # lets just stop and return.",
            "                        self.on_stop()",
            "                        return",
            "",
            "                    self.failing_since = None",
            "                    await self.store.update_pusher_failing_since(",
            "                        self.app_id, self.pushkey, self.user_id, self.failing_since",
            "                    )",
            "                else:",
            "                    logger.info(\"Push failed: delaying for %ds\", self.backoff_delay)",
            "                    self.timed_call = self.hs.get_reactor().callLater(",
            "                        self.backoff_delay, self.on_timer",
            "                    )",
            "                    self.backoff_delay = min(",
            "                        self.backoff_delay * 2, self.MAX_BACKOFF_SEC",
            "                    )",
            "                    break",
            "",
            "    async def _process_one(self, push_action):",
            "        if \"notify\" not in push_action[\"actions\"]:",
            "            return True",
            "",
            "        tweaks = push_rule_evaluator.tweaks_for_actions(push_action[\"actions\"])",
            "        badge = await push_tools.get_badge_count(",
            "            self.hs.get_datastore(),",
            "            self.user_id,",
            "            group_by_room=self._group_unread_count_by_room,",
            "        )",
            "",
            "        event = await self.store.get_event(push_action[\"event_id\"], allow_none=True)",
            "        if event is None:",
            "            return True  # It's been redacted",
            "        rejected = await self.dispatch_push(event, tweaks, badge)",
            "        if rejected is False:",
            "            return False",
            "",
            "        if isinstance(rejected, list) or isinstance(rejected, tuple):",
            "            for pk in rejected:",
            "                if pk != self.pushkey:",
            "                    # for sanity, we only remove the pushkey if it",
            "                    # was the one we actually sent...",
            "                    logger.warning(",
            "                        (\"Ignoring rejected pushkey %s because we didn't send it\"), pk,",
            "                    )",
            "                else:",
            "                    logger.info(\"Pushkey %s was rejected: removing\", pk)",
            "                    await self.hs.remove_pusher(self.app_id, pk, self.user_id)",
            "        return True",
            "",
            "    async def _build_notification_dict(self, event, tweaks, badge):",
            "        priority = \"low\"",
            "        if (",
            "            event.type == EventTypes.Encrypted",
            "            or tweaks.get(\"highlight\")",
            "            or tweaks.get(\"sound\")",
            "        ):",
            "            # HACK send our push as high priority only if it generates a sound, highlight",
            "            #  or may do so (i.e. is encrypted so has unknown effects).",
            "            priority = \"high\"",
            "",
            "        if self.data.get(\"format\") == \"event_id_only\":",
            "            d = {",
            "                \"notification\": {",
            "                    \"event_id\": event.event_id,",
            "                    \"room_id\": event.room_id,",
            "                    \"counts\": {\"unread\": badge},",
            "                    \"prio\": priority,",
            "                    \"devices\": [",
            "                        {",
            "                            \"app_id\": self.app_id,",
            "                            \"pushkey\": self.pushkey,",
            "                            \"pushkey_ts\": int(self.pushkey_ts / 1000),",
            "                            \"data\": self.data_minus_url,",
            "                        }",
            "                    ],",
            "                }",
            "            }",
            "            return d",
            "",
            "        ctx = await push_tools.get_context_for_event(",
            "            self.storage, self.state_handler, event, self.user_id",
            "        )",
            "",
            "        d = {",
            "            \"notification\": {",
            "                \"id\": event.event_id,  # deprecated: remove soon",
            "                \"event_id\": event.event_id,",
            "                \"room_id\": event.room_id,",
            "                \"type\": event.type,",
            "                \"sender\": event.user_id,",
            "                \"prio\": priority,",
            "                \"counts\": {",
            "                    \"unread\": badge,",
            "                    # 'missed_calls': 2",
            "                },",
            "                \"devices\": [",
            "                    {",
            "                        \"app_id\": self.app_id,",
            "                        \"pushkey\": self.pushkey,",
            "                        \"pushkey_ts\": int(self.pushkey_ts / 1000),",
            "                        \"data\": self.data_minus_url,",
            "                        \"tweaks\": tweaks,",
            "                    }",
            "                ],",
            "            }",
            "        }",
            "        if event.type == \"m.room.member\" and event.is_state():",
            "            d[\"notification\"][\"membership\"] = event.content[\"membership\"]",
            "            d[\"notification\"][\"user_is_target\"] = event.state_key == self.user_id",
            "        if self.hs.config.push_include_content and event.content:",
            "            d[\"notification\"][\"content\"] = event.content",
            "",
            "        # We no longer send aliases separately, instead, we send the human",
            "        # readable name of the room, which may be an alias.",
            "        if \"sender_display_name\" in ctx and len(ctx[\"sender_display_name\"]) > 0:",
            "            d[\"notification\"][\"sender_display_name\"] = ctx[\"sender_display_name\"]",
            "        if \"name\" in ctx and len(ctx[\"name\"]) > 0:",
            "            d[\"notification\"][\"room_name\"] = ctx[\"name\"]",
            "",
            "        return d",
            "",
            "    async def dispatch_push(self, event, tweaks, badge):",
            "        notification_dict = await self._build_notification_dict(event, tweaks, badge)",
            "        if not notification_dict:",
            "            return []",
            "        try:",
            "            resp = await self.http_client.post_json_get_json(",
            "                self.url, notification_dict",
            "            )",
            "        except Exception as e:",
            "            logger.warning(",
            "                \"Failed to push event %s to %s: %s %s\",",
            "                event.event_id,",
            "                self.name,",
            "                type(e),",
            "                e,",
            "            )",
            "            return False",
            "        rejected = []",
            "        if \"rejected\" in resp:",
            "            rejected = resp[\"rejected\"]",
            "        return rejected",
            "",
            "    async def _send_badge(self, badge):",
            "        \"\"\"",
            "        Args:",
            "            badge (int): number of unread messages",
            "        \"\"\"",
            "        logger.debug(\"Sending updated badge count %d to %s\", badge, self.name)",
            "        d = {",
            "            \"notification\": {",
            "                \"id\": \"\",",
            "                \"type\": None,",
            "                \"sender\": \"\",",
            "                \"counts\": {\"unread\": badge},",
            "                \"devices\": [",
            "                    {",
            "                        \"app_id\": self.app_id,",
            "                        \"pushkey\": self.pushkey,",
            "                        \"pushkey_ts\": int(self.pushkey_ts / 1000),",
            "                        \"data\": self.data_minus_url,",
            "                    }",
            "                ],",
            "            }",
            "        }",
            "        try:",
            "            await self.http_client.post_json_get_json(self.url, d)",
            "            http_badges_processed_counter.inc()",
            "        except Exception as e:",
            "            logger.warning(",
            "                \"Failed to send badge count to %s: %s %s\", self.name, type(e), e",
            "            )",
            "            http_badges_failed_counter.inc()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "103": [
                "HttpPusher",
                "__init__"
            ]
        },
        "addLocation": []
    },
    "synapse/rest/media/v1/media_repository.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 66,
                "PatchRowcode": "     def __init__(self, hs):"
            },
            "1": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 67,
                "PatchRowcode": "         self.hs = hs"
            },
            "2": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 68,
                "PatchRowcode": "         self.auth = hs.get_auth()"
            },
            "3": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.client = hs.get_http_client()"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 69,
                "PatchRowcode": "+        self.client = hs.get_federation_http_client()"
            },
            "5": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 70,
                "PatchRowcode": "         self.clock = hs.get_clock()"
            },
            "6": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 71,
                "PatchRowcode": "         self.server_name = hs.hostname"
            },
            "7": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "         self.store = hs.get_datastore()"
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import errno",
            "import logging",
            "import os",
            "import shutil",
            "from typing import IO, Dict, List, Optional, Tuple",
            "",
            "import twisted.internet.error",
            "import twisted.web.http",
            "from twisted.web.http import Request",
            "from twisted.web.resource import Resource",
            "",
            "from synapse.api.errors import (",
            "    FederationDeniedError,",
            "    HttpResponseException,",
            "    NotFoundError,",
            "    RequestSendFailed,",
            "    SynapseError,",
            ")",
            "from synapse.config._base import ConfigError",
            "from synapse.logging.context import defer_to_thread",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.util.async_helpers import Linearizer",
            "from synapse.util.retryutils import NotRetryingDestination",
            "from synapse.util.stringutils import random_string",
            "",
            "from ._base import (",
            "    FileInfo,",
            "    Responder,",
            "    get_filename_from_headers,",
            "    respond_404,",
            "    respond_with_responder,",
            ")",
            "from .config_resource import MediaConfigResource",
            "from .download_resource import DownloadResource",
            "from .filepath import MediaFilePaths",
            "from .media_storage import MediaStorage",
            "from .preview_url_resource import PreviewUrlResource",
            "from .storage_provider import StorageProviderWrapper",
            "from .thumbnail_resource import ThumbnailResource",
            "from .thumbnailer import Thumbnailer, ThumbnailError",
            "from .upload_resource import UploadResource",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "UPDATE_RECENTLY_ACCESSED_TS = 60 * 1000",
            "",
            "",
            "class MediaRepository:",
            "    def __init__(self, hs):",
            "        self.hs = hs",
            "        self.auth = hs.get_auth()",
            "        self.client = hs.get_http_client()",
            "        self.clock = hs.get_clock()",
            "        self.server_name = hs.hostname",
            "        self.store = hs.get_datastore()",
            "        self.max_upload_size = hs.config.max_upload_size",
            "        self.max_image_pixels = hs.config.max_image_pixels",
            "",
            "        self.primary_base_path = hs.config.media_store_path",
            "        self.filepaths = MediaFilePaths(self.primary_base_path)",
            "",
            "        self.dynamic_thumbnails = hs.config.dynamic_thumbnails",
            "        self.thumbnail_requirements = hs.config.thumbnail_requirements",
            "",
            "        self.remote_media_linearizer = Linearizer(name=\"media_remote\")",
            "",
            "        self.recently_accessed_remotes = set()",
            "        self.recently_accessed_locals = set()",
            "",
            "        self.federation_domain_whitelist = hs.config.federation_domain_whitelist",
            "",
            "        # List of StorageProviders where we should search for media and",
            "        # potentially upload to.",
            "        storage_providers = []",
            "",
            "        for clz, provider_config, wrapper_config in hs.config.media_storage_providers:",
            "            backend = clz(hs, provider_config)",
            "            provider = StorageProviderWrapper(",
            "                backend,",
            "                store_local=wrapper_config.store_local,",
            "                store_remote=wrapper_config.store_remote,",
            "                store_synchronous=wrapper_config.store_synchronous,",
            "            )",
            "            storage_providers.append(provider)",
            "",
            "        self.media_storage = MediaStorage(",
            "            self.hs, self.primary_base_path, self.filepaths, storage_providers",
            "        )",
            "",
            "        self.clock.looping_call(",
            "            self._start_update_recently_accessed, UPDATE_RECENTLY_ACCESSED_TS",
            "        )",
            "",
            "    def _start_update_recently_accessed(self):",
            "        return run_as_background_process(",
            "            \"update_recently_accessed_media\", self._update_recently_accessed",
            "        )",
            "",
            "    async def _update_recently_accessed(self):",
            "        remote_media = self.recently_accessed_remotes",
            "        self.recently_accessed_remotes = set()",
            "",
            "        local_media = self.recently_accessed_locals",
            "        self.recently_accessed_locals = set()",
            "",
            "        await self.store.update_cached_last_access_time(",
            "            local_media, remote_media, self.clock.time_msec()",
            "        )",
            "",
            "    def mark_recently_accessed(self, server_name, media_id):",
            "        \"\"\"Mark the given media as recently accessed.",
            "",
            "        Args:",
            "            server_name (str|None): Origin server of media, or None if local",
            "            media_id (str): The media ID of the content",
            "        \"\"\"",
            "        if server_name:",
            "            self.recently_accessed_remotes.add((server_name, media_id))",
            "        else:",
            "            self.recently_accessed_locals.add(media_id)",
            "",
            "    async def create_content(",
            "        self,",
            "        media_type: str,",
            "        upload_name: Optional[str],",
            "        content: IO,",
            "        content_length: int,",
            "        auth_user: str,",
            "    ) -> str:",
            "        \"\"\"Store uploaded content for a local user and return the mxc URL",
            "",
            "        Args:",
            "            media_type: The content type of the file.",
            "            upload_name: The name of the file, if provided.",
            "            content: A file like object that is the content to store",
            "            content_length: The length of the content",
            "            auth_user: The user_id of the uploader",
            "",
            "        Returns:",
            "            The mxc url of the stored content",
            "        \"\"\"",
            "",
            "        media_id = random_string(24)",
            "",
            "        file_info = FileInfo(server_name=None, file_id=media_id)",
            "",
            "        fname = await self.media_storage.store_file(content, file_info)",
            "",
            "        logger.info(\"Stored local media in file %r\", fname)",
            "",
            "        await self.store.store_local_media(",
            "            media_id=media_id,",
            "            media_type=media_type,",
            "            time_now_ms=self.clock.time_msec(),",
            "            upload_name=upload_name,",
            "            media_length=content_length,",
            "            user_id=auth_user,",
            "        )",
            "",
            "        await self._generate_thumbnails(None, media_id, media_id, media_type)",
            "",
            "        return \"mxc://%s/%s\" % (self.server_name, media_id)",
            "",
            "    async def get_local_media(",
            "        self, request: Request, media_id: str, name: Optional[str]",
            "    ) -> None:",
            "        \"\"\"Responds to reqests for local media, if exists, or returns 404.",
            "",
            "        Args:",
            "            request: The incoming request.",
            "            media_id: The media ID of the content. (This is the same as",
            "                the file_id for local content.)",
            "            name: Optional name that, if specified, will be used as",
            "                the filename in the Content-Disposition header of the response.",
            "",
            "        Returns:",
            "            Resolves once a response has successfully been written to request",
            "        \"\"\"",
            "        media_info = await self.store.get_local_media(media_id)",
            "        if not media_info or media_info[\"quarantined_by\"]:",
            "            respond_404(request)",
            "            return",
            "",
            "        self.mark_recently_accessed(None, media_id)",
            "",
            "        media_type = media_info[\"media_type\"]",
            "        media_length = media_info[\"media_length\"]",
            "        upload_name = name if name else media_info[\"upload_name\"]",
            "        url_cache = media_info[\"url_cache\"]",
            "",
            "        file_info = FileInfo(None, media_id, url_cache=url_cache)",
            "",
            "        responder = await self.media_storage.fetch_media(file_info)",
            "        await respond_with_responder(",
            "            request, responder, media_type, media_length, upload_name",
            "        )",
            "",
            "    async def get_remote_media(",
            "        self, request: Request, server_name: str, media_id: str, name: Optional[str]",
            "    ) -> None:",
            "        \"\"\"Respond to requests for remote media.",
            "",
            "        Args:",
            "            request: The incoming request.",
            "            server_name: Remote server_name where the media originated.",
            "            media_id: The media ID of the content (as defined by the remote server).",
            "            name: Optional name that, if specified, will be used as",
            "                the filename in the Content-Disposition header of the response.",
            "",
            "        Returns:",
            "            Resolves once a response has successfully been written to request",
            "        \"\"\"",
            "        if (",
            "            self.federation_domain_whitelist is not None",
            "            and server_name not in self.federation_domain_whitelist",
            "        ):",
            "            raise FederationDeniedError(server_name)",
            "",
            "        self.mark_recently_accessed(server_name, media_id)",
            "",
            "        # We linearize here to ensure that we don't try and download remote",
            "        # media multiple times concurrently",
            "        key = (server_name, media_id)",
            "        with (await self.remote_media_linearizer.queue(key)):",
            "            responder, media_info = await self._get_remote_media_impl(",
            "                server_name, media_id",
            "            )",
            "",
            "        # We deliberately stream the file outside the lock",
            "        if responder:",
            "            media_type = media_info[\"media_type\"]",
            "            media_length = media_info[\"media_length\"]",
            "            upload_name = name if name else media_info[\"upload_name\"]",
            "            await respond_with_responder(",
            "                request, responder, media_type, media_length, upload_name",
            "            )",
            "        else:",
            "            respond_404(request)",
            "",
            "    async def get_remote_media_info(self, server_name: str, media_id: str) -> dict:",
            "        \"\"\"Gets the media info associated with the remote file, downloading",
            "        if necessary.",
            "",
            "        Args:",
            "            server_name: Remote server_name where the media originated.",
            "            media_id: The media ID of the content (as defined by the remote server).",
            "",
            "        Returns:",
            "            The media info of the file",
            "        \"\"\"",
            "        if (",
            "            self.federation_domain_whitelist is not None",
            "            and server_name not in self.federation_domain_whitelist",
            "        ):",
            "            raise FederationDeniedError(server_name)",
            "",
            "        # We linearize here to ensure that we don't try and download remote",
            "        # media multiple times concurrently",
            "        key = (server_name, media_id)",
            "        with (await self.remote_media_linearizer.queue(key)):",
            "            responder, media_info = await self._get_remote_media_impl(",
            "                server_name, media_id",
            "            )",
            "",
            "        # Ensure we actually use the responder so that it releases resources",
            "        if responder:",
            "            with responder:",
            "                pass",
            "",
            "        return media_info",
            "",
            "    async def _get_remote_media_impl(",
            "        self, server_name: str, media_id: str",
            "    ) -> Tuple[Optional[Responder], dict]:",
            "        \"\"\"Looks for media in local cache, if not there then attempt to",
            "        download from remote server.",
            "",
            "        Args:",
            "            server_name (str): Remote server_name where the media originated.",
            "            media_id (str): The media ID of the content (as defined by the",
            "                remote server).",
            "",
            "        Returns:",
            "            A tuple of responder and the media info of the file.",
            "        \"\"\"",
            "        media_info = await self.store.get_cached_remote_media(server_name, media_id)",
            "",
            "        # file_id is the ID we use to track the file locally. If we've already",
            "        # seen the file then reuse the existing ID, otherwise genereate a new",
            "        # one.",
            "",
            "        # If we have an entry in the DB, try and look for it",
            "        if media_info:",
            "            file_id = media_info[\"filesystem_id\"]",
            "            file_info = FileInfo(server_name, file_id)",
            "",
            "            if media_info[\"quarantined_by\"]:",
            "                logger.info(\"Media is quarantined\")",
            "                raise NotFoundError()",
            "",
            "            responder = await self.media_storage.fetch_media(file_info)",
            "            if responder:",
            "                return responder, media_info",
            "",
            "        # Failed to find the file anywhere, lets download it.",
            "",
            "        try:",
            "            media_info = await self._download_remote_file(server_name, media_id,)",
            "        except SynapseError:",
            "            raise",
            "        except Exception as e:",
            "            # An exception may be because we downloaded media in another",
            "            # process, so let's check if we magically have the media.",
            "            media_info = await self.store.get_cached_remote_media(server_name, media_id)",
            "            if not media_info:",
            "                raise e",
            "",
            "        file_id = media_info[\"filesystem_id\"]",
            "        file_info = FileInfo(server_name, file_id)",
            "",
            "        # We generate thumbnails even if another process downloaded the media",
            "        # as a) it's conceivable that the other download request dies before it",
            "        # generates thumbnails, but mainly b) we want to be sure the thumbnails",
            "        # have finished being generated before responding to the client,",
            "        # otherwise they'll request thumbnails and get a 404 if they're not",
            "        # ready yet.",
            "        await self._generate_thumbnails(",
            "            server_name, media_id, file_id, media_info[\"media_type\"]",
            "        )",
            "",
            "        responder = await self.media_storage.fetch_media(file_info)",
            "        return responder, media_info",
            "",
            "    async def _download_remote_file(self, server_name: str, media_id: str,) -> dict:",
            "        \"\"\"Attempt to download the remote file from the given server name,",
            "        using the given file_id as the local id.",
            "",
            "        Args:",
            "            server_name: Originating server",
            "            media_id: The media ID of the content (as defined by the",
            "                remote server). This is different than the file_id, which is",
            "                locally generated.",
            "            file_id: Local file ID",
            "",
            "        Returns:",
            "            The media info of the file.",
            "        \"\"\"",
            "",
            "        file_id = random_string(24)",
            "",
            "        file_info = FileInfo(server_name=server_name, file_id=file_id)",
            "",
            "        with self.media_storage.store_into_file(file_info) as (f, fname, finish):",
            "            request_path = \"/\".join(",
            "                (\"/_matrix/media/r0/download\", server_name, media_id)",
            "            )",
            "            try:",
            "                length, headers = await self.client.get_file(",
            "                    server_name,",
            "                    request_path,",
            "                    output_stream=f,",
            "                    max_size=self.max_upload_size,",
            "                    args={",
            "                        # tell the remote server to 404 if it doesn't",
            "                        # recognise the server_name, to make sure we don't",
            "                        # end up with a routing loop.",
            "                        \"allow_remote\": \"false\"",
            "                    },",
            "                )",
            "            except RequestSendFailed as e:",
            "                logger.warning(",
            "                    \"Request failed fetching remote media %s/%s: %r\",",
            "                    server_name,",
            "                    media_id,",
            "                    e,",
            "                )",
            "                raise SynapseError(502, \"Failed to fetch remote media\")",
            "",
            "            except HttpResponseException as e:",
            "                logger.warning(",
            "                    \"HTTP error fetching remote media %s/%s: %s\",",
            "                    server_name,",
            "                    media_id,",
            "                    e.response,",
            "                )",
            "                if e.code == twisted.web.http.NOT_FOUND:",
            "                    raise e.to_synapse_error()",
            "                raise SynapseError(502, \"Failed to fetch remote media\")",
            "",
            "            except SynapseError:",
            "                logger.warning(",
            "                    \"Failed to fetch remote media %s/%s\", server_name, media_id",
            "                )",
            "                raise",
            "            except NotRetryingDestination:",
            "                logger.warning(\"Not retrying destination %r\", server_name)",
            "                raise SynapseError(502, \"Failed to fetch remote media\")",
            "            except Exception:",
            "                logger.exception(",
            "                    \"Failed to fetch remote media %s/%s\", server_name, media_id",
            "                )",
            "                raise SynapseError(502, \"Failed to fetch remote media\")",
            "",
            "            await finish()",
            "",
            "            media_type = headers[b\"Content-Type\"][0].decode(\"ascii\")",
            "            upload_name = get_filename_from_headers(headers)",
            "            time_now_ms = self.clock.time_msec()",
            "",
            "            # Multiple remote media download requests can race (when using",
            "            # multiple media repos), so this may throw a violation constraint",
            "            # exception. If it does we'll delete the newly downloaded file from",
            "            # disk (as we're in the ctx manager).",
            "            #",
            "            # However: we've already called `finish()` so we may have also",
            "            # written to the storage providers. This is preferable to the",
            "            # alternative where we call `finish()` *after* this, where we could",
            "            # end up having an entry in the DB but fail to write the files to",
            "            # the storage providers.",
            "            await self.store.store_cached_remote_media(",
            "                origin=server_name,",
            "                media_id=media_id,",
            "                media_type=media_type,",
            "                time_now_ms=self.clock.time_msec(),",
            "                upload_name=upload_name,",
            "                media_length=length,",
            "                filesystem_id=file_id,",
            "            )",
            "",
            "        logger.info(\"Stored remote media in file %r\", fname)",
            "",
            "        media_info = {",
            "            \"media_type\": media_type,",
            "            \"media_length\": length,",
            "            \"upload_name\": upload_name,",
            "            \"created_ts\": time_now_ms,",
            "            \"filesystem_id\": file_id,",
            "        }",
            "",
            "        return media_info",
            "",
            "    def _get_thumbnail_requirements(self, media_type):",
            "        return self.thumbnail_requirements.get(media_type, ())",
            "",
            "    def _generate_thumbnail(self, thumbnailer, t_width, t_height, t_method, t_type):",
            "        m_width = thumbnailer.width",
            "        m_height = thumbnailer.height",
            "",
            "        if m_width * m_height >= self.max_image_pixels:",
            "            logger.info(",
            "                \"Image too large to thumbnail %r x %r > %r\",",
            "                m_width,",
            "                m_height,",
            "                self.max_image_pixels,",
            "            )",
            "            return",
            "",
            "        if thumbnailer.transpose_method is not None:",
            "            m_width, m_height = thumbnailer.transpose()",
            "",
            "        if t_method == \"crop\":",
            "            t_byte_source = thumbnailer.crop(t_width, t_height, t_type)",
            "        elif t_method == \"scale\":",
            "            t_width, t_height = thumbnailer.aspect(t_width, t_height)",
            "            t_width = min(m_width, t_width)",
            "            t_height = min(m_height, t_height)",
            "            t_byte_source = thumbnailer.scale(t_width, t_height, t_type)",
            "        else:",
            "            t_byte_source = None",
            "",
            "        return t_byte_source",
            "",
            "    async def generate_local_exact_thumbnail(",
            "        self,",
            "        media_id: str,",
            "        t_width: int,",
            "        t_height: int,",
            "        t_method: str,",
            "        t_type: str,",
            "        url_cache: str,",
            "    ) -> Optional[str]:",
            "        input_path = await self.media_storage.ensure_media_is_in_local_cache(",
            "            FileInfo(None, media_id, url_cache=url_cache)",
            "        )",
            "",
            "        try:",
            "            thumbnailer = Thumbnailer(input_path)",
            "        except ThumbnailError as e:",
            "            logger.warning(",
            "                \"Unable to generate a thumbnail for local media %s using a method of %s and type of %s: %s\",",
            "                media_id,",
            "                t_method,",
            "                t_type,",
            "                e,",
            "            )",
            "            return None",
            "",
            "        t_byte_source = await defer_to_thread(",
            "            self.hs.get_reactor(),",
            "            self._generate_thumbnail,",
            "            thumbnailer,",
            "            t_width,",
            "            t_height,",
            "            t_method,",
            "            t_type,",
            "        )",
            "",
            "        if t_byte_source:",
            "            try:",
            "                file_info = FileInfo(",
            "                    server_name=None,",
            "                    file_id=media_id,",
            "                    url_cache=url_cache,",
            "                    thumbnail=True,",
            "                    thumbnail_width=t_width,",
            "                    thumbnail_height=t_height,",
            "                    thumbnail_method=t_method,",
            "                    thumbnail_type=t_type,",
            "                )",
            "",
            "                output_path = await self.media_storage.store_file(",
            "                    t_byte_source, file_info",
            "                )",
            "            finally:",
            "                t_byte_source.close()",
            "",
            "            logger.info(\"Stored thumbnail in file %r\", output_path)",
            "",
            "            t_len = os.path.getsize(output_path)",
            "",
            "            await self.store.store_local_thumbnail(",
            "                media_id, t_width, t_height, t_type, t_method, t_len",
            "            )",
            "",
            "            return output_path",
            "",
            "        # Could not generate thumbnail.",
            "        return None",
            "",
            "    async def generate_remote_exact_thumbnail(",
            "        self,",
            "        server_name: str,",
            "        file_id: str,",
            "        media_id: str,",
            "        t_width: int,",
            "        t_height: int,",
            "        t_method: str,",
            "        t_type: str,",
            "    ) -> Optional[str]:",
            "        input_path = await self.media_storage.ensure_media_is_in_local_cache(",
            "            FileInfo(server_name, file_id, url_cache=False)",
            "        )",
            "",
            "        try:",
            "            thumbnailer = Thumbnailer(input_path)",
            "        except ThumbnailError as e:",
            "            logger.warning(",
            "                \"Unable to generate a thumbnail for remote media %s from %s using a method of %s and type of %s: %s\",",
            "                media_id,",
            "                server_name,",
            "                t_method,",
            "                t_type,",
            "                e,",
            "            )",
            "            return None",
            "",
            "        t_byte_source = await defer_to_thread(",
            "            self.hs.get_reactor(),",
            "            self._generate_thumbnail,",
            "            thumbnailer,",
            "            t_width,",
            "            t_height,",
            "            t_method,",
            "            t_type,",
            "        )",
            "",
            "        if t_byte_source:",
            "            try:",
            "                file_info = FileInfo(",
            "                    server_name=server_name,",
            "                    file_id=file_id,",
            "                    thumbnail=True,",
            "                    thumbnail_width=t_width,",
            "                    thumbnail_height=t_height,",
            "                    thumbnail_method=t_method,",
            "                    thumbnail_type=t_type,",
            "                )",
            "",
            "                output_path = await self.media_storage.store_file(",
            "                    t_byte_source, file_info",
            "                )",
            "            finally:",
            "                t_byte_source.close()",
            "",
            "            logger.info(\"Stored thumbnail in file %r\", output_path)",
            "",
            "            t_len = os.path.getsize(output_path)",
            "",
            "            await self.store.store_remote_media_thumbnail(",
            "                server_name,",
            "                media_id,",
            "                file_id,",
            "                t_width,",
            "                t_height,",
            "                t_type,",
            "                t_method,",
            "                t_len,",
            "            )",
            "",
            "            return output_path",
            "",
            "        # Could not generate thumbnail.",
            "        return None",
            "",
            "    async def _generate_thumbnails(",
            "        self,",
            "        server_name: Optional[str],",
            "        media_id: str,",
            "        file_id: str,",
            "        media_type: str,",
            "        url_cache: bool = False,",
            "    ) -> Optional[dict]:",
            "        \"\"\"Generate and store thumbnails for an image.",
            "",
            "        Args:",
            "            server_name: The server name if remote media, else None if local",
            "            media_id: The media ID of the content. (This is the same as",
            "                the file_id for local content)",
            "            file_id: Local file ID",
            "            media_type: The content type of the file",
            "            url_cache: If we are thumbnailing images downloaded for the URL cache,",
            "                used exclusively by the url previewer",
            "",
            "        Returns:",
            "            Dict with \"width\" and \"height\" keys of original image or None if the",
            "            media cannot be thumbnailed.",
            "        \"\"\"",
            "        requirements = self._get_thumbnail_requirements(media_type)",
            "        if not requirements:",
            "            return None",
            "",
            "        input_path = await self.media_storage.ensure_media_is_in_local_cache(",
            "            FileInfo(server_name, file_id, url_cache=url_cache)",
            "        )",
            "",
            "        try:",
            "            thumbnailer = Thumbnailer(input_path)",
            "        except ThumbnailError as e:",
            "            logger.warning(",
            "                \"Unable to generate thumbnails for remote media %s from %s of type %s: %s\",",
            "                media_id,",
            "                server_name,",
            "                media_type,",
            "                e,",
            "            )",
            "            return None",
            "",
            "        m_width = thumbnailer.width",
            "        m_height = thumbnailer.height",
            "",
            "        if m_width * m_height >= self.max_image_pixels:",
            "            logger.info(",
            "                \"Image too large to thumbnail %r x %r > %r\",",
            "                m_width,",
            "                m_height,",
            "                self.max_image_pixels,",
            "            )",
            "            return None",
            "",
            "        if thumbnailer.transpose_method is not None:",
            "            m_width, m_height = await defer_to_thread(",
            "                self.hs.get_reactor(), thumbnailer.transpose",
            "            )",
            "",
            "        # We deduplicate the thumbnail sizes by ignoring the cropped versions if",
            "        # they have the same dimensions of a scaled one.",
            "        thumbnails = {}  # type: Dict[Tuple[int, int, str], str]",
            "        for r_width, r_height, r_method, r_type in requirements:",
            "            if r_method == \"crop\":",
            "                thumbnails.setdefault((r_width, r_height, r_type), r_method)",
            "            elif r_method == \"scale\":",
            "                t_width, t_height = thumbnailer.aspect(r_width, r_height)",
            "                t_width = min(m_width, t_width)",
            "                t_height = min(m_height, t_height)",
            "                thumbnails[(t_width, t_height, r_type)] = r_method",
            "",
            "        # Now we generate the thumbnails for each dimension, store it",
            "        for (t_width, t_height, t_type), t_method in thumbnails.items():",
            "            # Generate the thumbnail",
            "            if t_method == \"crop\":",
            "                t_byte_source = await defer_to_thread(",
            "                    self.hs.get_reactor(), thumbnailer.crop, t_width, t_height, t_type",
            "                )",
            "            elif t_method == \"scale\":",
            "                t_byte_source = await defer_to_thread(",
            "                    self.hs.get_reactor(), thumbnailer.scale, t_width, t_height, t_type",
            "                )",
            "            else:",
            "                logger.error(\"Unrecognized method: %r\", t_method)",
            "                continue",
            "",
            "            if not t_byte_source:",
            "                continue",
            "",
            "            file_info = FileInfo(",
            "                server_name=server_name,",
            "                file_id=file_id,",
            "                thumbnail=True,",
            "                thumbnail_width=t_width,",
            "                thumbnail_height=t_height,",
            "                thumbnail_method=t_method,",
            "                thumbnail_type=t_type,",
            "                url_cache=url_cache,",
            "            )",
            "",
            "            with self.media_storage.store_into_file(file_info) as (f, fname, finish):",
            "                try:",
            "                    await self.media_storage.write_to_file(t_byte_source, f)",
            "                    await finish()",
            "                finally:",
            "                    t_byte_source.close()",
            "",
            "                t_len = os.path.getsize(fname)",
            "",
            "                # Write to database",
            "                if server_name:",
            "                    # Multiple remote media download requests can race (when",
            "                    # using multiple media repos), so this may throw a violation",
            "                    # constraint exception. If it does we'll delete the newly",
            "                    # generated thumbnail from disk (as we're in the ctx",
            "                    # manager).",
            "                    #",
            "                    # However: we've already called `finish()` so we may have",
            "                    # also written to the storage providers. This is preferable",
            "                    # to the alternative where we call `finish()` *after* this,",
            "                    # where we could end up having an entry in the DB but fail",
            "                    # to write the files to the storage providers.",
            "                    try:",
            "                        await self.store.store_remote_media_thumbnail(",
            "                            server_name,",
            "                            media_id,",
            "                            file_id,",
            "                            t_width,",
            "                            t_height,",
            "                            t_type,",
            "                            t_method,",
            "                            t_len,",
            "                        )",
            "                    except Exception as e:",
            "                        thumbnail_exists = await self.store.get_remote_media_thumbnail(",
            "                            server_name, media_id, t_width, t_height, t_type,",
            "                        )",
            "                        if not thumbnail_exists:",
            "                            raise e",
            "                else:",
            "                    await self.store.store_local_thumbnail(",
            "                        media_id, t_width, t_height, t_type, t_method, t_len",
            "                    )",
            "",
            "        return {\"width\": m_width, \"height\": m_height}",
            "",
            "    async def delete_old_remote_media(self, before_ts):",
            "        old_media = await self.store.get_remote_media_before(before_ts)",
            "",
            "        deleted = 0",
            "",
            "        for media in old_media:",
            "            origin = media[\"media_origin\"]",
            "            media_id = media[\"media_id\"]",
            "            file_id = media[\"filesystem_id\"]",
            "            key = (origin, media_id)",
            "",
            "            logger.info(\"Deleting: %r\", key)",
            "",
            "            # TODO: Should we delete from the backup store",
            "",
            "            with (await self.remote_media_linearizer.queue(key)):",
            "                full_path = self.filepaths.remote_media_filepath(origin, file_id)",
            "                try:",
            "                    os.remove(full_path)",
            "                except OSError as e:",
            "                    logger.warning(\"Failed to remove file: %r\", full_path)",
            "                    if e.errno == errno.ENOENT:",
            "                        pass",
            "                    else:",
            "                        continue",
            "",
            "                thumbnail_dir = self.filepaths.remote_media_thumbnail_dir(",
            "                    origin, file_id",
            "                )",
            "                shutil.rmtree(thumbnail_dir, ignore_errors=True)",
            "",
            "                await self.store.delete_remote_media(origin, media_id)",
            "                deleted += 1",
            "",
            "        return {\"deleted\": deleted}",
            "",
            "    async def delete_local_media(self, media_id: str) -> Tuple[List[str], int]:",
            "        \"\"\"",
            "        Delete the given local or remote media ID from this server",
            "",
            "        Args:",
            "            media_id: The media ID to delete.",
            "        Returns:",
            "            A tuple of (list of deleted media IDs, total deleted media IDs).",
            "        \"\"\"",
            "        return await self._remove_local_media_from_disk([media_id])",
            "",
            "    async def delete_old_local_media(",
            "        self, before_ts: int, size_gt: int = 0, keep_profiles: bool = True,",
            "    ) -> Tuple[List[str], int]:",
            "        \"\"\"",
            "        Delete local or remote media from this server by size and timestamp. Removes",
            "        media files, any thumbnails and cached URLs.",
            "",
            "        Args:",
            "            before_ts: Unix timestamp in ms.",
            "                       Files that were last used before this timestamp will be deleted",
            "            size_gt: Size of the media in bytes. Files that are larger will be deleted",
            "            keep_profiles: Switch to delete also files that are still used in image data",
            "                           (e.g user profile, room avatar)",
            "                           If false these files will be deleted",
            "        Returns:",
            "            A tuple of (list of deleted media IDs, total deleted media IDs).",
            "        \"\"\"",
            "        old_media = await self.store.get_local_media_before(",
            "            before_ts, size_gt, keep_profiles,",
            "        )",
            "        return await self._remove_local_media_from_disk(old_media)",
            "",
            "    async def _remove_local_media_from_disk(",
            "        self, media_ids: List[str]",
            "    ) -> Tuple[List[str], int]:",
            "        \"\"\"",
            "        Delete local or remote media from this server. Removes media files,",
            "        any thumbnails and cached URLs.",
            "",
            "        Args:",
            "            media_ids: List of media_id to delete",
            "        Returns:",
            "            A tuple of (list of deleted media IDs, total deleted media IDs).",
            "        \"\"\"",
            "        removed_media = []",
            "        for media_id in media_ids:",
            "            logger.info(\"Deleting media with ID '%s'\", media_id)",
            "            full_path = self.filepaths.local_media_filepath(media_id)",
            "            try:",
            "                os.remove(full_path)",
            "            except OSError as e:",
            "                logger.warning(\"Failed to remove file: %r: %s\", full_path, e)",
            "                if e.errno == errno.ENOENT:",
            "                    pass",
            "                else:",
            "                    continue",
            "",
            "            thumbnail_dir = self.filepaths.local_media_thumbnail_dir(media_id)",
            "            shutil.rmtree(thumbnail_dir, ignore_errors=True)",
            "",
            "            await self.store.delete_remote_media(self.server_name, media_id)",
            "",
            "            await self.store.delete_url_cache((media_id,))",
            "            await self.store.delete_url_cache_media((media_id,))",
            "",
            "            removed_media.append(media_id)",
            "",
            "        return removed_media, len(removed_media)",
            "",
            "",
            "class MediaRepositoryResource(Resource):",
            "    \"\"\"File uploading and downloading.",
            "",
            "    Uploads are POSTed to a resource which returns a token which is used to GET",
            "    the download::",
            "",
            "        => POST /_matrix/media/r0/upload HTTP/1.1",
            "           Content-Type: <media-type>",
            "           Content-Length: <content-length>",
            "",
            "           <media>",
            "",
            "        <= HTTP/1.1 200 OK",
            "           Content-Type: application/json",
            "",
            "           { \"content_uri\": \"mxc://<server-name>/<media-id>\" }",
            "",
            "        => GET /_matrix/media/r0/download/<server-name>/<media-id> HTTP/1.1",
            "",
            "        <= HTTP/1.1 200 OK",
            "           Content-Type: <media-type>",
            "           Content-Disposition: attachment;filename=<upload-filename>",
            "",
            "           <media>",
            "",
            "    Clients can get thumbnails by supplying a desired width and height and",
            "    thumbnailing method::",
            "",
            "        => GET /_matrix/media/r0/thumbnail/<server_name>",
            "                /<media-id>?width=<w>&height=<h>&method=<m> HTTP/1.1",
            "",
            "        <= HTTP/1.1 200 OK",
            "           Content-Type: image/jpeg or image/png",
            "",
            "           <thumbnail>",
            "",
            "    The thumbnail methods are \"crop\" and \"scale\". \"scale\" trys to return an",
            "    image where either the width or the height is smaller than the requested",
            "    size. The client should then scale and letterbox the image if it needs to",
            "    fit within a given rectangle. \"crop\" trys to return an image where the",
            "    width and height are close to the requested size and the aspect matches",
            "    the requested size. The client should scale the image if it needs to fit",
            "    within a given rectangle.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs):",
            "        # If we're not configured to use it, raise if we somehow got here.",
            "        if not hs.config.can_load_media_repo:",
            "            raise ConfigError(\"Synapse is not configured to use a media repo.\")",
            "",
            "        super().__init__()",
            "        media_repo = hs.get_media_repository()",
            "",
            "        self.putChild(b\"upload\", UploadResource(hs, media_repo))",
            "        self.putChild(b\"download\", DownloadResource(hs, media_repo))",
            "        self.putChild(",
            "            b\"thumbnail\", ThumbnailResource(hs, media_repo, media_repo.media_storage)",
            "        )",
            "        if hs.config.url_preview_enabled:",
            "            self.putChild(",
            "                b\"preview_url\",",
            "                PreviewUrlResource(hs, media_repo, media_repo.media_storage),",
            "            )",
            "        self.putChild(b\"config\", MediaConfigResource(hs))"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import errno",
            "import logging",
            "import os",
            "import shutil",
            "from typing import IO, Dict, List, Optional, Tuple",
            "",
            "import twisted.internet.error",
            "import twisted.web.http",
            "from twisted.web.http import Request",
            "from twisted.web.resource import Resource",
            "",
            "from synapse.api.errors import (",
            "    FederationDeniedError,",
            "    HttpResponseException,",
            "    NotFoundError,",
            "    RequestSendFailed,",
            "    SynapseError,",
            ")",
            "from synapse.config._base import ConfigError",
            "from synapse.logging.context import defer_to_thread",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.util.async_helpers import Linearizer",
            "from synapse.util.retryutils import NotRetryingDestination",
            "from synapse.util.stringutils import random_string",
            "",
            "from ._base import (",
            "    FileInfo,",
            "    Responder,",
            "    get_filename_from_headers,",
            "    respond_404,",
            "    respond_with_responder,",
            ")",
            "from .config_resource import MediaConfigResource",
            "from .download_resource import DownloadResource",
            "from .filepath import MediaFilePaths",
            "from .media_storage import MediaStorage",
            "from .preview_url_resource import PreviewUrlResource",
            "from .storage_provider import StorageProviderWrapper",
            "from .thumbnail_resource import ThumbnailResource",
            "from .thumbnailer import Thumbnailer, ThumbnailError",
            "from .upload_resource import UploadResource",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "UPDATE_RECENTLY_ACCESSED_TS = 60 * 1000",
            "",
            "",
            "class MediaRepository:",
            "    def __init__(self, hs):",
            "        self.hs = hs",
            "        self.auth = hs.get_auth()",
            "        self.client = hs.get_federation_http_client()",
            "        self.clock = hs.get_clock()",
            "        self.server_name = hs.hostname",
            "        self.store = hs.get_datastore()",
            "        self.max_upload_size = hs.config.max_upload_size",
            "        self.max_image_pixels = hs.config.max_image_pixels",
            "",
            "        self.primary_base_path = hs.config.media_store_path",
            "        self.filepaths = MediaFilePaths(self.primary_base_path)",
            "",
            "        self.dynamic_thumbnails = hs.config.dynamic_thumbnails",
            "        self.thumbnail_requirements = hs.config.thumbnail_requirements",
            "",
            "        self.remote_media_linearizer = Linearizer(name=\"media_remote\")",
            "",
            "        self.recently_accessed_remotes = set()",
            "        self.recently_accessed_locals = set()",
            "",
            "        self.federation_domain_whitelist = hs.config.federation_domain_whitelist",
            "",
            "        # List of StorageProviders where we should search for media and",
            "        # potentially upload to.",
            "        storage_providers = []",
            "",
            "        for clz, provider_config, wrapper_config in hs.config.media_storage_providers:",
            "            backend = clz(hs, provider_config)",
            "            provider = StorageProviderWrapper(",
            "                backend,",
            "                store_local=wrapper_config.store_local,",
            "                store_remote=wrapper_config.store_remote,",
            "                store_synchronous=wrapper_config.store_synchronous,",
            "            )",
            "            storage_providers.append(provider)",
            "",
            "        self.media_storage = MediaStorage(",
            "            self.hs, self.primary_base_path, self.filepaths, storage_providers",
            "        )",
            "",
            "        self.clock.looping_call(",
            "            self._start_update_recently_accessed, UPDATE_RECENTLY_ACCESSED_TS",
            "        )",
            "",
            "    def _start_update_recently_accessed(self):",
            "        return run_as_background_process(",
            "            \"update_recently_accessed_media\", self._update_recently_accessed",
            "        )",
            "",
            "    async def _update_recently_accessed(self):",
            "        remote_media = self.recently_accessed_remotes",
            "        self.recently_accessed_remotes = set()",
            "",
            "        local_media = self.recently_accessed_locals",
            "        self.recently_accessed_locals = set()",
            "",
            "        await self.store.update_cached_last_access_time(",
            "            local_media, remote_media, self.clock.time_msec()",
            "        )",
            "",
            "    def mark_recently_accessed(self, server_name, media_id):",
            "        \"\"\"Mark the given media as recently accessed.",
            "",
            "        Args:",
            "            server_name (str|None): Origin server of media, or None if local",
            "            media_id (str): The media ID of the content",
            "        \"\"\"",
            "        if server_name:",
            "            self.recently_accessed_remotes.add((server_name, media_id))",
            "        else:",
            "            self.recently_accessed_locals.add(media_id)",
            "",
            "    async def create_content(",
            "        self,",
            "        media_type: str,",
            "        upload_name: Optional[str],",
            "        content: IO,",
            "        content_length: int,",
            "        auth_user: str,",
            "    ) -> str:",
            "        \"\"\"Store uploaded content for a local user and return the mxc URL",
            "",
            "        Args:",
            "            media_type: The content type of the file.",
            "            upload_name: The name of the file, if provided.",
            "            content: A file like object that is the content to store",
            "            content_length: The length of the content",
            "            auth_user: The user_id of the uploader",
            "",
            "        Returns:",
            "            The mxc url of the stored content",
            "        \"\"\"",
            "",
            "        media_id = random_string(24)",
            "",
            "        file_info = FileInfo(server_name=None, file_id=media_id)",
            "",
            "        fname = await self.media_storage.store_file(content, file_info)",
            "",
            "        logger.info(\"Stored local media in file %r\", fname)",
            "",
            "        await self.store.store_local_media(",
            "            media_id=media_id,",
            "            media_type=media_type,",
            "            time_now_ms=self.clock.time_msec(),",
            "            upload_name=upload_name,",
            "            media_length=content_length,",
            "            user_id=auth_user,",
            "        )",
            "",
            "        await self._generate_thumbnails(None, media_id, media_id, media_type)",
            "",
            "        return \"mxc://%s/%s\" % (self.server_name, media_id)",
            "",
            "    async def get_local_media(",
            "        self, request: Request, media_id: str, name: Optional[str]",
            "    ) -> None:",
            "        \"\"\"Responds to reqests for local media, if exists, or returns 404.",
            "",
            "        Args:",
            "            request: The incoming request.",
            "            media_id: The media ID of the content. (This is the same as",
            "                the file_id for local content.)",
            "            name: Optional name that, if specified, will be used as",
            "                the filename in the Content-Disposition header of the response.",
            "",
            "        Returns:",
            "            Resolves once a response has successfully been written to request",
            "        \"\"\"",
            "        media_info = await self.store.get_local_media(media_id)",
            "        if not media_info or media_info[\"quarantined_by\"]:",
            "            respond_404(request)",
            "            return",
            "",
            "        self.mark_recently_accessed(None, media_id)",
            "",
            "        media_type = media_info[\"media_type\"]",
            "        media_length = media_info[\"media_length\"]",
            "        upload_name = name if name else media_info[\"upload_name\"]",
            "        url_cache = media_info[\"url_cache\"]",
            "",
            "        file_info = FileInfo(None, media_id, url_cache=url_cache)",
            "",
            "        responder = await self.media_storage.fetch_media(file_info)",
            "        await respond_with_responder(",
            "            request, responder, media_type, media_length, upload_name",
            "        )",
            "",
            "    async def get_remote_media(",
            "        self, request: Request, server_name: str, media_id: str, name: Optional[str]",
            "    ) -> None:",
            "        \"\"\"Respond to requests for remote media.",
            "",
            "        Args:",
            "            request: The incoming request.",
            "            server_name: Remote server_name where the media originated.",
            "            media_id: The media ID of the content (as defined by the remote server).",
            "            name: Optional name that, if specified, will be used as",
            "                the filename in the Content-Disposition header of the response.",
            "",
            "        Returns:",
            "            Resolves once a response has successfully been written to request",
            "        \"\"\"",
            "        if (",
            "            self.federation_domain_whitelist is not None",
            "            and server_name not in self.federation_domain_whitelist",
            "        ):",
            "            raise FederationDeniedError(server_name)",
            "",
            "        self.mark_recently_accessed(server_name, media_id)",
            "",
            "        # We linearize here to ensure that we don't try and download remote",
            "        # media multiple times concurrently",
            "        key = (server_name, media_id)",
            "        with (await self.remote_media_linearizer.queue(key)):",
            "            responder, media_info = await self._get_remote_media_impl(",
            "                server_name, media_id",
            "            )",
            "",
            "        # We deliberately stream the file outside the lock",
            "        if responder:",
            "            media_type = media_info[\"media_type\"]",
            "            media_length = media_info[\"media_length\"]",
            "            upload_name = name if name else media_info[\"upload_name\"]",
            "            await respond_with_responder(",
            "                request, responder, media_type, media_length, upload_name",
            "            )",
            "        else:",
            "            respond_404(request)",
            "",
            "    async def get_remote_media_info(self, server_name: str, media_id: str) -> dict:",
            "        \"\"\"Gets the media info associated with the remote file, downloading",
            "        if necessary.",
            "",
            "        Args:",
            "            server_name: Remote server_name where the media originated.",
            "            media_id: The media ID of the content (as defined by the remote server).",
            "",
            "        Returns:",
            "            The media info of the file",
            "        \"\"\"",
            "        if (",
            "            self.federation_domain_whitelist is not None",
            "            and server_name not in self.federation_domain_whitelist",
            "        ):",
            "            raise FederationDeniedError(server_name)",
            "",
            "        # We linearize here to ensure that we don't try and download remote",
            "        # media multiple times concurrently",
            "        key = (server_name, media_id)",
            "        with (await self.remote_media_linearizer.queue(key)):",
            "            responder, media_info = await self._get_remote_media_impl(",
            "                server_name, media_id",
            "            )",
            "",
            "        # Ensure we actually use the responder so that it releases resources",
            "        if responder:",
            "            with responder:",
            "                pass",
            "",
            "        return media_info",
            "",
            "    async def _get_remote_media_impl(",
            "        self, server_name: str, media_id: str",
            "    ) -> Tuple[Optional[Responder], dict]:",
            "        \"\"\"Looks for media in local cache, if not there then attempt to",
            "        download from remote server.",
            "",
            "        Args:",
            "            server_name (str): Remote server_name where the media originated.",
            "            media_id (str): The media ID of the content (as defined by the",
            "                remote server).",
            "",
            "        Returns:",
            "            A tuple of responder and the media info of the file.",
            "        \"\"\"",
            "        media_info = await self.store.get_cached_remote_media(server_name, media_id)",
            "",
            "        # file_id is the ID we use to track the file locally. If we've already",
            "        # seen the file then reuse the existing ID, otherwise genereate a new",
            "        # one.",
            "",
            "        # If we have an entry in the DB, try and look for it",
            "        if media_info:",
            "            file_id = media_info[\"filesystem_id\"]",
            "            file_info = FileInfo(server_name, file_id)",
            "",
            "            if media_info[\"quarantined_by\"]:",
            "                logger.info(\"Media is quarantined\")",
            "                raise NotFoundError()",
            "",
            "            responder = await self.media_storage.fetch_media(file_info)",
            "            if responder:",
            "                return responder, media_info",
            "",
            "        # Failed to find the file anywhere, lets download it.",
            "",
            "        try:",
            "            media_info = await self._download_remote_file(server_name, media_id,)",
            "        except SynapseError:",
            "            raise",
            "        except Exception as e:",
            "            # An exception may be because we downloaded media in another",
            "            # process, so let's check if we magically have the media.",
            "            media_info = await self.store.get_cached_remote_media(server_name, media_id)",
            "            if not media_info:",
            "                raise e",
            "",
            "        file_id = media_info[\"filesystem_id\"]",
            "        file_info = FileInfo(server_name, file_id)",
            "",
            "        # We generate thumbnails even if another process downloaded the media",
            "        # as a) it's conceivable that the other download request dies before it",
            "        # generates thumbnails, but mainly b) we want to be sure the thumbnails",
            "        # have finished being generated before responding to the client,",
            "        # otherwise they'll request thumbnails and get a 404 if they're not",
            "        # ready yet.",
            "        await self._generate_thumbnails(",
            "            server_name, media_id, file_id, media_info[\"media_type\"]",
            "        )",
            "",
            "        responder = await self.media_storage.fetch_media(file_info)",
            "        return responder, media_info",
            "",
            "    async def _download_remote_file(self, server_name: str, media_id: str,) -> dict:",
            "        \"\"\"Attempt to download the remote file from the given server name,",
            "        using the given file_id as the local id.",
            "",
            "        Args:",
            "            server_name: Originating server",
            "            media_id: The media ID of the content (as defined by the",
            "                remote server). This is different than the file_id, which is",
            "                locally generated.",
            "            file_id: Local file ID",
            "",
            "        Returns:",
            "            The media info of the file.",
            "        \"\"\"",
            "",
            "        file_id = random_string(24)",
            "",
            "        file_info = FileInfo(server_name=server_name, file_id=file_id)",
            "",
            "        with self.media_storage.store_into_file(file_info) as (f, fname, finish):",
            "            request_path = \"/\".join(",
            "                (\"/_matrix/media/r0/download\", server_name, media_id)",
            "            )",
            "            try:",
            "                length, headers = await self.client.get_file(",
            "                    server_name,",
            "                    request_path,",
            "                    output_stream=f,",
            "                    max_size=self.max_upload_size,",
            "                    args={",
            "                        # tell the remote server to 404 if it doesn't",
            "                        # recognise the server_name, to make sure we don't",
            "                        # end up with a routing loop.",
            "                        \"allow_remote\": \"false\"",
            "                    },",
            "                )",
            "            except RequestSendFailed as e:",
            "                logger.warning(",
            "                    \"Request failed fetching remote media %s/%s: %r\",",
            "                    server_name,",
            "                    media_id,",
            "                    e,",
            "                )",
            "                raise SynapseError(502, \"Failed to fetch remote media\")",
            "",
            "            except HttpResponseException as e:",
            "                logger.warning(",
            "                    \"HTTP error fetching remote media %s/%s: %s\",",
            "                    server_name,",
            "                    media_id,",
            "                    e.response,",
            "                )",
            "                if e.code == twisted.web.http.NOT_FOUND:",
            "                    raise e.to_synapse_error()",
            "                raise SynapseError(502, \"Failed to fetch remote media\")",
            "",
            "            except SynapseError:",
            "                logger.warning(",
            "                    \"Failed to fetch remote media %s/%s\", server_name, media_id",
            "                )",
            "                raise",
            "            except NotRetryingDestination:",
            "                logger.warning(\"Not retrying destination %r\", server_name)",
            "                raise SynapseError(502, \"Failed to fetch remote media\")",
            "            except Exception:",
            "                logger.exception(",
            "                    \"Failed to fetch remote media %s/%s\", server_name, media_id",
            "                )",
            "                raise SynapseError(502, \"Failed to fetch remote media\")",
            "",
            "            await finish()",
            "",
            "            media_type = headers[b\"Content-Type\"][0].decode(\"ascii\")",
            "            upload_name = get_filename_from_headers(headers)",
            "            time_now_ms = self.clock.time_msec()",
            "",
            "            # Multiple remote media download requests can race (when using",
            "            # multiple media repos), so this may throw a violation constraint",
            "            # exception. If it does we'll delete the newly downloaded file from",
            "            # disk (as we're in the ctx manager).",
            "            #",
            "            # However: we've already called `finish()` so we may have also",
            "            # written to the storage providers. This is preferable to the",
            "            # alternative where we call `finish()` *after* this, where we could",
            "            # end up having an entry in the DB but fail to write the files to",
            "            # the storage providers.",
            "            await self.store.store_cached_remote_media(",
            "                origin=server_name,",
            "                media_id=media_id,",
            "                media_type=media_type,",
            "                time_now_ms=self.clock.time_msec(),",
            "                upload_name=upload_name,",
            "                media_length=length,",
            "                filesystem_id=file_id,",
            "            )",
            "",
            "        logger.info(\"Stored remote media in file %r\", fname)",
            "",
            "        media_info = {",
            "            \"media_type\": media_type,",
            "            \"media_length\": length,",
            "            \"upload_name\": upload_name,",
            "            \"created_ts\": time_now_ms,",
            "            \"filesystem_id\": file_id,",
            "        }",
            "",
            "        return media_info",
            "",
            "    def _get_thumbnail_requirements(self, media_type):",
            "        return self.thumbnail_requirements.get(media_type, ())",
            "",
            "    def _generate_thumbnail(self, thumbnailer, t_width, t_height, t_method, t_type):",
            "        m_width = thumbnailer.width",
            "        m_height = thumbnailer.height",
            "",
            "        if m_width * m_height >= self.max_image_pixels:",
            "            logger.info(",
            "                \"Image too large to thumbnail %r x %r > %r\",",
            "                m_width,",
            "                m_height,",
            "                self.max_image_pixels,",
            "            )",
            "            return",
            "",
            "        if thumbnailer.transpose_method is not None:",
            "            m_width, m_height = thumbnailer.transpose()",
            "",
            "        if t_method == \"crop\":",
            "            t_byte_source = thumbnailer.crop(t_width, t_height, t_type)",
            "        elif t_method == \"scale\":",
            "            t_width, t_height = thumbnailer.aspect(t_width, t_height)",
            "            t_width = min(m_width, t_width)",
            "            t_height = min(m_height, t_height)",
            "            t_byte_source = thumbnailer.scale(t_width, t_height, t_type)",
            "        else:",
            "            t_byte_source = None",
            "",
            "        return t_byte_source",
            "",
            "    async def generate_local_exact_thumbnail(",
            "        self,",
            "        media_id: str,",
            "        t_width: int,",
            "        t_height: int,",
            "        t_method: str,",
            "        t_type: str,",
            "        url_cache: str,",
            "    ) -> Optional[str]:",
            "        input_path = await self.media_storage.ensure_media_is_in_local_cache(",
            "            FileInfo(None, media_id, url_cache=url_cache)",
            "        )",
            "",
            "        try:",
            "            thumbnailer = Thumbnailer(input_path)",
            "        except ThumbnailError as e:",
            "            logger.warning(",
            "                \"Unable to generate a thumbnail for local media %s using a method of %s and type of %s: %s\",",
            "                media_id,",
            "                t_method,",
            "                t_type,",
            "                e,",
            "            )",
            "            return None",
            "",
            "        t_byte_source = await defer_to_thread(",
            "            self.hs.get_reactor(),",
            "            self._generate_thumbnail,",
            "            thumbnailer,",
            "            t_width,",
            "            t_height,",
            "            t_method,",
            "            t_type,",
            "        )",
            "",
            "        if t_byte_source:",
            "            try:",
            "                file_info = FileInfo(",
            "                    server_name=None,",
            "                    file_id=media_id,",
            "                    url_cache=url_cache,",
            "                    thumbnail=True,",
            "                    thumbnail_width=t_width,",
            "                    thumbnail_height=t_height,",
            "                    thumbnail_method=t_method,",
            "                    thumbnail_type=t_type,",
            "                )",
            "",
            "                output_path = await self.media_storage.store_file(",
            "                    t_byte_source, file_info",
            "                )",
            "            finally:",
            "                t_byte_source.close()",
            "",
            "            logger.info(\"Stored thumbnail in file %r\", output_path)",
            "",
            "            t_len = os.path.getsize(output_path)",
            "",
            "            await self.store.store_local_thumbnail(",
            "                media_id, t_width, t_height, t_type, t_method, t_len",
            "            )",
            "",
            "            return output_path",
            "",
            "        # Could not generate thumbnail.",
            "        return None",
            "",
            "    async def generate_remote_exact_thumbnail(",
            "        self,",
            "        server_name: str,",
            "        file_id: str,",
            "        media_id: str,",
            "        t_width: int,",
            "        t_height: int,",
            "        t_method: str,",
            "        t_type: str,",
            "    ) -> Optional[str]:",
            "        input_path = await self.media_storage.ensure_media_is_in_local_cache(",
            "            FileInfo(server_name, file_id, url_cache=False)",
            "        )",
            "",
            "        try:",
            "            thumbnailer = Thumbnailer(input_path)",
            "        except ThumbnailError as e:",
            "            logger.warning(",
            "                \"Unable to generate a thumbnail for remote media %s from %s using a method of %s and type of %s: %s\",",
            "                media_id,",
            "                server_name,",
            "                t_method,",
            "                t_type,",
            "                e,",
            "            )",
            "            return None",
            "",
            "        t_byte_source = await defer_to_thread(",
            "            self.hs.get_reactor(),",
            "            self._generate_thumbnail,",
            "            thumbnailer,",
            "            t_width,",
            "            t_height,",
            "            t_method,",
            "            t_type,",
            "        )",
            "",
            "        if t_byte_source:",
            "            try:",
            "                file_info = FileInfo(",
            "                    server_name=server_name,",
            "                    file_id=file_id,",
            "                    thumbnail=True,",
            "                    thumbnail_width=t_width,",
            "                    thumbnail_height=t_height,",
            "                    thumbnail_method=t_method,",
            "                    thumbnail_type=t_type,",
            "                )",
            "",
            "                output_path = await self.media_storage.store_file(",
            "                    t_byte_source, file_info",
            "                )",
            "            finally:",
            "                t_byte_source.close()",
            "",
            "            logger.info(\"Stored thumbnail in file %r\", output_path)",
            "",
            "            t_len = os.path.getsize(output_path)",
            "",
            "            await self.store.store_remote_media_thumbnail(",
            "                server_name,",
            "                media_id,",
            "                file_id,",
            "                t_width,",
            "                t_height,",
            "                t_type,",
            "                t_method,",
            "                t_len,",
            "            )",
            "",
            "            return output_path",
            "",
            "        # Could not generate thumbnail.",
            "        return None",
            "",
            "    async def _generate_thumbnails(",
            "        self,",
            "        server_name: Optional[str],",
            "        media_id: str,",
            "        file_id: str,",
            "        media_type: str,",
            "        url_cache: bool = False,",
            "    ) -> Optional[dict]:",
            "        \"\"\"Generate and store thumbnails for an image.",
            "",
            "        Args:",
            "            server_name: The server name if remote media, else None if local",
            "            media_id: The media ID of the content. (This is the same as",
            "                the file_id for local content)",
            "            file_id: Local file ID",
            "            media_type: The content type of the file",
            "            url_cache: If we are thumbnailing images downloaded for the URL cache,",
            "                used exclusively by the url previewer",
            "",
            "        Returns:",
            "            Dict with \"width\" and \"height\" keys of original image or None if the",
            "            media cannot be thumbnailed.",
            "        \"\"\"",
            "        requirements = self._get_thumbnail_requirements(media_type)",
            "        if not requirements:",
            "            return None",
            "",
            "        input_path = await self.media_storage.ensure_media_is_in_local_cache(",
            "            FileInfo(server_name, file_id, url_cache=url_cache)",
            "        )",
            "",
            "        try:",
            "            thumbnailer = Thumbnailer(input_path)",
            "        except ThumbnailError as e:",
            "            logger.warning(",
            "                \"Unable to generate thumbnails for remote media %s from %s of type %s: %s\",",
            "                media_id,",
            "                server_name,",
            "                media_type,",
            "                e,",
            "            )",
            "            return None",
            "",
            "        m_width = thumbnailer.width",
            "        m_height = thumbnailer.height",
            "",
            "        if m_width * m_height >= self.max_image_pixels:",
            "            logger.info(",
            "                \"Image too large to thumbnail %r x %r > %r\",",
            "                m_width,",
            "                m_height,",
            "                self.max_image_pixels,",
            "            )",
            "            return None",
            "",
            "        if thumbnailer.transpose_method is not None:",
            "            m_width, m_height = await defer_to_thread(",
            "                self.hs.get_reactor(), thumbnailer.transpose",
            "            )",
            "",
            "        # We deduplicate the thumbnail sizes by ignoring the cropped versions if",
            "        # they have the same dimensions of a scaled one.",
            "        thumbnails = {}  # type: Dict[Tuple[int, int, str], str]",
            "        for r_width, r_height, r_method, r_type in requirements:",
            "            if r_method == \"crop\":",
            "                thumbnails.setdefault((r_width, r_height, r_type), r_method)",
            "            elif r_method == \"scale\":",
            "                t_width, t_height = thumbnailer.aspect(r_width, r_height)",
            "                t_width = min(m_width, t_width)",
            "                t_height = min(m_height, t_height)",
            "                thumbnails[(t_width, t_height, r_type)] = r_method",
            "",
            "        # Now we generate the thumbnails for each dimension, store it",
            "        for (t_width, t_height, t_type), t_method in thumbnails.items():",
            "            # Generate the thumbnail",
            "            if t_method == \"crop\":",
            "                t_byte_source = await defer_to_thread(",
            "                    self.hs.get_reactor(), thumbnailer.crop, t_width, t_height, t_type",
            "                )",
            "            elif t_method == \"scale\":",
            "                t_byte_source = await defer_to_thread(",
            "                    self.hs.get_reactor(), thumbnailer.scale, t_width, t_height, t_type",
            "                )",
            "            else:",
            "                logger.error(\"Unrecognized method: %r\", t_method)",
            "                continue",
            "",
            "            if not t_byte_source:",
            "                continue",
            "",
            "            file_info = FileInfo(",
            "                server_name=server_name,",
            "                file_id=file_id,",
            "                thumbnail=True,",
            "                thumbnail_width=t_width,",
            "                thumbnail_height=t_height,",
            "                thumbnail_method=t_method,",
            "                thumbnail_type=t_type,",
            "                url_cache=url_cache,",
            "            )",
            "",
            "            with self.media_storage.store_into_file(file_info) as (f, fname, finish):",
            "                try:",
            "                    await self.media_storage.write_to_file(t_byte_source, f)",
            "                    await finish()",
            "                finally:",
            "                    t_byte_source.close()",
            "",
            "                t_len = os.path.getsize(fname)",
            "",
            "                # Write to database",
            "                if server_name:",
            "                    # Multiple remote media download requests can race (when",
            "                    # using multiple media repos), so this may throw a violation",
            "                    # constraint exception. If it does we'll delete the newly",
            "                    # generated thumbnail from disk (as we're in the ctx",
            "                    # manager).",
            "                    #",
            "                    # However: we've already called `finish()` so we may have",
            "                    # also written to the storage providers. This is preferable",
            "                    # to the alternative where we call `finish()` *after* this,",
            "                    # where we could end up having an entry in the DB but fail",
            "                    # to write the files to the storage providers.",
            "                    try:",
            "                        await self.store.store_remote_media_thumbnail(",
            "                            server_name,",
            "                            media_id,",
            "                            file_id,",
            "                            t_width,",
            "                            t_height,",
            "                            t_type,",
            "                            t_method,",
            "                            t_len,",
            "                        )",
            "                    except Exception as e:",
            "                        thumbnail_exists = await self.store.get_remote_media_thumbnail(",
            "                            server_name, media_id, t_width, t_height, t_type,",
            "                        )",
            "                        if not thumbnail_exists:",
            "                            raise e",
            "                else:",
            "                    await self.store.store_local_thumbnail(",
            "                        media_id, t_width, t_height, t_type, t_method, t_len",
            "                    )",
            "",
            "        return {\"width\": m_width, \"height\": m_height}",
            "",
            "    async def delete_old_remote_media(self, before_ts):",
            "        old_media = await self.store.get_remote_media_before(before_ts)",
            "",
            "        deleted = 0",
            "",
            "        for media in old_media:",
            "            origin = media[\"media_origin\"]",
            "            media_id = media[\"media_id\"]",
            "            file_id = media[\"filesystem_id\"]",
            "            key = (origin, media_id)",
            "",
            "            logger.info(\"Deleting: %r\", key)",
            "",
            "            # TODO: Should we delete from the backup store",
            "",
            "            with (await self.remote_media_linearizer.queue(key)):",
            "                full_path = self.filepaths.remote_media_filepath(origin, file_id)",
            "                try:",
            "                    os.remove(full_path)",
            "                except OSError as e:",
            "                    logger.warning(\"Failed to remove file: %r\", full_path)",
            "                    if e.errno == errno.ENOENT:",
            "                        pass",
            "                    else:",
            "                        continue",
            "",
            "                thumbnail_dir = self.filepaths.remote_media_thumbnail_dir(",
            "                    origin, file_id",
            "                )",
            "                shutil.rmtree(thumbnail_dir, ignore_errors=True)",
            "",
            "                await self.store.delete_remote_media(origin, media_id)",
            "                deleted += 1",
            "",
            "        return {\"deleted\": deleted}",
            "",
            "    async def delete_local_media(self, media_id: str) -> Tuple[List[str], int]:",
            "        \"\"\"",
            "        Delete the given local or remote media ID from this server",
            "",
            "        Args:",
            "            media_id: The media ID to delete.",
            "        Returns:",
            "            A tuple of (list of deleted media IDs, total deleted media IDs).",
            "        \"\"\"",
            "        return await self._remove_local_media_from_disk([media_id])",
            "",
            "    async def delete_old_local_media(",
            "        self, before_ts: int, size_gt: int = 0, keep_profiles: bool = True,",
            "    ) -> Tuple[List[str], int]:",
            "        \"\"\"",
            "        Delete local or remote media from this server by size and timestamp. Removes",
            "        media files, any thumbnails and cached URLs.",
            "",
            "        Args:",
            "            before_ts: Unix timestamp in ms.",
            "                       Files that were last used before this timestamp will be deleted",
            "            size_gt: Size of the media in bytes. Files that are larger will be deleted",
            "            keep_profiles: Switch to delete also files that are still used in image data",
            "                           (e.g user profile, room avatar)",
            "                           If false these files will be deleted",
            "        Returns:",
            "            A tuple of (list of deleted media IDs, total deleted media IDs).",
            "        \"\"\"",
            "        old_media = await self.store.get_local_media_before(",
            "            before_ts, size_gt, keep_profiles,",
            "        )",
            "        return await self._remove_local_media_from_disk(old_media)",
            "",
            "    async def _remove_local_media_from_disk(",
            "        self, media_ids: List[str]",
            "    ) -> Tuple[List[str], int]:",
            "        \"\"\"",
            "        Delete local or remote media from this server. Removes media files,",
            "        any thumbnails and cached URLs.",
            "",
            "        Args:",
            "            media_ids: List of media_id to delete",
            "        Returns:",
            "            A tuple of (list of deleted media IDs, total deleted media IDs).",
            "        \"\"\"",
            "        removed_media = []",
            "        for media_id in media_ids:",
            "            logger.info(\"Deleting media with ID '%s'\", media_id)",
            "            full_path = self.filepaths.local_media_filepath(media_id)",
            "            try:",
            "                os.remove(full_path)",
            "            except OSError as e:",
            "                logger.warning(\"Failed to remove file: %r: %s\", full_path, e)",
            "                if e.errno == errno.ENOENT:",
            "                    pass",
            "                else:",
            "                    continue",
            "",
            "            thumbnail_dir = self.filepaths.local_media_thumbnail_dir(media_id)",
            "            shutil.rmtree(thumbnail_dir, ignore_errors=True)",
            "",
            "            await self.store.delete_remote_media(self.server_name, media_id)",
            "",
            "            await self.store.delete_url_cache((media_id,))",
            "            await self.store.delete_url_cache_media((media_id,))",
            "",
            "            removed_media.append(media_id)",
            "",
            "        return removed_media, len(removed_media)",
            "",
            "",
            "class MediaRepositoryResource(Resource):",
            "    \"\"\"File uploading and downloading.",
            "",
            "    Uploads are POSTed to a resource which returns a token which is used to GET",
            "    the download::",
            "",
            "        => POST /_matrix/media/r0/upload HTTP/1.1",
            "           Content-Type: <media-type>",
            "           Content-Length: <content-length>",
            "",
            "           <media>",
            "",
            "        <= HTTP/1.1 200 OK",
            "           Content-Type: application/json",
            "",
            "           { \"content_uri\": \"mxc://<server-name>/<media-id>\" }",
            "",
            "        => GET /_matrix/media/r0/download/<server-name>/<media-id> HTTP/1.1",
            "",
            "        <= HTTP/1.1 200 OK",
            "           Content-Type: <media-type>",
            "           Content-Disposition: attachment;filename=<upload-filename>",
            "",
            "           <media>",
            "",
            "    Clients can get thumbnails by supplying a desired width and height and",
            "    thumbnailing method::",
            "",
            "        => GET /_matrix/media/r0/thumbnail/<server_name>",
            "                /<media-id>?width=<w>&height=<h>&method=<m> HTTP/1.1",
            "",
            "        <= HTTP/1.1 200 OK",
            "           Content-Type: image/jpeg or image/png",
            "",
            "           <thumbnail>",
            "",
            "    The thumbnail methods are \"crop\" and \"scale\". \"scale\" trys to return an",
            "    image where either the width or the height is smaller than the requested",
            "    size. The client should then scale and letterbox the image if it needs to",
            "    fit within a given rectangle. \"crop\" trys to return an image where the",
            "    width and height are close to the requested size and the aspect matches",
            "    the requested size. The client should scale the image if it needs to fit",
            "    within a given rectangle.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs):",
            "        # If we're not configured to use it, raise if we somehow got here.",
            "        if not hs.config.can_load_media_repo:",
            "            raise ConfigError(\"Synapse is not configured to use a media repo.\")",
            "",
            "        super().__init__()",
            "        media_repo = hs.get_media_repository()",
            "",
            "        self.putChild(b\"upload\", UploadResource(hs, media_repo))",
            "        self.putChild(b\"download\", DownloadResource(hs, media_repo))",
            "        self.putChild(",
            "            b\"thumbnail\", ThumbnailResource(hs, media_repo, media_repo.media_storage)",
            "        )",
            "        if hs.config.url_preview_enabled:",
            "            self.putChild(",
            "                b\"preview_url\",",
            "                PreviewUrlResource(hs, media_repo, media_repo.media_storage),",
            "            )",
            "        self.putChild(b\"config\", MediaConfigResource(hs))"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "69": [
                "MediaRepository",
                "__init__"
            ]
        },
        "addLocation": []
    }
}