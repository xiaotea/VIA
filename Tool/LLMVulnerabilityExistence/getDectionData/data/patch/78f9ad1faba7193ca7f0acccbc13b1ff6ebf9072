{
    "attic/archiver.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 62,
                "PatchRowcode": "         manifest.key = key"
            },
            "1": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 63,
                "PatchRowcode": "         manifest.write()"
            },
            "2": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 64,
                "PatchRowcode": "         repository.commit()"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 65,
                "PatchRowcode": "+        Cache(repository, key, manifest, warn_if_unencrypted=False)"
            },
            "4": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 66,
                "PatchRowcode": "         return self.exit_code"
            },
            "5": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 67,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 68,
                "PatchRowcode": "     def do_check(self, args):"
            }
        },
        "frontPatchFile": [
            "import argparse",
            "from binascii import hexlify",
            "from datetime import datetime",
            "from operator import attrgetter",
            "import functools",
            "import io",
            "import os",
            "import stat",
            "import sys",
            "import textwrap",
            "",
            "from attic import __version__",
            "from attic.archive import Archive, ArchiveChecker",
            "from attic.repository import Repository",
            "from attic.cache import Cache",
            "from attic.key import key_creator",
            "from attic.helpers import Error, location_validator, format_time, \\",
            "    format_file_mode, ExcludePattern, exclude_path, adjust_patterns, to_localtime, \\",
            "    get_cache_dir, get_keys_dir, format_timedelta, prune_within, prune_split, \\",
            "    Manifest, remove_surrogates, update_excludes, format_archive, check_extension_modules, Statistics, \\",
            "    is_cachedir, bigint_to_int",
            "from attic.remote import RepositoryServer, RemoteRepository",
            "",
            "",
            "class Archiver:",
            "",
            "    def __init__(self):",
            "        self.exit_code = 0",
            "",
            "    def open_repository(self, location, create=False, exclusive=False):",
            "        if location.proto == 'ssh':",
            "            repository = RemoteRepository(location, create=create)",
            "        else:",
            "            repository = Repository(location.path, create=create, exclusive=exclusive)",
            "        repository._location = location",
            "        return repository",
            "",
            "    def print_error(self, msg, *args):",
            "        msg = args and msg % args or msg",
            "        self.exit_code = 1",
            "        print('attic: ' + msg, file=sys.stderr)",
            "",
            "    def print_verbose(self, msg, *args, **kw):",
            "        if self.verbose:",
            "            msg = args and msg % args or msg",
            "            if kw.get('newline', True):",
            "                print(msg)",
            "            else:",
            "                print(msg, end=' ')",
            "",
            "    def do_serve(self, args):",
            "        \"\"\"Start Attic in server mode. This command is usually not used manually.",
            "        \"\"\"",
            "        return RepositoryServer(restrict_to_paths=args.restrict_to_paths).serve()",
            "",
            "    def do_init(self, args):",
            "        \"\"\"Initialize an empty repository\"\"\"",
            "        print('Initializing repository at \"%s\"' % args.repository.orig)",
            "        repository = self.open_repository(args.repository, create=True, exclusive=True)",
            "        key = key_creator(repository, args)",
            "        manifest = Manifest(key, repository)",
            "        manifest.key = key",
            "        manifest.write()",
            "        repository.commit()",
            "        return self.exit_code",
            "",
            "    def do_check(self, args):",
            "        \"\"\"Check repository consistency\"\"\"",
            "        repository = self.open_repository(args.repository, exclusive=args.repair)",
            "        if args.repair:",
            "            while not os.environ.get('ATTIC_CHECK_I_KNOW_WHAT_I_AM_DOING'):",
            "                self.print_error(\"\"\"Warning: 'check --repair' is an experimental feature that might result",
            "in data loss.",
            "",
            "Type \"Yes I am sure\" if you understand this and want to continue.\\n\"\"\")",
            "                if input('Do you want to continue? ') == 'Yes I am sure':",
            "                    break",
            "        if not args.archives_only:",
            "            print('Starting repository check...')",
            "            if repository.check(repair=args.repair):",
            "                print('Repository check complete, no problems found.')",
            "            else:",
            "                return 1",
            "        if not args.repo_only and not ArchiveChecker().check(repository, repair=args.repair):",
            "                return 1",
            "        return 0",
            "",
            "    def do_change_passphrase(self, args):",
            "        \"\"\"Change repository key file passphrase\"\"\"",
            "        repository = self.open_repository(args.repository)",
            "        manifest, key = Manifest.load(repository)",
            "        key.change_passphrase()",
            "        return 0",
            "",
            "    def do_create(self, args):",
            "        \"\"\"Create new archive\"\"\"",
            "        t0 = datetime.now()",
            "        repository = self.open_repository(args.archive, exclusive=True)",
            "        manifest, key = Manifest.load(repository)",
            "        cache = Cache(repository, key, manifest)",
            "        archive = Archive(repository, key, manifest, args.archive.archive, cache=cache,",
            "                          create=True, checkpoint_interval=args.checkpoint_interval,",
            "                          numeric_owner=args.numeric_owner)",
            "        # Add Attic cache dir to inode_skip list",
            "        skip_inodes = set()",
            "        try:",
            "            st = os.stat(get_cache_dir())",
            "            skip_inodes.add((st.st_ino, st.st_dev))",
            "        except IOError:",
            "            pass",
            "        # Add local repository dir to inode_skip list",
            "        if not args.archive.host:",
            "            try:",
            "                st = os.stat(args.archive.path)",
            "                skip_inodes.add((st.st_ino, st.st_dev))",
            "            except IOError:",
            "                pass",
            "        for path in args.paths:",
            "            path = os.path.normpath(path)",
            "            if args.dontcross:",
            "                try:",
            "                    restrict_dev = os.lstat(path).st_dev",
            "                except OSError as e:",
            "                    self.print_error('%s: %s', path, e)",
            "                    continue",
            "            else:",
            "                restrict_dev = None",
            "            self._process(archive, cache, args.excludes, args.exclude_caches, skip_inodes, path, restrict_dev)",
            "        archive.save()",
            "        if args.stats:",
            "            t = datetime.now()",
            "            diff = t - t0",
            "            print('-' * 78)",
            "            print('Archive name: %s' % args.archive.archive)",
            "            print('Archive fingerprint: %s' % hexlify(archive.id).decode('ascii'))",
            "            print('Start time: %s' % t0.strftime('%c'))",
            "            print('End time: %s' % t.strftime('%c'))",
            "            print('Duration: %s' % format_timedelta(diff))",
            "            print('Number of files: %d' % archive.stats.nfiles)",
            "            archive.stats.print_('This archive:', cache)",
            "            print('-' * 78)",
            "        return self.exit_code",
            "",
            "    def _process(self, archive, cache, excludes, exclude_caches, skip_inodes, path, restrict_dev):",
            "        if exclude_path(path, excludes):",
            "            return",
            "        try:",
            "            st = os.lstat(path)",
            "        except OSError as e:",
            "            self.print_error('%s: %s', path, e)",
            "            return",
            "        if (st.st_ino, st.st_dev) in skip_inodes:",
            "            return",
            "        # Entering a new filesystem?",
            "        if restrict_dev and st.st_dev != restrict_dev:",
            "            return",
            "        # Ignore unix sockets",
            "        if stat.S_ISSOCK(st.st_mode):",
            "            return",
            "        self.print_verbose(remove_surrogates(path))",
            "        if stat.S_ISREG(st.st_mode):",
            "            try:",
            "                archive.process_file(path, st, cache)",
            "            except IOError as e:",
            "                self.print_error('%s: %s', path, e)",
            "        elif stat.S_ISDIR(st.st_mode):",
            "            if exclude_caches and is_cachedir(path):",
            "                return",
            "            archive.process_item(path, st)",
            "            try:",
            "                entries = os.listdir(path)",
            "            except OSError as e:",
            "                self.print_error('%s: %s', path, e)",
            "            else:",
            "                for filename in sorted(entries):",
            "                    self._process(archive, cache, excludes, exclude_caches, skip_inodes,",
            "                                  os.path.join(path, filename), restrict_dev)",
            "        elif stat.S_ISLNK(st.st_mode):",
            "            archive.process_symlink(path, st)",
            "        elif stat.S_ISFIFO(st.st_mode):",
            "            archive.process_item(path, st)",
            "        elif stat.S_ISCHR(st.st_mode) or stat.S_ISBLK(st.st_mode):",
            "            archive.process_dev(path, st)",
            "        else:",
            "            self.print_error('Unknown file type: %s', path)",
            "",
            "    def do_extract(self, args):",
            "        \"\"\"Extract archive contents\"\"\"",
            "        # be restrictive when restoring files, restore permissions later",
            "        os.umask(0o077)",
            "        repository = self.open_repository(args.archive)",
            "        manifest, key = Manifest.load(repository)",
            "        archive = Archive(repository, key, manifest, args.archive.archive,",
            "                          numeric_owner=args.numeric_owner)",
            "        patterns = adjust_patterns(args.paths, args.excludes)",
            "        dry_run = args.dry_run",
            "        strip_components = args.strip_components",
            "        dirs = []",
            "        for item in archive.iter_items(lambda item: not exclude_path(item[b'path'], patterns), preload=True):",
            "            orig_path = item[b'path']",
            "            if strip_components:",
            "                item[b'path'] = os.sep.join(orig_path.split(os.sep)[strip_components:])",
            "                if not item[b'path']:",
            "                    continue",
            "            if not args.dry_run:",
            "                while dirs and not item[b'path'].startswith(dirs[-1][b'path']):",
            "                    archive.extract_item(dirs.pop(-1))",
            "            self.print_verbose(remove_surrogates(orig_path))",
            "            try:",
            "                if dry_run:",
            "                    archive.extract_item(item, dry_run=True)",
            "                else:",
            "                    if stat.S_ISDIR(item[b'mode']):",
            "                        dirs.append(item)",
            "                        archive.extract_item(item, restore_attrs=False)",
            "                    else:",
            "                        archive.extract_item(item)",
            "            except IOError as e:",
            "                self.print_error('%s: %s', remove_surrogates(orig_path), e)",
            "",
            "        if not args.dry_run:",
            "            while dirs:",
            "                archive.extract_item(dirs.pop(-1))",
            "        return self.exit_code",
            "",
            "    def do_delete(self, args):",
            "        \"\"\"Delete an existing archive\"\"\"",
            "        repository = self.open_repository(args.archive, exclusive=True)",
            "        manifest, key = Manifest.load(repository)",
            "        cache = Cache(repository, key, manifest)",
            "        archive = Archive(repository, key, manifest, args.archive.archive, cache=cache)",
            "        stats = Statistics()",
            "        archive.delete(stats)",
            "        manifest.write()",
            "        repository.commit()",
            "        cache.commit()",
            "        if args.stats:",
            "            stats.print_('Deleted data:', cache)",
            "        return self.exit_code",
            "",
            "    def do_mount(self, args):",
            "        \"\"\"Mount archive or an entire repository as a FUSE fileystem\"\"\"",
            "        try:",
            "            from attic.fuse import AtticOperations",
            "        except ImportError:",
            "            self.print_error('the \"llfuse\" module is required to use this feature')",
            "            return self.exit_code",
            "",
            "        if not os.path.isdir(args.mountpoint) or not os.access(args.mountpoint, os.R_OK | os.W_OK | os.X_OK):",
            "            self.print_error('%s: Mountpoint must be a writable directory' % args.mountpoint)",
            "            return self.exit_code",
            "",
            "        repository = self.open_repository(args.src)",
            "        manifest, key = Manifest.load(repository)",
            "        if args.src.archive:",
            "            archive = Archive(repository, key, manifest, args.src.archive)",
            "        else:",
            "            archive = None",
            "        operations = AtticOperations(key, repository, manifest, archive)",
            "        self.print_verbose(\"Mounting filesystem\")",
            "        try:",
            "            operations.mount(args.mountpoint, args.options, args.foreground)",
            "        except RuntimeError:",
            "            # Relevant error message already printed to stderr by fuse",
            "            self.exit_code = 1",
            "        return self.exit_code",
            "",
            "    def do_list(self, args):",
            "        \"\"\"List archive or repository contents\"\"\"",
            "        repository = self.open_repository(args.src)",
            "        manifest, key = Manifest.load(repository)",
            "        if args.src.archive:",
            "            tmap = {1: 'p', 2: 'c', 4: 'd', 6: 'b', 0o10: '-', 0o12: 'l', 0o14: 's'}",
            "            archive = Archive(repository, key, manifest, args.src.archive)",
            "            for item in archive.iter_items():",
            "                type = tmap.get(item[b'mode'] // 4096, '?')",
            "                mode = format_file_mode(item[b'mode'])",
            "                size = 0",
            "                if type == '-':",
            "                    try:",
            "                        size = sum(size for _, size, _ in item[b'chunks'])",
            "                    except KeyError:",
            "                        pass",
            "                mtime = format_time(datetime.fromtimestamp(bigint_to_int(item[b'mtime']) / 1e9))",
            "                if b'source' in item:",
            "                    if type == 'l':",
            "                        extra = ' -> %s' % item[b'source']",
            "                    else:",
            "                        type = 'h'",
            "                        extra = ' link to %s' % item[b'source']",
            "                else:",
            "                    extra = ''",
            "                print('%s%s %-6s %-6s %8d %s %s%s' % (type, mode, item[b'user'] or item[b'uid'],",
            "                                                  item[b'group'] or item[b'gid'], size, mtime,",
            "                                                  remove_surrogates(item[b'path']), extra))",
            "        else:",
            "            for archive in sorted(Archive.list_archives(repository, key, manifest), key=attrgetter('ts')):",
            "                print(format_archive(archive))",
            "        return self.exit_code",
            "",
            "    def do_info(self, args):",
            "        \"\"\"Show archive details such as disk space used\"\"\"",
            "        repository = self.open_repository(args.archive)",
            "        manifest, key = Manifest.load(repository)",
            "        cache = Cache(repository, key, manifest)",
            "        archive = Archive(repository, key, manifest, args.archive.archive, cache=cache)",
            "        stats = archive.calc_stats(cache)",
            "        print('Name:', archive.name)",
            "        print('Fingerprint: %s' % hexlify(archive.id).decode('ascii'))",
            "        print('Hostname:', archive.metadata[b'hostname'])",
            "        print('Username:', archive.metadata[b'username'])",
            "        print('Time: %s' % to_localtime(archive.ts).strftime('%c'))",
            "        print('Command line:', remove_surrogates(' '.join(archive.metadata[b'cmdline'])))",
            "        print('Number of files: %d' % stats.nfiles)",
            "        stats.print_('This archive:', cache)",
            "        return self.exit_code",
            "",
            "    def do_prune(self, args):",
            "        \"\"\"Prune repository archives according to specified rules\"\"\"",
            "        repository = self.open_repository(args.repository, exclusive=True)",
            "        manifest, key = Manifest.load(repository)",
            "        cache = Cache(repository, key, manifest)",
            "        archives = list(sorted(Archive.list_archives(repository, key, manifest, cache),",
            "                               key=attrgetter('ts'), reverse=True))",
            "        if args.hourly + args.daily + args.weekly + args.monthly + args.yearly == 0 and args.within is None:",
            "            self.print_error('At least one of the \"within\", \"hourly\", \"daily\", \"weekly\", \"monthly\" or \"yearly\" '",
            "                             'settings must be specified')",
            "            return 1",
            "        if args.prefix:",
            "            archives = [archive for archive in archives if archive.name.startswith(args.prefix)]",
            "        keep = []",
            "        if args.within:",
            "            keep += prune_within(archives, args.within)",
            "        if args.hourly:",
            "            keep += prune_split(archives, '%Y-%m-%d %H', args.hourly, keep)",
            "        if args.daily:",
            "            keep += prune_split(archives, '%Y-%m-%d', args.daily, keep)",
            "        if args.weekly:",
            "            keep += prune_split(archives, '%G-%V', args.weekly, keep)",
            "        if args.monthly:",
            "            keep += prune_split(archives, '%Y-%m', args.monthly, keep)",
            "        if args.yearly:",
            "            keep += prune_split(archives, '%Y', args.yearly, keep)",
            "",
            "        keep.sort(key=attrgetter('ts'), reverse=True)",
            "        to_delete = [a for a in archives if a not in keep]",
            "        stats = Statistics()",
            "        for archive in keep:",
            "            self.print_verbose('Keeping archive: %s' % format_archive(archive))",
            "        for archive in to_delete:",
            "            if args.dry_run:",
            "                self.print_verbose('Would prune:     %s' % format_archive(archive))",
            "            else:",
            "                self.print_verbose('Pruning archive: %s' % format_archive(archive))",
            "                archive.delete(stats)",
            "        if to_delete and not args.dry_run:",
            "            manifest.write()",
            "            repository.commit()",
            "            cache.commit()",
            "        if args.stats:",
            "            stats.print_('Deleted data:', cache)",
            "        return self.exit_code",
            "",
            "    helptext = {}",
            "    helptext['patterns'] = '''",
            "        Exclude patterns use a variant of shell pattern syntax, with '*' matching any",
            "        number of characters, '?' matching any single character, '[...]' matching any",
            "        single character specified, including ranges, and '[!...]' matching any",
            "        character not specified.  For the purpose of these patterns, the path",
            "        separator ('\\\\' for Windows and '/' on other systems) is not treated",
            "        specially.  For a path to match a pattern, it must completely match from",
            "        start to end, or must match from the start to just before a path separator.",
            "        Except for the root path, paths will never end in the path separator when",
            "        matching is attempted.  Thus, if a given pattern ends in a path separator, a",
            "        '*' is appended before matching is attempted.  Patterns with wildcards should",
            "        be quoted to protect them from shell expansion.",
            "",
            "        Examples:",
            "",
            "        # Exclude '/home/user/file.o' but not '/home/user/file.odt':",
            "        $ attic create -e '*.o' repo.attic /",
            "",
            "        # Exclude '/home/user/junk' and '/home/user/subdir/junk' but",
            "        # not '/home/user/importantjunk' or '/etc/junk':",
            "        $ attic create -e '/home/*/junk' repo.attic /",
            "",
            "        # Exclude the contents of '/home/user/cache' but not the directory itself:",
            "        $ attic create -e /home/user/cache/ repo.attic /",
            "",
            "        # The file '/home/user/cache/important' is *not* backed up:",
            "        $ attic create -e /home/user/cache/ repo.attic / /home/user/cache/important",
            "        '''",
            "",
            "    def do_help(self, parser, commands, args):",
            "        if not args.topic:",
            "            parser.print_help()",
            "        elif args.topic in self.helptext:",
            "            print(self.helptext[args.topic])",
            "        elif args.topic in commands:",
            "            if args.epilog_only:",
            "                print(commands[args.topic].epilog)",
            "            elif args.usage_only:",
            "                commands[args.topic].epilog = None",
            "                commands[args.topic].print_help()",
            "            else:",
            "                commands[args.topic].print_help()",
            "        else:",
            "            parser.error('No help available on %s' % (args.topic,))",
            "        return self.exit_code",
            "",
            "    def preprocess_args(self, args):",
            "        deprecations = [",
            "            ('--hourly', '--keep-hourly', 'Warning: \"--hourly\" has been deprecated. Use \"--keep-hourly\" instead.'),",
            "            ('--daily', '--keep-daily', 'Warning: \"--daily\" has been deprecated. Use \"--keep-daily\" instead.'),",
            "            ('--weekly', '--keep-weekly', 'Warning: \"--weekly\" has been deprecated. Use \"--keep-weekly\" instead.'),",
            "            ('--monthly', '--keep-monthly', 'Warning: \"--monthly\" has been deprecated. Use \"--keep-monthly\" instead.'),",
            "            ('--yearly', '--keep-yearly', 'Warning: \"--yearly\" has been deprecated. Use \"--keep-yearly\" instead.')",
            "        ]",
            "        if args and args[0] == 'verify':",
            "            print('Warning: \"attic verify\" has been deprecated. Use \"attic extract --dry-run\" instead.')",
            "            args = ['extract', '--dry-run'] + args[1:]",
            "        for i, arg in enumerate(args[:]):",
            "            for old_name, new_name, warning in deprecations:",
            "                if arg.startswith(old_name):",
            "                    args[i] = arg.replace(old_name, new_name)",
            "                    print(warning)",
            "        return args",
            "",
            "    def run(self, args=None):",
            "        check_extension_modules()",
            "        keys_dir = get_keys_dir()",
            "        if not os.path.exists(keys_dir):",
            "            os.makedirs(keys_dir)",
            "            os.chmod(keys_dir, stat.S_IRWXU)",
            "        cache_dir = get_cache_dir()",
            "        if not os.path.exists(cache_dir):",
            "            os.makedirs(cache_dir)",
            "            os.chmod(cache_dir, stat.S_IRWXU)",
            "            with open(os.path.join(cache_dir, 'CACHEDIR.TAG'), 'w') as fd:",
            "                fd.write(textwrap.dedent(\"\"\"",
            "                    Signature: 8a477f597d28d172789f06886806bc55",
            "                    # This file is a cache directory tag created by Attic.",
            "                    # For information about cache directory tags, see:",
            "                    #       http://www.brynosaurus.com/cachedir/",
            "                    \"\"\").lstrip())",
            "        common_parser = argparse.ArgumentParser(add_help=False)",
            "        common_parser.add_argument('-v', '--verbose', dest='verbose', action='store_true',",
            "                            default=False,",
            "                            help='verbose output')",
            "",
            "        # We can't use argparse for \"serve\" since we don't want it to show up in \"Available commands\"",
            "        if args:",
            "            args = self.preprocess_args(args)",
            "",
            "        parser = argparse.ArgumentParser(description='Attic %s - Deduplicated Backups' % __version__)",
            "        subparsers = parser.add_subparsers(title='Available commands')",
            "",
            "        subparser = subparsers.add_parser('serve', parents=[common_parser],",
            "                                          description=self.do_serve.__doc__)",
            "        subparser.set_defaults(func=self.do_serve)",
            "        subparser.add_argument('--restrict-to-path', dest='restrict_to_paths', action='append',",
            "                               metavar='PATH', help='restrict repository access to PATH')",
            "        init_epilog = textwrap.dedent(\"\"\"",
            "        This command initializes an empty repository. A repository is a filesystem",
            "        directory containing the deduplicated data from zero or more archives.",
            "        Encryption can be enabled at repository init time.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('init', parents=[common_parser],",
            "                                          description=self.do_init.__doc__, epilog=init_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_init)",
            "        subparser.add_argument('repository', metavar='REPOSITORY',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to create')",
            "        subparser.add_argument('-e', '--encryption', dest='encryption',",
            "                               choices=('none', 'passphrase', 'keyfile'), default='none',",
            "                               help='select encryption method')",
            "",
            "        check_epilog = textwrap.dedent(\"\"\"",
            "        The check command verifies the consistency of a repository and the corresponding",
            "        archives. The underlying repository data files are first checked to detect bit rot",
            "        and other types of damage. After that the consistency and correctness of the archive",
            "        metadata is verified.",
            "",
            "        The archive metadata checks can be time consuming and requires access to the key",
            "        file and/or passphrase if encryption is enabled. These checks can be skipped using",
            "        the --repository-only option.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('check', parents=[common_parser],",
            "                                          description=self.do_check.__doc__,",
            "                                          epilog=check_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_check)",
            "        subparser.add_argument('repository', metavar='REPOSITORY',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to check consistency of')",
            "        subparser.add_argument('--repository-only', dest='repo_only', action='store_true',",
            "                               default=False,",
            "                               help='only perform repository checks')",
            "        subparser.add_argument('--archives-only', dest='archives_only', action='store_true',",
            "                               default=False,",
            "                               help='only perform archives checks')",
            "        subparser.add_argument('--repair', dest='repair', action='store_true',",
            "                               default=False,",
            "                               help='attempt to repair any inconsistencies found')",
            "",
            "        change_passphrase_epilog = textwrap.dedent(\"\"\"",
            "        The key files used for repository encryption are optionally passphrase",
            "        protected. This command can be used to change this passphrase.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('change-passphrase', parents=[common_parser],",
            "                                          description=self.do_change_passphrase.__doc__,",
            "                                          epilog=change_passphrase_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_change_passphrase)",
            "        subparser.add_argument('repository', metavar='REPOSITORY',",
            "                               type=location_validator(archive=False))",
            "",
            "        create_epilog = textwrap.dedent(\"\"\"",
            "        This command creates a backup archive containing all files found while recursively",
            "        traversing all paths specified. The archive will consume almost no disk space for",
            "        files or parts of files that have already been stored in other archives.",
            "",
            "        See \"attic help patterns\" for more help on exclude patterns.",
            "        \"\"\")",
            "",
            "        subparser = subparsers.add_parser('create', parents=[common_parser],",
            "                                          description=self.do_create.__doc__,",
            "                                          epilog=create_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_create)",
            "        subparser.add_argument('-s', '--stats', dest='stats',",
            "                               action='store_true', default=False,",
            "                               help='print statistics for the created archive')",
            "        subparser.add_argument('-e', '--exclude', dest='excludes',",
            "                               type=ExcludePattern, action='append',",
            "                               metavar=\"PATTERN\", help='exclude paths matching PATTERN')",
            "        subparser.add_argument('--exclude-from', dest='exclude_files',",
            "                               type=argparse.FileType('r'), action='append',",
            "                               metavar='EXCLUDEFILE', help='read exclude patterns from EXCLUDEFILE, one per line')",
            "        subparser.add_argument('--exclude-caches', dest='exclude_caches',",
            "                               action='store_true', default=False,",
            "                               help='exclude directories that contain a CACHEDIR.TAG file (http://www.brynosaurus.com/cachedir/spec.html)')",
            "        subparser.add_argument('-c', '--checkpoint-interval', dest='checkpoint_interval',",
            "                               type=int, default=300, metavar='SECONDS',",
            "                               help='write checkpoint every SECONDS seconds (Default: 300)')",
            "        subparser.add_argument('--do-not-cross-mountpoints', dest='dontcross',",
            "                               action='store_true', default=False,",
            "                               help='do not cross mount points')",
            "        subparser.add_argument('--numeric-owner', dest='numeric_owner',",
            "                               action='store_true', default=False,",
            "                               help='only store numeric user and group identifiers')",
            "        subparser.add_argument('archive', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to create')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='+', type=str,",
            "                               help='paths to archive')",
            "",
            "        extract_epilog = textwrap.dedent(\"\"\"",
            "        This command extracts the contents of an archive. By default the entire",
            "        archive is extracted but a subset of files and directories can be selected",
            "        by passing a list of ``PATHs`` as arguments. The file selection can further",
            "        be restricted by using the ``--exclude`` option.",
            "",
            "        See \"attic help patterns\" for more help on exclude patterns.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('extract', parents=[common_parser],",
            "                                          description=self.do_extract.__doc__,",
            "                                          epilog=extract_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_extract)",
            "        subparser.add_argument('-n', '--dry-run', dest='dry_run',",
            "                               default=False, action='store_true',",
            "                               help='do not actually change any files')",
            "        subparser.add_argument('-e', '--exclude', dest='excludes',",
            "                               type=ExcludePattern, action='append',",
            "                               metavar=\"PATTERN\", help='exclude paths matching PATTERN')",
            "        subparser.add_argument('--exclude-from', dest='exclude_files',",
            "                               type=argparse.FileType('r'), action='append',",
            "                               metavar='EXCLUDEFILE', help='read exclude patterns from EXCLUDEFILE, one per line')",
            "        subparser.add_argument('--numeric-owner', dest='numeric_owner',",
            "                               action='store_true', default=False,",
            "                               help='only obey numeric user and group identifiers')",
            "        subparser.add_argument('--strip-components', dest='strip_components',",
            "                               type=int, default=0, metavar='NUMBER',",
            "                               help='Remove the specified number of leading path elements. Pathnames with fewer elements will be silently skipped.')",
            "        subparser.add_argument('archive', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to extract')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,",
            "                               help='paths to extract')",
            "",
            "        delete_epilog = textwrap.dedent(\"\"\"",
            "        This command deletes an archive from the repository. Any disk space not",
            "        shared with any other existing archive is also reclaimed.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('delete', parents=[common_parser],",
            "                                          description=self.do_delete.__doc__,",
            "                                          epilog=delete_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_delete)",
            "        subparser.add_argument('-s', '--stats', dest='stats',",
            "                               action='store_true', default=False,",
            "                               help='print statistics for the deleted archive')",
            "        subparser.add_argument('archive', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to delete')",
            "",
            "        list_epilog = textwrap.dedent(\"\"\"",
            "        This command lists the contents of a repository or an archive.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('list', parents=[common_parser],",
            "                                          description=self.do_list.__doc__,",
            "                                          epilog=list_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_list)",
            "        subparser.add_argument('src', metavar='REPOSITORY_OR_ARCHIVE', type=location_validator(),",
            "                               help='repository/archive to list contents of')",
            "        mount_epilog = textwrap.dedent(\"\"\"",
            "        This command mounts an archive as a FUSE filesystem. This can be useful for",
            "        browsing an archive or restoring individual files. Unless the ``--foreground``",
            "        option is given the command will run in the background until the filesystem",
            "        is ``umounted``.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('mount', parents=[common_parser],",
            "                                          description=self.do_mount.__doc__,",
            "                                          epilog=mount_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_mount)",
            "        subparser.add_argument('src', metavar='REPOSITORY_OR_ARCHIVE', type=location_validator(),",
            "                               help='repository/archive to mount')",
            "        subparser.add_argument('mountpoint', metavar='MOUNTPOINT', type=str,",
            "                               help='where to mount filesystem')",
            "        subparser.add_argument('-f', '--foreground', dest='foreground',",
            "                               action='store_true', default=False,",
            "                               help='stay in foreground, do not daemonize')",
            "        subparser.add_argument('-o', dest='options', type=str,",
            "                               help='Extra mount options')",
            "",
            "        info_epilog = textwrap.dedent(\"\"\"",
            "        This command displays some detailed information about the specified archive.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('info', parents=[common_parser],",
            "                                          description=self.do_info.__doc__,",
            "                                          epilog=info_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_info)",
            "        subparser.add_argument('archive', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to display information about')",
            "",
            "        prune_epilog = textwrap.dedent(\"\"\"",
            "        The prune command prunes a repository by deleting archives not matching",
            "        any of the specified retention options. This command is normally used by",
            "        automated backup scripts wanting to keep a certain number of historic backups.",
            "",
            "        As an example, \"-d 7\" means to keep the latest backup on each day for 7 days.",
            "        Days without backups do not count towards the total.",
            "        The rules are applied from hourly to yearly, and backups selected by previous",
            "        rules do not count towards those of later rules. The time that each backup",
            "        completes is used for pruning purposes. Dates and times are interpreted in",
            "        the local timezone, and weeks go from Monday to Sunday. Specifying a",
            "        negative number of archives to keep means that there is no limit.",
            "",
            "        The \"--keep-within\" option takes an argument of the form \"<int><char>\",",
            "        where char is \"H\", \"d\", \"w\", \"m\", \"y\". For example, \"--keep-within 2d\" means",
            "        to keep all archives that were created within the past 48 hours.",
            "        \"1m\" is taken to mean \"31d\". The archives kept with this option do not",
            "        count towards the totals specified by any other options.",
            "",
            "        If a prefix is set with -p, then only archives that start with the prefix are",
            "        considered for deletion and only those archives count towards the totals",
            "        specified by the rules.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('prune', parents=[common_parser],",
            "                                          description=self.do_prune.__doc__,",
            "                                          epilog=prune_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_prune)",
            "        subparser.add_argument('-n', '--dry-run', dest='dry_run',",
            "                               default=False, action='store_true',",
            "                               help='do not change repository')",
            "        subparser.add_argument('-s', '--stats', dest='stats',",
            "                               action='store_true', default=False,",
            "                               help='print statistics for the deleted archive')",
            "        subparser.add_argument('--keep-within', dest='within', type=str, metavar='WITHIN',",
            "                               help='keep all archives within this time interval')",
            "        subparser.add_argument('-H', '--keep-hourly', dest='hourly', type=int, default=0,",
            "                               help='number of hourly archives to keep')",
            "        subparser.add_argument('-d', '--keep-daily', dest='daily', type=int, default=0,",
            "                               help='number of daily archives to keep')",
            "        subparser.add_argument('-w', '--keep-weekly', dest='weekly', type=int, default=0,",
            "                               help='number of weekly archives to keep')",
            "        subparser.add_argument('-m', '--keep-monthly', dest='monthly', type=int, default=0,",
            "                               help='number of monthly archives to keep')",
            "        subparser.add_argument('-y', '--keep-yearly', dest='yearly', type=int, default=0,",
            "                               help='number of yearly archives to keep')",
            "        subparser.add_argument('-p', '--prefix', dest='prefix', type=str,",
            "                               help='only consider archive names starting with this prefix')",
            "        subparser.add_argument('repository', metavar='REPOSITORY',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to prune')",
            "",
            "        subparser = subparsers.add_parser('help', parents=[common_parser],",
            "                                          description='Extra help')",
            "        subparser.add_argument('--epilog-only', dest='epilog_only',",
            "                               action='store_true', default=False)",
            "        subparser.add_argument('--usage-only', dest='usage_only',",
            "                               action='store_true', default=False)",
            "        subparser.set_defaults(func=functools.partial(self.do_help, parser, subparsers.choices))",
            "        subparser.add_argument('topic', metavar='TOPIC', type=str, nargs='?',",
            "                               help='additional help on TOPIC')",
            "",
            "        args = parser.parse_args(args or ['-h'])",
            "        self.verbose = args.verbose",
            "        update_excludes(args)",
            "        return args.func(args)",
            "",
            "",
            "def main():",
            "    # Make sure stdout and stderr have errors='replace') to avoid unicode",
            "    # issues when print()-ing unicode file names",
            "    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, sys.stdout.encoding, 'replace', line_buffering=True)",
            "    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, sys.stderr.encoding, 'replace', line_buffering=True)",
            "    archiver = Archiver()",
            "    try:",
            "        exit_code = archiver.run(sys.argv[1:])",
            "    except Error as e:",
            "        archiver.print_error(e.get_message())",
            "        exit_code = e.exit_code",
            "    except KeyboardInterrupt:",
            "        archiver.print_error('Error: Keyboard interrupt')",
            "        exit_code = 1",
            "    else:",
            "        if exit_code:",
            "            archiver.print_error('Exiting with failure status due to previous errors')",
            "    sys.exit(exit_code)",
            "",
            "if __name__ == '__main__':",
            "    main()"
        ],
        "afterPatchFile": [
            "import argparse",
            "from binascii import hexlify",
            "from datetime import datetime",
            "from operator import attrgetter",
            "import functools",
            "import io",
            "import os",
            "import stat",
            "import sys",
            "import textwrap",
            "",
            "from attic import __version__",
            "from attic.archive import Archive, ArchiveChecker",
            "from attic.repository import Repository",
            "from attic.cache import Cache",
            "from attic.key import key_creator",
            "from attic.helpers import Error, location_validator, format_time, \\",
            "    format_file_mode, ExcludePattern, exclude_path, adjust_patterns, to_localtime, \\",
            "    get_cache_dir, get_keys_dir, format_timedelta, prune_within, prune_split, \\",
            "    Manifest, remove_surrogates, update_excludes, format_archive, check_extension_modules, Statistics, \\",
            "    is_cachedir, bigint_to_int",
            "from attic.remote import RepositoryServer, RemoteRepository",
            "",
            "",
            "class Archiver:",
            "",
            "    def __init__(self):",
            "        self.exit_code = 0",
            "",
            "    def open_repository(self, location, create=False, exclusive=False):",
            "        if location.proto == 'ssh':",
            "            repository = RemoteRepository(location, create=create)",
            "        else:",
            "            repository = Repository(location.path, create=create, exclusive=exclusive)",
            "        repository._location = location",
            "        return repository",
            "",
            "    def print_error(self, msg, *args):",
            "        msg = args and msg % args or msg",
            "        self.exit_code = 1",
            "        print('attic: ' + msg, file=sys.stderr)",
            "",
            "    def print_verbose(self, msg, *args, **kw):",
            "        if self.verbose:",
            "            msg = args and msg % args or msg",
            "            if kw.get('newline', True):",
            "                print(msg)",
            "            else:",
            "                print(msg, end=' ')",
            "",
            "    def do_serve(self, args):",
            "        \"\"\"Start Attic in server mode. This command is usually not used manually.",
            "        \"\"\"",
            "        return RepositoryServer(restrict_to_paths=args.restrict_to_paths).serve()",
            "",
            "    def do_init(self, args):",
            "        \"\"\"Initialize an empty repository\"\"\"",
            "        print('Initializing repository at \"%s\"' % args.repository.orig)",
            "        repository = self.open_repository(args.repository, create=True, exclusive=True)",
            "        key = key_creator(repository, args)",
            "        manifest = Manifest(key, repository)",
            "        manifest.key = key",
            "        manifest.write()",
            "        repository.commit()",
            "        Cache(repository, key, manifest, warn_if_unencrypted=False)",
            "        return self.exit_code",
            "",
            "    def do_check(self, args):",
            "        \"\"\"Check repository consistency\"\"\"",
            "        repository = self.open_repository(args.repository, exclusive=args.repair)",
            "        if args.repair:",
            "            while not os.environ.get('ATTIC_CHECK_I_KNOW_WHAT_I_AM_DOING'):",
            "                self.print_error(\"\"\"Warning: 'check --repair' is an experimental feature that might result",
            "in data loss.",
            "",
            "Type \"Yes I am sure\" if you understand this and want to continue.\\n\"\"\")",
            "                if input('Do you want to continue? ') == 'Yes I am sure':",
            "                    break",
            "        if not args.archives_only:",
            "            print('Starting repository check...')",
            "            if repository.check(repair=args.repair):",
            "                print('Repository check complete, no problems found.')",
            "            else:",
            "                return 1",
            "        if not args.repo_only and not ArchiveChecker().check(repository, repair=args.repair):",
            "                return 1",
            "        return 0",
            "",
            "    def do_change_passphrase(self, args):",
            "        \"\"\"Change repository key file passphrase\"\"\"",
            "        repository = self.open_repository(args.repository)",
            "        manifest, key = Manifest.load(repository)",
            "        key.change_passphrase()",
            "        return 0",
            "",
            "    def do_create(self, args):",
            "        \"\"\"Create new archive\"\"\"",
            "        t0 = datetime.now()",
            "        repository = self.open_repository(args.archive, exclusive=True)",
            "        manifest, key = Manifest.load(repository)",
            "        cache = Cache(repository, key, manifest)",
            "        archive = Archive(repository, key, manifest, args.archive.archive, cache=cache,",
            "                          create=True, checkpoint_interval=args.checkpoint_interval,",
            "                          numeric_owner=args.numeric_owner)",
            "        # Add Attic cache dir to inode_skip list",
            "        skip_inodes = set()",
            "        try:",
            "            st = os.stat(get_cache_dir())",
            "            skip_inodes.add((st.st_ino, st.st_dev))",
            "        except IOError:",
            "            pass",
            "        # Add local repository dir to inode_skip list",
            "        if not args.archive.host:",
            "            try:",
            "                st = os.stat(args.archive.path)",
            "                skip_inodes.add((st.st_ino, st.st_dev))",
            "            except IOError:",
            "                pass",
            "        for path in args.paths:",
            "            path = os.path.normpath(path)",
            "            if args.dontcross:",
            "                try:",
            "                    restrict_dev = os.lstat(path).st_dev",
            "                except OSError as e:",
            "                    self.print_error('%s: %s', path, e)",
            "                    continue",
            "            else:",
            "                restrict_dev = None",
            "            self._process(archive, cache, args.excludes, args.exclude_caches, skip_inodes, path, restrict_dev)",
            "        archive.save()",
            "        if args.stats:",
            "            t = datetime.now()",
            "            diff = t - t0",
            "            print('-' * 78)",
            "            print('Archive name: %s' % args.archive.archive)",
            "            print('Archive fingerprint: %s' % hexlify(archive.id).decode('ascii'))",
            "            print('Start time: %s' % t0.strftime('%c'))",
            "            print('End time: %s' % t.strftime('%c'))",
            "            print('Duration: %s' % format_timedelta(diff))",
            "            print('Number of files: %d' % archive.stats.nfiles)",
            "            archive.stats.print_('This archive:', cache)",
            "            print('-' * 78)",
            "        return self.exit_code",
            "",
            "    def _process(self, archive, cache, excludes, exclude_caches, skip_inodes, path, restrict_dev):",
            "        if exclude_path(path, excludes):",
            "            return",
            "        try:",
            "            st = os.lstat(path)",
            "        except OSError as e:",
            "            self.print_error('%s: %s', path, e)",
            "            return",
            "        if (st.st_ino, st.st_dev) in skip_inodes:",
            "            return",
            "        # Entering a new filesystem?",
            "        if restrict_dev and st.st_dev != restrict_dev:",
            "            return",
            "        # Ignore unix sockets",
            "        if stat.S_ISSOCK(st.st_mode):",
            "            return",
            "        self.print_verbose(remove_surrogates(path))",
            "        if stat.S_ISREG(st.st_mode):",
            "            try:",
            "                archive.process_file(path, st, cache)",
            "            except IOError as e:",
            "                self.print_error('%s: %s', path, e)",
            "        elif stat.S_ISDIR(st.st_mode):",
            "            if exclude_caches and is_cachedir(path):",
            "                return",
            "            archive.process_item(path, st)",
            "            try:",
            "                entries = os.listdir(path)",
            "            except OSError as e:",
            "                self.print_error('%s: %s', path, e)",
            "            else:",
            "                for filename in sorted(entries):",
            "                    self._process(archive, cache, excludes, exclude_caches, skip_inodes,",
            "                                  os.path.join(path, filename), restrict_dev)",
            "        elif stat.S_ISLNK(st.st_mode):",
            "            archive.process_symlink(path, st)",
            "        elif stat.S_ISFIFO(st.st_mode):",
            "            archive.process_item(path, st)",
            "        elif stat.S_ISCHR(st.st_mode) or stat.S_ISBLK(st.st_mode):",
            "            archive.process_dev(path, st)",
            "        else:",
            "            self.print_error('Unknown file type: %s', path)",
            "",
            "    def do_extract(self, args):",
            "        \"\"\"Extract archive contents\"\"\"",
            "        # be restrictive when restoring files, restore permissions later",
            "        os.umask(0o077)",
            "        repository = self.open_repository(args.archive)",
            "        manifest, key = Manifest.load(repository)",
            "        archive = Archive(repository, key, manifest, args.archive.archive,",
            "                          numeric_owner=args.numeric_owner)",
            "        patterns = adjust_patterns(args.paths, args.excludes)",
            "        dry_run = args.dry_run",
            "        strip_components = args.strip_components",
            "        dirs = []",
            "        for item in archive.iter_items(lambda item: not exclude_path(item[b'path'], patterns), preload=True):",
            "            orig_path = item[b'path']",
            "            if strip_components:",
            "                item[b'path'] = os.sep.join(orig_path.split(os.sep)[strip_components:])",
            "                if not item[b'path']:",
            "                    continue",
            "            if not args.dry_run:",
            "                while dirs and not item[b'path'].startswith(dirs[-1][b'path']):",
            "                    archive.extract_item(dirs.pop(-1))",
            "            self.print_verbose(remove_surrogates(orig_path))",
            "            try:",
            "                if dry_run:",
            "                    archive.extract_item(item, dry_run=True)",
            "                else:",
            "                    if stat.S_ISDIR(item[b'mode']):",
            "                        dirs.append(item)",
            "                        archive.extract_item(item, restore_attrs=False)",
            "                    else:",
            "                        archive.extract_item(item)",
            "            except IOError as e:",
            "                self.print_error('%s: %s', remove_surrogates(orig_path), e)",
            "",
            "        if not args.dry_run:",
            "            while dirs:",
            "                archive.extract_item(dirs.pop(-1))",
            "        return self.exit_code",
            "",
            "    def do_delete(self, args):",
            "        \"\"\"Delete an existing archive\"\"\"",
            "        repository = self.open_repository(args.archive, exclusive=True)",
            "        manifest, key = Manifest.load(repository)",
            "        cache = Cache(repository, key, manifest)",
            "        archive = Archive(repository, key, manifest, args.archive.archive, cache=cache)",
            "        stats = Statistics()",
            "        archive.delete(stats)",
            "        manifest.write()",
            "        repository.commit()",
            "        cache.commit()",
            "        if args.stats:",
            "            stats.print_('Deleted data:', cache)",
            "        return self.exit_code",
            "",
            "    def do_mount(self, args):",
            "        \"\"\"Mount archive or an entire repository as a FUSE fileystem\"\"\"",
            "        try:",
            "            from attic.fuse import AtticOperations",
            "        except ImportError:",
            "            self.print_error('the \"llfuse\" module is required to use this feature')",
            "            return self.exit_code",
            "",
            "        if not os.path.isdir(args.mountpoint) or not os.access(args.mountpoint, os.R_OK | os.W_OK | os.X_OK):",
            "            self.print_error('%s: Mountpoint must be a writable directory' % args.mountpoint)",
            "            return self.exit_code",
            "",
            "        repository = self.open_repository(args.src)",
            "        manifest, key = Manifest.load(repository)",
            "        if args.src.archive:",
            "            archive = Archive(repository, key, manifest, args.src.archive)",
            "        else:",
            "            archive = None",
            "        operations = AtticOperations(key, repository, manifest, archive)",
            "        self.print_verbose(\"Mounting filesystem\")",
            "        try:",
            "            operations.mount(args.mountpoint, args.options, args.foreground)",
            "        except RuntimeError:",
            "            # Relevant error message already printed to stderr by fuse",
            "            self.exit_code = 1",
            "        return self.exit_code",
            "",
            "    def do_list(self, args):",
            "        \"\"\"List archive or repository contents\"\"\"",
            "        repository = self.open_repository(args.src)",
            "        manifest, key = Manifest.load(repository)",
            "        if args.src.archive:",
            "            tmap = {1: 'p', 2: 'c', 4: 'd', 6: 'b', 0o10: '-', 0o12: 'l', 0o14: 's'}",
            "            archive = Archive(repository, key, manifest, args.src.archive)",
            "            for item in archive.iter_items():",
            "                type = tmap.get(item[b'mode'] // 4096, '?')",
            "                mode = format_file_mode(item[b'mode'])",
            "                size = 0",
            "                if type == '-':",
            "                    try:",
            "                        size = sum(size for _, size, _ in item[b'chunks'])",
            "                    except KeyError:",
            "                        pass",
            "                mtime = format_time(datetime.fromtimestamp(bigint_to_int(item[b'mtime']) / 1e9))",
            "                if b'source' in item:",
            "                    if type == 'l':",
            "                        extra = ' -> %s' % item[b'source']",
            "                    else:",
            "                        type = 'h'",
            "                        extra = ' link to %s' % item[b'source']",
            "                else:",
            "                    extra = ''",
            "                print('%s%s %-6s %-6s %8d %s %s%s' % (type, mode, item[b'user'] or item[b'uid'],",
            "                                                  item[b'group'] or item[b'gid'], size, mtime,",
            "                                                  remove_surrogates(item[b'path']), extra))",
            "        else:",
            "            for archive in sorted(Archive.list_archives(repository, key, manifest), key=attrgetter('ts')):",
            "                print(format_archive(archive))",
            "        return self.exit_code",
            "",
            "    def do_info(self, args):",
            "        \"\"\"Show archive details such as disk space used\"\"\"",
            "        repository = self.open_repository(args.archive)",
            "        manifest, key = Manifest.load(repository)",
            "        cache = Cache(repository, key, manifest)",
            "        archive = Archive(repository, key, manifest, args.archive.archive, cache=cache)",
            "        stats = archive.calc_stats(cache)",
            "        print('Name:', archive.name)",
            "        print('Fingerprint: %s' % hexlify(archive.id).decode('ascii'))",
            "        print('Hostname:', archive.metadata[b'hostname'])",
            "        print('Username:', archive.metadata[b'username'])",
            "        print('Time: %s' % to_localtime(archive.ts).strftime('%c'))",
            "        print('Command line:', remove_surrogates(' '.join(archive.metadata[b'cmdline'])))",
            "        print('Number of files: %d' % stats.nfiles)",
            "        stats.print_('This archive:', cache)",
            "        return self.exit_code",
            "",
            "    def do_prune(self, args):",
            "        \"\"\"Prune repository archives according to specified rules\"\"\"",
            "        repository = self.open_repository(args.repository, exclusive=True)",
            "        manifest, key = Manifest.load(repository)",
            "        cache = Cache(repository, key, manifest)",
            "        archives = list(sorted(Archive.list_archives(repository, key, manifest, cache),",
            "                               key=attrgetter('ts'), reverse=True))",
            "        if args.hourly + args.daily + args.weekly + args.monthly + args.yearly == 0 and args.within is None:",
            "            self.print_error('At least one of the \"within\", \"hourly\", \"daily\", \"weekly\", \"monthly\" or \"yearly\" '",
            "                             'settings must be specified')",
            "            return 1",
            "        if args.prefix:",
            "            archives = [archive for archive in archives if archive.name.startswith(args.prefix)]",
            "        keep = []",
            "        if args.within:",
            "            keep += prune_within(archives, args.within)",
            "        if args.hourly:",
            "            keep += prune_split(archives, '%Y-%m-%d %H', args.hourly, keep)",
            "        if args.daily:",
            "            keep += prune_split(archives, '%Y-%m-%d', args.daily, keep)",
            "        if args.weekly:",
            "            keep += prune_split(archives, '%G-%V', args.weekly, keep)",
            "        if args.monthly:",
            "            keep += prune_split(archives, '%Y-%m', args.monthly, keep)",
            "        if args.yearly:",
            "            keep += prune_split(archives, '%Y', args.yearly, keep)",
            "",
            "        keep.sort(key=attrgetter('ts'), reverse=True)",
            "        to_delete = [a for a in archives if a not in keep]",
            "        stats = Statistics()",
            "        for archive in keep:",
            "            self.print_verbose('Keeping archive: %s' % format_archive(archive))",
            "        for archive in to_delete:",
            "            if args.dry_run:",
            "                self.print_verbose('Would prune:     %s' % format_archive(archive))",
            "            else:",
            "                self.print_verbose('Pruning archive: %s' % format_archive(archive))",
            "                archive.delete(stats)",
            "        if to_delete and not args.dry_run:",
            "            manifest.write()",
            "            repository.commit()",
            "            cache.commit()",
            "        if args.stats:",
            "            stats.print_('Deleted data:', cache)",
            "        return self.exit_code",
            "",
            "    helptext = {}",
            "    helptext['patterns'] = '''",
            "        Exclude patterns use a variant of shell pattern syntax, with '*' matching any",
            "        number of characters, '?' matching any single character, '[...]' matching any",
            "        single character specified, including ranges, and '[!...]' matching any",
            "        character not specified.  For the purpose of these patterns, the path",
            "        separator ('\\\\' for Windows and '/' on other systems) is not treated",
            "        specially.  For a path to match a pattern, it must completely match from",
            "        start to end, or must match from the start to just before a path separator.",
            "        Except for the root path, paths will never end in the path separator when",
            "        matching is attempted.  Thus, if a given pattern ends in a path separator, a",
            "        '*' is appended before matching is attempted.  Patterns with wildcards should",
            "        be quoted to protect them from shell expansion.",
            "",
            "        Examples:",
            "",
            "        # Exclude '/home/user/file.o' but not '/home/user/file.odt':",
            "        $ attic create -e '*.o' repo.attic /",
            "",
            "        # Exclude '/home/user/junk' and '/home/user/subdir/junk' but",
            "        # not '/home/user/importantjunk' or '/etc/junk':",
            "        $ attic create -e '/home/*/junk' repo.attic /",
            "",
            "        # Exclude the contents of '/home/user/cache' but not the directory itself:",
            "        $ attic create -e /home/user/cache/ repo.attic /",
            "",
            "        # The file '/home/user/cache/important' is *not* backed up:",
            "        $ attic create -e /home/user/cache/ repo.attic / /home/user/cache/important",
            "        '''",
            "",
            "    def do_help(self, parser, commands, args):",
            "        if not args.topic:",
            "            parser.print_help()",
            "        elif args.topic in self.helptext:",
            "            print(self.helptext[args.topic])",
            "        elif args.topic in commands:",
            "            if args.epilog_only:",
            "                print(commands[args.topic].epilog)",
            "            elif args.usage_only:",
            "                commands[args.topic].epilog = None",
            "                commands[args.topic].print_help()",
            "            else:",
            "                commands[args.topic].print_help()",
            "        else:",
            "            parser.error('No help available on %s' % (args.topic,))",
            "        return self.exit_code",
            "",
            "    def preprocess_args(self, args):",
            "        deprecations = [",
            "            ('--hourly', '--keep-hourly', 'Warning: \"--hourly\" has been deprecated. Use \"--keep-hourly\" instead.'),",
            "            ('--daily', '--keep-daily', 'Warning: \"--daily\" has been deprecated. Use \"--keep-daily\" instead.'),",
            "            ('--weekly', '--keep-weekly', 'Warning: \"--weekly\" has been deprecated. Use \"--keep-weekly\" instead.'),",
            "            ('--monthly', '--keep-monthly', 'Warning: \"--monthly\" has been deprecated. Use \"--keep-monthly\" instead.'),",
            "            ('--yearly', '--keep-yearly', 'Warning: \"--yearly\" has been deprecated. Use \"--keep-yearly\" instead.')",
            "        ]",
            "        if args and args[0] == 'verify':",
            "            print('Warning: \"attic verify\" has been deprecated. Use \"attic extract --dry-run\" instead.')",
            "            args = ['extract', '--dry-run'] + args[1:]",
            "        for i, arg in enumerate(args[:]):",
            "            for old_name, new_name, warning in deprecations:",
            "                if arg.startswith(old_name):",
            "                    args[i] = arg.replace(old_name, new_name)",
            "                    print(warning)",
            "        return args",
            "",
            "    def run(self, args=None):",
            "        check_extension_modules()",
            "        keys_dir = get_keys_dir()",
            "        if not os.path.exists(keys_dir):",
            "            os.makedirs(keys_dir)",
            "            os.chmod(keys_dir, stat.S_IRWXU)",
            "        cache_dir = get_cache_dir()",
            "        if not os.path.exists(cache_dir):",
            "            os.makedirs(cache_dir)",
            "            os.chmod(cache_dir, stat.S_IRWXU)",
            "            with open(os.path.join(cache_dir, 'CACHEDIR.TAG'), 'w') as fd:",
            "                fd.write(textwrap.dedent(\"\"\"",
            "                    Signature: 8a477f597d28d172789f06886806bc55",
            "                    # This file is a cache directory tag created by Attic.",
            "                    # For information about cache directory tags, see:",
            "                    #       http://www.brynosaurus.com/cachedir/",
            "                    \"\"\").lstrip())",
            "        common_parser = argparse.ArgumentParser(add_help=False)",
            "        common_parser.add_argument('-v', '--verbose', dest='verbose', action='store_true',",
            "                            default=False,",
            "                            help='verbose output')",
            "",
            "        # We can't use argparse for \"serve\" since we don't want it to show up in \"Available commands\"",
            "        if args:",
            "            args = self.preprocess_args(args)",
            "",
            "        parser = argparse.ArgumentParser(description='Attic %s - Deduplicated Backups' % __version__)",
            "        subparsers = parser.add_subparsers(title='Available commands')",
            "",
            "        subparser = subparsers.add_parser('serve', parents=[common_parser],",
            "                                          description=self.do_serve.__doc__)",
            "        subparser.set_defaults(func=self.do_serve)",
            "        subparser.add_argument('--restrict-to-path', dest='restrict_to_paths', action='append',",
            "                               metavar='PATH', help='restrict repository access to PATH')",
            "        init_epilog = textwrap.dedent(\"\"\"",
            "        This command initializes an empty repository. A repository is a filesystem",
            "        directory containing the deduplicated data from zero or more archives.",
            "        Encryption can be enabled at repository init time.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('init', parents=[common_parser],",
            "                                          description=self.do_init.__doc__, epilog=init_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_init)",
            "        subparser.add_argument('repository', metavar='REPOSITORY',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to create')",
            "        subparser.add_argument('-e', '--encryption', dest='encryption',",
            "                               choices=('none', 'passphrase', 'keyfile'), default='none',",
            "                               help='select encryption method')",
            "",
            "        check_epilog = textwrap.dedent(\"\"\"",
            "        The check command verifies the consistency of a repository and the corresponding",
            "        archives. The underlying repository data files are first checked to detect bit rot",
            "        and other types of damage. After that the consistency and correctness of the archive",
            "        metadata is verified.",
            "",
            "        The archive metadata checks can be time consuming and requires access to the key",
            "        file and/or passphrase if encryption is enabled. These checks can be skipped using",
            "        the --repository-only option.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('check', parents=[common_parser],",
            "                                          description=self.do_check.__doc__,",
            "                                          epilog=check_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_check)",
            "        subparser.add_argument('repository', metavar='REPOSITORY',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to check consistency of')",
            "        subparser.add_argument('--repository-only', dest='repo_only', action='store_true',",
            "                               default=False,",
            "                               help='only perform repository checks')",
            "        subparser.add_argument('--archives-only', dest='archives_only', action='store_true',",
            "                               default=False,",
            "                               help='only perform archives checks')",
            "        subparser.add_argument('--repair', dest='repair', action='store_true',",
            "                               default=False,",
            "                               help='attempt to repair any inconsistencies found')",
            "",
            "        change_passphrase_epilog = textwrap.dedent(\"\"\"",
            "        The key files used for repository encryption are optionally passphrase",
            "        protected. This command can be used to change this passphrase.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('change-passphrase', parents=[common_parser],",
            "                                          description=self.do_change_passphrase.__doc__,",
            "                                          epilog=change_passphrase_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_change_passphrase)",
            "        subparser.add_argument('repository', metavar='REPOSITORY',",
            "                               type=location_validator(archive=False))",
            "",
            "        create_epilog = textwrap.dedent(\"\"\"",
            "        This command creates a backup archive containing all files found while recursively",
            "        traversing all paths specified. The archive will consume almost no disk space for",
            "        files or parts of files that have already been stored in other archives.",
            "",
            "        See \"attic help patterns\" for more help on exclude patterns.",
            "        \"\"\")",
            "",
            "        subparser = subparsers.add_parser('create', parents=[common_parser],",
            "                                          description=self.do_create.__doc__,",
            "                                          epilog=create_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_create)",
            "        subparser.add_argument('-s', '--stats', dest='stats',",
            "                               action='store_true', default=False,",
            "                               help='print statistics for the created archive')",
            "        subparser.add_argument('-e', '--exclude', dest='excludes',",
            "                               type=ExcludePattern, action='append',",
            "                               metavar=\"PATTERN\", help='exclude paths matching PATTERN')",
            "        subparser.add_argument('--exclude-from', dest='exclude_files',",
            "                               type=argparse.FileType('r'), action='append',",
            "                               metavar='EXCLUDEFILE', help='read exclude patterns from EXCLUDEFILE, one per line')",
            "        subparser.add_argument('--exclude-caches', dest='exclude_caches',",
            "                               action='store_true', default=False,",
            "                               help='exclude directories that contain a CACHEDIR.TAG file (http://www.brynosaurus.com/cachedir/spec.html)')",
            "        subparser.add_argument('-c', '--checkpoint-interval', dest='checkpoint_interval',",
            "                               type=int, default=300, metavar='SECONDS',",
            "                               help='write checkpoint every SECONDS seconds (Default: 300)')",
            "        subparser.add_argument('--do-not-cross-mountpoints', dest='dontcross',",
            "                               action='store_true', default=False,",
            "                               help='do not cross mount points')",
            "        subparser.add_argument('--numeric-owner', dest='numeric_owner',",
            "                               action='store_true', default=False,",
            "                               help='only store numeric user and group identifiers')",
            "        subparser.add_argument('archive', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to create')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='+', type=str,",
            "                               help='paths to archive')",
            "",
            "        extract_epilog = textwrap.dedent(\"\"\"",
            "        This command extracts the contents of an archive. By default the entire",
            "        archive is extracted but a subset of files and directories can be selected",
            "        by passing a list of ``PATHs`` as arguments. The file selection can further",
            "        be restricted by using the ``--exclude`` option.",
            "",
            "        See \"attic help patterns\" for more help on exclude patterns.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('extract', parents=[common_parser],",
            "                                          description=self.do_extract.__doc__,",
            "                                          epilog=extract_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_extract)",
            "        subparser.add_argument('-n', '--dry-run', dest='dry_run',",
            "                               default=False, action='store_true',",
            "                               help='do not actually change any files')",
            "        subparser.add_argument('-e', '--exclude', dest='excludes',",
            "                               type=ExcludePattern, action='append',",
            "                               metavar=\"PATTERN\", help='exclude paths matching PATTERN')",
            "        subparser.add_argument('--exclude-from', dest='exclude_files',",
            "                               type=argparse.FileType('r'), action='append',",
            "                               metavar='EXCLUDEFILE', help='read exclude patterns from EXCLUDEFILE, one per line')",
            "        subparser.add_argument('--numeric-owner', dest='numeric_owner',",
            "                               action='store_true', default=False,",
            "                               help='only obey numeric user and group identifiers')",
            "        subparser.add_argument('--strip-components', dest='strip_components',",
            "                               type=int, default=0, metavar='NUMBER',",
            "                               help='Remove the specified number of leading path elements. Pathnames with fewer elements will be silently skipped.')",
            "        subparser.add_argument('archive', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to extract')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,",
            "                               help='paths to extract')",
            "",
            "        delete_epilog = textwrap.dedent(\"\"\"",
            "        This command deletes an archive from the repository. Any disk space not",
            "        shared with any other existing archive is also reclaimed.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('delete', parents=[common_parser],",
            "                                          description=self.do_delete.__doc__,",
            "                                          epilog=delete_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_delete)",
            "        subparser.add_argument('-s', '--stats', dest='stats',",
            "                               action='store_true', default=False,",
            "                               help='print statistics for the deleted archive')",
            "        subparser.add_argument('archive', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to delete')",
            "",
            "        list_epilog = textwrap.dedent(\"\"\"",
            "        This command lists the contents of a repository or an archive.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('list', parents=[common_parser],",
            "                                          description=self.do_list.__doc__,",
            "                                          epilog=list_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_list)",
            "        subparser.add_argument('src', metavar='REPOSITORY_OR_ARCHIVE', type=location_validator(),",
            "                               help='repository/archive to list contents of')",
            "        mount_epilog = textwrap.dedent(\"\"\"",
            "        This command mounts an archive as a FUSE filesystem. This can be useful for",
            "        browsing an archive or restoring individual files. Unless the ``--foreground``",
            "        option is given the command will run in the background until the filesystem",
            "        is ``umounted``.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('mount', parents=[common_parser],",
            "                                          description=self.do_mount.__doc__,",
            "                                          epilog=mount_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_mount)",
            "        subparser.add_argument('src', metavar='REPOSITORY_OR_ARCHIVE', type=location_validator(),",
            "                               help='repository/archive to mount')",
            "        subparser.add_argument('mountpoint', metavar='MOUNTPOINT', type=str,",
            "                               help='where to mount filesystem')",
            "        subparser.add_argument('-f', '--foreground', dest='foreground',",
            "                               action='store_true', default=False,",
            "                               help='stay in foreground, do not daemonize')",
            "        subparser.add_argument('-o', dest='options', type=str,",
            "                               help='Extra mount options')",
            "",
            "        info_epilog = textwrap.dedent(\"\"\"",
            "        This command displays some detailed information about the specified archive.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('info', parents=[common_parser],",
            "                                          description=self.do_info.__doc__,",
            "                                          epilog=info_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_info)",
            "        subparser.add_argument('archive', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to display information about')",
            "",
            "        prune_epilog = textwrap.dedent(\"\"\"",
            "        The prune command prunes a repository by deleting archives not matching",
            "        any of the specified retention options. This command is normally used by",
            "        automated backup scripts wanting to keep a certain number of historic backups.",
            "",
            "        As an example, \"-d 7\" means to keep the latest backup on each day for 7 days.",
            "        Days without backups do not count towards the total.",
            "        The rules are applied from hourly to yearly, and backups selected by previous",
            "        rules do not count towards those of later rules. The time that each backup",
            "        completes is used for pruning purposes. Dates and times are interpreted in",
            "        the local timezone, and weeks go from Monday to Sunday. Specifying a",
            "        negative number of archives to keep means that there is no limit.",
            "",
            "        The \"--keep-within\" option takes an argument of the form \"<int><char>\",",
            "        where char is \"H\", \"d\", \"w\", \"m\", \"y\". For example, \"--keep-within 2d\" means",
            "        to keep all archives that were created within the past 48 hours.",
            "        \"1m\" is taken to mean \"31d\". The archives kept with this option do not",
            "        count towards the totals specified by any other options.",
            "",
            "        If a prefix is set with -p, then only archives that start with the prefix are",
            "        considered for deletion and only those archives count towards the totals",
            "        specified by the rules.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('prune', parents=[common_parser],",
            "                                          description=self.do_prune.__doc__,",
            "                                          epilog=prune_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=self.do_prune)",
            "        subparser.add_argument('-n', '--dry-run', dest='dry_run',",
            "                               default=False, action='store_true',",
            "                               help='do not change repository')",
            "        subparser.add_argument('-s', '--stats', dest='stats',",
            "                               action='store_true', default=False,",
            "                               help='print statistics for the deleted archive')",
            "        subparser.add_argument('--keep-within', dest='within', type=str, metavar='WITHIN',",
            "                               help='keep all archives within this time interval')",
            "        subparser.add_argument('-H', '--keep-hourly', dest='hourly', type=int, default=0,",
            "                               help='number of hourly archives to keep')",
            "        subparser.add_argument('-d', '--keep-daily', dest='daily', type=int, default=0,",
            "                               help='number of daily archives to keep')",
            "        subparser.add_argument('-w', '--keep-weekly', dest='weekly', type=int, default=0,",
            "                               help='number of weekly archives to keep')",
            "        subparser.add_argument('-m', '--keep-monthly', dest='monthly', type=int, default=0,",
            "                               help='number of monthly archives to keep')",
            "        subparser.add_argument('-y', '--keep-yearly', dest='yearly', type=int, default=0,",
            "                               help='number of yearly archives to keep')",
            "        subparser.add_argument('-p', '--prefix', dest='prefix', type=str,",
            "                               help='only consider archive names starting with this prefix')",
            "        subparser.add_argument('repository', metavar='REPOSITORY',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to prune')",
            "",
            "        subparser = subparsers.add_parser('help', parents=[common_parser],",
            "                                          description='Extra help')",
            "        subparser.add_argument('--epilog-only', dest='epilog_only',",
            "                               action='store_true', default=False)",
            "        subparser.add_argument('--usage-only', dest='usage_only',",
            "                               action='store_true', default=False)",
            "        subparser.set_defaults(func=functools.partial(self.do_help, parser, subparsers.choices))",
            "        subparser.add_argument('topic', metavar='TOPIC', type=str, nargs='?',",
            "                               help='additional help on TOPIC')",
            "",
            "        args = parser.parse_args(args or ['-h'])",
            "        self.verbose = args.verbose",
            "        update_excludes(args)",
            "        return args.func(args)",
            "",
            "",
            "def main():",
            "    # Make sure stdout and stderr have errors='replace') to avoid unicode",
            "    # issues when print()-ing unicode file names",
            "    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, sys.stdout.encoding, 'replace', line_buffering=True)",
            "    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, sys.stderr.encoding, 'replace', line_buffering=True)",
            "    archiver = Archiver()",
            "    try:",
            "        exit_code = archiver.run(sys.argv[1:])",
            "    except Error as e:",
            "        archiver.print_error(e.get_message())",
            "        exit_code = e.exit_code",
            "    except KeyboardInterrupt:",
            "        archiver.print_error('Error: Keyboard interrupt')",
            "        exit_code = 1",
            "    else:",
            "        if exit_code:",
            "            archiver.print_error('Exiting with failure status due to previous errors')",
            "    sys.exit(exit_code)",
            "",
            "if __name__ == '__main__':",
            "    main()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "shuup.front.urls"
        ]
    },
    "attic/cache.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " from attic.remote import cache_if_remote"
            },
            "1": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " import msgpack"
            },
            "2": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " import os"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 5,
                "PatchRowcode": "+import sys"
            },
            "4": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " from binascii import hexlify"
            },
            "5": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " import shutil"
            },
            "6": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 9,
                "PatchRowcode": "+from .key import PlaintextKey"
            },
            "8": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " from .helpers import Error, get_cache_dir, decode_dict, st_mtime_ns, unhexlify, UpgradableLock, int_to_bigint, \\"
            },
            "9": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 11,
                "PatchRowcode": "     bigint_to_int"
            },
            "10": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " from .hashindex import ChunkIndex"
            },
            "11": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 18,
                "PatchRowcode": "     class RepositoryReplay(Error):"
            },
            "12": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 19,
                "PatchRowcode": "         \"\"\"Cache is newer than repository, refusing to continue\"\"\""
            },
            "13": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def __init__(self, repository, key, manifest, path=None, sync=True):"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 21,
                "PatchRowcode": "+    class CacheInitAbortedError(Error):"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 22,
                "PatchRowcode": "+        \"\"\"Cache initialization aborted\"\"\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 23,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 24,
                "PatchRowcode": "+"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+    class EncryptionMethodMismatch(Error):"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+        \"\"\"Repository encryption method changed since last acccess, refusing to continue"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 27,
                "PatchRowcode": "+        \"\"\""
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 28,
                "PatchRowcode": "+"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+    def __init__(self, repository, key, manifest, path=None, sync=True, warn_if_unencrypted=True):"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+        self.lock = None"
            },
            "25": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 31,
                "PatchRowcode": "         self.timestamp = None"
            },
            "26": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 32,
                "PatchRowcode": "         self.txn_active = False"
            },
            "27": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 33,
                "PatchRowcode": "         self.repository = repository"
            },
            "28": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 34,
                "PatchRowcode": "         self.key = key"
            },
            "29": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 35,
                "PatchRowcode": "         self.manifest = manifest"
            },
            "30": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 36,
                "PatchRowcode": "         self.path = path or os.path.join(get_cache_dir(), hexlify(repository.id).decode('ascii'))"
            },
            "31": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 37,
                "PatchRowcode": "         if not os.path.exists(self.path):"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 38,
                "PatchRowcode": "+            if warn_if_unencrypted and isinstance(key, PlaintextKey):"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 39,
                "PatchRowcode": "+                if 'ATTIC_UNKNOWN_UNENCRYPTED_REPO_ACCESS_IS_OK' not in os.environ:"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 40,
                "PatchRowcode": "+                    print(\"\"\"Warning: Attempting to access a previously unknown unencrypted repository\\n\"\"\", file=sys.stderr)"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 41,
                "PatchRowcode": "+                    answer = input('Do you want to continue? [yN] ')"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+                    if not (answer and answer in 'Yy'):"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+                        raise self.CacheInitAbortedError()"
            },
            "38": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 44,
                "PatchRowcode": "             self.create()"
            },
            "39": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 45,
                "PatchRowcode": "         self.open()"
            },
            "40": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 46,
                "PatchRowcode": "         if sync and self.manifest.id != self.manifest_id:"
            },
            "41": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 47,
                "PatchRowcode": "             # If repository is older than the cache something fishy is going on"
            },
            "42": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 48,
                "PatchRowcode": "             if self.timestamp and self.timestamp > manifest.timestamp:"
            },
            "43": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 49,
                "PatchRowcode": "                 raise self.RepositoryReplay()"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+            # Make sure an encrypted repository has not been swapped for an unencrypted repository"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+            if self.key_type is not None and self.key_type != str(key.TYPE):"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+                raise self.EncryptionMethodMismatch()"
            },
            "47": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 53,
                "PatchRowcode": "             self.sync()"
            },
            "48": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 54,
                "PatchRowcode": "             self.commit()"
            },
            "49": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 55,
                "PatchRowcode": " "
            },
            "50": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 85,
                "PatchRowcode": "         self.id = self.config.get('cache', 'repository')"
            },
            "51": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 86,
                "PatchRowcode": "         self.manifest_id = unhexlify(self.config.get('cache', 'manifest'))"
            },
            "52": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 87,
                "PatchRowcode": "         self.timestamp = self.config.get('cache', 'timestamp', fallback=None)"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+        self.key_type = self.config.get('cache', 'key_type', fallback=None)"
            },
            "54": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 89,
                "PatchRowcode": "         self.chunks = ChunkIndex.read(os.path.join(self.path, 'chunks').encode('utf-8'))"
            },
            "55": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 90,
                "PatchRowcode": "         self.files = None"
            },
            "56": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 91,
                "PatchRowcode": " "
            },
            "57": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 92,
                "PatchRowcode": "     def close(self):"
            },
            "58": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.lock.release()"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+        if self.lock:"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+            self.lock.release()"
            },
            "61": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 95,
                "PatchRowcode": " "
            },
            "62": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 96,
                "PatchRowcode": "     def _read_files(self):"
            },
            "63": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 97,
                "PatchRowcode": "         self.files = {}"
            },
            "64": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": 133,
                "PatchRowcode": "                         msgpack.pack((path_hash, item), fd)"
            },
            "65": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": 134,
                "PatchRowcode": "         self.config.set('cache', 'manifest', hexlify(self.manifest.id).decode('ascii'))"
            },
            "66": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": 135,
                "PatchRowcode": "         self.config.set('cache', 'timestamp', self.manifest.timestamp)"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 136,
                "PatchRowcode": "+        self.config.set('cache', 'key_type', str(self.key.TYPE))"
            },
            "68": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": 137,
                "PatchRowcode": "         with open(os.path.join(self.path, 'config'), 'w') as fd:"
            },
            "69": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 138,
                "PatchRowcode": "             self.config.write(fd)"
            },
            "70": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 139,
                "PatchRowcode": "         self.chunks.write(os.path.join(self.path, 'chunks').encode('utf-8'))"
            }
        },
        "frontPatchFile": [
            "from configparser import RawConfigParser",
            "from attic.remote import cache_if_remote",
            "import msgpack",
            "import os",
            "from binascii import hexlify",
            "import shutil",
            "",
            "from .helpers import Error, get_cache_dir, decode_dict, st_mtime_ns, unhexlify, UpgradableLock, int_to_bigint, \\",
            "    bigint_to_int",
            "from .hashindex import ChunkIndex",
            "",
            "",
            "class Cache(object):",
            "    \"\"\"Client Side cache",
            "    \"\"\"",
            "    class RepositoryReplay(Error):",
            "        \"\"\"Cache is newer than repository, refusing to continue\"\"\"",
            "",
            "    def __init__(self, repository, key, manifest, path=None, sync=True):",
            "        self.timestamp = None",
            "        self.txn_active = False",
            "        self.repository = repository",
            "        self.key = key",
            "        self.manifest = manifest",
            "        self.path = path or os.path.join(get_cache_dir(), hexlify(repository.id).decode('ascii'))",
            "        if not os.path.exists(self.path):",
            "            self.create()",
            "        self.open()",
            "        if sync and self.manifest.id != self.manifest_id:",
            "            # If repository is older than the cache something fishy is going on",
            "            if self.timestamp and self.timestamp > manifest.timestamp:",
            "                raise self.RepositoryReplay()",
            "            self.sync()",
            "            self.commit()",
            "",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    def create(self):",
            "        \"\"\"Create a new empty cache at `path`",
            "        \"\"\"",
            "        os.makedirs(self.path)",
            "        with open(os.path.join(self.path, 'README'), 'w') as fd:",
            "            fd.write('This is an Attic cache')",
            "        config = RawConfigParser()",
            "        config.add_section('cache')",
            "        config.set('cache', 'version', '1')",
            "        config.set('cache', 'repository', hexlify(self.repository.id).decode('ascii'))",
            "        config.set('cache', 'manifest', '')",
            "        with open(os.path.join(self.path, 'config'), 'w') as fd:",
            "            config.write(fd)",
            "        ChunkIndex().write(os.path.join(self.path, 'chunks').encode('utf-8'))",
            "        with open(os.path.join(self.path, 'files'), 'w') as fd:",
            "            pass  # empty file",
            "",
            "    def open(self):",
            "        if not os.path.isdir(self.path):",
            "            raise Exception('%s Does not look like an Attic cache' % self.path)",
            "        self.lock = UpgradableLock(os.path.join(self.path, 'config'), exclusive=True)",
            "        self.rollback()",
            "        self.config = RawConfigParser()",
            "        self.config.read(os.path.join(self.path, 'config'))",
            "        if self.config.getint('cache', 'version') != 1:",
            "            raise Exception('%s Does not look like an Attic cache')",
            "        self.id = self.config.get('cache', 'repository')",
            "        self.manifest_id = unhexlify(self.config.get('cache', 'manifest'))",
            "        self.timestamp = self.config.get('cache', 'timestamp', fallback=None)",
            "        self.chunks = ChunkIndex.read(os.path.join(self.path, 'chunks').encode('utf-8'))",
            "        self.files = None",
            "",
            "    def close(self):",
            "        self.lock.release()",
            "",
            "    def _read_files(self):",
            "        self.files = {}",
            "        self._newest_mtime = 0",
            "        with open(os.path.join(self.path, 'files'), 'rb') as fd:",
            "            u = msgpack.Unpacker(use_list=True)",
            "            while True:",
            "                data = fd.read(64 * 1024)",
            "                if not data:",
            "                    break",
            "                u.feed(data)",
            "                for path_hash, item in u:",
            "                    item[0] += 1",
            "                    self.files[path_hash] = msgpack.packb(item)",
            "",
            "    def begin_txn(self):",
            "        # Initialize transaction snapshot",
            "        txn_dir = os.path.join(self.path, 'txn.tmp')",
            "        os.mkdir(txn_dir)",
            "        shutil.copy(os.path.join(self.path, 'config'), txn_dir)",
            "        shutil.copy(os.path.join(self.path, 'chunks'), txn_dir)",
            "        shutil.copy(os.path.join(self.path, 'files'), txn_dir)",
            "        os.rename(os.path.join(self.path, 'txn.tmp'),",
            "                  os.path.join(self.path, 'txn.active'))",
            "        self.txn_active = True",
            "",
            "    def commit(self):",
            "        \"\"\"Commit transaction",
            "        \"\"\"",
            "        if not self.txn_active:",
            "            return",
            "        if self.files is not None:",
            "            with open(os.path.join(self.path, 'files'), 'wb') as fd:",
            "                for path_hash, item in self.files.items():",
            "                    # Discard cached files with the newest mtime to avoid",
            "                    # issues with filesystem snapshots and mtime precision",
            "                    item = msgpack.unpackb(item)",
            "                    if item[0] < 10 and bigint_to_int(item[3]) < self._newest_mtime:",
            "                        msgpack.pack((path_hash, item), fd)",
            "        self.config.set('cache', 'manifest', hexlify(self.manifest.id).decode('ascii'))",
            "        self.config.set('cache', 'timestamp', self.manifest.timestamp)",
            "        with open(os.path.join(self.path, 'config'), 'w') as fd:",
            "            self.config.write(fd)",
            "        self.chunks.write(os.path.join(self.path, 'chunks').encode('utf-8'))",
            "        os.rename(os.path.join(self.path, 'txn.active'),",
            "                  os.path.join(self.path, 'txn.tmp'))",
            "        shutil.rmtree(os.path.join(self.path, 'txn.tmp'))",
            "        self.txn_active = False",
            "",
            "    def rollback(self):",
            "        \"\"\"Roll back partial and aborted transactions",
            "        \"\"\"",
            "        # Remove partial transaction",
            "        if os.path.exists(os.path.join(self.path, 'txn.tmp')):",
            "            shutil.rmtree(os.path.join(self.path, 'txn.tmp'))",
            "        # Roll back active transaction",
            "        txn_dir = os.path.join(self.path, 'txn.active')",
            "        if os.path.exists(txn_dir):",
            "            shutil.copy(os.path.join(txn_dir, 'config'), self.path)",
            "            shutil.copy(os.path.join(txn_dir, 'chunks'), self.path)",
            "            shutil.copy(os.path.join(txn_dir, 'files'), self.path)",
            "            os.rename(txn_dir, os.path.join(self.path, 'txn.tmp'))",
            "            if os.path.exists(os.path.join(self.path, 'txn.tmp')):",
            "                shutil.rmtree(os.path.join(self.path, 'txn.tmp'))",
            "        self.txn_active = False",
            "",
            "    def sync(self):",
            "        \"\"\"Initializes cache by fetching and reading all archive indicies",
            "        \"\"\"",
            "        def add(id, size, csize):",
            "            try:",
            "                count, size, csize = self.chunks[id]",
            "                self.chunks[id] = count + 1, size, csize",
            "            except KeyError:",
            "                self.chunks[id] = 1, size, csize",
            "        self.begin_txn()",
            "        print('Initializing cache...')",
            "        self.chunks.clear()",
            "        unpacker = msgpack.Unpacker()",
            "        repository = cache_if_remote(self.repository)",
            "        for name, info in self.manifest.archives.items():",
            "            archive_id = info[b'id']",
            "            cdata = repository.get(archive_id)",
            "            data = self.key.decrypt(archive_id, cdata)",
            "            add(archive_id, len(data), len(cdata))",
            "            archive = msgpack.unpackb(data)",
            "            if archive[b'version'] != 1:",
            "                raise Exception('Unknown archive metadata version')",
            "            decode_dict(archive, (b'name',))",
            "            print('Analyzing archive:', archive[b'name'])",
            "            for key, chunk in zip(archive[b'items'], repository.get_many(archive[b'items'])):",
            "                data = self.key.decrypt(key, chunk)",
            "                add(key, len(data), len(chunk))",
            "                unpacker.feed(data)",
            "                for item in unpacker:",
            "                    if b'chunks' in item:",
            "                        for chunk_id, size, csize in item[b'chunks']:",
            "                            add(chunk_id, size, csize)",
            "",
            "    def add_chunk(self, id, data, stats):",
            "        if not self.txn_active:",
            "            self.begin_txn()",
            "        if self.seen_chunk(id):",
            "            return self.chunk_incref(id, stats)",
            "        size = len(data)",
            "        data = self.key.encrypt(data)",
            "        csize = len(data)",
            "        self.repository.put(id, data, wait=False)",
            "        self.chunks[id] = (1, size, csize)",
            "        stats.update(size, csize, True)",
            "        return id, size, csize",
            "",
            "    def seen_chunk(self, id):",
            "        return self.chunks.get(id, (0, 0, 0))[0]",
            "",
            "    def chunk_incref(self, id, stats):",
            "        if not self.txn_active:",
            "            self.begin_txn()",
            "        count, size, csize = self.chunks[id]",
            "        self.chunks[id] = (count + 1, size, csize)",
            "        stats.update(size, csize, False)",
            "        return id, size, csize",
            "",
            "    def chunk_decref(self, id, stats):",
            "        if not self.txn_active:",
            "            self.begin_txn()",
            "        count, size, csize = self.chunks[id]",
            "        if count == 1:",
            "            del self.chunks[id]",
            "            self.repository.delete(id, wait=False)",
            "            stats.update(-size, -csize, True)",
            "        else:",
            "            self.chunks[id] = (count - 1, size, csize)",
            "            stats.update(-size, -csize, False)",
            "",
            "    def file_known_and_unchanged(self, path_hash, st):",
            "        if self.files is None:",
            "            self._read_files()",
            "        entry = self.files.get(path_hash)",
            "        if not entry:",
            "            return None",
            "        entry = msgpack.unpackb(entry)",
            "        if entry[2] == st.st_size and bigint_to_int(entry[3]) == st_mtime_ns(st) and entry[1] == st.st_ino:",
            "            # reset entry age",
            "            entry[0] = 0",
            "            self.files[path_hash] = msgpack.packb(entry)",
            "            return entry[4]",
            "        else:",
            "            return None",
            "",
            "    def memorize_file(self, path_hash, st, ids):",
            "        # Entry: Age, inode, size, mtime, chunk ids",
            "        mtime_ns = st_mtime_ns(st)",
            "        self.files[path_hash] = msgpack.packb((0, st.st_ino, st.st_size, int_to_bigint(mtime_ns), ids))",
            "        self._newest_mtime = max(self._newest_mtime, mtime_ns)"
        ],
        "afterPatchFile": [
            "from configparser import RawConfigParser",
            "from attic.remote import cache_if_remote",
            "import msgpack",
            "import os",
            "import sys",
            "from binascii import hexlify",
            "import shutil",
            "",
            "from .key import PlaintextKey",
            "from .helpers import Error, get_cache_dir, decode_dict, st_mtime_ns, unhexlify, UpgradableLock, int_to_bigint, \\",
            "    bigint_to_int",
            "from .hashindex import ChunkIndex",
            "",
            "",
            "class Cache(object):",
            "    \"\"\"Client Side cache",
            "    \"\"\"",
            "    class RepositoryReplay(Error):",
            "        \"\"\"Cache is newer than repository, refusing to continue\"\"\"",
            "",
            "    class CacheInitAbortedError(Error):",
            "        \"\"\"Cache initialization aborted\"\"\"",
            "",
            "",
            "    class EncryptionMethodMismatch(Error):",
            "        \"\"\"Repository encryption method changed since last acccess, refusing to continue",
            "        \"\"\"",
            "",
            "    def __init__(self, repository, key, manifest, path=None, sync=True, warn_if_unencrypted=True):",
            "        self.lock = None",
            "        self.timestamp = None",
            "        self.txn_active = False",
            "        self.repository = repository",
            "        self.key = key",
            "        self.manifest = manifest",
            "        self.path = path or os.path.join(get_cache_dir(), hexlify(repository.id).decode('ascii'))",
            "        if not os.path.exists(self.path):",
            "            if warn_if_unencrypted and isinstance(key, PlaintextKey):",
            "                if 'ATTIC_UNKNOWN_UNENCRYPTED_REPO_ACCESS_IS_OK' not in os.environ:",
            "                    print(\"\"\"Warning: Attempting to access a previously unknown unencrypted repository\\n\"\"\", file=sys.stderr)",
            "                    answer = input('Do you want to continue? [yN] ')",
            "                    if not (answer and answer in 'Yy'):",
            "                        raise self.CacheInitAbortedError()",
            "            self.create()",
            "        self.open()",
            "        if sync and self.manifest.id != self.manifest_id:",
            "            # If repository is older than the cache something fishy is going on",
            "            if self.timestamp and self.timestamp > manifest.timestamp:",
            "                raise self.RepositoryReplay()",
            "            # Make sure an encrypted repository has not been swapped for an unencrypted repository",
            "            if self.key_type is not None and self.key_type != str(key.TYPE):",
            "                raise self.EncryptionMethodMismatch()",
            "            self.sync()",
            "            self.commit()",
            "",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    def create(self):",
            "        \"\"\"Create a new empty cache at `path`",
            "        \"\"\"",
            "        os.makedirs(self.path)",
            "        with open(os.path.join(self.path, 'README'), 'w') as fd:",
            "            fd.write('This is an Attic cache')",
            "        config = RawConfigParser()",
            "        config.add_section('cache')",
            "        config.set('cache', 'version', '1')",
            "        config.set('cache', 'repository', hexlify(self.repository.id).decode('ascii'))",
            "        config.set('cache', 'manifest', '')",
            "        with open(os.path.join(self.path, 'config'), 'w') as fd:",
            "            config.write(fd)",
            "        ChunkIndex().write(os.path.join(self.path, 'chunks').encode('utf-8'))",
            "        with open(os.path.join(self.path, 'files'), 'w') as fd:",
            "            pass  # empty file",
            "",
            "    def open(self):",
            "        if not os.path.isdir(self.path):",
            "            raise Exception('%s Does not look like an Attic cache' % self.path)",
            "        self.lock = UpgradableLock(os.path.join(self.path, 'config'), exclusive=True)",
            "        self.rollback()",
            "        self.config = RawConfigParser()",
            "        self.config.read(os.path.join(self.path, 'config'))",
            "        if self.config.getint('cache', 'version') != 1:",
            "            raise Exception('%s Does not look like an Attic cache')",
            "        self.id = self.config.get('cache', 'repository')",
            "        self.manifest_id = unhexlify(self.config.get('cache', 'manifest'))",
            "        self.timestamp = self.config.get('cache', 'timestamp', fallback=None)",
            "        self.key_type = self.config.get('cache', 'key_type', fallback=None)",
            "        self.chunks = ChunkIndex.read(os.path.join(self.path, 'chunks').encode('utf-8'))",
            "        self.files = None",
            "",
            "    def close(self):",
            "        if self.lock:",
            "            self.lock.release()",
            "",
            "    def _read_files(self):",
            "        self.files = {}",
            "        self._newest_mtime = 0",
            "        with open(os.path.join(self.path, 'files'), 'rb') as fd:",
            "            u = msgpack.Unpacker(use_list=True)",
            "            while True:",
            "                data = fd.read(64 * 1024)",
            "                if not data:",
            "                    break",
            "                u.feed(data)",
            "                for path_hash, item in u:",
            "                    item[0] += 1",
            "                    self.files[path_hash] = msgpack.packb(item)",
            "",
            "    def begin_txn(self):",
            "        # Initialize transaction snapshot",
            "        txn_dir = os.path.join(self.path, 'txn.tmp')",
            "        os.mkdir(txn_dir)",
            "        shutil.copy(os.path.join(self.path, 'config'), txn_dir)",
            "        shutil.copy(os.path.join(self.path, 'chunks'), txn_dir)",
            "        shutil.copy(os.path.join(self.path, 'files'), txn_dir)",
            "        os.rename(os.path.join(self.path, 'txn.tmp'),",
            "                  os.path.join(self.path, 'txn.active'))",
            "        self.txn_active = True",
            "",
            "    def commit(self):",
            "        \"\"\"Commit transaction",
            "        \"\"\"",
            "        if not self.txn_active:",
            "            return",
            "        if self.files is not None:",
            "            with open(os.path.join(self.path, 'files'), 'wb') as fd:",
            "                for path_hash, item in self.files.items():",
            "                    # Discard cached files with the newest mtime to avoid",
            "                    # issues with filesystem snapshots and mtime precision",
            "                    item = msgpack.unpackb(item)",
            "                    if item[0] < 10 and bigint_to_int(item[3]) < self._newest_mtime:",
            "                        msgpack.pack((path_hash, item), fd)",
            "        self.config.set('cache', 'manifest', hexlify(self.manifest.id).decode('ascii'))",
            "        self.config.set('cache', 'timestamp', self.manifest.timestamp)",
            "        self.config.set('cache', 'key_type', str(self.key.TYPE))",
            "        with open(os.path.join(self.path, 'config'), 'w') as fd:",
            "            self.config.write(fd)",
            "        self.chunks.write(os.path.join(self.path, 'chunks').encode('utf-8'))",
            "        os.rename(os.path.join(self.path, 'txn.active'),",
            "                  os.path.join(self.path, 'txn.tmp'))",
            "        shutil.rmtree(os.path.join(self.path, 'txn.tmp'))",
            "        self.txn_active = False",
            "",
            "    def rollback(self):",
            "        \"\"\"Roll back partial and aborted transactions",
            "        \"\"\"",
            "        # Remove partial transaction",
            "        if os.path.exists(os.path.join(self.path, 'txn.tmp')):",
            "            shutil.rmtree(os.path.join(self.path, 'txn.tmp'))",
            "        # Roll back active transaction",
            "        txn_dir = os.path.join(self.path, 'txn.active')",
            "        if os.path.exists(txn_dir):",
            "            shutil.copy(os.path.join(txn_dir, 'config'), self.path)",
            "            shutil.copy(os.path.join(txn_dir, 'chunks'), self.path)",
            "            shutil.copy(os.path.join(txn_dir, 'files'), self.path)",
            "            os.rename(txn_dir, os.path.join(self.path, 'txn.tmp'))",
            "            if os.path.exists(os.path.join(self.path, 'txn.tmp')):",
            "                shutil.rmtree(os.path.join(self.path, 'txn.tmp'))",
            "        self.txn_active = False",
            "",
            "    def sync(self):",
            "        \"\"\"Initializes cache by fetching and reading all archive indicies",
            "        \"\"\"",
            "        def add(id, size, csize):",
            "            try:",
            "                count, size, csize = self.chunks[id]",
            "                self.chunks[id] = count + 1, size, csize",
            "            except KeyError:",
            "                self.chunks[id] = 1, size, csize",
            "        self.begin_txn()",
            "        print('Initializing cache...')",
            "        self.chunks.clear()",
            "        unpacker = msgpack.Unpacker()",
            "        repository = cache_if_remote(self.repository)",
            "        for name, info in self.manifest.archives.items():",
            "            archive_id = info[b'id']",
            "            cdata = repository.get(archive_id)",
            "            data = self.key.decrypt(archive_id, cdata)",
            "            add(archive_id, len(data), len(cdata))",
            "            archive = msgpack.unpackb(data)",
            "            if archive[b'version'] != 1:",
            "                raise Exception('Unknown archive metadata version')",
            "            decode_dict(archive, (b'name',))",
            "            print('Analyzing archive:', archive[b'name'])",
            "            for key, chunk in zip(archive[b'items'], repository.get_many(archive[b'items'])):",
            "                data = self.key.decrypt(key, chunk)",
            "                add(key, len(data), len(chunk))",
            "                unpacker.feed(data)",
            "                for item in unpacker:",
            "                    if b'chunks' in item:",
            "                        for chunk_id, size, csize in item[b'chunks']:",
            "                            add(chunk_id, size, csize)",
            "",
            "    def add_chunk(self, id, data, stats):",
            "        if not self.txn_active:",
            "            self.begin_txn()",
            "        if self.seen_chunk(id):",
            "            return self.chunk_incref(id, stats)",
            "        size = len(data)",
            "        data = self.key.encrypt(data)",
            "        csize = len(data)",
            "        self.repository.put(id, data, wait=False)",
            "        self.chunks[id] = (1, size, csize)",
            "        stats.update(size, csize, True)",
            "        return id, size, csize",
            "",
            "    def seen_chunk(self, id):",
            "        return self.chunks.get(id, (0, 0, 0))[0]",
            "",
            "    def chunk_incref(self, id, stats):",
            "        if not self.txn_active:",
            "            self.begin_txn()",
            "        count, size, csize = self.chunks[id]",
            "        self.chunks[id] = (count + 1, size, csize)",
            "        stats.update(size, csize, False)",
            "        return id, size, csize",
            "",
            "    def chunk_decref(self, id, stats):",
            "        if not self.txn_active:",
            "            self.begin_txn()",
            "        count, size, csize = self.chunks[id]",
            "        if count == 1:",
            "            del self.chunks[id]",
            "            self.repository.delete(id, wait=False)",
            "            stats.update(-size, -csize, True)",
            "        else:",
            "            self.chunks[id] = (count - 1, size, csize)",
            "            stats.update(-size, -csize, False)",
            "",
            "    def file_known_and_unchanged(self, path_hash, st):",
            "        if self.files is None:",
            "            self._read_files()",
            "        entry = self.files.get(path_hash)",
            "        if not entry:",
            "            return None",
            "        entry = msgpack.unpackb(entry)",
            "        if entry[2] == st.st_size and bigint_to_int(entry[3]) == st_mtime_ns(st) and entry[1] == st.st_ino:",
            "            # reset entry age",
            "            entry[0] = 0",
            "            self.files[path_hash] = msgpack.packb(entry)",
            "            return entry[4]",
            "        else:",
            "            return None",
            "",
            "    def memorize_file(self, path_hash, st, ids):",
            "        # Entry: Age, inode, size, mtime, chunk ids",
            "        mtime_ns = st_mtime_ns(st)",
            "        self.files[path_hash] = msgpack.packb((0, st.st_ino, st.st_size, int_to_bigint(mtime_ns), ids))",
            "        self._newest_mtime = max(self._newest_mtime, mtime_ns)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "19": [
                "Cache",
                "__init__"
            ],
            "72": [
                "Cache",
                "close"
            ]
        },
        "addLocation": []
    },
    "attic/testsuite/archiver.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1,
                "PatchRowcode": "+from binascii import hexlify"
            },
            "1": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2,
                "PatchRowcode": "+from configparser import RawConfigParser"
            },
            "2": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " import os"
            },
            "3": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " from io import StringIO"
            },
            "4": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " import stat"
            },
            "5": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " from attic import xattr"
            },
            "6": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " from attic.archive import Archive, ChunkBuffer"
            },
            "7": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " from attic.archiver import Archiver"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+from attic.cache import Cache"
            },
            "9": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " from attic.crypto import bytes_to_long, num_aes_blocks"
            },
            "10": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from attic.helpers import Manifest"
            },
            "11": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " from attic.remote import RemoteRepository, PathNotAllowed"
            },
            "12": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 44,
                "PatchRowcode": "         os.chdir(self.old)"
            },
            "13": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 45,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 46,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+class environment_variable:"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+    def __init__(self, **values):"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+        self.values = values"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+        self.old_values = {}"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+    def __enter__(self):"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+        for k, v in self.values.items():"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 54,
                "PatchRowcode": "+            self.old_values[k] = os.environ.get(k)"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+            os.environ[k] = v"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 56,
                "PatchRowcode": "+"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 57,
                "PatchRowcode": "+    def __exit__(self, *args, **kw):"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 58,
                "PatchRowcode": "+        for k, v in self.old_values.items():"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+            if v is not None:"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 60,
                "PatchRowcode": "+                os.environ[k] = v"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 61,
                "PatchRowcode": "+"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+"
            },
            "31": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 63,
                "PatchRowcode": " class ArchiverTestCaseBase(AtticTestCase):"
            },
            "32": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": 64,
                "PatchRowcode": " "
            },
            "33": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 65,
                "PatchRowcode": "     prefix = ''"
            },
            "34": {
                "beforePatchRowNumber": 161,
                "afterPatchRowNumber": 180,
                "PatchRowcode": "         info_output = self.attic('info', self.repository_location + '::test')"
            },
            "35": {
                "beforePatchRowNumber": 162,
                "afterPatchRowNumber": 181,
                "PatchRowcode": "         self.assert_in('Number of files: 4', info_output)"
            },
            "36": {
                "beforePatchRowNumber": 163,
                "afterPatchRowNumber": 182,
                "PatchRowcode": "         shutil.rmtree(self.cache_path)"
            },
            "37": {
                "beforePatchRowNumber": 164,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        info_output2 = self.attic('info', self.repository_location + '::test')"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 183,
                "PatchRowcode": "+        with environment_variable(ATTIC_UNKNOWN_UNENCRYPTED_REPO_ACCESS_IS_OK='1'):"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 184,
                "PatchRowcode": "+            info_output2 = self.attic('info', self.repository_location + '::test')"
            },
            "40": {
                "beforePatchRowNumber": 165,
                "afterPatchRowNumber": 185,
                "PatchRowcode": "         # info_output2 starts with some \"initializing cache\" text but should"
            },
            "41": {
                "beforePatchRowNumber": 166,
                "afterPatchRowNumber": 186,
                "PatchRowcode": "         # end the same way as info_output"
            },
            "42": {
                "beforePatchRowNumber": 167,
                "afterPatchRowNumber": 187,
                "PatchRowcode": "         assert info_output2.endswith(info_output)"
            },
            "43": {
                "beforePatchRowNumber": 168,
                "afterPatchRowNumber": 188,
                "PatchRowcode": " "
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 189,
                "PatchRowcode": "+    def _extract_repository_id(self, path):"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 190,
                "PatchRowcode": "+        return Repository(self.repository_path).id"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 191,
                "PatchRowcode": "+"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 192,
                "PatchRowcode": "+    def _set_repository_id(self, path, id):"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 193,
                "PatchRowcode": "+        config = RawConfigParser()"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 194,
                "PatchRowcode": "+        config.read(os.path.join(path, 'config'))"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 195,
                "PatchRowcode": "+        config.set('repository', 'id', hexlify(id).decode('ascii'))"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 196,
                "PatchRowcode": "+        with open(os.path.join(path, 'config'), 'w') as fd:"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 197,
                "PatchRowcode": "+            config.write(fd)"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 198,
                "PatchRowcode": "+        return Repository(self.repository_path).id"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 199,
                "PatchRowcode": "+"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 200,
                "PatchRowcode": "+    def test_repository_swap_detection(self):"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 201,
                "PatchRowcode": "+        self.create_test_files()"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 202,
                "PatchRowcode": "+        os.environ['ATTIC_PASSPHRASE'] = 'passphrase'"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 203,
                "PatchRowcode": "+        self.attic('init', '--encryption=passphrase', self.repository_location)"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 204,
                "PatchRowcode": "+        repository_id = self._extract_repository_id(self.repository_path)"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 205,
                "PatchRowcode": "+        self.attic('create', self.repository_location + '::test', 'input')"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 206,
                "PatchRowcode": "+        shutil.rmtree(self.repository_path)"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 207,
                "PatchRowcode": "+        self.attic('init', '--encryption=none', self.repository_location)"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 208,
                "PatchRowcode": "+        self._set_repository_id(self.repository_path, repository_id)"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 209,
                "PatchRowcode": "+        self.assert_equal(repository_id, self._extract_repository_id(self.repository_path))"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 210,
                "PatchRowcode": "+        self.assert_raises(Cache.EncryptionMethodMismatch, lambda :self.attic('create', self.repository_location + '::test.2', 'input'))"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 211,
                "PatchRowcode": "+"
            },
            "67": {
                "beforePatchRowNumber": 169,
                "afterPatchRowNumber": 212,
                "PatchRowcode": "     def test_strip_components(self):"
            },
            "68": {
                "beforePatchRowNumber": 170,
                "afterPatchRowNumber": 213,
                "PatchRowcode": "         self.attic('init', self.repository_location)"
            },
            "69": {
                "beforePatchRowNumber": 171,
                "afterPatchRowNumber": 214,
                "PatchRowcode": "         self.create_regular_file('dir/file')"
            }
        },
        "frontPatchFile": [
            "import os",
            "from io import StringIO",
            "import stat",
            "import subprocess",
            "import sys",
            "import shutil",
            "import tempfile",
            "import time",
            "import unittest",
            "from hashlib import sha256",
            "from attic import xattr",
            "from attic.archive import Archive, ChunkBuffer",
            "from attic.archiver import Archiver",
            "from attic.crypto import bytes_to_long, num_aes_blocks",
            "from attic.helpers import Manifest",
            "from attic.remote import RemoteRepository, PathNotAllowed",
            "from attic.repository import Repository",
            "from attic.testsuite import AtticTestCase",
            "from attic.testsuite.mock import patch",
            "",
            "try:",
            "    import llfuse",
            "    has_llfuse = True",
            "except ImportError:",
            "    has_llfuse = False",
            "",
            "has_lchflags = hasattr(os, 'lchflags')",
            "",
            "src_dir = os.path.join(os.getcwd(), os.path.dirname(__file__), '..')",
            "",
            "",
            "class changedir:",
            "    def __init__(self, dir):",
            "        self.dir = dir",
            "",
            "    def __enter__(self):",
            "        self.old = os.getcwd()",
            "        os.chdir(self.dir)",
            "",
            "    def __exit__(self, *args, **kw):",
            "        os.chdir(self.old)",
            "",
            "",
            "class ArchiverTestCaseBase(AtticTestCase):",
            "",
            "    prefix = ''",
            "",
            "    def setUp(self):",
            "        os.environ['ATTIC_CHECK_I_KNOW_WHAT_I_AM_DOING'] = '1'",
            "        self.archiver = Archiver()",
            "        self.tmpdir = tempfile.mkdtemp()",
            "        self.repository_path = os.path.join(self.tmpdir, 'repository')",
            "        self.repository_location = self.prefix + self.repository_path",
            "        self.input_path = os.path.join(self.tmpdir, 'input')",
            "        self.output_path = os.path.join(self.tmpdir, 'output')",
            "        self.keys_path = os.path.join(self.tmpdir, 'keys')",
            "        self.cache_path = os.path.join(self.tmpdir, 'cache')",
            "        self.exclude_file_path = os.path.join(self.tmpdir, 'excludes')",
            "        os.environ['ATTIC_KEYS_DIR'] = self.keys_path",
            "        os.environ['ATTIC_CACHE_DIR'] = self.cache_path",
            "        os.mkdir(self.input_path)",
            "        os.mkdir(self.output_path)",
            "        os.mkdir(self.keys_path)",
            "        os.mkdir(self.cache_path)",
            "        with open(self.exclude_file_path, 'wb') as fd:",
            "            fd.write(b'input/file2\\n# A commment line, then a blank line\\n\\n')",
            "        self._old_wd = os.getcwd()",
            "        os.chdir(self.tmpdir)",
            "",
            "    def tearDown(self):",
            "        shutil.rmtree(self.tmpdir)",
            "        os.chdir(self._old_wd)",
            "",
            "    def attic(self, *args, **kw):",
            "        exit_code = kw.get('exit_code', 0)",
            "        fork = kw.get('fork', False)",
            "        if fork:",
            "            try:",
            "                output = subprocess.check_output((sys.executable, '-m', 'attic.archiver') + args)",
            "                ret = 0",
            "            except subprocess.CalledProcessError as e:",
            "                output = e.output",
            "                ret = e.returncode",
            "            output = os.fsdecode(output)",
            "            if ret != exit_code:",
            "                print(output)",
            "            self.assert_equal(exit_code, ret)",
            "            return output",
            "        args = list(args)",
            "        stdout, stderr = sys.stdout, sys.stderr",
            "        try:",
            "            output = StringIO()",
            "            sys.stdout = sys.stderr = output",
            "            ret = self.archiver.run(args)",
            "            sys.stdout, sys.stderr = stdout, stderr",
            "            if ret != exit_code:",
            "                print(output.getvalue())",
            "            self.assert_equal(exit_code, ret)",
            "            return output.getvalue()",
            "        finally:",
            "            sys.stdout, sys.stderr = stdout, stderr",
            "",
            "    def create_src_archive(self, name):",
            "        self.attic('create', self.repository_location + '::' + name, src_dir)",
            "",
            "",
            "class ArchiverTestCase(ArchiverTestCaseBase):",
            "",
            "    def create_regular_file(self, name, size=0, contents=None):",
            "        filename = os.path.join(self.input_path, name)",
            "        if not os.path.exists(os.path.dirname(filename)):",
            "            os.makedirs(os.path.dirname(filename))",
            "        with open(filename, 'wb') as fd:",
            "            if contents is None:",
            "                contents = b'X' * size",
            "            fd.write(contents)",
            "",
            "    def create_test_files(self):",
            "        \"\"\"Create a minimal test case including all supported file types",
            "        \"\"\"",
            "        # File",
            "        self.create_regular_file('empty', size=0)",
            "        # 2600-01-01 > 2**64 ns",
            "        os.utime('input/empty', (19880895600, 19880895600))",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('flagfile', size=1024)",
            "        # Directory",
            "        self.create_regular_file('dir2/file2', size=1024 * 80)",
            "        # File owner",
            "        os.chown('input/file1', 100, 200)",
            "        # File mode",
            "        os.chmod('input/file1', 0o7755)",
            "        os.chmod('input/dir2', 0o555)",
            "        # Block device",
            "        os.mknod('input/bdev', 0o600 | stat.S_IFBLK,  os.makedev(10, 20))",
            "        # Char device",
            "        os.mknod('input/cdev', 0o600 | stat.S_IFCHR,  os.makedev(30, 40))",
            "        # Hard link",
            "        os.link(os.path.join(self.input_path, 'file1'),",
            "                os.path.join(self.input_path, 'hardlink'))",
            "        # Symlink",
            "        os.symlink('somewhere', os.path.join(self.input_path, 'link1'))",
            "        if xattr.is_enabled():",
            "            xattr.setxattr(os.path.join(self.input_path, 'file1'), 'user.foo', b'bar')",
            "            xattr.setxattr(os.path.join(self.input_path, 'link1'), 'user.foo_symlink', b'bar_symlink', follow_symlinks=False)",
            "        # FIFO node",
            "        os.mkfifo(os.path.join(self.input_path, 'fifo1'))",
            "        if has_lchflags:",
            "            os.lchflags(os.path.join(self.input_path, 'flagfile'), stat.UF_NODUMP)",
            "",
            "    def test_basic_functionality(self):",
            "        self.create_test_files()",
            "        self.attic('init', self.repository_location)",
            "        self.attic('create', self.repository_location + '::test', 'input')",
            "        self.attic('create', self.repository_location + '::test.2', 'input')",
            "        with changedir('output'):",
            "            self.attic('extract', self.repository_location + '::test')",
            "        self.assert_equal(len(self.attic('list', self.repository_location).splitlines()), 2)",
            "        self.assert_equal(len(self.attic('list', self.repository_location + '::test').splitlines()), 11)",
            "        self.assert_dirs_equal('input', 'output/input')",
            "        info_output = self.attic('info', self.repository_location + '::test')",
            "        self.assert_in('Number of files: 4', info_output)",
            "        shutil.rmtree(self.cache_path)",
            "        info_output2 = self.attic('info', self.repository_location + '::test')",
            "        # info_output2 starts with some \"initializing cache\" text but should",
            "        # end the same way as info_output",
            "        assert info_output2.endswith(info_output)",
            "",
            "    def test_strip_components(self):",
            "        self.attic('init', self.repository_location)",
            "        self.create_regular_file('dir/file')",
            "        self.attic('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.attic('extract', self.repository_location + '::test', '--strip-components', '3')",
            "            self.assert_true(not os.path.exists('file'))",
            "            with self.assert_creates_file('file'):",
            "                self.attic('extract', self.repository_location + '::test', '--strip-components', '2')",
            "            with self.assert_creates_file('dir/file'):",
            "                self.attic('extract', self.repository_location + '::test', '--strip-components', '1')",
            "            with self.assert_creates_file('input/dir/file'):",
            "                self.attic('extract', self.repository_location + '::test', '--strip-components', '0')",
            "",
            "    def test_extract_include_exclude(self):",
            "        self.attic('init', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.create_regular_file('file3', size=1024 * 80)",
            "        self.create_regular_file('file4', size=1024 * 80)",
            "        self.attic('create', '--exclude=input/file4', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.attic('extract', self.repository_location + '::test', 'input/file1', )",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1'])",
            "        with changedir('output'):",
            "            self.attic('extract', '--exclude=input/file2', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file3'])",
            "        with changedir('output'):",
            "            self.attic('extract', '--exclude-from=' + self.exclude_file_path, self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file3'])",
            "",
            "    def test_exclude_caches(self):",
            "        self.attic('init', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('cache1/CACHEDIR.TAG', contents = b'Signature: 8a477f597d28d172789f06886806bc55 extra stuff')",
            "        self.create_regular_file('cache2/CACHEDIR.TAG', contents = b'invalid signature')",
            "        self.attic('create', '--exclude-caches', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.attic('extract', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['cache2', 'file1'])",
            "        self.assert_equal(sorted(os.listdir('output/input/cache2')), ['CACHEDIR.TAG'])",
            "",
            "    def test_path_normalization(self):",
            "        self.attic('init', self.repository_location)",
            "        self.create_regular_file('dir1/dir2/file', size=1024 * 80)",
            "        with changedir('input/dir1/dir2'):",
            "            self.attic('create', self.repository_location + '::test', '../../../input/dir1/../dir1/dir2/..')",
            "        output = self.attic('list', self.repository_location + '::test')",
            "        self.assert_not_in('..', output)",
            "        self.assert_in(' input/dir1/dir2/file', output)",
            "",
            "    def test_repeated_files(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.attic('init', self.repository_location)",
            "        self.attic('create', self.repository_location + '::test', 'input', 'input')",
            "",
            "    def test_overwrite(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('dir2/file2', size=1024 * 80)",
            "        self.attic('init', self.repository_location)",
            "        self.attic('create', self.repository_location + '::test', 'input')",
            "        # Overwriting regular files and directories should be supported",
            "        os.mkdir('output/input')",
            "        os.mkdir('output/input/file1')",
            "        os.mkdir('output/input/dir2')",
            "        with changedir('output'):",
            "            self.attic('extract', self.repository_location + '::test')",
            "        self.assert_dirs_equal('input', 'output/input')",
            "        # But non-empty dirs should fail",
            "        os.unlink('output/input/file1')",
            "        os.mkdir('output/input/file1')",
            "        os.mkdir('output/input/file1/dir')",
            "        with changedir('output'):",
            "            self.attic('extract', self.repository_location + '::test', exit_code=1)",
            "",
            "    def test_delete(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('dir2/file2', size=1024 * 80)",
            "        self.attic('init', self.repository_location)",
            "        self.attic('create', self.repository_location + '::test', 'input')",
            "        self.attic('create', self.repository_location + '::test.2', 'input')",
            "        self.attic('extract', '--dry-run', self.repository_location + '::test')",
            "        self.attic('extract', '--dry-run', self.repository_location + '::test.2')",
            "        self.attic('delete', self.repository_location + '::test')",
            "        self.attic('extract', '--dry-run', self.repository_location + '::test.2')",
            "        self.attic('delete', self.repository_location + '::test.2')",
            "        # Make sure all data except the manifest has been deleted",
            "        repository = Repository(self.repository_path)",
            "        self.assert_equal(len(repository), 1)",
            "",
            "    def test_corrupted_repository(self):",
            "        self.attic('init', self.repository_location)",
            "        self.create_src_archive('test')",
            "        self.attic('extract', '--dry-run', self.repository_location + '::test')",
            "        self.attic('check', self.repository_location)",
            "        name = sorted(os.listdir(os.path.join(self.tmpdir, 'repository', 'data', '0')), reverse=True)[0]",
            "        fd = open(os.path.join(self.tmpdir, 'repository', 'data', '0', name), 'r+')",
            "        fd.seek(100)",
            "        fd.write('XXXX')",
            "        fd.close()",
            "        self.attic('check', self.repository_location, exit_code=1)",
            "",
            "    def test_readonly_repository(self):",
            "        self.attic('init', self.repository_location)",
            "        self.create_src_archive('test')",
            "        os.system('chmod -R ugo-w ' + self.repository_path)",
            "        try:",
            "            self.attic('extract', '--dry-run', self.repository_location + '::test')",
            "        finally:",
            "            # Restore permissions so shutil.rmtree is able to delete it",
            "            os.system('chmod -R u+w ' + self.repository_path)",
            "",
            "    def test_cmdline_compatibility(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.attic('init', self.repository_location)",
            "        self.attic('create', self.repository_location + '::test', 'input')",
            "        output = self.attic('verify', '-v', self.repository_location + '::test')",
            "        self.assert_in('\"attic verify\" has been deprecated', output)",
            "        output = self.attic('prune', self.repository_location, '--hourly=1')",
            "        self.assert_in('\"--hourly\" has been deprecated. Use \"--keep-hourly\" instead', output)",
            "",
            "    def test_prune_repository(self):",
            "        self.attic('init', self.repository_location)",
            "        self.attic('create', self.repository_location + '::test1', src_dir)",
            "        self.attic('create', self.repository_location + '::test2', src_dir)",
            "        output = self.attic('prune', '-v', '--dry-run', self.repository_location, '--keep-daily=2')",
            "        self.assert_in('Keeping archive: test2', output)",
            "        self.assert_in('Would prune:     test1', output)",
            "        output = self.attic('list', self.repository_location)",
            "        self.assert_in('test1', output)",
            "        self.assert_in('test2', output)",
            "        self.attic('prune', self.repository_location, '--keep-daily=2')",
            "        output = self.attic('list', self.repository_location)",
            "        self.assert_not_in('test1', output)",
            "        self.assert_in('test2', output)",
            "",
            "    def test_usage(self):",
            "        self.assert_raises(SystemExit, lambda: self.attic())",
            "        self.assert_raises(SystemExit, lambda: self.attic('-h'))",
            "",
            "    @unittest.skipUnless(has_llfuse, 'llfuse not installed')",
            "    def test_fuse_mount_repository(self):",
            "        mountpoint = os.path.join(self.tmpdir, 'mountpoint')",
            "        os.mkdir(mountpoint)",
            "        self.attic('init', self.repository_location)",
            "        self.create_test_files()",
            "        self.attic('create', self.repository_location + '::archive', 'input')",
            "        self.attic('create', self.repository_location + '::archive2', 'input')",
            "        try:",
            "            self.attic('mount', self.repository_location, mountpoint, fork=True)",
            "            self.wait_for_mount(mountpoint)",
            "            self.assert_dirs_equal(self.input_path, os.path.join(mountpoint, 'archive', 'input'))",
            "            self.assert_dirs_equal(self.input_path, os.path.join(mountpoint, 'archive2', 'input'))",
            "        finally:",
            "            if sys.platform.startswith('linux'):",
            "                os.system('fusermount -u ' + mountpoint)",
            "            else:",
            "                os.system('umount ' + mountpoint)",
            "            os.rmdir(mountpoint)",
            "            # Give the daemon some time to exit",
            "            time.sleep(.2)",
            "",
            "    @unittest.skipUnless(has_llfuse, 'llfuse not installed')",
            "    def test_fuse_mount_archive(self):",
            "        mountpoint = os.path.join(self.tmpdir, 'mountpoint')",
            "        os.mkdir(mountpoint)",
            "        self.attic('init', self.repository_location)",
            "        self.create_test_files()",
            "        self.attic('create', self.repository_location + '::archive', 'input')",
            "        try:",
            "            self.attic('mount', self.repository_location + '::archive', mountpoint, fork=True)",
            "            self.wait_for_mount(mountpoint)",
            "            self.assert_dirs_equal(self.input_path, os.path.join(mountpoint, 'input'))",
            "        finally:",
            "            if sys.platform.startswith('linux'):",
            "                os.system('fusermount -u ' + mountpoint)",
            "            else:",
            "                os.system('umount ' + mountpoint)",
            "            os.rmdir(mountpoint)",
            "            # Give the daemon some time to exit",
            "            time.sleep(.2)",
            "",
            "    def verify_aes_counter_uniqueness(self, method):",
            "        seen = set()  # Chunks already seen",
            "        used = set()  # counter values already used",
            "",
            "        def verify_uniqueness():",
            "            repository = Repository(self.repository_path)",
            "            for key, _ in repository.open_index(repository.get_transaction_id()).iteritems():",
            "                data = repository.get(key)",
            "                hash = sha256(data).digest()",
            "                if not hash in seen:",
            "                    seen.add(hash)",
            "                    num_blocks = num_aes_blocks(len(data) - 41)",
            "                    nonce = bytes_to_long(data[33:41])",
            "                    for counter in range(nonce, nonce + num_blocks):",
            "                        self.assert_not_in(counter, used)",
            "                        used.add(counter)",
            "",
            "        self.create_test_files()",
            "        os.environ['ATTIC_PASSPHRASE'] = 'passphrase'",
            "        self.attic('init', '--encryption=' + method, self.repository_location)",
            "        verify_uniqueness()",
            "        self.attic('create', self.repository_location + '::test', 'input')",
            "        verify_uniqueness()",
            "        self.attic('create', self.repository_location + '::test.2', 'input')",
            "        verify_uniqueness()",
            "        self.attic('delete', self.repository_location + '::test.2')",
            "        verify_uniqueness()",
            "        self.assert_equal(used, set(range(len(used))))",
            "",
            "    def test_aes_counter_uniqueness_keyfile(self):",
            "        self.verify_aes_counter_uniqueness('keyfile')",
            "",
            "    def test_aes_counter_uniqueness_passphrase(self):",
            "        self.verify_aes_counter_uniqueness('passphrase')",
            "",
            "",
            "class ArchiverCheckTestCase(ArchiverTestCaseBase):",
            "",
            "    def setUp(self):",
            "        super(ArchiverCheckTestCase, self).setUp()",
            "        with patch.object(ChunkBuffer, 'BUFFER_SIZE', 10):",
            "            self.attic('init', self.repository_location)",
            "            self.create_src_archive('archive1')",
            "            self.create_src_archive('archive2')",
            "",
            "    def open_archive(self, name):",
            "        repository = Repository(self.repository_path)",
            "        manifest, key = Manifest.load(repository)",
            "        archive = Archive(repository, key, manifest, name)",
            "        return archive, repository",
            "",
            "    def test_check_usage(self):",
            "        output = self.attic('check', self.repository_location, exit_code=0)",
            "        self.assert_in('Starting repository check', output)",
            "        self.assert_in('Starting archive consistency check', output)",
            "        output = self.attic('check', '--repository-only', self.repository_location, exit_code=0)",
            "        self.assert_in('Starting repository check', output)",
            "        self.assert_not_in('Starting archive consistency check', output)",
            "        output = self.attic('check', '--archives-only', self.repository_location, exit_code=0)",
            "        self.assert_not_in('Starting repository check', output)",
            "        self.assert_in('Starting archive consistency check', output)",
            "",
            "    def test_missing_file_chunk(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        for item in archive.iter_items():",
            "            if item[b'path'].endswith('testsuite/archiver.py'):",
            "                repository.delete(item[b'chunks'][-1][0])",
            "                break",
            "        repository.commit()",
            "        self.attic('check', self.repository_location, exit_code=1)",
            "        self.attic('check', '--repair', self.repository_location, exit_code=0)",
            "        self.attic('check', self.repository_location, exit_code=0)",
            "",
            "    def test_missing_archive_item_chunk(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        repository.delete(archive.metadata[b'items'][-5])",
            "        repository.commit()",
            "        self.attic('check', self.repository_location, exit_code=1)",
            "        self.attic('check', '--repair', self.repository_location, exit_code=0)",
            "        self.attic('check', self.repository_location, exit_code=0)",
            "",
            "    def test_missing_archive_metadata(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        repository.delete(archive.id)",
            "        repository.commit()",
            "        self.attic('check', self.repository_location, exit_code=1)",
            "        self.attic('check', '--repair', self.repository_location, exit_code=0)",
            "        self.attic('check', self.repository_location, exit_code=0)",
            "",
            "    def test_missing_manifest(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        repository.delete(Manifest.MANIFEST_ID)",
            "        repository.commit()",
            "        self.attic('check', self.repository_location, exit_code=1)",
            "        output = self.attic('check', '--repair', self.repository_location, exit_code=0)",
            "        self.assert_in('archive1', output)",
            "        self.assert_in('archive2', output)",
            "        self.attic('check', self.repository_location, exit_code=0)",
            "",
            "    def test_extra_chunks(self):",
            "        self.attic('check', self.repository_location, exit_code=0)",
            "        repository = Repository(self.repository_location)",
            "        repository.put(b'01234567890123456789012345678901', b'xxxx')",
            "        repository.commit()",
            "        repository.close()",
            "        self.attic('check', self.repository_location, exit_code=1)",
            "        self.attic('check', self.repository_location, exit_code=1)",
            "        self.attic('check', '--repair', self.repository_location, exit_code=0)",
            "        self.attic('check', self.repository_location, exit_code=0)",
            "        self.attic('extract', '--dry-run', self.repository_location + '::archive1', exit_code=0)",
            "",
            "",
            "class RemoteArchiverTestCase(ArchiverTestCase):",
            "    prefix = '__testsuite__:'",
            "",
            "    def test_remote_repo_restrict_to_path(self):",
            "        self.attic('init', self.repository_location)",
            "        path_prefix = os.path.dirname(self.repository_path)",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-path', '/foo']):",
            "            self.assert_raises(PathNotAllowed, lambda: self.attic('init', self.repository_location + '_1'))",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-path', path_prefix]):",
            "            self.attic('init', self.repository_location + '_2')",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-path', '/foo', '--restrict-to-path', path_prefix]):",
            "            self.attic('init', self.repository_location + '_3')"
        ],
        "afterPatchFile": [
            "from binascii import hexlify",
            "from configparser import RawConfigParser",
            "import os",
            "from io import StringIO",
            "import stat",
            "import subprocess",
            "import sys",
            "import shutil",
            "import tempfile",
            "import time",
            "import unittest",
            "from hashlib import sha256",
            "from attic import xattr",
            "from attic.archive import Archive, ChunkBuffer",
            "from attic.archiver import Archiver",
            "from attic.cache import Cache",
            "from attic.crypto import bytes_to_long, num_aes_blocks",
            "from attic.helpers import Manifest",
            "from attic.remote import RemoteRepository, PathNotAllowed",
            "from attic.repository import Repository",
            "from attic.testsuite import AtticTestCase",
            "from attic.testsuite.mock import patch",
            "",
            "try:",
            "    import llfuse",
            "    has_llfuse = True",
            "except ImportError:",
            "    has_llfuse = False",
            "",
            "has_lchflags = hasattr(os, 'lchflags')",
            "",
            "src_dir = os.path.join(os.getcwd(), os.path.dirname(__file__), '..')",
            "",
            "",
            "class changedir:",
            "    def __init__(self, dir):",
            "        self.dir = dir",
            "",
            "    def __enter__(self):",
            "        self.old = os.getcwd()",
            "        os.chdir(self.dir)",
            "",
            "    def __exit__(self, *args, **kw):",
            "        os.chdir(self.old)",
            "",
            "",
            "class environment_variable:",
            "    def __init__(self, **values):",
            "        self.values = values",
            "        self.old_values = {}",
            "",
            "    def __enter__(self):",
            "        for k, v in self.values.items():",
            "            self.old_values[k] = os.environ.get(k)",
            "            os.environ[k] = v",
            "",
            "    def __exit__(self, *args, **kw):",
            "        for k, v in self.old_values.items():",
            "            if v is not None:",
            "                os.environ[k] = v",
            "",
            "",
            "class ArchiverTestCaseBase(AtticTestCase):",
            "",
            "    prefix = ''",
            "",
            "    def setUp(self):",
            "        os.environ['ATTIC_CHECK_I_KNOW_WHAT_I_AM_DOING'] = '1'",
            "        self.archiver = Archiver()",
            "        self.tmpdir = tempfile.mkdtemp()",
            "        self.repository_path = os.path.join(self.tmpdir, 'repository')",
            "        self.repository_location = self.prefix + self.repository_path",
            "        self.input_path = os.path.join(self.tmpdir, 'input')",
            "        self.output_path = os.path.join(self.tmpdir, 'output')",
            "        self.keys_path = os.path.join(self.tmpdir, 'keys')",
            "        self.cache_path = os.path.join(self.tmpdir, 'cache')",
            "        self.exclude_file_path = os.path.join(self.tmpdir, 'excludes')",
            "        os.environ['ATTIC_KEYS_DIR'] = self.keys_path",
            "        os.environ['ATTIC_CACHE_DIR'] = self.cache_path",
            "        os.mkdir(self.input_path)",
            "        os.mkdir(self.output_path)",
            "        os.mkdir(self.keys_path)",
            "        os.mkdir(self.cache_path)",
            "        with open(self.exclude_file_path, 'wb') as fd:",
            "            fd.write(b'input/file2\\n# A commment line, then a blank line\\n\\n')",
            "        self._old_wd = os.getcwd()",
            "        os.chdir(self.tmpdir)",
            "",
            "    def tearDown(self):",
            "        shutil.rmtree(self.tmpdir)",
            "        os.chdir(self._old_wd)",
            "",
            "    def attic(self, *args, **kw):",
            "        exit_code = kw.get('exit_code', 0)",
            "        fork = kw.get('fork', False)",
            "        if fork:",
            "            try:",
            "                output = subprocess.check_output((sys.executable, '-m', 'attic.archiver') + args)",
            "                ret = 0",
            "            except subprocess.CalledProcessError as e:",
            "                output = e.output",
            "                ret = e.returncode",
            "            output = os.fsdecode(output)",
            "            if ret != exit_code:",
            "                print(output)",
            "            self.assert_equal(exit_code, ret)",
            "            return output",
            "        args = list(args)",
            "        stdout, stderr = sys.stdout, sys.stderr",
            "        try:",
            "            output = StringIO()",
            "            sys.stdout = sys.stderr = output",
            "            ret = self.archiver.run(args)",
            "            sys.stdout, sys.stderr = stdout, stderr",
            "            if ret != exit_code:",
            "                print(output.getvalue())",
            "            self.assert_equal(exit_code, ret)",
            "            return output.getvalue()",
            "        finally:",
            "            sys.stdout, sys.stderr = stdout, stderr",
            "",
            "    def create_src_archive(self, name):",
            "        self.attic('create', self.repository_location + '::' + name, src_dir)",
            "",
            "",
            "class ArchiverTestCase(ArchiverTestCaseBase):",
            "",
            "    def create_regular_file(self, name, size=0, contents=None):",
            "        filename = os.path.join(self.input_path, name)",
            "        if not os.path.exists(os.path.dirname(filename)):",
            "            os.makedirs(os.path.dirname(filename))",
            "        with open(filename, 'wb') as fd:",
            "            if contents is None:",
            "                contents = b'X' * size",
            "            fd.write(contents)",
            "",
            "    def create_test_files(self):",
            "        \"\"\"Create a minimal test case including all supported file types",
            "        \"\"\"",
            "        # File",
            "        self.create_regular_file('empty', size=0)",
            "        # 2600-01-01 > 2**64 ns",
            "        os.utime('input/empty', (19880895600, 19880895600))",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('flagfile', size=1024)",
            "        # Directory",
            "        self.create_regular_file('dir2/file2', size=1024 * 80)",
            "        # File owner",
            "        os.chown('input/file1', 100, 200)",
            "        # File mode",
            "        os.chmod('input/file1', 0o7755)",
            "        os.chmod('input/dir2', 0o555)",
            "        # Block device",
            "        os.mknod('input/bdev', 0o600 | stat.S_IFBLK,  os.makedev(10, 20))",
            "        # Char device",
            "        os.mknod('input/cdev', 0o600 | stat.S_IFCHR,  os.makedev(30, 40))",
            "        # Hard link",
            "        os.link(os.path.join(self.input_path, 'file1'),",
            "                os.path.join(self.input_path, 'hardlink'))",
            "        # Symlink",
            "        os.symlink('somewhere', os.path.join(self.input_path, 'link1'))",
            "        if xattr.is_enabled():",
            "            xattr.setxattr(os.path.join(self.input_path, 'file1'), 'user.foo', b'bar')",
            "            xattr.setxattr(os.path.join(self.input_path, 'link1'), 'user.foo_symlink', b'bar_symlink', follow_symlinks=False)",
            "        # FIFO node",
            "        os.mkfifo(os.path.join(self.input_path, 'fifo1'))",
            "        if has_lchflags:",
            "            os.lchflags(os.path.join(self.input_path, 'flagfile'), stat.UF_NODUMP)",
            "",
            "    def test_basic_functionality(self):",
            "        self.create_test_files()",
            "        self.attic('init', self.repository_location)",
            "        self.attic('create', self.repository_location + '::test', 'input')",
            "        self.attic('create', self.repository_location + '::test.2', 'input')",
            "        with changedir('output'):",
            "            self.attic('extract', self.repository_location + '::test')",
            "        self.assert_equal(len(self.attic('list', self.repository_location).splitlines()), 2)",
            "        self.assert_equal(len(self.attic('list', self.repository_location + '::test').splitlines()), 11)",
            "        self.assert_dirs_equal('input', 'output/input')",
            "        info_output = self.attic('info', self.repository_location + '::test')",
            "        self.assert_in('Number of files: 4', info_output)",
            "        shutil.rmtree(self.cache_path)",
            "        with environment_variable(ATTIC_UNKNOWN_UNENCRYPTED_REPO_ACCESS_IS_OK='1'):",
            "            info_output2 = self.attic('info', self.repository_location + '::test')",
            "        # info_output2 starts with some \"initializing cache\" text but should",
            "        # end the same way as info_output",
            "        assert info_output2.endswith(info_output)",
            "",
            "    def _extract_repository_id(self, path):",
            "        return Repository(self.repository_path).id",
            "",
            "    def _set_repository_id(self, path, id):",
            "        config = RawConfigParser()",
            "        config.read(os.path.join(path, 'config'))",
            "        config.set('repository', 'id', hexlify(id).decode('ascii'))",
            "        with open(os.path.join(path, 'config'), 'w') as fd:",
            "            config.write(fd)",
            "        return Repository(self.repository_path).id",
            "",
            "    def test_repository_swap_detection(self):",
            "        self.create_test_files()",
            "        os.environ['ATTIC_PASSPHRASE'] = 'passphrase'",
            "        self.attic('init', '--encryption=passphrase', self.repository_location)",
            "        repository_id = self._extract_repository_id(self.repository_path)",
            "        self.attic('create', self.repository_location + '::test', 'input')",
            "        shutil.rmtree(self.repository_path)",
            "        self.attic('init', '--encryption=none', self.repository_location)",
            "        self._set_repository_id(self.repository_path, repository_id)",
            "        self.assert_equal(repository_id, self._extract_repository_id(self.repository_path))",
            "        self.assert_raises(Cache.EncryptionMethodMismatch, lambda :self.attic('create', self.repository_location + '::test.2', 'input'))",
            "",
            "    def test_strip_components(self):",
            "        self.attic('init', self.repository_location)",
            "        self.create_regular_file('dir/file')",
            "        self.attic('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.attic('extract', self.repository_location + '::test', '--strip-components', '3')",
            "            self.assert_true(not os.path.exists('file'))",
            "            with self.assert_creates_file('file'):",
            "                self.attic('extract', self.repository_location + '::test', '--strip-components', '2')",
            "            with self.assert_creates_file('dir/file'):",
            "                self.attic('extract', self.repository_location + '::test', '--strip-components', '1')",
            "            with self.assert_creates_file('input/dir/file'):",
            "                self.attic('extract', self.repository_location + '::test', '--strip-components', '0')",
            "",
            "    def test_extract_include_exclude(self):",
            "        self.attic('init', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.create_regular_file('file3', size=1024 * 80)",
            "        self.create_regular_file('file4', size=1024 * 80)",
            "        self.attic('create', '--exclude=input/file4', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.attic('extract', self.repository_location + '::test', 'input/file1', )",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1'])",
            "        with changedir('output'):",
            "            self.attic('extract', '--exclude=input/file2', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file3'])",
            "        with changedir('output'):",
            "            self.attic('extract', '--exclude-from=' + self.exclude_file_path, self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file3'])",
            "",
            "    def test_exclude_caches(self):",
            "        self.attic('init', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('cache1/CACHEDIR.TAG', contents = b'Signature: 8a477f597d28d172789f06886806bc55 extra stuff')",
            "        self.create_regular_file('cache2/CACHEDIR.TAG', contents = b'invalid signature')",
            "        self.attic('create', '--exclude-caches', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.attic('extract', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['cache2', 'file1'])",
            "        self.assert_equal(sorted(os.listdir('output/input/cache2')), ['CACHEDIR.TAG'])",
            "",
            "    def test_path_normalization(self):",
            "        self.attic('init', self.repository_location)",
            "        self.create_regular_file('dir1/dir2/file', size=1024 * 80)",
            "        with changedir('input/dir1/dir2'):",
            "            self.attic('create', self.repository_location + '::test', '../../../input/dir1/../dir1/dir2/..')",
            "        output = self.attic('list', self.repository_location + '::test')",
            "        self.assert_not_in('..', output)",
            "        self.assert_in(' input/dir1/dir2/file', output)",
            "",
            "    def test_repeated_files(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.attic('init', self.repository_location)",
            "        self.attic('create', self.repository_location + '::test', 'input', 'input')",
            "",
            "    def test_overwrite(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('dir2/file2', size=1024 * 80)",
            "        self.attic('init', self.repository_location)",
            "        self.attic('create', self.repository_location + '::test', 'input')",
            "        # Overwriting regular files and directories should be supported",
            "        os.mkdir('output/input')",
            "        os.mkdir('output/input/file1')",
            "        os.mkdir('output/input/dir2')",
            "        with changedir('output'):",
            "            self.attic('extract', self.repository_location + '::test')",
            "        self.assert_dirs_equal('input', 'output/input')",
            "        # But non-empty dirs should fail",
            "        os.unlink('output/input/file1')",
            "        os.mkdir('output/input/file1')",
            "        os.mkdir('output/input/file1/dir')",
            "        with changedir('output'):",
            "            self.attic('extract', self.repository_location + '::test', exit_code=1)",
            "",
            "    def test_delete(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('dir2/file2', size=1024 * 80)",
            "        self.attic('init', self.repository_location)",
            "        self.attic('create', self.repository_location + '::test', 'input')",
            "        self.attic('create', self.repository_location + '::test.2', 'input')",
            "        self.attic('extract', '--dry-run', self.repository_location + '::test')",
            "        self.attic('extract', '--dry-run', self.repository_location + '::test.2')",
            "        self.attic('delete', self.repository_location + '::test')",
            "        self.attic('extract', '--dry-run', self.repository_location + '::test.2')",
            "        self.attic('delete', self.repository_location + '::test.2')",
            "        # Make sure all data except the manifest has been deleted",
            "        repository = Repository(self.repository_path)",
            "        self.assert_equal(len(repository), 1)",
            "",
            "    def test_corrupted_repository(self):",
            "        self.attic('init', self.repository_location)",
            "        self.create_src_archive('test')",
            "        self.attic('extract', '--dry-run', self.repository_location + '::test')",
            "        self.attic('check', self.repository_location)",
            "        name = sorted(os.listdir(os.path.join(self.tmpdir, 'repository', 'data', '0')), reverse=True)[0]",
            "        fd = open(os.path.join(self.tmpdir, 'repository', 'data', '0', name), 'r+')",
            "        fd.seek(100)",
            "        fd.write('XXXX')",
            "        fd.close()",
            "        self.attic('check', self.repository_location, exit_code=1)",
            "",
            "    def test_readonly_repository(self):",
            "        self.attic('init', self.repository_location)",
            "        self.create_src_archive('test')",
            "        os.system('chmod -R ugo-w ' + self.repository_path)",
            "        try:",
            "            self.attic('extract', '--dry-run', self.repository_location + '::test')",
            "        finally:",
            "            # Restore permissions so shutil.rmtree is able to delete it",
            "            os.system('chmod -R u+w ' + self.repository_path)",
            "",
            "    def test_cmdline_compatibility(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.attic('init', self.repository_location)",
            "        self.attic('create', self.repository_location + '::test', 'input')",
            "        output = self.attic('verify', '-v', self.repository_location + '::test')",
            "        self.assert_in('\"attic verify\" has been deprecated', output)",
            "        output = self.attic('prune', self.repository_location, '--hourly=1')",
            "        self.assert_in('\"--hourly\" has been deprecated. Use \"--keep-hourly\" instead', output)",
            "",
            "    def test_prune_repository(self):",
            "        self.attic('init', self.repository_location)",
            "        self.attic('create', self.repository_location + '::test1', src_dir)",
            "        self.attic('create', self.repository_location + '::test2', src_dir)",
            "        output = self.attic('prune', '-v', '--dry-run', self.repository_location, '--keep-daily=2')",
            "        self.assert_in('Keeping archive: test2', output)",
            "        self.assert_in('Would prune:     test1', output)",
            "        output = self.attic('list', self.repository_location)",
            "        self.assert_in('test1', output)",
            "        self.assert_in('test2', output)",
            "        self.attic('prune', self.repository_location, '--keep-daily=2')",
            "        output = self.attic('list', self.repository_location)",
            "        self.assert_not_in('test1', output)",
            "        self.assert_in('test2', output)",
            "",
            "    def test_usage(self):",
            "        self.assert_raises(SystemExit, lambda: self.attic())",
            "        self.assert_raises(SystemExit, lambda: self.attic('-h'))",
            "",
            "    @unittest.skipUnless(has_llfuse, 'llfuse not installed')",
            "    def test_fuse_mount_repository(self):",
            "        mountpoint = os.path.join(self.tmpdir, 'mountpoint')",
            "        os.mkdir(mountpoint)",
            "        self.attic('init', self.repository_location)",
            "        self.create_test_files()",
            "        self.attic('create', self.repository_location + '::archive', 'input')",
            "        self.attic('create', self.repository_location + '::archive2', 'input')",
            "        try:",
            "            self.attic('mount', self.repository_location, mountpoint, fork=True)",
            "            self.wait_for_mount(mountpoint)",
            "            self.assert_dirs_equal(self.input_path, os.path.join(mountpoint, 'archive', 'input'))",
            "            self.assert_dirs_equal(self.input_path, os.path.join(mountpoint, 'archive2', 'input'))",
            "        finally:",
            "            if sys.platform.startswith('linux'):",
            "                os.system('fusermount -u ' + mountpoint)",
            "            else:",
            "                os.system('umount ' + mountpoint)",
            "            os.rmdir(mountpoint)",
            "            # Give the daemon some time to exit",
            "            time.sleep(.2)",
            "",
            "    @unittest.skipUnless(has_llfuse, 'llfuse not installed')",
            "    def test_fuse_mount_archive(self):",
            "        mountpoint = os.path.join(self.tmpdir, 'mountpoint')",
            "        os.mkdir(mountpoint)",
            "        self.attic('init', self.repository_location)",
            "        self.create_test_files()",
            "        self.attic('create', self.repository_location + '::archive', 'input')",
            "        try:",
            "            self.attic('mount', self.repository_location + '::archive', mountpoint, fork=True)",
            "            self.wait_for_mount(mountpoint)",
            "            self.assert_dirs_equal(self.input_path, os.path.join(mountpoint, 'input'))",
            "        finally:",
            "            if sys.platform.startswith('linux'):",
            "                os.system('fusermount -u ' + mountpoint)",
            "            else:",
            "                os.system('umount ' + mountpoint)",
            "            os.rmdir(mountpoint)",
            "            # Give the daemon some time to exit",
            "            time.sleep(.2)",
            "",
            "    def verify_aes_counter_uniqueness(self, method):",
            "        seen = set()  # Chunks already seen",
            "        used = set()  # counter values already used",
            "",
            "        def verify_uniqueness():",
            "            repository = Repository(self.repository_path)",
            "            for key, _ in repository.open_index(repository.get_transaction_id()).iteritems():",
            "                data = repository.get(key)",
            "                hash = sha256(data).digest()",
            "                if not hash in seen:",
            "                    seen.add(hash)",
            "                    num_blocks = num_aes_blocks(len(data) - 41)",
            "                    nonce = bytes_to_long(data[33:41])",
            "                    for counter in range(nonce, nonce + num_blocks):",
            "                        self.assert_not_in(counter, used)",
            "                        used.add(counter)",
            "",
            "        self.create_test_files()",
            "        os.environ['ATTIC_PASSPHRASE'] = 'passphrase'",
            "        self.attic('init', '--encryption=' + method, self.repository_location)",
            "        verify_uniqueness()",
            "        self.attic('create', self.repository_location + '::test', 'input')",
            "        verify_uniqueness()",
            "        self.attic('create', self.repository_location + '::test.2', 'input')",
            "        verify_uniqueness()",
            "        self.attic('delete', self.repository_location + '::test.2')",
            "        verify_uniqueness()",
            "        self.assert_equal(used, set(range(len(used))))",
            "",
            "    def test_aes_counter_uniqueness_keyfile(self):",
            "        self.verify_aes_counter_uniqueness('keyfile')",
            "",
            "    def test_aes_counter_uniqueness_passphrase(self):",
            "        self.verify_aes_counter_uniqueness('passphrase')",
            "",
            "",
            "class ArchiverCheckTestCase(ArchiverTestCaseBase):",
            "",
            "    def setUp(self):",
            "        super(ArchiverCheckTestCase, self).setUp()",
            "        with patch.object(ChunkBuffer, 'BUFFER_SIZE', 10):",
            "            self.attic('init', self.repository_location)",
            "            self.create_src_archive('archive1')",
            "            self.create_src_archive('archive2')",
            "",
            "    def open_archive(self, name):",
            "        repository = Repository(self.repository_path)",
            "        manifest, key = Manifest.load(repository)",
            "        archive = Archive(repository, key, manifest, name)",
            "        return archive, repository",
            "",
            "    def test_check_usage(self):",
            "        output = self.attic('check', self.repository_location, exit_code=0)",
            "        self.assert_in('Starting repository check', output)",
            "        self.assert_in('Starting archive consistency check', output)",
            "        output = self.attic('check', '--repository-only', self.repository_location, exit_code=0)",
            "        self.assert_in('Starting repository check', output)",
            "        self.assert_not_in('Starting archive consistency check', output)",
            "        output = self.attic('check', '--archives-only', self.repository_location, exit_code=0)",
            "        self.assert_not_in('Starting repository check', output)",
            "        self.assert_in('Starting archive consistency check', output)",
            "",
            "    def test_missing_file_chunk(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        for item in archive.iter_items():",
            "            if item[b'path'].endswith('testsuite/archiver.py'):",
            "                repository.delete(item[b'chunks'][-1][0])",
            "                break",
            "        repository.commit()",
            "        self.attic('check', self.repository_location, exit_code=1)",
            "        self.attic('check', '--repair', self.repository_location, exit_code=0)",
            "        self.attic('check', self.repository_location, exit_code=0)",
            "",
            "    def test_missing_archive_item_chunk(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        repository.delete(archive.metadata[b'items'][-5])",
            "        repository.commit()",
            "        self.attic('check', self.repository_location, exit_code=1)",
            "        self.attic('check', '--repair', self.repository_location, exit_code=0)",
            "        self.attic('check', self.repository_location, exit_code=0)",
            "",
            "    def test_missing_archive_metadata(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        repository.delete(archive.id)",
            "        repository.commit()",
            "        self.attic('check', self.repository_location, exit_code=1)",
            "        self.attic('check', '--repair', self.repository_location, exit_code=0)",
            "        self.attic('check', self.repository_location, exit_code=0)",
            "",
            "    def test_missing_manifest(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        repository.delete(Manifest.MANIFEST_ID)",
            "        repository.commit()",
            "        self.attic('check', self.repository_location, exit_code=1)",
            "        output = self.attic('check', '--repair', self.repository_location, exit_code=0)",
            "        self.assert_in('archive1', output)",
            "        self.assert_in('archive2', output)",
            "        self.attic('check', self.repository_location, exit_code=0)",
            "",
            "    def test_extra_chunks(self):",
            "        self.attic('check', self.repository_location, exit_code=0)",
            "        repository = Repository(self.repository_location)",
            "        repository.put(b'01234567890123456789012345678901', b'xxxx')",
            "        repository.commit()",
            "        repository.close()",
            "        self.attic('check', self.repository_location, exit_code=1)",
            "        self.attic('check', self.repository_location, exit_code=1)",
            "        self.attic('check', '--repair', self.repository_location, exit_code=0)",
            "        self.attic('check', self.repository_location, exit_code=0)",
            "        self.attic('extract', '--dry-run', self.repository_location + '::archive1', exit_code=0)",
            "",
            "",
            "class RemoteArchiverTestCase(ArchiverTestCase):",
            "    prefix = '__testsuite__:'",
            "",
            "    def test_remote_repo_restrict_to_path(self):",
            "        self.attic('init', self.repository_location)",
            "        path_prefix = os.path.dirname(self.repository_path)",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-path', '/foo']):",
            "            self.assert_raises(PathNotAllowed, lambda: self.attic('init', self.repository_location + '_1'))",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-path', path_prefix]):",
            "            self.attic('init', self.repository_location + '_2')",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-path', '/foo', '--restrict-to-path', path_prefix]):",
            "            self.attic('init', self.repository_location + '_3')"
        ],
        "action": [
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "164": [
                "ArchiverTestCase",
                "test_basic_functionality"
            ]
        },
        "addLocation": [
            "attic.testsuite.archiver.ArchiverTestCase.test_basic_functionality",
            "shuup.front.urls"
        ]
    }
}