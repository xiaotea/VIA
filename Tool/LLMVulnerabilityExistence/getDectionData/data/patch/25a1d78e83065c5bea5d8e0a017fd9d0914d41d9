{
    "nova/scheduler/filter_scheduler.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " from nova import rpc"
            },
            "1": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " from nova.scheduler import client"
            },
            "2": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " from nova.scheduler import driver"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+from nova.scheduler import utils"
            },
            "4": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " CONF = nova.conf.CONF"
            },
            "6": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " LOG = logging.getLogger(__name__)"
            },
            "7": {
                "beforePatchRowNumber": 290,
                "afterPatchRowNumber": 291,
                "PatchRowcode": "                            API's PUT /allocations/{consumer_uuid} call to claim"
            },
            "8": {
                "beforePatchRowNumber": 291,
                "afterPatchRowNumber": 292,
                "PatchRowcode": "                            resources for the instance"
            },
            "9": {
                "beforePatchRowNumber": 292,
                "afterPatchRowNumber": 293,
                "PatchRowcode": "         \"\"\""
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 294,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 295,
                "PatchRowcode": "+        if utils.request_is_rebuild(spec_obj):"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 296,
                "PatchRowcode": "+            # NOTE(danms): This is a rebuild-only scheduling request, so we"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 297,
                "PatchRowcode": "+            # should not be doing any extra claiming"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 298,
                "PatchRowcode": "+            LOG.debug('Not claiming resources in the placement API for '"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 299,
                "PatchRowcode": "+                      'rebuild-only scheduling of instance %(uuid)s',"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 300,
                "PatchRowcode": "+                      {'uuid': instance_uuid})"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 301,
                "PatchRowcode": "+            return True"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 302,
                "PatchRowcode": "+"
            },
            "19": {
                "beforePatchRowNumber": 293,
                "afterPatchRowNumber": 303,
                "PatchRowcode": "         LOG.debug(\"Attempting to claim resources in the placement API for \""
            },
            "20": {
                "beforePatchRowNumber": 294,
                "afterPatchRowNumber": 304,
                "PatchRowcode": "                   \"instance %s\", instance_uuid)"
            },
            "21": {
                "beforePatchRowNumber": 295,
                "afterPatchRowNumber": 305,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# Copyright (c) 2011 OpenStack Foundation",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "\"\"\"",
            "The FilterScheduler is for creating instances locally.",
            "You can customize this scheduler by specifying your own Host Filters and",
            "Weighing Functions.",
            "\"\"\"",
            "",
            "import random",
            "",
            "from oslo_log import log as logging",
            "from six.moves import range",
            "",
            "import nova.conf",
            "from nova import exception",
            "from nova.i18n import _",
            "from nova import rpc",
            "from nova.scheduler import client",
            "from nova.scheduler import driver",
            "",
            "CONF = nova.conf.CONF",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "class FilterScheduler(driver.Scheduler):",
            "    \"\"\"Scheduler that can be used for filtering and weighing.\"\"\"",
            "    def __init__(self, *args, **kwargs):",
            "        super(FilterScheduler, self).__init__(*args, **kwargs)",
            "        self.notifier = rpc.get_notifier('scheduler')",
            "        scheduler_client = client.SchedulerClient()",
            "        self.placement_client = scheduler_client.reportclient",
            "",
            "    def select_destinations(self, context, spec_obj, instance_uuids,",
            "            alloc_reqs_by_rp_uuid, provider_summaries):",
            "        \"\"\"Returns a list of sorted lists of HostState objects (1 for each",
            "        instance) that would satisfy the supplied request_spec. Each of those",
            "        lists consist of [chosen_host, alternate1, ..., alternateN], where the",
            "        'chosen_host' has already had its resources claimed in Placement,",
            "        followed by zero or more alternates. The alternates are hosts that can",
            "        satisfy the request, and are included so that if the build for the",
            "        chosen host fails, the cell conductor can retry.",
            "",
            "        :param context: The RequestContext object",
            "        :param spec_obj: The RequestSpec object",
            "        :param instance_uuids: List of UUIDs, one for each value of the spec",
            "                               object's num_instances attribute",
            "        :param alloc_reqs_by_rp_uuid: Optional dict, keyed by resource provider",
            "                                      UUID, of the allocation_requests that may",
            "                                      be used to claim resources against",
            "                                      matched hosts. If None, indicates either",
            "                                      the placement API wasn't reachable or",
            "                                      that there were no allocation_requests",
            "                                      returned by the placement API. If the",
            "                                      latter, the provider_summaries will be an",
            "                                      empty dict, not None.",
            "        :param provider_summaries: Optional dict, keyed by resource provider",
            "                                   UUID, of information that will be used by",
            "                                   the filters/weighers in selecting matching",
            "                                   hosts for a request. If None, indicates that",
            "                                   the scheduler driver should grab all compute",
            "                                   node information locally and that the",
            "                                   Placement API is not used. If an empty dict,",
            "                                   indicates the Placement API returned no",
            "                                   potential matches for the requested",
            "                                   resources.",
            "        \"\"\"",
            "        self.notifier.info(",
            "            context, 'scheduler.select_destinations.start',",
            "            dict(request_spec=spec_obj.to_legacy_request_spec_dict()))",
            "",
            "        # NOTE(sbauza): The RequestSpec.num_instances field contains the number",
            "        # of instances created when the RequestSpec was used to first boot some",
            "        # instances. This is incorrect when doing a move or resize operation,",
            "        # so prefer the length of instance_uuids unless it is None.",
            "        num_instances = (len(instance_uuids) if instance_uuids",
            "                         else spec_obj.num_instances)",
            "        selected_host_lists = self._schedule(context, spec_obj, instance_uuids,",
            "            alloc_reqs_by_rp_uuid, provider_summaries)",
            "",
            "        # Couldn't fulfill the request_spec",
            "        if len(selected_host_lists) < num_instances:",
            "            # NOTE(Rui Chen): If multiple creates failed, set the updated time",
            "            # of selected HostState to None so that these HostStates are",
            "            # refreshed according to database in next schedule, and release",
            "            # the resource consumed by instance in the process of selecting",
            "            # host.",
            "            for host_list in selected_host_lists:",
            "                host_list[0].updated = None",
            "",
            "            # Log the details but don't put those into the reason since",
            "            # we don't want to give away too much information about our",
            "            # actual environment.",
            "            LOG.debug('There are %(hosts)d hosts available but '",
            "                      '%(num_instances)d instances requested to build.',",
            "                      {'hosts': len(selected_host_lists),",
            "                       'num_instances': num_instances})",
            "",
            "            reason = _('There are not enough hosts available.')",
            "            raise exception.NoValidHost(reason=reason)",
            "",
            "        self.notifier.info(",
            "            context, 'scheduler.select_destinations.end',",
            "            dict(request_spec=spec_obj.to_legacy_request_spec_dict()))",
            "        # NOTE(edleafe) - In this patch we only create the lists of [chosen,",
            "        # alt1, alt2, etc.]. In a later patch we will change what we return, so",
            "        # for this patch just return the selected hosts.",
            "        selected_hosts = [sel_host[0] for sel_host in selected_host_lists]",
            "        return selected_hosts",
            "",
            "    def _schedule(self, context, spec_obj, instance_uuids,",
            "            alloc_reqs_by_rp_uuid, provider_summaries):",
            "        \"\"\"Returns a list of hosts that meet the required specs, ordered by",
            "        their fitness.",
            "",
            "        These hosts will have already had their resources claimed in Placement.",
            "",
            "        :param context: The RequestContext object",
            "        :param spec_obj: The RequestSpec object",
            "        :param instance_uuids: List of instance UUIDs to place or move.",
            "        :param alloc_reqs_by_rp_uuid: Optional dict, keyed by resource provider",
            "                                      UUID, of the allocation_requests that may",
            "                                      be used to claim resources against",
            "                                      matched hosts. If None, indicates either",
            "                                      the placement API wasn't reachable or",
            "                                      that there were no allocation_requests",
            "                                      returned by the placement API. If the",
            "                                      latter, the provider_summaries will be an",
            "                                      empty dict, not None.",
            "        :param provider_summaries: Optional dict, keyed by resource provider",
            "                                   UUID, of information that will be used by",
            "                                   the filters/weighers in selecting matching",
            "                                   hosts for a request. If None, indicates that",
            "                                   the scheduler driver should grab all compute",
            "                                   node information locally and that the",
            "                                   Placement API is not used. If an empty dict,",
            "                                   indicates the Placement API returned no",
            "                                   potential matches for the requested",
            "                                   resources.",
            "        \"\"\"",
            "        elevated = context.elevated()",
            "",
            "        # Find our local list of acceptable hosts by repeatedly",
            "        # filtering and weighing our options. Each time we choose a",
            "        # host, we virtually consume resources on it so subsequent",
            "        # selections can adjust accordingly.",
            "",
            "        # Note: remember, we are using an iterator here. So only",
            "        # traverse this list once. This can bite you if the hosts",
            "        # are being scanned in a filter or weighing function.",
            "        hosts = self._get_all_host_states(elevated, spec_obj,",
            "            provider_summaries)",
            "",
            "        # NOTE(sbauza): The RequestSpec.num_instances field contains the number",
            "        # of instances created when the RequestSpec was used to first boot some",
            "        # instances. This is incorrect when doing a move or resize operation,",
            "        # so prefer the length of instance_uuids unless it is None.",
            "        num_instances = (len(instance_uuids) if instance_uuids",
            "                         else spec_obj.num_instances)",
            "",
            "        # For each requested instance, we want to return a host whose resources",
            "        # for the instance have been claimed, along with zero or more",
            "        # alternates. These alternates will be passed to the cell that the",
            "        # selected host is in, so that if for some reason the build fails, the",
            "        # cell conductor can retry building the instance on one of these",
            "        # alternates instead of having to simply fail. The number of alternates",
            "        # is based on CONF.scheduler.max_attempts; note that if there are not",
            "        # enough filtered hosts to provide the full number of alternates, the",
            "        # list of hosts may be shorter than this amount.",
            "        num_to_return = CONF.scheduler.max_attempts",
            "",
            "        if (instance_uuids is None or",
            "                not self.USES_ALLOCATION_CANDIDATES or",
            "                alloc_reqs_by_rp_uuid is None):",
            "            # We need to support the caching scheduler, which doesn't use the",
            "            # placement API (and has USES_ALLOCATION_CANDIDATE = False) and",
            "            # therefore we skip all the claiming logic for that scheduler",
            "            # driver. Also, if there was a problem communicating with the",
            "            # placement API, alloc_reqs_by_rp_uuid will be None, so we skip",
            "            # claiming in that case as well. In the case where instance_uuids",
            "            # is None, that indicates an older conductor, so we need to return",
            "            # the older-style HostState objects without alternates.",
            "            # NOTE(edleafe): moving this logic into a separate method, as this",
            "            # method is already way too long. It will also make it easier to",
            "            # clean up once we no longer have to worry about older conductors.",
            "            include_alternates = (instance_uuids is not None)",
            "            return self._legacy_find_hosts(num_instances, spec_obj, hosts,",
            "                    num_to_return, include_alternates)",
            "",
            "        # A list of the instance UUIDs that were successfully claimed against",
            "        # in the placement API. If we are not able to successfully claim for",
            "        # all involved instances, we use this list to remove those allocations",
            "        # before returning",
            "        claimed_instance_uuids = []",
            "",
            "        # The list of hosts that have been selected (and claimed).",
            "        claimed_hosts = []",
            "",
            "        for num in range(num_instances):",
            "            hosts = self._get_sorted_hosts(spec_obj, hosts, num)",
            "            if not hosts:",
            "                # NOTE(jaypipes): If we get here, that means not all instances",
            "                # in instance_uuids were able to be matched to a selected host.",
            "                # So, let's clean up any already-claimed allocations here",
            "                # before breaking and returning",
            "                self._cleanup_allocations(claimed_instance_uuids)",
            "                break",
            "",
            "            instance_uuid = instance_uuids[num]",
            "            # Attempt to claim the resources against one or more resource",
            "            # providers, looping over the sorted list of possible hosts",
            "            # looking for an allocation_request that contains that host's",
            "            # resource provider UUID",
            "            claimed_host = None",
            "            for host in hosts:",
            "                cn_uuid = host.uuid",
            "                if cn_uuid not in alloc_reqs_by_rp_uuid:",
            "                    LOG.debug(\"Found host state %s that wasn't in \"",
            "                              \"allocation_requests. Skipping.\", cn_uuid)",
            "                    continue",
            "",
            "                alloc_reqs = alloc_reqs_by_rp_uuid[cn_uuid]",
            "                if self._claim_resources(elevated, spec_obj, instance_uuid,",
            "                        alloc_reqs):",
            "                    claimed_host = host",
            "                    break",
            "",
            "            if claimed_host is None:",
            "                # We weren't able to claim resources in the placement API",
            "                # for any of the sorted hosts identified. So, clean up any",
            "                # successfully-claimed resources for prior instances in",
            "                # this request and return an empty list which will cause",
            "                # select_destinations() to raise NoValidHost",
            "                LOG.debug(\"Unable to successfully claim against any host.\")",
            "                self._cleanup_allocations(claimed_instance_uuids)",
            "                return []",
            "",
            "            claimed_instance_uuids.append(instance_uuid)",
            "            claimed_hosts.append(claimed_host)",
            "",
            "            # Now consume the resources so the filter/weights will change for",
            "            # the next instance.",
            "            self._consume_selected_host(claimed_host, spec_obj)",
            "",
            "        # We have selected and claimed hosts for each instance. Now we need to",
            "        # find alternates for each host.",
            "        selections_to_return = self._get_alternate_hosts(",
            "            claimed_hosts, spec_obj, hosts, num, num_to_return)",
            "        return selections_to_return",
            "",
            "    def _cleanup_allocations(self, instance_uuids):",
            "        \"\"\"Removes allocations for the supplied instance UUIDs.\"\"\"",
            "        if not instance_uuids:",
            "            return",
            "        LOG.debug(\"Cleaning up allocations for %s\", instance_uuids)",
            "        for uuid in instance_uuids:",
            "            self.placement_client.delete_allocation_for_instance(uuid)",
            "",
            "    def _claim_resources(self, ctx, spec_obj, instance_uuid, alloc_reqs):",
            "        \"\"\"Given an instance UUID (representing the consumer of resources), the",
            "        HostState object for the host that was chosen for the instance, and a",
            "        list of allocation_request JSON objects, attempt to claim resources for",
            "        the instance in the placement API. Returns True if the claim process",
            "        was successful, False otherwise.",
            "",
            "        :param ctx: The RequestContext object",
            "        :param spec_obj: The RequestSpec object",
            "        :param instance_uuid: The UUID of the consuming instance",
            "        :param cn_uuid: UUID of the host to allocate against",
            "        :param alloc_reqs: A list of allocation_request JSON objects that",
            "                           allocate against (at least) the compute host",
            "                           selected by the _schedule() method. These",
            "                           allocation_requests were constructed from a call to",
            "                           the GET /allocation_candidates placement API call.",
            "                           Each allocation_request satisfies the original",
            "                           request for resources and can be supplied as-is",
            "                           (along with the project and user ID to the placement",
            "                           API's PUT /allocations/{consumer_uuid} call to claim",
            "                           resources for the instance",
            "        \"\"\"",
            "        LOG.debug(\"Attempting to claim resources in the placement API for \"",
            "                  \"instance %s\", instance_uuid)",
            "",
            "        project_id = spec_obj.project_id",
            "",
            "        # NOTE(jaypipes): So, the RequestSpec doesn't store the user_id,",
            "        # only the project_id, so we need to grab the user information from",
            "        # the context. Perhaps we should consider putting the user ID in",
            "        # the spec object?",
            "        user_id = ctx.user_id",
            "",
            "        # TODO(jaypipes): Loop through all allocation_requests instead of just",
            "        # trying the first one. For now, since we'll likely want to order the",
            "        # allocation_requests in the future based on information in the",
            "        # provider summaries, we'll just try to claim resources using the first",
            "        # allocation_request",
            "        alloc_req = alloc_reqs[0]",
            "",
            "        return self.placement_client.claim_resources(instance_uuid,",
            "            alloc_req, project_id, user_id)",
            "",
            "    def _legacy_find_hosts(self, num_instances, spec_obj, hosts,",
            "            num_to_return, include_alternates):",
            "        \"\"\"Some schedulers do not do claiming, or we can sometimes not be able",
            "        to if the Placement service is not reachable. Additionally, we may be",
            "        working with older conductors that don't pass in instance_uuids.",
            "        \"\"\"",
            "        # The list of hosts selected for each instance",
            "        selected_hosts = []",
            "        # This the overall list of values to be returned. There will be one",
            "        # item per instance, and when 'include_alternates' is True, that item",
            "        # will be a list of HostState objects representing the selected host",
            "        # along with alternates from the same cell. When 'include_alternates'",
            "        # is False, the return value will be a list of HostState objects, with",
            "        # one per requested instance.",
            "        selections_to_return = []",
            "",
            "        for num in range(num_instances):",
            "            hosts = self._get_sorted_hosts(spec_obj, hosts, num)",
            "            if not hosts:",
            "                return []",
            "            selected_host = hosts[0]",
            "            selected_hosts.append(selected_host)",
            "            self._consume_selected_host(selected_host, spec_obj)",
            "",
            "        if include_alternates:",
            "            selections_to_return = self._get_alternate_hosts(",
            "                selected_hosts, spec_obj, hosts, num, num_to_return)",
            "            return selections_to_return",
            "        # No alternatives but we still need to return a list of lists of hosts",
            "        return [[host] for host in selected_hosts]",
            "",
            "    @staticmethod",
            "    def _consume_selected_host(selected_host, spec_obj):",
            "        LOG.debug(\"Selected host: %(host)s\", {'host': selected_host})",
            "        selected_host.consume_from_request(spec_obj)",
            "        if spec_obj.instance_group is not None:",
            "            spec_obj.instance_group.hosts.append(selected_host.host)",
            "            # hosts has to be not part of the updates when saving",
            "            spec_obj.instance_group.obj_reset_changes(['hosts'])",
            "",
            "    def _get_alternate_hosts(self, selected_hosts, spec_obj, hosts, index,",
            "                             num_to_return):",
            "        # We only need to filter/weigh the hosts again if we're dealing with",
            "        # more than one instance since the single selected host will get",
            "        # filtered out of the list of alternates below.",
            "        if index > 0:",
            "            # The selected_hosts have all had resources 'claimed' via",
            "            # _consume_selected_host, so we need to filter/weigh and sort the",
            "            # hosts again to get an accurate count for alternates.",
            "            hosts = self._get_sorted_hosts(spec_obj, hosts, index)",
            "        # This is the overall list of values to be returned. There will be one",
            "        # item per instance, and that item will be a list of HostState objects",
            "        # representing the selected host along with alternates from the same",
            "        # cell.",
            "        selections_to_return = []",
            "        for selected_host in selected_hosts:",
            "            # This is the list of hosts for one particular instance.",
            "            selected_plus_alts = [selected_host]",
            "            cell_uuid = selected_host.cell_uuid",
            "            # This will populate the alternates with many of the same unclaimed",
            "            # hosts. This is OK, as it should be rare for a build to fail. And",
            "            # if there are not enough hosts to fully populate the alternates,",
            "            # it's fine to return fewer than we'd like. Note that we exclude",
            "            # any claimed host from consideration as an alternate because it",
            "            # will have had its resources reduced and will have a much lower",
            "            # chance of being able to fit another instance on it.",
            "            for host in hosts:",
            "                if len(selected_plus_alts) >= num_to_return:",
            "                    break",
            "                if host.cell_uuid == cell_uuid and host not in selected_hosts:",
            "                    selected_plus_alts.append(host)",
            "            selections_to_return.append(selected_plus_alts)",
            "        return selections_to_return",
            "",
            "    def _get_sorted_hosts(self, spec_obj, host_states, index):",
            "        \"\"\"Returns a list of HostState objects that match the required",
            "        scheduling constraints for the request spec object and have been sorted",
            "        according to the weighers.",
            "        \"\"\"",
            "        filtered_hosts = self.host_manager.get_filtered_hosts(host_states,",
            "            spec_obj, index)",
            "",
            "        LOG.debug(\"Filtered %(hosts)s\", {'hosts': filtered_hosts})",
            "",
            "        if not filtered_hosts:",
            "            return []",
            "",
            "        weighed_hosts = self.host_manager.get_weighed_hosts(filtered_hosts,",
            "            spec_obj)",
            "        if CONF.filter_scheduler.shuffle_best_same_weighed_hosts:",
            "            # NOTE(pas-ha) Randomize best hosts, relying on weighed_hosts",
            "            # being already sorted by weight in descending order.",
            "            # This decreases possible contention and rescheduling attempts",
            "            # when there is a large number of hosts having the same best",
            "            # weight, especially so when host_subset_size is 1 (default)",
            "            best_hosts = [w for w in weighed_hosts",
            "                          if w.weight == weighed_hosts[0].weight]",
            "            random.shuffle(best_hosts)",
            "            weighed_hosts = best_hosts + weighed_hosts[len(best_hosts):]",
            "        # Strip off the WeighedHost wrapper class...",
            "        weighed_hosts = [h.obj for h in weighed_hosts]",
            "",
            "        LOG.debug(\"Weighed %(hosts)s\", {'hosts': weighed_hosts})",
            "",
            "        # We randomize the first element in the returned list to alleviate",
            "        # congestion where the same host is consistently selected among",
            "        # numerous potential hosts for similar request specs.",
            "        host_subset_size = CONF.filter_scheduler.host_subset_size",
            "        if host_subset_size < len(weighed_hosts):",
            "            weighed_subset = weighed_hosts[0:host_subset_size]",
            "        else:",
            "            weighed_subset = weighed_hosts",
            "        chosen_host = random.choice(weighed_subset)",
            "        weighed_hosts.remove(chosen_host)",
            "        return [chosen_host] + weighed_hosts",
            "",
            "    def _get_all_host_states(self, context, spec_obj, provider_summaries):",
            "        \"\"\"Template method, so a subclass can implement caching.\"\"\"",
            "        # NOTE(jaypipes): provider_summaries being None is treated differently",
            "        # from an empty dict. provider_summaries is None when we want to grab",
            "        # all compute nodes, for instance when using the caching scheduler.",
            "        # The provider_summaries variable will be an empty dict when the",
            "        # Placement API found no providers that match the requested",
            "        # constraints, which in turn makes compute_uuids an empty list and",
            "        # get_host_states_by_uuids will return an empty tuple also, which will",
            "        # eventually result in a NoValidHost error.",
            "        compute_uuids = None",
            "        if provider_summaries is not None:",
            "            compute_uuids = list(provider_summaries.keys())",
            "        return self.host_manager.get_host_states_by_uuids(context,",
            "                                                          compute_uuids,",
            "                                                          spec_obj)"
        ],
        "afterPatchFile": [
            "# Copyright (c) 2011 OpenStack Foundation",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "\"\"\"",
            "The FilterScheduler is for creating instances locally.",
            "You can customize this scheduler by specifying your own Host Filters and",
            "Weighing Functions.",
            "\"\"\"",
            "",
            "import random",
            "",
            "from oslo_log import log as logging",
            "from six.moves import range",
            "",
            "import nova.conf",
            "from nova import exception",
            "from nova.i18n import _",
            "from nova import rpc",
            "from nova.scheduler import client",
            "from nova.scheduler import driver",
            "from nova.scheduler import utils",
            "",
            "CONF = nova.conf.CONF",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "class FilterScheduler(driver.Scheduler):",
            "    \"\"\"Scheduler that can be used for filtering and weighing.\"\"\"",
            "    def __init__(self, *args, **kwargs):",
            "        super(FilterScheduler, self).__init__(*args, **kwargs)",
            "        self.notifier = rpc.get_notifier('scheduler')",
            "        scheduler_client = client.SchedulerClient()",
            "        self.placement_client = scheduler_client.reportclient",
            "",
            "    def select_destinations(self, context, spec_obj, instance_uuids,",
            "            alloc_reqs_by_rp_uuid, provider_summaries):",
            "        \"\"\"Returns a list of sorted lists of HostState objects (1 for each",
            "        instance) that would satisfy the supplied request_spec. Each of those",
            "        lists consist of [chosen_host, alternate1, ..., alternateN], where the",
            "        'chosen_host' has already had its resources claimed in Placement,",
            "        followed by zero or more alternates. The alternates are hosts that can",
            "        satisfy the request, and are included so that if the build for the",
            "        chosen host fails, the cell conductor can retry.",
            "",
            "        :param context: The RequestContext object",
            "        :param spec_obj: The RequestSpec object",
            "        :param instance_uuids: List of UUIDs, one for each value of the spec",
            "                               object's num_instances attribute",
            "        :param alloc_reqs_by_rp_uuid: Optional dict, keyed by resource provider",
            "                                      UUID, of the allocation_requests that may",
            "                                      be used to claim resources against",
            "                                      matched hosts. If None, indicates either",
            "                                      the placement API wasn't reachable or",
            "                                      that there were no allocation_requests",
            "                                      returned by the placement API. If the",
            "                                      latter, the provider_summaries will be an",
            "                                      empty dict, not None.",
            "        :param provider_summaries: Optional dict, keyed by resource provider",
            "                                   UUID, of information that will be used by",
            "                                   the filters/weighers in selecting matching",
            "                                   hosts for a request. If None, indicates that",
            "                                   the scheduler driver should grab all compute",
            "                                   node information locally and that the",
            "                                   Placement API is not used. If an empty dict,",
            "                                   indicates the Placement API returned no",
            "                                   potential matches for the requested",
            "                                   resources.",
            "        \"\"\"",
            "        self.notifier.info(",
            "            context, 'scheduler.select_destinations.start',",
            "            dict(request_spec=spec_obj.to_legacy_request_spec_dict()))",
            "",
            "        # NOTE(sbauza): The RequestSpec.num_instances field contains the number",
            "        # of instances created when the RequestSpec was used to first boot some",
            "        # instances. This is incorrect when doing a move or resize operation,",
            "        # so prefer the length of instance_uuids unless it is None.",
            "        num_instances = (len(instance_uuids) if instance_uuids",
            "                         else spec_obj.num_instances)",
            "        selected_host_lists = self._schedule(context, spec_obj, instance_uuids,",
            "            alloc_reqs_by_rp_uuid, provider_summaries)",
            "",
            "        # Couldn't fulfill the request_spec",
            "        if len(selected_host_lists) < num_instances:",
            "            # NOTE(Rui Chen): If multiple creates failed, set the updated time",
            "            # of selected HostState to None so that these HostStates are",
            "            # refreshed according to database in next schedule, and release",
            "            # the resource consumed by instance in the process of selecting",
            "            # host.",
            "            for host_list in selected_host_lists:",
            "                host_list[0].updated = None",
            "",
            "            # Log the details but don't put those into the reason since",
            "            # we don't want to give away too much information about our",
            "            # actual environment.",
            "            LOG.debug('There are %(hosts)d hosts available but '",
            "                      '%(num_instances)d instances requested to build.',",
            "                      {'hosts': len(selected_host_lists),",
            "                       'num_instances': num_instances})",
            "",
            "            reason = _('There are not enough hosts available.')",
            "            raise exception.NoValidHost(reason=reason)",
            "",
            "        self.notifier.info(",
            "            context, 'scheduler.select_destinations.end',",
            "            dict(request_spec=spec_obj.to_legacy_request_spec_dict()))",
            "        # NOTE(edleafe) - In this patch we only create the lists of [chosen,",
            "        # alt1, alt2, etc.]. In a later patch we will change what we return, so",
            "        # for this patch just return the selected hosts.",
            "        selected_hosts = [sel_host[0] for sel_host in selected_host_lists]",
            "        return selected_hosts",
            "",
            "    def _schedule(self, context, spec_obj, instance_uuids,",
            "            alloc_reqs_by_rp_uuid, provider_summaries):",
            "        \"\"\"Returns a list of hosts that meet the required specs, ordered by",
            "        their fitness.",
            "",
            "        These hosts will have already had their resources claimed in Placement.",
            "",
            "        :param context: The RequestContext object",
            "        :param spec_obj: The RequestSpec object",
            "        :param instance_uuids: List of instance UUIDs to place or move.",
            "        :param alloc_reqs_by_rp_uuid: Optional dict, keyed by resource provider",
            "                                      UUID, of the allocation_requests that may",
            "                                      be used to claim resources against",
            "                                      matched hosts. If None, indicates either",
            "                                      the placement API wasn't reachable or",
            "                                      that there were no allocation_requests",
            "                                      returned by the placement API. If the",
            "                                      latter, the provider_summaries will be an",
            "                                      empty dict, not None.",
            "        :param provider_summaries: Optional dict, keyed by resource provider",
            "                                   UUID, of information that will be used by",
            "                                   the filters/weighers in selecting matching",
            "                                   hosts for a request. If None, indicates that",
            "                                   the scheduler driver should grab all compute",
            "                                   node information locally and that the",
            "                                   Placement API is not used. If an empty dict,",
            "                                   indicates the Placement API returned no",
            "                                   potential matches for the requested",
            "                                   resources.",
            "        \"\"\"",
            "        elevated = context.elevated()",
            "",
            "        # Find our local list of acceptable hosts by repeatedly",
            "        # filtering and weighing our options. Each time we choose a",
            "        # host, we virtually consume resources on it so subsequent",
            "        # selections can adjust accordingly.",
            "",
            "        # Note: remember, we are using an iterator here. So only",
            "        # traverse this list once. This can bite you if the hosts",
            "        # are being scanned in a filter or weighing function.",
            "        hosts = self._get_all_host_states(elevated, spec_obj,",
            "            provider_summaries)",
            "",
            "        # NOTE(sbauza): The RequestSpec.num_instances field contains the number",
            "        # of instances created when the RequestSpec was used to first boot some",
            "        # instances. This is incorrect when doing a move or resize operation,",
            "        # so prefer the length of instance_uuids unless it is None.",
            "        num_instances = (len(instance_uuids) if instance_uuids",
            "                         else spec_obj.num_instances)",
            "",
            "        # For each requested instance, we want to return a host whose resources",
            "        # for the instance have been claimed, along with zero or more",
            "        # alternates. These alternates will be passed to the cell that the",
            "        # selected host is in, so that if for some reason the build fails, the",
            "        # cell conductor can retry building the instance on one of these",
            "        # alternates instead of having to simply fail. The number of alternates",
            "        # is based on CONF.scheduler.max_attempts; note that if there are not",
            "        # enough filtered hosts to provide the full number of alternates, the",
            "        # list of hosts may be shorter than this amount.",
            "        num_to_return = CONF.scheduler.max_attempts",
            "",
            "        if (instance_uuids is None or",
            "                not self.USES_ALLOCATION_CANDIDATES or",
            "                alloc_reqs_by_rp_uuid is None):",
            "            # We need to support the caching scheduler, which doesn't use the",
            "            # placement API (and has USES_ALLOCATION_CANDIDATE = False) and",
            "            # therefore we skip all the claiming logic for that scheduler",
            "            # driver. Also, if there was a problem communicating with the",
            "            # placement API, alloc_reqs_by_rp_uuid will be None, so we skip",
            "            # claiming in that case as well. In the case where instance_uuids",
            "            # is None, that indicates an older conductor, so we need to return",
            "            # the older-style HostState objects without alternates.",
            "            # NOTE(edleafe): moving this logic into a separate method, as this",
            "            # method is already way too long. It will also make it easier to",
            "            # clean up once we no longer have to worry about older conductors.",
            "            include_alternates = (instance_uuids is not None)",
            "            return self._legacy_find_hosts(num_instances, spec_obj, hosts,",
            "                    num_to_return, include_alternates)",
            "",
            "        # A list of the instance UUIDs that were successfully claimed against",
            "        # in the placement API. If we are not able to successfully claim for",
            "        # all involved instances, we use this list to remove those allocations",
            "        # before returning",
            "        claimed_instance_uuids = []",
            "",
            "        # The list of hosts that have been selected (and claimed).",
            "        claimed_hosts = []",
            "",
            "        for num in range(num_instances):",
            "            hosts = self._get_sorted_hosts(spec_obj, hosts, num)",
            "            if not hosts:",
            "                # NOTE(jaypipes): If we get here, that means not all instances",
            "                # in instance_uuids were able to be matched to a selected host.",
            "                # So, let's clean up any already-claimed allocations here",
            "                # before breaking and returning",
            "                self._cleanup_allocations(claimed_instance_uuids)",
            "                break",
            "",
            "            instance_uuid = instance_uuids[num]",
            "            # Attempt to claim the resources against one or more resource",
            "            # providers, looping over the sorted list of possible hosts",
            "            # looking for an allocation_request that contains that host's",
            "            # resource provider UUID",
            "            claimed_host = None",
            "            for host in hosts:",
            "                cn_uuid = host.uuid",
            "                if cn_uuid not in alloc_reqs_by_rp_uuid:",
            "                    LOG.debug(\"Found host state %s that wasn't in \"",
            "                              \"allocation_requests. Skipping.\", cn_uuid)",
            "                    continue",
            "",
            "                alloc_reqs = alloc_reqs_by_rp_uuid[cn_uuid]",
            "                if self._claim_resources(elevated, spec_obj, instance_uuid,",
            "                        alloc_reqs):",
            "                    claimed_host = host",
            "                    break",
            "",
            "            if claimed_host is None:",
            "                # We weren't able to claim resources in the placement API",
            "                # for any of the sorted hosts identified. So, clean up any",
            "                # successfully-claimed resources for prior instances in",
            "                # this request and return an empty list which will cause",
            "                # select_destinations() to raise NoValidHost",
            "                LOG.debug(\"Unable to successfully claim against any host.\")",
            "                self._cleanup_allocations(claimed_instance_uuids)",
            "                return []",
            "",
            "            claimed_instance_uuids.append(instance_uuid)",
            "            claimed_hosts.append(claimed_host)",
            "",
            "            # Now consume the resources so the filter/weights will change for",
            "            # the next instance.",
            "            self._consume_selected_host(claimed_host, spec_obj)",
            "",
            "        # We have selected and claimed hosts for each instance. Now we need to",
            "        # find alternates for each host.",
            "        selections_to_return = self._get_alternate_hosts(",
            "            claimed_hosts, spec_obj, hosts, num, num_to_return)",
            "        return selections_to_return",
            "",
            "    def _cleanup_allocations(self, instance_uuids):",
            "        \"\"\"Removes allocations for the supplied instance UUIDs.\"\"\"",
            "        if not instance_uuids:",
            "            return",
            "        LOG.debug(\"Cleaning up allocations for %s\", instance_uuids)",
            "        for uuid in instance_uuids:",
            "            self.placement_client.delete_allocation_for_instance(uuid)",
            "",
            "    def _claim_resources(self, ctx, spec_obj, instance_uuid, alloc_reqs):",
            "        \"\"\"Given an instance UUID (representing the consumer of resources), the",
            "        HostState object for the host that was chosen for the instance, and a",
            "        list of allocation_request JSON objects, attempt to claim resources for",
            "        the instance in the placement API. Returns True if the claim process",
            "        was successful, False otherwise.",
            "",
            "        :param ctx: The RequestContext object",
            "        :param spec_obj: The RequestSpec object",
            "        :param instance_uuid: The UUID of the consuming instance",
            "        :param cn_uuid: UUID of the host to allocate against",
            "        :param alloc_reqs: A list of allocation_request JSON objects that",
            "                           allocate against (at least) the compute host",
            "                           selected by the _schedule() method. These",
            "                           allocation_requests were constructed from a call to",
            "                           the GET /allocation_candidates placement API call.",
            "                           Each allocation_request satisfies the original",
            "                           request for resources and can be supplied as-is",
            "                           (along with the project and user ID to the placement",
            "                           API's PUT /allocations/{consumer_uuid} call to claim",
            "                           resources for the instance",
            "        \"\"\"",
            "",
            "        if utils.request_is_rebuild(spec_obj):",
            "            # NOTE(danms): This is a rebuild-only scheduling request, so we",
            "            # should not be doing any extra claiming",
            "            LOG.debug('Not claiming resources in the placement API for '",
            "                      'rebuild-only scheduling of instance %(uuid)s',",
            "                      {'uuid': instance_uuid})",
            "            return True",
            "",
            "        LOG.debug(\"Attempting to claim resources in the placement API for \"",
            "                  \"instance %s\", instance_uuid)",
            "",
            "        project_id = spec_obj.project_id",
            "",
            "        # NOTE(jaypipes): So, the RequestSpec doesn't store the user_id,",
            "        # only the project_id, so we need to grab the user information from",
            "        # the context. Perhaps we should consider putting the user ID in",
            "        # the spec object?",
            "        user_id = ctx.user_id",
            "",
            "        # TODO(jaypipes): Loop through all allocation_requests instead of just",
            "        # trying the first one. For now, since we'll likely want to order the",
            "        # allocation_requests in the future based on information in the",
            "        # provider summaries, we'll just try to claim resources using the first",
            "        # allocation_request",
            "        alloc_req = alloc_reqs[0]",
            "",
            "        return self.placement_client.claim_resources(instance_uuid,",
            "            alloc_req, project_id, user_id)",
            "",
            "    def _legacy_find_hosts(self, num_instances, spec_obj, hosts,",
            "            num_to_return, include_alternates):",
            "        \"\"\"Some schedulers do not do claiming, or we can sometimes not be able",
            "        to if the Placement service is not reachable. Additionally, we may be",
            "        working with older conductors that don't pass in instance_uuids.",
            "        \"\"\"",
            "        # The list of hosts selected for each instance",
            "        selected_hosts = []",
            "        # This the overall list of values to be returned. There will be one",
            "        # item per instance, and when 'include_alternates' is True, that item",
            "        # will be a list of HostState objects representing the selected host",
            "        # along with alternates from the same cell. When 'include_alternates'",
            "        # is False, the return value will be a list of HostState objects, with",
            "        # one per requested instance.",
            "        selections_to_return = []",
            "",
            "        for num in range(num_instances):",
            "            hosts = self._get_sorted_hosts(spec_obj, hosts, num)",
            "            if not hosts:",
            "                return []",
            "            selected_host = hosts[0]",
            "            selected_hosts.append(selected_host)",
            "            self._consume_selected_host(selected_host, spec_obj)",
            "",
            "        if include_alternates:",
            "            selections_to_return = self._get_alternate_hosts(",
            "                selected_hosts, spec_obj, hosts, num, num_to_return)",
            "            return selections_to_return",
            "        # No alternatives but we still need to return a list of lists of hosts",
            "        return [[host] for host in selected_hosts]",
            "",
            "    @staticmethod",
            "    def _consume_selected_host(selected_host, spec_obj):",
            "        LOG.debug(\"Selected host: %(host)s\", {'host': selected_host})",
            "        selected_host.consume_from_request(spec_obj)",
            "        if spec_obj.instance_group is not None:",
            "            spec_obj.instance_group.hosts.append(selected_host.host)",
            "            # hosts has to be not part of the updates when saving",
            "            spec_obj.instance_group.obj_reset_changes(['hosts'])",
            "",
            "    def _get_alternate_hosts(self, selected_hosts, spec_obj, hosts, index,",
            "                             num_to_return):",
            "        # We only need to filter/weigh the hosts again if we're dealing with",
            "        # more than one instance since the single selected host will get",
            "        # filtered out of the list of alternates below.",
            "        if index > 0:",
            "            # The selected_hosts have all had resources 'claimed' via",
            "            # _consume_selected_host, so we need to filter/weigh and sort the",
            "            # hosts again to get an accurate count for alternates.",
            "            hosts = self._get_sorted_hosts(spec_obj, hosts, index)",
            "        # This is the overall list of values to be returned. There will be one",
            "        # item per instance, and that item will be a list of HostState objects",
            "        # representing the selected host along with alternates from the same",
            "        # cell.",
            "        selections_to_return = []",
            "        for selected_host in selected_hosts:",
            "            # This is the list of hosts for one particular instance.",
            "            selected_plus_alts = [selected_host]",
            "            cell_uuid = selected_host.cell_uuid",
            "            # This will populate the alternates with many of the same unclaimed",
            "            # hosts. This is OK, as it should be rare for a build to fail. And",
            "            # if there are not enough hosts to fully populate the alternates,",
            "            # it's fine to return fewer than we'd like. Note that we exclude",
            "            # any claimed host from consideration as an alternate because it",
            "            # will have had its resources reduced and will have a much lower",
            "            # chance of being able to fit another instance on it.",
            "            for host in hosts:",
            "                if len(selected_plus_alts) >= num_to_return:",
            "                    break",
            "                if host.cell_uuid == cell_uuid and host not in selected_hosts:",
            "                    selected_plus_alts.append(host)",
            "            selections_to_return.append(selected_plus_alts)",
            "        return selections_to_return",
            "",
            "    def _get_sorted_hosts(self, spec_obj, host_states, index):",
            "        \"\"\"Returns a list of HostState objects that match the required",
            "        scheduling constraints for the request spec object and have been sorted",
            "        according to the weighers.",
            "        \"\"\"",
            "        filtered_hosts = self.host_manager.get_filtered_hosts(host_states,",
            "            spec_obj, index)",
            "",
            "        LOG.debug(\"Filtered %(hosts)s\", {'hosts': filtered_hosts})",
            "",
            "        if not filtered_hosts:",
            "            return []",
            "",
            "        weighed_hosts = self.host_manager.get_weighed_hosts(filtered_hosts,",
            "            spec_obj)",
            "        if CONF.filter_scheduler.shuffle_best_same_weighed_hosts:",
            "            # NOTE(pas-ha) Randomize best hosts, relying on weighed_hosts",
            "            # being already sorted by weight in descending order.",
            "            # This decreases possible contention and rescheduling attempts",
            "            # when there is a large number of hosts having the same best",
            "            # weight, especially so when host_subset_size is 1 (default)",
            "            best_hosts = [w for w in weighed_hosts",
            "                          if w.weight == weighed_hosts[0].weight]",
            "            random.shuffle(best_hosts)",
            "            weighed_hosts = best_hosts + weighed_hosts[len(best_hosts):]",
            "        # Strip off the WeighedHost wrapper class...",
            "        weighed_hosts = [h.obj for h in weighed_hosts]",
            "",
            "        LOG.debug(\"Weighed %(hosts)s\", {'hosts': weighed_hosts})",
            "",
            "        # We randomize the first element in the returned list to alleviate",
            "        # congestion where the same host is consistently selected among",
            "        # numerous potential hosts for similar request specs.",
            "        host_subset_size = CONF.filter_scheduler.host_subset_size",
            "        if host_subset_size < len(weighed_hosts):",
            "            weighed_subset = weighed_hosts[0:host_subset_size]",
            "        else:",
            "            weighed_subset = weighed_hosts",
            "        chosen_host = random.choice(weighed_subset)",
            "        weighed_hosts.remove(chosen_host)",
            "        return [chosen_host] + weighed_hosts",
            "",
            "    def _get_all_host_states(self, context, spec_obj, provider_summaries):",
            "        \"\"\"Template method, so a subclass can implement caching.\"\"\"",
            "        # NOTE(jaypipes): provider_summaries being None is treated differently",
            "        # from an empty dict. provider_summaries is None when we want to grab",
            "        # all compute nodes, for instance when using the caching scheduler.",
            "        # The provider_summaries variable will be an empty dict when the",
            "        # Placement API found no providers that match the requested",
            "        # constraints, which in turn makes compute_uuids an empty list and",
            "        # get_host_states_by_uuids will return an empty tuple also, which will",
            "        # eventually result in a NoValidHost error.",
            "        compute_uuids = None",
            "        if provider_summaries is not None:",
            "            compute_uuids = list(provider_summaries.keys())",
            "        return self.host_manager.get_host_states_by_uuids(context,",
            "                                                          compute_uuids,",
            "                                                          spec_obj)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "nova.scheduler.filter_scheduler.FilterScheduler._schedule",
            "omeroweb.webgateway.views"
        ]
    },
    "nova/tests/functional/test_servers.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1170,
                "afterPatchRowNumber": 1170,
                "PatchRowcode": "             self.assertEqual(flavor['ram'], allocation['MEMORY_MB'])"
            },
            "1": {
                "beforePatchRowNumber": 1171,
                "afterPatchRowNumber": 1171,
                "PatchRowcode": "             self.assertEqual(flavor['disk'], allocation['DISK_GB'])"
            },
            "2": {
                "beforePatchRowNumber": 1172,
                "afterPatchRowNumber": 1172,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 1173,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        def assertFlavorsMatchAllocation(old_flavor, new_flavor,"
            },
            "4": {
                "beforePatchRowNumber": 1174,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                         allocation):"
            },
            "5": {
                "beforePatchRowNumber": 1175,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self.assertEqual(old_flavor['vcpus'] + new_flavor['vcpus'],"
            },
            "6": {
                "beforePatchRowNumber": 1176,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                             allocation['VCPU'])"
            },
            "7": {
                "beforePatchRowNumber": 1177,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self.assertEqual(old_flavor['ram'] + new_flavor['ram'],"
            },
            "8": {
                "beforePatchRowNumber": 1178,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                             allocation['MEMORY_MB'])"
            },
            "9": {
                "beforePatchRowNumber": 1179,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self.assertEqual(old_flavor['disk'] + new_flavor['disk'],"
            },
            "10": {
                "beforePatchRowNumber": 1180,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                             allocation['DISK_GB'])"
            },
            "11": {
                "beforePatchRowNumber": 1181,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "12": {
                "beforePatchRowNumber": 1182,
                "afterPatchRowNumber": 1173,
                "PatchRowcode": "         nodename = self.compute.manager._get_nodename(None)"
            },
            "13": {
                "beforePatchRowNumber": 1183,
                "afterPatchRowNumber": 1174,
                "PatchRowcode": "         rp_uuid = _get_provider_uuid_by_host(nodename)"
            },
            "14": {
                "beforePatchRowNumber": 1184,
                "afterPatchRowNumber": 1175,
                "PatchRowcode": "         # make sure we start with no usage on the compute node"
            },
            "15": {
                "beforePatchRowNumber": 1225,
                "afterPatchRowNumber": 1216,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 1226,
                "afterPatchRowNumber": 1217,
                "PatchRowcode": "         # The usage and allocations should not have changed."
            },
            "17": {
                "beforePatchRowNumber": 1227,
                "afterPatchRowNumber": 1218,
                "PatchRowcode": "         rp_usages = _get_provider_usages(rp_uuid)"
            },
            "18": {
                "beforePatchRowNumber": 1228,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # FIXME(mriedem): This is a bug where the scheduler doubled up the"
            },
            "19": {
                "beforePatchRowNumber": 1229,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # allocations for the instance even though we're just rebuilding"
            },
            "20": {
                "beforePatchRowNumber": 1230,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # to the same host. Uncomment this once fixed."
            },
            "21": {
                "beforePatchRowNumber": 1231,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # assertFlavorMatchesAllocation(flavor, rp_usages)"
            },
            "22": {
                "beforePatchRowNumber": 1232,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        assertFlavorsMatchAllocation(flavor, flavor, rp_usages)"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1219,
                "PatchRowcode": "+        assertFlavorMatchesAllocation(flavor, rp_usages)"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1220,
                "PatchRowcode": "+"
            },
            "25": {
                "beforePatchRowNumber": 1233,
                "afterPatchRowNumber": 1221,
                "PatchRowcode": "         allocs = _get_allocations_by_server_uuid(server['id'])"
            },
            "26": {
                "beforePatchRowNumber": 1234,
                "afterPatchRowNumber": 1222,
                "PatchRowcode": "         self.assertIn(rp_uuid, allocs)"
            },
            "27": {
                "beforePatchRowNumber": 1235,
                "afterPatchRowNumber": 1223,
                "PatchRowcode": "         allocs = allocs[rp_uuid]['resources']"
            },
            "28": {
                "beforePatchRowNumber": 1236,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # assertFlavorMatchesAllocation(flavor, allocs)"
            },
            "29": {
                "beforePatchRowNumber": 1237,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        assertFlavorsMatchAllocation(flavor, flavor, allocs)"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1224,
                "PatchRowcode": "+        assertFlavorMatchesAllocation(flavor, allocs)"
            },
            "31": {
                "beforePatchRowNumber": 1238,
                "afterPatchRowNumber": 1225,
                "PatchRowcode": " "
            },
            "32": {
                "beforePatchRowNumber": 1239,
                "afterPatchRowNumber": 1226,
                "PatchRowcode": " "
            },
            "33": {
                "beforePatchRowNumber": 1240,
                "afterPatchRowNumber": 1227,
                "PatchRowcode": " class ProviderUsageBaseTestCase(test.TestCase,"
            }
        },
        "frontPatchFile": [
            "# Copyright 2011 Justin Santa Barbara",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "import datetime",
            "import time",
            "import zlib",
            "",
            "import mock",
            "from oslo_log import log as logging",
            "from oslo_serialization import base64",
            "from oslo_utils import timeutils",
            "",
            "from nova.compute import api as compute_api",
            "from nova.compute import instance_actions",
            "from nova.compute import rpcapi",
            "from nova import context",
            "from nova import db",
            "from nova import exception",
            "from nova import objects",
            "from nova.objects import block_device as block_device_obj",
            "from nova import test",
            "from nova.tests import fixtures as nova_fixtures",
            "from nova.tests.functional.api import client",
            "from nova.tests.functional import integrated_helpers",
            "from nova.tests.unit.api.openstack import fakes",
            "from nova.tests.unit import fake_block_device",
            "from nova.tests.unit import fake_network",
            "from nova.tests.unit import fake_notifier",
            "import nova.tests.unit.image.fake",
            "from nova.tests.unit import policy_fixture",
            "from nova.virt import fake",
            "from nova import volume",
            "",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "class ServersTestBase(integrated_helpers._IntegratedTestBase):",
            "    api_major_version = 'v2'",
            "    _force_delete_parameter = 'forceDelete'",
            "    _image_ref_parameter = 'imageRef'",
            "    _flavor_ref_parameter = 'flavorRef'",
            "    _access_ipv4_parameter = 'accessIPv4'",
            "    _access_ipv6_parameter = 'accessIPv6'",
            "    _return_resv_id_parameter = 'return_reservation_id'",
            "    _min_count_parameter = 'min_count'",
            "",
            "    def setUp(self):",
            "        super(ServersTestBase, self).setUp()",
            "        # The network service is called as part of server creates but no",
            "        # networks have been populated in the db, so stub the methods.",
            "        # The networks aren't relevant to what is being tested.",
            "        fake_network.set_stub_network_methods(self)",
            "        self.conductor = self.start_service(",
            "            'conductor', manager='nova.conductor.manager.ConductorManager')",
            "",
            "    def _wait_for_state_change(self, server, from_status):",
            "        for i in range(0, 50):",
            "            server = self.api.get_server(server['id'])",
            "            if server['status'] != from_status:",
            "                break",
            "            time.sleep(.1)",
            "",
            "        return server",
            "",
            "    def _wait_for_deletion(self, server_id):",
            "        # Wait (briefly) for deletion",
            "        for _retries in range(50):",
            "            try:",
            "                found_server = self.api.get_server(server_id)",
            "            except client.OpenStackApiNotFoundException:",
            "                found_server = None",
            "                LOG.debug(\"Got 404, proceeding\")",
            "                break",
            "",
            "            LOG.debug(\"Found_server=%s\", found_server)",
            "",
            "            # TODO(justinsb): Mock doesn't yet do accurate state changes",
            "            # if found_server['status'] != 'deleting':",
            "            #    break",
            "            time.sleep(.1)",
            "",
            "        # Should be gone",
            "        self.assertFalse(found_server)",
            "",
            "    def _delete_server(self, server_id):",
            "        # Delete the server",
            "        self.api.delete_server(server_id)",
            "        self._wait_for_deletion(server_id)",
            "",
            "    def _get_access_ips_params(self):",
            "        return {self._access_ipv4_parameter: \"172.19.0.2\",",
            "                self._access_ipv6_parameter: \"fe80::2\"}",
            "",
            "    def _verify_access_ips(self, server):",
            "        self.assertEqual('172.19.0.2',",
            "                         server[self._access_ipv4_parameter])",
            "        self.assertEqual('fe80::2', server[self._access_ipv6_parameter])",
            "",
            "",
            "class ServersTest(ServersTestBase):",
            "",
            "    def test_get_servers(self):",
            "        # Simple check that listing servers works.",
            "        servers = self.api.get_servers()",
            "        for server in servers:",
            "            LOG.debug(\"server: %s\", server)",
            "",
            "    def test_create_server_with_error(self):",
            "        # Create a server which will enter error state.",
            "",
            "        def throw_error(*args, **kwargs):",
            "            raise exception.BuildAbortException(reason='',",
            "                    instance_uuid='fake')",
            "",
            "        self.stub_out('nova.virt.fake.FakeDriver.spawn', throw_error)",
            "",
            "        server = self._build_minimal_create_server_request()",
            "        created_server = self.api.post_server({\"server\": server})",
            "        created_server_id = created_server['id']",
            "",
            "        found_server = self.api.get_server(created_server_id)",
            "        self.assertEqual(created_server_id, found_server['id'])",
            "",
            "        found_server = self._wait_for_state_change(found_server, 'BUILD')",
            "",
            "        self.assertEqual('ERROR', found_server['status'])",
            "        self._delete_server(created_server_id)",
            "",
            "    def test_create_and_delete_server(self):",
            "        # Creates and deletes a server.",
            "",
            "        # Create server",
            "        # Build the server data gradually, checking errors along the way",
            "        server = {}",
            "        good_server = self._build_minimal_create_server_request()",
            "",
            "        post = {'server': server}",
            "",
            "        # Without an imageRef, this throws 500.",
            "        # TODO(justinsb): Check whatever the spec says should be thrown here",
            "        self.assertRaises(client.OpenStackApiException,",
            "                          self.api.post_server, post)",
            "",
            "        # With an invalid imageRef, this throws 500.",
            "        server[self._image_ref_parameter] = self.get_invalid_image()",
            "        # TODO(justinsb): Check whatever the spec says should be thrown here",
            "        self.assertRaises(client.OpenStackApiException,",
            "                          self.api.post_server, post)",
            "",
            "        # Add a valid imageRef",
            "        server[self._image_ref_parameter] = good_server.get(",
            "            self._image_ref_parameter)",
            "",
            "        # Without flavorRef, this throws 500",
            "        # TODO(justinsb): Check whatever the spec says should be thrown here",
            "        self.assertRaises(client.OpenStackApiException,",
            "                          self.api.post_server, post)",
            "",
            "        server[self._flavor_ref_parameter] = good_server.get(",
            "            self._flavor_ref_parameter)",
            "",
            "        # Without a name, this throws 500",
            "        # TODO(justinsb): Check whatever the spec says should be thrown here",
            "        self.assertRaises(client.OpenStackApiException,",
            "                          self.api.post_server, post)",
            "",
            "        # Set a valid server name",
            "        server['name'] = good_server['name']",
            "",
            "        created_server = self.api.post_server(post)",
            "        LOG.debug(\"created_server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "        created_server_id = created_server['id']",
            "",
            "        # Check it's there",
            "        found_server = self.api.get_server(created_server_id)",
            "        self.assertEqual(created_server_id, found_server['id'])",
            "",
            "        # It should also be in the all-servers list",
            "        servers = self.api.get_servers()",
            "        server_ids = [s['id'] for s in servers]",
            "        self.assertIn(created_server_id, server_ids)",
            "",
            "        found_server = self._wait_for_state_change(found_server, 'BUILD')",
            "        # It should be available...",
            "        # TODO(justinsb): Mock doesn't yet do this...",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "        servers = self.api.get_servers(detail=True)",
            "        for server in servers:",
            "            self.assertIn(\"image\", server)",
            "            self.assertIn(\"flavor\", server)",
            "",
            "        self._delete_server(created_server_id)",
            "",
            "    def _force_reclaim(self):",
            "        # Make sure that compute manager thinks the instance is",
            "        # old enough to be expired",
            "        the_past = timeutils.utcnow() + datetime.timedelta(hours=1)",
            "        timeutils.set_time_override(override_time=the_past)",
            "        self.addCleanup(timeutils.clear_time_override)",
            "        ctxt = context.get_admin_context()",
            "        self.compute._reclaim_queued_deletes(ctxt)",
            "",
            "    def test_deferred_delete(self):",
            "        # Creates, deletes and waits for server to be reclaimed.",
            "        self.flags(reclaim_instance_interval=1)",
            "",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "",
            "        created_server = self.api.post_server({'server': server})",
            "        LOG.debug(\"created_server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "        created_server_id = created_server['id']",
            "",
            "        # Wait for it to finish being created",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "",
            "        # It should be available...",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Cannot restore unless instance is deleted",
            "        self.assertRaises(client.OpenStackApiException,",
            "                          self.api.post_server_action, created_server_id,",
            "                          {'restore': {}})",
            "",
            "        # Delete the server",
            "        self.api.delete_server(created_server_id)",
            "",
            "        # Wait for queued deletion",
            "        found_server = self._wait_for_state_change(found_server, 'ACTIVE')",
            "        self.assertEqual('SOFT_DELETED', found_server['status'])",
            "",
            "        self._force_reclaim()",
            "",
            "        # Wait for real deletion",
            "        self._wait_for_deletion(created_server_id)",
            "",
            "    def test_deferred_delete_restore(self):",
            "        # Creates, deletes and restores a server.",
            "        self.flags(reclaim_instance_interval=3600)",
            "",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "",
            "        created_server = self.api.post_server({'server': server})",
            "        LOG.debug(\"created_server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "        created_server_id = created_server['id']",
            "",
            "        # Wait for it to finish being created",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "",
            "        # It should be available...",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Delete the server",
            "        self.api.delete_server(created_server_id)",
            "",
            "        # Wait for queued deletion",
            "        found_server = self._wait_for_state_change(found_server, 'ACTIVE')",
            "        self.assertEqual('SOFT_DELETED', found_server['status'])",
            "",
            "        # Restore server",
            "        self.api.post_server_action(created_server_id, {'restore': {}})",
            "",
            "        # Wait for server to become active again",
            "        found_server = self._wait_for_state_change(found_server, 'DELETED')",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "    def test_deferred_delete_restore_overquota(self):",
            "        # Test that a restore that would put the user over quota fails",
            "        self.flags(instances=1, group='quota')",
            "        # Creates, deletes and restores a server.",
            "        self.flags(reclaim_instance_interval=3600)",
            "",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "",
            "        created_server1 = self.api.post_server({'server': server})",
            "        LOG.debug(\"created_server: %s\", created_server1)",
            "        self.assertTrue(created_server1['id'])",
            "        created_server_id1 = created_server1['id']",
            "",
            "        # Wait for it to finish being created",
            "        found_server1 = self._wait_for_state_change(created_server1, 'BUILD')",
            "",
            "        # It should be available...",
            "        self.assertEqual('ACTIVE', found_server1['status'])",
            "",
            "        # Delete the server",
            "        self.api.delete_server(created_server_id1)",
            "",
            "        # Wait for queued deletion",
            "        found_server1 = self._wait_for_state_change(found_server1, 'ACTIVE')",
            "        self.assertEqual('SOFT_DELETED', found_server1['status'])",
            "",
            "        # Create a second server",
            "        server = self._build_minimal_create_server_request()",
            "",
            "        created_server2 = self.api.post_server({'server': server})",
            "        LOG.debug(\"created_server: %s\", created_server2)",
            "        self.assertTrue(created_server2['id'])",
            "",
            "        # Wait for it to finish being created",
            "        found_server2 = self._wait_for_state_change(created_server2, 'BUILD')",
            "",
            "        # It should be available...",
            "        self.assertEqual('ACTIVE', found_server2['status'])",
            "",
            "        # Try to restore the first server, it should fail",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.post_server_action,",
            "                               created_server_id1, {'restore': {}})",
            "        self.assertEqual(403, ex.response.status_code)",
            "        self.assertEqual('SOFT_DELETED', found_server1['status'])",
            "",
            "    def test_deferred_delete_force(self):",
            "        # Creates, deletes and force deletes a server.",
            "        self.flags(reclaim_instance_interval=3600)",
            "",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "",
            "        created_server = self.api.post_server({'server': server})",
            "        LOG.debug(\"created_server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "        created_server_id = created_server['id']",
            "",
            "        # Wait for it to finish being created",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "",
            "        # It should be available...",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Delete the server",
            "        self.api.delete_server(created_server_id)",
            "",
            "        # Wait for queued deletion",
            "        found_server = self._wait_for_state_change(found_server, 'ACTIVE')",
            "        self.assertEqual('SOFT_DELETED', found_server['status'])",
            "",
            "        # Force delete server",
            "        self.api.post_server_action(created_server_id,",
            "                                    {self._force_delete_parameter: {}})",
            "",
            "        # Wait for real deletion",
            "        self._wait_for_deletion(created_server_id)",
            "",
            "    def test_create_server_with_metadata(self):",
            "        # Creates a server with metadata.",
            "",
            "        # Build the server data gradually, checking errors along the way",
            "        server = self._build_minimal_create_server_request()",
            "",
            "        metadata = {}",
            "        for i in range(30):",
            "            metadata['key_%s' % i] = 'value_%s' % i",
            "",
            "        server['metadata'] = metadata",
            "",
            "        post = {'server': server}",
            "        created_server = self.api.post_server(post)",
            "        LOG.debug(\"created_server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "        created_server_id = created_server['id']",
            "",
            "        found_server = self.api.get_server(created_server_id)",
            "        self.assertEqual(created_server_id, found_server['id'])",
            "        self.assertEqual(metadata, found_server.get('metadata'))",
            "",
            "        # The server should also be in the all-servers details list",
            "        servers = self.api.get_servers(detail=True)",
            "        server_map = {server['id']: server for server in servers}",
            "        found_server = server_map.get(created_server_id)",
            "        self.assertTrue(found_server)",
            "        # Details do include metadata",
            "        self.assertEqual(metadata, found_server.get('metadata'))",
            "",
            "        # The server should also be in the all-servers summary list",
            "        servers = self.api.get_servers(detail=False)",
            "        server_map = {server['id']: server for server in servers}",
            "        found_server = server_map.get(created_server_id)",
            "        self.assertTrue(found_server)",
            "        # Summary should not include metadata",
            "        self.assertFalse(found_server.get('metadata'))",
            "",
            "        # Cleanup",
            "        self._delete_server(created_server_id)",
            "",
            "    def test_server_metadata_actions_negative_invalid_state(self):",
            "        # Create server with metadata",
            "        server = self._build_minimal_create_server_request()",
            "",
            "        metadata = {'key_1': 'value_1'}",
            "",
            "        server['metadata'] = metadata",
            "",
            "        post = {'server': server}",
            "        created_server = self.api.post_server(post)",
            "",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "        self.assertEqual(metadata, found_server.get('metadata'))",
            "        server_id = found_server['id']",
            "",
            "        # Change status from ACTIVE to SHELVED for negative test",
            "        self.flags(shelved_offload_time = -1)",
            "        self.api.post_server_action(server_id, {'shelve': {}})",
            "        found_server = self._wait_for_state_change(found_server, 'ACTIVE')",
            "        self.assertEqual('SHELVED', found_server['status'])",
            "",
            "        metadata = {'key_2': 'value_2'}",
            "",
            "        # Update Metadata item in SHELVED (not ACTIVE, etc.)",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.post_server_metadata,",
            "                               server_id, metadata)",
            "        self.assertEqual(409, ex.response.status_code)",
            "        self.assertEqual('SHELVED', found_server['status'])",
            "",
            "        # Delete Metadata item in SHELVED (not ACTIVE, etc.)",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.delete_server_metadata,",
            "                               server_id, 'key_1')",
            "        self.assertEqual(409, ex.response.status_code)",
            "        self.assertEqual('SHELVED', found_server['status'])",
            "",
            "        # Cleanup",
            "        self._delete_server(server_id)",
            "",
            "    def test_create_and_rebuild_server(self):",
            "        # Rebuild a server with metadata.",
            "",
            "        # create a server with initially has no metadata",
            "        server = self._build_minimal_create_server_request()",
            "        server_post = {'server': server}",
            "",
            "        metadata = {}",
            "        for i in range(30):",
            "            metadata['key_%s' % i] = 'value_%s' % i",
            "",
            "        server_post['server']['metadata'] = metadata",
            "",
            "        created_server = self.api.post_server(server_post)",
            "        LOG.debug(\"created_server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "        created_server_id = created_server['id']",
            "",
            "        created_server = self._wait_for_state_change(created_server, 'BUILD')",
            "",
            "        # rebuild the server with metadata and other server attributes",
            "        post = {}",
            "        post['rebuild'] = {",
            "            self._image_ref_parameter: \"76fa36fc-c930-4bf3-8c8a-ea2a2420deb6\",",
            "            \"name\": \"blah\",",
            "            self._access_ipv4_parameter: \"172.19.0.2\",",
            "            self._access_ipv6_parameter: \"fe80::2\",",
            "            \"metadata\": {'some': 'thing'},",
            "        }",
            "        post['rebuild'].update(self._get_access_ips_params())",
            "",
            "        self.api.post_server_action(created_server_id, post)",
            "        LOG.debug(\"rebuilt server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "",
            "        found_server = self.api.get_server(created_server_id)",
            "        self.assertEqual(created_server_id, found_server['id'])",
            "        self.assertEqual({'some': 'thing'}, found_server.get('metadata'))",
            "        self.assertEqual('blah', found_server.get('name'))",
            "        self.assertEqual(post['rebuild'][self._image_ref_parameter],",
            "                         found_server.get('image')['id'])",
            "        self._verify_access_ips(found_server)",
            "",
            "        # rebuild the server with empty metadata and nothing else",
            "        post = {}",
            "        post['rebuild'] = {",
            "            self._image_ref_parameter: \"76fa36fc-c930-4bf3-8c8a-ea2a2420deb6\",",
            "            \"metadata\": {},",
            "        }",
            "",
            "        self.api.post_server_action(created_server_id, post)",
            "        LOG.debug(\"rebuilt server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "",
            "        found_server = self.api.get_server(created_server_id)",
            "        self.assertEqual(created_server_id, found_server['id'])",
            "        self.assertEqual({}, found_server.get('metadata'))",
            "        self.assertEqual('blah', found_server.get('name'))",
            "        self.assertEqual(post['rebuild'][self._image_ref_parameter],",
            "                         found_server.get('image')['id'])",
            "        self._verify_access_ips(found_server)",
            "",
            "        # Cleanup",
            "        self._delete_server(created_server_id)",
            "",
            "    def test_rename_server(self):",
            "        # Test building and renaming a server.",
            "",
            "        # Create a server",
            "        server = self._build_minimal_create_server_request()",
            "        created_server = self.api.post_server({'server': server})",
            "        LOG.debug(\"created_server: %s\", created_server)",
            "        server_id = created_server['id']",
            "        self.assertTrue(server_id)",
            "",
            "        # Rename the server to 'new-name'",
            "        self.api.put_server(server_id, {'server': {'name': 'new-name'}})",
            "",
            "        # Check the name of the server",
            "        created_server = self.api.get_server(server_id)",
            "        self.assertEqual(created_server['name'], 'new-name')",
            "",
            "        # Cleanup",
            "        self._delete_server(server_id)",
            "",
            "    def test_create_multiple_servers(self):",
            "        # Creates multiple servers and checks for reservation_id.",
            "",
            "        # Create 2 servers, setting 'return_reservation_id, which should",
            "        # return a reservation_id",
            "        server = self._build_minimal_create_server_request()",
            "        server[self._min_count_parameter] = 2",
            "        server[self._return_resv_id_parameter] = True",
            "        post = {'server': server}",
            "        response = self.api.post_server(post)",
            "        self.assertIn('reservation_id', response)",
            "        reservation_id = response['reservation_id']",
            "        self.assertNotIn(reservation_id, ['', None])",
            "",
            "        # Create 1 more server, which should not return a reservation_id",
            "        server = self._build_minimal_create_server_request()",
            "        post = {'server': server}",
            "        created_server = self.api.post_server(post)",
            "        self.assertTrue(created_server['id'])",
            "        created_server_id = created_server['id']",
            "",
            "        # lookup servers created by the first request.",
            "        servers = self.api.get_servers(detail=True,",
            "                search_opts={'reservation_id': reservation_id})",
            "        server_map = {server['id']: server for server in servers}",
            "        found_server = server_map.get(created_server_id)",
            "        # The server from the 2nd request should not be there.",
            "        self.assertIsNone(found_server)",
            "        # Should have found 2 servers.",
            "        self.assertEqual(len(server_map), 2)",
            "",
            "        # Cleanup",
            "        self._delete_server(created_server_id)",
            "        for server_id in server_map:",
            "            self._delete_server(server_id)",
            "",
            "    def test_create_server_with_injected_files(self):",
            "        # Creates a server with injected_files.",
            "        personality = []",
            "",
            "        # Inject a text file",
            "        data = 'Hello, World!'",
            "        personality.append({",
            "            'path': '/helloworld.txt',",
            "            'contents': base64.encode_as_bytes(data),",
            "        })",
            "",
            "        # Inject a binary file",
            "        data = zlib.compress(b'Hello, World!')",
            "        personality.append({",
            "            'path': '/helloworld.zip',",
            "            'contents': base64.encode_as_bytes(data),",
            "        })",
            "",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "        server['personality'] = personality",
            "",
            "        post = {'server': server}",
            "",
            "        created_server = self.api.post_server(post)",
            "        LOG.debug(\"created_server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "        created_server_id = created_server['id']",
            "",
            "        # Check it's there",
            "        found_server = self.api.get_server(created_server_id)",
            "        self.assertEqual(created_server_id, found_server['id'])",
            "",
            "        found_server = self._wait_for_state_change(found_server, 'BUILD')",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Cleanup",
            "        self._delete_server(created_server_id)",
            "",
            "    def test_stop_start_servers_negative_invalid_state(self):",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "        created_server = self.api.post_server({\"server\": server})",
            "        created_server_id = created_server['id']",
            "",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Start server in ACTIVE",
            "        # NOTE(mkoshiya): When os-start API runs, the server status",
            "        # must be SHUTOFF.",
            "        # By returning 409, I want to confirm that the ACTIVE server does not",
            "        # cause unexpected behavior.",
            "        post = {'os-start': {}}",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.post_server_action,",
            "                               created_server_id, post)",
            "        self.assertEqual(409, ex.response.status_code)",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Stop server",
            "        post = {'os-stop': {}}",
            "        self.api.post_server_action(created_server_id, post)",
            "        found_server = self._wait_for_state_change(found_server, 'ACTIVE')",
            "        self.assertEqual('SHUTOFF', found_server['status'])",
            "",
            "        # Stop server in SHUTOFF",
            "        # NOTE(mkoshiya): When os-stop API runs, the server status",
            "        # must be ACTIVE or ERROR.",
            "        # By returning 409, I want to confirm that the SHUTOFF server does not",
            "        # cause unexpected behavior.",
            "        post = {'os-stop': {}}",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.post_server_action,",
            "                               created_server_id, post)",
            "        self.assertEqual(409, ex.response.status_code)",
            "        self.assertEqual('SHUTOFF', found_server['status'])",
            "",
            "        # Cleanup",
            "        self._delete_server(created_server_id)",
            "",
            "    def test_revert_resized_server_negative_invalid_state(self):",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "        created_server = self.api.post_server({\"server\": server})",
            "        created_server_id = created_server['id']",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Revert resized server in ACTIVE",
            "        # NOTE(yatsumi): When revert resized server API runs,",
            "        # the server status must be VERIFY_RESIZE.",
            "        # By returning 409, I want to confirm that the ACTIVE server does not",
            "        # cause unexpected behavior.",
            "        post = {'revertResize': {}}",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.post_server_action,",
            "                               created_server_id, post)",
            "        self.assertEqual(409, ex.response.status_code)",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Cleanup",
            "        self._delete_server(created_server_id)",
            "",
            "    def test_resize_server_negative_invalid_state(self):",
            "        # Avoid migration",
            "        self.flags(allow_resize_to_same_host=True)",
            "",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "        created_server = self.api.post_server({\"server\": server})",
            "        created_server_id = created_server['id']",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Resize server(flavorRef: 1 -> 2)",
            "        post = {'resize': {\"flavorRef\": \"2\", \"OS-DCF:diskConfig\": \"AUTO\"}}",
            "        self.api.post_server_action(created_server_id, post)",
            "        found_server = self._wait_for_state_change(found_server, 'RESIZE')",
            "        self.assertEqual('VERIFY_RESIZE', found_server['status'])",
            "",
            "        # Resize server in VERIFY_RESIZE(flavorRef: 2 -> 1)",
            "        # NOTE(yatsumi): When resize API runs, the server status",
            "        # must be ACTIVE or SHUTOFF.",
            "        # By returning 409, I want to confirm that the VERIFY_RESIZE server",
            "        # does not cause unexpected behavior.",
            "        post = {'resize': {\"flavorRef\": \"1\", \"OS-DCF:diskConfig\": \"AUTO\"}}",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.post_server_action,",
            "                               created_server_id, post)",
            "        self.assertEqual(409, ex.response.status_code)",
            "        self.assertEqual('VERIFY_RESIZE', found_server['status'])",
            "",
            "        # Cleanup",
            "        self._delete_server(created_server_id)",
            "",
            "    def test_confirm_resized_server_negative_invalid_state(self):",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "        created_server = self.api.post_server({\"server\": server})",
            "        created_server_id = created_server['id']",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Confirm resized server in ACTIVE",
            "        # NOTE(yatsumi): When confirm resized server API runs,",
            "        # the server status must be VERIFY_RESIZE.",
            "        # By returning 409, I want to confirm that the ACTIVE server does not",
            "        # cause unexpected behavior.",
            "        post = {'confirmResize': {}}",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.post_server_action,",
            "                               created_server_id, post)",
            "        self.assertEqual(409, ex.response.status_code)",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Cleanup",
            "        self._delete_server(created_server_id)",
            "",
            "    def test_resize_server_overquota(self):",
            "        self.flags(cores=1, group='quota')",
            "        self.flags(ram=512, group='quota')",
            "        # Create server with default flavor, 1 core, 512 ram",
            "        server = self._build_minimal_create_server_request()",
            "        created_server = self.api.post_server({\"server\": server})",
            "        created_server_id = created_server['id']",
            "",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Try to resize to flavorid 2, 1 core, 2048 ram",
            "        post = {'resize': {'flavorRef': '2'}}",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.post_server_action,",
            "                               created_server_id, post)",
            "        self.assertEqual(403, ex.response.status_code)",
            "",
            "",
            "class ServersTestV21(ServersTest):",
            "    api_major_version = 'v2.1'",
            "",
            "",
            "class ServersTestV219(ServersTestBase):",
            "    api_major_version = 'v2.1'",
            "",
            "    def _create_server(self, set_desc = True, desc = None):",
            "        server = self._build_minimal_create_server_request()",
            "        if set_desc:",
            "            server['description'] = desc",
            "        post = {'server': server}",
            "        response = self.api.api_post('/servers', post).body",
            "        return (server, response['server'])",
            "",
            "    def _update_server(self, server_id, set_desc = True, desc = None):",
            "        new_name = integrated_helpers.generate_random_alphanumeric(8)",
            "        server = {'server': {'name': new_name}}",
            "        if set_desc:",
            "            server['server']['description'] = desc",
            "        self.api.api_put('/servers/%s' % server_id, server)",
            "",
            "    def _rebuild_server(self, server_id, set_desc = True, desc = None):",
            "        new_name = integrated_helpers.generate_random_alphanumeric(8)",
            "        post = {}",
            "        post['rebuild'] = {",
            "            \"name\": new_name,",
            "            self._image_ref_parameter: \"76fa36fc-c930-4bf3-8c8a-ea2a2420deb6\",",
            "            self._access_ipv4_parameter: \"172.19.0.2\",",
            "            self._access_ipv6_parameter: \"fe80::2\",",
            "            \"metadata\": {'some': 'thing'},",
            "        }",
            "        post['rebuild'].update(self._get_access_ips_params())",
            "        if set_desc:",
            "            post['rebuild']['description'] = desc",
            "        self.api.api_post('/servers/%s/action' % server_id, post)",
            "",
            "    def _create_server_and_verify(self, set_desc = True, expected_desc = None):",
            "        # Creates a server with a description and verifies it is",
            "        # in the GET responses.",
            "        created_server_id = self._create_server(set_desc,",
            "                                                expected_desc)[1]['id']",
            "        self._verify_server_description(created_server_id, expected_desc)",
            "        self._delete_server(created_server_id)",
            "",
            "    def _update_server_and_verify(self, server_id, set_desc = True,",
            "                                  expected_desc = None):",
            "        # Updates a server with a description and verifies it is",
            "        # in the GET responses.",
            "        self._update_server(server_id, set_desc, expected_desc)",
            "        self._verify_server_description(server_id, expected_desc)",
            "",
            "    def _rebuild_server_and_verify(self, server_id, set_desc = True,",
            "                                  expected_desc = None):",
            "        # Rebuilds a server with a description and verifies it is",
            "        # in the GET responses.",
            "        self._rebuild_server(server_id, set_desc, expected_desc)",
            "        self._verify_server_description(server_id, expected_desc)",
            "",
            "    def _verify_server_description(self, server_id, expected_desc = None,",
            "                                   desc_in_resp = True):",
            "        # Calls GET on the servers and verifies that the description",
            "        # is set as expected in the response, or not set at all.",
            "        response = self.api.api_get('/servers/%s' % server_id)",
            "        found_server = response.body['server']",
            "        self.assertEqual(server_id, found_server['id'])",
            "        if desc_in_resp:",
            "            # Verify the description is set as expected (can be None)",
            "            self.assertEqual(expected_desc, found_server.get('description'))",
            "        else:",
            "            # Verify the description is not included in the response.",
            "            self.assertNotIn('description', found_server)",
            "",
            "        servers = self.api.api_get('/servers/detail').body['servers']",
            "        server_map = {server['id']: server for server in servers}",
            "        found_server = server_map.get(server_id)",
            "        self.assertTrue(found_server)",
            "        if desc_in_resp:",
            "            # Verify the description is set as expected (can be None)",
            "            self.assertEqual(expected_desc, found_server.get('description'))",
            "        else:",
            "            # Verify the description is not included in the response.",
            "            self.assertNotIn('description', found_server)",
            "",
            "    def _create_assertRaisesRegex(self, desc):",
            "        # Verifies that a 400 error is thrown on create server",
            "        with self.assertRaisesRegex(client.OpenStackApiException,",
            "                                    \".*Unexpected status code.*\") as cm:",
            "            self._create_server(True, desc)",
            "            self.assertEqual(400, cm.exception.response.status_code)",
            "",
            "    def _update_assertRaisesRegex(self, server_id, desc):",
            "        # Verifies that a 400 error is thrown on update server",
            "        with self.assertRaisesRegex(client.OpenStackApiException,",
            "                                    \".*Unexpected status code.*\") as cm:",
            "            self._update_server(server_id, True, desc)",
            "            self.assertEqual(400, cm.exception.response.status_code)",
            "",
            "    def _rebuild_assertRaisesRegex(self, server_id, desc):",
            "        # Verifies that a 400 error is thrown on rebuild server",
            "        with self.assertRaisesRegex(client.OpenStackApiException,",
            "                                    \".*Unexpected status code.*\") as cm:",
            "            self._rebuild_server(server_id, True, desc)",
            "            self.assertEqual(400, cm.exception.response.status_code)",
            "",
            "    def test_create_server_with_description(self):",
            "        self.api.microversion = '2.19'",
            "        # Create and get a server with a description",
            "        self._create_server_and_verify(True, 'test description')",
            "        # Create and get a server with an empty description",
            "        self._create_server_and_verify(True, '')",
            "        # Create and get a server with description set to None",
            "        self._create_server_and_verify()",
            "        # Create and get a server without setting the description",
            "        self._create_server_and_verify(False)",
            "",
            "    def test_update_server_with_description(self):",
            "        self.api.microversion = '2.19'",
            "        # Create a server with an initial description",
            "        server_id = self._create_server(True, 'test desc 1')[1]['id']",
            "",
            "        # Update and get the server with a description",
            "        self._update_server_and_verify(server_id, True, 'updated desc')",
            "        # Update and get the server name without changing the description",
            "        self._update_server_and_verify(server_id, False, 'updated desc')",
            "        # Update and get the server with an empty description",
            "        self._update_server_and_verify(server_id, True, '')",
            "        # Update and get the server by removing the description (set to None)",
            "        self._update_server_and_verify(server_id)",
            "        # Update and get the server with a 2nd new description",
            "        self._update_server_and_verify(server_id, True, 'updated desc2')",
            "",
            "        # Cleanup",
            "        self._delete_server(server_id)",
            "",
            "    def test_rebuild_server_with_description(self):",
            "        self.api.microversion = '2.19'",
            "",
            "        # Create a server with an initial description",
            "        server = self._create_server(True, 'test desc 1')[1]",
            "        server_id = server['id']",
            "        self._wait_for_state_change(server, 'BUILD')",
            "",
            "        # Rebuild and get the server with a description",
            "        self._rebuild_server_and_verify(server_id, True, 'updated desc')",
            "        # Rebuild and get the server name without changing the description",
            "        self._rebuild_server_and_verify(server_id, False, 'updated desc')",
            "        # Rebuild and get the server with an empty description",
            "        self._rebuild_server_and_verify(server_id, True, '')",
            "        # Rebuild and get the server by removing the description (set to None)",
            "        self._rebuild_server_and_verify(server_id)",
            "        # Rebuild and get the server with a 2nd new description",
            "        self._rebuild_server_and_verify(server_id, True, 'updated desc2')",
            "",
            "        # Cleanup",
            "        self._delete_server(server_id)",
            "",
            "    def test_version_compatibility(self):",
            "        # Create a server with microversion v2.19 and a description.",
            "        self.api.microversion = '2.19'",
            "        server_id = self._create_server(True, 'test desc 1')[1]['id']",
            "        # Verify that the description is not included on V2.18 GETs",
            "        self.api.microversion = '2.18'",
            "        self._verify_server_description(server_id, desc_in_resp = False)",
            "        # Verify that updating the server with description on V2.18",
            "        # results in a 400 error",
            "        self._update_assertRaisesRegex(server_id, 'test update 2.18')",
            "        # Verify that rebuilding the server with description on V2.18",
            "        # results in a 400 error",
            "        self._rebuild_assertRaisesRegex(server_id, 'test rebuild 2.18')",
            "",
            "        # Cleanup",
            "        self._delete_server(server_id)",
            "",
            "        # Create a server on V2.18 and verify that the description",
            "        # defaults to the name on a V2.19 GET",
            "        server_req, response = self._create_server(False)",
            "        server_id = response['id']",
            "        self.api.microversion = '2.19'",
            "        self._verify_server_description(server_id, server_req['name'])",
            "",
            "        # Cleanup",
            "        self._delete_server(server_id)",
            "",
            "        # Verify that creating a server with description on V2.18",
            "        # results in a 400 error",
            "        self.api.microversion = '2.18'",
            "        self._create_assertRaisesRegex('test create 2.18')",
            "",
            "    def test_description_errors(self):",
            "        self.api.microversion = '2.19'",
            "        # Create servers with invalid descriptions.  These throw 400.",
            "        # Invalid unicode with non-printable control char",
            "        self._create_assertRaisesRegex(u'invalid\\0dstring')",
            "        # Description is longer than 255 chars",
            "        self._create_assertRaisesRegex('x' * 256)",
            "",
            "        # Update and rebuild servers with invalid descriptions.",
            "        # These throw 400.",
            "        server_id = self._create_server(True, \"desc\")[1]['id']",
            "        # Invalid unicode with non-printable control char",
            "        self._update_assertRaisesRegex(server_id, u'invalid\\u0604string')",
            "        self._rebuild_assertRaisesRegex(server_id, u'invalid\\u0604string')",
            "        # Description is longer than 255 chars",
            "        self._update_assertRaisesRegex(server_id, 'x' * 256)",
            "        self._rebuild_assertRaisesRegex(server_id, 'x' * 256)",
            "",
            "",
            "class ServerTestV220(ServersTestBase):",
            "    api_major_version = 'v2.1'",
            "",
            "    def setUp(self):",
            "        super(ServerTestV220, self).setUp()",
            "        self.api.microversion = '2.20'",
            "        fake_network.set_stub_network_methods(self)",
            "        self.ctxt = context.get_admin_context()",
            "",
            "    def _create_server(self):",
            "        server = self._build_minimal_create_server_request()",
            "        post = {'server': server}",
            "        response = self.api.api_post('/servers', post).body",
            "        return (server, response['server'])",
            "",
            "    def _shelve_server(self):",
            "        server = self._create_server()[1]",
            "        server_id = server['id']",
            "        self._wait_for_state_change(server, 'BUILD')",
            "        self.api.post_server_action(server_id, {'shelve': None})",
            "        return self._wait_for_state_change(server, 'ACTIVE')",
            "",
            "    def _get_fake_bdms(self, ctxt):",
            "        return block_device_obj.block_device_make_list(self.ctxt,",
            "                    [fake_block_device.FakeDbBlockDeviceDict(",
            "                    {'device_name': '/dev/vda',",
            "                     'source_type': 'volume',",
            "                     'destination_type': 'volume',",
            "                     'volume_id': '5d721593-f033-4f6d-ab6f-b5b067e61bc4'})])",
            "",
            "    def test_attach_detach_vol_to_shelved_server(self):",
            "        self.flags(shelved_offload_time=-1)",
            "        found_server = self._shelve_server()",
            "        self.assertEqual('SHELVED', found_server['status'])",
            "        server_id = found_server['id']",
            "",
            "        # Test attach volume",
            "        with test.nested(mock.patch.object(compute_api.API,",
            "                                       '_check_attach_and_reserve_volume'),",
            "                         mock.patch.object(rpcapi.ComputeAPI,",
            "                                       'attach_volume')) as (mock_reserve,",
            "                                                             mock_attach):",
            "            volume_attachment = {\"volumeAttachment\": {\"volumeId\":",
            "                                       \"5d721593-f033-4f6d-ab6f-b5b067e61bc4\"}}",
            "            self.api.api_post(",
            "                            '/servers/%s/os-volume_attachments' % (server_id),",
            "                            volume_attachment)",
            "            self.assertTrue(mock_reserve.called)",
            "            self.assertTrue(mock_attach.called)",
            "",
            "        # Test detach volume",
            "        self.stub_out('nova.volume.cinder.API.get', fakes.stub_volume_get)",
            "        with test.nested(mock.patch.object(volume.cinder.API,",
            "                                           'begin_detaching'),",
            "                         mock.patch.object(objects.BlockDeviceMappingList,",
            "                                           'get_by_instance_uuid'),",
            "                         mock.patch.object(rpcapi.ComputeAPI,",
            "                                           'detach_volume')",
            "                         ) as (mock_check, mock_get_bdms, mock_rpc):",
            "",
            "            mock_get_bdms.return_value = self._get_fake_bdms(self.ctxt)",
            "            attachment_id = mock_get_bdms.return_value[0]['volume_id']",
            "",
            "            self.api.api_delete('/servers/%s/os-volume_attachments/%s' %",
            "                            (server_id, attachment_id))",
            "            self.assertTrue(mock_check.called)",
            "            self.assertTrue(mock_rpc.called)",
            "",
            "        self._delete_server(server_id)",
            "",
            "    def test_attach_detach_vol_to_shelved_offloaded_server(self):",
            "        self.flags(shelved_offload_time=0)",
            "        found_server = self._shelve_server()",
            "        self.assertEqual('SHELVED_OFFLOADED', found_server['status'])",
            "        server_id = found_server['id']",
            "",
            "        # Test attach volume",
            "        with test.nested(mock.patch.object(compute_api.API,",
            "                                       '_check_attach_and_reserve_volume'),",
            "                         mock.patch.object(volume.cinder.API,",
            "                                       'attach')) as (mock_reserve, mock_vol):",
            "            volume_attachment = {\"volumeAttachment\": {\"volumeId\":",
            "                                       \"5d721593-f033-4f6d-ab6f-b5b067e61bc4\"}}",
            "            attach_response = self.api.api_post(",
            "                             '/servers/%s/os-volume_attachments' % (server_id),",
            "                             volume_attachment).body['volumeAttachment']",
            "            self.assertTrue(mock_reserve.called)",
            "            self.assertTrue(mock_vol.called)",
            "            self.assertIsNone(attach_response['device'])",
            "",
            "        # Test detach volume",
            "        self.stub_out('nova.volume.cinder.API.get', fakes.stub_volume_get)",
            "        with test.nested(mock.patch.object(volume.cinder.API,",
            "                                           'begin_detaching'),",
            "                         mock.patch.object(objects.BlockDeviceMappingList,",
            "                                           'get_by_instance_uuid'),",
            "                         mock.patch.object(compute_api.API,",
            "                                           '_local_cleanup_bdm_volumes')",
            "                         ) as (mock_check, mock_get_bdms, mock_clean_vols):",
            "",
            "            mock_get_bdms.return_value = self._get_fake_bdms(self.ctxt)",
            "            attachment_id = mock_get_bdms.return_value[0]['volume_id']",
            "            self.api.api_delete('/servers/%s/os-volume_attachments/%s' %",
            "                            (server_id, attachment_id))",
            "            self.assertTrue(mock_check.called)",
            "            self.assertTrue(mock_clean_vols.called)",
            "",
            "        self._delete_server(server_id)",
            "",
            "",
            "class ServerRebuildTestCase(integrated_helpers._IntegratedTestBase,",
            "                            integrated_helpers.InstanceHelperMixin):",
            "    api_major_version = 'v2.1'",
            "    # We have to cap the microversion at 2.38 because that's the max we",
            "    # can use to update image metadata via our compute images proxy API.",
            "    microversion = '2.38'",
            "",
            "    def _disable_compute_for(self, server):",
            "        # Refresh to get its host",
            "        server = self.api.get_server(server['id'])",
            "        host = server['OS-EXT-SRV-ATTR:host']",
            "",
            "        # Disable the service it is on",
            "        self.api_fixture.admin_api.put_service('disable',",
            "                                               {'host': host,",
            "                                                'binary': 'nova-compute'})",
            "",
            "    def test_rebuild_with_image_novalidhost(self):",
            "        \"\"\"Creates a server with an image that is valid for the single compute",
            "        that we have. Then rebuilds the server, passing in an image with",
            "        metadata that does not fit the single compute which should result in",
            "        a NoValidHost error. The ImagePropertiesFilter filter is enabled by",
            "        default so that should filter out the host based on the image meta.",
            "        \"\"\"",
            "",
            "        fake.set_nodes(['host2'])",
            "        self.addCleanup(fake.restore_nodes)",
            "        self.flags(host='host2')",
            "        self.compute2 = self.start_service('compute', host='host2')",
            "",
            "        server_req_body = {",
            "            'server': {",
            "                # We hard-code from a fake image since we can't get images",
            "                # via the compute /images proxy API with microversion > 2.35.",
            "                'imageRef': '155d900f-4e14-4e4c-a73d-069cbf4541e6',",
            "                'flavorRef': '1',   # m1.tiny from DefaultFlavorsFixture,",
            "                'name': 'test_rebuild_with_image_novalidhost',",
            "                # We don't care about networking for this test. This requires",
            "                # microversion >= 2.37.",
            "                'networks': 'none'",
            "            }",
            "        }",
            "        server = self.api.post_server(server_req_body)",
            "        self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        # Disable the host we're on so ComputeFilter would have ruled it out",
            "        # normally",
            "        self._disable_compute_for(server)",
            "",
            "        # Now update the image metadata to be something that won't work with",
            "        # the fake compute driver we're using since the fake driver has an",
            "        # \"x86_64\" architecture.",
            "        rebuild_image_ref = (",
            "            nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)",
            "        self.api.put_image_meta_key(",
            "            rebuild_image_ref, 'hw_architecture', 'unicore32')",
            "        # Now rebuild the server with that updated image and it should result",
            "        # in a NoValidHost failure from the scheduler.",
            "        rebuild_req_body = {",
            "            'rebuild': {",
            "                'imageRef': rebuild_image_ref",
            "            }",
            "        }",
            "        # Since we're using the CastAsCall fixture, the NoValidHost error",
            "        # should actually come back to the API and result in a 500 error.",
            "        # Normally the user would get a 202 response because nova-api RPC casts",
            "        # to nova-conductor which RPC calls the scheduler which raises the",
            "        # NoValidHost. We can mimic the end user way to figure out the failure",
            "        # by looking for the failed 'rebuild' instance action event.",
            "        self.api.api_post('/servers/%s/action' % server['id'],",
            "                          rebuild_req_body, check_response_status=[500])",
            "        # Look for the failed rebuild action.",
            "        self._wait_for_action_fail_completion(",
            "            server, instance_actions.REBUILD, 'rebuild_server',",
            "            # Before microversion 2.51 events are only returned for instance",
            "            # actions if you're an admin.",
            "            self.api_fixture.admin_api)",
            "        # Unfortunately the server's image_ref is updated to be the new image",
            "        # even though the rebuild should not work.",
            "        server = self.api.get_server(server['id'])",
            "        self.assertEqual(rebuild_image_ref, server['image']['id'])",
            "",
            "    def test_rebuild_with_new_image(self):",
            "        \"\"\"Rebuilds a server with a different image which will run it through",
            "        the scheduler to validate the image is still OK with the compute host",
            "        that the instance is running on.",
            "",
            "        Validates that additional resources are not allocated against the",
            "        instance.host in Placement due to the rebuild on same host.",
            "        \"\"\"",
            "        admin_api = self.api_fixture.admin_api",
            "        admin_api.microversion = '2.53'",
            "",
            "        def _get_provider_uuid_by_host(host):",
            "            resp = admin_api.api_get(",
            "                'os-hypervisors?hypervisor_hostname_pattern=%s' % host).body",
            "            return resp['hypervisors'][0]['id']",
            "",
            "        def _get_provider_usages(provider_uuid):",
            "            return self.placement_api.get(",
            "                '/resource_providers/%s/usages' % provider_uuid).body['usages']",
            "",
            "        def _get_allocations_by_server_uuid(server_uuid):",
            "            return self.placement_api.get(",
            "                '/allocations/%s' % server_uuid).body['allocations']",
            "",
            "        def assertFlavorMatchesAllocation(flavor, allocation):",
            "            self.assertEqual(flavor['vcpus'], allocation['VCPU'])",
            "            self.assertEqual(flavor['ram'], allocation['MEMORY_MB'])",
            "            self.assertEqual(flavor['disk'], allocation['DISK_GB'])",
            "",
            "        def assertFlavorsMatchAllocation(old_flavor, new_flavor,",
            "                                         allocation):",
            "            self.assertEqual(old_flavor['vcpus'] + new_flavor['vcpus'],",
            "                             allocation['VCPU'])",
            "            self.assertEqual(old_flavor['ram'] + new_flavor['ram'],",
            "                             allocation['MEMORY_MB'])",
            "            self.assertEqual(old_flavor['disk'] + new_flavor['disk'],",
            "                             allocation['DISK_GB'])",
            "",
            "        nodename = self.compute.manager._get_nodename(None)",
            "        rp_uuid = _get_provider_uuid_by_host(nodename)",
            "        # make sure we start with no usage on the compute node",
            "        rp_usages = _get_provider_usages(rp_uuid)",
            "        self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)",
            "",
            "        server_req_body = {",
            "            'server': {",
            "                # We hard-code from a fake image since we can't get images",
            "                # via the compute /images proxy API with microversion > 2.35.",
            "                'imageRef': '155d900f-4e14-4e4c-a73d-069cbf4541e6',",
            "                'flavorRef': '1',   # m1.tiny from DefaultFlavorsFixture,",
            "                'name': 'test_rebuild_with_new_image',",
            "                # We don't care about networking for this test. This requires",
            "                # microversion >= 2.37.",
            "                'networks': 'none'",
            "            }",
            "        }",
            "        server = self.api.post_server(server_req_body)",
            "        self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        flavor = self.api.api_get('/flavors/1').body['flavor']",
            "",
            "        # There should be usage for the server on the compute node now.",
            "        rp_usages = _get_provider_usages(rp_uuid)",
            "        assertFlavorMatchesAllocation(flavor, rp_usages)",
            "        allocs = _get_allocations_by_server_uuid(server['id'])",
            "        self.assertIn(rp_uuid, allocs)",
            "        allocs = allocs[rp_uuid]['resources']",
            "        assertFlavorMatchesAllocation(flavor, allocs)",
            "",
            "        rebuild_image_ref = (",
            "            nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)",
            "        # Now rebuild the server with a different image.",
            "        rebuild_req_body = {",
            "            'rebuild': {",
            "                'imageRef': rebuild_image_ref",
            "            }",
            "        }",
            "        self.api.api_post('/servers/%s/action' % server['id'],",
            "                          rebuild_req_body)",
            "        self._wait_for_server_parameter(",
            "            self.api, server, {'OS-EXT-STS:task_state': None})",
            "",
            "        # The usage and allocations should not have changed.",
            "        rp_usages = _get_provider_usages(rp_uuid)",
            "        # FIXME(mriedem): This is a bug where the scheduler doubled up the",
            "        # allocations for the instance even though we're just rebuilding",
            "        # to the same host. Uncomment this once fixed.",
            "        # assertFlavorMatchesAllocation(flavor, rp_usages)",
            "        assertFlavorsMatchAllocation(flavor, flavor, rp_usages)",
            "        allocs = _get_allocations_by_server_uuid(server['id'])",
            "        self.assertIn(rp_uuid, allocs)",
            "        allocs = allocs[rp_uuid]['resources']",
            "        # assertFlavorMatchesAllocation(flavor, allocs)",
            "        assertFlavorsMatchAllocation(flavor, flavor, allocs)",
            "",
            "",
            "class ProviderUsageBaseTestCase(test.TestCase,",
            "                                integrated_helpers.InstanceHelperMixin):",
            "    \"\"\"Base test class for functional tests that check provider usage",
            "    and consumer allocations in Placement during various operations.",
            "",
            "    Subclasses must define a **compute_driver** attribute for the virt driver",
            "    to use.",
            "",
            "    This class sets up standard fixtures and controller services but does not",
            "    start any compute services, that is left to the subclass.",
            "    \"\"\"",
            "",
            "    microversion = 'latest'",
            "",
            "    def setUp(self):",
            "        self.flags(compute_driver=self.compute_driver)",
            "        super(ProviderUsageBaseTestCase, self).setUp()",
            "",
            "        self.useFixture(policy_fixture.RealPolicyFixture())",
            "        self.useFixture(nova_fixtures.NeutronFixture(self))",
            "        self.useFixture(nova_fixtures.AllServicesCurrent())",
            "",
            "        placement = self.useFixture(nova_fixtures.PlacementFixture())",
            "        self.placement_api = placement.api",
            "        api_fixture = self.useFixture(nova_fixtures.OSAPIFixture(",
            "            api_version='v2.1'))",
            "",
            "        self.admin_api = api_fixture.admin_api",
            "        self.admin_api.microversion = self.microversion",
            "        self.api = self.admin_api",
            "",
            "        # the image fake backend needed for image discovery",
            "        nova.tests.unit.image.fake.stub_out_image_service(self)",
            "",
            "        self.start_service('conductor')",
            "        self.start_service('scheduler')",
            "",
            "        self.addCleanup(nova.tests.unit.image.fake.FakeImageService_reset)",
            "        fake_network.set_stub_network_methods(self)",
            "",
            "        self.computes = {}",
            "",
            "    def _start_compute(self, host):",
            "        \"\"\"Start a nova compute service on the given host",
            "",
            "        :param host: the name of the host that will be associated to the",
            "                     compute service.",
            "        :return: the nova compute service object",
            "        \"\"\"",
            "        fake.set_nodes([host])",
            "        self.addCleanup(fake.restore_nodes)",
            "        self.flags(host=host)",
            "        compute = self.start_service('compute', host=host)",
            "        self.computes[host] = compute",
            "        return compute",
            "",
            "    def _get_provider_uuid_by_host(self, host):",
            "        # NOTE(gibi): the compute node id is the same as the compute node",
            "        # provider uuid on that compute",
            "        resp = self.admin_api.api_get(",
            "            'os-hypervisors?hypervisor_hostname_pattern=%s' % host).body",
            "        return resp['hypervisors'][0]['id']",
            "",
            "    def _get_provider_usages(self, provider_uuid):",
            "        return self.placement_api.get(",
            "            '/resource_providers/%s/usages' % provider_uuid).body['usages']",
            "",
            "    def _get_allocations_by_server_uuid(self, server_uuid):",
            "        return self.placement_api.get(",
            "            '/allocations/%s' % server_uuid).body['allocations']",
            "",
            "    def assertFlavorMatchesAllocation(self, flavor, allocation):",
            "        self.assertEqual(flavor['vcpus'], allocation['VCPU'])",
            "        self.assertEqual(flavor['ram'], allocation['MEMORY_MB'])",
            "        self.assertEqual(flavor['disk'], allocation['DISK_GB'])",
            "",
            "    def assertFlavorsMatchAllocation(self, old_flavor, new_flavor, allocation):",
            "        self.assertEqual(old_flavor['vcpus'] + new_flavor['vcpus'],",
            "                         allocation['VCPU'])",
            "        self.assertEqual(old_flavor['ram'] + new_flavor['ram'],",
            "                         allocation['MEMORY_MB'])",
            "        self.assertEqual(old_flavor['disk'] + new_flavor['disk'],",
            "                         allocation['DISK_GB'])",
            "",
            "    def get_migration_uuid_for_instance(self, instance_uuid):",
            "        # NOTE(danms): This is too much introspection for a test like this, but",
            "        # we can't see the migration uuid from the API, so we just encapsulate",
            "        # the peek behind the curtains here to keep it out of the tests.",
            "        # TODO(danms): Get the migration uuid from the API once it is exposed",
            "        ctxt = context.get_admin_context()",
            "        migrations = db.migration_get_all_by_filters(",
            "            ctxt, {'instance_uuid': instance_uuid})",
            "        self.assertEqual(1, len(migrations),",
            "                         'Test expected a single migration, '",
            "                         'but found %i' % len(migrations))",
            "        return migrations[0].uuid",
            "",
            "    def _boot_and_check_allocations(self, flavor, source_hostname):",
            "        \"\"\"Boot an instance and check that the resource allocation is correct",
            "",
            "        After booting an instance on the given host with a given flavor it",
            "        asserts that both the providers usages and resource allocations match",
            "        with the resources requested in the flavor. It also asserts that",
            "        running the periodic update_available_resource call does not change the",
            "        resource state.",
            "",
            "        :param flavor: the flavor the instance will be booted with",
            "        :param source_hostname: the name of the host the instance will be",
            "                                booted on",
            "        :return: the API representation of the booted instance",
            "        \"\"\"",
            "        server_req = self._build_minimal_create_server_request(",
            "            self.api, 'some-server', flavor_id=flavor['id'],",
            "            image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',",
            "            networks=[])",
            "        server_req['availability_zone'] = 'nova:%s' % source_hostname",
            "        LOG.info('booting on %s', source_hostname)",
            "        created_server = self.api.post_server({'server': server_req})",
            "        server = self._wait_for_state_change(",
            "            self.admin_api, created_server, 'ACTIVE')",
            "",
            "        # Verify that our source host is what the server ended up on",
            "        self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])",
            "",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "",
            "        # Before we run periodics, make sure that we have allocations/usages",
            "        # only on the source host",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(flavor, source_usages)",
            "",
            "        # Check that the other providers has no usage",
            "        for rp_uuid in [self._get_provider_uuid_by_host(hostname)",
            "                        for hostname in self.computes.keys()",
            "                        if hostname != source_hostname]:",
            "            usages = self._get_provider_usages(rp_uuid)",
            "            self.assertEqual({'VCPU': 0,",
            "                              'MEMORY_MB': 0,",
            "                              'DISK_GB': 0}, usages)",
            "",
            "        # Check that the server only allocates resource from the host it is",
            "        # booted on",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations),",
            "                         'No allocation for the server on the host it '",
            "                         'is booted on')",
            "        allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(flavor, allocation)",
            "",
            "        self._run_periodics()",
            "",
            "        # After running the periodics but before we start any other operation,",
            "        # we should have exactly the same allocation/usage information as",
            "        # before running the periodics",
            "",
            "        # Check usages on the selected host after boot",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(flavor, source_usages)",
            "",
            "        # Check that the server only allocates resource from the host it is",
            "        # booted on",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations),",
            "                         'No allocation for the server on the host it '",
            "                         'is booted on')",
            "        allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(flavor, allocation)",
            "",
            "        # Check that the other providers has no usage",
            "        for rp_uuid in [self._get_provider_uuid_by_host(hostname)",
            "                        for hostname in self.computes.keys()",
            "                        if hostname != source_hostname]:",
            "            usages = self._get_provider_usages(rp_uuid)",
            "            self.assertEqual({'VCPU': 0,",
            "                              'MEMORY_MB': 0,",
            "                              'DISK_GB': 0}, usages)",
            "        return server",
            "",
            "    def _delete_and_check_allocations(self, server):",
            "        \"\"\"Delete the instance and asserts that the allocations are cleaned",
            "",
            "        :param server: The API representation of the instance to be deleted",
            "        \"\"\"",
            "",
            "        self.api.delete_server(server['id'])",
            "        self._wait_until_deleted(server)",
            "        # NOTE(gibi): The resource allocation is deleted after the instance is",
            "        # destroyed in the db so wait_until_deleted might return before the",
            "        # the resource are deleted in placement. So we need to wait for the",
            "        # instance.delete.end notification as that is emitted after the",
            "        # resources are freed.",
            "        fake_notifier.wait_for_versioned_notifications('instance.delete.end')",
            "",
            "        for rp_uuid in [self._get_provider_uuid_by_host(hostname)",
            "                        for hostname in self.computes.keys()]:",
            "            usages = self._get_provider_usages(rp_uuid)",
            "            self.assertEqual({'VCPU': 0,",
            "                              'MEMORY_MB': 0,",
            "                              'DISK_GB': 0}, usages)",
            "",
            "        # and no allocations for the deleted server",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(0, len(allocations))",
            "",
            "    def _run_periodics(self):",
            "        \"\"\"Run the update_available_resource task on every compute manager",
            "",
            "        This runs periodics on the computes in an undefined order; some child",
            "        class redefined this function to force a specific order.",
            "        \"\"\"",
            "",
            "        ctx = context.get_admin_context()",
            "        for compute in self.computes.values():",
            "            LOG.info('Running periodic for compute (%s)',",
            "                compute.manager.host)",
            "            compute.manager.update_available_resource(ctx)",
            "        LOG.info('Finished with periodics')",
            "",
            "    def _wait_for_migration_status(self, server, expected_status):",
            "        \"\"\"Waits for a migration record with the given status to be found",
            "        for the given server, else the test fails. The migration record, if",
            "        found, is returned.",
            "        \"\"\"",
            "        for attempt in range(10):",
            "            migrations = self.api.api_get('/os-migrations').body['migrations']",
            "            for migration in migrations:",
            "                if (migration['instance_uuid'] == server['id'] and",
            "                        migration['status'].lower() ==",
            "                        expected_status.lower()):",
            "                    return migration",
            "            time.sleep(0.5)",
            "        self.fail('Timed out waiting for migration with status \"%s\" for '",
            "                  'instance: %s' % (expected_status, server['id']))",
            "",
            "",
            "class ServerMovingTests(ProviderUsageBaseTestCase):",
            "    \"\"\"Tests moving servers while checking the resource allocations and usages",
            "",
            "    These tests use two compute hosts. Boot a server on one of them then try to",
            "    move the server to the other. At every step resource allocation of the",
            "    server and the resource usages of the computes are queried from placement",
            "    API and asserted.",
            "    \"\"\"",
            "",
            "    REQUIRES_LOCKING = True",
            "    # NOTE(danms): The test defaults to using SmallFakeDriver,",
            "    # which only has one vcpu, which can't take the doubled allocation",
            "    # we're now giving it. So, use the bigger MediumFakeDriver here.",
            "    compute_driver = 'fake.MediumFakeDriver'",
            "",
            "    def setUp(self):",
            "        super(ServerMovingTests, self).setUp()",
            "        fake_notifier.stub_notifier(self)",
            "        self.addCleanup(fake_notifier.reset)",
            "",
            "        self.compute1 = self._start_compute(host='host1')",
            "        self.compute2 = self._start_compute(host='host2')",
            "",
            "        flavors = self.api.get_flavors()",
            "        self.flavor1 = flavors[0]",
            "        self.flavor2 = flavors[1]",
            "        # create flavor3 which has less MEMORY_MB but more DISK_GB than flavor2",
            "        flavor_body = {'flavor':",
            "                           {'name': 'test_flavor3',",
            "                            'ram': int(self.flavor2['ram'] / 2),",
            "                            'vcpus': 1,",
            "                            'disk': self.flavor2['disk'] * 2,",
            "                            'id': 'a22d5517-147c-4147-a0d1-e698df5cd4e3'",
            "                            }}",
            "",
            "        self.flavor3 = self.api.post_flavor(flavor_body)",
            "",
            "    def _other_hostname(self, host):",
            "        other_host = {'host1': 'host2',",
            "                      'host2': 'host1'}",
            "        return other_host[host]",
            "",
            "    def _run_periodics(self):",
            "        # NOTE(jaypipes): We always run periodics in the same order: first on",
            "        # compute1, then on compute2. However, we want to test scenarios when",
            "        # the periodics run at different times during mover operations. This is",
            "        # why we have the \"reverse\" tests which simply switch the source and",
            "        # dest host while keeping the order in which we run the",
            "        # periodics. This effectively allows us to test the matrix of timing",
            "        # scenarios during move operations.",
            "        ctx = context.get_admin_context()",
            "        LOG.info('Running periodic for compute1 (%s)',",
            "            self.compute1.manager.host)",
            "        self.compute1.manager.update_available_resource(ctx)",
            "        LOG.info('Running periodic for compute2 (%s)',",
            "            self.compute2.manager.host)",
            "        self.compute2.manager.update_available_resource(ctx)",
            "        LOG.info('Finished with periodics')",
            "",
            "    def test_resize_revert(self):",
            "        self._test_resize_revert(dest_hostname='host1')",
            "",
            "    def test_resize_revert_reverse(self):",
            "        self._test_resize_revert(dest_hostname='host2')",
            "",
            "    def test_resize_confirm(self):",
            "        self._test_resize_confirm(dest_hostname='host1')",
            "",
            "    def test_resize_confirm_reverse(self):",
            "        self._test_resize_confirm(dest_hostname='host2')",
            "",
            "    def _resize_and_check_allocations(self, server, old_flavor, new_flavor,",
            "            source_rp_uuid, dest_rp_uuid):",
            "        # Resize the server and check usages in VERIFY_RESIZE state",
            "        self.flags(allow_resize_to_same_host=False)",
            "        resize_req = {",
            "            'resize': {",
            "                'flavorRef': new_flavor['id']",
            "            }",
            "        }",
            "        self.api.post_server_action(server['id'], resize_req)",
            "        self._wait_for_state_change(self.api, server, 'VERIFY_RESIZE')",
            "",
            "        # OK, so the resize operation has run, but we have not yet confirmed or",
            "        # reverted the resize operation. Before we run periodics, make sure",
            "        # that we have allocations/usages on BOTH the source and the",
            "        # destination hosts.",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(old_flavor, source_usages)",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(new_flavor, dest_usages)",
            "",
            "        # The instance should own the new_flavor allocation against the",
            "        # destination host created by the scheduler",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        dest_alloc = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(new_flavor, dest_alloc)",
            "",
            "        # The migration should own the old_flavor allocation against the",
            "        # source host created by conductor",
            "        migration_uuid = self.get_migration_uuid_for_instance(server['id'])",
            "        allocations = self._get_allocations_by_server_uuid(migration_uuid)",
            "        source_alloc = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(old_flavor, source_alloc)",
            "",
            "        self._run_periodics()",
            "",
            "        # the original host expected to have the old resource usage",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(old_flavor, source_usages)",
            "",
            "        # the dest host expected to have resource allocation based on",
            "        # the new flavor the server is resized to",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(new_flavor, dest_usages)",
            "",
            "        # The instance should own the new_flavor allocation against the",
            "        # destination host",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(new_flavor, dest_allocation)",
            "",
            "        # The migration should own the old_flavor allocation against the",
            "        # source host",
            "        migration_uuid = self.get_migration_uuid_for_instance(server['id'])",
            "        allocations = self._get_allocations_by_server_uuid(migration_uuid)",
            "        source_alloc = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(old_flavor, source_alloc)",
            "",
            "    def _test_resize_revert(self, dest_hostname):",
            "        source_hostname = self._other_hostname(dest_hostname)",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(self.flavor1,",
            "            source_hostname)",
            "",
            "        self._resize_and_check_allocations(server, self.flavor1, self.flavor2,",
            "            source_rp_uuid, dest_rp_uuid)",
            "",
            "        # Revert the resize and check the usages",
            "        post = {'revertResize': None}",
            "        self.api.post_server_action(server['id'], post)",
            "        self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        self._run_periodics()",
            "",
            "        # the original host expected to have the old resource allocation",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertEqual({'VCPU': 0,",
            "                          'MEMORY_MB': 0,",
            "                          'DISK_GB': 0}, dest_usages,",
            "                          'Target host %s still has usage after the resize '",
            "                          'has been reverted' % dest_hostname)",
            "",
            "        # Check that the server only allocates resource from the original host",
            "        self.assertEqual(1, len(allocations))",
            "",
            "        source_allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def _test_resize_confirm(self, dest_hostname):",
            "        source_hostname = self._other_hostname(dest_hostname)",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(self.flavor1,",
            "            source_hostname)",
            "",
            "        self._resize_and_check_allocations(server, self.flavor1, self.flavor2,",
            "            source_rp_uuid, dest_rp_uuid)",
            "",
            "        # Confirm the resize and check the usages",
            "        post = {'confirmResize': None}",
            "        self.api.post_server_action(",
            "            server['id'], post, check_response_status=[204])",
            "        self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        # After confirming, we should have an allocation only on the",
            "        # destination host",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "",
            "        # and the server allocates only from the target host",
            "        self.assertEqual(1, len(allocations))",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "",
            "        # and the target host allocation should be according to the new flavor",
            "        self.assertFlavorMatchesAllocation(self.flavor2, dest_usages)",
            "        self.assertEqual({'VCPU': 0,",
            "                          'MEMORY_MB': 0,",
            "                          'DISK_GB': 0}, source_usages,",
            "                         'The source host %s still has usages after the '",
            "                         'resize has been confirmed' % source_hostname)",
            "",
            "        # and the target host allocation should be according to the new flavor",
            "        self.assertFlavorMatchesAllocation(self.flavor2, dest_usages)",
            "",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor2, dest_allocation)",
            "",
            "        self._run_periodics()",
            "",
            "        # Check we're still accurate after running the periodics",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "",
            "        # and the target host allocation should be according to the new flavor",
            "        self.assertFlavorMatchesAllocation(self.flavor2, dest_usages)",
            "        self.assertEqual({'VCPU': 0,",
            "                          'MEMORY_MB': 0,",
            "                          'DISK_GB': 0}, source_usages,",
            "                          'The source host %s still has usages after the '",
            "                          'resize has been confirmed' % source_hostname)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "",
            "        # and the server allocates only from the target host",
            "        self.assertEqual(1, len(allocations))",
            "",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor2, dest_allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def _resize_to_same_host_and_check_allocations(self, server, old_flavor,",
            "                                                   new_flavor, rp_uuid):",
            "        # Resize the server to the same host and check usages in VERIFY_RESIZE",
            "        # state",
            "        self.flags(allow_resize_to_same_host=True)",
            "        resize_req = {",
            "            'resize': {",
            "                'flavorRef': new_flavor['id']",
            "            }",
            "        }",
            "        self.api.post_server_action(server['id'], resize_req)",
            "        self._wait_for_state_change(self.api, server, 'VERIFY_RESIZE')",
            "",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorsMatchAllocation(old_flavor, new_flavor, usages)",
            "",
            "        # The instance should hold a new_flavor allocation",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(new_flavor, allocation)",
            "",
            "        # The migration should hold an old_flavor allocation",
            "        migration_uuid = self.get_migration_uuid_for_instance(server['id'])",
            "        allocations = self._get_allocations_by_server_uuid(migration_uuid)",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(old_flavor, allocation)",
            "",
            "        # We've resized to the same host and have doubled allocations for both",
            "        # the old and new flavor on the same host. Run the periodic on the",
            "        # compute to see if it tramples on what the scheduler did.",
            "        self._run_periodics()",
            "",
            "        usages = self._get_provider_usages(rp_uuid)",
            "",
            "        # In terms of usage, it's still double on the host because the instance",
            "        # and the migration each hold an allocation for the new and old",
            "        # flavors respectively.",
            "        self.assertFlavorsMatchAllocation(old_flavor, new_flavor, usages)",
            "",
            "        # The instance should hold a new_flavor allocation",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(new_flavor, allocation)",
            "",
            "        # The migration should hold an old_flavor allocation",
            "        allocations = self._get_allocations_by_server_uuid(migration_uuid)",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(old_flavor, allocation)",
            "",
            "    def test_resize_revert_same_host(self):",
            "        # make sure that the test only uses a single host",
            "        compute2_service_id = self.admin_api.get_services(",
            "            host=self.compute2.host, binary='nova-compute')[0]['id']",
            "        self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})",
            "",
            "        hostname = self.compute1.manager.host",
            "        rp_uuid = self._get_provider_uuid_by_host(hostname)",
            "",
            "        server = self._boot_and_check_allocations(self.flavor2, hostname)",
            "",
            "        self._resize_to_same_host_and_check_allocations(",
            "            server, self.flavor2, self.flavor3, rp_uuid)",
            "",
            "        # Revert the resize and check the usages",
            "        post = {'revertResize': None}",
            "        self.api.post_server_action(server['id'], post)",
            "        self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        self._run_periodics()",
            "",
            "        # after revert only allocations due to the old flavor should remain",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor2, usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor2, allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_resize_confirm_same_host(self):",
            "        # make sure that the test only uses a single host",
            "        compute2_service_id = self.admin_api.get_services(",
            "            host=self.compute2.host, binary='nova-compute')[0]['id']",
            "        self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})",
            "",
            "        hostname = self.compute1.manager.host",
            "        rp_uuid = self._get_provider_uuid_by_host(hostname)",
            "",
            "        server = self._boot_and_check_allocations(self.flavor2, hostname)",
            "",
            "        self._resize_to_same_host_and_check_allocations(",
            "            server, self.flavor2, self.flavor3, rp_uuid)",
            "",
            "        # Confirm the resize and check the usages",
            "        post = {'confirmResize': None}",
            "        self.api.post_server_action(",
            "            server['id'], post, check_response_status=[204])",
            "        self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        self._run_periodics()",
            "",
            "        # after confirm only allocations due to the new flavor should remain",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor3, usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor3, allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_resize_not_enough_resource(self):",
            "        # Try to resize to a flavor that requests more VCPU than what the",
            "        # compute hosts has available and expect the resize to fail",
            "",
            "        flavor_body = {'flavor':",
            "                           {'name': 'test_too_big_flavor',",
            "                            'ram': 1024,",
            "                            'vcpus': fake.MediumFakeDriver.vcpus + 1,",
            "                            'disk': 20,",
            "                            }}",
            "",
            "        big_flavor = self.api.post_flavor(flavor_body)",
            "",
            "        dest_hostname = self.compute2.host",
            "        source_hostname = self._other_hostname(dest_hostname)",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        self.flags(allow_resize_to_same_host=False)",
            "        resize_req = {",
            "            'resize': {",
            "                'flavorRef': big_flavor['id']",
            "            }",
            "        }",
            "",
            "        resp = self.api.post_server_action(",
            "            server['id'], resize_req, check_response_status=[400])",
            "        self.assertEqual(",
            "            resp['badRequest']['message'],",
            "            \"No valid host was found. No valid host found for resize\")",
            "        server = self.admin_api.get_server(server['id'])",
            "        self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])",
            "",
            "        # only the source host shall have usages after the failed resize",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        # Check that the other provider has no usage",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertEqual({'VCPU': 0,",
            "                          'MEMORY_MB': 0,",
            "                          'DISK_GB': 0}, dest_usages)",
            "",
            "        # Check that the server only allocates resource from the host it is",
            "        # booted on",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_evacuate(self):",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        source_compute_id = self.admin_api.get_services(",
            "            host=source_hostname, binary='nova-compute')[0]['id']",
            "",
            "        self.compute1.stop()",
            "        # force it down to avoid waiting for the service group to time out",
            "        self.admin_api.put_service(",
            "            source_compute_id, {'forced_down': 'true'})",
            "",
            "        # evacuate the server",
            "        post = {'evacuate': {}}",
            "        self.api.post_server_action(",
            "            server['id'], post)",
            "        expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,",
            "                           'status': 'ACTIVE'}",
            "        server = self._wait_for_server_parameter(self.api, server,",
            "                                                 expected_params)",
            "",
            "        # Expect to have allocation and usages on both computes as the",
            "        # source compute is still down",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(2, len(allocations))",
            "        source_allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_allocation)",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_allocation)",
            "",
            "        # restart the source compute",
            "        self.restart_compute_service(self.compute1)",
            "",
            "        self.admin_api.put_service(",
            "            source_compute_id, {'forced_down': 'false'})",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertEqual({'VCPU': 0,",
            "                          'MEMORY_MB': 0,",
            "                          'DISK_GB': 0},",
            "                         source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_evacuate_forced_host(self):",
            "        \"\"\"Evacuating a server with a forced host bypasses the scheduler",
            "        which means conductor has to create the allocations against the",
            "        destination node. This test recreates the scenarios and asserts",
            "        the allocations on the source and destination nodes are as expected.",
            "        \"\"\"",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        source_compute_id = self.admin_api.get_services(",
            "            host=source_hostname, binary='nova-compute')[0]['id']",
            "",
            "        self.compute1.stop()",
            "        # force it down to avoid waiting for the service group to time out",
            "        self.admin_api.put_service(",
            "            source_compute_id, {'forced_down': 'true'})",
            "",
            "        # evacuate the server and force the destination host which bypasses",
            "        # the scheduler",
            "        post = {",
            "            'evacuate': {",
            "                'host': dest_hostname,",
            "                'force': True",
            "            }",
            "        }",
            "        self.api.post_server_action(server['id'], post)",
            "        expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,",
            "                           'status': 'ACTIVE'}",
            "        server = self._wait_for_server_parameter(self.api, server,",
            "                                                 expected_params)",
            "",
            "        # Run the periodics to show those don't modify allocations.",
            "        self._run_periodics()",
            "",
            "        # Expect to have allocation and usages on both computes as the",
            "        # source compute is still down",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(2, len(allocations))",
            "        source_allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_allocation)",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_allocation)",
            "",
            "        # restart the source compute",
            "        self.restart_compute_service(self.compute1)",
            "        self.admin_api.put_service(",
            "            source_compute_id, {'forced_down': 'false'})",
            "",
            "        # Run the periodics again to show they don't change anything.",
            "        self._run_periodics()",
            "",
            "        # When the source node starts up, the instance has moved so the",
            "        # ResourceTracker should cleanup allocations for the source node.",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertEqual(",
            "            {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, source_usages)",
            "",
            "        # The usages/allocations should still exist on the destination node",
            "        # after the source node starts back up.",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_evacuate_claim_on_dest_fails(self):",
            "        \"\"\"Tests that the allocations on the destination node are cleaned up",
            "        when the rebuild move claim fails due to insufficient resources.",
            "        \"\"\"",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        source_compute_id = self.admin_api.get_services(",
            "            host=source_hostname, binary='nova-compute')[0]['id']",
            "",
            "        self.compute1.stop()",
            "        # force it down to avoid waiting for the service group to time out",
            "        self.admin_api.put_service(",
            "            source_compute_id, {'forced_down': 'true'})",
            "",
            "        # NOTE(mriedem): This isn't great, and I'd like to fake out the driver",
            "        # to make the claim fail, by doing something like returning a too high",
            "        # memory_mb overhead, but the limits dict passed to the claim is empty",
            "        # so the claim test is considering it as unlimited and never actually",
            "        # performs a claim test. Configuring the scheduler to use the RamFilter",
            "        # to get the memory_mb limit at least seems like it should work but",
            "        # it doesn't appear to for some reason...",
            "        def fake_move_claim(*args, **kwargs):",
            "            # Assert the destination node allocation exists.",
            "            dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "            self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "            raise exception.ComputeResourcesUnavailable(",
            "                    reason='test_evacuate_claim_on_dest_fails')",
            "",
            "        with mock.patch('nova.compute.claims.MoveClaim', fake_move_claim):",
            "            # evacuate the server",
            "            self.api.post_server_action(server['id'], {'evacuate': {}})",
            "            # the migration will fail on the dest node and the instance will",
            "            # go into error state",
            "            server = self._wait_for_state_change(self.api, server, 'ERROR')",
            "",
            "        # Run the periodics to show those don't modify allocations.",
            "        self._run_periodics()",
            "",
            "        # The allocation should still exist on the source node since it's",
            "        # still down, and the allocation on the destination node should be",
            "        # cleaned up.",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(",
            "            {'vcpus': 0, 'ram': 0, 'disk': 0}, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        source_allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_allocation)",
            "",
            "        # restart the source compute",
            "        self.restart_compute_service(self.compute1)",
            "        self.admin_api.put_service(",
            "            source_compute_id, {'forced_down': 'false'})",
            "",
            "        # Run the periodics again to show they don't change anything.",
            "        self._run_periodics()",
            "",
            "        # The source compute shouldn't have cleaned up the allocation for",
            "        # itself since the instance didn't move.",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        source_allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_allocation)",
            "",
            "    def test_evacuate_rebuild_on_dest_fails(self):",
            "        \"\"\"Tests that the allocations on the destination node are cleaned up",
            "        automatically when the claim is made but the actual rebuild",
            "        via the driver fails.",
            "",
            "        \"\"\"",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        source_compute_id = self.admin_api.get_services(",
            "            host=source_hostname, binary='nova-compute')[0]['id']",
            "",
            "        self.compute1.stop()",
            "        # force it down to avoid waiting for the service group to time out",
            "        self.admin_api.put_service(",
            "            source_compute_id, {'forced_down': 'true'})",
            "",
            "        def fake_rebuild(*args, **kwargs):",
            "            # Assert the destination node allocation exists.",
            "            dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "            self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "            raise test.TestingException('test_evacuate_rebuild_on_dest_fails')",
            "",
            "        with mock.patch.object(",
            "                self.compute2.driver, 'rebuild', fake_rebuild):",
            "            # evacuate the server",
            "            self.api.post_server_action(server['id'], {'evacuate': {}})",
            "            # the migration will fail on the dest node and the instance will",
            "            # go into error state",
            "            server = self._wait_for_state_change(self.api, server, 'ERROR')",
            "",
            "        # Run the periodics to show those don't modify allocations.",
            "        self._run_periodics()",
            "",
            "        # The allocation should still exist on the source node since it's",
            "        # still down, and the allocation on the destination node should be",
            "        # cleaned up.",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(",
            "            {'vcpus': 0, 'ram': 0, 'disk': 0}, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        source_allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_allocation)",
            "",
            "        # restart the source compute",
            "        self.restart_compute_service(self.compute1)",
            "        self.admin_api.put_service(",
            "            source_compute_id, {'forced_down': 'false'})",
            "",
            "        # Run the periodics again to show they don't change anything.",
            "        self._run_periodics()",
            "",
            "        # The source compute shouldn't have cleaned up the allocation for",
            "        # itself since the instance didn't move.",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        source_allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_allocation)",
            "",
            "    def _boot_then_shelve_and_check_allocations(self, hostname, rp_uuid):",
            "        # avoid automatic shelve offloading",
            "        self.flags(shelved_offload_time=-1)",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, hostname)",
            "        req = {",
            "            'shelve': {}",
            "        }",
            "        self.api.post_server_action(server['id'], req)",
            "        self._wait_for_state_change(self.api, server, 'SHELVED')",
            "        # the host should maintain the existing allocation for this instance",
            "        # while the instance is shelved",
            "        source_usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "        # Check that the server only allocates resource from the host it is",
            "        # booted on",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, allocation)",
            "        return server",
            "",
            "    def test_shelve_unshelve(self):",
            "        source_hostname = self.compute1.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        server = self._boot_then_shelve_and_check_allocations(",
            "            source_hostname, source_rp_uuid)",
            "",
            "        req = {",
            "            'unshelve': {}",
            "        }",
            "        self.api.post_server_action(server['id'], req)",
            "        self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        # the host should have resource usage as the instance is ACTIVE",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        # Check that the server only allocates resource from the host it is",
            "        # booted on",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def _shelve_offload_and_check_allocations(self, server, source_rp_uuid):",
            "        req = {",
            "            'shelveOffload': {}",
            "        }",
            "        self.api.post_server_action(server['id'], req)",
            "        self._wait_for_server_parameter(",
            "            self.api, server, {'status': 'SHELVED_OFFLOADED',",
            "                               'OS-EXT-SRV-ATTR:host': None})",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertEqual({'VCPU': 0,",
            "                          'MEMORY_MB': 0,",
            "                          'DISK_GB': 0},",
            "                         source_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(0, len(allocations))",
            "",
            "    def test_shelve_offload_unshelve_diff_host(self):",
            "        source_hostname = self.compute1.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        server = self._boot_then_shelve_and_check_allocations(",
            "            source_hostname, source_rp_uuid)",
            "",
            "        self._shelve_offload_and_check_allocations(server, source_rp_uuid)",
            "",
            "        # unshelve after shelve offload will do scheduling. this test case",
            "        # wants to test the scenario when the scheduler select a different host",
            "        # to ushelve the instance. So we disable the original host.",
            "        source_service_id = self.admin_api.get_services(",
            "            host=source_hostname, binary='nova-compute')[0]['id']",
            "        self.admin_api.put_service(source_service_id, {'status': 'disabled'})",
            "",
            "        req = {",
            "            'unshelve': {}",
            "        }",
            "        self.api.post_server_action(server['id'], req)",
            "        server = self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "        # unshelving an offloaded instance will call the scheduler so the",
            "        # instance might end up on a different host",
            "        current_hostname = server['OS-EXT-SRV-ATTR:host']",
            "        self.assertEqual(current_hostname, self._other_hostname(",
            "            source_hostname))",
            "",
            "        # the host running the instance should have resource usage",
            "        current_rp_uuid = self._get_provider_uuid_by_host(current_hostname)",
            "        current_usages = self._get_provider_usages(current_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, current_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[current_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_shelve_offload_unshelve_same_host(self):",
            "        source_hostname = self.compute1.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        server = self._boot_then_shelve_and_check_allocations(",
            "            source_hostname, source_rp_uuid)",
            "",
            "        self._shelve_offload_and_check_allocations(server, source_rp_uuid)",
            "",
            "        # unshelve after shelve offload will do scheduling. this test case",
            "        # wants to test the scenario when the scheduler select the same host",
            "        # to ushelve the instance. So we disable the other host.",
            "        source_service_id = self.admin_api.get_services(",
            "            host=self._other_hostname(source_hostname),",
            "            binary='nova-compute')[0]['id']",
            "        self.admin_api.put_service(source_service_id, {'status': 'disabled'})",
            "",
            "        req = {",
            "            'unshelve': {}",
            "        }",
            "        self.api.post_server_action(server['id'], req)",
            "        server = self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "        # unshelving an offloaded instance will call the scheduler so the",
            "        # instance might end up on a different host",
            "        current_hostname = server['OS-EXT-SRV-ATTR:host']",
            "        self.assertEqual(current_hostname, source_hostname)",
            "",
            "        # the host running the instance should have resource usage",
            "        current_rp_uuid = self._get_provider_uuid_by_host(current_hostname)",
            "        current_usages = self._get_provider_usages(current_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, current_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[current_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_live_migrate_force(self):",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "        post = {",
            "            'os-migrateLive': {",
            "                'host': dest_hostname,",
            "                'block_migration': True,",
            "                'force': True,",
            "            }",
            "        }",
            "",
            "        self.api.post_server_action(server['id'], post)",
            "        self._wait_for_server_parameter(self.api, server,",
            "            {'OS-EXT-SRV-ATTR:host': dest_hostname,",
            "             'status': 'ACTIVE'})",
            "",
            "        self._run_periodics()",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        # NOTE(danms): There should be no usage for the source",
            "        self.assertFlavorMatchesAllocation(",
            "            {'ram': 0, 'disk': 0, 'vcpus': 0}, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        # the server has an allocation on only the dest node",
            "        self.assertEqual(1, len(allocations))",
            "        self.assertNotIn(source_rp_uuid, allocations)",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_live_migrate(self):",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "        post = {",
            "            'os-migrateLive': {",
            "                'host': dest_hostname,",
            "                'block_migration': True,",
            "            }",
            "        }",
            "",
            "        self.api.post_server_action(server['id'], post)",
            "        self._wait_for_server_parameter(self.api, server,",
            "                                        {'OS-EXT-SRV-ATTR:host': dest_hostname,",
            "                                         'status': 'ACTIVE'})",
            "",
            "        self._run_periodics()",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        # NOTE(danms): There should be no usage for the source",
            "        self.assertFlavorMatchesAllocation(",
            "            {'ram': 0, 'disk': 0, 'vcpus': 0}, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        self.assertNotIn(source_rp_uuid, allocations)",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_live_migrate_pre_check_fails(self):",
            "        \"\"\"Tests the case that the LiveMigrationTask in conductor has",
            "        called the scheduler which picked a host and created allocations",
            "        against it in Placement, but then when the conductor task calls",
            "        check_can_live_migrate_destination on the destination compute it",
            "        fails. The allocations on the destination compute node should be",
            "        cleaned up before the conductor task asks the scheduler for another",
            "        host to try the live migration.",
            "        \"\"\"",
            "        self.failed_hostname = None",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        def fake_check_can_live_migrate_destination(",
            "                context, instance, src_compute_info, dst_compute_info,",
            "                block_migration=False, disk_over_commit=False):",
            "            self.failed_hostname = dst_compute_info['host']",
            "            raise exception.MigrationPreCheckError(",
            "                reason='test_live_migrate_pre_check_fails')",
            "",
            "        with mock.patch('nova.virt.fake.FakeDriver.'",
            "                        'check_can_live_migrate_destination',",
            "                        side_effect=fake_check_can_live_migrate_destination):",
            "            post = {",
            "                'os-migrateLive': {",
            "                    'host': dest_hostname,",
            "                    'block_migration': True,",
            "                }",
            "            }",
            "            self.api.post_server_action(server['id'], post)",
            "            # As there are only two computes and we failed to live migrate to",
            "            # the only other destination host, the LiveMigrationTask raises",
            "            # MaxRetriesExceeded back to the conductor manager which handles it",
            "            # generically and sets the instance back to ACTIVE status and",
            "            # clears the task_state. The migration record status is set to",
            "            # 'error', so that's what we need to look for to know when this",
            "            # is done.",
            "            migration = self._wait_for_migration_status(server, 'error')",
            "",
            "        # The source_compute should be set on the migration record, but the",
            "        # destination shouldn't be as we never made it to one.",
            "        self.assertEqual(source_hostname, migration['source_compute'])",
            "        self.assertIsNone(migration['dest_compute'])",
            "        # Make sure the destination host (the only other host) is the failed",
            "        # host.",
            "        self.assertEqual(dest_hostname, self.failed_hostname)",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        # Since the instance didn't move, assert the allocations are still",
            "        # on the source node.",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        # Assert the allocations, created by the scheduler, are cleaned up",
            "        # after the migration pre-check error happens.",
            "        self.assertFlavorMatchesAllocation(",
            "            {'vcpus': 0, 'ram': 0, 'disk': 0}, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        # There should only be 1 allocation for the instance on the source node",
            "        self.assertEqual(1, len(allocations))",
            "        self.assertIn(source_rp_uuid, allocations)",
            "        self.assertFlavorMatchesAllocation(",
            "            self.flavor1, allocations[source_rp_uuid]['resources'])",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    @mock.patch('nova.virt.fake.FakeDriver.pre_live_migration',",
            "                # The actual type of exception here doesn't matter. The point",
            "                # is that the virt driver raised an exception from the",
            "                # pre_live_migration method on the destination host.",
            "                side_effect=test.TestingException(",
            "                    'test_live_migrate_rollback_cleans_dest_node_allocations'))",
            "    def test_live_migrate_rollback_cleans_dest_node_allocations(",
            "            self, mock_pre_live_migration):",
            "        \"\"\"Tests the case that when live migration fails, either during the",
            "        call to pre_live_migration on the destination, or during the actual",
            "        live migration in the virt driver, the allocations on the destination",
            "        node are rolled back since the instance is still on the source node.",
            "        \"\"\"",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        post = {",
            "            'os-migrateLive': {",
            "                'host': dest_hostname,",
            "                'block_migration': True,",
            "            }",
            "        }",
            "        self.api.post_server_action(server['id'], post)",
            "        # The compute manager will put the migration record into error status",
            "        # when pre_live_migration fails, so wait for that to happen.",
            "        migration = self._wait_for_migration_status(server, 'error')",
            "        # The _rollback_live_migration method in the compute manager will reset",
            "        # the task_state on the instance, so wait for that to happen.",
            "        server = self._wait_for_server_parameter(",
            "            self.api, server, {'OS-EXT-STS:task_state': None})",
            "",
            "        self.assertEqual(source_hostname, migration['source_compute'])",
            "        self.assertEqual(dest_hostname, migration['dest_compute'])",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        # Since the instance didn't move, assert the allocations are still",
            "        # on the source node.",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        # Assert the allocations, created by the scheduler, are cleaned up",
            "        # after the rollback happens.",
            "        self.assertFlavorMatchesAllocation(",
            "            {'vcpus': 0, 'ram': 0, 'disk': 0}, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        # There should only be 1 allocation for the instance on the source node",
            "        self.assertEqual(1, len(allocations))",
            "        self.assertIn(source_rp_uuid, allocations)",
            "        self.assertFlavorMatchesAllocation(",
            "            self.flavor1, allocations[source_rp_uuid]['resources'])",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_rescheduling_when_migrating_instance(self):",
            "        \"\"\"Tests that allocations are removed from the destination node by",
            "        the compute service when a cold migrate / resize fails and a reschedule",
            "        request is sent back to conductor.",
            "        \"\"\"",
            "        source_hostname = self.compute1.manager.host",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        def fake_prep_resize(*args, **kwargs):",
            "            dest_hostname = self._other_hostname(source_hostname)",
            "            dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "            dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "            self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "            allocations = self._get_allocations_by_server_uuid(server['id'])",
            "            self.assertIn(dest_rp_uuid, allocations)",
            "",
            "            source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "            source_usages = self._get_provider_usages(source_rp_uuid)",
            "            self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "            migration_uuid = self.get_migration_uuid_for_instance(server['id'])",
            "            allocations = self._get_allocations_by_server_uuid(migration_uuid)",
            "            self.assertIn(source_rp_uuid, allocations)",
            "",
            "            raise test.TestingException('Simulated _prep_resize failure.')",
            "",
            "        # Yes this isn't great in a functional test, but it's simple.",
            "        self.stub_out('nova.compute.manager.ComputeManager._prep_resize',",
            "                      fake_prep_resize)",
            "",
            "        # Now migrate the server which is going to fail on the destination.",
            "        self.api.post_server_action(server['id'], {'migrate': None})",
            "",
            "        self._wait_for_action_fail_completion(",
            "            server, instance_actions.MIGRATE, 'compute_prep_resize')",
            "",
            "        dest_hostname = self._other_hostname(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        failed_usages = self._get_provider_usages(dest_rp_uuid)",
            "        # Expects no allocation records on the failed host.",
            "        self.assertFlavorMatchesAllocation(",
            "           {'vcpus': 0, 'ram': 0, 'disk': 0}, failed_usages)",
            "",
            "        # Ensure the allocation records still exist on the source host.",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertIn(source_rp_uuid, allocations)",
            "",
            "    def test_resize_to_same_host_prep_resize_fails(self):",
            "        \"\"\"Tests that when we resize to the same host and resize fails in",
            "        the prep_resize method, we cleanup the allocations before rescheduling.",
            "        \"\"\"",
            "        # make sure that the test only uses a single host",
            "        compute2_service_id = self.admin_api.get_services(",
            "            host=self.compute2.host, binary='nova-compute')[0]['id']",
            "        self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})",
            "",
            "        hostname = self.compute1.manager.host",
            "        rp_uuid = self._get_provider_uuid_by_host(hostname)",
            "",
            "        server = self._boot_and_check_allocations(self.flavor1, hostname)",
            "",
            "        def fake_prep_resize(*args, **kwargs):",
            "            # Ensure the allocations are doubled now before we fail.",
            "            usages = self._get_provider_usages(rp_uuid)",
            "            self.assertFlavorsMatchAllocation(",
            "                self.flavor1, self.flavor2, usages)",
            "            raise test.TestingException('Simulated _prep_resize failure.')",
            "",
            "        # Yes this isn't great in a functional test, but it's simple.",
            "        self.stub_out('nova.compute.manager.ComputeManager._prep_resize',",
            "                      fake_prep_resize)",
            "",
            "        self.flags(allow_resize_to_same_host=True)",
            "        resize_req = {",
            "            'resize': {",
            "                'flavorRef': self.flavor2['id']",
            "            }",
            "        }",
            "        self.api.post_server_action(server['id'], resize_req)",
            "",
            "        self._wait_for_action_fail_completion(",
            "            server, instance_actions.RESIZE, 'compute_prep_resize')",
            "",
            "        # Ensure the allocation records still exist on the host.",
            "        source_rp_uuid = self._get_provider_uuid_by_host(hostname)",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        # The new_flavor should have been subtracted from the doubled",
            "        # allocation which just leaves us with the original flavor.",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "",
            "class ServerLiveMigrateForceAndAbort(ProviderUsageBaseTestCase):",
            "    \"\"\"Test Server live migrations, which delete the migration or",
            "    force_complete it, and check the allocations after the operations.",
            "",
            "    The test are using fakedriver to handle the force_completion and deletion",
            "    of live migration.",
            "    \"\"\"",
            "",
            "    compute_driver = 'fake.FakeLiveMigrateDriver'",
            "",
            "    def setUp(self):",
            "        super(ServerLiveMigrateForceAndAbort, self).setUp()",
            "",
            "        self.compute1 = self._start_compute(host='host1')",
            "        self.compute2 = self._start_compute(host='host2')",
            "",
            "        flavors = self.api.get_flavors()",
            "        self.flavor1 = flavors[0]",
            "",
            "    def test_live_migrate_force_complete(self):",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        post = {",
            "            'os-migrateLive': {",
            "                'host': dest_hostname,",
            "                'block_migration': True,",
            "            }",
            "        }",
            "        self.api.post_server_action(server['id'], post)",
            "",
            "        migration = self._wait_for_migration_status(server, 'running')",
            "        self.api.force_complete_migration(server['id'],",
            "                                          migration['id'])",
            "",
            "        self._wait_for_server_parameter(self.api, server,",
            "                                        {'OS-EXT-SRV-ATTR:host': dest_hostname,",
            "                                         'status': 'ACTIVE'})",
            "",
            "        self._run_periodics()",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(",
            "            {'ram': 0, 'disk': 0, 'vcpus': 0}, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        self.assertNotIn(source_rp_uuid, allocations)",
            "",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_live_migrate_delete(self):",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        post = {",
            "            'os-migrateLive': {",
            "                'host': dest_hostname,",
            "                'block_migration': True,",
            "            }",
            "        }",
            "        self.api.post_server_action(server['id'], post)",
            "",
            "        migration = self._wait_for_migration_status(server, 'running')",
            "",
            "        self.api.delete_migration(server['id'], migration['id'])",
            "        self._wait_for_server_parameter(self.api, server,",
            "            {'OS-EXT-SRV-ATTR:host': source_hostname,",
            "             'status': 'ACTIVE'})",
            "",
            "        self._run_periodics()",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        self.assertNotIn(dest_rp_uuid, allocations)",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        source_allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_allocation)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(",
            "           {'ram': 0, 'disk': 0, 'vcpus': 0}, dest_usages)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "",
            "class ServerRescheduleTests(ProviderUsageBaseTestCase):",
            "    \"\"\"Tests server create scenarios which trigger a reschedule during",
            "    a server build and validates that allocations in Placement",
            "    are properly cleaned up.",
            "",
            "    Uses a fake virt driver that fails the build on the first attempt.",
            "    \"\"\"",
            "",
            "    compute_driver = 'fake.FakeRescheduleDriver'",
            "",
            "    def setUp(self):",
            "        super(ServerRescheduleTests, self).setUp()",
            "        self.compute1 = self._start_compute(host='host1')",
            "        self.compute2 = self._start_compute(host='host2')",
            "",
            "        flavors = self.api.get_flavors()",
            "        self.flavor1 = flavors[0]",
            "",
            "    def _other_hostname(self, host):",
            "        other_host = {'host1': 'host2',",
            "                      'host2': 'host1'}",
            "        return other_host[host]",
            "",
            "    def test_rescheduling_when_booting_instance(self):",
            "        \"\"\"Tests that allocations, created by the scheduler, are cleaned",
            "        from the source node when the build fails on that node and is",
            "        rescheduled to another node.",
            "        \"\"\"",
            "        server_req = self._build_minimal_create_server_request(",
            "                self.api, 'some-server', flavor_id=self.flavor1['id'],",
            "                image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',",
            "                networks=[])",
            "",
            "        created_server = self.api.post_server({'server': server_req})",
            "        server = self._wait_for_state_change(",
            "                self.api, created_server, 'ACTIVE')",
            "        dest_hostname = server['OS-EXT-SRV-ATTR:host']",
            "        failed_hostname = self._other_hostname(dest_hostname)",
            "",
            "        LOG.info('failed on %s', failed_hostname)",
            "        LOG.info('booting on %s', dest_hostname)",
            "",
            "        failed_rp_uuid = self._get_provider_uuid_by_host(failed_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        failed_usages = self._get_provider_usages(failed_rp_uuid)",
            "        # Expects no allocation records on the failed host.",
            "        self.assertFlavorMatchesAllocation(",
            "           {'vcpus': 0, 'ram': 0, 'disk': 0}, failed_usages)",
            "",
            "        # Ensure the allocation records on the destination host.",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "",
            "",
            "class ServerBuildAbortTests(ProviderUsageBaseTestCase):",
            "    \"\"\"Tests server create scenarios which trigger a build abort during",
            "    a server build and validates that allocations in Placement",
            "    are properly cleaned up.",
            "",
            "    Uses a fake virt driver that aborts the build on the first attempt.",
            "    \"\"\"",
            "",
            "    compute_driver = 'fake.FakeBuildAbortDriver'",
            "",
            "    def setUp(self):",
            "        super(ServerBuildAbortTests, self).setUp()",
            "        # We only need one compute service/host/node for these tests.",
            "        self.compute1 = self._start_compute(host='host1')",
            "",
            "        flavors = self.api.get_flavors()",
            "        self.flavor1 = flavors[0]",
            "",
            "    def test_abort_when_booting_instance(self):",
            "        \"\"\"Tests that allocations, created by the scheduler, are cleaned",
            "        from the source node when the build is aborted on that node.",
            "        \"\"\"",
            "        server_req = self._build_minimal_create_server_request(",
            "                self.api, 'some-server', flavor_id=self.flavor1['id'],",
            "                image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',",
            "                networks=[])",
            "",
            "        created_server = self.api.post_server({'server': server_req})",
            "        self._wait_for_state_change(self.api, created_server, 'ERROR')",
            "",
            "        failed_hostname = self.compute1.manager.host",
            "",
            "        failed_rp_uuid = self._get_provider_uuid_by_host(failed_hostname)",
            "        failed_usages = self._get_provider_usages(failed_rp_uuid)",
            "        # Expects no allocation records on the failed host.",
            "        self.assertFlavorMatchesAllocation(",
            "           {'vcpus': 0, 'ram': 0, 'disk': 0}, failed_usages)",
            "",
            "",
            "class ServerUnshelveSpawnFailTests(ProviderUsageBaseTestCase):",
            "    \"\"\"Tests server unshelve scenarios which trigger a",
            "    VirtualInterfaceCreateException during driver.spawn() and validates that",
            "    allocations in Placement are properly cleaned up.",
            "    \"\"\"",
            "",
            "    compute_driver = 'fake.FakeUnshelveSpawnFailDriver'",
            "",
            "    def setUp(self):",
            "        super(ServerUnshelveSpawnFailTests, self).setUp()",
            "        # We only need one compute service/host/node for these tests.",
            "        fake.set_nodes(['host1'])",
            "        self.flags(host='host1')",
            "        self.compute1 = self._start_compute('host1')",
            "",
            "        flavors = self.api.get_flavors()",
            "        self.flavor1 = flavors[0]",
            "",
            "    def test_driver_spawn_fail_when_unshelving_instance(self):",
            "        \"\"\"Tests that allocations, created by the scheduler, are cleaned",
            "        from the target node when the unshelve driver.spawn fails on that node.",
            "        \"\"\"",
            "        hostname = self.compute1.manager.host",
            "        rp_uuid = self._get_provider_uuid_by_host(hostname)",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        # We start with no usages on the host.",
            "        self.assertFlavorMatchesAllocation(",
            "           {'vcpus': 0, 'ram': 0, 'disk': 0}, usages)",
            "",
            "        server_req = self._build_minimal_create_server_request(",
            "            self.api, 'unshelve-spawn-fail', flavor_id=self.flavor1['id'],",
            "            image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',",
            "            networks='none')",
            "",
            "        server = self.api.post_server({'server': server_req})",
            "        self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        # assert allocations exist for the host",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, usages)",
            "",
            "        # shelve offload the server",
            "        self.flags(shelved_offload_time=0)",
            "        self.api.post_server_action(server['id'], {'shelve': None})",
            "        self._wait_for_server_parameter(",
            "            self.api, server, {'status': 'SHELVED_OFFLOADED',",
            "                               'OS-EXT-SRV-ATTR:host': None})",
            "",
            "        # assert allocations were removed from the host",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(",
            "           {'vcpus': 0, 'ram': 0, 'disk': 0}, usages)",
            "",
            "        # unshelve the server, which should fail",
            "        self.api.post_server_action(server['id'], {'unshelve': None})",
            "        self._wait_for_action_fail_completion(",
            "            server, instance_actions.UNSHELVE, 'compute_unshelve_instance')",
            "",
            "        # assert allocations were removed from the host",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(",
            "           {'vcpus': 0, 'ram': 0, 'disk': 0}, usages)",
            "",
            "",
            "class ServerSoftDeleteTests(ProviderUsageBaseTestCase):",
            "",
            "    compute_driver = 'fake.SmallFakeDriver'",
            "",
            "    def setUp(self):",
            "        super(ServerSoftDeleteTests, self).setUp()",
            "        # We only need one compute service/host/node for these tests.",
            "        self.compute1 = self._start_compute('host1')",
            "",
            "        flavors = self.api.get_flavors()",
            "        self.flavor1 = flavors[0]",
            "",
            "    def _soft_delete_and_check_allocation(self, server, hostname):",
            "        self.api.delete_server(server['id'])",
            "        server = self._wait_for_state_change(self.api, server, 'SOFT_DELETED')",
            "",
            "        self._run_periodics()",
            "",
            "        # in soft delete state nova should keep the resource allocation as",
            "        # the instance can be restored",
            "        rp_uuid = self._get_provider_uuid_by_host(hostname)",
            "",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, allocation)",
            "",
            "        # run the periodic reclaim but as time isn't advanced it should not",
            "        # reclaim the instance",
            "        ctxt = context.get_admin_context()",
            "        self.compute1._reclaim_queued_deletes(ctxt)",
            "",
            "        self._run_periodics()",
            "",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, allocation)",
            "",
            "    def test_soft_delete_then_reclaim(self):",
            "        \"\"\"Asserts that the automatic reclaim of soft deleted instance cleans",
            "        up the allocations in placement.",
            "        \"\"\"",
            "",
            "        # make sure that instance will go to SOFT_DELETED state instead of",
            "        # deleted immediately",
            "        self.flags(reclaim_instance_interval=30)",
            "",
            "        hostname = self.compute1.host",
            "        rp_uuid = self._get_provider_uuid_by_host(hostname)",
            "",
            "        server = self._boot_and_check_allocations(self.flavor1, hostname)",
            "",
            "        self._soft_delete_and_check_allocation(server, hostname)",
            "",
            "        # advance the time and run periodic reclaim, instance should be deleted",
            "        # and resources should be freed",
            "        the_past = timeutils.utcnow() + datetime.timedelta(hours=1)",
            "        timeutils.set_time_override(override_time=the_past)",
            "        self.addCleanup(timeutils.clear_time_override)",
            "        ctxt = context.get_admin_context()",
            "        self.compute1._reclaim_queued_deletes(ctxt)",
            "",
            "        # Wait for real deletion",
            "        self._wait_until_deleted(server)",
            "",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertEqual({'VCPU': 0,",
            "                          'MEMORY_MB': 0,",
            "                          'DISK_GB': 0}, usages)",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(0, len(allocations))",
            "",
            "    def test_soft_delete_then_restore(self):",
            "        \"\"\"Asserts that restoring a soft deleted instance keeps the proper",
            "        allocation in placement.",
            "        \"\"\"",
            "",
            "        # make sure that instance will go to SOFT_DELETED state instead of",
            "        # deleted immediately",
            "        self.flags(reclaim_instance_interval=30)",
            "",
            "        hostname = self.compute1.host",
            "        rp_uuid = self._get_provider_uuid_by_host(hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, hostname)",
            "",
            "        self._soft_delete_and_check_allocation(server, hostname)",
            "",
            "        post = {'restore': {}}",
            "        self.api.post_server_action(server['id'], post)",
            "        server = self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        # after restore the allocations should be kept",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, allocation)",
            "",
            "        # Now we want a real delete",
            "        self.flags(reclaim_instance_interval=0)",
            "        self._delete_and_check_allocations(server)"
        ],
        "afterPatchFile": [
            "# Copyright 2011 Justin Santa Barbara",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "import datetime",
            "import time",
            "import zlib",
            "",
            "import mock",
            "from oslo_log import log as logging",
            "from oslo_serialization import base64",
            "from oslo_utils import timeutils",
            "",
            "from nova.compute import api as compute_api",
            "from nova.compute import instance_actions",
            "from nova.compute import rpcapi",
            "from nova import context",
            "from nova import db",
            "from nova import exception",
            "from nova import objects",
            "from nova.objects import block_device as block_device_obj",
            "from nova import test",
            "from nova.tests import fixtures as nova_fixtures",
            "from nova.tests.functional.api import client",
            "from nova.tests.functional import integrated_helpers",
            "from nova.tests.unit.api.openstack import fakes",
            "from nova.tests.unit import fake_block_device",
            "from nova.tests.unit import fake_network",
            "from nova.tests.unit import fake_notifier",
            "import nova.tests.unit.image.fake",
            "from nova.tests.unit import policy_fixture",
            "from nova.virt import fake",
            "from nova import volume",
            "",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "class ServersTestBase(integrated_helpers._IntegratedTestBase):",
            "    api_major_version = 'v2'",
            "    _force_delete_parameter = 'forceDelete'",
            "    _image_ref_parameter = 'imageRef'",
            "    _flavor_ref_parameter = 'flavorRef'",
            "    _access_ipv4_parameter = 'accessIPv4'",
            "    _access_ipv6_parameter = 'accessIPv6'",
            "    _return_resv_id_parameter = 'return_reservation_id'",
            "    _min_count_parameter = 'min_count'",
            "",
            "    def setUp(self):",
            "        super(ServersTestBase, self).setUp()",
            "        # The network service is called as part of server creates but no",
            "        # networks have been populated in the db, so stub the methods.",
            "        # The networks aren't relevant to what is being tested.",
            "        fake_network.set_stub_network_methods(self)",
            "        self.conductor = self.start_service(",
            "            'conductor', manager='nova.conductor.manager.ConductorManager')",
            "",
            "    def _wait_for_state_change(self, server, from_status):",
            "        for i in range(0, 50):",
            "            server = self.api.get_server(server['id'])",
            "            if server['status'] != from_status:",
            "                break",
            "            time.sleep(.1)",
            "",
            "        return server",
            "",
            "    def _wait_for_deletion(self, server_id):",
            "        # Wait (briefly) for deletion",
            "        for _retries in range(50):",
            "            try:",
            "                found_server = self.api.get_server(server_id)",
            "            except client.OpenStackApiNotFoundException:",
            "                found_server = None",
            "                LOG.debug(\"Got 404, proceeding\")",
            "                break",
            "",
            "            LOG.debug(\"Found_server=%s\", found_server)",
            "",
            "            # TODO(justinsb): Mock doesn't yet do accurate state changes",
            "            # if found_server['status'] != 'deleting':",
            "            #    break",
            "            time.sleep(.1)",
            "",
            "        # Should be gone",
            "        self.assertFalse(found_server)",
            "",
            "    def _delete_server(self, server_id):",
            "        # Delete the server",
            "        self.api.delete_server(server_id)",
            "        self._wait_for_deletion(server_id)",
            "",
            "    def _get_access_ips_params(self):",
            "        return {self._access_ipv4_parameter: \"172.19.0.2\",",
            "                self._access_ipv6_parameter: \"fe80::2\"}",
            "",
            "    def _verify_access_ips(self, server):",
            "        self.assertEqual('172.19.0.2',",
            "                         server[self._access_ipv4_parameter])",
            "        self.assertEqual('fe80::2', server[self._access_ipv6_parameter])",
            "",
            "",
            "class ServersTest(ServersTestBase):",
            "",
            "    def test_get_servers(self):",
            "        # Simple check that listing servers works.",
            "        servers = self.api.get_servers()",
            "        for server in servers:",
            "            LOG.debug(\"server: %s\", server)",
            "",
            "    def test_create_server_with_error(self):",
            "        # Create a server which will enter error state.",
            "",
            "        def throw_error(*args, **kwargs):",
            "            raise exception.BuildAbortException(reason='',",
            "                    instance_uuid='fake')",
            "",
            "        self.stub_out('nova.virt.fake.FakeDriver.spawn', throw_error)",
            "",
            "        server = self._build_minimal_create_server_request()",
            "        created_server = self.api.post_server({\"server\": server})",
            "        created_server_id = created_server['id']",
            "",
            "        found_server = self.api.get_server(created_server_id)",
            "        self.assertEqual(created_server_id, found_server['id'])",
            "",
            "        found_server = self._wait_for_state_change(found_server, 'BUILD')",
            "",
            "        self.assertEqual('ERROR', found_server['status'])",
            "        self._delete_server(created_server_id)",
            "",
            "    def test_create_and_delete_server(self):",
            "        # Creates and deletes a server.",
            "",
            "        # Create server",
            "        # Build the server data gradually, checking errors along the way",
            "        server = {}",
            "        good_server = self._build_minimal_create_server_request()",
            "",
            "        post = {'server': server}",
            "",
            "        # Without an imageRef, this throws 500.",
            "        # TODO(justinsb): Check whatever the spec says should be thrown here",
            "        self.assertRaises(client.OpenStackApiException,",
            "                          self.api.post_server, post)",
            "",
            "        # With an invalid imageRef, this throws 500.",
            "        server[self._image_ref_parameter] = self.get_invalid_image()",
            "        # TODO(justinsb): Check whatever the spec says should be thrown here",
            "        self.assertRaises(client.OpenStackApiException,",
            "                          self.api.post_server, post)",
            "",
            "        # Add a valid imageRef",
            "        server[self._image_ref_parameter] = good_server.get(",
            "            self._image_ref_parameter)",
            "",
            "        # Without flavorRef, this throws 500",
            "        # TODO(justinsb): Check whatever the spec says should be thrown here",
            "        self.assertRaises(client.OpenStackApiException,",
            "                          self.api.post_server, post)",
            "",
            "        server[self._flavor_ref_parameter] = good_server.get(",
            "            self._flavor_ref_parameter)",
            "",
            "        # Without a name, this throws 500",
            "        # TODO(justinsb): Check whatever the spec says should be thrown here",
            "        self.assertRaises(client.OpenStackApiException,",
            "                          self.api.post_server, post)",
            "",
            "        # Set a valid server name",
            "        server['name'] = good_server['name']",
            "",
            "        created_server = self.api.post_server(post)",
            "        LOG.debug(\"created_server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "        created_server_id = created_server['id']",
            "",
            "        # Check it's there",
            "        found_server = self.api.get_server(created_server_id)",
            "        self.assertEqual(created_server_id, found_server['id'])",
            "",
            "        # It should also be in the all-servers list",
            "        servers = self.api.get_servers()",
            "        server_ids = [s['id'] for s in servers]",
            "        self.assertIn(created_server_id, server_ids)",
            "",
            "        found_server = self._wait_for_state_change(found_server, 'BUILD')",
            "        # It should be available...",
            "        # TODO(justinsb): Mock doesn't yet do this...",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "        servers = self.api.get_servers(detail=True)",
            "        for server in servers:",
            "            self.assertIn(\"image\", server)",
            "            self.assertIn(\"flavor\", server)",
            "",
            "        self._delete_server(created_server_id)",
            "",
            "    def _force_reclaim(self):",
            "        # Make sure that compute manager thinks the instance is",
            "        # old enough to be expired",
            "        the_past = timeutils.utcnow() + datetime.timedelta(hours=1)",
            "        timeutils.set_time_override(override_time=the_past)",
            "        self.addCleanup(timeutils.clear_time_override)",
            "        ctxt = context.get_admin_context()",
            "        self.compute._reclaim_queued_deletes(ctxt)",
            "",
            "    def test_deferred_delete(self):",
            "        # Creates, deletes and waits for server to be reclaimed.",
            "        self.flags(reclaim_instance_interval=1)",
            "",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "",
            "        created_server = self.api.post_server({'server': server})",
            "        LOG.debug(\"created_server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "        created_server_id = created_server['id']",
            "",
            "        # Wait for it to finish being created",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "",
            "        # It should be available...",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Cannot restore unless instance is deleted",
            "        self.assertRaises(client.OpenStackApiException,",
            "                          self.api.post_server_action, created_server_id,",
            "                          {'restore': {}})",
            "",
            "        # Delete the server",
            "        self.api.delete_server(created_server_id)",
            "",
            "        # Wait for queued deletion",
            "        found_server = self._wait_for_state_change(found_server, 'ACTIVE')",
            "        self.assertEqual('SOFT_DELETED', found_server['status'])",
            "",
            "        self._force_reclaim()",
            "",
            "        # Wait for real deletion",
            "        self._wait_for_deletion(created_server_id)",
            "",
            "    def test_deferred_delete_restore(self):",
            "        # Creates, deletes and restores a server.",
            "        self.flags(reclaim_instance_interval=3600)",
            "",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "",
            "        created_server = self.api.post_server({'server': server})",
            "        LOG.debug(\"created_server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "        created_server_id = created_server['id']",
            "",
            "        # Wait for it to finish being created",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "",
            "        # It should be available...",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Delete the server",
            "        self.api.delete_server(created_server_id)",
            "",
            "        # Wait for queued deletion",
            "        found_server = self._wait_for_state_change(found_server, 'ACTIVE')",
            "        self.assertEqual('SOFT_DELETED', found_server['status'])",
            "",
            "        # Restore server",
            "        self.api.post_server_action(created_server_id, {'restore': {}})",
            "",
            "        # Wait for server to become active again",
            "        found_server = self._wait_for_state_change(found_server, 'DELETED')",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "    def test_deferred_delete_restore_overquota(self):",
            "        # Test that a restore that would put the user over quota fails",
            "        self.flags(instances=1, group='quota')",
            "        # Creates, deletes and restores a server.",
            "        self.flags(reclaim_instance_interval=3600)",
            "",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "",
            "        created_server1 = self.api.post_server({'server': server})",
            "        LOG.debug(\"created_server: %s\", created_server1)",
            "        self.assertTrue(created_server1['id'])",
            "        created_server_id1 = created_server1['id']",
            "",
            "        # Wait for it to finish being created",
            "        found_server1 = self._wait_for_state_change(created_server1, 'BUILD')",
            "",
            "        # It should be available...",
            "        self.assertEqual('ACTIVE', found_server1['status'])",
            "",
            "        # Delete the server",
            "        self.api.delete_server(created_server_id1)",
            "",
            "        # Wait for queued deletion",
            "        found_server1 = self._wait_for_state_change(found_server1, 'ACTIVE')",
            "        self.assertEqual('SOFT_DELETED', found_server1['status'])",
            "",
            "        # Create a second server",
            "        server = self._build_minimal_create_server_request()",
            "",
            "        created_server2 = self.api.post_server({'server': server})",
            "        LOG.debug(\"created_server: %s\", created_server2)",
            "        self.assertTrue(created_server2['id'])",
            "",
            "        # Wait for it to finish being created",
            "        found_server2 = self._wait_for_state_change(created_server2, 'BUILD')",
            "",
            "        # It should be available...",
            "        self.assertEqual('ACTIVE', found_server2['status'])",
            "",
            "        # Try to restore the first server, it should fail",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.post_server_action,",
            "                               created_server_id1, {'restore': {}})",
            "        self.assertEqual(403, ex.response.status_code)",
            "        self.assertEqual('SOFT_DELETED', found_server1['status'])",
            "",
            "    def test_deferred_delete_force(self):",
            "        # Creates, deletes and force deletes a server.",
            "        self.flags(reclaim_instance_interval=3600)",
            "",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "",
            "        created_server = self.api.post_server({'server': server})",
            "        LOG.debug(\"created_server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "        created_server_id = created_server['id']",
            "",
            "        # Wait for it to finish being created",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "",
            "        # It should be available...",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Delete the server",
            "        self.api.delete_server(created_server_id)",
            "",
            "        # Wait for queued deletion",
            "        found_server = self._wait_for_state_change(found_server, 'ACTIVE')",
            "        self.assertEqual('SOFT_DELETED', found_server['status'])",
            "",
            "        # Force delete server",
            "        self.api.post_server_action(created_server_id,",
            "                                    {self._force_delete_parameter: {}})",
            "",
            "        # Wait for real deletion",
            "        self._wait_for_deletion(created_server_id)",
            "",
            "    def test_create_server_with_metadata(self):",
            "        # Creates a server with metadata.",
            "",
            "        # Build the server data gradually, checking errors along the way",
            "        server = self._build_minimal_create_server_request()",
            "",
            "        metadata = {}",
            "        for i in range(30):",
            "            metadata['key_%s' % i] = 'value_%s' % i",
            "",
            "        server['metadata'] = metadata",
            "",
            "        post = {'server': server}",
            "        created_server = self.api.post_server(post)",
            "        LOG.debug(\"created_server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "        created_server_id = created_server['id']",
            "",
            "        found_server = self.api.get_server(created_server_id)",
            "        self.assertEqual(created_server_id, found_server['id'])",
            "        self.assertEqual(metadata, found_server.get('metadata'))",
            "",
            "        # The server should also be in the all-servers details list",
            "        servers = self.api.get_servers(detail=True)",
            "        server_map = {server['id']: server for server in servers}",
            "        found_server = server_map.get(created_server_id)",
            "        self.assertTrue(found_server)",
            "        # Details do include metadata",
            "        self.assertEqual(metadata, found_server.get('metadata'))",
            "",
            "        # The server should also be in the all-servers summary list",
            "        servers = self.api.get_servers(detail=False)",
            "        server_map = {server['id']: server for server in servers}",
            "        found_server = server_map.get(created_server_id)",
            "        self.assertTrue(found_server)",
            "        # Summary should not include metadata",
            "        self.assertFalse(found_server.get('metadata'))",
            "",
            "        # Cleanup",
            "        self._delete_server(created_server_id)",
            "",
            "    def test_server_metadata_actions_negative_invalid_state(self):",
            "        # Create server with metadata",
            "        server = self._build_minimal_create_server_request()",
            "",
            "        metadata = {'key_1': 'value_1'}",
            "",
            "        server['metadata'] = metadata",
            "",
            "        post = {'server': server}",
            "        created_server = self.api.post_server(post)",
            "",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "        self.assertEqual(metadata, found_server.get('metadata'))",
            "        server_id = found_server['id']",
            "",
            "        # Change status from ACTIVE to SHELVED for negative test",
            "        self.flags(shelved_offload_time = -1)",
            "        self.api.post_server_action(server_id, {'shelve': {}})",
            "        found_server = self._wait_for_state_change(found_server, 'ACTIVE')",
            "        self.assertEqual('SHELVED', found_server['status'])",
            "",
            "        metadata = {'key_2': 'value_2'}",
            "",
            "        # Update Metadata item in SHELVED (not ACTIVE, etc.)",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.post_server_metadata,",
            "                               server_id, metadata)",
            "        self.assertEqual(409, ex.response.status_code)",
            "        self.assertEqual('SHELVED', found_server['status'])",
            "",
            "        # Delete Metadata item in SHELVED (not ACTIVE, etc.)",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.delete_server_metadata,",
            "                               server_id, 'key_1')",
            "        self.assertEqual(409, ex.response.status_code)",
            "        self.assertEqual('SHELVED', found_server['status'])",
            "",
            "        # Cleanup",
            "        self._delete_server(server_id)",
            "",
            "    def test_create_and_rebuild_server(self):",
            "        # Rebuild a server with metadata.",
            "",
            "        # create a server with initially has no metadata",
            "        server = self._build_minimal_create_server_request()",
            "        server_post = {'server': server}",
            "",
            "        metadata = {}",
            "        for i in range(30):",
            "            metadata['key_%s' % i] = 'value_%s' % i",
            "",
            "        server_post['server']['metadata'] = metadata",
            "",
            "        created_server = self.api.post_server(server_post)",
            "        LOG.debug(\"created_server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "        created_server_id = created_server['id']",
            "",
            "        created_server = self._wait_for_state_change(created_server, 'BUILD')",
            "",
            "        # rebuild the server with metadata and other server attributes",
            "        post = {}",
            "        post['rebuild'] = {",
            "            self._image_ref_parameter: \"76fa36fc-c930-4bf3-8c8a-ea2a2420deb6\",",
            "            \"name\": \"blah\",",
            "            self._access_ipv4_parameter: \"172.19.0.2\",",
            "            self._access_ipv6_parameter: \"fe80::2\",",
            "            \"metadata\": {'some': 'thing'},",
            "        }",
            "        post['rebuild'].update(self._get_access_ips_params())",
            "",
            "        self.api.post_server_action(created_server_id, post)",
            "        LOG.debug(\"rebuilt server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "",
            "        found_server = self.api.get_server(created_server_id)",
            "        self.assertEqual(created_server_id, found_server['id'])",
            "        self.assertEqual({'some': 'thing'}, found_server.get('metadata'))",
            "        self.assertEqual('blah', found_server.get('name'))",
            "        self.assertEqual(post['rebuild'][self._image_ref_parameter],",
            "                         found_server.get('image')['id'])",
            "        self._verify_access_ips(found_server)",
            "",
            "        # rebuild the server with empty metadata and nothing else",
            "        post = {}",
            "        post['rebuild'] = {",
            "            self._image_ref_parameter: \"76fa36fc-c930-4bf3-8c8a-ea2a2420deb6\",",
            "            \"metadata\": {},",
            "        }",
            "",
            "        self.api.post_server_action(created_server_id, post)",
            "        LOG.debug(\"rebuilt server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "",
            "        found_server = self.api.get_server(created_server_id)",
            "        self.assertEqual(created_server_id, found_server['id'])",
            "        self.assertEqual({}, found_server.get('metadata'))",
            "        self.assertEqual('blah', found_server.get('name'))",
            "        self.assertEqual(post['rebuild'][self._image_ref_parameter],",
            "                         found_server.get('image')['id'])",
            "        self._verify_access_ips(found_server)",
            "",
            "        # Cleanup",
            "        self._delete_server(created_server_id)",
            "",
            "    def test_rename_server(self):",
            "        # Test building and renaming a server.",
            "",
            "        # Create a server",
            "        server = self._build_minimal_create_server_request()",
            "        created_server = self.api.post_server({'server': server})",
            "        LOG.debug(\"created_server: %s\", created_server)",
            "        server_id = created_server['id']",
            "        self.assertTrue(server_id)",
            "",
            "        # Rename the server to 'new-name'",
            "        self.api.put_server(server_id, {'server': {'name': 'new-name'}})",
            "",
            "        # Check the name of the server",
            "        created_server = self.api.get_server(server_id)",
            "        self.assertEqual(created_server['name'], 'new-name')",
            "",
            "        # Cleanup",
            "        self._delete_server(server_id)",
            "",
            "    def test_create_multiple_servers(self):",
            "        # Creates multiple servers and checks for reservation_id.",
            "",
            "        # Create 2 servers, setting 'return_reservation_id, which should",
            "        # return a reservation_id",
            "        server = self._build_minimal_create_server_request()",
            "        server[self._min_count_parameter] = 2",
            "        server[self._return_resv_id_parameter] = True",
            "        post = {'server': server}",
            "        response = self.api.post_server(post)",
            "        self.assertIn('reservation_id', response)",
            "        reservation_id = response['reservation_id']",
            "        self.assertNotIn(reservation_id, ['', None])",
            "",
            "        # Create 1 more server, which should not return a reservation_id",
            "        server = self._build_minimal_create_server_request()",
            "        post = {'server': server}",
            "        created_server = self.api.post_server(post)",
            "        self.assertTrue(created_server['id'])",
            "        created_server_id = created_server['id']",
            "",
            "        # lookup servers created by the first request.",
            "        servers = self.api.get_servers(detail=True,",
            "                search_opts={'reservation_id': reservation_id})",
            "        server_map = {server['id']: server for server in servers}",
            "        found_server = server_map.get(created_server_id)",
            "        # The server from the 2nd request should not be there.",
            "        self.assertIsNone(found_server)",
            "        # Should have found 2 servers.",
            "        self.assertEqual(len(server_map), 2)",
            "",
            "        # Cleanup",
            "        self._delete_server(created_server_id)",
            "        for server_id in server_map:",
            "            self._delete_server(server_id)",
            "",
            "    def test_create_server_with_injected_files(self):",
            "        # Creates a server with injected_files.",
            "        personality = []",
            "",
            "        # Inject a text file",
            "        data = 'Hello, World!'",
            "        personality.append({",
            "            'path': '/helloworld.txt',",
            "            'contents': base64.encode_as_bytes(data),",
            "        })",
            "",
            "        # Inject a binary file",
            "        data = zlib.compress(b'Hello, World!')",
            "        personality.append({",
            "            'path': '/helloworld.zip',",
            "            'contents': base64.encode_as_bytes(data),",
            "        })",
            "",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "        server['personality'] = personality",
            "",
            "        post = {'server': server}",
            "",
            "        created_server = self.api.post_server(post)",
            "        LOG.debug(\"created_server: %s\", created_server)",
            "        self.assertTrue(created_server['id'])",
            "        created_server_id = created_server['id']",
            "",
            "        # Check it's there",
            "        found_server = self.api.get_server(created_server_id)",
            "        self.assertEqual(created_server_id, found_server['id'])",
            "",
            "        found_server = self._wait_for_state_change(found_server, 'BUILD')",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Cleanup",
            "        self._delete_server(created_server_id)",
            "",
            "    def test_stop_start_servers_negative_invalid_state(self):",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "        created_server = self.api.post_server({\"server\": server})",
            "        created_server_id = created_server['id']",
            "",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Start server in ACTIVE",
            "        # NOTE(mkoshiya): When os-start API runs, the server status",
            "        # must be SHUTOFF.",
            "        # By returning 409, I want to confirm that the ACTIVE server does not",
            "        # cause unexpected behavior.",
            "        post = {'os-start': {}}",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.post_server_action,",
            "                               created_server_id, post)",
            "        self.assertEqual(409, ex.response.status_code)",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Stop server",
            "        post = {'os-stop': {}}",
            "        self.api.post_server_action(created_server_id, post)",
            "        found_server = self._wait_for_state_change(found_server, 'ACTIVE')",
            "        self.assertEqual('SHUTOFF', found_server['status'])",
            "",
            "        # Stop server in SHUTOFF",
            "        # NOTE(mkoshiya): When os-stop API runs, the server status",
            "        # must be ACTIVE or ERROR.",
            "        # By returning 409, I want to confirm that the SHUTOFF server does not",
            "        # cause unexpected behavior.",
            "        post = {'os-stop': {}}",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.post_server_action,",
            "                               created_server_id, post)",
            "        self.assertEqual(409, ex.response.status_code)",
            "        self.assertEqual('SHUTOFF', found_server['status'])",
            "",
            "        # Cleanup",
            "        self._delete_server(created_server_id)",
            "",
            "    def test_revert_resized_server_negative_invalid_state(self):",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "        created_server = self.api.post_server({\"server\": server})",
            "        created_server_id = created_server['id']",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Revert resized server in ACTIVE",
            "        # NOTE(yatsumi): When revert resized server API runs,",
            "        # the server status must be VERIFY_RESIZE.",
            "        # By returning 409, I want to confirm that the ACTIVE server does not",
            "        # cause unexpected behavior.",
            "        post = {'revertResize': {}}",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.post_server_action,",
            "                               created_server_id, post)",
            "        self.assertEqual(409, ex.response.status_code)",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Cleanup",
            "        self._delete_server(created_server_id)",
            "",
            "    def test_resize_server_negative_invalid_state(self):",
            "        # Avoid migration",
            "        self.flags(allow_resize_to_same_host=True)",
            "",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "        created_server = self.api.post_server({\"server\": server})",
            "        created_server_id = created_server['id']",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Resize server(flavorRef: 1 -> 2)",
            "        post = {'resize': {\"flavorRef\": \"2\", \"OS-DCF:diskConfig\": \"AUTO\"}}",
            "        self.api.post_server_action(created_server_id, post)",
            "        found_server = self._wait_for_state_change(found_server, 'RESIZE')",
            "        self.assertEqual('VERIFY_RESIZE', found_server['status'])",
            "",
            "        # Resize server in VERIFY_RESIZE(flavorRef: 2 -> 1)",
            "        # NOTE(yatsumi): When resize API runs, the server status",
            "        # must be ACTIVE or SHUTOFF.",
            "        # By returning 409, I want to confirm that the VERIFY_RESIZE server",
            "        # does not cause unexpected behavior.",
            "        post = {'resize': {\"flavorRef\": \"1\", \"OS-DCF:diskConfig\": \"AUTO\"}}",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.post_server_action,",
            "                               created_server_id, post)",
            "        self.assertEqual(409, ex.response.status_code)",
            "        self.assertEqual('VERIFY_RESIZE', found_server['status'])",
            "",
            "        # Cleanup",
            "        self._delete_server(created_server_id)",
            "",
            "    def test_confirm_resized_server_negative_invalid_state(self):",
            "        # Create server",
            "        server = self._build_minimal_create_server_request()",
            "        created_server = self.api.post_server({\"server\": server})",
            "        created_server_id = created_server['id']",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Confirm resized server in ACTIVE",
            "        # NOTE(yatsumi): When confirm resized server API runs,",
            "        # the server status must be VERIFY_RESIZE.",
            "        # By returning 409, I want to confirm that the ACTIVE server does not",
            "        # cause unexpected behavior.",
            "        post = {'confirmResize': {}}",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.post_server_action,",
            "                               created_server_id, post)",
            "        self.assertEqual(409, ex.response.status_code)",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Cleanup",
            "        self._delete_server(created_server_id)",
            "",
            "    def test_resize_server_overquota(self):",
            "        self.flags(cores=1, group='quota')",
            "        self.flags(ram=512, group='quota')",
            "        # Create server with default flavor, 1 core, 512 ram",
            "        server = self._build_minimal_create_server_request()",
            "        created_server = self.api.post_server({\"server\": server})",
            "        created_server_id = created_server['id']",
            "",
            "        found_server = self._wait_for_state_change(created_server, 'BUILD')",
            "        self.assertEqual('ACTIVE', found_server['status'])",
            "",
            "        # Try to resize to flavorid 2, 1 core, 2048 ram",
            "        post = {'resize': {'flavorRef': '2'}}",
            "        ex = self.assertRaises(client.OpenStackApiException,",
            "                               self.api.post_server_action,",
            "                               created_server_id, post)",
            "        self.assertEqual(403, ex.response.status_code)",
            "",
            "",
            "class ServersTestV21(ServersTest):",
            "    api_major_version = 'v2.1'",
            "",
            "",
            "class ServersTestV219(ServersTestBase):",
            "    api_major_version = 'v2.1'",
            "",
            "    def _create_server(self, set_desc = True, desc = None):",
            "        server = self._build_minimal_create_server_request()",
            "        if set_desc:",
            "            server['description'] = desc",
            "        post = {'server': server}",
            "        response = self.api.api_post('/servers', post).body",
            "        return (server, response['server'])",
            "",
            "    def _update_server(self, server_id, set_desc = True, desc = None):",
            "        new_name = integrated_helpers.generate_random_alphanumeric(8)",
            "        server = {'server': {'name': new_name}}",
            "        if set_desc:",
            "            server['server']['description'] = desc",
            "        self.api.api_put('/servers/%s' % server_id, server)",
            "",
            "    def _rebuild_server(self, server_id, set_desc = True, desc = None):",
            "        new_name = integrated_helpers.generate_random_alphanumeric(8)",
            "        post = {}",
            "        post['rebuild'] = {",
            "            \"name\": new_name,",
            "            self._image_ref_parameter: \"76fa36fc-c930-4bf3-8c8a-ea2a2420deb6\",",
            "            self._access_ipv4_parameter: \"172.19.0.2\",",
            "            self._access_ipv6_parameter: \"fe80::2\",",
            "            \"metadata\": {'some': 'thing'},",
            "        }",
            "        post['rebuild'].update(self._get_access_ips_params())",
            "        if set_desc:",
            "            post['rebuild']['description'] = desc",
            "        self.api.api_post('/servers/%s/action' % server_id, post)",
            "",
            "    def _create_server_and_verify(self, set_desc = True, expected_desc = None):",
            "        # Creates a server with a description and verifies it is",
            "        # in the GET responses.",
            "        created_server_id = self._create_server(set_desc,",
            "                                                expected_desc)[1]['id']",
            "        self._verify_server_description(created_server_id, expected_desc)",
            "        self._delete_server(created_server_id)",
            "",
            "    def _update_server_and_verify(self, server_id, set_desc = True,",
            "                                  expected_desc = None):",
            "        # Updates a server with a description and verifies it is",
            "        # in the GET responses.",
            "        self._update_server(server_id, set_desc, expected_desc)",
            "        self._verify_server_description(server_id, expected_desc)",
            "",
            "    def _rebuild_server_and_verify(self, server_id, set_desc = True,",
            "                                  expected_desc = None):",
            "        # Rebuilds a server with a description and verifies it is",
            "        # in the GET responses.",
            "        self._rebuild_server(server_id, set_desc, expected_desc)",
            "        self._verify_server_description(server_id, expected_desc)",
            "",
            "    def _verify_server_description(self, server_id, expected_desc = None,",
            "                                   desc_in_resp = True):",
            "        # Calls GET on the servers and verifies that the description",
            "        # is set as expected in the response, or not set at all.",
            "        response = self.api.api_get('/servers/%s' % server_id)",
            "        found_server = response.body['server']",
            "        self.assertEqual(server_id, found_server['id'])",
            "        if desc_in_resp:",
            "            # Verify the description is set as expected (can be None)",
            "            self.assertEqual(expected_desc, found_server.get('description'))",
            "        else:",
            "            # Verify the description is not included in the response.",
            "            self.assertNotIn('description', found_server)",
            "",
            "        servers = self.api.api_get('/servers/detail').body['servers']",
            "        server_map = {server['id']: server for server in servers}",
            "        found_server = server_map.get(server_id)",
            "        self.assertTrue(found_server)",
            "        if desc_in_resp:",
            "            # Verify the description is set as expected (can be None)",
            "            self.assertEqual(expected_desc, found_server.get('description'))",
            "        else:",
            "            # Verify the description is not included in the response.",
            "            self.assertNotIn('description', found_server)",
            "",
            "    def _create_assertRaisesRegex(self, desc):",
            "        # Verifies that a 400 error is thrown on create server",
            "        with self.assertRaisesRegex(client.OpenStackApiException,",
            "                                    \".*Unexpected status code.*\") as cm:",
            "            self._create_server(True, desc)",
            "            self.assertEqual(400, cm.exception.response.status_code)",
            "",
            "    def _update_assertRaisesRegex(self, server_id, desc):",
            "        # Verifies that a 400 error is thrown on update server",
            "        with self.assertRaisesRegex(client.OpenStackApiException,",
            "                                    \".*Unexpected status code.*\") as cm:",
            "            self._update_server(server_id, True, desc)",
            "            self.assertEqual(400, cm.exception.response.status_code)",
            "",
            "    def _rebuild_assertRaisesRegex(self, server_id, desc):",
            "        # Verifies that a 400 error is thrown on rebuild server",
            "        with self.assertRaisesRegex(client.OpenStackApiException,",
            "                                    \".*Unexpected status code.*\") as cm:",
            "            self._rebuild_server(server_id, True, desc)",
            "            self.assertEqual(400, cm.exception.response.status_code)",
            "",
            "    def test_create_server_with_description(self):",
            "        self.api.microversion = '2.19'",
            "        # Create and get a server with a description",
            "        self._create_server_and_verify(True, 'test description')",
            "        # Create and get a server with an empty description",
            "        self._create_server_and_verify(True, '')",
            "        # Create and get a server with description set to None",
            "        self._create_server_and_verify()",
            "        # Create and get a server without setting the description",
            "        self._create_server_and_verify(False)",
            "",
            "    def test_update_server_with_description(self):",
            "        self.api.microversion = '2.19'",
            "        # Create a server with an initial description",
            "        server_id = self._create_server(True, 'test desc 1')[1]['id']",
            "",
            "        # Update and get the server with a description",
            "        self._update_server_and_verify(server_id, True, 'updated desc')",
            "        # Update and get the server name without changing the description",
            "        self._update_server_and_verify(server_id, False, 'updated desc')",
            "        # Update and get the server with an empty description",
            "        self._update_server_and_verify(server_id, True, '')",
            "        # Update and get the server by removing the description (set to None)",
            "        self._update_server_and_verify(server_id)",
            "        # Update and get the server with a 2nd new description",
            "        self._update_server_and_verify(server_id, True, 'updated desc2')",
            "",
            "        # Cleanup",
            "        self._delete_server(server_id)",
            "",
            "    def test_rebuild_server_with_description(self):",
            "        self.api.microversion = '2.19'",
            "",
            "        # Create a server with an initial description",
            "        server = self._create_server(True, 'test desc 1')[1]",
            "        server_id = server['id']",
            "        self._wait_for_state_change(server, 'BUILD')",
            "",
            "        # Rebuild and get the server with a description",
            "        self._rebuild_server_and_verify(server_id, True, 'updated desc')",
            "        # Rebuild and get the server name without changing the description",
            "        self._rebuild_server_and_verify(server_id, False, 'updated desc')",
            "        # Rebuild and get the server with an empty description",
            "        self._rebuild_server_and_verify(server_id, True, '')",
            "        # Rebuild and get the server by removing the description (set to None)",
            "        self._rebuild_server_and_verify(server_id)",
            "        # Rebuild and get the server with a 2nd new description",
            "        self._rebuild_server_and_verify(server_id, True, 'updated desc2')",
            "",
            "        # Cleanup",
            "        self._delete_server(server_id)",
            "",
            "    def test_version_compatibility(self):",
            "        # Create a server with microversion v2.19 and a description.",
            "        self.api.microversion = '2.19'",
            "        server_id = self._create_server(True, 'test desc 1')[1]['id']",
            "        # Verify that the description is not included on V2.18 GETs",
            "        self.api.microversion = '2.18'",
            "        self._verify_server_description(server_id, desc_in_resp = False)",
            "        # Verify that updating the server with description on V2.18",
            "        # results in a 400 error",
            "        self._update_assertRaisesRegex(server_id, 'test update 2.18')",
            "        # Verify that rebuilding the server with description on V2.18",
            "        # results in a 400 error",
            "        self._rebuild_assertRaisesRegex(server_id, 'test rebuild 2.18')",
            "",
            "        # Cleanup",
            "        self._delete_server(server_id)",
            "",
            "        # Create a server on V2.18 and verify that the description",
            "        # defaults to the name on a V2.19 GET",
            "        server_req, response = self._create_server(False)",
            "        server_id = response['id']",
            "        self.api.microversion = '2.19'",
            "        self._verify_server_description(server_id, server_req['name'])",
            "",
            "        # Cleanup",
            "        self._delete_server(server_id)",
            "",
            "        # Verify that creating a server with description on V2.18",
            "        # results in a 400 error",
            "        self.api.microversion = '2.18'",
            "        self._create_assertRaisesRegex('test create 2.18')",
            "",
            "    def test_description_errors(self):",
            "        self.api.microversion = '2.19'",
            "        # Create servers with invalid descriptions.  These throw 400.",
            "        # Invalid unicode with non-printable control char",
            "        self._create_assertRaisesRegex(u'invalid\\0dstring')",
            "        # Description is longer than 255 chars",
            "        self._create_assertRaisesRegex('x' * 256)",
            "",
            "        # Update and rebuild servers with invalid descriptions.",
            "        # These throw 400.",
            "        server_id = self._create_server(True, \"desc\")[1]['id']",
            "        # Invalid unicode with non-printable control char",
            "        self._update_assertRaisesRegex(server_id, u'invalid\\u0604string')",
            "        self._rebuild_assertRaisesRegex(server_id, u'invalid\\u0604string')",
            "        # Description is longer than 255 chars",
            "        self._update_assertRaisesRegex(server_id, 'x' * 256)",
            "        self._rebuild_assertRaisesRegex(server_id, 'x' * 256)",
            "",
            "",
            "class ServerTestV220(ServersTestBase):",
            "    api_major_version = 'v2.1'",
            "",
            "    def setUp(self):",
            "        super(ServerTestV220, self).setUp()",
            "        self.api.microversion = '2.20'",
            "        fake_network.set_stub_network_methods(self)",
            "        self.ctxt = context.get_admin_context()",
            "",
            "    def _create_server(self):",
            "        server = self._build_minimal_create_server_request()",
            "        post = {'server': server}",
            "        response = self.api.api_post('/servers', post).body",
            "        return (server, response['server'])",
            "",
            "    def _shelve_server(self):",
            "        server = self._create_server()[1]",
            "        server_id = server['id']",
            "        self._wait_for_state_change(server, 'BUILD')",
            "        self.api.post_server_action(server_id, {'shelve': None})",
            "        return self._wait_for_state_change(server, 'ACTIVE')",
            "",
            "    def _get_fake_bdms(self, ctxt):",
            "        return block_device_obj.block_device_make_list(self.ctxt,",
            "                    [fake_block_device.FakeDbBlockDeviceDict(",
            "                    {'device_name': '/dev/vda',",
            "                     'source_type': 'volume',",
            "                     'destination_type': 'volume',",
            "                     'volume_id': '5d721593-f033-4f6d-ab6f-b5b067e61bc4'})])",
            "",
            "    def test_attach_detach_vol_to_shelved_server(self):",
            "        self.flags(shelved_offload_time=-1)",
            "        found_server = self._shelve_server()",
            "        self.assertEqual('SHELVED', found_server['status'])",
            "        server_id = found_server['id']",
            "",
            "        # Test attach volume",
            "        with test.nested(mock.patch.object(compute_api.API,",
            "                                       '_check_attach_and_reserve_volume'),",
            "                         mock.patch.object(rpcapi.ComputeAPI,",
            "                                       'attach_volume')) as (mock_reserve,",
            "                                                             mock_attach):",
            "            volume_attachment = {\"volumeAttachment\": {\"volumeId\":",
            "                                       \"5d721593-f033-4f6d-ab6f-b5b067e61bc4\"}}",
            "            self.api.api_post(",
            "                            '/servers/%s/os-volume_attachments' % (server_id),",
            "                            volume_attachment)",
            "            self.assertTrue(mock_reserve.called)",
            "            self.assertTrue(mock_attach.called)",
            "",
            "        # Test detach volume",
            "        self.stub_out('nova.volume.cinder.API.get', fakes.stub_volume_get)",
            "        with test.nested(mock.patch.object(volume.cinder.API,",
            "                                           'begin_detaching'),",
            "                         mock.patch.object(objects.BlockDeviceMappingList,",
            "                                           'get_by_instance_uuid'),",
            "                         mock.patch.object(rpcapi.ComputeAPI,",
            "                                           'detach_volume')",
            "                         ) as (mock_check, mock_get_bdms, mock_rpc):",
            "",
            "            mock_get_bdms.return_value = self._get_fake_bdms(self.ctxt)",
            "            attachment_id = mock_get_bdms.return_value[0]['volume_id']",
            "",
            "            self.api.api_delete('/servers/%s/os-volume_attachments/%s' %",
            "                            (server_id, attachment_id))",
            "            self.assertTrue(mock_check.called)",
            "            self.assertTrue(mock_rpc.called)",
            "",
            "        self._delete_server(server_id)",
            "",
            "    def test_attach_detach_vol_to_shelved_offloaded_server(self):",
            "        self.flags(shelved_offload_time=0)",
            "        found_server = self._shelve_server()",
            "        self.assertEqual('SHELVED_OFFLOADED', found_server['status'])",
            "        server_id = found_server['id']",
            "",
            "        # Test attach volume",
            "        with test.nested(mock.patch.object(compute_api.API,",
            "                                       '_check_attach_and_reserve_volume'),",
            "                         mock.patch.object(volume.cinder.API,",
            "                                       'attach')) as (mock_reserve, mock_vol):",
            "            volume_attachment = {\"volumeAttachment\": {\"volumeId\":",
            "                                       \"5d721593-f033-4f6d-ab6f-b5b067e61bc4\"}}",
            "            attach_response = self.api.api_post(",
            "                             '/servers/%s/os-volume_attachments' % (server_id),",
            "                             volume_attachment).body['volumeAttachment']",
            "            self.assertTrue(mock_reserve.called)",
            "            self.assertTrue(mock_vol.called)",
            "            self.assertIsNone(attach_response['device'])",
            "",
            "        # Test detach volume",
            "        self.stub_out('nova.volume.cinder.API.get', fakes.stub_volume_get)",
            "        with test.nested(mock.patch.object(volume.cinder.API,",
            "                                           'begin_detaching'),",
            "                         mock.patch.object(objects.BlockDeviceMappingList,",
            "                                           'get_by_instance_uuid'),",
            "                         mock.patch.object(compute_api.API,",
            "                                           '_local_cleanup_bdm_volumes')",
            "                         ) as (mock_check, mock_get_bdms, mock_clean_vols):",
            "",
            "            mock_get_bdms.return_value = self._get_fake_bdms(self.ctxt)",
            "            attachment_id = mock_get_bdms.return_value[0]['volume_id']",
            "            self.api.api_delete('/servers/%s/os-volume_attachments/%s' %",
            "                            (server_id, attachment_id))",
            "            self.assertTrue(mock_check.called)",
            "            self.assertTrue(mock_clean_vols.called)",
            "",
            "        self._delete_server(server_id)",
            "",
            "",
            "class ServerRebuildTestCase(integrated_helpers._IntegratedTestBase,",
            "                            integrated_helpers.InstanceHelperMixin):",
            "    api_major_version = 'v2.1'",
            "    # We have to cap the microversion at 2.38 because that's the max we",
            "    # can use to update image metadata via our compute images proxy API.",
            "    microversion = '2.38'",
            "",
            "    def _disable_compute_for(self, server):",
            "        # Refresh to get its host",
            "        server = self.api.get_server(server['id'])",
            "        host = server['OS-EXT-SRV-ATTR:host']",
            "",
            "        # Disable the service it is on",
            "        self.api_fixture.admin_api.put_service('disable',",
            "                                               {'host': host,",
            "                                                'binary': 'nova-compute'})",
            "",
            "    def test_rebuild_with_image_novalidhost(self):",
            "        \"\"\"Creates a server with an image that is valid for the single compute",
            "        that we have. Then rebuilds the server, passing in an image with",
            "        metadata that does not fit the single compute which should result in",
            "        a NoValidHost error. The ImagePropertiesFilter filter is enabled by",
            "        default so that should filter out the host based on the image meta.",
            "        \"\"\"",
            "",
            "        fake.set_nodes(['host2'])",
            "        self.addCleanup(fake.restore_nodes)",
            "        self.flags(host='host2')",
            "        self.compute2 = self.start_service('compute', host='host2')",
            "",
            "        server_req_body = {",
            "            'server': {",
            "                # We hard-code from a fake image since we can't get images",
            "                # via the compute /images proxy API with microversion > 2.35.",
            "                'imageRef': '155d900f-4e14-4e4c-a73d-069cbf4541e6',",
            "                'flavorRef': '1',   # m1.tiny from DefaultFlavorsFixture,",
            "                'name': 'test_rebuild_with_image_novalidhost',",
            "                # We don't care about networking for this test. This requires",
            "                # microversion >= 2.37.",
            "                'networks': 'none'",
            "            }",
            "        }",
            "        server = self.api.post_server(server_req_body)",
            "        self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        # Disable the host we're on so ComputeFilter would have ruled it out",
            "        # normally",
            "        self._disable_compute_for(server)",
            "",
            "        # Now update the image metadata to be something that won't work with",
            "        # the fake compute driver we're using since the fake driver has an",
            "        # \"x86_64\" architecture.",
            "        rebuild_image_ref = (",
            "            nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)",
            "        self.api.put_image_meta_key(",
            "            rebuild_image_ref, 'hw_architecture', 'unicore32')",
            "        # Now rebuild the server with that updated image and it should result",
            "        # in a NoValidHost failure from the scheduler.",
            "        rebuild_req_body = {",
            "            'rebuild': {",
            "                'imageRef': rebuild_image_ref",
            "            }",
            "        }",
            "        # Since we're using the CastAsCall fixture, the NoValidHost error",
            "        # should actually come back to the API and result in a 500 error.",
            "        # Normally the user would get a 202 response because nova-api RPC casts",
            "        # to nova-conductor which RPC calls the scheduler which raises the",
            "        # NoValidHost. We can mimic the end user way to figure out the failure",
            "        # by looking for the failed 'rebuild' instance action event.",
            "        self.api.api_post('/servers/%s/action' % server['id'],",
            "                          rebuild_req_body, check_response_status=[500])",
            "        # Look for the failed rebuild action.",
            "        self._wait_for_action_fail_completion(",
            "            server, instance_actions.REBUILD, 'rebuild_server',",
            "            # Before microversion 2.51 events are only returned for instance",
            "            # actions if you're an admin.",
            "            self.api_fixture.admin_api)",
            "        # Unfortunately the server's image_ref is updated to be the new image",
            "        # even though the rebuild should not work.",
            "        server = self.api.get_server(server['id'])",
            "        self.assertEqual(rebuild_image_ref, server['image']['id'])",
            "",
            "    def test_rebuild_with_new_image(self):",
            "        \"\"\"Rebuilds a server with a different image which will run it through",
            "        the scheduler to validate the image is still OK with the compute host",
            "        that the instance is running on.",
            "",
            "        Validates that additional resources are not allocated against the",
            "        instance.host in Placement due to the rebuild on same host.",
            "        \"\"\"",
            "        admin_api = self.api_fixture.admin_api",
            "        admin_api.microversion = '2.53'",
            "",
            "        def _get_provider_uuid_by_host(host):",
            "            resp = admin_api.api_get(",
            "                'os-hypervisors?hypervisor_hostname_pattern=%s' % host).body",
            "            return resp['hypervisors'][0]['id']",
            "",
            "        def _get_provider_usages(provider_uuid):",
            "            return self.placement_api.get(",
            "                '/resource_providers/%s/usages' % provider_uuid).body['usages']",
            "",
            "        def _get_allocations_by_server_uuid(server_uuid):",
            "            return self.placement_api.get(",
            "                '/allocations/%s' % server_uuid).body['allocations']",
            "",
            "        def assertFlavorMatchesAllocation(flavor, allocation):",
            "            self.assertEqual(flavor['vcpus'], allocation['VCPU'])",
            "            self.assertEqual(flavor['ram'], allocation['MEMORY_MB'])",
            "            self.assertEqual(flavor['disk'], allocation['DISK_GB'])",
            "",
            "        nodename = self.compute.manager._get_nodename(None)",
            "        rp_uuid = _get_provider_uuid_by_host(nodename)",
            "        # make sure we start with no usage on the compute node",
            "        rp_usages = _get_provider_usages(rp_uuid)",
            "        self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)",
            "",
            "        server_req_body = {",
            "            'server': {",
            "                # We hard-code from a fake image since we can't get images",
            "                # via the compute /images proxy API with microversion > 2.35.",
            "                'imageRef': '155d900f-4e14-4e4c-a73d-069cbf4541e6',",
            "                'flavorRef': '1',   # m1.tiny from DefaultFlavorsFixture,",
            "                'name': 'test_rebuild_with_new_image',",
            "                # We don't care about networking for this test. This requires",
            "                # microversion >= 2.37.",
            "                'networks': 'none'",
            "            }",
            "        }",
            "        server = self.api.post_server(server_req_body)",
            "        self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        flavor = self.api.api_get('/flavors/1').body['flavor']",
            "",
            "        # There should be usage for the server on the compute node now.",
            "        rp_usages = _get_provider_usages(rp_uuid)",
            "        assertFlavorMatchesAllocation(flavor, rp_usages)",
            "        allocs = _get_allocations_by_server_uuid(server['id'])",
            "        self.assertIn(rp_uuid, allocs)",
            "        allocs = allocs[rp_uuid]['resources']",
            "        assertFlavorMatchesAllocation(flavor, allocs)",
            "",
            "        rebuild_image_ref = (",
            "            nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)",
            "        # Now rebuild the server with a different image.",
            "        rebuild_req_body = {",
            "            'rebuild': {",
            "                'imageRef': rebuild_image_ref",
            "            }",
            "        }",
            "        self.api.api_post('/servers/%s/action' % server['id'],",
            "                          rebuild_req_body)",
            "        self._wait_for_server_parameter(",
            "            self.api, server, {'OS-EXT-STS:task_state': None})",
            "",
            "        # The usage and allocations should not have changed.",
            "        rp_usages = _get_provider_usages(rp_uuid)",
            "        assertFlavorMatchesAllocation(flavor, rp_usages)",
            "",
            "        allocs = _get_allocations_by_server_uuid(server['id'])",
            "        self.assertIn(rp_uuid, allocs)",
            "        allocs = allocs[rp_uuid]['resources']",
            "        assertFlavorMatchesAllocation(flavor, allocs)",
            "",
            "",
            "class ProviderUsageBaseTestCase(test.TestCase,",
            "                                integrated_helpers.InstanceHelperMixin):",
            "    \"\"\"Base test class for functional tests that check provider usage",
            "    and consumer allocations in Placement during various operations.",
            "",
            "    Subclasses must define a **compute_driver** attribute for the virt driver",
            "    to use.",
            "",
            "    This class sets up standard fixtures and controller services but does not",
            "    start any compute services, that is left to the subclass.",
            "    \"\"\"",
            "",
            "    microversion = 'latest'",
            "",
            "    def setUp(self):",
            "        self.flags(compute_driver=self.compute_driver)",
            "        super(ProviderUsageBaseTestCase, self).setUp()",
            "",
            "        self.useFixture(policy_fixture.RealPolicyFixture())",
            "        self.useFixture(nova_fixtures.NeutronFixture(self))",
            "        self.useFixture(nova_fixtures.AllServicesCurrent())",
            "",
            "        placement = self.useFixture(nova_fixtures.PlacementFixture())",
            "        self.placement_api = placement.api",
            "        api_fixture = self.useFixture(nova_fixtures.OSAPIFixture(",
            "            api_version='v2.1'))",
            "",
            "        self.admin_api = api_fixture.admin_api",
            "        self.admin_api.microversion = self.microversion",
            "        self.api = self.admin_api",
            "",
            "        # the image fake backend needed for image discovery",
            "        nova.tests.unit.image.fake.stub_out_image_service(self)",
            "",
            "        self.start_service('conductor')",
            "        self.start_service('scheduler')",
            "",
            "        self.addCleanup(nova.tests.unit.image.fake.FakeImageService_reset)",
            "        fake_network.set_stub_network_methods(self)",
            "",
            "        self.computes = {}",
            "",
            "    def _start_compute(self, host):",
            "        \"\"\"Start a nova compute service on the given host",
            "",
            "        :param host: the name of the host that will be associated to the",
            "                     compute service.",
            "        :return: the nova compute service object",
            "        \"\"\"",
            "        fake.set_nodes([host])",
            "        self.addCleanup(fake.restore_nodes)",
            "        self.flags(host=host)",
            "        compute = self.start_service('compute', host=host)",
            "        self.computes[host] = compute",
            "        return compute",
            "",
            "    def _get_provider_uuid_by_host(self, host):",
            "        # NOTE(gibi): the compute node id is the same as the compute node",
            "        # provider uuid on that compute",
            "        resp = self.admin_api.api_get(",
            "            'os-hypervisors?hypervisor_hostname_pattern=%s' % host).body",
            "        return resp['hypervisors'][0]['id']",
            "",
            "    def _get_provider_usages(self, provider_uuid):",
            "        return self.placement_api.get(",
            "            '/resource_providers/%s/usages' % provider_uuid).body['usages']",
            "",
            "    def _get_allocations_by_server_uuid(self, server_uuid):",
            "        return self.placement_api.get(",
            "            '/allocations/%s' % server_uuid).body['allocations']",
            "",
            "    def assertFlavorMatchesAllocation(self, flavor, allocation):",
            "        self.assertEqual(flavor['vcpus'], allocation['VCPU'])",
            "        self.assertEqual(flavor['ram'], allocation['MEMORY_MB'])",
            "        self.assertEqual(flavor['disk'], allocation['DISK_GB'])",
            "",
            "    def assertFlavorsMatchAllocation(self, old_flavor, new_flavor, allocation):",
            "        self.assertEqual(old_flavor['vcpus'] + new_flavor['vcpus'],",
            "                         allocation['VCPU'])",
            "        self.assertEqual(old_flavor['ram'] + new_flavor['ram'],",
            "                         allocation['MEMORY_MB'])",
            "        self.assertEqual(old_flavor['disk'] + new_flavor['disk'],",
            "                         allocation['DISK_GB'])",
            "",
            "    def get_migration_uuid_for_instance(self, instance_uuid):",
            "        # NOTE(danms): This is too much introspection for a test like this, but",
            "        # we can't see the migration uuid from the API, so we just encapsulate",
            "        # the peek behind the curtains here to keep it out of the tests.",
            "        # TODO(danms): Get the migration uuid from the API once it is exposed",
            "        ctxt = context.get_admin_context()",
            "        migrations = db.migration_get_all_by_filters(",
            "            ctxt, {'instance_uuid': instance_uuid})",
            "        self.assertEqual(1, len(migrations),",
            "                         'Test expected a single migration, '",
            "                         'but found %i' % len(migrations))",
            "        return migrations[0].uuid",
            "",
            "    def _boot_and_check_allocations(self, flavor, source_hostname):",
            "        \"\"\"Boot an instance and check that the resource allocation is correct",
            "",
            "        After booting an instance on the given host with a given flavor it",
            "        asserts that both the providers usages and resource allocations match",
            "        with the resources requested in the flavor. It also asserts that",
            "        running the periodic update_available_resource call does not change the",
            "        resource state.",
            "",
            "        :param flavor: the flavor the instance will be booted with",
            "        :param source_hostname: the name of the host the instance will be",
            "                                booted on",
            "        :return: the API representation of the booted instance",
            "        \"\"\"",
            "        server_req = self._build_minimal_create_server_request(",
            "            self.api, 'some-server', flavor_id=flavor['id'],",
            "            image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',",
            "            networks=[])",
            "        server_req['availability_zone'] = 'nova:%s' % source_hostname",
            "        LOG.info('booting on %s', source_hostname)",
            "        created_server = self.api.post_server({'server': server_req})",
            "        server = self._wait_for_state_change(",
            "            self.admin_api, created_server, 'ACTIVE')",
            "",
            "        # Verify that our source host is what the server ended up on",
            "        self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])",
            "",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "",
            "        # Before we run periodics, make sure that we have allocations/usages",
            "        # only on the source host",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(flavor, source_usages)",
            "",
            "        # Check that the other providers has no usage",
            "        for rp_uuid in [self._get_provider_uuid_by_host(hostname)",
            "                        for hostname in self.computes.keys()",
            "                        if hostname != source_hostname]:",
            "            usages = self._get_provider_usages(rp_uuid)",
            "            self.assertEqual({'VCPU': 0,",
            "                              'MEMORY_MB': 0,",
            "                              'DISK_GB': 0}, usages)",
            "",
            "        # Check that the server only allocates resource from the host it is",
            "        # booted on",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations),",
            "                         'No allocation for the server on the host it '",
            "                         'is booted on')",
            "        allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(flavor, allocation)",
            "",
            "        self._run_periodics()",
            "",
            "        # After running the periodics but before we start any other operation,",
            "        # we should have exactly the same allocation/usage information as",
            "        # before running the periodics",
            "",
            "        # Check usages on the selected host after boot",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(flavor, source_usages)",
            "",
            "        # Check that the server only allocates resource from the host it is",
            "        # booted on",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations),",
            "                         'No allocation for the server on the host it '",
            "                         'is booted on')",
            "        allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(flavor, allocation)",
            "",
            "        # Check that the other providers has no usage",
            "        for rp_uuid in [self._get_provider_uuid_by_host(hostname)",
            "                        for hostname in self.computes.keys()",
            "                        if hostname != source_hostname]:",
            "            usages = self._get_provider_usages(rp_uuid)",
            "            self.assertEqual({'VCPU': 0,",
            "                              'MEMORY_MB': 0,",
            "                              'DISK_GB': 0}, usages)",
            "        return server",
            "",
            "    def _delete_and_check_allocations(self, server):",
            "        \"\"\"Delete the instance and asserts that the allocations are cleaned",
            "",
            "        :param server: The API representation of the instance to be deleted",
            "        \"\"\"",
            "",
            "        self.api.delete_server(server['id'])",
            "        self._wait_until_deleted(server)",
            "        # NOTE(gibi): The resource allocation is deleted after the instance is",
            "        # destroyed in the db so wait_until_deleted might return before the",
            "        # the resource are deleted in placement. So we need to wait for the",
            "        # instance.delete.end notification as that is emitted after the",
            "        # resources are freed.",
            "        fake_notifier.wait_for_versioned_notifications('instance.delete.end')",
            "",
            "        for rp_uuid in [self._get_provider_uuid_by_host(hostname)",
            "                        for hostname in self.computes.keys()]:",
            "            usages = self._get_provider_usages(rp_uuid)",
            "            self.assertEqual({'VCPU': 0,",
            "                              'MEMORY_MB': 0,",
            "                              'DISK_GB': 0}, usages)",
            "",
            "        # and no allocations for the deleted server",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(0, len(allocations))",
            "",
            "    def _run_periodics(self):",
            "        \"\"\"Run the update_available_resource task on every compute manager",
            "",
            "        This runs periodics on the computes in an undefined order; some child",
            "        class redefined this function to force a specific order.",
            "        \"\"\"",
            "",
            "        ctx = context.get_admin_context()",
            "        for compute in self.computes.values():",
            "            LOG.info('Running periodic for compute (%s)',",
            "                compute.manager.host)",
            "            compute.manager.update_available_resource(ctx)",
            "        LOG.info('Finished with periodics')",
            "",
            "    def _wait_for_migration_status(self, server, expected_status):",
            "        \"\"\"Waits for a migration record with the given status to be found",
            "        for the given server, else the test fails. The migration record, if",
            "        found, is returned.",
            "        \"\"\"",
            "        for attempt in range(10):",
            "            migrations = self.api.api_get('/os-migrations').body['migrations']",
            "            for migration in migrations:",
            "                if (migration['instance_uuid'] == server['id'] and",
            "                        migration['status'].lower() ==",
            "                        expected_status.lower()):",
            "                    return migration",
            "            time.sleep(0.5)",
            "        self.fail('Timed out waiting for migration with status \"%s\" for '",
            "                  'instance: %s' % (expected_status, server['id']))",
            "",
            "",
            "class ServerMovingTests(ProviderUsageBaseTestCase):",
            "    \"\"\"Tests moving servers while checking the resource allocations and usages",
            "",
            "    These tests use two compute hosts. Boot a server on one of them then try to",
            "    move the server to the other. At every step resource allocation of the",
            "    server and the resource usages of the computes are queried from placement",
            "    API and asserted.",
            "    \"\"\"",
            "",
            "    REQUIRES_LOCKING = True",
            "    # NOTE(danms): The test defaults to using SmallFakeDriver,",
            "    # which only has one vcpu, which can't take the doubled allocation",
            "    # we're now giving it. So, use the bigger MediumFakeDriver here.",
            "    compute_driver = 'fake.MediumFakeDriver'",
            "",
            "    def setUp(self):",
            "        super(ServerMovingTests, self).setUp()",
            "        fake_notifier.stub_notifier(self)",
            "        self.addCleanup(fake_notifier.reset)",
            "",
            "        self.compute1 = self._start_compute(host='host1')",
            "        self.compute2 = self._start_compute(host='host2')",
            "",
            "        flavors = self.api.get_flavors()",
            "        self.flavor1 = flavors[0]",
            "        self.flavor2 = flavors[1]",
            "        # create flavor3 which has less MEMORY_MB but more DISK_GB than flavor2",
            "        flavor_body = {'flavor':",
            "                           {'name': 'test_flavor3',",
            "                            'ram': int(self.flavor2['ram'] / 2),",
            "                            'vcpus': 1,",
            "                            'disk': self.flavor2['disk'] * 2,",
            "                            'id': 'a22d5517-147c-4147-a0d1-e698df5cd4e3'",
            "                            }}",
            "",
            "        self.flavor3 = self.api.post_flavor(flavor_body)",
            "",
            "    def _other_hostname(self, host):",
            "        other_host = {'host1': 'host2',",
            "                      'host2': 'host1'}",
            "        return other_host[host]",
            "",
            "    def _run_periodics(self):",
            "        # NOTE(jaypipes): We always run periodics in the same order: first on",
            "        # compute1, then on compute2. However, we want to test scenarios when",
            "        # the periodics run at different times during mover operations. This is",
            "        # why we have the \"reverse\" tests which simply switch the source and",
            "        # dest host while keeping the order in which we run the",
            "        # periodics. This effectively allows us to test the matrix of timing",
            "        # scenarios during move operations.",
            "        ctx = context.get_admin_context()",
            "        LOG.info('Running periodic for compute1 (%s)',",
            "            self.compute1.manager.host)",
            "        self.compute1.manager.update_available_resource(ctx)",
            "        LOG.info('Running periodic for compute2 (%s)',",
            "            self.compute2.manager.host)",
            "        self.compute2.manager.update_available_resource(ctx)",
            "        LOG.info('Finished with periodics')",
            "",
            "    def test_resize_revert(self):",
            "        self._test_resize_revert(dest_hostname='host1')",
            "",
            "    def test_resize_revert_reverse(self):",
            "        self._test_resize_revert(dest_hostname='host2')",
            "",
            "    def test_resize_confirm(self):",
            "        self._test_resize_confirm(dest_hostname='host1')",
            "",
            "    def test_resize_confirm_reverse(self):",
            "        self._test_resize_confirm(dest_hostname='host2')",
            "",
            "    def _resize_and_check_allocations(self, server, old_flavor, new_flavor,",
            "            source_rp_uuid, dest_rp_uuid):",
            "        # Resize the server and check usages in VERIFY_RESIZE state",
            "        self.flags(allow_resize_to_same_host=False)",
            "        resize_req = {",
            "            'resize': {",
            "                'flavorRef': new_flavor['id']",
            "            }",
            "        }",
            "        self.api.post_server_action(server['id'], resize_req)",
            "        self._wait_for_state_change(self.api, server, 'VERIFY_RESIZE')",
            "",
            "        # OK, so the resize operation has run, but we have not yet confirmed or",
            "        # reverted the resize operation. Before we run periodics, make sure",
            "        # that we have allocations/usages on BOTH the source and the",
            "        # destination hosts.",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(old_flavor, source_usages)",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(new_flavor, dest_usages)",
            "",
            "        # The instance should own the new_flavor allocation against the",
            "        # destination host created by the scheduler",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        dest_alloc = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(new_flavor, dest_alloc)",
            "",
            "        # The migration should own the old_flavor allocation against the",
            "        # source host created by conductor",
            "        migration_uuid = self.get_migration_uuid_for_instance(server['id'])",
            "        allocations = self._get_allocations_by_server_uuid(migration_uuid)",
            "        source_alloc = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(old_flavor, source_alloc)",
            "",
            "        self._run_periodics()",
            "",
            "        # the original host expected to have the old resource usage",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(old_flavor, source_usages)",
            "",
            "        # the dest host expected to have resource allocation based on",
            "        # the new flavor the server is resized to",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(new_flavor, dest_usages)",
            "",
            "        # The instance should own the new_flavor allocation against the",
            "        # destination host",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(new_flavor, dest_allocation)",
            "",
            "        # The migration should own the old_flavor allocation against the",
            "        # source host",
            "        migration_uuid = self.get_migration_uuid_for_instance(server['id'])",
            "        allocations = self._get_allocations_by_server_uuid(migration_uuid)",
            "        source_alloc = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(old_flavor, source_alloc)",
            "",
            "    def _test_resize_revert(self, dest_hostname):",
            "        source_hostname = self._other_hostname(dest_hostname)",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(self.flavor1,",
            "            source_hostname)",
            "",
            "        self._resize_and_check_allocations(server, self.flavor1, self.flavor2,",
            "            source_rp_uuid, dest_rp_uuid)",
            "",
            "        # Revert the resize and check the usages",
            "        post = {'revertResize': None}",
            "        self.api.post_server_action(server['id'], post)",
            "        self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        self._run_periodics()",
            "",
            "        # the original host expected to have the old resource allocation",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertEqual({'VCPU': 0,",
            "                          'MEMORY_MB': 0,",
            "                          'DISK_GB': 0}, dest_usages,",
            "                          'Target host %s still has usage after the resize '",
            "                          'has been reverted' % dest_hostname)",
            "",
            "        # Check that the server only allocates resource from the original host",
            "        self.assertEqual(1, len(allocations))",
            "",
            "        source_allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def _test_resize_confirm(self, dest_hostname):",
            "        source_hostname = self._other_hostname(dest_hostname)",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(self.flavor1,",
            "            source_hostname)",
            "",
            "        self._resize_and_check_allocations(server, self.flavor1, self.flavor2,",
            "            source_rp_uuid, dest_rp_uuid)",
            "",
            "        # Confirm the resize and check the usages",
            "        post = {'confirmResize': None}",
            "        self.api.post_server_action(",
            "            server['id'], post, check_response_status=[204])",
            "        self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        # After confirming, we should have an allocation only on the",
            "        # destination host",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "",
            "        # and the server allocates only from the target host",
            "        self.assertEqual(1, len(allocations))",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "",
            "        # and the target host allocation should be according to the new flavor",
            "        self.assertFlavorMatchesAllocation(self.flavor2, dest_usages)",
            "        self.assertEqual({'VCPU': 0,",
            "                          'MEMORY_MB': 0,",
            "                          'DISK_GB': 0}, source_usages,",
            "                         'The source host %s still has usages after the '",
            "                         'resize has been confirmed' % source_hostname)",
            "",
            "        # and the target host allocation should be according to the new flavor",
            "        self.assertFlavorMatchesAllocation(self.flavor2, dest_usages)",
            "",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor2, dest_allocation)",
            "",
            "        self._run_periodics()",
            "",
            "        # Check we're still accurate after running the periodics",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "",
            "        # and the target host allocation should be according to the new flavor",
            "        self.assertFlavorMatchesAllocation(self.flavor2, dest_usages)",
            "        self.assertEqual({'VCPU': 0,",
            "                          'MEMORY_MB': 0,",
            "                          'DISK_GB': 0}, source_usages,",
            "                          'The source host %s still has usages after the '",
            "                          'resize has been confirmed' % source_hostname)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "",
            "        # and the server allocates only from the target host",
            "        self.assertEqual(1, len(allocations))",
            "",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor2, dest_allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def _resize_to_same_host_and_check_allocations(self, server, old_flavor,",
            "                                                   new_flavor, rp_uuid):",
            "        # Resize the server to the same host and check usages in VERIFY_RESIZE",
            "        # state",
            "        self.flags(allow_resize_to_same_host=True)",
            "        resize_req = {",
            "            'resize': {",
            "                'flavorRef': new_flavor['id']",
            "            }",
            "        }",
            "        self.api.post_server_action(server['id'], resize_req)",
            "        self._wait_for_state_change(self.api, server, 'VERIFY_RESIZE')",
            "",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorsMatchAllocation(old_flavor, new_flavor, usages)",
            "",
            "        # The instance should hold a new_flavor allocation",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(new_flavor, allocation)",
            "",
            "        # The migration should hold an old_flavor allocation",
            "        migration_uuid = self.get_migration_uuid_for_instance(server['id'])",
            "        allocations = self._get_allocations_by_server_uuid(migration_uuid)",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(old_flavor, allocation)",
            "",
            "        # We've resized to the same host and have doubled allocations for both",
            "        # the old and new flavor on the same host. Run the periodic on the",
            "        # compute to see if it tramples on what the scheduler did.",
            "        self._run_periodics()",
            "",
            "        usages = self._get_provider_usages(rp_uuid)",
            "",
            "        # In terms of usage, it's still double on the host because the instance",
            "        # and the migration each hold an allocation for the new and old",
            "        # flavors respectively.",
            "        self.assertFlavorsMatchAllocation(old_flavor, new_flavor, usages)",
            "",
            "        # The instance should hold a new_flavor allocation",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(new_flavor, allocation)",
            "",
            "        # The migration should hold an old_flavor allocation",
            "        allocations = self._get_allocations_by_server_uuid(migration_uuid)",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(old_flavor, allocation)",
            "",
            "    def test_resize_revert_same_host(self):",
            "        # make sure that the test only uses a single host",
            "        compute2_service_id = self.admin_api.get_services(",
            "            host=self.compute2.host, binary='nova-compute')[0]['id']",
            "        self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})",
            "",
            "        hostname = self.compute1.manager.host",
            "        rp_uuid = self._get_provider_uuid_by_host(hostname)",
            "",
            "        server = self._boot_and_check_allocations(self.flavor2, hostname)",
            "",
            "        self._resize_to_same_host_and_check_allocations(",
            "            server, self.flavor2, self.flavor3, rp_uuid)",
            "",
            "        # Revert the resize and check the usages",
            "        post = {'revertResize': None}",
            "        self.api.post_server_action(server['id'], post)",
            "        self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        self._run_periodics()",
            "",
            "        # after revert only allocations due to the old flavor should remain",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor2, usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor2, allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_resize_confirm_same_host(self):",
            "        # make sure that the test only uses a single host",
            "        compute2_service_id = self.admin_api.get_services(",
            "            host=self.compute2.host, binary='nova-compute')[0]['id']",
            "        self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})",
            "",
            "        hostname = self.compute1.manager.host",
            "        rp_uuid = self._get_provider_uuid_by_host(hostname)",
            "",
            "        server = self._boot_and_check_allocations(self.flavor2, hostname)",
            "",
            "        self._resize_to_same_host_and_check_allocations(",
            "            server, self.flavor2, self.flavor3, rp_uuid)",
            "",
            "        # Confirm the resize and check the usages",
            "        post = {'confirmResize': None}",
            "        self.api.post_server_action(",
            "            server['id'], post, check_response_status=[204])",
            "        self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        self._run_periodics()",
            "",
            "        # after confirm only allocations due to the new flavor should remain",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor3, usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor3, allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_resize_not_enough_resource(self):",
            "        # Try to resize to a flavor that requests more VCPU than what the",
            "        # compute hosts has available and expect the resize to fail",
            "",
            "        flavor_body = {'flavor':",
            "                           {'name': 'test_too_big_flavor',",
            "                            'ram': 1024,",
            "                            'vcpus': fake.MediumFakeDriver.vcpus + 1,",
            "                            'disk': 20,",
            "                            }}",
            "",
            "        big_flavor = self.api.post_flavor(flavor_body)",
            "",
            "        dest_hostname = self.compute2.host",
            "        source_hostname = self._other_hostname(dest_hostname)",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        self.flags(allow_resize_to_same_host=False)",
            "        resize_req = {",
            "            'resize': {",
            "                'flavorRef': big_flavor['id']",
            "            }",
            "        }",
            "",
            "        resp = self.api.post_server_action(",
            "            server['id'], resize_req, check_response_status=[400])",
            "        self.assertEqual(",
            "            resp['badRequest']['message'],",
            "            \"No valid host was found. No valid host found for resize\")",
            "        server = self.admin_api.get_server(server['id'])",
            "        self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])",
            "",
            "        # only the source host shall have usages after the failed resize",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        # Check that the other provider has no usage",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertEqual({'VCPU': 0,",
            "                          'MEMORY_MB': 0,",
            "                          'DISK_GB': 0}, dest_usages)",
            "",
            "        # Check that the server only allocates resource from the host it is",
            "        # booted on",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_evacuate(self):",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        source_compute_id = self.admin_api.get_services(",
            "            host=source_hostname, binary='nova-compute')[0]['id']",
            "",
            "        self.compute1.stop()",
            "        # force it down to avoid waiting for the service group to time out",
            "        self.admin_api.put_service(",
            "            source_compute_id, {'forced_down': 'true'})",
            "",
            "        # evacuate the server",
            "        post = {'evacuate': {}}",
            "        self.api.post_server_action(",
            "            server['id'], post)",
            "        expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,",
            "                           'status': 'ACTIVE'}",
            "        server = self._wait_for_server_parameter(self.api, server,",
            "                                                 expected_params)",
            "",
            "        # Expect to have allocation and usages on both computes as the",
            "        # source compute is still down",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(2, len(allocations))",
            "        source_allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_allocation)",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_allocation)",
            "",
            "        # restart the source compute",
            "        self.restart_compute_service(self.compute1)",
            "",
            "        self.admin_api.put_service(",
            "            source_compute_id, {'forced_down': 'false'})",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertEqual({'VCPU': 0,",
            "                          'MEMORY_MB': 0,",
            "                          'DISK_GB': 0},",
            "                         source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_evacuate_forced_host(self):",
            "        \"\"\"Evacuating a server with a forced host bypasses the scheduler",
            "        which means conductor has to create the allocations against the",
            "        destination node. This test recreates the scenarios and asserts",
            "        the allocations on the source and destination nodes are as expected.",
            "        \"\"\"",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        source_compute_id = self.admin_api.get_services(",
            "            host=source_hostname, binary='nova-compute')[0]['id']",
            "",
            "        self.compute1.stop()",
            "        # force it down to avoid waiting for the service group to time out",
            "        self.admin_api.put_service(",
            "            source_compute_id, {'forced_down': 'true'})",
            "",
            "        # evacuate the server and force the destination host which bypasses",
            "        # the scheduler",
            "        post = {",
            "            'evacuate': {",
            "                'host': dest_hostname,",
            "                'force': True",
            "            }",
            "        }",
            "        self.api.post_server_action(server['id'], post)",
            "        expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,",
            "                           'status': 'ACTIVE'}",
            "        server = self._wait_for_server_parameter(self.api, server,",
            "                                                 expected_params)",
            "",
            "        # Run the periodics to show those don't modify allocations.",
            "        self._run_periodics()",
            "",
            "        # Expect to have allocation and usages on both computes as the",
            "        # source compute is still down",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(2, len(allocations))",
            "        source_allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_allocation)",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_allocation)",
            "",
            "        # restart the source compute",
            "        self.restart_compute_service(self.compute1)",
            "        self.admin_api.put_service(",
            "            source_compute_id, {'forced_down': 'false'})",
            "",
            "        # Run the periodics again to show they don't change anything.",
            "        self._run_periodics()",
            "",
            "        # When the source node starts up, the instance has moved so the",
            "        # ResourceTracker should cleanup allocations for the source node.",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertEqual(",
            "            {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, source_usages)",
            "",
            "        # The usages/allocations should still exist on the destination node",
            "        # after the source node starts back up.",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_evacuate_claim_on_dest_fails(self):",
            "        \"\"\"Tests that the allocations on the destination node are cleaned up",
            "        when the rebuild move claim fails due to insufficient resources.",
            "        \"\"\"",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        source_compute_id = self.admin_api.get_services(",
            "            host=source_hostname, binary='nova-compute')[0]['id']",
            "",
            "        self.compute1.stop()",
            "        # force it down to avoid waiting for the service group to time out",
            "        self.admin_api.put_service(",
            "            source_compute_id, {'forced_down': 'true'})",
            "",
            "        # NOTE(mriedem): This isn't great, and I'd like to fake out the driver",
            "        # to make the claim fail, by doing something like returning a too high",
            "        # memory_mb overhead, but the limits dict passed to the claim is empty",
            "        # so the claim test is considering it as unlimited and never actually",
            "        # performs a claim test. Configuring the scheduler to use the RamFilter",
            "        # to get the memory_mb limit at least seems like it should work but",
            "        # it doesn't appear to for some reason...",
            "        def fake_move_claim(*args, **kwargs):",
            "            # Assert the destination node allocation exists.",
            "            dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "            self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "            raise exception.ComputeResourcesUnavailable(",
            "                    reason='test_evacuate_claim_on_dest_fails')",
            "",
            "        with mock.patch('nova.compute.claims.MoveClaim', fake_move_claim):",
            "            # evacuate the server",
            "            self.api.post_server_action(server['id'], {'evacuate': {}})",
            "            # the migration will fail on the dest node and the instance will",
            "            # go into error state",
            "            server = self._wait_for_state_change(self.api, server, 'ERROR')",
            "",
            "        # Run the periodics to show those don't modify allocations.",
            "        self._run_periodics()",
            "",
            "        # The allocation should still exist on the source node since it's",
            "        # still down, and the allocation on the destination node should be",
            "        # cleaned up.",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(",
            "            {'vcpus': 0, 'ram': 0, 'disk': 0}, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        source_allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_allocation)",
            "",
            "        # restart the source compute",
            "        self.restart_compute_service(self.compute1)",
            "        self.admin_api.put_service(",
            "            source_compute_id, {'forced_down': 'false'})",
            "",
            "        # Run the periodics again to show they don't change anything.",
            "        self._run_periodics()",
            "",
            "        # The source compute shouldn't have cleaned up the allocation for",
            "        # itself since the instance didn't move.",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        source_allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_allocation)",
            "",
            "    def test_evacuate_rebuild_on_dest_fails(self):",
            "        \"\"\"Tests that the allocations on the destination node are cleaned up",
            "        automatically when the claim is made but the actual rebuild",
            "        via the driver fails.",
            "",
            "        \"\"\"",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        source_compute_id = self.admin_api.get_services(",
            "            host=source_hostname, binary='nova-compute')[0]['id']",
            "",
            "        self.compute1.stop()",
            "        # force it down to avoid waiting for the service group to time out",
            "        self.admin_api.put_service(",
            "            source_compute_id, {'forced_down': 'true'})",
            "",
            "        def fake_rebuild(*args, **kwargs):",
            "            # Assert the destination node allocation exists.",
            "            dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "            self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "            raise test.TestingException('test_evacuate_rebuild_on_dest_fails')",
            "",
            "        with mock.patch.object(",
            "                self.compute2.driver, 'rebuild', fake_rebuild):",
            "            # evacuate the server",
            "            self.api.post_server_action(server['id'], {'evacuate': {}})",
            "            # the migration will fail on the dest node and the instance will",
            "            # go into error state",
            "            server = self._wait_for_state_change(self.api, server, 'ERROR')",
            "",
            "        # Run the periodics to show those don't modify allocations.",
            "        self._run_periodics()",
            "",
            "        # The allocation should still exist on the source node since it's",
            "        # still down, and the allocation on the destination node should be",
            "        # cleaned up.",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(",
            "            {'vcpus': 0, 'ram': 0, 'disk': 0}, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        source_allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_allocation)",
            "",
            "        # restart the source compute",
            "        self.restart_compute_service(self.compute1)",
            "        self.admin_api.put_service(",
            "            source_compute_id, {'forced_down': 'false'})",
            "",
            "        # Run the periodics again to show they don't change anything.",
            "        self._run_periodics()",
            "",
            "        # The source compute shouldn't have cleaned up the allocation for",
            "        # itself since the instance didn't move.",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        source_allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_allocation)",
            "",
            "    def _boot_then_shelve_and_check_allocations(self, hostname, rp_uuid):",
            "        # avoid automatic shelve offloading",
            "        self.flags(shelved_offload_time=-1)",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, hostname)",
            "        req = {",
            "            'shelve': {}",
            "        }",
            "        self.api.post_server_action(server['id'], req)",
            "        self._wait_for_state_change(self.api, server, 'SHELVED')",
            "        # the host should maintain the existing allocation for this instance",
            "        # while the instance is shelved",
            "        source_usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "        # Check that the server only allocates resource from the host it is",
            "        # booted on",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, allocation)",
            "        return server",
            "",
            "    def test_shelve_unshelve(self):",
            "        source_hostname = self.compute1.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        server = self._boot_then_shelve_and_check_allocations(",
            "            source_hostname, source_rp_uuid)",
            "",
            "        req = {",
            "            'unshelve': {}",
            "        }",
            "        self.api.post_server_action(server['id'], req)",
            "        self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        # the host should have resource usage as the instance is ACTIVE",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        # Check that the server only allocates resource from the host it is",
            "        # booted on",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def _shelve_offload_and_check_allocations(self, server, source_rp_uuid):",
            "        req = {",
            "            'shelveOffload': {}",
            "        }",
            "        self.api.post_server_action(server['id'], req)",
            "        self._wait_for_server_parameter(",
            "            self.api, server, {'status': 'SHELVED_OFFLOADED',",
            "                               'OS-EXT-SRV-ATTR:host': None})",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertEqual({'VCPU': 0,",
            "                          'MEMORY_MB': 0,",
            "                          'DISK_GB': 0},",
            "                         source_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(0, len(allocations))",
            "",
            "    def test_shelve_offload_unshelve_diff_host(self):",
            "        source_hostname = self.compute1.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        server = self._boot_then_shelve_and_check_allocations(",
            "            source_hostname, source_rp_uuid)",
            "",
            "        self._shelve_offload_and_check_allocations(server, source_rp_uuid)",
            "",
            "        # unshelve after shelve offload will do scheduling. this test case",
            "        # wants to test the scenario when the scheduler select a different host",
            "        # to ushelve the instance. So we disable the original host.",
            "        source_service_id = self.admin_api.get_services(",
            "            host=source_hostname, binary='nova-compute')[0]['id']",
            "        self.admin_api.put_service(source_service_id, {'status': 'disabled'})",
            "",
            "        req = {",
            "            'unshelve': {}",
            "        }",
            "        self.api.post_server_action(server['id'], req)",
            "        server = self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "        # unshelving an offloaded instance will call the scheduler so the",
            "        # instance might end up on a different host",
            "        current_hostname = server['OS-EXT-SRV-ATTR:host']",
            "        self.assertEqual(current_hostname, self._other_hostname(",
            "            source_hostname))",
            "",
            "        # the host running the instance should have resource usage",
            "        current_rp_uuid = self._get_provider_uuid_by_host(current_hostname)",
            "        current_usages = self._get_provider_usages(current_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, current_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[current_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_shelve_offload_unshelve_same_host(self):",
            "        source_hostname = self.compute1.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        server = self._boot_then_shelve_and_check_allocations(",
            "            source_hostname, source_rp_uuid)",
            "",
            "        self._shelve_offload_and_check_allocations(server, source_rp_uuid)",
            "",
            "        # unshelve after shelve offload will do scheduling. this test case",
            "        # wants to test the scenario when the scheduler select the same host",
            "        # to ushelve the instance. So we disable the other host.",
            "        source_service_id = self.admin_api.get_services(",
            "            host=self._other_hostname(source_hostname),",
            "            binary='nova-compute')[0]['id']",
            "        self.admin_api.put_service(source_service_id, {'status': 'disabled'})",
            "",
            "        req = {",
            "            'unshelve': {}",
            "        }",
            "        self.api.post_server_action(server['id'], req)",
            "        server = self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "        # unshelving an offloaded instance will call the scheduler so the",
            "        # instance might end up on a different host",
            "        current_hostname = server['OS-EXT-SRV-ATTR:host']",
            "        self.assertEqual(current_hostname, source_hostname)",
            "",
            "        # the host running the instance should have resource usage",
            "        current_rp_uuid = self._get_provider_uuid_by_host(current_hostname)",
            "        current_usages = self._get_provider_usages(current_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, current_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[current_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_live_migrate_force(self):",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "        post = {",
            "            'os-migrateLive': {",
            "                'host': dest_hostname,",
            "                'block_migration': True,",
            "                'force': True,",
            "            }",
            "        }",
            "",
            "        self.api.post_server_action(server['id'], post)",
            "        self._wait_for_server_parameter(self.api, server,",
            "            {'OS-EXT-SRV-ATTR:host': dest_hostname,",
            "             'status': 'ACTIVE'})",
            "",
            "        self._run_periodics()",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        # NOTE(danms): There should be no usage for the source",
            "        self.assertFlavorMatchesAllocation(",
            "            {'ram': 0, 'disk': 0, 'vcpus': 0}, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        # the server has an allocation on only the dest node",
            "        self.assertEqual(1, len(allocations))",
            "        self.assertNotIn(source_rp_uuid, allocations)",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_live_migrate(self):",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "        post = {",
            "            'os-migrateLive': {",
            "                'host': dest_hostname,",
            "                'block_migration': True,",
            "            }",
            "        }",
            "",
            "        self.api.post_server_action(server['id'], post)",
            "        self._wait_for_server_parameter(self.api, server,",
            "                                        {'OS-EXT-SRV-ATTR:host': dest_hostname,",
            "                                         'status': 'ACTIVE'})",
            "",
            "        self._run_periodics()",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        # NOTE(danms): There should be no usage for the source",
            "        self.assertFlavorMatchesAllocation(",
            "            {'ram': 0, 'disk': 0, 'vcpus': 0}, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        self.assertNotIn(source_rp_uuid, allocations)",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_live_migrate_pre_check_fails(self):",
            "        \"\"\"Tests the case that the LiveMigrationTask in conductor has",
            "        called the scheduler which picked a host and created allocations",
            "        against it in Placement, but then when the conductor task calls",
            "        check_can_live_migrate_destination on the destination compute it",
            "        fails. The allocations on the destination compute node should be",
            "        cleaned up before the conductor task asks the scheduler for another",
            "        host to try the live migration.",
            "        \"\"\"",
            "        self.failed_hostname = None",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        def fake_check_can_live_migrate_destination(",
            "                context, instance, src_compute_info, dst_compute_info,",
            "                block_migration=False, disk_over_commit=False):",
            "            self.failed_hostname = dst_compute_info['host']",
            "            raise exception.MigrationPreCheckError(",
            "                reason='test_live_migrate_pre_check_fails')",
            "",
            "        with mock.patch('nova.virt.fake.FakeDriver.'",
            "                        'check_can_live_migrate_destination',",
            "                        side_effect=fake_check_can_live_migrate_destination):",
            "            post = {",
            "                'os-migrateLive': {",
            "                    'host': dest_hostname,",
            "                    'block_migration': True,",
            "                }",
            "            }",
            "            self.api.post_server_action(server['id'], post)",
            "            # As there are only two computes and we failed to live migrate to",
            "            # the only other destination host, the LiveMigrationTask raises",
            "            # MaxRetriesExceeded back to the conductor manager which handles it",
            "            # generically and sets the instance back to ACTIVE status and",
            "            # clears the task_state. The migration record status is set to",
            "            # 'error', so that's what we need to look for to know when this",
            "            # is done.",
            "            migration = self._wait_for_migration_status(server, 'error')",
            "",
            "        # The source_compute should be set on the migration record, but the",
            "        # destination shouldn't be as we never made it to one.",
            "        self.assertEqual(source_hostname, migration['source_compute'])",
            "        self.assertIsNone(migration['dest_compute'])",
            "        # Make sure the destination host (the only other host) is the failed",
            "        # host.",
            "        self.assertEqual(dest_hostname, self.failed_hostname)",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        # Since the instance didn't move, assert the allocations are still",
            "        # on the source node.",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        # Assert the allocations, created by the scheduler, are cleaned up",
            "        # after the migration pre-check error happens.",
            "        self.assertFlavorMatchesAllocation(",
            "            {'vcpus': 0, 'ram': 0, 'disk': 0}, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        # There should only be 1 allocation for the instance on the source node",
            "        self.assertEqual(1, len(allocations))",
            "        self.assertIn(source_rp_uuid, allocations)",
            "        self.assertFlavorMatchesAllocation(",
            "            self.flavor1, allocations[source_rp_uuid]['resources'])",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    @mock.patch('nova.virt.fake.FakeDriver.pre_live_migration',",
            "                # The actual type of exception here doesn't matter. The point",
            "                # is that the virt driver raised an exception from the",
            "                # pre_live_migration method on the destination host.",
            "                side_effect=test.TestingException(",
            "                    'test_live_migrate_rollback_cleans_dest_node_allocations'))",
            "    def test_live_migrate_rollback_cleans_dest_node_allocations(",
            "            self, mock_pre_live_migration):",
            "        \"\"\"Tests the case that when live migration fails, either during the",
            "        call to pre_live_migration on the destination, or during the actual",
            "        live migration in the virt driver, the allocations on the destination",
            "        node are rolled back since the instance is still on the source node.",
            "        \"\"\"",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        post = {",
            "            'os-migrateLive': {",
            "                'host': dest_hostname,",
            "                'block_migration': True,",
            "            }",
            "        }",
            "        self.api.post_server_action(server['id'], post)",
            "        # The compute manager will put the migration record into error status",
            "        # when pre_live_migration fails, so wait for that to happen.",
            "        migration = self._wait_for_migration_status(server, 'error')",
            "        # The _rollback_live_migration method in the compute manager will reset",
            "        # the task_state on the instance, so wait for that to happen.",
            "        server = self._wait_for_server_parameter(",
            "            self.api, server, {'OS-EXT-STS:task_state': None})",
            "",
            "        self.assertEqual(source_hostname, migration['source_compute'])",
            "        self.assertEqual(dest_hostname, migration['dest_compute'])",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        # Since the instance didn't move, assert the allocations are still",
            "        # on the source node.",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        # Assert the allocations, created by the scheduler, are cleaned up",
            "        # after the rollback happens.",
            "        self.assertFlavorMatchesAllocation(",
            "            {'vcpus': 0, 'ram': 0, 'disk': 0}, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        # There should only be 1 allocation for the instance on the source node",
            "        self.assertEqual(1, len(allocations))",
            "        self.assertIn(source_rp_uuid, allocations)",
            "        self.assertFlavorMatchesAllocation(",
            "            self.flavor1, allocations[source_rp_uuid]['resources'])",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_rescheduling_when_migrating_instance(self):",
            "        \"\"\"Tests that allocations are removed from the destination node by",
            "        the compute service when a cold migrate / resize fails and a reschedule",
            "        request is sent back to conductor.",
            "        \"\"\"",
            "        source_hostname = self.compute1.manager.host",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        def fake_prep_resize(*args, **kwargs):",
            "            dest_hostname = self._other_hostname(source_hostname)",
            "            dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "            dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "            self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "            allocations = self._get_allocations_by_server_uuid(server['id'])",
            "            self.assertIn(dest_rp_uuid, allocations)",
            "",
            "            source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "            source_usages = self._get_provider_usages(source_rp_uuid)",
            "            self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "            migration_uuid = self.get_migration_uuid_for_instance(server['id'])",
            "            allocations = self._get_allocations_by_server_uuid(migration_uuid)",
            "            self.assertIn(source_rp_uuid, allocations)",
            "",
            "            raise test.TestingException('Simulated _prep_resize failure.')",
            "",
            "        # Yes this isn't great in a functional test, but it's simple.",
            "        self.stub_out('nova.compute.manager.ComputeManager._prep_resize',",
            "                      fake_prep_resize)",
            "",
            "        # Now migrate the server which is going to fail on the destination.",
            "        self.api.post_server_action(server['id'], {'migrate': None})",
            "",
            "        self._wait_for_action_fail_completion(",
            "            server, instance_actions.MIGRATE, 'compute_prep_resize')",
            "",
            "        dest_hostname = self._other_hostname(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        failed_usages = self._get_provider_usages(dest_rp_uuid)",
            "        # Expects no allocation records on the failed host.",
            "        self.assertFlavorMatchesAllocation(",
            "           {'vcpus': 0, 'ram': 0, 'disk': 0}, failed_usages)",
            "",
            "        # Ensure the allocation records still exist on the source host.",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertIn(source_rp_uuid, allocations)",
            "",
            "    def test_resize_to_same_host_prep_resize_fails(self):",
            "        \"\"\"Tests that when we resize to the same host and resize fails in",
            "        the prep_resize method, we cleanup the allocations before rescheduling.",
            "        \"\"\"",
            "        # make sure that the test only uses a single host",
            "        compute2_service_id = self.admin_api.get_services(",
            "            host=self.compute2.host, binary='nova-compute')[0]['id']",
            "        self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})",
            "",
            "        hostname = self.compute1.manager.host",
            "        rp_uuid = self._get_provider_uuid_by_host(hostname)",
            "",
            "        server = self._boot_and_check_allocations(self.flavor1, hostname)",
            "",
            "        def fake_prep_resize(*args, **kwargs):",
            "            # Ensure the allocations are doubled now before we fail.",
            "            usages = self._get_provider_usages(rp_uuid)",
            "            self.assertFlavorsMatchAllocation(",
            "                self.flavor1, self.flavor2, usages)",
            "            raise test.TestingException('Simulated _prep_resize failure.')",
            "",
            "        # Yes this isn't great in a functional test, but it's simple.",
            "        self.stub_out('nova.compute.manager.ComputeManager._prep_resize',",
            "                      fake_prep_resize)",
            "",
            "        self.flags(allow_resize_to_same_host=True)",
            "        resize_req = {",
            "            'resize': {",
            "                'flavorRef': self.flavor2['id']",
            "            }",
            "        }",
            "        self.api.post_server_action(server['id'], resize_req)",
            "",
            "        self._wait_for_action_fail_completion(",
            "            server, instance_actions.RESIZE, 'compute_prep_resize')",
            "",
            "        # Ensure the allocation records still exist on the host.",
            "        source_rp_uuid = self._get_provider_uuid_by_host(hostname)",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        # The new_flavor should have been subtracted from the doubled",
            "        # allocation which just leaves us with the original flavor.",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "",
            "class ServerLiveMigrateForceAndAbort(ProviderUsageBaseTestCase):",
            "    \"\"\"Test Server live migrations, which delete the migration or",
            "    force_complete it, and check the allocations after the operations.",
            "",
            "    The test are using fakedriver to handle the force_completion and deletion",
            "    of live migration.",
            "    \"\"\"",
            "",
            "    compute_driver = 'fake.FakeLiveMigrateDriver'",
            "",
            "    def setUp(self):",
            "        super(ServerLiveMigrateForceAndAbort, self).setUp()",
            "",
            "        self.compute1 = self._start_compute(host='host1')",
            "        self.compute2 = self._start_compute(host='host2')",
            "",
            "        flavors = self.api.get_flavors()",
            "        self.flavor1 = flavors[0]",
            "",
            "    def test_live_migrate_force_complete(self):",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        post = {",
            "            'os-migrateLive': {",
            "                'host': dest_hostname,",
            "                'block_migration': True,",
            "            }",
            "        }",
            "        self.api.post_server_action(server['id'], post)",
            "",
            "        migration = self._wait_for_migration_status(server, 'running')",
            "        self.api.force_complete_migration(server['id'],",
            "                                          migration['id'])",
            "",
            "        self._wait_for_server_parameter(self.api, server,",
            "                                        {'OS-EXT-SRV-ATTR:host': dest_hostname,",
            "                                         'status': 'ACTIVE'})",
            "",
            "        self._run_periodics()",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(",
            "            {'ram': 0, 'disk': 0, 'vcpus': 0}, source_usages)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        self.assertNotIn(source_rp_uuid, allocations)",
            "",
            "        dest_allocation = allocations[dest_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_allocation)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "    def test_live_migrate_delete(self):",
            "        source_hostname = self.compute1.host",
            "        dest_hostname = self.compute2.host",
            "        source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, source_hostname)",
            "",
            "        post = {",
            "            'os-migrateLive': {",
            "                'host': dest_hostname,",
            "                'block_migration': True,",
            "            }",
            "        }",
            "        self.api.post_server_action(server['id'], post)",
            "",
            "        migration = self._wait_for_migration_status(server, 'running')",
            "",
            "        self.api.delete_migration(server['id'], migration['id'])",
            "        self._wait_for_server_parameter(self.api, server,",
            "            {'OS-EXT-SRV-ATTR:host': source_hostname,",
            "             'status': 'ACTIVE'})",
            "",
            "        self._run_periodics()",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        self.assertNotIn(dest_rp_uuid, allocations)",
            "",
            "        source_usages = self._get_provider_usages(source_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_usages)",
            "",
            "        source_allocation = allocations[source_rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, source_allocation)",
            "",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(",
            "           {'ram': 0, 'disk': 0, 'vcpus': 0}, dest_usages)",
            "",
            "        self._delete_and_check_allocations(server)",
            "",
            "",
            "class ServerRescheduleTests(ProviderUsageBaseTestCase):",
            "    \"\"\"Tests server create scenarios which trigger a reschedule during",
            "    a server build and validates that allocations in Placement",
            "    are properly cleaned up.",
            "",
            "    Uses a fake virt driver that fails the build on the first attempt.",
            "    \"\"\"",
            "",
            "    compute_driver = 'fake.FakeRescheduleDriver'",
            "",
            "    def setUp(self):",
            "        super(ServerRescheduleTests, self).setUp()",
            "        self.compute1 = self._start_compute(host='host1')",
            "        self.compute2 = self._start_compute(host='host2')",
            "",
            "        flavors = self.api.get_flavors()",
            "        self.flavor1 = flavors[0]",
            "",
            "    def _other_hostname(self, host):",
            "        other_host = {'host1': 'host2',",
            "                      'host2': 'host1'}",
            "        return other_host[host]",
            "",
            "    def test_rescheduling_when_booting_instance(self):",
            "        \"\"\"Tests that allocations, created by the scheduler, are cleaned",
            "        from the source node when the build fails on that node and is",
            "        rescheduled to another node.",
            "        \"\"\"",
            "        server_req = self._build_minimal_create_server_request(",
            "                self.api, 'some-server', flavor_id=self.flavor1['id'],",
            "                image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',",
            "                networks=[])",
            "",
            "        created_server = self.api.post_server({'server': server_req})",
            "        server = self._wait_for_state_change(",
            "                self.api, created_server, 'ACTIVE')",
            "        dest_hostname = server['OS-EXT-SRV-ATTR:host']",
            "        failed_hostname = self._other_hostname(dest_hostname)",
            "",
            "        LOG.info('failed on %s', failed_hostname)",
            "        LOG.info('booting on %s', dest_hostname)",
            "",
            "        failed_rp_uuid = self._get_provider_uuid_by_host(failed_hostname)",
            "        dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)",
            "",
            "        failed_usages = self._get_provider_usages(failed_rp_uuid)",
            "        # Expects no allocation records on the failed host.",
            "        self.assertFlavorMatchesAllocation(",
            "           {'vcpus': 0, 'ram': 0, 'disk': 0}, failed_usages)",
            "",
            "        # Ensure the allocation records on the destination host.",
            "        dest_usages = self._get_provider_usages(dest_rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, dest_usages)",
            "",
            "",
            "class ServerBuildAbortTests(ProviderUsageBaseTestCase):",
            "    \"\"\"Tests server create scenarios which trigger a build abort during",
            "    a server build and validates that allocations in Placement",
            "    are properly cleaned up.",
            "",
            "    Uses a fake virt driver that aborts the build on the first attempt.",
            "    \"\"\"",
            "",
            "    compute_driver = 'fake.FakeBuildAbortDriver'",
            "",
            "    def setUp(self):",
            "        super(ServerBuildAbortTests, self).setUp()",
            "        # We only need one compute service/host/node for these tests.",
            "        self.compute1 = self._start_compute(host='host1')",
            "",
            "        flavors = self.api.get_flavors()",
            "        self.flavor1 = flavors[0]",
            "",
            "    def test_abort_when_booting_instance(self):",
            "        \"\"\"Tests that allocations, created by the scheduler, are cleaned",
            "        from the source node when the build is aborted on that node.",
            "        \"\"\"",
            "        server_req = self._build_minimal_create_server_request(",
            "                self.api, 'some-server', flavor_id=self.flavor1['id'],",
            "                image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',",
            "                networks=[])",
            "",
            "        created_server = self.api.post_server({'server': server_req})",
            "        self._wait_for_state_change(self.api, created_server, 'ERROR')",
            "",
            "        failed_hostname = self.compute1.manager.host",
            "",
            "        failed_rp_uuid = self._get_provider_uuid_by_host(failed_hostname)",
            "        failed_usages = self._get_provider_usages(failed_rp_uuid)",
            "        # Expects no allocation records on the failed host.",
            "        self.assertFlavorMatchesAllocation(",
            "           {'vcpus': 0, 'ram': 0, 'disk': 0}, failed_usages)",
            "",
            "",
            "class ServerUnshelveSpawnFailTests(ProviderUsageBaseTestCase):",
            "    \"\"\"Tests server unshelve scenarios which trigger a",
            "    VirtualInterfaceCreateException during driver.spawn() and validates that",
            "    allocations in Placement are properly cleaned up.",
            "    \"\"\"",
            "",
            "    compute_driver = 'fake.FakeUnshelveSpawnFailDriver'",
            "",
            "    def setUp(self):",
            "        super(ServerUnshelveSpawnFailTests, self).setUp()",
            "        # We only need one compute service/host/node for these tests.",
            "        fake.set_nodes(['host1'])",
            "        self.flags(host='host1')",
            "        self.compute1 = self._start_compute('host1')",
            "",
            "        flavors = self.api.get_flavors()",
            "        self.flavor1 = flavors[0]",
            "",
            "    def test_driver_spawn_fail_when_unshelving_instance(self):",
            "        \"\"\"Tests that allocations, created by the scheduler, are cleaned",
            "        from the target node when the unshelve driver.spawn fails on that node.",
            "        \"\"\"",
            "        hostname = self.compute1.manager.host",
            "        rp_uuid = self._get_provider_uuid_by_host(hostname)",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        # We start with no usages on the host.",
            "        self.assertFlavorMatchesAllocation(",
            "           {'vcpus': 0, 'ram': 0, 'disk': 0}, usages)",
            "",
            "        server_req = self._build_minimal_create_server_request(",
            "            self.api, 'unshelve-spawn-fail', flavor_id=self.flavor1['id'],",
            "            image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',",
            "            networks='none')",
            "",
            "        server = self.api.post_server({'server': server_req})",
            "        self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        # assert allocations exist for the host",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, usages)",
            "",
            "        # shelve offload the server",
            "        self.flags(shelved_offload_time=0)",
            "        self.api.post_server_action(server['id'], {'shelve': None})",
            "        self._wait_for_server_parameter(",
            "            self.api, server, {'status': 'SHELVED_OFFLOADED',",
            "                               'OS-EXT-SRV-ATTR:host': None})",
            "",
            "        # assert allocations were removed from the host",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(",
            "           {'vcpus': 0, 'ram': 0, 'disk': 0}, usages)",
            "",
            "        # unshelve the server, which should fail",
            "        self.api.post_server_action(server['id'], {'unshelve': None})",
            "        self._wait_for_action_fail_completion(",
            "            server, instance_actions.UNSHELVE, 'compute_unshelve_instance')",
            "",
            "        # assert allocations were removed from the host",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(",
            "           {'vcpus': 0, 'ram': 0, 'disk': 0}, usages)",
            "",
            "",
            "class ServerSoftDeleteTests(ProviderUsageBaseTestCase):",
            "",
            "    compute_driver = 'fake.SmallFakeDriver'",
            "",
            "    def setUp(self):",
            "        super(ServerSoftDeleteTests, self).setUp()",
            "        # We only need one compute service/host/node for these tests.",
            "        self.compute1 = self._start_compute('host1')",
            "",
            "        flavors = self.api.get_flavors()",
            "        self.flavor1 = flavors[0]",
            "",
            "    def _soft_delete_and_check_allocation(self, server, hostname):",
            "        self.api.delete_server(server['id'])",
            "        server = self._wait_for_state_change(self.api, server, 'SOFT_DELETED')",
            "",
            "        self._run_periodics()",
            "",
            "        # in soft delete state nova should keep the resource allocation as",
            "        # the instance can be restored",
            "        rp_uuid = self._get_provider_uuid_by_host(hostname)",
            "",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, allocation)",
            "",
            "        # run the periodic reclaim but as time isn't advanced it should not",
            "        # reclaim the instance",
            "        ctxt = context.get_admin_context()",
            "        self.compute1._reclaim_queued_deletes(ctxt)",
            "",
            "        self._run_periodics()",
            "",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, allocation)",
            "",
            "    def test_soft_delete_then_reclaim(self):",
            "        \"\"\"Asserts that the automatic reclaim of soft deleted instance cleans",
            "        up the allocations in placement.",
            "        \"\"\"",
            "",
            "        # make sure that instance will go to SOFT_DELETED state instead of",
            "        # deleted immediately",
            "        self.flags(reclaim_instance_interval=30)",
            "",
            "        hostname = self.compute1.host",
            "        rp_uuid = self._get_provider_uuid_by_host(hostname)",
            "",
            "        server = self._boot_and_check_allocations(self.flavor1, hostname)",
            "",
            "        self._soft_delete_and_check_allocation(server, hostname)",
            "",
            "        # advance the time and run periodic reclaim, instance should be deleted",
            "        # and resources should be freed",
            "        the_past = timeutils.utcnow() + datetime.timedelta(hours=1)",
            "        timeutils.set_time_override(override_time=the_past)",
            "        self.addCleanup(timeutils.clear_time_override)",
            "        ctxt = context.get_admin_context()",
            "        self.compute1._reclaim_queued_deletes(ctxt)",
            "",
            "        # Wait for real deletion",
            "        self._wait_until_deleted(server)",
            "",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertEqual({'VCPU': 0,",
            "                          'MEMORY_MB': 0,",
            "                          'DISK_GB': 0}, usages)",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(0, len(allocations))",
            "",
            "    def test_soft_delete_then_restore(self):",
            "        \"\"\"Asserts that restoring a soft deleted instance keeps the proper",
            "        allocation in placement.",
            "        \"\"\"",
            "",
            "        # make sure that instance will go to SOFT_DELETED state instead of",
            "        # deleted immediately",
            "        self.flags(reclaim_instance_interval=30)",
            "",
            "        hostname = self.compute1.host",
            "        rp_uuid = self._get_provider_uuid_by_host(hostname)",
            "",
            "        server = self._boot_and_check_allocations(",
            "            self.flavor1, hostname)",
            "",
            "        self._soft_delete_and_check_allocation(server, hostname)",
            "",
            "        post = {'restore': {}}",
            "        self.api.post_server_action(server['id'], post)",
            "        server = self._wait_for_state_change(self.api, server, 'ACTIVE')",
            "",
            "        # after restore the allocations should be kept",
            "        usages = self._get_provider_usages(rp_uuid)",
            "        self.assertFlavorMatchesAllocation(self.flavor1, usages)",
            "",
            "        allocations = self._get_allocations_by_server_uuid(server['id'])",
            "        self.assertEqual(1, len(allocations))",
            "        allocation = allocations[rp_uuid]['resources']",
            "        self.assertFlavorMatchesAllocation(self.flavor1, allocation)",
            "",
            "        # Now we want a real delete",
            "        self.flags(reclaim_instance_interval=0)",
            "        self._delete_and_check_allocations(server)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1173": [
                "ServerRebuildTestCase",
                "test_rebuild_with_new_image",
                "assertFlavorsMatchAllocation"
            ],
            "1174": [
                "ServerRebuildTestCase",
                "test_rebuild_with_new_image",
                "assertFlavorsMatchAllocation"
            ],
            "1175": [
                "ServerRebuildTestCase",
                "test_rebuild_with_new_image",
                "assertFlavorsMatchAllocation"
            ],
            "1176": [
                "ServerRebuildTestCase",
                "test_rebuild_with_new_image",
                "assertFlavorsMatchAllocation"
            ],
            "1177": [
                "ServerRebuildTestCase",
                "test_rebuild_with_new_image",
                "assertFlavorsMatchAllocation"
            ],
            "1178": [
                "ServerRebuildTestCase",
                "test_rebuild_with_new_image",
                "assertFlavorsMatchAllocation"
            ],
            "1179": [
                "ServerRebuildTestCase",
                "test_rebuild_with_new_image",
                "assertFlavorsMatchAllocation"
            ],
            "1180": [
                "ServerRebuildTestCase",
                "test_rebuild_with_new_image",
                "assertFlavorsMatchAllocation"
            ],
            "1181": [
                "ServerRebuildTestCase",
                "test_rebuild_with_new_image"
            ],
            "1228": [
                "ServerRebuildTestCase",
                "test_rebuild_with_new_image"
            ],
            "1229": [
                "ServerRebuildTestCase",
                "test_rebuild_with_new_image"
            ],
            "1230": [
                "ServerRebuildTestCase",
                "test_rebuild_with_new_image"
            ],
            "1231": [
                "ServerRebuildTestCase",
                "test_rebuild_with_new_image"
            ],
            "1232": [
                "ServerRebuildTestCase",
                "test_rebuild_with_new_image"
            ],
            "1236": [
                "ServerRebuildTestCase",
                "test_rebuild_with_new_image"
            ],
            "1237": [
                "ServerRebuildTestCase",
                "test_rebuild_with_new_image"
            ]
        },
        "addLocation": []
    },
    "nova/tests/unit/scheduler/test_filter_scheduler.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 526,
                "afterPatchRowNumber": 526,
                "PatchRowcode": "         instance."
            },
            "1": {
                "beforePatchRowNumber": 527,
                "afterPatchRowNumber": 527,
                "PatchRowcode": "         \"\"\""
            },
            "2": {
                "beforePatchRowNumber": 528,
                "afterPatchRowNumber": 528,
                "PatchRowcode": "         ctx = mock.Mock(user_id=uuids.user_id)"
            },
            "3": {
                "beforePatchRowNumber": 529,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        spec_obj = mock.Mock(project_id=uuids.project_id)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 529,
                "PatchRowcode": "+        spec_obj = objects.RequestSpec(project_id=uuids.project_id)"
            },
            "5": {
                "beforePatchRowNumber": 530,
                "afterPatchRowNumber": 530,
                "PatchRowcode": "         instance_uuid = uuids.instance"
            },
            "6": {
                "beforePatchRowNumber": 531,
                "afterPatchRowNumber": 531,
                "PatchRowcode": "         alloc_reqs = [mock.sentinel.alloc_req]"
            },
            "7": {
                "beforePatchRowNumber": 532,
                "afterPatchRowNumber": 532,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 539,
                "afterPatchRowNumber": 539,
                "PatchRowcode": "             mock.sentinel.alloc_req, uuids.project_id, uuids.user_id)"
            },
            "9": {
                "beforePatchRowNumber": 540,
                "afterPatchRowNumber": 540,
                "PatchRowcode": "         self.assertTrue(res)"
            },
            "10": {
                "beforePatchRowNumber": 541,
                "afterPatchRowNumber": 541,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 542,
                "PatchRowcode": "+    @mock.patch('nova.scheduler.utils.request_is_rebuild')"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 543,
                "PatchRowcode": "+    def test_claim_resouces_for_policy_check(self, mock_policy):"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 544,
                "PatchRowcode": "+        mock_policy.return_value = True"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 545,
                "PatchRowcode": "+        res = self.driver._claim_resources(None, mock.sentinel.spec_obj,"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 546,
                "PatchRowcode": "+                                           mock.sentinel.instance_uuid,"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 547,
                "PatchRowcode": "+                                           [])"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 548,
                "PatchRowcode": "+        self.assertTrue(res)"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 549,
                "PatchRowcode": "+        mock_policy.assert_called_once_with(mock.sentinel.spec_obj)"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 550,
                "PatchRowcode": "+        self.assertFalse(self.placement_client.claim_resources.called)"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 551,
                "PatchRowcode": "+"
            },
            "21": {
                "beforePatchRowNumber": 542,
                "afterPatchRowNumber": 552,
                "PatchRowcode": "     def test_add_retry_host(self):"
            },
            "22": {
                "beforePatchRowNumber": 543,
                "afterPatchRowNumber": 553,
                "PatchRowcode": "         retry = dict(num_attempts=1, hosts=[])"
            },
            "23": {
                "beforePatchRowNumber": 544,
                "afterPatchRowNumber": 554,
                "PatchRowcode": "         filter_properties = dict(retry=retry)"
            }
        },
        "frontPatchFile": [
            "# Copyright 2011 OpenStack Foundation",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "\"\"\"",
            "Tests For Filter Scheduler.",
            "\"\"\"",
            "",
            "import mock",
            "",
            "from nova import exception",
            "from nova import objects",
            "from nova.scheduler import client",
            "from nova.scheduler.client import report",
            "from nova.scheduler import filter_scheduler",
            "from nova.scheduler import host_manager",
            "from nova.scheduler import utils as scheduler_utils",
            "from nova.scheduler import weights",
            "from nova import test  # noqa",
            "from nova.tests.unit.scheduler import test_scheduler",
            "from nova.tests import uuidsentinel as uuids",
            "",
            "",
            "class FilterSchedulerTestCase(test_scheduler.SchedulerTestCase):",
            "    \"\"\"Test case for Filter Scheduler.\"\"\"",
            "",
            "    driver_cls = filter_scheduler.FilterScheduler",
            "",
            "    @mock.patch('nova.scheduler.client.SchedulerClient')",
            "    def setUp(self, mock_client):",
            "        pc_client = mock.Mock(spec=report.SchedulerReportClient)",
            "        sched_client = mock.Mock(spec=client.SchedulerClient)",
            "        sched_client.reportclient = pc_client",
            "        mock_client.return_value = sched_client",
            "        self.placement_client = pc_client",
            "        super(FilterSchedulerTestCase, self).setUp()",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_claim_resources')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_all_host_states')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_sorted_hosts')",
            "    def test_schedule_placement_bad_comms(self, mock_get_hosts,",
            "            mock_get_all_states, mock_claim):",
            "        \"\"\"If there was a problem communicating with the Placement service,",
            "        alloc_reqs_by_rp_uuid will be None and we need to avoid trying to claim",
            "        in the Placement API.",
            "        \"\"\"",
            "        spec_obj = objects.RequestSpec(",
            "            num_instances=1,",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            instance_group=None)",
            "",
            "        host_state = mock.Mock(spec=host_manager.HostState,",
            "                host=mock.sentinel.host, uuid=uuids.cn1, cell_uuid=uuids.cell)",
            "        all_host_states = [host_state]",
            "        mock_get_all_states.return_value = all_host_states",
            "        mock_get_hosts.return_value = all_host_states",
            "",
            "        instance_uuids = None",
            "        ctx = mock.Mock()",
            "        selected_hosts = self.driver._schedule(ctx, spec_obj,",
            "            instance_uuids, None, mock.sentinel.provider_summaries)",
            "",
            "        mock_get_all_states.assert_called_once_with(",
            "            ctx.elevated.return_value, spec_obj,",
            "            mock.sentinel.provider_summaries)",
            "        mock_get_hosts.assert_called_once_with(spec_obj, all_host_states, 0)",
            "",
            "        self.assertEqual(len(selected_hosts), 1)",
            "        self.assertEqual([[host_state]], selected_hosts)",
            "",
            "        # Ensure that we have consumed the resources on the chosen host states",
            "        host_state.consume_from_request.assert_called_once_with(spec_obj)",
            "",
            "        # And ensure we never called _claim_resources()",
            "        self.assertFalse(mock_claim.called)",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_claim_resources')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_all_host_states')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_sorted_hosts')",
            "    def test_schedule_old_conductor(self, mock_get_hosts,",
            "            mock_get_all_states, mock_claim):",
            "        \"\"\"Old conductor can call scheduler without the instance_uuids",
            "        parameter. When this happens, we need to ensure we do not attempt to",
            "        claim resources in the placement API since obviously we need instance",
            "        UUIDs to perform those claims.",
            "        \"\"\"",
            "        spec_obj = objects.RequestSpec(",
            "            num_instances=1,",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            instance_group=None)",
            "",
            "        host_state = mock.Mock(spec=host_manager.HostState,",
            "            host=mock.sentinel.host, uuid=uuids.cn1)",
            "        all_host_states = [host_state]",
            "        mock_get_all_states.return_value = all_host_states",
            "        mock_get_hosts.return_value = all_host_states",
            "",
            "        instance_uuids = None",
            "        ctx = mock.Mock()",
            "        selected_hosts = self.driver._schedule(ctx, spec_obj,",
            "            instance_uuids, mock.sentinel.alloc_reqs_by_rp_uuid,",
            "            mock.sentinel.provider_summaries)",
            "",
            "        mock_get_all_states.assert_called_once_with(",
            "            ctx.elevated.return_value, spec_obj,",
            "            mock.sentinel.provider_summaries)",
            "        mock_get_hosts.assert_called_once_with(spec_obj, all_host_states, 0)",
            "",
            "        self.assertEqual(len(selected_hosts), 1)",
            "        self.assertEqual([[host_state]], selected_hosts)",
            "",
            "        # Ensure that we have consumed the resources on the chosen host states",
            "        host_state.consume_from_request.assert_called_once_with(spec_obj)",
            "",
            "        # And ensure we never called _claim_resources()",
            "        self.assertFalse(mock_claim.called)",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_claim_resources')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_all_host_states')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_sorted_hosts')",
            "    def _test_schedule_successful_claim(self, mock_get_hosts,",
            "            mock_get_all_states, mock_claim, num_instances=1):",
            "        spec_obj = objects.RequestSpec(",
            "            num_instances=num_instances,",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            instance_group=None)",
            "",
            "        host_state = mock.Mock(spec=host_manager.HostState,",
            "            host=mock.sentinel.host, uuid=uuids.cn1, cell_uuid=uuids.cell1)",
            "        all_host_states = [host_state]",
            "        mock_get_all_states.return_value = all_host_states",
            "        mock_get_hosts.return_value = all_host_states",
            "        mock_claim.return_value = True",
            "",
            "        instance_uuids = [uuids.instance]",
            "        alloc_reqs_by_rp_uuid = {",
            "            uuids.cn1: [mock.sentinel.alloc_req],",
            "        }",
            "        ctx = mock.Mock()",
            "        selected_hosts = self.driver._schedule(ctx, spec_obj, instance_uuids,",
            "                alloc_reqs_by_rp_uuid, mock.sentinel.provider_summaries)",
            "",
            "        mock_get_all_states.assert_called_once_with(",
            "            ctx.elevated.return_value, spec_obj,",
            "            mock.sentinel.provider_summaries)",
            "        mock_get_hosts.assert_called()",
            "        mock_claim.assert_called_once_with(ctx.elevated.return_value, spec_obj,",
            "            uuids.instance, [mock.sentinel.alloc_req])",
            "",
            "        self.assertEqual(len(selected_hosts), 1)",
            "        self.assertEqual([[host_state]], selected_hosts)",
            "",
            "        # Ensure that we have consumed the resources on the chosen host states",
            "        host_state.consume_from_request.assert_called_once_with(spec_obj)",
            "",
            "    def test_schedule_successful_claim(self):",
            "        self._test_schedule_successful_claim()",
            "",
            "    def test_schedule_old_reqspec_and_move_operation(self):",
            "        \"\"\"This test is for verifying that in case of a move operation with an",
            "        original RequestSpec created for 3 concurrent instances, we only verify",
            "        the instance that is moved.",
            "        \"\"\"",
            "        self._test_schedule_successful_claim(num_instances=3)",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_cleanup_allocations')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_claim_resources')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_all_host_states')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_sorted_hosts')",
            "    def test_schedule_unsuccessful_claim(self, mock_get_hosts,",
            "            mock_get_all_states, mock_claim, mock_cleanup):",
            "        \"\"\"Tests that we return an empty list if we are unable to successfully",
            "        claim resources for the instance",
            "        \"\"\"",
            "        spec_obj = objects.RequestSpec(",
            "            num_instances=1,",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            instance_group=None)",
            "",
            "        host_state = mock.Mock(spec=host_manager.HostState,",
            "            host=mock.sentinel.host, uuid=uuids.cn1, cell_uuid=uuids.cell1)",
            "        all_host_states = [host_state]",
            "        mock_get_all_states.return_value = all_host_states",
            "        mock_get_hosts.return_value = all_host_states",
            "        mock_claim.return_value = False",
            "",
            "        instance_uuids = [uuids.instance]",
            "        alloc_reqs_by_rp_uuid = {",
            "            uuids.cn1: [mock.sentinel.alloc_req],",
            "        }",
            "        ctx = mock.Mock()",
            "        selected_hosts = self.driver._schedule(ctx, spec_obj,",
            "            instance_uuids, alloc_reqs_by_rp_uuid,",
            "            mock.sentinel.provider_summaries)",
            "",
            "        mock_get_all_states.assert_called_once_with(",
            "            ctx.elevated.return_value, spec_obj,",
            "            mock.sentinel.provider_summaries)",
            "        mock_get_hosts.assert_called_once_with(spec_obj, all_host_states, 0)",
            "        mock_claim.assert_called_once_with(ctx.elevated.return_value, spec_obj,",
            "            uuids.instance, [mock.sentinel.alloc_req])",
            "",
            "        self.assertEqual([], selected_hosts)",
            "",
            "        mock_cleanup.assert_called_once_with([])",
            "",
            "        # Ensure that we have consumed the resources on the chosen host states",
            "        self.assertFalse(host_state.consume_from_request.called)",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_cleanup_allocations')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_claim_resources')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_all_host_states')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_sorted_hosts')",
            "    def test_schedule_not_all_instance_clean_claimed(self, mock_get_hosts,",
            "            mock_get_all_states, mock_claim, mock_cleanup):",
            "        \"\"\"Tests that we clean up previously-allocated instances if not all",
            "        instances could be scheduled",
            "        \"\"\"",
            "        spec_obj = objects.RequestSpec(",
            "            num_instances=2,",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            instance_group=None)",
            "",
            "        host_state = mock.Mock(spec=host_manager.HostState,",
            "            host=mock.sentinel.host, uuid=uuids.cn1, cell_uuid=uuids.cell1)",
            "        all_host_states = [host_state]",
            "        mock_get_all_states.return_value = all_host_states",
            "        mock_get_hosts.side_effect = [",
            "            all_host_states,  # first instance: return all the hosts (only one)",
            "            [],  # second: act as if no more hosts that meet criteria",
            "            all_host_states,  # the final call when creating alternates",
            "        ]",
            "        mock_claim.return_value = True",
            "",
            "        instance_uuids = [uuids.instance1, uuids.instance2]",
            "        alloc_reqs_by_rp_uuid = {",
            "            uuids.cn1: [mock.sentinel.alloc_req],",
            "        }",
            "        ctx = mock.Mock()",
            "        self.driver._schedule(ctx, spec_obj, instance_uuids,",
            "            alloc_reqs_by_rp_uuid, mock.sentinel.provider_summaries)",
            "",
            "        # Ensure we cleaned up the first successfully-claimed instance",
            "        mock_cleanup.assert_called_once_with([uuids.instance1])",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_claim_resources')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_all_host_states')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_sorted_hosts')",
            "    def test_schedule_instance_group(self, mock_get_hosts,",
            "            mock_get_all_states, mock_claim):",
            "        \"\"\"Test that since the request spec object contains an instance group",
            "        object, that upon choosing a host in the primary schedule loop,",
            "        that we update the request spec's instance group information",
            "        \"\"\"",
            "        num_instances = 2",
            "        ig = objects.InstanceGroup(hosts=[])",
            "        spec_obj = objects.RequestSpec(",
            "            num_instances=num_instances,",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            instance_group=ig)",
            "",
            "        hs1 = mock.Mock(spec=host_manager.HostState, host='host1',",
            "            uuid=uuids.cn1, cell_uuid=uuids.cell1)",
            "        hs2 = mock.Mock(spec=host_manager.HostState, host='host2',",
            "            uuid=uuids.cn2, cell_uuid=uuids.cell2)",
            "        all_host_states = [hs1, hs2]",
            "        mock_get_all_states.return_value = all_host_states",
            "        mock_claim.return_value = True",
            "",
            "        alloc_reqs_by_rp_uuid = {",
            "            uuids.cn1: [mock.sentinel.alloc_req_cn1],",
            "            uuids.cn2: [mock.sentinel.alloc_req_cn2],",
            "        }",
            "",
            "        # Simulate host 1 and host 2 being randomly returned first by",
            "        # _get_sorted_hosts() in the two iterations for each instance in",
            "        # num_instances",
            "        mock_get_hosts.side_effect = ([hs2, hs1], [hs1, hs2],",
            "                                      [hs2, hs1], [hs1, hs2])",
            "        instance_uuids = [",
            "            getattr(uuids, 'instance%d' % x) for x in range(num_instances)",
            "        ]",
            "        ctx = mock.Mock()",
            "        self.driver._schedule(ctx, spec_obj, instance_uuids,",
            "            alloc_reqs_by_rp_uuid, mock.sentinel.provider_summaries)",
            "",
            "        # Check that we called _claim_resources() for both the first and second",
            "        # host state",
            "        claim_calls = [",
            "            mock.call(ctx.elevated.return_value, spec_obj,",
            "                uuids.instance0, [mock.sentinel.alloc_req_cn2]),",
            "            mock.call(ctx.elevated.return_value, spec_obj,",
            "                uuids.instance1, [mock.sentinel.alloc_req_cn1]),",
            "        ]",
            "        mock_claim.assert_has_calls(claim_calls)",
            "",
            "        # Check that _get_sorted_hosts() is called twice and that the",
            "        # second time, we pass it the hosts that were returned from",
            "        # _get_sorted_hosts() the first time",
            "        sorted_host_calls = [",
            "            mock.call(spec_obj, all_host_states, 0),",
            "            mock.call(spec_obj, [hs2, hs1], 1),",
            "        ]",
            "        mock_get_hosts.assert_has_calls(sorted_host_calls)",
            "",
            "        # The instance group object should have both host1 and host2 in its",
            "        # instance group hosts list and there should not be any \"changes\" to",
            "        # save in the instance group object",
            "        self.assertEqual(['host2', 'host1'], ig.hosts)",
            "        self.assertEqual({}, ig.obj_get_changes())",
            "",
            "    @mock.patch('random.choice', side_effect=lambda x: x[1])",
            "    @mock.patch('nova.scheduler.host_manager.HostManager.get_weighed_hosts')",
            "    @mock.patch('nova.scheduler.host_manager.HostManager.get_filtered_hosts')",
            "    def test_get_sorted_hosts(self, mock_filt, mock_weighed, mock_rand):",
            "        \"\"\"Tests the call that returns a sorted list of hosts by calling the",
            "        host manager's filtering and weighing routines",
            "        \"\"\"",
            "        self.flags(host_subset_size=2, group='filter_scheduler')",
            "        hs1 = mock.Mock(spec=host_manager.HostState, host='host1',",
            "                cell_uuid=uuids.cell1)",
            "        hs2 = mock.Mock(spec=host_manager.HostState, host='host2',",
            "                cell_uuid=uuids.cell2)",
            "        all_host_states = [hs1, hs2]",
            "",
            "        mock_weighed.return_value = [",
            "            weights.WeighedHost(hs1, 1.0), weights.WeighedHost(hs2, 1.0),",
            "        ]",
            "",
            "        results = self.driver._get_sorted_hosts(mock.sentinel.spec,",
            "            all_host_states, mock.sentinel.index)",
            "",
            "        mock_filt.assert_called_once_with(all_host_states, mock.sentinel.spec,",
            "            mock.sentinel.index)",
            "",
            "        mock_weighed.assert_called_once_with(mock_filt.return_value,",
            "            mock.sentinel.spec)",
            "",
            "        # We override random.choice() to pick the **second** element of the",
            "        # returned weighed hosts list, which is the host state #2. This tests",
            "        # the code path that combines the randomly-chosen host with the",
            "        # remaining list of weighed host state objects",
            "        self.assertEqual([hs2, hs1], results)",
            "",
            "    @mock.patch('random.choice', side_effect=lambda x: x[0])",
            "    @mock.patch('nova.scheduler.host_manager.HostManager.get_weighed_hosts')",
            "    @mock.patch('nova.scheduler.host_manager.HostManager.get_filtered_hosts')",
            "    def test_get_sorted_hosts_subset_less_than_num_weighed(self, mock_filt,",
            "            mock_weighed, mock_rand):",
            "        \"\"\"Tests that when we have >1 weighed hosts but a host subset size of",
            "        1, that we always pick the first host in the weighed host",
            "        \"\"\"",
            "        self.flags(host_subset_size=1, group='filter_scheduler')",
            "        hs1 = mock.Mock(spec=host_manager.HostState, host='host1',",
            "                cell_uuid=uuids.cell1)",
            "        hs2 = mock.Mock(spec=host_manager.HostState, host='host2',",
            "                cell_uuid=uuids.cell2)",
            "        all_host_states = [hs1, hs2]",
            "",
            "        mock_weighed.return_value = [",
            "            weights.WeighedHost(hs1, 1.0), weights.WeighedHost(hs2, 1.0),",
            "        ]",
            "",
            "        results = self.driver._get_sorted_hosts(mock.sentinel.spec,",
            "            all_host_states, mock.sentinel.index)",
            "",
            "        mock_filt.assert_called_once_with(all_host_states, mock.sentinel.spec,",
            "            mock.sentinel.index)",
            "",
            "        mock_weighed.assert_called_once_with(mock_filt.return_value,",
            "            mock.sentinel.spec)",
            "",
            "        # We should be randomly selecting only from a list of one host state",
            "        mock_rand.assert_called_once_with([hs1])",
            "        self.assertEqual([hs1, hs2], results)",
            "",
            "    @mock.patch('random.choice', side_effect=lambda x: x[0])",
            "    @mock.patch('nova.scheduler.host_manager.HostManager.get_weighed_hosts')",
            "    @mock.patch('nova.scheduler.host_manager.HostManager.get_filtered_hosts')",
            "    def test_get_sorted_hosts_subset_greater_than_num_weighed(self, mock_filt,",
            "            mock_weighed, mock_rand):",
            "        \"\"\"Hosts should still be chosen if host subset size is larger than",
            "        number of weighed hosts.",
            "        \"\"\"",
            "        self.flags(host_subset_size=20, group='filter_scheduler')",
            "        hs1 = mock.Mock(spec=host_manager.HostState, host='host1',",
            "                cell_uuid=uuids.cell1)",
            "        hs2 = mock.Mock(spec=host_manager.HostState, host='host2',",
            "                cell_uuid=uuids.cell2)",
            "        all_host_states = [hs1, hs2]",
            "",
            "        mock_weighed.return_value = [",
            "            weights.WeighedHost(hs1, 1.0), weights.WeighedHost(hs2, 1.0),",
            "        ]",
            "",
            "        results = self.driver._get_sorted_hosts(mock.sentinel.spec,",
            "            all_host_states, mock.sentinel.index)",
            "",
            "        mock_filt.assert_called_once_with(all_host_states, mock.sentinel.spec,",
            "            mock.sentinel.index)",
            "",
            "        mock_weighed.assert_called_once_with(mock_filt.return_value,",
            "            mock.sentinel.spec)",
            "",
            "        # We overrode random.choice() to return the first element in the list,",
            "        # so even though we had a host_subset_size greater than the number of",
            "        # weighed hosts (2), we just random.choice() on the entire set of",
            "        # weighed hosts and thus return [hs1, hs2]",
            "        self.assertEqual([hs1, hs2], results)",
            "",
            "    @mock.patch('random.shuffle', side_effect=lambda x: x.reverse())",
            "    @mock.patch('nova.scheduler.host_manager.HostManager.get_weighed_hosts')",
            "    @mock.patch('nova.scheduler.host_manager.HostManager.get_filtered_hosts')",
            "    def test_get_sorted_hosts_shuffle_top_equal(self, mock_filt, mock_weighed,",
            "                                                mock_shuffle):",
            "        \"\"\"Tests that top best weighed hosts are shuffled when enabled.",
            "        \"\"\"",
            "        self.flags(host_subset_size=1, group='filter_scheduler')",
            "        self.flags(shuffle_best_same_weighed_hosts=True,",
            "                   group='filter_scheduler')",
            "        hs1 = mock.Mock(spec=host_manager.HostState, host='host1')",
            "        hs2 = mock.Mock(spec=host_manager.HostState, host='host2')",
            "        hs3 = mock.Mock(spec=host_manager.HostState, host='host3')",
            "        hs4 = mock.Mock(spec=host_manager.HostState, host='host4')",
            "        all_host_states = [hs1, hs2, hs3, hs4]",
            "",
            "        mock_weighed.return_value = [",
            "            weights.WeighedHost(hs1, 1.0),",
            "            weights.WeighedHost(hs2, 1.0),",
            "            weights.WeighedHost(hs3, 0.5),",
            "            weights.WeighedHost(hs4, 0.5),",
            "        ]",
            "",
            "        results = self.driver._get_sorted_hosts(mock.sentinel.spec,",
            "            all_host_states, mock.sentinel.index)",
            "",
            "        mock_filt.assert_called_once_with(all_host_states, mock.sentinel.spec,",
            "            mock.sentinel.index)",
            "",
            "        mock_weighed.assert_called_once_with(mock_filt.return_value,",
            "            mock.sentinel.spec)",
            "",
            "        # We override random.shuffle() to reverse the list, thus the",
            "        # head of the list should become [host#2, host#1]",
            "        # (as the host_subset_size is 1) and the tail should stay the same.",
            "        self.assertEqual([hs2, hs1, hs3, hs4], results)",
            "",
            "    def test_cleanup_allocations(self):",
            "        instance_uuids = []",
            "        # Check we don't do anything if there's no instance UUIDs to cleanup",
            "        # allocations for",
            "        pc = self.placement_client",
            "",
            "        self.driver._cleanup_allocations(instance_uuids)",
            "        self.assertFalse(pc.delete_allocation_for_instance.called)",
            "",
            "        instance_uuids = [uuids.instance1, uuids.instance2]",
            "        self.driver._cleanup_allocations(instance_uuids)",
            "",
            "        exp_calls = [mock.call(uuids.instance1), mock.call(uuids.instance2)]",
            "        pc.delete_allocation_for_instance.assert_has_calls(exp_calls)",
            "",
            "    def test_claim_resources(self):",
            "        \"\"\"Tests that when _schedule() calls _claim_resources(), that we",
            "        appropriately call the placement client to claim resources for the",
            "        instance.",
            "        \"\"\"",
            "        ctx = mock.Mock(user_id=uuids.user_id)",
            "        spec_obj = mock.Mock(project_id=uuids.project_id)",
            "        instance_uuid = uuids.instance",
            "        alloc_reqs = [mock.sentinel.alloc_req]",
            "",
            "        res = self.driver._claim_resources(ctx, spec_obj, instance_uuid,",
            "            alloc_reqs)",
            "",
            "        pc = self.placement_client",
            "        pc.claim_resources.return_value = True",
            "        pc.claim_resources.assert_called_once_with(uuids.instance,",
            "            mock.sentinel.alloc_req, uuids.project_id, uuids.user_id)",
            "        self.assertTrue(res)",
            "",
            "    def test_add_retry_host(self):",
            "        retry = dict(num_attempts=1, hosts=[])",
            "        filter_properties = dict(retry=retry)",
            "        host = \"fakehost\"",
            "        node = \"fakenode\"",
            "",
            "        scheduler_utils._add_retry_host(filter_properties, host, node)",
            "",
            "        hosts = filter_properties['retry']['hosts']",
            "        self.assertEqual(1, len(hosts))",
            "        self.assertEqual([host, node], hosts[0])",
            "",
            "    def test_post_select_populate(self):",
            "        # Test addition of certain filter props after a node is selected.",
            "        retry = {'hosts': [], 'num_attempts': 1}",
            "        filter_properties = {'retry': retry}",
            "",
            "        host_state = host_manager.HostState('host', 'node', uuids.cell)",
            "        host_state.limits['vcpu'] = 5",
            "        scheduler_utils.populate_filter_properties(filter_properties,",
            "                host_state)",
            "",
            "        self.assertEqual(['host', 'node'],",
            "                         filter_properties['retry']['hosts'][0])",
            "",
            "        self.assertEqual({'vcpu': 5}, host_state.limits)",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_schedule')",
            "    def test_select_destinations_match_num_instances(self, mock_schedule):",
            "        \"\"\"Tests that the select_destinations() method returns the list of",
            "        hosts from the _schedule() method when the number of returned hosts",
            "        equals the number of instance UUIDs passed in.",
            "        \"\"\"",
            "        spec_obj = objects.RequestSpec(",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            num_instances=1)",
            "",
            "        mock_schedule.return_value = [[mock.sentinel.hs1]]",
            "",
            "        dests = self.driver.select_destinations(self.context, spec_obj,",
            "            [mock.sentinel.instance_uuid], mock.sentinel.alloc_reqs_by_rp_uuid,",
            "            mock.sentinel.p_sums)",
            "",
            "        mock_schedule.assert_called_once_with(self.context, spec_obj,",
            "            [mock.sentinel.instance_uuid], mock.sentinel.alloc_reqs_by_rp_uuid,",
            "            mock.sentinel.p_sums)",
            "",
            "        self.assertEqual([mock.sentinel.hs1], dests)",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_schedule')",
            "    def test_select_destinations_for_move_ops(self, mock_schedule):",
            "        \"\"\"Tests that the select_destinations() method verifies the number of",
            "        hosts returned from the _schedule() method against the number of",
            "        instance UUIDs passed as a parameter and not against the RequestSpec",
            "        num_instances field since the latter could be wrong in case of a move",
            "        operation.",
            "        \"\"\"",
            "        spec_obj = objects.RequestSpec(",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            num_instances=2)",
            "",
            "        host_state = mock.Mock(spec=host_manager.HostState,",
            "                cell_uuid=uuids.cell)",
            "        mock_schedule.return_value = [[host_state]]",
            "",
            "        dests = self.driver.select_destinations(self.context, spec_obj,",
            "            [mock.sentinel.instance_uuid], mock.sentinel.alloc_reqs_by_rp_uuid,",
            "            mock.sentinel.p_sums)",
            "",
            "        mock_schedule.assert_called_once_with(self.context, spec_obj,",
            "            [mock.sentinel.instance_uuid], mock.sentinel.alloc_reqs_by_rp_uuid,",
            "            mock.sentinel.p_sums)",
            "",
            "        self.assertEqual([host_state], dests)",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_schedule')",
            "    def test_select_destinations_fewer_num_instances(self, mock_schedule):",
            "        \"\"\"Tests that the select_destinations() method properly handles",
            "        resetting host state objects and raising NoValidHost when the",
            "        _schedule() method returns no host matches.",
            "        \"\"\"",
            "        spec_obj = objects.RequestSpec(",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            num_instances=2)",
            "",
            "        host_state = mock.Mock(spec=host_manager.HostState,",
            "                cell_uuid=uuids.cell)",
            "        mock_schedule.return_value = [[host_state]]",
            "",
            "        self.assertRaises(exception.NoValidHost,",
            "            self.driver.select_destinations, self.context, spec_obj,",
            "            [mock.sentinel.instance_uuid1, mock.sentinel.instance_uuid2],",
            "            mock.sentinel.alloc_reqs_by_rp_uuid, mock.sentinel.p_sums)",
            "",
            "        # Verify that the host state object has been marked as not updated so",
            "        # it's picked up in the next pull from the DB for compute node objects",
            "        self.assertIsNone(host_state.updated)",
            "",
            "    @mock.patch(\"nova.scheduler.host_manager.HostState.consume_from_request\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_claim_resources\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_get_sorted_hosts\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_get_all_host_states\")",
            "    def _test_alternates_returned(self, mock_get_all_hosts, mock_sorted,",
            "            mock_claim, mock_consume, num_instances=2, num_alternates=2):",
            "        all_host_states = []",
            "        alloc_reqs = {}",
            "        for num in range(10):",
            "            host_name = \"host%s\" % num",
            "            hs = host_manager.HostState(host_name, \"node%s\" % num,",
            "                    uuids.cell)",
            "            hs.uuid = getattr(uuids, host_name)",
            "            all_host_states.append(hs)",
            "            alloc_reqs[hs.uuid] = {}",
            "",
            "        mock_get_all_hosts.return_value = all_host_states",
            "        mock_sorted.return_value = all_host_states",
            "        mock_claim.return_value = True",
            "        total_returned = num_alternates + 1",
            "        self.flags(max_attempts=total_returned, group=\"scheduler\")",
            "        instance_uuids = [getattr(uuids, \"inst%s\" % num)",
            "                for num in range(num_instances)]",
            "",
            "        spec_obj = objects.RequestSpec(",
            "                num_instances=num_instances,",
            "                flavor=objects.Flavor(memory_mb=512,",
            "                                      root_gb=512,",
            "                                      ephemeral_gb=0,",
            "                                      swap=0,",
            "                                      vcpus=1),",
            "                project_id=uuids.project_id,",
            "                instance_group=None)",
            "",
            "        dests = self.driver._schedule(self.context, spec_obj,",
            "                instance_uuids, alloc_reqs, None)",
            "        self.assertEqual(num_instances, len(dests))",
            "        # Filtering and weighing hosts should be called num_instances + 1 times",
            "        # unless num_instances == 1.",
            "        self.assertEqual(num_instances + 1 if num_instances > 1 else 1,",
            "                         mock_sorted.call_count,",
            "                         'Unexpected number of calls to filter hosts for %s '",
            "                         'instances.' % num_instances)",
            "        selected_hosts = [dest[0] for dest in dests]",
            "        for dest in dests:",
            "            self.assertEqual(total_returned, len(dest))",
            "            # Verify that there are no duplicates among a destination",
            "            self.assertEqual(len(dest), len(set(dest)))",
            "            # Verify that none of the selected hosts appear in the alternates.",
            "            for alt in dest[1:]:",
            "                self.assertNotIn(alt, selected_hosts)",
            "",
            "    def test_alternates_returned(self):",
            "        self._test_alternates_returned(num_instances=1, num_alternates=1)",
            "        self._test_alternates_returned(num_instances=3, num_alternates=0)",
            "        self._test_alternates_returned(num_instances=1, num_alternates=4)",
            "        self._test_alternates_returned(num_instances=2, num_alternates=3)",
            "        self._test_alternates_returned(num_instances=8, num_alternates=8)",
            "",
            "    @mock.patch(\"nova.scheduler.host_manager.HostState.consume_from_request\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_claim_resources\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_get_sorted_hosts\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_get_all_host_states\")",
            "    def test_alternates_same_cell(self, mock_get_all_hosts, mock_sorted,",
            "            mock_claim, mock_consume):",
            "        \"\"\"Tests getting alternates plus claims where the hosts are spread",
            "        across two cells.",
            "        \"\"\"",
            "        all_host_states = []",
            "        alloc_reqs = {}",
            "        for num in range(10):",
            "            host_name = \"host%s\" % num",
            "            cell_uuid = uuids.cell1 if num % 2 else uuids.cell2",
            "            hs = host_manager.HostState(host_name, \"node%s\" % num,",
            "                    cell_uuid)",
            "            hs.uuid = getattr(uuids, host_name)",
            "            all_host_states.append(hs)",
            "            alloc_reqs[hs.uuid] = {}",
            "",
            "        mock_get_all_hosts.return_value = all_host_states",
            "        # There are two instances so _get_sorted_hosts is called once per",
            "        # instance and then once again before picking alternates.",
            "        mock_sorted.side_effect = [all_host_states,",
            "                                   list(reversed(all_host_states)),",
            "                                   all_host_states]",
            "        mock_claim.return_value = True",
            "        total_returned = 3",
            "        self.flags(max_attempts=total_returned, group=\"scheduler\")",
            "        instance_uuids = [uuids.inst1, uuids.inst2]",
            "        num_instances = len(instance_uuids)",
            "",
            "        spec_obj = objects.RequestSpec(",
            "                num_instances=num_instances,",
            "                flavor=objects.Flavor(memory_mb=512,",
            "                                      root_gb=512,",
            "                                      ephemeral_gb=0,",
            "                                      swap=0,",
            "                                      vcpus=1),",
            "                project_id=uuids.project_id,",
            "                instance_group=None)",
            "",
            "        dests = self.driver._schedule(self.context, spec_obj,",
            "                instance_uuids, alloc_reqs, None)",
            "        # There should be max_attempts hosts per instance (1 selected, 2 alts)",
            "        self.assertEqual(total_returned, len(dests[0]))",
            "        self.assertEqual(total_returned, len(dests[1]))",
            "        # Verify that the two selected hosts are not in the same cell.",
            "        self.assertNotEqual(dests[0][0].cell_uuid, dests[1][0].cell_uuid)",
            "        for dest in dests:",
            "            selected_host = dest[0]",
            "            selected_cell_uuid = selected_host.cell_uuid",
            "            for alternate in dest[1:]:",
            "                self.assertEqual(alternate.cell_uuid, selected_cell_uuid)",
            "",
            "    @mock.patch(\"nova.scheduler.host_manager.HostState.consume_from_request\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_claim_resources\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_get_sorted_hosts\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_get_all_host_states\")",
            "    def _test_not_enough_alternates(self, mock_get_all_hosts, mock_sorted,",
            "            mock_claim, mock_consume, num_hosts, max_attempts):",
            "        all_host_states = []",
            "        alloc_reqs = {}",
            "        for num in range(num_hosts):",
            "            host_name = \"host%s\" % num",
            "            hs = host_manager.HostState(host_name, \"node%s\" % num,",
            "                    uuids.cell)",
            "            hs.uuid = getattr(uuids, host_name)",
            "            all_host_states.append(hs)",
            "            alloc_reqs[hs.uuid] = {}",
            "",
            "        mock_get_all_hosts.return_value = all_host_states",
            "        mock_sorted.return_value = all_host_states",
            "        mock_claim.return_value = True",
            "        # Set the total returned to more than the number of available hosts",
            "        self.flags(max_attempts=max_attempts, group=\"scheduler\")",
            "        instance_uuids = [uuids.inst1, uuids.inst2]",
            "        num_instances = len(instance_uuids)",
            "",
            "        spec_obj = objects.RequestSpec(",
            "                num_instances=num_instances,",
            "                flavor=objects.Flavor(memory_mb=512,",
            "                                      root_gb=512,",
            "                                      ephemeral_gb=0,",
            "                                      swap=0,",
            "                                      vcpus=1),",
            "                project_id=uuids.project_id,",
            "                instance_group=None)",
            "",
            "        dests = self.driver._schedule(self.context, spec_obj,",
            "                instance_uuids, alloc_reqs, None)",
            "        self.assertEqual(num_instances, len(dests))",
            "        selected_hosts = [dest[0] for dest in dests]",
            "        # The number returned for each destination should be the less of the",
            "        # number of available host and the max_attempts setting.",
            "        expected_number = min(num_hosts, max_attempts)",
            "        for dest in dests:",
            "            self.assertEqual(expected_number, len(dest))",
            "            # Verify that there are no duplicates among a destination",
            "            self.assertEqual(len(dest), len(set(dest)))",
            "            # Verify that none of the selected hosts appear in the alternates.",
            "            for alt in dest[1:]:",
            "                self.assertNotIn(alt, selected_hosts)",
            "",
            "    def test_not_enough_alternates(self):",
            "        self._test_not_enough_alternates(num_hosts=100, max_attempts=5)",
            "        self._test_not_enough_alternates(num_hosts=5, max_attempts=5)",
            "        self._test_not_enough_alternates(num_hosts=3, max_attempts=5)",
            "        self._test_not_enough_alternates(num_hosts=20, max_attempts=5)",
            "",
            "    @mock.patch.object(filter_scheduler.FilterScheduler, '_schedule')",
            "    def test_select_destinations_notifications(self, mock_schedule):",
            "        mock_schedule.return_value = [[mock.Mock()]]",
            "",
            "        with mock.patch.object(self.driver.notifier, 'info') as mock_info:",
            "            expected = {'num_instances': 1,",
            "                        'instance_properties': {'uuid': uuids.instance},",
            "                        'instance_type': {},",
            "                        'image': {}}",
            "            spec_obj = objects.RequestSpec(num_instances=1,",
            "                                           instance_uuid=uuids.instance)",
            "",
            "            self.driver.select_destinations(self.context, spec_obj,",
            "                    [uuids.instance], {}, None)",
            "",
            "            expected = [",
            "                mock.call(self.context, 'scheduler.select_destinations.start',",
            "                 dict(request_spec=expected)),",
            "                mock.call(self.context, 'scheduler.select_destinations.end',",
            "                 dict(request_spec=expected))]",
            "            self.assertEqual(expected, mock_info.call_args_list)",
            "",
            "    def test_get_all_host_states_provider_summaries_is_none(self):",
            "        \"\"\"Tests that HostManager.get_host_states_by_uuids is called with",
            "        compute_uuids being None when the incoming provider_summaries is None.",
            "        \"\"\"",
            "        with mock.patch.object(self.driver.host_manager,",
            "                               'get_host_states_by_uuids') as get_host_states:",
            "            self.driver._get_all_host_states(",
            "                mock.sentinel.ctxt, mock.sentinel.spec_obj, None)",
            "        # Make sure get_host_states_by_uuids was called with",
            "        # compute_uuids being None.",
            "        get_host_states.assert_called_once_with(",
            "            mock.sentinel.ctxt, None, mock.sentinel.spec_obj)",
            "",
            "    def test_get_all_host_states_provider_summaries_is_empty(self):",
            "        \"\"\"Tests that HostManager.get_host_states_by_uuids is called with",
            "        compute_uuids being [] when the incoming provider_summaries is {}.",
            "        \"\"\"",
            "        with mock.patch.object(self.driver.host_manager,",
            "                               'get_host_states_by_uuids') as get_host_states:",
            "            self.driver._get_all_host_states(",
            "                mock.sentinel.ctxt, mock.sentinel.spec_obj, {})",
            "        # Make sure get_host_states_by_uuids was called with",
            "        # compute_uuids being [].",
            "        get_host_states.assert_called_once_with(",
            "            mock.sentinel.ctxt, [], mock.sentinel.spec_obj)"
        ],
        "afterPatchFile": [
            "# Copyright 2011 OpenStack Foundation",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "\"\"\"",
            "Tests For Filter Scheduler.",
            "\"\"\"",
            "",
            "import mock",
            "",
            "from nova import exception",
            "from nova import objects",
            "from nova.scheduler import client",
            "from nova.scheduler.client import report",
            "from nova.scheduler import filter_scheduler",
            "from nova.scheduler import host_manager",
            "from nova.scheduler import utils as scheduler_utils",
            "from nova.scheduler import weights",
            "from nova import test  # noqa",
            "from nova.tests.unit.scheduler import test_scheduler",
            "from nova.tests import uuidsentinel as uuids",
            "",
            "",
            "class FilterSchedulerTestCase(test_scheduler.SchedulerTestCase):",
            "    \"\"\"Test case for Filter Scheduler.\"\"\"",
            "",
            "    driver_cls = filter_scheduler.FilterScheduler",
            "",
            "    @mock.patch('nova.scheduler.client.SchedulerClient')",
            "    def setUp(self, mock_client):",
            "        pc_client = mock.Mock(spec=report.SchedulerReportClient)",
            "        sched_client = mock.Mock(spec=client.SchedulerClient)",
            "        sched_client.reportclient = pc_client",
            "        mock_client.return_value = sched_client",
            "        self.placement_client = pc_client",
            "        super(FilterSchedulerTestCase, self).setUp()",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_claim_resources')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_all_host_states')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_sorted_hosts')",
            "    def test_schedule_placement_bad_comms(self, mock_get_hosts,",
            "            mock_get_all_states, mock_claim):",
            "        \"\"\"If there was a problem communicating with the Placement service,",
            "        alloc_reqs_by_rp_uuid will be None and we need to avoid trying to claim",
            "        in the Placement API.",
            "        \"\"\"",
            "        spec_obj = objects.RequestSpec(",
            "            num_instances=1,",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            instance_group=None)",
            "",
            "        host_state = mock.Mock(spec=host_manager.HostState,",
            "                host=mock.sentinel.host, uuid=uuids.cn1, cell_uuid=uuids.cell)",
            "        all_host_states = [host_state]",
            "        mock_get_all_states.return_value = all_host_states",
            "        mock_get_hosts.return_value = all_host_states",
            "",
            "        instance_uuids = None",
            "        ctx = mock.Mock()",
            "        selected_hosts = self.driver._schedule(ctx, spec_obj,",
            "            instance_uuids, None, mock.sentinel.provider_summaries)",
            "",
            "        mock_get_all_states.assert_called_once_with(",
            "            ctx.elevated.return_value, spec_obj,",
            "            mock.sentinel.provider_summaries)",
            "        mock_get_hosts.assert_called_once_with(spec_obj, all_host_states, 0)",
            "",
            "        self.assertEqual(len(selected_hosts), 1)",
            "        self.assertEqual([[host_state]], selected_hosts)",
            "",
            "        # Ensure that we have consumed the resources on the chosen host states",
            "        host_state.consume_from_request.assert_called_once_with(spec_obj)",
            "",
            "        # And ensure we never called _claim_resources()",
            "        self.assertFalse(mock_claim.called)",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_claim_resources')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_all_host_states')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_sorted_hosts')",
            "    def test_schedule_old_conductor(self, mock_get_hosts,",
            "            mock_get_all_states, mock_claim):",
            "        \"\"\"Old conductor can call scheduler without the instance_uuids",
            "        parameter. When this happens, we need to ensure we do not attempt to",
            "        claim resources in the placement API since obviously we need instance",
            "        UUIDs to perform those claims.",
            "        \"\"\"",
            "        spec_obj = objects.RequestSpec(",
            "            num_instances=1,",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            instance_group=None)",
            "",
            "        host_state = mock.Mock(spec=host_manager.HostState,",
            "            host=mock.sentinel.host, uuid=uuids.cn1)",
            "        all_host_states = [host_state]",
            "        mock_get_all_states.return_value = all_host_states",
            "        mock_get_hosts.return_value = all_host_states",
            "",
            "        instance_uuids = None",
            "        ctx = mock.Mock()",
            "        selected_hosts = self.driver._schedule(ctx, spec_obj,",
            "            instance_uuids, mock.sentinel.alloc_reqs_by_rp_uuid,",
            "            mock.sentinel.provider_summaries)",
            "",
            "        mock_get_all_states.assert_called_once_with(",
            "            ctx.elevated.return_value, spec_obj,",
            "            mock.sentinel.provider_summaries)",
            "        mock_get_hosts.assert_called_once_with(spec_obj, all_host_states, 0)",
            "",
            "        self.assertEqual(len(selected_hosts), 1)",
            "        self.assertEqual([[host_state]], selected_hosts)",
            "",
            "        # Ensure that we have consumed the resources on the chosen host states",
            "        host_state.consume_from_request.assert_called_once_with(spec_obj)",
            "",
            "        # And ensure we never called _claim_resources()",
            "        self.assertFalse(mock_claim.called)",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_claim_resources')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_all_host_states')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_sorted_hosts')",
            "    def _test_schedule_successful_claim(self, mock_get_hosts,",
            "            mock_get_all_states, mock_claim, num_instances=1):",
            "        spec_obj = objects.RequestSpec(",
            "            num_instances=num_instances,",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            instance_group=None)",
            "",
            "        host_state = mock.Mock(spec=host_manager.HostState,",
            "            host=mock.sentinel.host, uuid=uuids.cn1, cell_uuid=uuids.cell1)",
            "        all_host_states = [host_state]",
            "        mock_get_all_states.return_value = all_host_states",
            "        mock_get_hosts.return_value = all_host_states",
            "        mock_claim.return_value = True",
            "",
            "        instance_uuids = [uuids.instance]",
            "        alloc_reqs_by_rp_uuid = {",
            "            uuids.cn1: [mock.sentinel.alloc_req],",
            "        }",
            "        ctx = mock.Mock()",
            "        selected_hosts = self.driver._schedule(ctx, spec_obj, instance_uuids,",
            "                alloc_reqs_by_rp_uuid, mock.sentinel.provider_summaries)",
            "",
            "        mock_get_all_states.assert_called_once_with(",
            "            ctx.elevated.return_value, spec_obj,",
            "            mock.sentinel.provider_summaries)",
            "        mock_get_hosts.assert_called()",
            "        mock_claim.assert_called_once_with(ctx.elevated.return_value, spec_obj,",
            "            uuids.instance, [mock.sentinel.alloc_req])",
            "",
            "        self.assertEqual(len(selected_hosts), 1)",
            "        self.assertEqual([[host_state]], selected_hosts)",
            "",
            "        # Ensure that we have consumed the resources on the chosen host states",
            "        host_state.consume_from_request.assert_called_once_with(spec_obj)",
            "",
            "    def test_schedule_successful_claim(self):",
            "        self._test_schedule_successful_claim()",
            "",
            "    def test_schedule_old_reqspec_and_move_operation(self):",
            "        \"\"\"This test is for verifying that in case of a move operation with an",
            "        original RequestSpec created for 3 concurrent instances, we only verify",
            "        the instance that is moved.",
            "        \"\"\"",
            "        self._test_schedule_successful_claim(num_instances=3)",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_cleanup_allocations')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_claim_resources')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_all_host_states')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_sorted_hosts')",
            "    def test_schedule_unsuccessful_claim(self, mock_get_hosts,",
            "            mock_get_all_states, mock_claim, mock_cleanup):",
            "        \"\"\"Tests that we return an empty list if we are unable to successfully",
            "        claim resources for the instance",
            "        \"\"\"",
            "        spec_obj = objects.RequestSpec(",
            "            num_instances=1,",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            instance_group=None)",
            "",
            "        host_state = mock.Mock(spec=host_manager.HostState,",
            "            host=mock.sentinel.host, uuid=uuids.cn1, cell_uuid=uuids.cell1)",
            "        all_host_states = [host_state]",
            "        mock_get_all_states.return_value = all_host_states",
            "        mock_get_hosts.return_value = all_host_states",
            "        mock_claim.return_value = False",
            "",
            "        instance_uuids = [uuids.instance]",
            "        alloc_reqs_by_rp_uuid = {",
            "            uuids.cn1: [mock.sentinel.alloc_req],",
            "        }",
            "        ctx = mock.Mock()",
            "        selected_hosts = self.driver._schedule(ctx, spec_obj,",
            "            instance_uuids, alloc_reqs_by_rp_uuid,",
            "            mock.sentinel.provider_summaries)",
            "",
            "        mock_get_all_states.assert_called_once_with(",
            "            ctx.elevated.return_value, spec_obj,",
            "            mock.sentinel.provider_summaries)",
            "        mock_get_hosts.assert_called_once_with(spec_obj, all_host_states, 0)",
            "        mock_claim.assert_called_once_with(ctx.elevated.return_value, spec_obj,",
            "            uuids.instance, [mock.sentinel.alloc_req])",
            "",
            "        self.assertEqual([], selected_hosts)",
            "",
            "        mock_cleanup.assert_called_once_with([])",
            "",
            "        # Ensure that we have consumed the resources on the chosen host states",
            "        self.assertFalse(host_state.consume_from_request.called)",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_cleanup_allocations')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_claim_resources')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_all_host_states')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_sorted_hosts')",
            "    def test_schedule_not_all_instance_clean_claimed(self, mock_get_hosts,",
            "            mock_get_all_states, mock_claim, mock_cleanup):",
            "        \"\"\"Tests that we clean up previously-allocated instances if not all",
            "        instances could be scheduled",
            "        \"\"\"",
            "        spec_obj = objects.RequestSpec(",
            "            num_instances=2,",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            instance_group=None)",
            "",
            "        host_state = mock.Mock(spec=host_manager.HostState,",
            "            host=mock.sentinel.host, uuid=uuids.cn1, cell_uuid=uuids.cell1)",
            "        all_host_states = [host_state]",
            "        mock_get_all_states.return_value = all_host_states",
            "        mock_get_hosts.side_effect = [",
            "            all_host_states,  # first instance: return all the hosts (only one)",
            "            [],  # second: act as if no more hosts that meet criteria",
            "            all_host_states,  # the final call when creating alternates",
            "        ]",
            "        mock_claim.return_value = True",
            "",
            "        instance_uuids = [uuids.instance1, uuids.instance2]",
            "        alloc_reqs_by_rp_uuid = {",
            "            uuids.cn1: [mock.sentinel.alloc_req],",
            "        }",
            "        ctx = mock.Mock()",
            "        self.driver._schedule(ctx, spec_obj, instance_uuids,",
            "            alloc_reqs_by_rp_uuid, mock.sentinel.provider_summaries)",
            "",
            "        # Ensure we cleaned up the first successfully-claimed instance",
            "        mock_cleanup.assert_called_once_with([uuids.instance1])",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_claim_resources')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_all_host_states')",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_get_sorted_hosts')",
            "    def test_schedule_instance_group(self, mock_get_hosts,",
            "            mock_get_all_states, mock_claim):",
            "        \"\"\"Test that since the request spec object contains an instance group",
            "        object, that upon choosing a host in the primary schedule loop,",
            "        that we update the request spec's instance group information",
            "        \"\"\"",
            "        num_instances = 2",
            "        ig = objects.InstanceGroup(hosts=[])",
            "        spec_obj = objects.RequestSpec(",
            "            num_instances=num_instances,",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            instance_group=ig)",
            "",
            "        hs1 = mock.Mock(spec=host_manager.HostState, host='host1',",
            "            uuid=uuids.cn1, cell_uuid=uuids.cell1)",
            "        hs2 = mock.Mock(spec=host_manager.HostState, host='host2',",
            "            uuid=uuids.cn2, cell_uuid=uuids.cell2)",
            "        all_host_states = [hs1, hs2]",
            "        mock_get_all_states.return_value = all_host_states",
            "        mock_claim.return_value = True",
            "",
            "        alloc_reqs_by_rp_uuid = {",
            "            uuids.cn1: [mock.sentinel.alloc_req_cn1],",
            "            uuids.cn2: [mock.sentinel.alloc_req_cn2],",
            "        }",
            "",
            "        # Simulate host 1 and host 2 being randomly returned first by",
            "        # _get_sorted_hosts() in the two iterations for each instance in",
            "        # num_instances",
            "        mock_get_hosts.side_effect = ([hs2, hs1], [hs1, hs2],",
            "                                      [hs2, hs1], [hs1, hs2])",
            "        instance_uuids = [",
            "            getattr(uuids, 'instance%d' % x) for x in range(num_instances)",
            "        ]",
            "        ctx = mock.Mock()",
            "        self.driver._schedule(ctx, spec_obj, instance_uuids,",
            "            alloc_reqs_by_rp_uuid, mock.sentinel.provider_summaries)",
            "",
            "        # Check that we called _claim_resources() for both the first and second",
            "        # host state",
            "        claim_calls = [",
            "            mock.call(ctx.elevated.return_value, spec_obj,",
            "                uuids.instance0, [mock.sentinel.alloc_req_cn2]),",
            "            mock.call(ctx.elevated.return_value, spec_obj,",
            "                uuids.instance1, [mock.sentinel.alloc_req_cn1]),",
            "        ]",
            "        mock_claim.assert_has_calls(claim_calls)",
            "",
            "        # Check that _get_sorted_hosts() is called twice and that the",
            "        # second time, we pass it the hosts that were returned from",
            "        # _get_sorted_hosts() the first time",
            "        sorted_host_calls = [",
            "            mock.call(spec_obj, all_host_states, 0),",
            "            mock.call(spec_obj, [hs2, hs1], 1),",
            "        ]",
            "        mock_get_hosts.assert_has_calls(sorted_host_calls)",
            "",
            "        # The instance group object should have both host1 and host2 in its",
            "        # instance group hosts list and there should not be any \"changes\" to",
            "        # save in the instance group object",
            "        self.assertEqual(['host2', 'host1'], ig.hosts)",
            "        self.assertEqual({}, ig.obj_get_changes())",
            "",
            "    @mock.patch('random.choice', side_effect=lambda x: x[1])",
            "    @mock.patch('nova.scheduler.host_manager.HostManager.get_weighed_hosts')",
            "    @mock.patch('nova.scheduler.host_manager.HostManager.get_filtered_hosts')",
            "    def test_get_sorted_hosts(self, mock_filt, mock_weighed, mock_rand):",
            "        \"\"\"Tests the call that returns a sorted list of hosts by calling the",
            "        host manager's filtering and weighing routines",
            "        \"\"\"",
            "        self.flags(host_subset_size=2, group='filter_scheduler')",
            "        hs1 = mock.Mock(spec=host_manager.HostState, host='host1',",
            "                cell_uuid=uuids.cell1)",
            "        hs2 = mock.Mock(spec=host_manager.HostState, host='host2',",
            "                cell_uuid=uuids.cell2)",
            "        all_host_states = [hs1, hs2]",
            "",
            "        mock_weighed.return_value = [",
            "            weights.WeighedHost(hs1, 1.0), weights.WeighedHost(hs2, 1.0),",
            "        ]",
            "",
            "        results = self.driver._get_sorted_hosts(mock.sentinel.spec,",
            "            all_host_states, mock.sentinel.index)",
            "",
            "        mock_filt.assert_called_once_with(all_host_states, mock.sentinel.spec,",
            "            mock.sentinel.index)",
            "",
            "        mock_weighed.assert_called_once_with(mock_filt.return_value,",
            "            mock.sentinel.spec)",
            "",
            "        # We override random.choice() to pick the **second** element of the",
            "        # returned weighed hosts list, which is the host state #2. This tests",
            "        # the code path that combines the randomly-chosen host with the",
            "        # remaining list of weighed host state objects",
            "        self.assertEqual([hs2, hs1], results)",
            "",
            "    @mock.patch('random.choice', side_effect=lambda x: x[0])",
            "    @mock.patch('nova.scheduler.host_manager.HostManager.get_weighed_hosts')",
            "    @mock.patch('nova.scheduler.host_manager.HostManager.get_filtered_hosts')",
            "    def test_get_sorted_hosts_subset_less_than_num_weighed(self, mock_filt,",
            "            mock_weighed, mock_rand):",
            "        \"\"\"Tests that when we have >1 weighed hosts but a host subset size of",
            "        1, that we always pick the first host in the weighed host",
            "        \"\"\"",
            "        self.flags(host_subset_size=1, group='filter_scheduler')",
            "        hs1 = mock.Mock(spec=host_manager.HostState, host='host1',",
            "                cell_uuid=uuids.cell1)",
            "        hs2 = mock.Mock(spec=host_manager.HostState, host='host2',",
            "                cell_uuid=uuids.cell2)",
            "        all_host_states = [hs1, hs2]",
            "",
            "        mock_weighed.return_value = [",
            "            weights.WeighedHost(hs1, 1.0), weights.WeighedHost(hs2, 1.0),",
            "        ]",
            "",
            "        results = self.driver._get_sorted_hosts(mock.sentinel.spec,",
            "            all_host_states, mock.sentinel.index)",
            "",
            "        mock_filt.assert_called_once_with(all_host_states, mock.sentinel.spec,",
            "            mock.sentinel.index)",
            "",
            "        mock_weighed.assert_called_once_with(mock_filt.return_value,",
            "            mock.sentinel.spec)",
            "",
            "        # We should be randomly selecting only from a list of one host state",
            "        mock_rand.assert_called_once_with([hs1])",
            "        self.assertEqual([hs1, hs2], results)",
            "",
            "    @mock.patch('random.choice', side_effect=lambda x: x[0])",
            "    @mock.patch('nova.scheduler.host_manager.HostManager.get_weighed_hosts')",
            "    @mock.patch('nova.scheduler.host_manager.HostManager.get_filtered_hosts')",
            "    def test_get_sorted_hosts_subset_greater_than_num_weighed(self, mock_filt,",
            "            mock_weighed, mock_rand):",
            "        \"\"\"Hosts should still be chosen if host subset size is larger than",
            "        number of weighed hosts.",
            "        \"\"\"",
            "        self.flags(host_subset_size=20, group='filter_scheduler')",
            "        hs1 = mock.Mock(spec=host_manager.HostState, host='host1',",
            "                cell_uuid=uuids.cell1)",
            "        hs2 = mock.Mock(spec=host_manager.HostState, host='host2',",
            "                cell_uuid=uuids.cell2)",
            "        all_host_states = [hs1, hs2]",
            "",
            "        mock_weighed.return_value = [",
            "            weights.WeighedHost(hs1, 1.0), weights.WeighedHost(hs2, 1.0),",
            "        ]",
            "",
            "        results = self.driver._get_sorted_hosts(mock.sentinel.spec,",
            "            all_host_states, mock.sentinel.index)",
            "",
            "        mock_filt.assert_called_once_with(all_host_states, mock.sentinel.spec,",
            "            mock.sentinel.index)",
            "",
            "        mock_weighed.assert_called_once_with(mock_filt.return_value,",
            "            mock.sentinel.spec)",
            "",
            "        # We overrode random.choice() to return the first element in the list,",
            "        # so even though we had a host_subset_size greater than the number of",
            "        # weighed hosts (2), we just random.choice() on the entire set of",
            "        # weighed hosts and thus return [hs1, hs2]",
            "        self.assertEqual([hs1, hs2], results)",
            "",
            "    @mock.patch('random.shuffle', side_effect=lambda x: x.reverse())",
            "    @mock.patch('nova.scheduler.host_manager.HostManager.get_weighed_hosts')",
            "    @mock.patch('nova.scheduler.host_manager.HostManager.get_filtered_hosts')",
            "    def test_get_sorted_hosts_shuffle_top_equal(self, mock_filt, mock_weighed,",
            "                                                mock_shuffle):",
            "        \"\"\"Tests that top best weighed hosts are shuffled when enabled.",
            "        \"\"\"",
            "        self.flags(host_subset_size=1, group='filter_scheduler')",
            "        self.flags(shuffle_best_same_weighed_hosts=True,",
            "                   group='filter_scheduler')",
            "        hs1 = mock.Mock(spec=host_manager.HostState, host='host1')",
            "        hs2 = mock.Mock(spec=host_manager.HostState, host='host2')",
            "        hs3 = mock.Mock(spec=host_manager.HostState, host='host3')",
            "        hs4 = mock.Mock(spec=host_manager.HostState, host='host4')",
            "        all_host_states = [hs1, hs2, hs3, hs4]",
            "",
            "        mock_weighed.return_value = [",
            "            weights.WeighedHost(hs1, 1.0),",
            "            weights.WeighedHost(hs2, 1.0),",
            "            weights.WeighedHost(hs3, 0.5),",
            "            weights.WeighedHost(hs4, 0.5),",
            "        ]",
            "",
            "        results = self.driver._get_sorted_hosts(mock.sentinel.spec,",
            "            all_host_states, mock.sentinel.index)",
            "",
            "        mock_filt.assert_called_once_with(all_host_states, mock.sentinel.spec,",
            "            mock.sentinel.index)",
            "",
            "        mock_weighed.assert_called_once_with(mock_filt.return_value,",
            "            mock.sentinel.spec)",
            "",
            "        # We override random.shuffle() to reverse the list, thus the",
            "        # head of the list should become [host#2, host#1]",
            "        # (as the host_subset_size is 1) and the tail should stay the same.",
            "        self.assertEqual([hs2, hs1, hs3, hs4], results)",
            "",
            "    def test_cleanup_allocations(self):",
            "        instance_uuids = []",
            "        # Check we don't do anything if there's no instance UUIDs to cleanup",
            "        # allocations for",
            "        pc = self.placement_client",
            "",
            "        self.driver._cleanup_allocations(instance_uuids)",
            "        self.assertFalse(pc.delete_allocation_for_instance.called)",
            "",
            "        instance_uuids = [uuids.instance1, uuids.instance2]",
            "        self.driver._cleanup_allocations(instance_uuids)",
            "",
            "        exp_calls = [mock.call(uuids.instance1), mock.call(uuids.instance2)]",
            "        pc.delete_allocation_for_instance.assert_has_calls(exp_calls)",
            "",
            "    def test_claim_resources(self):",
            "        \"\"\"Tests that when _schedule() calls _claim_resources(), that we",
            "        appropriately call the placement client to claim resources for the",
            "        instance.",
            "        \"\"\"",
            "        ctx = mock.Mock(user_id=uuids.user_id)",
            "        spec_obj = objects.RequestSpec(project_id=uuids.project_id)",
            "        instance_uuid = uuids.instance",
            "        alloc_reqs = [mock.sentinel.alloc_req]",
            "",
            "        res = self.driver._claim_resources(ctx, spec_obj, instance_uuid,",
            "            alloc_reqs)",
            "",
            "        pc = self.placement_client",
            "        pc.claim_resources.return_value = True",
            "        pc.claim_resources.assert_called_once_with(uuids.instance,",
            "            mock.sentinel.alloc_req, uuids.project_id, uuids.user_id)",
            "        self.assertTrue(res)",
            "",
            "    @mock.patch('nova.scheduler.utils.request_is_rebuild')",
            "    def test_claim_resouces_for_policy_check(self, mock_policy):",
            "        mock_policy.return_value = True",
            "        res = self.driver._claim_resources(None, mock.sentinel.spec_obj,",
            "                                           mock.sentinel.instance_uuid,",
            "                                           [])",
            "        self.assertTrue(res)",
            "        mock_policy.assert_called_once_with(mock.sentinel.spec_obj)",
            "        self.assertFalse(self.placement_client.claim_resources.called)",
            "",
            "    def test_add_retry_host(self):",
            "        retry = dict(num_attempts=1, hosts=[])",
            "        filter_properties = dict(retry=retry)",
            "        host = \"fakehost\"",
            "        node = \"fakenode\"",
            "",
            "        scheduler_utils._add_retry_host(filter_properties, host, node)",
            "",
            "        hosts = filter_properties['retry']['hosts']",
            "        self.assertEqual(1, len(hosts))",
            "        self.assertEqual([host, node], hosts[0])",
            "",
            "    def test_post_select_populate(self):",
            "        # Test addition of certain filter props after a node is selected.",
            "        retry = {'hosts': [], 'num_attempts': 1}",
            "        filter_properties = {'retry': retry}",
            "",
            "        host_state = host_manager.HostState('host', 'node', uuids.cell)",
            "        host_state.limits['vcpu'] = 5",
            "        scheduler_utils.populate_filter_properties(filter_properties,",
            "                host_state)",
            "",
            "        self.assertEqual(['host', 'node'],",
            "                         filter_properties['retry']['hosts'][0])",
            "",
            "        self.assertEqual({'vcpu': 5}, host_state.limits)",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_schedule')",
            "    def test_select_destinations_match_num_instances(self, mock_schedule):",
            "        \"\"\"Tests that the select_destinations() method returns the list of",
            "        hosts from the _schedule() method when the number of returned hosts",
            "        equals the number of instance UUIDs passed in.",
            "        \"\"\"",
            "        spec_obj = objects.RequestSpec(",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            num_instances=1)",
            "",
            "        mock_schedule.return_value = [[mock.sentinel.hs1]]",
            "",
            "        dests = self.driver.select_destinations(self.context, spec_obj,",
            "            [mock.sentinel.instance_uuid], mock.sentinel.alloc_reqs_by_rp_uuid,",
            "            mock.sentinel.p_sums)",
            "",
            "        mock_schedule.assert_called_once_with(self.context, spec_obj,",
            "            [mock.sentinel.instance_uuid], mock.sentinel.alloc_reqs_by_rp_uuid,",
            "            mock.sentinel.p_sums)",
            "",
            "        self.assertEqual([mock.sentinel.hs1], dests)",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_schedule')",
            "    def test_select_destinations_for_move_ops(self, mock_schedule):",
            "        \"\"\"Tests that the select_destinations() method verifies the number of",
            "        hosts returned from the _schedule() method against the number of",
            "        instance UUIDs passed as a parameter and not against the RequestSpec",
            "        num_instances field since the latter could be wrong in case of a move",
            "        operation.",
            "        \"\"\"",
            "        spec_obj = objects.RequestSpec(",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            num_instances=2)",
            "",
            "        host_state = mock.Mock(spec=host_manager.HostState,",
            "                cell_uuid=uuids.cell)",
            "        mock_schedule.return_value = [[host_state]]",
            "",
            "        dests = self.driver.select_destinations(self.context, spec_obj,",
            "            [mock.sentinel.instance_uuid], mock.sentinel.alloc_reqs_by_rp_uuid,",
            "            mock.sentinel.p_sums)",
            "",
            "        mock_schedule.assert_called_once_with(self.context, spec_obj,",
            "            [mock.sentinel.instance_uuid], mock.sentinel.alloc_reqs_by_rp_uuid,",
            "            mock.sentinel.p_sums)",
            "",
            "        self.assertEqual([host_state], dests)",
            "",
            "    @mock.patch('nova.scheduler.filter_scheduler.FilterScheduler.'",
            "                '_schedule')",
            "    def test_select_destinations_fewer_num_instances(self, mock_schedule):",
            "        \"\"\"Tests that the select_destinations() method properly handles",
            "        resetting host state objects and raising NoValidHost when the",
            "        _schedule() method returns no host matches.",
            "        \"\"\"",
            "        spec_obj = objects.RequestSpec(",
            "            flavor=objects.Flavor(memory_mb=512,",
            "                                  root_gb=512,",
            "                                  ephemeral_gb=0,",
            "                                  swap=0,",
            "                                  vcpus=1),",
            "            project_id=uuids.project_id,",
            "            num_instances=2)",
            "",
            "        host_state = mock.Mock(spec=host_manager.HostState,",
            "                cell_uuid=uuids.cell)",
            "        mock_schedule.return_value = [[host_state]]",
            "",
            "        self.assertRaises(exception.NoValidHost,",
            "            self.driver.select_destinations, self.context, spec_obj,",
            "            [mock.sentinel.instance_uuid1, mock.sentinel.instance_uuid2],",
            "            mock.sentinel.alloc_reqs_by_rp_uuid, mock.sentinel.p_sums)",
            "",
            "        # Verify that the host state object has been marked as not updated so",
            "        # it's picked up in the next pull from the DB for compute node objects",
            "        self.assertIsNone(host_state.updated)",
            "",
            "    @mock.patch(\"nova.scheduler.host_manager.HostState.consume_from_request\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_claim_resources\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_get_sorted_hosts\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_get_all_host_states\")",
            "    def _test_alternates_returned(self, mock_get_all_hosts, mock_sorted,",
            "            mock_claim, mock_consume, num_instances=2, num_alternates=2):",
            "        all_host_states = []",
            "        alloc_reqs = {}",
            "        for num in range(10):",
            "            host_name = \"host%s\" % num",
            "            hs = host_manager.HostState(host_name, \"node%s\" % num,",
            "                    uuids.cell)",
            "            hs.uuid = getattr(uuids, host_name)",
            "            all_host_states.append(hs)",
            "            alloc_reqs[hs.uuid] = {}",
            "",
            "        mock_get_all_hosts.return_value = all_host_states",
            "        mock_sorted.return_value = all_host_states",
            "        mock_claim.return_value = True",
            "        total_returned = num_alternates + 1",
            "        self.flags(max_attempts=total_returned, group=\"scheduler\")",
            "        instance_uuids = [getattr(uuids, \"inst%s\" % num)",
            "                for num in range(num_instances)]",
            "",
            "        spec_obj = objects.RequestSpec(",
            "                num_instances=num_instances,",
            "                flavor=objects.Flavor(memory_mb=512,",
            "                                      root_gb=512,",
            "                                      ephemeral_gb=0,",
            "                                      swap=0,",
            "                                      vcpus=1),",
            "                project_id=uuids.project_id,",
            "                instance_group=None)",
            "",
            "        dests = self.driver._schedule(self.context, spec_obj,",
            "                instance_uuids, alloc_reqs, None)",
            "        self.assertEqual(num_instances, len(dests))",
            "        # Filtering and weighing hosts should be called num_instances + 1 times",
            "        # unless num_instances == 1.",
            "        self.assertEqual(num_instances + 1 if num_instances > 1 else 1,",
            "                         mock_sorted.call_count,",
            "                         'Unexpected number of calls to filter hosts for %s '",
            "                         'instances.' % num_instances)",
            "        selected_hosts = [dest[0] for dest in dests]",
            "        for dest in dests:",
            "            self.assertEqual(total_returned, len(dest))",
            "            # Verify that there are no duplicates among a destination",
            "            self.assertEqual(len(dest), len(set(dest)))",
            "            # Verify that none of the selected hosts appear in the alternates.",
            "            for alt in dest[1:]:",
            "                self.assertNotIn(alt, selected_hosts)",
            "",
            "    def test_alternates_returned(self):",
            "        self._test_alternates_returned(num_instances=1, num_alternates=1)",
            "        self._test_alternates_returned(num_instances=3, num_alternates=0)",
            "        self._test_alternates_returned(num_instances=1, num_alternates=4)",
            "        self._test_alternates_returned(num_instances=2, num_alternates=3)",
            "        self._test_alternates_returned(num_instances=8, num_alternates=8)",
            "",
            "    @mock.patch(\"nova.scheduler.host_manager.HostState.consume_from_request\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_claim_resources\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_get_sorted_hosts\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_get_all_host_states\")",
            "    def test_alternates_same_cell(self, mock_get_all_hosts, mock_sorted,",
            "            mock_claim, mock_consume):",
            "        \"\"\"Tests getting alternates plus claims where the hosts are spread",
            "        across two cells.",
            "        \"\"\"",
            "        all_host_states = []",
            "        alloc_reqs = {}",
            "        for num in range(10):",
            "            host_name = \"host%s\" % num",
            "            cell_uuid = uuids.cell1 if num % 2 else uuids.cell2",
            "            hs = host_manager.HostState(host_name, \"node%s\" % num,",
            "                    cell_uuid)",
            "            hs.uuid = getattr(uuids, host_name)",
            "            all_host_states.append(hs)",
            "            alloc_reqs[hs.uuid] = {}",
            "",
            "        mock_get_all_hosts.return_value = all_host_states",
            "        # There are two instances so _get_sorted_hosts is called once per",
            "        # instance and then once again before picking alternates.",
            "        mock_sorted.side_effect = [all_host_states,",
            "                                   list(reversed(all_host_states)),",
            "                                   all_host_states]",
            "        mock_claim.return_value = True",
            "        total_returned = 3",
            "        self.flags(max_attempts=total_returned, group=\"scheduler\")",
            "        instance_uuids = [uuids.inst1, uuids.inst2]",
            "        num_instances = len(instance_uuids)",
            "",
            "        spec_obj = objects.RequestSpec(",
            "                num_instances=num_instances,",
            "                flavor=objects.Flavor(memory_mb=512,",
            "                                      root_gb=512,",
            "                                      ephemeral_gb=0,",
            "                                      swap=0,",
            "                                      vcpus=1),",
            "                project_id=uuids.project_id,",
            "                instance_group=None)",
            "",
            "        dests = self.driver._schedule(self.context, spec_obj,",
            "                instance_uuids, alloc_reqs, None)",
            "        # There should be max_attempts hosts per instance (1 selected, 2 alts)",
            "        self.assertEqual(total_returned, len(dests[0]))",
            "        self.assertEqual(total_returned, len(dests[1]))",
            "        # Verify that the two selected hosts are not in the same cell.",
            "        self.assertNotEqual(dests[0][0].cell_uuid, dests[1][0].cell_uuid)",
            "        for dest in dests:",
            "            selected_host = dest[0]",
            "            selected_cell_uuid = selected_host.cell_uuid",
            "            for alternate in dest[1:]:",
            "                self.assertEqual(alternate.cell_uuid, selected_cell_uuid)",
            "",
            "    @mock.patch(\"nova.scheduler.host_manager.HostState.consume_from_request\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_claim_resources\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_get_sorted_hosts\")",
            "    @mock.patch(\"nova.scheduler.filter_scheduler.FilterScheduler.\"",
            "                \"_get_all_host_states\")",
            "    def _test_not_enough_alternates(self, mock_get_all_hosts, mock_sorted,",
            "            mock_claim, mock_consume, num_hosts, max_attempts):",
            "        all_host_states = []",
            "        alloc_reqs = {}",
            "        for num in range(num_hosts):",
            "            host_name = \"host%s\" % num",
            "            hs = host_manager.HostState(host_name, \"node%s\" % num,",
            "                    uuids.cell)",
            "            hs.uuid = getattr(uuids, host_name)",
            "            all_host_states.append(hs)",
            "            alloc_reqs[hs.uuid] = {}",
            "",
            "        mock_get_all_hosts.return_value = all_host_states",
            "        mock_sorted.return_value = all_host_states",
            "        mock_claim.return_value = True",
            "        # Set the total returned to more than the number of available hosts",
            "        self.flags(max_attempts=max_attempts, group=\"scheduler\")",
            "        instance_uuids = [uuids.inst1, uuids.inst2]",
            "        num_instances = len(instance_uuids)",
            "",
            "        spec_obj = objects.RequestSpec(",
            "                num_instances=num_instances,",
            "                flavor=objects.Flavor(memory_mb=512,",
            "                                      root_gb=512,",
            "                                      ephemeral_gb=0,",
            "                                      swap=0,",
            "                                      vcpus=1),",
            "                project_id=uuids.project_id,",
            "                instance_group=None)",
            "",
            "        dests = self.driver._schedule(self.context, spec_obj,",
            "                instance_uuids, alloc_reqs, None)",
            "        self.assertEqual(num_instances, len(dests))",
            "        selected_hosts = [dest[0] for dest in dests]",
            "        # The number returned for each destination should be the less of the",
            "        # number of available host and the max_attempts setting.",
            "        expected_number = min(num_hosts, max_attempts)",
            "        for dest in dests:",
            "            self.assertEqual(expected_number, len(dest))",
            "            # Verify that there are no duplicates among a destination",
            "            self.assertEqual(len(dest), len(set(dest)))",
            "            # Verify that none of the selected hosts appear in the alternates.",
            "            for alt in dest[1:]:",
            "                self.assertNotIn(alt, selected_hosts)",
            "",
            "    def test_not_enough_alternates(self):",
            "        self._test_not_enough_alternates(num_hosts=100, max_attempts=5)",
            "        self._test_not_enough_alternates(num_hosts=5, max_attempts=5)",
            "        self._test_not_enough_alternates(num_hosts=3, max_attempts=5)",
            "        self._test_not_enough_alternates(num_hosts=20, max_attempts=5)",
            "",
            "    @mock.patch.object(filter_scheduler.FilterScheduler, '_schedule')",
            "    def test_select_destinations_notifications(self, mock_schedule):",
            "        mock_schedule.return_value = [[mock.Mock()]]",
            "",
            "        with mock.patch.object(self.driver.notifier, 'info') as mock_info:",
            "            expected = {'num_instances': 1,",
            "                        'instance_properties': {'uuid': uuids.instance},",
            "                        'instance_type': {},",
            "                        'image': {}}",
            "            spec_obj = objects.RequestSpec(num_instances=1,",
            "                                           instance_uuid=uuids.instance)",
            "",
            "            self.driver.select_destinations(self.context, spec_obj,",
            "                    [uuids.instance], {}, None)",
            "",
            "            expected = [",
            "                mock.call(self.context, 'scheduler.select_destinations.start',",
            "                 dict(request_spec=expected)),",
            "                mock.call(self.context, 'scheduler.select_destinations.end',",
            "                 dict(request_spec=expected))]",
            "            self.assertEqual(expected, mock_info.call_args_list)",
            "",
            "    def test_get_all_host_states_provider_summaries_is_none(self):",
            "        \"\"\"Tests that HostManager.get_host_states_by_uuids is called with",
            "        compute_uuids being None when the incoming provider_summaries is None.",
            "        \"\"\"",
            "        with mock.patch.object(self.driver.host_manager,",
            "                               'get_host_states_by_uuids') as get_host_states:",
            "            self.driver._get_all_host_states(",
            "                mock.sentinel.ctxt, mock.sentinel.spec_obj, None)",
            "        # Make sure get_host_states_by_uuids was called with",
            "        # compute_uuids being None.",
            "        get_host_states.assert_called_once_with(",
            "            mock.sentinel.ctxt, None, mock.sentinel.spec_obj)",
            "",
            "    def test_get_all_host_states_provider_summaries_is_empty(self):",
            "        \"\"\"Tests that HostManager.get_host_states_by_uuids is called with",
            "        compute_uuids being [] when the incoming provider_summaries is {}.",
            "        \"\"\"",
            "        with mock.patch.object(self.driver.host_manager,",
            "                               'get_host_states_by_uuids') as get_host_states:",
            "            self.driver._get_all_host_states(",
            "                mock.sentinel.ctxt, mock.sentinel.spec_obj, {})",
            "        # Make sure get_host_states_by_uuids was called with",
            "        # compute_uuids being [].",
            "        get_host_states.assert_called_once_with(",
            "            mock.sentinel.ctxt, [], mock.sentinel.spec_obj)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "529": [
                "FilterSchedulerTestCase",
                "test_claim_resources"
            ]
        },
        "addLocation": []
    }
}