{
    "label_studio/core/decorators.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1,
                "PatchRowcode": "+from functools import wraps"
            },
            "1": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2,
                "PatchRowcode": "+"
            },
            "2": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3,
                "PatchRowcode": "+"
            },
            "3": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " def permission_required(*permissions, fn=None):"
            },
            "4": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 5,
                "PatchRowcode": "     def decorator(view):"
            },
            "5": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 6,
                "PatchRowcode": "         def wrapped_view(self, request, *args, **kwargs):"
            },
            "6": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 22,
                "PatchRowcode": "         return wrapped_view"
            },
            "7": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 24,
                "PatchRowcode": "     return decorator"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 27,
                "PatchRowcode": "+def override_report_only_csp(view_func):"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 28,
                "PatchRowcode": "+    \"\"\""
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+    Decorator to switch report-only CSP to regular CSP. For use with core.middleware.HumanSignalCspMiddleware."
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+    \"\"\""
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+    @wraps(view_func)"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+    def wrapper(*args, **kwargs):"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+        response = view_func(*args, **kwargs)"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+        setattr(response, '_override_report_only_csp', True)"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 36,
                "PatchRowcode": "+        return response"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 38,
                "PatchRowcode": "+    return wrapper"
            }
        },
        "frontPatchFile": [
            "def permission_required(*permissions, fn=None):",
            "    def decorator(view):",
            "        def wrapped_view(self, request, *args, **kwargs):",
            "",
            "            if callable(fn):",
            "                obj = fn(request, *args, **kwargs)",
            "            else:",
            "                obj = fn",
            "",
            "            missing_permissions = [perm for perm in permissions if not request.user.has_perm(perm, obj)]",
            "            if any(missing_permissions):",
            "                # raises a permission denied exception causing a 403 response",
            "                self.permission_denied(",
            "                    request, message=('Permission denied: {}'.format(', '.join(missing_permissions)))",
            "                )",
            "",
            "            return view(self, request, *args, **kwargs)",
            "",
            "        return wrapped_view",
            "",
            "    return decorator"
        ],
        "afterPatchFile": [
            "from functools import wraps",
            "",
            "",
            "def permission_required(*permissions, fn=None):",
            "    def decorator(view):",
            "        def wrapped_view(self, request, *args, **kwargs):",
            "",
            "            if callable(fn):",
            "                obj = fn(request, *args, **kwargs)",
            "            else:",
            "                obj = fn",
            "",
            "            missing_permissions = [perm for perm in permissions if not request.user.has_perm(perm, obj)]",
            "            if any(missing_permissions):",
            "                # raises a permission denied exception causing a 403 response",
            "                self.permission_denied(",
            "                    request, message=('Permission denied: {}'.format(', '.join(missing_permissions)))",
            "                )",
            "",
            "            return view(self, request, *args, **kwargs)",
            "",
            "        return wrapped_view",
            "",
            "    return decorator",
            "",
            "",
            "def override_report_only_csp(view_func):",
            "    \"\"\"",
            "    Decorator to switch report-only CSP to regular CSP. For use with core.middleware.HumanSignalCspMiddleware.",
            "    \"\"\"",
            "",
            "    @wraps(view_func)",
            "    def wrapper(*args, **kwargs):",
            "        response = view_func(*args, **kwargs)",
            "        setattr(response, '_override_report_only_csp', True)",
            "        return response",
            "",
            "    return wrapper"
        ],
        "action": [
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "airflow.configuration"
        ]
    },
    "label_studio/core/middleware.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " import ujson as json"
            },
            "2": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " from core.utils.contextlog import ContextLog"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 9,
                "PatchRowcode": "+from csp.middleware import CSPMiddleware"
            },
            "4": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " from django.conf import settings"
            },
            "5": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " from django.contrib.auth import logout"
            },
            "6": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " from django.core.exceptions import MiddlewareNotUsed"
            },
            "7": {
                "beforePatchRowNumber": 207,
                "afterPatchRowNumber": 208,
                "PatchRowcode": "         request.session.set_expiry("
            },
            "8": {
                "beforePatchRowNumber": 208,
                "afterPatchRowNumber": 209,
                "PatchRowcode": "             settings.MAX_TIME_BETWEEN_ACTIVITY if request.session.get('keep_me_logged_in', True) else 0"
            },
            "9": {
                "beforePatchRowNumber": 209,
                "afterPatchRowNumber": 210,
                "PatchRowcode": "         )"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 211,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 212,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 213,
                "PatchRowcode": "+class HumanSignalCspMiddleware(CSPMiddleware):"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 214,
                "PatchRowcode": "+    \"\"\""
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 215,
                "PatchRowcode": "+    Extend CSPMiddleware to support switching report-only CSP to regular CSP."
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 216,
                "PatchRowcode": "+"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 217,
                "PatchRowcode": "+    For use with core.decorators.override_report_only_csp."
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 218,
                "PatchRowcode": "+    \"\"\""
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 219,
                "PatchRowcode": "+"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 220,
                "PatchRowcode": "+    def process_response(self, request, response):"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 221,
                "PatchRowcode": "+        response = super().process_response(request, response)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 222,
                "PatchRowcode": "+        if getattr(response, '_override_report_only_csp', False):"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 223,
                "PatchRowcode": "+            if csp_policy := response.get('Content-Security-Policy-Report-Only'):"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 224,
                "PatchRowcode": "+                response['Content-Security-Policy'] = csp_policy"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 225,
                "PatchRowcode": "+                del response['Content-Security-Policy-Report-Only']"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 226,
                "PatchRowcode": "+            delattr(response, '_override_report_only_csp')"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 227,
                "PatchRowcode": "+        return response"
            }
        },
        "frontPatchFile": [
            "\"\"\"This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.",
            "\"\"\"",
            "import logging",
            "import time",
            "from uuid import uuid4",
            "",
            "import ujson as json",
            "from core.utils.contextlog import ContextLog",
            "from django.conf import settings",
            "from django.contrib.auth import logout",
            "from django.core.exceptions import MiddlewareNotUsed",
            "from django.core.handlers.base import BaseHandler",
            "from django.http import HttpResponsePermanentRedirect",
            "from django.middleware.common import CommonMiddleware",
            "from django.utils.deprecation import MiddlewareMixin",
            "from django.utils.http import escape_leading_slashes",
            "from rest_framework.permissions import SAFE_METHODS",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "def enforce_csrf_checks(func):",
            "    \"\"\"Enable csrf for specified view func\"\"\"",
            "    # USE_ENFORCE_CSRF_CHECKS=False is for tests",
            "    if settings.USE_ENFORCE_CSRF_CHECKS:",
            "",
            "        def wrapper(request, *args, **kwargs):",
            "            return func(request, *args, **kwargs)",
            "",
            "        wrapper._dont_enforce_csrf_checks = False",
            "        return wrapper",
            "    else:",
            "        return func",
            "",
            "",
            "class DisableCSRF(MiddlewareMixin):",
            "    # disable csrf for api requests",
            "    def process_view(self, request, callback, *args, **kwargs):",
            "        if hasattr(callback, '_dont_enforce_csrf_checks'):",
            "            setattr(request, '_dont_enforce_csrf_checks', callback._dont_enforce_csrf_checks)",
            "        elif request.GET.get('enforce_csrf_checks'):  # _dont_enforce_csrf_checks is for test",
            "            setattr(request, '_dont_enforce_csrf_checks', False)",
            "        else:",
            "            setattr(request, '_dont_enforce_csrf_checks', True)",
            "",
            "",
            "class HttpSmartRedirectResponse(HttpResponsePermanentRedirect):",
            "    pass",
            "",
            "",
            "class CommonMiddlewareAppendSlashWithoutRedirect(CommonMiddleware):",
            "    \"\"\"This class converts HttpSmartRedirectResponse to the common response",
            "    of Django view, without redirect. This is necessary to match status_codes",
            "    for urls like /url?q=1 and /url/?q=1. If you don't use it, you will have 302",
            "    code always on pages without slash.",
            "    \"\"\"",
            "",
            "    response_redirect_class = HttpSmartRedirectResponse",
            "",
            "    def __init__(self, *args, **kwargs):",
            "        # create django request resolver",
            "        self.handler = BaseHandler()",
            "",
            "        # prevent recursive includes",
            "        old = settings.MIDDLEWARE",
            "        name = self.__module__ + '.' + self.__class__.__name__",
            "        settings.MIDDLEWARE = [i for i in settings.MIDDLEWARE if i != name]",
            "",
            "        self.handler.load_middleware()",
            "",
            "        settings.MIDDLEWARE = old",
            "        super(CommonMiddlewareAppendSlashWithoutRedirect, self).__init__(*args, **kwargs)",
            "",
            "    def get_full_path_with_slash(self, request):",
            "        \"\"\"Return the full path of the request with a trailing slash appended",
            "        without Exception in Debug mode",
            "        \"\"\"",
            "        new_path = request.get_full_path(force_append_slash=True)",
            "        # Prevent construction of scheme relative urls.",
            "        new_path = escape_leading_slashes(new_path)",
            "        return new_path",
            "",
            "    def process_response(self, request, response):",
            "        response = super(CommonMiddlewareAppendSlashWithoutRedirect, self).process_response(request, response)",
            "",
            "        request.editor_keymap = settings.EDITOR_KEYMAP",
            "",
            "        if isinstance(response, HttpSmartRedirectResponse):",
            "            if not request.path.endswith('/'):",
            "                # remove prefix SCRIPT_NAME",
            "                path = request.path[len(settings.FORCE_SCRIPT_NAME) :] if settings.FORCE_SCRIPT_NAME else request.path",
            "                request.path = path + '/'",
            "            # we don't need query string in path_info because it's in request.GET already",
            "            request.path_info = request.path",
            "            response = self.handler.get_response(request)",
            "",
            "        return response",
            "",
            "",
            "class SetSessionUIDMiddleware(CommonMiddleware):",
            "    def process_request(self, request):",
            "        if 'uid' not in request.session:",
            "            request.session['uid'] = str(uuid4())",
            "",
            "",
            "class ContextLogMiddleware(CommonMiddleware):",
            "    def __init__(self, get_response):",
            "        self.get_response = get_response",
            "        self.log = ContextLog()",
            "",
            "    def __call__(self, request):",
            "        body = None",
            "        try:",
            "            body = json.loads(request.body)",
            "        except:  # noqa: E722",
            "            try:",
            "                body = request.body.decode('utf-8')",
            "            except:  # noqa: E722",
            "                pass",
            "",
            "        if 'server_id' not in request:",
            "            setattr(request, 'server_id', self.log._get_server_id())",
            "",
            "        response = self.get_response(request)",
            "        self.log.send(request=request, response=response, body=body)",
            "",
            "        return response",
            "",
            "    def process_request(self, request):",
            "        if 'server_id' not in request:",
            "            setattr(request, 'server_id', self.log._get_server_id())",
            "",
            "",
            "class DatabaseIsLockedRetryMiddleware(CommonMiddleware):",
            "    \"\"\"Workaround for sqlite performance issues",
            "    we wait and retry request if database is locked\"\"\"",
            "",
            "    def __init__(self, get_response):",
            "        if settings.DJANGO_DB != settings.DJANGO_DB_SQLITE:",
            "            raise MiddlewareNotUsed()",
            "        self.get_response = get_response",
            "",
            "    def __call__(self, request):",
            "        response = self.get_response(request)",
            "        retries_number = 0",
            "        sleep_time = 1",
            "        backoff = 1.5",
            "        while (",
            "            response.status_code == 500",
            "            and hasattr(response, 'content')",
            "            and b'database-is-locked-error' in response.content",
            "            and retries_number < 15",
            "        ):",
            "            time.sleep(sleep_time)",
            "            response = self.get_response(request)",
            "            retries_number += 1",
            "            sleep_time *= backoff",
            "        return response",
            "",
            "",
            "class UpdateLastActivityMiddleware(CommonMiddleware):",
            "    def process_view(self, request, view_func, view_args, view_kwargs):",
            "        if hasattr(request, 'user') and request.method not in SAFE_METHODS:",
            "            if request.user.is_authenticated:",
            "                request.user.update_last_activity()",
            "",
            "",
            "class InactivitySessionTimeoutMiddleWare(CommonMiddleware):",
            "    \"\"\"Log the user out if they have been logged in for too long",
            "    or inactive for too long\"\"\"",
            "",
            "    # paths that don't count as user activity",
            "    NOT_USER_ACTIVITY_PATHS = []",
            "",
            "    def process_request(self, request) -> None:",
            "        if (",
            "            not hasattr(request, 'session')",
            "            or request.session.is_empty()",
            "            or not hasattr(request, 'user')",
            "            or not request.user.is_authenticated",
            "            or",
            "            # scim assign request.user implicitly, check CustomSCIMAuthCheckMiddleware",
            "            (hasattr(request, 'is_scim') and request.is_scim)",
            "        ):",
            "            return",
            "",
            "        current_time = time.time()",
            "        last_login = request.session['last_login'] if 'last_login' in request.session else 0",
            "",
            "        # Check if this request is too far from when the login happened",
            "        if (current_time - last_login) > settings.MAX_SESSION_AGE:",
            "            logger.info(",
            "                f'Request is too far from last login {current_time - last_login:.0f} > {settings.MAX_SESSION_AGE}; logout'",
            "            )",
            "            logout(request)",
            "",
            "        # Push the expiry to the max every time a new request is made to a url that indicates user activity",
            "        # but only if it's not a URL we want to ignore",
            "        for path in self.NOT_USER_ACTIVITY_PATHS:",
            "            if isinstance(path, str) and path == str(request.path_info):",
            "                return",
            "            elif 'query' in path:",
            "                parts = str(request.path_info).split('?')",
            "                if len(parts) == 2 and path['query'] in parts[1]:",
            "                    return",
            "",
            "        request.session.set_expiry(",
            "            settings.MAX_TIME_BETWEEN_ACTIVITY if request.session.get('keep_me_logged_in', True) else 0",
            "        )"
        ],
        "afterPatchFile": [
            "\"\"\"This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.",
            "\"\"\"",
            "import logging",
            "import time",
            "from uuid import uuid4",
            "",
            "import ujson as json",
            "from core.utils.contextlog import ContextLog",
            "from csp.middleware import CSPMiddleware",
            "from django.conf import settings",
            "from django.contrib.auth import logout",
            "from django.core.exceptions import MiddlewareNotUsed",
            "from django.core.handlers.base import BaseHandler",
            "from django.http import HttpResponsePermanentRedirect",
            "from django.middleware.common import CommonMiddleware",
            "from django.utils.deprecation import MiddlewareMixin",
            "from django.utils.http import escape_leading_slashes",
            "from rest_framework.permissions import SAFE_METHODS",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "def enforce_csrf_checks(func):",
            "    \"\"\"Enable csrf for specified view func\"\"\"",
            "    # USE_ENFORCE_CSRF_CHECKS=False is for tests",
            "    if settings.USE_ENFORCE_CSRF_CHECKS:",
            "",
            "        def wrapper(request, *args, **kwargs):",
            "            return func(request, *args, **kwargs)",
            "",
            "        wrapper._dont_enforce_csrf_checks = False",
            "        return wrapper",
            "    else:",
            "        return func",
            "",
            "",
            "class DisableCSRF(MiddlewareMixin):",
            "    # disable csrf for api requests",
            "    def process_view(self, request, callback, *args, **kwargs):",
            "        if hasattr(callback, '_dont_enforce_csrf_checks'):",
            "            setattr(request, '_dont_enforce_csrf_checks', callback._dont_enforce_csrf_checks)",
            "        elif request.GET.get('enforce_csrf_checks'):  # _dont_enforce_csrf_checks is for test",
            "            setattr(request, '_dont_enforce_csrf_checks', False)",
            "        else:",
            "            setattr(request, '_dont_enforce_csrf_checks', True)",
            "",
            "",
            "class HttpSmartRedirectResponse(HttpResponsePermanentRedirect):",
            "    pass",
            "",
            "",
            "class CommonMiddlewareAppendSlashWithoutRedirect(CommonMiddleware):",
            "    \"\"\"This class converts HttpSmartRedirectResponse to the common response",
            "    of Django view, without redirect. This is necessary to match status_codes",
            "    for urls like /url?q=1 and /url/?q=1. If you don't use it, you will have 302",
            "    code always on pages without slash.",
            "    \"\"\"",
            "",
            "    response_redirect_class = HttpSmartRedirectResponse",
            "",
            "    def __init__(self, *args, **kwargs):",
            "        # create django request resolver",
            "        self.handler = BaseHandler()",
            "",
            "        # prevent recursive includes",
            "        old = settings.MIDDLEWARE",
            "        name = self.__module__ + '.' + self.__class__.__name__",
            "        settings.MIDDLEWARE = [i for i in settings.MIDDLEWARE if i != name]",
            "",
            "        self.handler.load_middleware()",
            "",
            "        settings.MIDDLEWARE = old",
            "        super(CommonMiddlewareAppendSlashWithoutRedirect, self).__init__(*args, **kwargs)",
            "",
            "    def get_full_path_with_slash(self, request):",
            "        \"\"\"Return the full path of the request with a trailing slash appended",
            "        without Exception in Debug mode",
            "        \"\"\"",
            "        new_path = request.get_full_path(force_append_slash=True)",
            "        # Prevent construction of scheme relative urls.",
            "        new_path = escape_leading_slashes(new_path)",
            "        return new_path",
            "",
            "    def process_response(self, request, response):",
            "        response = super(CommonMiddlewareAppendSlashWithoutRedirect, self).process_response(request, response)",
            "",
            "        request.editor_keymap = settings.EDITOR_KEYMAP",
            "",
            "        if isinstance(response, HttpSmartRedirectResponse):",
            "            if not request.path.endswith('/'):",
            "                # remove prefix SCRIPT_NAME",
            "                path = request.path[len(settings.FORCE_SCRIPT_NAME) :] if settings.FORCE_SCRIPT_NAME else request.path",
            "                request.path = path + '/'",
            "            # we don't need query string in path_info because it's in request.GET already",
            "            request.path_info = request.path",
            "            response = self.handler.get_response(request)",
            "",
            "        return response",
            "",
            "",
            "class SetSessionUIDMiddleware(CommonMiddleware):",
            "    def process_request(self, request):",
            "        if 'uid' not in request.session:",
            "            request.session['uid'] = str(uuid4())",
            "",
            "",
            "class ContextLogMiddleware(CommonMiddleware):",
            "    def __init__(self, get_response):",
            "        self.get_response = get_response",
            "        self.log = ContextLog()",
            "",
            "    def __call__(self, request):",
            "        body = None",
            "        try:",
            "            body = json.loads(request.body)",
            "        except:  # noqa: E722",
            "            try:",
            "                body = request.body.decode('utf-8')",
            "            except:  # noqa: E722",
            "                pass",
            "",
            "        if 'server_id' not in request:",
            "            setattr(request, 'server_id', self.log._get_server_id())",
            "",
            "        response = self.get_response(request)",
            "        self.log.send(request=request, response=response, body=body)",
            "",
            "        return response",
            "",
            "    def process_request(self, request):",
            "        if 'server_id' not in request:",
            "            setattr(request, 'server_id', self.log._get_server_id())",
            "",
            "",
            "class DatabaseIsLockedRetryMiddleware(CommonMiddleware):",
            "    \"\"\"Workaround for sqlite performance issues",
            "    we wait and retry request if database is locked\"\"\"",
            "",
            "    def __init__(self, get_response):",
            "        if settings.DJANGO_DB != settings.DJANGO_DB_SQLITE:",
            "            raise MiddlewareNotUsed()",
            "        self.get_response = get_response",
            "",
            "    def __call__(self, request):",
            "        response = self.get_response(request)",
            "        retries_number = 0",
            "        sleep_time = 1",
            "        backoff = 1.5",
            "        while (",
            "            response.status_code == 500",
            "            and hasattr(response, 'content')",
            "            and b'database-is-locked-error' in response.content",
            "            and retries_number < 15",
            "        ):",
            "            time.sleep(sleep_time)",
            "            response = self.get_response(request)",
            "            retries_number += 1",
            "            sleep_time *= backoff",
            "        return response",
            "",
            "",
            "class UpdateLastActivityMiddleware(CommonMiddleware):",
            "    def process_view(self, request, view_func, view_args, view_kwargs):",
            "        if hasattr(request, 'user') and request.method not in SAFE_METHODS:",
            "            if request.user.is_authenticated:",
            "                request.user.update_last_activity()",
            "",
            "",
            "class InactivitySessionTimeoutMiddleWare(CommonMiddleware):",
            "    \"\"\"Log the user out if they have been logged in for too long",
            "    or inactive for too long\"\"\"",
            "",
            "    # paths that don't count as user activity",
            "    NOT_USER_ACTIVITY_PATHS = []",
            "",
            "    def process_request(self, request) -> None:",
            "        if (",
            "            not hasattr(request, 'session')",
            "            or request.session.is_empty()",
            "            or not hasattr(request, 'user')",
            "            or not request.user.is_authenticated",
            "            or",
            "            # scim assign request.user implicitly, check CustomSCIMAuthCheckMiddleware",
            "            (hasattr(request, 'is_scim') and request.is_scim)",
            "        ):",
            "            return",
            "",
            "        current_time = time.time()",
            "        last_login = request.session['last_login'] if 'last_login' in request.session else 0",
            "",
            "        # Check if this request is too far from when the login happened",
            "        if (current_time - last_login) > settings.MAX_SESSION_AGE:",
            "            logger.info(",
            "                f'Request is too far from last login {current_time - last_login:.0f} > {settings.MAX_SESSION_AGE}; logout'",
            "            )",
            "            logout(request)",
            "",
            "        # Push the expiry to the max every time a new request is made to a url that indicates user activity",
            "        # but only if it's not a URL we want to ignore",
            "        for path in self.NOT_USER_ACTIVITY_PATHS:",
            "            if isinstance(path, str) and path == str(request.path_info):",
            "                return",
            "            elif 'query' in path:",
            "                parts = str(request.path_info).split('?')",
            "                if len(parts) == 2 and path['query'] in parts[1]:",
            "                    return",
            "",
            "        request.session.set_expiry(",
            "            settings.MAX_TIME_BETWEEN_ACTIVITY if request.session.get('keep_me_logged_in', True) else 0",
            "        )",
            "",
            "",
            "class HumanSignalCspMiddleware(CSPMiddleware):",
            "    \"\"\"",
            "    Extend CSPMiddleware to support switching report-only CSP to regular CSP.",
            "",
            "    For use with core.decorators.override_report_only_csp.",
            "    \"\"\"",
            "",
            "    def process_response(self, request, response):",
            "        response = super().process_response(request, response)",
            "        if getattr(response, '_override_report_only_csp', False):",
            "            if csp_policy := response.get('Content-Security-Policy-Report-Only'):",
            "                response['Content-Security-Policy'] = csp_policy",
            "                del response['Content-Security-Policy-Report-Only']",
            "            delattr(response, '_override_report_only_csp')",
            "        return response"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "airflow.configuration"
        ]
    },
    "label_studio/core/settings/base.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 674,
                "afterPatchRowNumber": 674,
                "PatchRowcode": " DATA_MANAGER_FILTER_ALLOWLIST = list("
            },
            "1": {
                "beforePatchRowNumber": 675,
                "afterPatchRowNumber": 675,
                "PatchRowcode": "     set(get_env_list('DATA_MANAGER_FILTER_ALLOWLIST') + ['updated_by__active_organization'])"
            },
            "2": {
                "beforePatchRowNumber": 676,
                "afterPatchRowNumber": 676,
                "PatchRowcode": " )"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 677,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 678,
                "PatchRowcode": "+if ENABLE_CSP := get_bool_env('ENABLE_CSP', True):"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 679,
                "PatchRowcode": "+    CSP_DEFAULT_SRC = ("
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 680,
                "PatchRowcode": "+        \"'self'\","
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 681,
                "PatchRowcode": "+        \"'report-sample'\","
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 682,
                "PatchRowcode": "+    )"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 683,
                "PatchRowcode": "+    CSP_STYLE_SRC = (\"'self'\", \"'report-sample'\", \"'unsafe-inline'\")"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 684,
                "PatchRowcode": "+    CSP_SCRIPT_SRC = ("
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 685,
                "PatchRowcode": "+        \"'self'\","
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 686,
                "PatchRowcode": "+        \"'report-sample'\","
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 687,
                "PatchRowcode": "+        \"'unsafe-inline'\","
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 688,
                "PatchRowcode": "+        \"'unsafe-eval'\","
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 689,
                "PatchRowcode": "+        'blob:',"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 690,
                "PatchRowcode": "+        'browser.sentry-cdn.com',"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 691,
                "PatchRowcode": "+        'https://*.googletagmanager.com',"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 692,
                "PatchRowcode": "+    )"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 693,
                "PatchRowcode": "+    CSP_IMG_SRC = ("
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 694,
                "PatchRowcode": "+        \"'self'\","
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 695,
                "PatchRowcode": "+        \"'report-sample'\","
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 696,
                "PatchRowcode": "+        'data:',"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 697,
                "PatchRowcode": "+        'https://*.google-analytics.com',"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 698,
                "PatchRowcode": "+        'https://*.googletagmanager.com',"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 699,
                "PatchRowcode": "+        'https://*.google.com',"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 700,
                "PatchRowcode": "+    )"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 701,
                "PatchRowcode": "+    CSP_CONNECT_SRC = ("
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 702,
                "PatchRowcode": "+        \"'self'\","
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 703,
                "PatchRowcode": "+        \"'report-sample'\","
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 704,
                "PatchRowcode": "+        'https://*.google-analytics.com',"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 705,
                "PatchRowcode": "+        'https://*.analytics.google.com',"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 706,
                "PatchRowcode": "+        'https://analytics.google.com',"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 707,
                "PatchRowcode": "+        'https://*.googletagmanager.com',"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 708,
                "PatchRowcode": "+        'https://*.g.double' + 'click.net',  # hacky way of suppressing codespell complaint"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 709,
                "PatchRowcode": "+        'https://*.ingest.sentry.io',"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 710,
                "PatchRowcode": "+    )"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 711,
                "PatchRowcode": "+    # Note that this will be overridden to real CSP for views that use the override_report_only_csp decorator"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 712,
                "PatchRowcode": "+    CSP_REPORT_ONLY = get_bool_env('LS_CSP_REPORT_ONLY', True)"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 713,
                "PatchRowcode": "+    CSP_REPORT_URI = get_env('LS_CSP_REPORT_URI', None)"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 714,
                "PatchRowcode": "+    CSP_INCLUDE_NONCE_IN = ['script-src', 'default-src']"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 715,
                "PatchRowcode": "+"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 716,
                "PatchRowcode": "+    MIDDLEWARE.append('core.middleware.HumanSignalCspMiddleware')"
            }
        },
        "frontPatchFile": [
            "\"\"\"This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.",
            "\"\"\"",
            "\"\"\"",
            "Django Base settings for Label Studio.",
            "",
            "For more information on this file, see",
            "https://docs.djangoproject.com/en/3.1/topics/settings/",
            "",
            "For the full list of settings and their values, see",
            "https://docs.djangoproject.com/en/3.1/ref/settings/",
            "\"\"\"",
            "import json",
            "import logging",
            "import os",
            "import re",
            "from datetime import timedelta",
            "",
            "from label_studio.core.utils.params import get_bool_env, get_env_list",
            "",
            "formatter = 'standard'",
            "JSON_LOG = get_bool_env('JSON_LOG', False)",
            "if JSON_LOG:",
            "    formatter = 'json'",
            "",
            "LOGGING = {",
            "    'version': 1,",
            "    'disable_existing_loggers': False,",
            "    'formatters': {",
            "        'json': {",
            "            '()': 'label_studio.core.utils.formatter.CustomJsonFormatter',",
            "            'format': '[%(asctime)s] [%(name)s::%(funcName)s::%(lineno)d] [%(levelname)s] [%(user_id)s] %(message)s',",
            "            'datefmt': '%d/%b/%Y:%H:%M:%S %z',",
            "        },",
            "        'standard': {",
            "            'format': '[%(asctime)s] [%(name)s::%(funcName)s::%(lineno)d] [%(levelname)s] %(message)s',",
            "        },",
            "    },",
            "    'handlers': {",
            "        'console': {",
            "            'class': 'logging.StreamHandler',",
            "            'formatter': formatter,",
            "        },",
            "    },",
            "    'root': {",
            "        'handlers': ['console'],",
            "        'level': os.environ.get('LOG_LEVEL', 'DEBUG'),",
            "    },",
            "    'loggers': {",
            "        'pykwalify': {'level': 'ERROR', 'propagate': False},",
            "        'tavern': {'level': 'ERROR', 'propagate': False},",
            "        'asyncio': {'level': 'WARNING'},",
            "        'rules': {'level': 'WARNING'},",
            "        'django': {",
            "            'handlers': ['console'],",
            "            # 'propagate': True,",
            "        },",
            "        'django_auth_ldap': {'level': os.environ.get('LOG_LEVEL', 'DEBUG')},",
            "        'rq.worker': {",
            "            'handlers': ['console'],",
            "            'level': os.environ.get('LOG_LEVEL', 'INFO'),",
            "        },",
            "        'ddtrace': {",
            "            'handlers': ['console'],",
            "            'level': 'WARNING',",
            "        },",
            "        'ldclient.util': {",
            "            'handlers': ['console'],",
            "            'level': 'ERROR',",
            "        },",
            "    },",
            "}",
            "",
            "# for printing messages before main logging config applied",
            "if not logging.getLogger().hasHandlers():",
            "    logging.basicConfig(level=logging.DEBUG, format='%(message)s')",
            "",
            "from label_studio.core.utils.io import get_data_dir",
            "from label_studio.core.utils.params import get_bool_env, get_env",
            "",
            "logger = logging.getLogger(__name__)",
            "SILENCED_SYSTEM_CHECKS = []",
            "",
            "# Hostname is used for proper path generation to the resources, pages, etc",
            "HOSTNAME = get_env('HOST', '')",
            "if HOSTNAME:",
            "    if not HOSTNAME.startswith('http://') and not HOSTNAME.startswith('https://'):",
            "        logger.info(",
            "            '! HOST variable found in environment, but it must start with http:// or https://, ignore it: %s', HOSTNAME",
            "        )",
            "        HOSTNAME = ''",
            "    else:",
            "        logger.info('=> Hostname correctly is set to: %s', HOSTNAME)",
            "        if HOSTNAME.endswith('/'):",
            "            HOSTNAME = HOSTNAME[0:-1]",
            "",
            "        # for django url resolver",
            "        if HOSTNAME:",
            "            # http[s]://domain.com:8080/script_name => /script_name",
            "            pattern = re.compile(r'^http[s]?:\\/\\/([^:\\/\\s]+(:\\d*)?)(.*)?')",
            "            match = pattern.match(HOSTNAME)",
            "            FORCE_SCRIPT_NAME = match.group(3)",
            "            if FORCE_SCRIPT_NAME:",
            "                logger.info('=> Django URL prefix is set to: %s', FORCE_SCRIPT_NAME)",
            "",
            "INTERNAL_PORT = '8080'",
            "",
            "# SECURITY WARNING: don't run with debug turned on in production!",
            "DEBUG = get_bool_env('DEBUG', True)",
            "DEBUG_MODAL_EXCEPTIONS = get_bool_env('DEBUG_MODAL_EXCEPTIONS', True)",
            "",
            "# Whether to verify SSL certs when making external requests, eg in the uploader",
            "# \u26a0\ufe0f Turning this off means assuming risk. \u26a0\ufe0f",
            "# Overridable at organization level via Organization#verify_ssl_certs",
            "VERIFY_SSL_CERTS = get_bool_env('VERIFY_SSL_CERTS', True)",
            "",
            "# 'sqlite-dll-<arch>-<version>.zip' should be hosted at this prefix",
            "WINDOWS_SQLITE_BINARY_HOST_PREFIX = get_env('WINDOWS_SQLITE_BINARY_HOST_PREFIX', 'https://www.sqlite.org/2023/')",
            "",
            "# Build paths inside the project like this: os.path.join(BASE_DIR, ...)",
            "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))",
            "",
            "# Base path for media root and other uploaded files",
            "BASE_DATA_DIR = get_env('BASE_DATA_DIR', get_data_dir())",
            "os.makedirs(BASE_DATA_DIR, exist_ok=True)",
            "logger.info('=> Database and media directory: %s', BASE_DATA_DIR)",
            "",
            "# Databases",
            "# https://docs.djangoproject.com/en/2.1/ref/settings/#databases",
            "DJANGO_DB_MYSQL = 'mysql'",
            "DJANGO_DB_SQLITE = 'sqlite'",
            "DJANGO_DB_POSTGRESQL = 'postgresql'",
            "DJANGO_DB = 'default'",
            "DATABASE_NAME_DEFAULT = os.path.join(BASE_DATA_DIR, 'label_studio.sqlite3')",
            "DATABASE_NAME = get_env('DATABASE_NAME', DATABASE_NAME_DEFAULT)",
            "DATABASES_ALL = {",
            "    DJANGO_DB_POSTGRESQL: {",
            "        'ENGINE': 'django.db.backends.postgresql',",
            "        'USER': get_env('POSTGRE_USER', 'postgres'),",
            "        'PASSWORD': get_env('POSTGRE_PASSWORD', 'postgres'),",
            "        'NAME': get_env('POSTGRE_NAME', 'postgres'),",
            "        'HOST': get_env('POSTGRE_HOST', 'localhost'),",
            "        'PORT': int(get_env('POSTGRE_PORT', '5432')),",
            "    },",
            "    DJANGO_DB_MYSQL: {",
            "        'ENGINE': 'django.db.backends.mysql',",
            "        'USER': get_env('MYSQL_USER', 'root'),",
            "        'PASSWORD': get_env('MYSQL_PASSWORD', ''),",
            "        'NAME': get_env('MYSQL_NAME', 'labelstudio'),",
            "        'HOST': get_env('MYSQL_HOST', 'localhost'),",
            "        'PORT': int(get_env('MYSQL_PORT', '3306')),",
            "    },",
            "    DJANGO_DB_SQLITE: {",
            "        'ENGINE': 'django.db.backends.sqlite3',",
            "        'NAME': DATABASE_NAME,",
            "        'OPTIONS': {",
            "            # 'timeout': 20,",
            "        },",
            "    },",
            "}",
            "DATABASES_ALL['default'] = DATABASES_ALL[DJANGO_DB_POSTGRESQL]",
            "DATABASES = {'default': DATABASES_ALL.get(get_env('DJANGO_DB', 'default'))}",
            "",
            "DEFAULT_AUTO_FIELD = 'django.db.models.AutoField'",
            "",
            "if get_bool_env('GOOGLE_LOGGING_ENABLED', False):",
            "    logging.info('Google Cloud Logging handler is enabled.')",
            "    try:",
            "        import google.cloud.logging",
            "        from google.auth.exceptions import GoogleAuthError",
            "",
            "        client = google.cloud.logging.Client()",
            "        client.setup_logging()",
            "",
            "        LOGGING['handlers']['google_cloud_logging'] = {",
            "            'level': get_env('LOG_LEVEL', 'WARNING'),",
            "            'class': 'google.cloud.logging.handlers.CloudLoggingHandler',",
            "            'client': client,",
            "        }",
            "        LOGGING['root']['handlers'].append('google_cloud_logging')",
            "    except GoogleAuthError:",
            "        logger.exception('Google Cloud Logging handler could not be setup.')",
            "",
            "INSTALLED_APPS = [",
            "    'django.contrib.admin',",
            "    'django.contrib.auth',",
            "    'django.contrib.contenttypes',",
            "    'django.contrib.sessions',",
            "    'django.contrib.messages',",
            "    'django.contrib.staticfiles',",
            "    'django.contrib.humanize',",
            "    'drf_yasg',",
            "    'corsheaders',",
            "    'django_extensions',",
            "    'django_rq',",
            "    'django_filters',",
            "    'rules',",
            "    'annoying',",
            "    'rest_framework',",
            "    'rest_framework.authtoken',",
            "    'drf_generators',",
            "    'core',",
            "    'users',",
            "    'organizations',",
            "    'data_import',",
            "    'data_export',",
            "    'projects',",
            "    'tasks',",
            "    'data_manager',",
            "    'io_storages',",
            "    'ml',",
            "    'webhooks',",
            "    'labels_manager',",
            "]",
            "",
            "MIDDLEWARE = [",
            "    'corsheaders.middleware.CorsMiddleware',",
            "    'django.middleware.security.SecurityMiddleware',",
            "    'django.contrib.sessions.middleware.SessionMiddleware',",
            "    'django.middleware.locale.LocaleMiddleware',",
            "    'core.middleware.DisableCSRF',",
            "    'django.middleware.csrf.CsrfViewMiddleware',",
            "    'django.contrib.auth.middleware.AuthenticationMiddleware',",
            "    'django.contrib.messages.middleware.MessageMiddleware',",
            "    'core.middleware.CommonMiddlewareAppendSlashWithoutRedirect',  # instead of 'CommonMiddleware'",
            "    'core.middleware.CommonMiddleware',",
            "    'django_user_agents.middleware.UserAgentMiddleware',",
            "    'core.middleware.SetSessionUIDMiddleware',",
            "    'core.middleware.ContextLogMiddleware',",
            "    'core.middleware.DatabaseIsLockedRetryMiddleware',",
            "    'core.current_request.ThreadLocalMiddleware',",
            "]",
            "",
            "REST_FRAMEWORK = {",
            "    'DEFAULT_FILTER_BACKENDS': ['django_filters.rest_framework.DjangoFilterBackend'],",
            "    'DEFAULT_AUTHENTICATION_CLASSES': (",
            "        'rest_framework.authentication.TokenAuthentication',",
            "        'rest_framework.authentication.SessionAuthentication',",
            "    ),",
            "    'DEFAULT_PERMISSION_CLASSES': [",
            "        'core.api_permissions.HasObjectPermission',",
            "        'rest_framework.permissions.IsAuthenticated',",
            "    ],",
            "    'EXCEPTION_HANDLER': 'core.utils.common.custom_exception_handler',",
            "    'DEFAULT_RENDERER_CLASSES': ('rest_framework.renderers.JSONRenderer',),",
            "    'DEFAULT_VERSIONING_CLASS': 'rest_framework.versioning.NamespaceVersioning',",
            "    'PAGE_SIZE': 100,",
            "    # 'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.PageNumberPagination'",
            "}",
            "SILENCED_SYSTEM_CHECKS += ['rest_framework.W001']",
            "",
            "# CORS & Host settings",
            "INTERNAL_IPS = [  # django debug toolbar for django==2.2 requirement",
            "    '127.0.0.1',",
            "    'localhost',",
            "]",
            "CORS_ORIGIN_ALLOW_ALL = True",
            "CORS_ALLOW_METHODS = [",
            "    'DELETE',",
            "    'GET',",
            "    'OPTIONS',",
            "    'PATCH',",
            "    'POST',",
            "    'PUT',",
            "]",
            "ALLOWED_HOSTS = ['*']",
            "",
            "# Auth modules",
            "AUTH_USER_MODEL = 'users.User'",
            "AUTHENTICATION_BACKENDS = [",
            "    'rules.permissions.ObjectPermissionBackend',",
            "    'django.contrib.auth.backends.ModelBackend',",
            "]",
            "USE_USERNAME_FOR_LOGIN = False",
            "",
            "DISABLE_SIGNUP_WITHOUT_LINK = get_bool_env('DISABLE_SIGNUP_WITHOUT_LINK', False)",
            "",
            "# Password validation:",
            "# https://docs.djangoproject.com/en/2.1/ref/settings/#auth-password-validators",
            "AUTH_PASSWORD_VALIDATORS = [",
            "    {'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator'},",
            "    {'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator'},",
            "    {'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator'},",
            "    {'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator'},",
            "]",
            "",
            "# Django templates",
            "TEMPLATES_DIR = os.path.join(os.path.dirname(BASE_DIR), 'templates')  # ../../from_this = 'web' dir",
            "TEMPLATES = [",
            "    {",
            "        'BACKEND': 'django.template.backends.django.DjangoTemplates',",
            "        'DIRS': [TEMPLATES_DIR],",
            "        'APP_DIRS': True,",
            "        'OPTIONS': {",
            "            'context_processors': [",
            "                'django.template.context_processors.debug',",
            "                'django.template.context_processors.request',",
            "                'django.contrib.auth.context_processors.auth',",
            "                'django.contrib.messages.context_processors.messages',",
            "                'core.context_processors.settings',",
            "            ],",
            "            'builtins': ['django.templatetags.i18n'],",
            "        },",
            "    }",
            "]",
            "",
            "# RQ",
            "RQ_QUEUES = {",
            "    'critical': {",
            "        'HOST': 'localhost',",
            "        'PORT': 6379,",
            "        'DB': 0,",
            "        'DEFAULT_TIMEOUT': 180,",
            "    },",
            "    'high': {",
            "        'HOST': 'localhost',",
            "        'PORT': 6379,",
            "        'DB': 0,",
            "        'DEFAULT_TIMEOUT': 180,",
            "    },",
            "    'default': {",
            "        'HOST': 'localhost',",
            "        'PORT': 6379,",
            "        'DB': 0,",
            "        'DEFAULT_TIMEOUT': 180,",
            "    },",
            "    'low': {",
            "        'HOST': 'localhost',",
            "        'PORT': 6379,",
            "        'DB': 0,",
            "        'DEFAULT_TIMEOUT': 180,",
            "    },",
            "}",
            "",
            "# Swagger: automatic API documentation",
            "SWAGGER_SETTINGS = {",
            "    'SECURITY_DEFINITIONS': {",
            "        'Token': {",
            "            'type': 'apiKey',",
            "            'name': 'Authorization',",
            "            'in': 'header',",
            "            'description': 'The token (or API key) must be passed as a request header. '",
            "            'You can find your user token on the User Account page in Label Studio. Example: '",
            "            '<br><pre><code class=\"language-bash\">'",
            "            'curl https://label-studio-host/api/projects -H \"Authorization: Token [your-token]\"'",
            "            '</code></pre>',",
            "        }",
            "    },",
            "    'APIS_SORTER': 'alpha',",
            "    'SUPPORTED_SUBMIT_METHODS': ['get', 'post', 'put', 'delete', 'patch'],",
            "    'OPERATIONS_SORTER': 'alpha',",
            "}",
            "",
            "SENTRY_DSN = get_env('SENTRY_DSN', None)",
            "SENTRY_RATE = float(get_env('SENTRY_RATE', 0.25))",
            "SENTRY_ENVIRONMENT = get_env('SENTRY_ENVIRONMENT', 'stage.opensource')",
            "SENTRY_REDIS_ENABLED = False",
            "FRONTEND_SENTRY_DSN = get_env('FRONTEND_SENTRY_DSN', None)",
            "FRONTEND_SENTRY_RATE = get_env('FRONTEND_SENTRY_RATE', 0.1)",
            "FRONTEND_SENTRY_ENVIRONMENT = get_env('FRONTEND_SENTRY_ENVIRONMENT', 'stage.opensource')",
            "",
            "ROOT_URLCONF = 'core.urls'",
            "WSGI_APPLICATION = 'core.wsgi.application'",
            "GRAPHIQL = True",
            "",
            "# Internationalization",
            "# https://docs.djangoproject.com/en/2.1/topics/i18n/",
            "LANGUAGE_CODE = 'en-us'",
            "TIME_ZONE = 'UTC'",
            "USE_I18N = False",
            "USE_L10N = True",
            "USE_TZ = True",
            "",
            "# Static files (CSS, JavaScript, Images)",
            "# https://docs.djangoproject.com/en/2.1/howto/static-files/",
            "STATIC_URL = '/static/'",
            "# if FORCE_SCRIPT_NAME:",
            "#    STATIC_URL = FORCE_SCRIPT_NAME + STATIC_URL",
            "logger.info(f'=> Static URL is set to: {STATIC_URL}')",
            "",
            "STATIC_ROOT = os.path.join(BASE_DIR, 'static_build')",
            "STATICFILES_DIRS = [os.path.join(BASE_DIR, 'static')]",
            "STATICFILES_FINDERS = (",
            "    'django.contrib.staticfiles.finders.FileSystemFinder',",
            "    'django.contrib.staticfiles.finders.AppDirectoriesFinder',",
            ")",
            "STATICFILES_STORAGE = 'core.storage.SkipMissedManifestStaticFilesStorage'",
            "",
            "# Sessions and CSRF",
            "SESSION_COOKIE_SECURE = bool(int(get_env('SESSION_COOKIE_SECURE', False)))",
            "SESSION_COOKIE_SAMESITE = get_env('SESSION_COOKIE_SAMESITE', 'Lax')",
            "",
            "CSRF_COOKIE_SECURE = bool(int(get_env('CSRF_COOKIE_SECURE', SESSION_COOKIE_SECURE)))",
            "CSRF_COOKIE_HTTPONLY = bool(int(get_env('CSRF_COOKIE_HTTPONLY', SESSION_COOKIE_SECURE)))",
            "CSRF_COOKIE_SAMESITE = get_env('CSRF_COOKIE_SAMESITE', 'Lax')",
            "",
            "# Inactivity user sessions",
            "INACTIVITY_SESSION_TIMEOUT_ENABLED = bool(int(get_env('INACTIVITY_SESSION_TIMEOUT_ENABLED', True)))",
            "# The most time a login will last, regardless of activity",
            "MAX_SESSION_AGE = int(get_env('MAX_SESSION_AGE', timedelta(days=14).total_seconds()))",
            "# The most time that can elapse between activity with the server before the user is logged out",
            "MAX_TIME_BETWEEN_ACTIVITY = int(get_env('MAX_TIME_BETWEEN_ACTIVITY', timedelta(days=5).total_seconds()))",
            "",
            "SSRF_PROTECTION_ENABLED = get_bool_env('SSRF_PROTECTION_ENABLED', False)",
            "",
            "# user media files",
            "MEDIA_ROOT = os.path.join(BASE_DATA_DIR, 'media')",
            "os.makedirs(MEDIA_ROOT, exist_ok=True)",
            "MEDIA_URL = '/data/'",
            "UPLOAD_DIR = 'upload'",
            "AVATAR_PATH = 'avatars'",
            "",
            "SUPPORTED_EXTENSIONS = set(",
            "    [",
            "        '.bmp',",
            "        '.csv',",
            "        '.flac',",
            "        '.gif',",
            "        '.htm',",
            "        '.html',",
            "        '.jpg',",
            "        '.jpeg',",
            "        '.json',",
            "        '.m4a',",
            "        '.mp3',",
            "        '.ogg',",
            "        '.png',",
            "        '.svg',",
            "        '.tsv',",
            "        '.txt',",
            "        '.wav',",
            "        '.xml',",
            "        '.mp4',",
            "        '.webm',",
            "        '.webp',",
            "    ]",
            ")",
            "",
            "# directory for files created during unit tests",
            "TEST_DATA_ROOT = os.path.join(BASE_DATA_DIR, 'test_data')",
            "os.makedirs(TEST_DATA_ROOT, exist_ok=True)",
            "",
            "# project exports",
            "EXPORT_DIR = os.path.join(BASE_DATA_DIR, 'export')",
            "EXPORT_URL_ROOT = '/export/'",
            "EXPORT_MIXIN = 'data_export.mixins.ExportMixin'",
            "# old export dir",
            "os.makedirs(EXPORT_DIR, exist_ok=True)",
            "# dir for delayed export",
            "DELAYED_EXPORT_DIR = 'export'",
            "os.makedirs(os.path.join(BASE_DATA_DIR, MEDIA_ROOT, DELAYED_EXPORT_DIR), exist_ok=True)",
            "",
            "# file / task size limits",
            "DATA_UPLOAD_MAX_MEMORY_SIZE = int(get_env('DATA_UPLOAD_MAX_MEMORY_SIZE', 250 * 1024 * 1024))",
            "DATA_UPLOAD_MAX_NUMBER_FILES = int(get_env('DATA_UPLOAD_MAX_NUMBER_FILES', 100))",
            "TASKS_MAX_NUMBER = 1000000",
            "TASKS_MAX_FILE_SIZE = DATA_UPLOAD_MAX_MEMORY_SIZE",
            "",
            "TASK_LOCK_TTL = int(get_env('TASK_LOCK_TTL', default=86400))",
            "",
            "LABEL_STREAM_HISTORY_LIMIT = int(get_env('LABEL_STREAM_HISTORY_LIMIT', default=100))",
            "",
            "RANDOM_NEXT_TASK_SAMPLE_SIZE = int(get_env('RANDOM_NEXT_TASK_SAMPLE_SIZE', 50))",
            "",
            "TASK_API_PAGE_SIZE_MAX = int(get_env('TASK_API_PAGE_SIZE_MAX', 0)) or None",
            "",
            "# Email backend",
            "FROM_EMAIL = get_env('FROM_EMAIL', 'Label Studio <hello@labelstud.io>')",
            "EMAIL_BACKEND = get_env('EMAIL_BACKEND', 'django.core.mail.backends.dummy.EmailBackend')",
            "",
            "ENABLE_LOCAL_FILES_STORAGE = get_bool_env('ENABLE_LOCAL_FILES_STORAGE', default=True)",
            "LOCAL_FILES_SERVING_ENABLED = get_bool_env('LOCAL_FILES_SERVING_ENABLED', default=False)",
            "LOCAL_FILES_DOCUMENT_ROOT = get_env('LOCAL_FILES_DOCUMENT_ROOT', default=os.path.abspath(os.sep))",
            "",
            "SYNC_ON_TARGET_STORAGE_CREATION = get_bool_env('SYNC_ON_TARGET_STORAGE_CREATION', default=True)",
            "",
            "ALLOW_IMPORT_TASKS_WITH_UNKNOWN_EMAILS = get_bool_env('ALLOW_IMPORT_TASKS_WITH_UNKNOWN_EMAILS', default=False)",
            "",
            "\"\"\" React Libraries: do not forget to change this dir in /etc/nginx/nginx.conf \"\"\"",
            "# EDITOR = label-studio-frontend repository",
            "EDITOR_ROOT = os.path.join(BASE_DIR, '../frontend/dist/lsf')",
            "# DM = data manager (included into FRONTEND due npm building, we need only version.json file from there)",
            "DM_ROOT = os.path.join(BASE_DIR, '../frontend/dist/dm')",
            "# FRONTEND = GUI for django backend",
            "REACT_APP_ROOT = os.path.join(BASE_DIR, '../frontend/dist/react-app')",
            "",
            "# per project settings",
            "BATCH_SIZE = 1000",
            "PROJECT_TITLE_MIN_LEN = 3",
            "PROJECT_TITLE_MAX_LEN = 50",
            "LOGIN_REDIRECT_URL = '/'",
            "LOGIN_URL = '/'",
            "MIN_GROUND_TRUTH = 10",
            "DATA_UNDEFINED_NAME = '$undefined$'",
            "LICENSE = {}",
            "VERSIONS = {}",
            "VERSION_EDITION = 'Community'",
            "LATEST_VERSION_CHECK = True",
            "VERSIONS_CHECK_TIME = 0",
            "ALLOW_ORGANIZATION_WEBHOOKS = get_bool_env('ALLOW_ORGANIZATION_WEBHOOKS', False)",
            "CONVERTER_DOWNLOAD_RESOURCES = get_bool_env('CONVERTER_DOWNLOAD_RESOURCES', True)",
            "EXPERIMENTAL_FEATURES = get_bool_env('EXPERIMENTAL_FEATURES', False)",
            "USE_ENFORCE_CSRF_CHECKS = get_bool_env('USE_ENFORCE_CSRF_CHECKS', True)  # False is for tests",
            "CLOUD_FILE_STORAGE_ENABLED = False",
            "",
            "IO_STORAGES_IMPORT_LINK_NAMES = [",
            "    'io_storages_s3importstoragelink',",
            "    'io_storages_gcsimportstoragelink',",
            "    'io_storages_azureblobimportstoragelink',",
            "    'io_storages_localfilesimportstoragelink',",
            "    'io_storages_redisimportstoragelink',",
            "]",
            "",
            "CREATE_ORGANIZATION = 'organizations.functions.create_organization'",
            "SAVE_USER = 'users.functions.save_user'",
            "POST_PROCESS_REIMPORT = 'core.utils.common.empty'",
            "USER_SERIALIZER = 'users.serializers.BaseUserSerializer'",
            "USER_SERIALIZER_UPDATE = 'users.serializers.BaseUserSerializerUpdate'",
            "TASK_SERIALIZER = 'tasks.serializers.BaseTaskSerializer'",
            "EXPORT_DATA_SERIALIZER = 'data_export.serializers.BaseExportDataSerializer'",
            "DATA_MANAGER_GET_ALL_COLUMNS = 'data_manager.functions.get_all_columns'",
            "DATA_MANAGER_ANNOTATIONS_MAP = {}",
            "DATA_MANAGER_ACTIONS = {}",
            "DATA_MANAGER_CUSTOM_FILTER_EXPRESSIONS = 'data_manager.functions.custom_filter_expressions'",
            "DATA_MANAGER_PREPROCESS_FILTER = 'data_manager.functions.preprocess_filter'",
            "USER_LOGIN_FORM = 'users.forms.LoginForm'",
            "PROJECT_MIXIN = 'projects.mixins.ProjectMixin'",
            "TASK_MIXIN = 'tasks.mixins.TaskMixin'",
            "ANNOTATION_MIXIN = 'tasks.mixins.AnnotationMixin'",
            "ORGANIZATION_MIXIN = 'organizations.mixins.OrganizationMixin'",
            "USER_MIXIN = 'users.mixins.UserMixin'",
            "ORGANIZATION_MEMBER_MIXIN = 'organizations.mixins.OrganizationMemberMixin'",
            "MEMBER_PERM = 'core.api_permissions.MemberHasOwnerPermission'",
            "RECALCULATE_ALL_STATS = None",
            "GET_STORAGE_LIST = 'io_storages.functions.get_storage_list'",
            "STORAGE_ANNOTATION_SERIALIZER = 'io_storages.serializers.StorageAnnotationSerializer'",
            "TASK_SERIALIZER_BULK = 'tasks.serializers.BaseTaskSerializerBulk'",
            "PREPROCESS_FIELD_NAME = 'data_manager.functions.preprocess_field_name'",
            "INTERACTIVE_DATA_SERIALIZER = 'data_export.serializers.BaseExportDataSerializerForInteractive'",
            "DELETE_TASKS_ANNOTATIONS_POSTPROCESS = None",
            "",
            "",
            "def project_delete(project):",
            "    project.delete()",
            "",
            "",
            "def user_auth(user_model, email, password):",
            "    return None",
            "",
            "",
            "def collect_versions_dummy(**kwargs):",
            "    return {}",
            "",
            "",
            "PROJECT_DELETE = project_delete",
            "USER_AUTH = user_auth",
            "COLLECT_VERSIONS = collect_versions_dummy",
            "",
            "WEBHOOK_TIMEOUT = float(get_env('WEBHOOK_TIMEOUT', 1.0))",
            "WEBHOOK_BATCH_SIZE = int(get_env('WEBHOOK_BATCH_SIZE', 100))",
            "WEBHOOK_SERIALIZERS = {",
            "    'project': 'webhooks.serializers_for_hooks.ProjectWebhookSerializer',",
            "    'task': 'webhooks.serializers_for_hooks.TaskWebhookSerializer',",
            "    'annotation': 'webhooks.serializers_for_hooks.AnnotationWebhookSerializer',",
            "    'label': 'labels_manager.serializers.LabelSerializer',",
            "    'label_link': 'labels_manager.serializers.LabelLinkSerializer',",
            "}",
            "",
            "EDITOR_KEYMAP = json.dumps(get_env('EDITOR_KEYMAP'))",
            "",
            "# fix a problem with Windows mimetypes for JS and PNG",
            "import mimetypes",
            "",
            "mimetypes.add_type('application/javascript', '.js', True)",
            "mimetypes.add_type('image/png', '.png', True)",
            "",
            "# fields name was used in DM api before",
            "REST_FLEX_FIELDS = {'FIELDS_PARAM': 'include'}",
            "",
            "INTERPOLATE_KEY_FRAMES = get_env('INTERPOLATE_KEY_FRAMES', False)",
            "",
            "# Feature Flags",
            "FEATURE_FLAGS_API_KEY = get_env('FEATURE_FLAGS_API_KEY', default='any key')",
            "",
            "# we may set feature flags from file",
            "FEATURE_FLAGS_FROM_FILE = get_bool_env('FEATURE_FLAGS_FROM_FILE', False)",
            "FEATURE_FLAGS_FILE = get_env('FEATURE_FLAGS_FILE', 'feature_flags.json')",
            "# or if file is not set, default is using offline mode",
            "FEATURE_FLAGS_OFFLINE = get_bool_env('FEATURE_FLAGS_OFFLINE', True)",
            "# default value for feature flags (if not overridden by environment or client)",
            "FEATURE_FLAGS_DEFAULT_VALUE = False",
            "",
            "# Whether to send analytics telemetry data",
            "COLLECT_ANALYTICS = get_bool_env('collect_analytics', True)",
            "",
            "# Strip harmful content from SVG files by default",
            "SVG_SECURITY_CLEANUP = get_bool_env('SVG_SECURITY_CLEANUP', False)",
            "",
            "ML_BLOCK_LOCAL_IP = get_bool_env('ML_BLOCK_LOCAL_IP', False)",
            "",
            "RQ_LONG_JOB_TIMEOUT = int(get_env('RQ_LONG_JOB_TIMEOUT', 36000))",
            "",
            "APP_WEBSERVER = get_env('APP_WEBSERVER', 'django')",
            "",
            "BATCH_JOB_RETRY_TIMEOUT = int(get_env('BATCH_JOB_RETRY_TIMEOUT', 60))",
            "",
            "FUTURE_SAVE_TASK_TO_STORAGE = get_bool_env('FUTURE_SAVE_TASK_TO_STORAGE', default=False)",
            "FUTURE_SAVE_TASK_TO_STORAGE_JSON_EXT = get_bool_env('FUTURE_SAVE_TASK_TO_STORAGE_JSON_EXT', default=True)",
            "STORAGE_IN_PROGRESS_TIMER = float(get_env('STORAGE_IN_PROGRESS_TIMER', 5.0))",
            "",
            "USE_NGINX_FOR_EXPORT_DOWNLOADS = get_bool_env('USE_NGINX_FOR_EXPORT_DOWNLOADS', False)",
            "",
            "if get_env('MINIO_STORAGE_ENDPOINT') and not get_bool_env('MINIO_SKIP', False):",
            "    CLOUD_FILE_STORAGE_ENABLED = True",
            "    DEFAULT_FILE_STORAGE = 'storages.backends.s3boto3.S3Boto3Storage'",
            "    AWS_STORAGE_BUCKET_NAME = get_env('MINIO_STORAGE_BUCKET_NAME')",
            "    AWS_ACCESS_KEY_ID = get_env('MINIO_STORAGE_ACCESS_KEY')",
            "    AWS_SECRET_ACCESS_KEY = get_env('MINIO_STORAGE_SECRET_KEY')",
            "    AWS_S3_ENDPOINT_URL = get_env('MINIO_STORAGE_ENDPOINT')",
            "    AWS_QUERYSTRING_AUTH = False",
            "    # make domain for FileUpload.file",
            "    AWS_S3_SECURE_URLS = False",
            "    AWS_S3_URL_PROTOCOL = 'http:' if HOSTNAME.startswith('http://') else 'https:'",
            "    AWS_S3_CUSTOM_DOMAIN = HOSTNAME.replace('http://', '').replace('https://', '') + '/data'",
            "",
            "if get_env('STORAGE_TYPE') == 's3':",
            "    CLOUD_FILE_STORAGE_ENABLED = True",
            "    DEFAULT_FILE_STORAGE = 'core.storage.CustomS3Boto3Storage'",
            "    if get_env('STORAGE_AWS_ACCESS_KEY_ID'):",
            "        AWS_ACCESS_KEY_ID = get_env('STORAGE_AWS_ACCESS_KEY_ID')",
            "    if get_env('STORAGE_AWS_SECRET_ACCESS_KEY'):",
            "        AWS_SECRET_ACCESS_KEY = get_env('STORAGE_AWS_SECRET_ACCESS_KEY')",
            "    AWS_STORAGE_BUCKET_NAME = get_env('STORAGE_AWS_BUCKET_NAME')",
            "    AWS_S3_REGION_NAME = get_env('STORAGE_AWS_REGION_NAME', None)",
            "    AWS_S3_ENDPOINT_URL = get_env('STORAGE_AWS_ENDPOINT_URL', None)",
            "    if get_env('STORAGE_AWS_OBJECT_PARAMETERS'):",
            "        AWS_S3_OBJECT_PARAMETERS = json.loads(get_env('STORAGE_AWS_OBJECT_PARAMETERS'))",
            "    AWS_QUERYSTRING_EXPIRE = int(get_env('STORAGE_AWS_X_AMZ_EXPIRES', '86400'))",
            "    AWS_LOCATION = get_env('STORAGE_AWS_FOLDER', default='')",
            "    AWS_S3_USE_SSL = get_bool_env('STORAGE_AWS_S3_USE_SSL', True)",
            "    AWS_S3_VERIFY = get_env('STORAGE_AWS_S3_VERIFY', None)",
            "    if AWS_S3_VERIFY == 'false' or AWS_S3_VERIFY == 'False' or AWS_S3_VERIFY == '0':",
            "        AWS_S3_VERIFY = False",
            "    AWS_S3_SIGNATURE_VERSION = get_env('STORAGE_AWS_S3_SIGNATURE_VERSION', None)",
            "",
            "if get_env('STORAGE_TYPE') == 'azure':",
            "    CLOUD_FILE_STORAGE_ENABLED = True",
            "    DEFAULT_FILE_STORAGE = 'core.storage.CustomAzureStorage'",
            "    AZURE_ACCOUNT_NAME = get_env('STORAGE_AZURE_ACCOUNT_NAME')",
            "    AZURE_ACCOUNT_KEY = get_env('STORAGE_AZURE_ACCOUNT_KEY')",
            "    AZURE_CONTAINER = get_env('STORAGE_AZURE_CONTAINER_NAME')",
            "    AZURE_URL_EXPIRATION_SECS = int(get_env('STORAGE_AZURE_URL_EXPIRATION_SECS', '86400'))",
            "    AZURE_LOCATION = get_env('STORAGE_AZURE_FOLDER', default='')",
            "",
            "if get_env('STORAGE_TYPE') == 'gcs':",
            "    CLOUD_FILE_STORAGE_ENABLED = True",
            "    # DEFAULT_FILE_STORAGE = 'storages.backends.gcloud.GoogleCloudStorage'",
            "    DEFAULT_FILE_STORAGE = 'core.storage.AlternativeGoogleCloudStorage'",
            "    GS_PROJECT_ID = get_env('STORAGE_GCS_PROJECT_ID')",
            "    GS_BUCKET_NAME = get_env('STORAGE_GCS_BUCKET_NAME')",
            "    GS_EXPIRATION = timedelta(seconds=int(get_env('STORAGE_GCS_EXPIRATION_SECS', '86400')))",
            "    GS_LOCATION = get_env('STORAGE_GCS_FOLDER', default='')",
            "    GS_CUSTOM_ENDPOINT = get_env('STORAGE_GCS_ENDPOINT')",
            "",
            "CSRF_TRUSTED_ORIGINS = get_env('CSRF_TRUSTED_ORIGINS', [])",
            "if CSRF_TRUSTED_ORIGINS:",
            "    CSRF_TRUSTED_ORIGINS = CSRF_TRUSTED_ORIGINS.split(',')",
            "",
            "REAL_HOSTNAME = os.getenv('HOSTNAME')  # we have to use getenv, because we don't use LABEL_STUDIO_ prefix",
            "GCS_CLOUD_STORAGE_FORCE_DEFAULT_CREDENTIALS = get_bool_env('GCS_CLOUD_STORAGE_FORCE_DEFAULT_CREDENTIALS', False)",
            "PUBLIC_API_DOCS = get_bool_env('PUBLIC_API_DOCS', False)",
            "",
            "# By default, we disallow filters with foreign keys in data manager for security reasons.",
            "# Add to this list (either here in code, or via the env) to allow specific filters that rely on foreign keys.",
            "DATA_MANAGER_FILTER_ALLOWLIST = list(",
            "    set(get_env_list('DATA_MANAGER_FILTER_ALLOWLIST') + ['updated_by__active_organization'])",
            ")"
        ],
        "afterPatchFile": [
            "\"\"\"This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.",
            "\"\"\"",
            "\"\"\"",
            "Django Base settings for Label Studio.",
            "",
            "For more information on this file, see",
            "https://docs.djangoproject.com/en/3.1/topics/settings/",
            "",
            "For the full list of settings and their values, see",
            "https://docs.djangoproject.com/en/3.1/ref/settings/",
            "\"\"\"",
            "import json",
            "import logging",
            "import os",
            "import re",
            "from datetime import timedelta",
            "",
            "from label_studio.core.utils.params import get_bool_env, get_env_list",
            "",
            "formatter = 'standard'",
            "JSON_LOG = get_bool_env('JSON_LOG', False)",
            "if JSON_LOG:",
            "    formatter = 'json'",
            "",
            "LOGGING = {",
            "    'version': 1,",
            "    'disable_existing_loggers': False,",
            "    'formatters': {",
            "        'json': {",
            "            '()': 'label_studio.core.utils.formatter.CustomJsonFormatter',",
            "            'format': '[%(asctime)s] [%(name)s::%(funcName)s::%(lineno)d] [%(levelname)s] [%(user_id)s] %(message)s',",
            "            'datefmt': '%d/%b/%Y:%H:%M:%S %z',",
            "        },",
            "        'standard': {",
            "            'format': '[%(asctime)s] [%(name)s::%(funcName)s::%(lineno)d] [%(levelname)s] %(message)s',",
            "        },",
            "    },",
            "    'handlers': {",
            "        'console': {",
            "            'class': 'logging.StreamHandler',",
            "            'formatter': formatter,",
            "        },",
            "    },",
            "    'root': {",
            "        'handlers': ['console'],",
            "        'level': os.environ.get('LOG_LEVEL', 'DEBUG'),",
            "    },",
            "    'loggers': {",
            "        'pykwalify': {'level': 'ERROR', 'propagate': False},",
            "        'tavern': {'level': 'ERROR', 'propagate': False},",
            "        'asyncio': {'level': 'WARNING'},",
            "        'rules': {'level': 'WARNING'},",
            "        'django': {",
            "            'handlers': ['console'],",
            "            # 'propagate': True,",
            "        },",
            "        'django_auth_ldap': {'level': os.environ.get('LOG_LEVEL', 'DEBUG')},",
            "        'rq.worker': {",
            "            'handlers': ['console'],",
            "            'level': os.environ.get('LOG_LEVEL', 'INFO'),",
            "        },",
            "        'ddtrace': {",
            "            'handlers': ['console'],",
            "            'level': 'WARNING',",
            "        },",
            "        'ldclient.util': {",
            "            'handlers': ['console'],",
            "            'level': 'ERROR',",
            "        },",
            "    },",
            "}",
            "",
            "# for printing messages before main logging config applied",
            "if not logging.getLogger().hasHandlers():",
            "    logging.basicConfig(level=logging.DEBUG, format='%(message)s')",
            "",
            "from label_studio.core.utils.io import get_data_dir",
            "from label_studio.core.utils.params import get_bool_env, get_env",
            "",
            "logger = logging.getLogger(__name__)",
            "SILENCED_SYSTEM_CHECKS = []",
            "",
            "# Hostname is used for proper path generation to the resources, pages, etc",
            "HOSTNAME = get_env('HOST', '')",
            "if HOSTNAME:",
            "    if not HOSTNAME.startswith('http://') and not HOSTNAME.startswith('https://'):",
            "        logger.info(",
            "            '! HOST variable found in environment, but it must start with http:// or https://, ignore it: %s', HOSTNAME",
            "        )",
            "        HOSTNAME = ''",
            "    else:",
            "        logger.info('=> Hostname correctly is set to: %s', HOSTNAME)",
            "        if HOSTNAME.endswith('/'):",
            "            HOSTNAME = HOSTNAME[0:-1]",
            "",
            "        # for django url resolver",
            "        if HOSTNAME:",
            "            # http[s]://domain.com:8080/script_name => /script_name",
            "            pattern = re.compile(r'^http[s]?:\\/\\/([^:\\/\\s]+(:\\d*)?)(.*)?')",
            "            match = pattern.match(HOSTNAME)",
            "            FORCE_SCRIPT_NAME = match.group(3)",
            "            if FORCE_SCRIPT_NAME:",
            "                logger.info('=> Django URL prefix is set to: %s', FORCE_SCRIPT_NAME)",
            "",
            "INTERNAL_PORT = '8080'",
            "",
            "# SECURITY WARNING: don't run with debug turned on in production!",
            "DEBUG = get_bool_env('DEBUG', True)",
            "DEBUG_MODAL_EXCEPTIONS = get_bool_env('DEBUG_MODAL_EXCEPTIONS', True)",
            "",
            "# Whether to verify SSL certs when making external requests, eg in the uploader",
            "# \u26a0\ufe0f Turning this off means assuming risk. \u26a0\ufe0f",
            "# Overridable at organization level via Organization#verify_ssl_certs",
            "VERIFY_SSL_CERTS = get_bool_env('VERIFY_SSL_CERTS', True)",
            "",
            "# 'sqlite-dll-<arch>-<version>.zip' should be hosted at this prefix",
            "WINDOWS_SQLITE_BINARY_HOST_PREFIX = get_env('WINDOWS_SQLITE_BINARY_HOST_PREFIX', 'https://www.sqlite.org/2023/')",
            "",
            "# Build paths inside the project like this: os.path.join(BASE_DIR, ...)",
            "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))",
            "",
            "# Base path for media root and other uploaded files",
            "BASE_DATA_DIR = get_env('BASE_DATA_DIR', get_data_dir())",
            "os.makedirs(BASE_DATA_DIR, exist_ok=True)",
            "logger.info('=> Database and media directory: %s', BASE_DATA_DIR)",
            "",
            "# Databases",
            "# https://docs.djangoproject.com/en/2.1/ref/settings/#databases",
            "DJANGO_DB_MYSQL = 'mysql'",
            "DJANGO_DB_SQLITE = 'sqlite'",
            "DJANGO_DB_POSTGRESQL = 'postgresql'",
            "DJANGO_DB = 'default'",
            "DATABASE_NAME_DEFAULT = os.path.join(BASE_DATA_DIR, 'label_studio.sqlite3')",
            "DATABASE_NAME = get_env('DATABASE_NAME', DATABASE_NAME_DEFAULT)",
            "DATABASES_ALL = {",
            "    DJANGO_DB_POSTGRESQL: {",
            "        'ENGINE': 'django.db.backends.postgresql',",
            "        'USER': get_env('POSTGRE_USER', 'postgres'),",
            "        'PASSWORD': get_env('POSTGRE_PASSWORD', 'postgres'),",
            "        'NAME': get_env('POSTGRE_NAME', 'postgres'),",
            "        'HOST': get_env('POSTGRE_HOST', 'localhost'),",
            "        'PORT': int(get_env('POSTGRE_PORT', '5432')),",
            "    },",
            "    DJANGO_DB_MYSQL: {",
            "        'ENGINE': 'django.db.backends.mysql',",
            "        'USER': get_env('MYSQL_USER', 'root'),",
            "        'PASSWORD': get_env('MYSQL_PASSWORD', ''),",
            "        'NAME': get_env('MYSQL_NAME', 'labelstudio'),",
            "        'HOST': get_env('MYSQL_HOST', 'localhost'),",
            "        'PORT': int(get_env('MYSQL_PORT', '3306')),",
            "    },",
            "    DJANGO_DB_SQLITE: {",
            "        'ENGINE': 'django.db.backends.sqlite3',",
            "        'NAME': DATABASE_NAME,",
            "        'OPTIONS': {",
            "            # 'timeout': 20,",
            "        },",
            "    },",
            "}",
            "DATABASES_ALL['default'] = DATABASES_ALL[DJANGO_DB_POSTGRESQL]",
            "DATABASES = {'default': DATABASES_ALL.get(get_env('DJANGO_DB', 'default'))}",
            "",
            "DEFAULT_AUTO_FIELD = 'django.db.models.AutoField'",
            "",
            "if get_bool_env('GOOGLE_LOGGING_ENABLED', False):",
            "    logging.info('Google Cloud Logging handler is enabled.')",
            "    try:",
            "        import google.cloud.logging",
            "        from google.auth.exceptions import GoogleAuthError",
            "",
            "        client = google.cloud.logging.Client()",
            "        client.setup_logging()",
            "",
            "        LOGGING['handlers']['google_cloud_logging'] = {",
            "            'level': get_env('LOG_LEVEL', 'WARNING'),",
            "            'class': 'google.cloud.logging.handlers.CloudLoggingHandler',",
            "            'client': client,",
            "        }",
            "        LOGGING['root']['handlers'].append('google_cloud_logging')",
            "    except GoogleAuthError:",
            "        logger.exception('Google Cloud Logging handler could not be setup.')",
            "",
            "INSTALLED_APPS = [",
            "    'django.contrib.admin',",
            "    'django.contrib.auth',",
            "    'django.contrib.contenttypes',",
            "    'django.contrib.sessions',",
            "    'django.contrib.messages',",
            "    'django.contrib.staticfiles',",
            "    'django.contrib.humanize',",
            "    'drf_yasg',",
            "    'corsheaders',",
            "    'django_extensions',",
            "    'django_rq',",
            "    'django_filters',",
            "    'rules',",
            "    'annoying',",
            "    'rest_framework',",
            "    'rest_framework.authtoken',",
            "    'drf_generators',",
            "    'core',",
            "    'users',",
            "    'organizations',",
            "    'data_import',",
            "    'data_export',",
            "    'projects',",
            "    'tasks',",
            "    'data_manager',",
            "    'io_storages',",
            "    'ml',",
            "    'webhooks',",
            "    'labels_manager',",
            "]",
            "",
            "MIDDLEWARE = [",
            "    'corsheaders.middleware.CorsMiddleware',",
            "    'django.middleware.security.SecurityMiddleware',",
            "    'django.contrib.sessions.middleware.SessionMiddleware',",
            "    'django.middleware.locale.LocaleMiddleware',",
            "    'core.middleware.DisableCSRF',",
            "    'django.middleware.csrf.CsrfViewMiddleware',",
            "    'django.contrib.auth.middleware.AuthenticationMiddleware',",
            "    'django.contrib.messages.middleware.MessageMiddleware',",
            "    'core.middleware.CommonMiddlewareAppendSlashWithoutRedirect',  # instead of 'CommonMiddleware'",
            "    'core.middleware.CommonMiddleware',",
            "    'django_user_agents.middleware.UserAgentMiddleware',",
            "    'core.middleware.SetSessionUIDMiddleware',",
            "    'core.middleware.ContextLogMiddleware',",
            "    'core.middleware.DatabaseIsLockedRetryMiddleware',",
            "    'core.current_request.ThreadLocalMiddleware',",
            "]",
            "",
            "REST_FRAMEWORK = {",
            "    'DEFAULT_FILTER_BACKENDS': ['django_filters.rest_framework.DjangoFilterBackend'],",
            "    'DEFAULT_AUTHENTICATION_CLASSES': (",
            "        'rest_framework.authentication.TokenAuthentication',",
            "        'rest_framework.authentication.SessionAuthentication',",
            "    ),",
            "    'DEFAULT_PERMISSION_CLASSES': [",
            "        'core.api_permissions.HasObjectPermission',",
            "        'rest_framework.permissions.IsAuthenticated',",
            "    ],",
            "    'EXCEPTION_HANDLER': 'core.utils.common.custom_exception_handler',",
            "    'DEFAULT_RENDERER_CLASSES': ('rest_framework.renderers.JSONRenderer',),",
            "    'DEFAULT_VERSIONING_CLASS': 'rest_framework.versioning.NamespaceVersioning',",
            "    'PAGE_SIZE': 100,",
            "    # 'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.PageNumberPagination'",
            "}",
            "SILENCED_SYSTEM_CHECKS += ['rest_framework.W001']",
            "",
            "# CORS & Host settings",
            "INTERNAL_IPS = [  # django debug toolbar for django==2.2 requirement",
            "    '127.0.0.1',",
            "    'localhost',",
            "]",
            "CORS_ORIGIN_ALLOW_ALL = True",
            "CORS_ALLOW_METHODS = [",
            "    'DELETE',",
            "    'GET',",
            "    'OPTIONS',",
            "    'PATCH',",
            "    'POST',",
            "    'PUT',",
            "]",
            "ALLOWED_HOSTS = ['*']",
            "",
            "# Auth modules",
            "AUTH_USER_MODEL = 'users.User'",
            "AUTHENTICATION_BACKENDS = [",
            "    'rules.permissions.ObjectPermissionBackend',",
            "    'django.contrib.auth.backends.ModelBackend',",
            "]",
            "USE_USERNAME_FOR_LOGIN = False",
            "",
            "DISABLE_SIGNUP_WITHOUT_LINK = get_bool_env('DISABLE_SIGNUP_WITHOUT_LINK', False)",
            "",
            "# Password validation:",
            "# https://docs.djangoproject.com/en/2.1/ref/settings/#auth-password-validators",
            "AUTH_PASSWORD_VALIDATORS = [",
            "    {'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator'},",
            "    {'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator'},",
            "    {'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator'},",
            "    {'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator'},",
            "]",
            "",
            "# Django templates",
            "TEMPLATES_DIR = os.path.join(os.path.dirname(BASE_DIR), 'templates')  # ../../from_this = 'web' dir",
            "TEMPLATES = [",
            "    {",
            "        'BACKEND': 'django.template.backends.django.DjangoTemplates',",
            "        'DIRS': [TEMPLATES_DIR],",
            "        'APP_DIRS': True,",
            "        'OPTIONS': {",
            "            'context_processors': [",
            "                'django.template.context_processors.debug',",
            "                'django.template.context_processors.request',",
            "                'django.contrib.auth.context_processors.auth',",
            "                'django.contrib.messages.context_processors.messages',",
            "                'core.context_processors.settings',",
            "            ],",
            "            'builtins': ['django.templatetags.i18n'],",
            "        },",
            "    }",
            "]",
            "",
            "# RQ",
            "RQ_QUEUES = {",
            "    'critical': {",
            "        'HOST': 'localhost',",
            "        'PORT': 6379,",
            "        'DB': 0,",
            "        'DEFAULT_TIMEOUT': 180,",
            "    },",
            "    'high': {",
            "        'HOST': 'localhost',",
            "        'PORT': 6379,",
            "        'DB': 0,",
            "        'DEFAULT_TIMEOUT': 180,",
            "    },",
            "    'default': {",
            "        'HOST': 'localhost',",
            "        'PORT': 6379,",
            "        'DB': 0,",
            "        'DEFAULT_TIMEOUT': 180,",
            "    },",
            "    'low': {",
            "        'HOST': 'localhost',",
            "        'PORT': 6379,",
            "        'DB': 0,",
            "        'DEFAULT_TIMEOUT': 180,",
            "    },",
            "}",
            "",
            "# Swagger: automatic API documentation",
            "SWAGGER_SETTINGS = {",
            "    'SECURITY_DEFINITIONS': {",
            "        'Token': {",
            "            'type': 'apiKey',",
            "            'name': 'Authorization',",
            "            'in': 'header',",
            "            'description': 'The token (or API key) must be passed as a request header. '",
            "            'You can find your user token on the User Account page in Label Studio. Example: '",
            "            '<br><pre><code class=\"language-bash\">'",
            "            'curl https://label-studio-host/api/projects -H \"Authorization: Token [your-token]\"'",
            "            '</code></pre>',",
            "        }",
            "    },",
            "    'APIS_SORTER': 'alpha',",
            "    'SUPPORTED_SUBMIT_METHODS': ['get', 'post', 'put', 'delete', 'patch'],",
            "    'OPERATIONS_SORTER': 'alpha',",
            "}",
            "",
            "SENTRY_DSN = get_env('SENTRY_DSN', None)",
            "SENTRY_RATE = float(get_env('SENTRY_RATE', 0.25))",
            "SENTRY_ENVIRONMENT = get_env('SENTRY_ENVIRONMENT', 'stage.opensource')",
            "SENTRY_REDIS_ENABLED = False",
            "FRONTEND_SENTRY_DSN = get_env('FRONTEND_SENTRY_DSN', None)",
            "FRONTEND_SENTRY_RATE = get_env('FRONTEND_SENTRY_RATE', 0.1)",
            "FRONTEND_SENTRY_ENVIRONMENT = get_env('FRONTEND_SENTRY_ENVIRONMENT', 'stage.opensource')",
            "",
            "ROOT_URLCONF = 'core.urls'",
            "WSGI_APPLICATION = 'core.wsgi.application'",
            "GRAPHIQL = True",
            "",
            "# Internationalization",
            "# https://docs.djangoproject.com/en/2.1/topics/i18n/",
            "LANGUAGE_CODE = 'en-us'",
            "TIME_ZONE = 'UTC'",
            "USE_I18N = False",
            "USE_L10N = True",
            "USE_TZ = True",
            "",
            "# Static files (CSS, JavaScript, Images)",
            "# https://docs.djangoproject.com/en/2.1/howto/static-files/",
            "STATIC_URL = '/static/'",
            "# if FORCE_SCRIPT_NAME:",
            "#    STATIC_URL = FORCE_SCRIPT_NAME + STATIC_URL",
            "logger.info(f'=> Static URL is set to: {STATIC_URL}')",
            "",
            "STATIC_ROOT = os.path.join(BASE_DIR, 'static_build')",
            "STATICFILES_DIRS = [os.path.join(BASE_DIR, 'static')]",
            "STATICFILES_FINDERS = (",
            "    'django.contrib.staticfiles.finders.FileSystemFinder',",
            "    'django.contrib.staticfiles.finders.AppDirectoriesFinder',",
            ")",
            "STATICFILES_STORAGE = 'core.storage.SkipMissedManifestStaticFilesStorage'",
            "",
            "# Sessions and CSRF",
            "SESSION_COOKIE_SECURE = bool(int(get_env('SESSION_COOKIE_SECURE', False)))",
            "SESSION_COOKIE_SAMESITE = get_env('SESSION_COOKIE_SAMESITE', 'Lax')",
            "",
            "CSRF_COOKIE_SECURE = bool(int(get_env('CSRF_COOKIE_SECURE', SESSION_COOKIE_SECURE)))",
            "CSRF_COOKIE_HTTPONLY = bool(int(get_env('CSRF_COOKIE_HTTPONLY', SESSION_COOKIE_SECURE)))",
            "CSRF_COOKIE_SAMESITE = get_env('CSRF_COOKIE_SAMESITE', 'Lax')",
            "",
            "# Inactivity user sessions",
            "INACTIVITY_SESSION_TIMEOUT_ENABLED = bool(int(get_env('INACTIVITY_SESSION_TIMEOUT_ENABLED', True)))",
            "# The most time a login will last, regardless of activity",
            "MAX_SESSION_AGE = int(get_env('MAX_SESSION_AGE', timedelta(days=14).total_seconds()))",
            "# The most time that can elapse between activity with the server before the user is logged out",
            "MAX_TIME_BETWEEN_ACTIVITY = int(get_env('MAX_TIME_BETWEEN_ACTIVITY', timedelta(days=5).total_seconds()))",
            "",
            "SSRF_PROTECTION_ENABLED = get_bool_env('SSRF_PROTECTION_ENABLED', False)",
            "",
            "# user media files",
            "MEDIA_ROOT = os.path.join(BASE_DATA_DIR, 'media')",
            "os.makedirs(MEDIA_ROOT, exist_ok=True)",
            "MEDIA_URL = '/data/'",
            "UPLOAD_DIR = 'upload'",
            "AVATAR_PATH = 'avatars'",
            "",
            "SUPPORTED_EXTENSIONS = set(",
            "    [",
            "        '.bmp',",
            "        '.csv',",
            "        '.flac',",
            "        '.gif',",
            "        '.htm',",
            "        '.html',",
            "        '.jpg',",
            "        '.jpeg',",
            "        '.json',",
            "        '.m4a',",
            "        '.mp3',",
            "        '.ogg',",
            "        '.png',",
            "        '.svg',",
            "        '.tsv',",
            "        '.txt',",
            "        '.wav',",
            "        '.xml',",
            "        '.mp4',",
            "        '.webm',",
            "        '.webp',",
            "    ]",
            ")",
            "",
            "# directory for files created during unit tests",
            "TEST_DATA_ROOT = os.path.join(BASE_DATA_DIR, 'test_data')",
            "os.makedirs(TEST_DATA_ROOT, exist_ok=True)",
            "",
            "# project exports",
            "EXPORT_DIR = os.path.join(BASE_DATA_DIR, 'export')",
            "EXPORT_URL_ROOT = '/export/'",
            "EXPORT_MIXIN = 'data_export.mixins.ExportMixin'",
            "# old export dir",
            "os.makedirs(EXPORT_DIR, exist_ok=True)",
            "# dir for delayed export",
            "DELAYED_EXPORT_DIR = 'export'",
            "os.makedirs(os.path.join(BASE_DATA_DIR, MEDIA_ROOT, DELAYED_EXPORT_DIR), exist_ok=True)",
            "",
            "# file / task size limits",
            "DATA_UPLOAD_MAX_MEMORY_SIZE = int(get_env('DATA_UPLOAD_MAX_MEMORY_SIZE', 250 * 1024 * 1024))",
            "DATA_UPLOAD_MAX_NUMBER_FILES = int(get_env('DATA_UPLOAD_MAX_NUMBER_FILES', 100))",
            "TASKS_MAX_NUMBER = 1000000",
            "TASKS_MAX_FILE_SIZE = DATA_UPLOAD_MAX_MEMORY_SIZE",
            "",
            "TASK_LOCK_TTL = int(get_env('TASK_LOCK_TTL', default=86400))",
            "",
            "LABEL_STREAM_HISTORY_LIMIT = int(get_env('LABEL_STREAM_HISTORY_LIMIT', default=100))",
            "",
            "RANDOM_NEXT_TASK_SAMPLE_SIZE = int(get_env('RANDOM_NEXT_TASK_SAMPLE_SIZE', 50))",
            "",
            "TASK_API_PAGE_SIZE_MAX = int(get_env('TASK_API_PAGE_SIZE_MAX', 0)) or None",
            "",
            "# Email backend",
            "FROM_EMAIL = get_env('FROM_EMAIL', 'Label Studio <hello@labelstud.io>')",
            "EMAIL_BACKEND = get_env('EMAIL_BACKEND', 'django.core.mail.backends.dummy.EmailBackend')",
            "",
            "ENABLE_LOCAL_FILES_STORAGE = get_bool_env('ENABLE_LOCAL_FILES_STORAGE', default=True)",
            "LOCAL_FILES_SERVING_ENABLED = get_bool_env('LOCAL_FILES_SERVING_ENABLED', default=False)",
            "LOCAL_FILES_DOCUMENT_ROOT = get_env('LOCAL_FILES_DOCUMENT_ROOT', default=os.path.abspath(os.sep))",
            "",
            "SYNC_ON_TARGET_STORAGE_CREATION = get_bool_env('SYNC_ON_TARGET_STORAGE_CREATION', default=True)",
            "",
            "ALLOW_IMPORT_TASKS_WITH_UNKNOWN_EMAILS = get_bool_env('ALLOW_IMPORT_TASKS_WITH_UNKNOWN_EMAILS', default=False)",
            "",
            "\"\"\" React Libraries: do not forget to change this dir in /etc/nginx/nginx.conf \"\"\"",
            "# EDITOR = label-studio-frontend repository",
            "EDITOR_ROOT = os.path.join(BASE_DIR, '../frontend/dist/lsf')",
            "# DM = data manager (included into FRONTEND due npm building, we need only version.json file from there)",
            "DM_ROOT = os.path.join(BASE_DIR, '../frontend/dist/dm')",
            "# FRONTEND = GUI for django backend",
            "REACT_APP_ROOT = os.path.join(BASE_DIR, '../frontend/dist/react-app')",
            "",
            "# per project settings",
            "BATCH_SIZE = 1000",
            "PROJECT_TITLE_MIN_LEN = 3",
            "PROJECT_TITLE_MAX_LEN = 50",
            "LOGIN_REDIRECT_URL = '/'",
            "LOGIN_URL = '/'",
            "MIN_GROUND_TRUTH = 10",
            "DATA_UNDEFINED_NAME = '$undefined$'",
            "LICENSE = {}",
            "VERSIONS = {}",
            "VERSION_EDITION = 'Community'",
            "LATEST_VERSION_CHECK = True",
            "VERSIONS_CHECK_TIME = 0",
            "ALLOW_ORGANIZATION_WEBHOOKS = get_bool_env('ALLOW_ORGANIZATION_WEBHOOKS', False)",
            "CONVERTER_DOWNLOAD_RESOURCES = get_bool_env('CONVERTER_DOWNLOAD_RESOURCES', True)",
            "EXPERIMENTAL_FEATURES = get_bool_env('EXPERIMENTAL_FEATURES', False)",
            "USE_ENFORCE_CSRF_CHECKS = get_bool_env('USE_ENFORCE_CSRF_CHECKS', True)  # False is for tests",
            "CLOUD_FILE_STORAGE_ENABLED = False",
            "",
            "IO_STORAGES_IMPORT_LINK_NAMES = [",
            "    'io_storages_s3importstoragelink',",
            "    'io_storages_gcsimportstoragelink',",
            "    'io_storages_azureblobimportstoragelink',",
            "    'io_storages_localfilesimportstoragelink',",
            "    'io_storages_redisimportstoragelink',",
            "]",
            "",
            "CREATE_ORGANIZATION = 'organizations.functions.create_organization'",
            "SAVE_USER = 'users.functions.save_user'",
            "POST_PROCESS_REIMPORT = 'core.utils.common.empty'",
            "USER_SERIALIZER = 'users.serializers.BaseUserSerializer'",
            "USER_SERIALIZER_UPDATE = 'users.serializers.BaseUserSerializerUpdate'",
            "TASK_SERIALIZER = 'tasks.serializers.BaseTaskSerializer'",
            "EXPORT_DATA_SERIALIZER = 'data_export.serializers.BaseExportDataSerializer'",
            "DATA_MANAGER_GET_ALL_COLUMNS = 'data_manager.functions.get_all_columns'",
            "DATA_MANAGER_ANNOTATIONS_MAP = {}",
            "DATA_MANAGER_ACTIONS = {}",
            "DATA_MANAGER_CUSTOM_FILTER_EXPRESSIONS = 'data_manager.functions.custom_filter_expressions'",
            "DATA_MANAGER_PREPROCESS_FILTER = 'data_manager.functions.preprocess_filter'",
            "USER_LOGIN_FORM = 'users.forms.LoginForm'",
            "PROJECT_MIXIN = 'projects.mixins.ProjectMixin'",
            "TASK_MIXIN = 'tasks.mixins.TaskMixin'",
            "ANNOTATION_MIXIN = 'tasks.mixins.AnnotationMixin'",
            "ORGANIZATION_MIXIN = 'organizations.mixins.OrganizationMixin'",
            "USER_MIXIN = 'users.mixins.UserMixin'",
            "ORGANIZATION_MEMBER_MIXIN = 'organizations.mixins.OrganizationMemberMixin'",
            "MEMBER_PERM = 'core.api_permissions.MemberHasOwnerPermission'",
            "RECALCULATE_ALL_STATS = None",
            "GET_STORAGE_LIST = 'io_storages.functions.get_storage_list'",
            "STORAGE_ANNOTATION_SERIALIZER = 'io_storages.serializers.StorageAnnotationSerializer'",
            "TASK_SERIALIZER_BULK = 'tasks.serializers.BaseTaskSerializerBulk'",
            "PREPROCESS_FIELD_NAME = 'data_manager.functions.preprocess_field_name'",
            "INTERACTIVE_DATA_SERIALIZER = 'data_export.serializers.BaseExportDataSerializerForInteractive'",
            "DELETE_TASKS_ANNOTATIONS_POSTPROCESS = None",
            "",
            "",
            "def project_delete(project):",
            "    project.delete()",
            "",
            "",
            "def user_auth(user_model, email, password):",
            "    return None",
            "",
            "",
            "def collect_versions_dummy(**kwargs):",
            "    return {}",
            "",
            "",
            "PROJECT_DELETE = project_delete",
            "USER_AUTH = user_auth",
            "COLLECT_VERSIONS = collect_versions_dummy",
            "",
            "WEBHOOK_TIMEOUT = float(get_env('WEBHOOK_TIMEOUT', 1.0))",
            "WEBHOOK_BATCH_SIZE = int(get_env('WEBHOOK_BATCH_SIZE', 100))",
            "WEBHOOK_SERIALIZERS = {",
            "    'project': 'webhooks.serializers_for_hooks.ProjectWebhookSerializer',",
            "    'task': 'webhooks.serializers_for_hooks.TaskWebhookSerializer',",
            "    'annotation': 'webhooks.serializers_for_hooks.AnnotationWebhookSerializer',",
            "    'label': 'labels_manager.serializers.LabelSerializer',",
            "    'label_link': 'labels_manager.serializers.LabelLinkSerializer',",
            "}",
            "",
            "EDITOR_KEYMAP = json.dumps(get_env('EDITOR_KEYMAP'))",
            "",
            "# fix a problem with Windows mimetypes for JS and PNG",
            "import mimetypes",
            "",
            "mimetypes.add_type('application/javascript', '.js', True)",
            "mimetypes.add_type('image/png', '.png', True)",
            "",
            "# fields name was used in DM api before",
            "REST_FLEX_FIELDS = {'FIELDS_PARAM': 'include'}",
            "",
            "INTERPOLATE_KEY_FRAMES = get_env('INTERPOLATE_KEY_FRAMES', False)",
            "",
            "# Feature Flags",
            "FEATURE_FLAGS_API_KEY = get_env('FEATURE_FLAGS_API_KEY', default='any key')",
            "",
            "# we may set feature flags from file",
            "FEATURE_FLAGS_FROM_FILE = get_bool_env('FEATURE_FLAGS_FROM_FILE', False)",
            "FEATURE_FLAGS_FILE = get_env('FEATURE_FLAGS_FILE', 'feature_flags.json')",
            "# or if file is not set, default is using offline mode",
            "FEATURE_FLAGS_OFFLINE = get_bool_env('FEATURE_FLAGS_OFFLINE', True)",
            "# default value for feature flags (if not overridden by environment or client)",
            "FEATURE_FLAGS_DEFAULT_VALUE = False",
            "",
            "# Whether to send analytics telemetry data",
            "COLLECT_ANALYTICS = get_bool_env('collect_analytics', True)",
            "",
            "# Strip harmful content from SVG files by default",
            "SVG_SECURITY_CLEANUP = get_bool_env('SVG_SECURITY_CLEANUP', False)",
            "",
            "ML_BLOCK_LOCAL_IP = get_bool_env('ML_BLOCK_LOCAL_IP', False)",
            "",
            "RQ_LONG_JOB_TIMEOUT = int(get_env('RQ_LONG_JOB_TIMEOUT', 36000))",
            "",
            "APP_WEBSERVER = get_env('APP_WEBSERVER', 'django')",
            "",
            "BATCH_JOB_RETRY_TIMEOUT = int(get_env('BATCH_JOB_RETRY_TIMEOUT', 60))",
            "",
            "FUTURE_SAVE_TASK_TO_STORAGE = get_bool_env('FUTURE_SAVE_TASK_TO_STORAGE', default=False)",
            "FUTURE_SAVE_TASK_TO_STORAGE_JSON_EXT = get_bool_env('FUTURE_SAVE_TASK_TO_STORAGE_JSON_EXT', default=True)",
            "STORAGE_IN_PROGRESS_TIMER = float(get_env('STORAGE_IN_PROGRESS_TIMER', 5.0))",
            "",
            "USE_NGINX_FOR_EXPORT_DOWNLOADS = get_bool_env('USE_NGINX_FOR_EXPORT_DOWNLOADS', False)",
            "",
            "if get_env('MINIO_STORAGE_ENDPOINT') and not get_bool_env('MINIO_SKIP', False):",
            "    CLOUD_FILE_STORAGE_ENABLED = True",
            "    DEFAULT_FILE_STORAGE = 'storages.backends.s3boto3.S3Boto3Storage'",
            "    AWS_STORAGE_BUCKET_NAME = get_env('MINIO_STORAGE_BUCKET_NAME')",
            "    AWS_ACCESS_KEY_ID = get_env('MINIO_STORAGE_ACCESS_KEY')",
            "    AWS_SECRET_ACCESS_KEY = get_env('MINIO_STORAGE_SECRET_KEY')",
            "    AWS_S3_ENDPOINT_URL = get_env('MINIO_STORAGE_ENDPOINT')",
            "    AWS_QUERYSTRING_AUTH = False",
            "    # make domain for FileUpload.file",
            "    AWS_S3_SECURE_URLS = False",
            "    AWS_S3_URL_PROTOCOL = 'http:' if HOSTNAME.startswith('http://') else 'https:'",
            "    AWS_S3_CUSTOM_DOMAIN = HOSTNAME.replace('http://', '').replace('https://', '') + '/data'",
            "",
            "if get_env('STORAGE_TYPE') == 's3':",
            "    CLOUD_FILE_STORAGE_ENABLED = True",
            "    DEFAULT_FILE_STORAGE = 'core.storage.CustomS3Boto3Storage'",
            "    if get_env('STORAGE_AWS_ACCESS_KEY_ID'):",
            "        AWS_ACCESS_KEY_ID = get_env('STORAGE_AWS_ACCESS_KEY_ID')",
            "    if get_env('STORAGE_AWS_SECRET_ACCESS_KEY'):",
            "        AWS_SECRET_ACCESS_KEY = get_env('STORAGE_AWS_SECRET_ACCESS_KEY')",
            "    AWS_STORAGE_BUCKET_NAME = get_env('STORAGE_AWS_BUCKET_NAME')",
            "    AWS_S3_REGION_NAME = get_env('STORAGE_AWS_REGION_NAME', None)",
            "    AWS_S3_ENDPOINT_URL = get_env('STORAGE_AWS_ENDPOINT_URL', None)",
            "    if get_env('STORAGE_AWS_OBJECT_PARAMETERS'):",
            "        AWS_S3_OBJECT_PARAMETERS = json.loads(get_env('STORAGE_AWS_OBJECT_PARAMETERS'))",
            "    AWS_QUERYSTRING_EXPIRE = int(get_env('STORAGE_AWS_X_AMZ_EXPIRES', '86400'))",
            "    AWS_LOCATION = get_env('STORAGE_AWS_FOLDER', default='')",
            "    AWS_S3_USE_SSL = get_bool_env('STORAGE_AWS_S3_USE_SSL', True)",
            "    AWS_S3_VERIFY = get_env('STORAGE_AWS_S3_VERIFY', None)",
            "    if AWS_S3_VERIFY == 'false' or AWS_S3_VERIFY == 'False' or AWS_S3_VERIFY == '0':",
            "        AWS_S3_VERIFY = False",
            "    AWS_S3_SIGNATURE_VERSION = get_env('STORAGE_AWS_S3_SIGNATURE_VERSION', None)",
            "",
            "if get_env('STORAGE_TYPE') == 'azure':",
            "    CLOUD_FILE_STORAGE_ENABLED = True",
            "    DEFAULT_FILE_STORAGE = 'core.storage.CustomAzureStorage'",
            "    AZURE_ACCOUNT_NAME = get_env('STORAGE_AZURE_ACCOUNT_NAME')",
            "    AZURE_ACCOUNT_KEY = get_env('STORAGE_AZURE_ACCOUNT_KEY')",
            "    AZURE_CONTAINER = get_env('STORAGE_AZURE_CONTAINER_NAME')",
            "    AZURE_URL_EXPIRATION_SECS = int(get_env('STORAGE_AZURE_URL_EXPIRATION_SECS', '86400'))",
            "    AZURE_LOCATION = get_env('STORAGE_AZURE_FOLDER', default='')",
            "",
            "if get_env('STORAGE_TYPE') == 'gcs':",
            "    CLOUD_FILE_STORAGE_ENABLED = True",
            "    # DEFAULT_FILE_STORAGE = 'storages.backends.gcloud.GoogleCloudStorage'",
            "    DEFAULT_FILE_STORAGE = 'core.storage.AlternativeGoogleCloudStorage'",
            "    GS_PROJECT_ID = get_env('STORAGE_GCS_PROJECT_ID')",
            "    GS_BUCKET_NAME = get_env('STORAGE_GCS_BUCKET_NAME')",
            "    GS_EXPIRATION = timedelta(seconds=int(get_env('STORAGE_GCS_EXPIRATION_SECS', '86400')))",
            "    GS_LOCATION = get_env('STORAGE_GCS_FOLDER', default='')",
            "    GS_CUSTOM_ENDPOINT = get_env('STORAGE_GCS_ENDPOINT')",
            "",
            "CSRF_TRUSTED_ORIGINS = get_env('CSRF_TRUSTED_ORIGINS', [])",
            "if CSRF_TRUSTED_ORIGINS:",
            "    CSRF_TRUSTED_ORIGINS = CSRF_TRUSTED_ORIGINS.split(',')",
            "",
            "REAL_HOSTNAME = os.getenv('HOSTNAME')  # we have to use getenv, because we don't use LABEL_STUDIO_ prefix",
            "GCS_CLOUD_STORAGE_FORCE_DEFAULT_CREDENTIALS = get_bool_env('GCS_CLOUD_STORAGE_FORCE_DEFAULT_CREDENTIALS', False)",
            "PUBLIC_API_DOCS = get_bool_env('PUBLIC_API_DOCS', False)",
            "",
            "# By default, we disallow filters with foreign keys in data manager for security reasons.",
            "# Add to this list (either here in code, or via the env) to allow specific filters that rely on foreign keys.",
            "DATA_MANAGER_FILTER_ALLOWLIST = list(",
            "    set(get_env_list('DATA_MANAGER_FILTER_ALLOWLIST') + ['updated_by__active_organization'])",
            ")",
            "",
            "if ENABLE_CSP := get_bool_env('ENABLE_CSP', True):",
            "    CSP_DEFAULT_SRC = (",
            "        \"'self'\",",
            "        \"'report-sample'\",",
            "    )",
            "    CSP_STYLE_SRC = (\"'self'\", \"'report-sample'\", \"'unsafe-inline'\")",
            "    CSP_SCRIPT_SRC = (",
            "        \"'self'\",",
            "        \"'report-sample'\",",
            "        \"'unsafe-inline'\",",
            "        \"'unsafe-eval'\",",
            "        'blob:',",
            "        'browser.sentry-cdn.com',",
            "        'https://*.googletagmanager.com',",
            "    )",
            "    CSP_IMG_SRC = (",
            "        \"'self'\",",
            "        \"'report-sample'\",",
            "        'data:',",
            "        'https://*.google-analytics.com',",
            "        'https://*.googletagmanager.com',",
            "        'https://*.google.com',",
            "    )",
            "    CSP_CONNECT_SRC = (",
            "        \"'self'\",",
            "        \"'report-sample'\",",
            "        'https://*.google-analytics.com',",
            "        'https://*.analytics.google.com',",
            "        'https://analytics.google.com',",
            "        'https://*.googletagmanager.com',",
            "        'https://*.g.double' + 'click.net',  # hacky way of suppressing codespell complaint",
            "        'https://*.ingest.sentry.io',",
            "    )",
            "    # Note that this will be overridden to real CSP for views that use the override_report_only_csp decorator",
            "    CSP_REPORT_ONLY = get_bool_env('LS_CSP_REPORT_ONLY', True)",
            "    CSP_REPORT_URI = get_env('LS_CSP_REPORT_URI', None)",
            "    CSP_INCLUDE_NONCE_IN = ['script-src', 'default-src']",
            "",
            "    MIDDLEWARE.append('core.middleware.HumanSignalCspMiddleware')"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "label_studio/data_import/api.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " from urllib.parse import unquote, urlparse"
            },
            "1": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " import drf_yasg.openapi as openapi"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 12,
                "PatchRowcode": "+from core.decorators import override_report_only_csp"
            },
            "4": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " from core.feature_flags import flag_set"
            },
            "5": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " from core.permissions import ViewClassPermission, all_permissions"
            },
            "6": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " from core.redis import start_job_async_or_sync"
            },
            "7": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " from core.utils.common import retry_database_locked, timeit"
            },
            "8": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " from core.utils.exceptions import LabelStudioValidationErrorSentryIgnored"
            },
            "9": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from core.utils.params import bool_from_request, list_of_strings_from_request"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 19,
                "PatchRowcode": "+from csp.decorators import csp"
            },
            "11": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " from django.conf import settings"
            },
            "12": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from django.db import transaction"
            },
            "13": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " from django.http import HttpRequest, HttpResponse, HttpResponseRedirect"
            },
            "14": {
                "beforePatchRowNumber": 596,
                "afterPatchRowNumber": 598,
                "PatchRowcode": " class UploadedFileResponse(generics.RetrieveAPIView):"
            },
            "15": {
                "beforePatchRowNumber": 597,
                "afterPatchRowNumber": 599,
                "PatchRowcode": "     permission_classes = (IsAuthenticated,)"
            },
            "16": {
                "beforePatchRowNumber": 598,
                "afterPatchRowNumber": 600,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 601,
                "PatchRowcode": "+    @override_report_only_csp"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 602,
                "PatchRowcode": "+    @csp(SANDBOX=[])"
            },
            "19": {
                "beforePatchRowNumber": 599,
                "afterPatchRowNumber": 603,
                "PatchRowcode": "     @swagger_auto_schema(auto_schema=None)"
            },
            "20": {
                "beforePatchRowNumber": 600,
                "afterPatchRowNumber": 604,
                "PatchRowcode": "     def get(self, *args, **kwargs):"
            },
            "21": {
                "beforePatchRowNumber": 601,
                "afterPatchRowNumber": 605,
                "PatchRowcode": "         request = self.request"
            },
            "22": {
                "beforePatchRowNumber": 613,
                "afterPatchRowNumber": 617,
                "PatchRowcode": "             content_type, encoding = mimetypes.guess_type(str(file.name))"
            },
            "23": {
                "beforePatchRowNumber": 614,
                "afterPatchRowNumber": 618,
                "PatchRowcode": "             content_type = content_type or 'application/octet-stream'"
            },
            "24": {
                "beforePatchRowNumber": 615,
                "afterPatchRowNumber": 619,
                "PatchRowcode": "             return RangedFileResponse(request, file.open(mode='rb'), content_type=content_type)"
            },
            "25": {
                "beforePatchRowNumber": 616,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        else:"
            },
            "26": {
                "beforePatchRowNumber": 617,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return Response(status=status.HTTP_404_NOT_FOUND)"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 620,
                "PatchRowcode": "+"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 621,
                "PatchRowcode": "+        return Response(status=status.HTTP_404_NOT_FOUND)"
            },
            "29": {
                "beforePatchRowNumber": 618,
                "afterPatchRowNumber": 622,
                "PatchRowcode": " "
            },
            "30": {
                "beforePatchRowNumber": 619,
                "afterPatchRowNumber": 623,
                "PatchRowcode": " "
            },
            "31": {
                "beforePatchRowNumber": 620,
                "afterPatchRowNumber": 624,
                "PatchRowcode": " class DownloadStorageData(APIView):"
            }
        },
        "frontPatchFile": [
            "\"\"\"This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.",
            "\"\"\"",
            "import base64",
            "import json",
            "import logging",
            "import mimetypes",
            "import time",
            "from typing import Union",
            "from urllib.parse import unquote, urlparse",
            "",
            "import drf_yasg.openapi as openapi",
            "from core.feature_flags import flag_set",
            "from core.permissions import ViewClassPermission, all_permissions",
            "from core.redis import start_job_async_or_sync",
            "from core.utils.common import retry_database_locked, timeit",
            "from core.utils.exceptions import LabelStudioValidationErrorSentryIgnored",
            "from core.utils.params import bool_from_request, list_of_strings_from_request",
            "from django.conf import settings",
            "from django.db import transaction",
            "from django.http import HttpRequest, HttpResponse, HttpResponseRedirect",
            "from django.utils.decorators import method_decorator",
            "from drf_yasg.utils import swagger_auto_schema",
            "from projects.models import Project, ProjectImport, ProjectReimport",
            "from ranged_fileresponse import RangedFileResponse",
            "from rest_framework import generics, status",
            "from rest_framework.exceptions import ValidationError",
            "from rest_framework.parsers import FormParser, JSONParser, MultiPartParser",
            "from rest_framework.permissions import IsAuthenticated",
            "from rest_framework.response import Response",
            "from rest_framework.views import APIView",
            "from tasks.models import Prediction, Task",
            "from users.models import User",
            "from webhooks.models import WebhookAction",
            "from webhooks.utils import emit_webhooks_for_instance",
            "",
            "from .functions import (",
            "    async_import_background,",
            "    async_reimport_background,",
            "    reformat_predictions,",
            "    set_import_background_failure,",
            "    set_reimport_background_failure,",
            ")",
            "from .models import FileUpload",
            "from .serializers import FileUploadSerializer, ImportApiSerializer, PredictionSerializer",
            "from .uploader import create_file_uploads, load_tasks",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "task_create_response_scheme = {",
            "    201: openapi.Response(",
            "        description='Tasks successfully imported',",
            "        schema=openapi.Schema(",
            "            title='Task creation response',",
            "            description='Task creation response',",
            "            type=openapi.TYPE_OBJECT,",
            "            properties={",
            "                'task_count': openapi.Schema(",
            "                    title='task_count', description='Number of tasks added', type=openapi.TYPE_INTEGER",
            "                ),",
            "                'annotation_count': openapi.Schema(",
            "                    title='annotation_count', description='Number of annotations added', type=openapi.TYPE_INTEGER",
            "                ),",
            "                'predictions_count': openapi.Schema(",
            "                    title='predictions_count', description='Number of predictions added', type=openapi.TYPE_INTEGER",
            "                ),",
            "                'duration': openapi.Schema(",
            "                    title='duration', description='Time in seconds to create', type=openapi.TYPE_NUMBER",
            "                ),",
            "                'file_upload_ids': openapi.Schema(",
            "                    title='file_upload_ids',",
            "                    description='Database IDs of uploaded files',",
            "                    type=openapi.TYPE_ARRAY,",
            "                    items=openapi.Schema(title='File Upload IDs', type=openapi.TYPE_INTEGER),",
            "                ),",
            "                'could_be_tasks_list': openapi.Schema(",
            "                    title='could_be_tasks_list',",
            "                    description='Whether uploaded files can contain lists of tasks, like CSV/TSV files',",
            "                    type=openapi.TYPE_BOOLEAN,",
            "                ),",
            "                'found_formats': openapi.Schema(",
            "                    title='found_formats',",
            "                    description='The list of found file formats',",
            "                    type=openapi.TYPE_ARRAY,",
            "                    items=openapi.Schema(title='File format', type=openapi.TYPE_STRING),",
            "                ),",
            "                'data_columns': openapi.Schema(",
            "                    title='data_columns',",
            "                    description='The list of found data columns',",
            "                    type=openapi.TYPE_ARRAY,",
            "                    items=openapi.Schema(title='Data column name', type=openapi.TYPE_STRING),",
            "                ),",
            "            },",
            "        ),",
            "    ),",
            "    400: openapi.Schema(",
            "        title='Incorrect task data', description='String with error description', type=openapi.TYPE_STRING",
            "    ),",
            "}",
            "",
            "",
            "@method_decorator(",
            "    name='post',",
            "    decorator=swagger_auto_schema(",
            "        tags=['Import'],",
            "        responses=task_create_response_scheme,",
            "        manual_parameters=[",
            "            openapi.Parameter(",
            "                name='id',",
            "                type=openapi.TYPE_INTEGER,",
            "                in_=openapi.IN_PATH,",
            "                description='A unique integer value identifying this project.',",
            "            ),",
            "        ],",
            "        operation_summary='Import tasks',",
            "        operation_description=\"\"\"",
            "            Import data as labeling tasks in bulk using this API endpoint. You can use this API endpoint to import multiple tasks.",
            "            One POST request is limited at 250K tasks and 200 MB.",
            "",
            "            **Note:** Imported data is verified against a project *label_config* and must",
            "            include all variables that were used in the *label_config*. For example,",
            "            if the label configuration has a *$text* variable, then each item in a data object",
            "            must include a \"text\" field.",
            "            <br>",
            "",
            "            ## POST requests",
            "            <hr style=\"opacity:0.3\">",
            "",
            "            There are three possible ways to import tasks with this endpoint:",
            "",
            "            ### 1\\. **POST with data**",
            "            Send JSON tasks as POST data. Only JSON is supported for POSTing files directly.",
            "            Update this example to specify your authorization token and Label Studio instance host, then run the following from",
            "            the command line.",
            "",
            "            ```bash",
            "            curl -H 'Content-Type: application/json' -H 'Authorization: Token abc123' \\\\",
            "            -X POST '{host}/api/projects/1/import' --data '[{{\"text\": \"Some text 1\"}}, {{\"text\": \"Some text 2\"}}]'",
            "            ```",
            "",
            "            ### 2\\. **POST with files**",
            "            Send tasks as files. You can attach multiple files with different names.",
            "",
            "            - **JSON**: text files in JavaScript object notation format",
            "            - **CSV**: text files with tables in Comma Separated Values format",
            "            - **TSV**: text files with tables in Tab Separated Value format",
            "            - **TXT**: simple text files are similar to CSV with one column and no header, supported for projects with one source only",
            "",
            "            Update this example to specify your authorization token, Label Studio instance host, and file name and path,",
            "            then run the following from the command line:",
            "",
            "            ```bash",
            "            curl -H 'Authorization: Token abc123' \\\\",
            "            -X POST '{host}/api/projects/1/import' -F \u2018file=@path/to/my_file.csv\u2019",
            "            ```",
            "",
            "            ### 3\\. **POST with URL**",
            "            You can also provide a URL to a file with labeling tasks. Supported file formats are the same as in option 2.",
            "",
            "            ```bash",
            "            curl -H 'Content-Type: application/json' -H 'Authorization: Token abc123' \\\\",
            "            -X POST '{host}/api/projects/1/import' \\\\",
            "            --data '[{{\"url\": \"http://example.com/test1.csv\"}}, {{\"url\": \"http://example.com/test2.csv\"}}]'",
            "            ```",
            "",
            "            <br>",
            "        \"\"\".format(",
            "            host=(settings.HOSTNAME or 'https://localhost:8080')",
            "        ),",
            "    ),",
            ")",
            "# Import",
            "class ImportAPI(generics.CreateAPIView):",
            "    permission_required = all_permissions.projects_change",
            "    parser_classes = (JSONParser, MultiPartParser, FormParser)",
            "    serializer_class = ImportApiSerializer",
            "    queryset = Task.objects.all()",
            "",
            "    def get_serializer_context(self):",
            "        project_id = self.kwargs.get('pk')",
            "        if project_id:",
            "            project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=project_id)",
            "        else:",
            "            project = None",
            "        return {'project': project, 'user': self.request.user}",
            "",
            "    def post(self, *args, **kwargs):",
            "        return super(ImportAPI, self).post(*args, **kwargs)",
            "",
            "    def _save(self, tasks):",
            "        serializer = self.get_serializer(data=tasks, many=True)",
            "        serializer.is_valid(raise_exception=True)",
            "        task_instances = serializer.save(project_id=self.kwargs['pk'])",
            "        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs['pk'])",
            "        emit_webhooks_for_instance(",
            "            self.request.user.active_organization, project, WebhookAction.TASKS_CREATED, task_instances",
            "        )",
            "        return task_instances, serializer",
            "",
            "    def sync_import(self, request, project, preannotated_from_fields, commit_to_project, return_task_ids):",
            "        start = time.time()",
            "        tasks = None",
            "        # upload files from request, and parse all tasks",
            "        # TODO: Stop passing request to load_tasks function, make all validation before",
            "        parsed_data, file_upload_ids, could_be_tasks_list, found_formats, data_columns = load_tasks(request, project)",
            "",
            "        if preannotated_from_fields:",
            "            # turn flat task JSONs {\"column1\": value, \"column2\": value} into {\"data\": {\"column1\"..}, \"predictions\": [{...\"column2\"}]",
            "            parsed_data = reformat_predictions(parsed_data, preannotated_from_fields)",
            "",
            "        if commit_to_project:",
            "            # Immediately create project tasks and update project states and counters",
            "            tasks, serializer = self._save(parsed_data)",
            "            task_count = len(tasks)",
            "            annotation_count = len(serializer.db_annotations)",
            "            prediction_count = len(serializer.db_predictions)",
            "",
            "            recalculate_stats_counts = {",
            "                'task_count': task_count,",
            "                'annotation_count': annotation_count,",
            "                'prediction_count': prediction_count,",
            "            }",
            "",
            "            # Update counters (like total_annotations) for new tasks and after bulk update tasks stats. It should be a",
            "            # single operation as counters affect bulk is_labeled update",
            "            project.update_tasks_counters_and_task_states(",
            "                tasks_queryset=tasks,",
            "                maximum_annotations_changed=False,",
            "                overlap_cohort_percentage_changed=False,",
            "                tasks_number_changed=True,",
            "                recalculate_stats_counts=recalculate_stats_counts,",
            "            )",
            "            logger.info('Tasks bulk_update finished (sync import)')",
            "",
            "            project.summary.update_data_columns(parsed_data)",
            "            # TODO: project.summary.update_created_annotations_and_labels",
            "        else:",
            "            # Do nothing - just output file upload ids for further use",
            "            task_count = len(parsed_data)",
            "            annotation_count = None",
            "            prediction_count = None",
            "",
            "        duration = time.time() - start",
            "",
            "        response = {",
            "            'task_count': task_count,",
            "            'annotation_count': annotation_count,",
            "            'prediction_count': prediction_count,",
            "            'duration': duration,",
            "            'file_upload_ids': file_upload_ids,",
            "            'could_be_tasks_list': could_be_tasks_list,",
            "            'found_formats': found_formats,",
            "            'data_columns': data_columns,",
            "        }",
            "        if tasks and return_task_ids:",
            "            response['task_ids'] = [task.id for task in tasks]",
            "",
            "        return Response(response, status=status.HTTP_201_CREATED)",
            "",
            "    @timeit",
            "    def async_import(self, request, project, preannotated_from_fields, commit_to_project, return_task_ids):",
            "",
            "        project_import = ProjectImport.objects.create(",
            "            project=project,",
            "            preannotated_from_fields=preannotated_from_fields,",
            "            commit_to_project=commit_to_project,",
            "            return_task_ids=return_task_ids,",
            "        )",
            "",
            "        if len(request.FILES):",
            "            logger.debug(f'Import from files: {request.FILES}')",
            "            file_upload_ids, could_be_tasks_list = create_file_uploads(request.user, project, request.FILES)",
            "            project_import.file_upload_ids = file_upload_ids",
            "            project_import.could_be_tasks_list = could_be_tasks_list",
            "            project_import.save(update_fields=['file_upload_ids', 'could_be_tasks_list'])",
            "        elif 'application/x-www-form-urlencoded' in request.content_type:",
            "            logger.debug(f'Import from url: {request.data.get(\"url\")}')",
            "            # empty url",
            "            url = request.data.get('url')",
            "            if not url:",
            "                raise ValidationError('\"url\" is not found in request data')",
            "            project_import.url = url",
            "            project_import.save(update_fields=['url'])",
            "        # take one task from request DATA",
            "        elif 'application/json' in request.content_type and isinstance(request.data, dict):",
            "            project_import.tasks = [request.data]",
            "            project_import.save(update_fields=['tasks'])",
            "",
            "        # take many tasks from request DATA",
            "        elif 'application/json' in request.content_type and isinstance(request.data, list):",
            "            project_import.tasks = request.data",
            "            project_import.save(update_fields=['tasks'])",
            "",
            "        # incorrect data source",
            "        else:",
            "            raise ValidationError('load_tasks: No data found in DATA or in FILES')",
            "",
            "        start_job_async_or_sync(",
            "            async_import_background,",
            "            project_import.id,",
            "            request.user.id,",
            "            on_failure=set_import_background_failure,",
            "            project_id=project.id,",
            "            organization_id=request.user.active_organization.id,",
            "        )",
            "",
            "        response = {'import': project_import.id}",
            "        return Response(response, status=status.HTTP_201_CREATED)",
            "",
            "    def create(self, request, *args, **kwargs):",
            "        commit_to_project = bool_from_request(request.query_params, 'commit_to_project', True)",
            "        return_task_ids = bool_from_request(request.query_params, 'return_task_ids', False)",
            "        preannotated_from_fields = list_of_strings_from_request(request.query_params, 'preannotated_from_fields', None)",
            "",
            "        # check project permissions",
            "        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs['pk'])",
            "",
            "        if (",
            "            flag_set('fflag_feat_all_lsdv_4915_async_task_import_13042023_short', request.user)",
            "            and settings.VERSION_EDITION != 'Community'",
            "        ):",
            "            return self.async_import(request, project, preannotated_from_fields, commit_to_project, return_task_ids)",
            "        else:",
            "            return self.sync_import(request, project, preannotated_from_fields, commit_to_project, return_task_ids)",
            "",
            "",
            "# Import",
            "class ImportPredictionsAPI(generics.CreateAPIView):",
            "    permission_required = all_permissions.projects_change",
            "    parser_classes = (JSONParser, MultiPartParser, FormParser)",
            "    serializer_class = PredictionSerializer",
            "    queryset = Project.objects.all()",
            "    swagger_schema = None  # TODO: create API schema",
            "",
            "    def create(self, request, *args, **kwargs):",
            "        # check project permissions",
            "        project = self.get_object()",
            "",
            "        tasks_ids = set(Task.objects.filter(project=project).values_list('id', flat=True))",
            "",
            "        logger.debug(",
            "            f'Importing {len(self.request.data)} predictions to project {project} with {len(tasks_ids)} tasks'",
            "        )",
            "        predictions = []",
            "        for item in self.request.data:",
            "            if item.get('task') not in tasks_ids:",
            "                raise LabelStudioValidationErrorSentryIgnored(",
            "                    f'{item} contains invalid \"task\" field: corresponding task ID couldn\\'t be retrieved '",
            "                    f'from project {project} tasks'",
            "                )",
            "            predictions.append(",
            "                Prediction(",
            "                    task_id=item['task'],",
            "                    project_id=project.id,",
            "                    result=Prediction.prepare_prediction_result(item.get('result'), project),",
            "                    score=item.get('score'),",
            "                    model_version=item.get('model_version', 'undefined'),",
            "                )",
            "            )",
            "        predictions_obj = Prediction.objects.bulk_create(predictions, batch_size=settings.BATCH_SIZE)",
            "        project.update_tasks_counters(Task.objects.filter(id__in=tasks_ids))",
            "        return Response({'created': len(predictions_obj)}, status=status.HTTP_201_CREATED)",
            "",
            "",
            "class TasksBulkCreateAPI(ImportAPI):",
            "    # just for compatibility - can be safely removed",
            "    swagger_schema = None",
            "",
            "",
            "class ReImportAPI(ImportAPI):",
            "    permission_required = all_permissions.projects_change",
            "",
            "    def sync_reimport(self, project, file_upload_ids, files_as_tasks_list):",
            "        start = time.time()",
            "        tasks, found_formats, data_columns = FileUpload.load_tasks_from_uploaded_files(",
            "            project, file_upload_ids, files_as_tasks_list=files_as_tasks_list",
            "        )",
            "",
            "        with transaction.atomic():",
            "            project.remove_tasks_by_file_uploads(file_upload_ids)",
            "            tasks, serializer = self._save(tasks)",
            "        duration = time.time() - start",
            "",
            "        task_count = len(tasks)",
            "        annotation_count = len(serializer.db_annotations)",
            "        prediction_count = len(serializer.db_predictions)",
            "",
            "        # Update counters (like total_annotations) for new tasks and after bulk update tasks stats. It should be a",
            "        # single operation as counters affect bulk is_labeled update",
            "        project.update_tasks_counters_and_task_states(",
            "            tasks_queryset=tasks,",
            "            maximum_annotations_changed=False,",
            "            overlap_cohort_percentage_changed=False,",
            "            tasks_number_changed=True,",
            "            recalculate_stats_counts={",
            "                'task_count': task_count,",
            "                'annotation_count': annotation_count,",
            "                'prediction_count': prediction_count,",
            "            },",
            "        )",
            "        logger.info('Tasks bulk_update finished (sync reimport)')",
            "",
            "        project.summary.update_data_columns(tasks)",
            "        # TODO: project.summary.update_created_annotations_and_labels",
            "",
            "        return Response(",
            "            {",
            "                'task_count': task_count,",
            "                'annotation_count': annotation_count,",
            "                'prediction_count': prediction_count,",
            "                'duration': duration,",
            "                'file_upload_ids': file_upload_ids,",
            "                'found_formats': found_formats,",
            "                'data_columns': data_columns,",
            "            },",
            "            status=status.HTTP_201_CREATED,",
            "        )",
            "",
            "    def async_reimport(self, project, file_upload_ids, files_as_tasks_list, organization_id):",
            "",
            "        project_reimport = ProjectReimport.objects.create(",
            "            project=project, file_upload_ids=file_upload_ids, files_as_tasks_list=files_as_tasks_list",
            "        )",
            "",
            "        start_job_async_or_sync(",
            "            async_reimport_background,",
            "            project_reimport.id,",
            "            organization_id,",
            "            self.request.user,",
            "            on_failure=set_reimport_background_failure,",
            "            project_id=project.id,",
            "        )",
            "",
            "        response = {'reimport': project_reimport.id}",
            "        return Response(response, status=status.HTTP_201_CREATED)",
            "",
            "    @retry_database_locked()",
            "    def create(self, request, *args, **kwargs):",
            "        files_as_tasks_list = bool_from_request(request.data, 'files_as_tasks_list', True)",
            "        file_upload_ids = self.request.data.get('file_upload_ids')",
            "",
            "        # check project permissions",
            "        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs['pk'])",
            "",
            "        if not file_upload_ids:",
            "            return Response(",
            "                {",
            "                    'task_count': 0,",
            "                    'annotation_count': 0,",
            "                    'prediction_count': 0,",
            "                    'duration': 0,",
            "                    'file_upload_ids': [],",
            "                    'found_formats': {},",
            "                    'data_columns': [],",
            "                },",
            "                status=status.HTTP_200_OK,",
            "            )",
            "",
            "        if (",
            "            flag_set('fflag_fix_all_lsdv_4971_async_reimport_09052023_short', request.user)",
            "            and settings.VERSION_EDITION != 'Community'",
            "        ):",
            "            return self.async_reimport(",
            "                project, file_upload_ids, files_as_tasks_list, request.user.active_organization_id",
            "            )",
            "        else:",
            "            return self.sync_reimport(project, file_upload_ids, files_as_tasks_list)",
            "",
            "    @swagger_auto_schema(",
            "        auto_schema=None,",
            "        operation_summary='Re-import tasks',",
            "        operation_description=\"\"\"",
            "        Re-import tasks using the specified file upload IDs for a specific project.",
            "        \"\"\",",
            "    )",
            "    def post(self, *args, **kwargs):",
            "        return super(ReImportAPI, self).post(*args, **kwargs)",
            "",
            "",
            "@method_decorator(",
            "    name='get',",
            "    decorator=swagger_auto_schema(",
            "        tags=['Import'],",
            "        operation_summary='Get files list',",
            "        manual_parameters=[",
            "            openapi.Parameter(",
            "                name='all',",
            "                type=openapi.TYPE_BOOLEAN,",
            "                in_=openapi.IN_QUERY,",
            "                description='Set to \"true\" if you want to retrieve all file uploads',",
            "            ),",
            "            openapi.Parameter(",
            "                name='ids',",
            "                type=openapi.TYPE_ARRAY,",
            "                in_=openapi.IN_QUERY,",
            "                items=openapi.Schema(title='File upload ID', type=openapi.TYPE_INTEGER),",
            "                description='Specify the list of file upload IDs to retrieve, e.g. ids=[1,2,3]',",
            "            ),",
            "        ],",
            "        operation_description=\"\"\"",
            "        Retrieve the list of uploaded files used to create labeling tasks for a specific project.",
            "        \"\"\",",
            "    ),",
            ")",
            "@method_decorator(",
            "    name='delete',",
            "    decorator=swagger_auto_schema(",
            "        tags=['Import'],",
            "        operation_summary='Delete files',",
            "        operation_description=\"\"\"",
            "        Delete uploaded files for a specific project.",
            "        \"\"\",",
            "    ),",
            ")",
            "class FileUploadListAPI(generics.mixins.ListModelMixin, generics.mixins.DestroyModelMixin, generics.GenericAPIView):",
            "    parser_classes = (JSONParser, MultiPartParser, FormParser)",
            "    serializer_class = FileUploadSerializer",
            "    permission_required = ViewClassPermission(",
            "        GET=all_permissions.projects_view,",
            "        DELETE=all_permissions.projects_change,",
            "    )",
            "    queryset = FileUpload.objects.all()",
            "",
            "    def get_queryset(self):",
            "        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs.get('pk', 0))",
            "        if project.is_draft or bool_from_request(self.request.query_params, 'all', False):",
            "            # If project is in draft state, we return all uploaded files, ignoring queried ids",
            "            logger.debug(f'Return all uploaded files for draft project {project}')",
            "            return FileUpload.objects.filter(project_id=project.id, user=self.request.user)",
            "",
            "        # If requested in regular import, only queried IDs are returned to avoid showing previously imported",
            "        ids = json.loads(self.request.query_params.get('ids', '[]'))",
            "        logger.debug(f'File Upload IDs found: {ids}')",
            "        return FileUpload.objects.filter(project_id=project.id, id__in=ids, user=self.request.user)",
            "",
            "    def get(self, request, *args, **kwargs):",
            "        return self.list(request, *args, **kwargs)",
            "",
            "    def delete(self, request, *args, **kwargs):",
            "        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs['pk'])",
            "        ids = self.request.data.get('file_upload_ids')",
            "        if ids is None:",
            "            deleted, _ = FileUpload.objects.filter(project=project).delete()",
            "        elif isinstance(ids, list):",
            "            deleted, _ = FileUpload.objects.filter(project=project, id__in=ids).delete()",
            "        else:",
            "            raise ValueError('\"file_upload_ids\" parameter must be a list of integers')",
            "        return Response({'deleted': deleted}, status=status.HTTP_200_OK)",
            "",
            "",
            "@method_decorator(",
            "    name='get',",
            "    decorator=swagger_auto_schema(",
            "        tags=['Import'],",
            "        operation_summary='Get file upload',",
            "        operation_description='Retrieve details about a specific uploaded file.',",
            "    ),",
            ")",
            "@method_decorator(",
            "    name='patch',",
            "    decorator=swagger_auto_schema(",
            "        tags=['Import'],",
            "        operation_summary='Update file upload',",
            "        operation_description='Update a specific uploaded file.',",
            "        request_body=FileUploadSerializer,",
            "    ),",
            ")",
            "@method_decorator(",
            "    name='delete',",
            "    decorator=swagger_auto_schema(",
            "        tags=['Import'],",
            "        operation_summary='Delete file upload',",
            "        operation_description='Delete a specific uploaded file.',",
            "    ),",
            ")",
            "class FileUploadAPI(generics.RetrieveUpdateDestroyAPIView):",
            "    parser_classes = (JSONParser, MultiPartParser, FormParser)",
            "    permission_classes = (IsAuthenticated,)",
            "    serializer_class = FileUploadSerializer",
            "    queryset = FileUpload.objects.all()",
            "",
            "    def get(self, *args, **kwargs):",
            "        return super(FileUploadAPI, self).get(*args, **kwargs)",
            "",
            "    def patch(self, *args, **kwargs):",
            "        return super(FileUploadAPI, self).patch(*args, **kwargs)",
            "",
            "    def delete(self, *args, **kwargs):",
            "        return super(FileUploadAPI, self).delete(*args, **kwargs)",
            "",
            "    @swagger_auto_schema(auto_schema=None)",
            "    def put(self, *args, **kwargs):",
            "        return super(FileUploadAPI, self).put(*args, **kwargs)",
            "",
            "",
            "class UploadedFileResponse(generics.RetrieveAPIView):",
            "    permission_classes = (IsAuthenticated,)",
            "",
            "    @swagger_auto_schema(auto_schema=None)",
            "    def get(self, *args, **kwargs):",
            "        request = self.request",
            "        filename = kwargs['filename']",
            "        # XXX needed, on windows os.path.join generates '\\' which breaks FileUpload",
            "        file = settings.UPLOAD_DIR + ('/' if not settings.UPLOAD_DIR.endswith('/') else '') + filename",
            "        logger.debug(f'Fetch uploaded file by user {request.user} => {file}')",
            "        file_upload = FileUpload.objects.filter(file=file).last()",
            "",
            "        if not file_upload.has_permission(request.user):",
            "            return Response(status=status.HTTP_403_FORBIDDEN)",
            "",
            "        file = file_upload.file",
            "        if file.storage.exists(file.name):",
            "            content_type, encoding = mimetypes.guess_type(str(file.name))",
            "            content_type = content_type or 'application/octet-stream'",
            "            return RangedFileResponse(request, file.open(mode='rb'), content_type=content_type)",
            "        else:",
            "            return Response(status=status.HTTP_404_NOT_FOUND)",
            "",
            "",
            "class DownloadStorageData(APIView):",
            "    \"\"\"Check auth for nginx auth_request\"\"\"",
            "",
            "    swagger_schema = None",
            "    http_method_names = ['get']",
            "    permission_classes = (IsAuthenticated,)",
            "",
            "    def get(self, request, *args, **kwargs):",
            "        \"\"\"Get export files list\"\"\"",
            "        request = self.request",
            "        filepath = request.GET.get('filepath')",
            "        if filepath is None:",
            "            return Response(status=status.HTTP_404_NOT_FOUND)",
            "",
            "        filepath = unquote(request.GET['filepath'])",
            "",
            "        url = None",
            "        if filepath.startswith(settings.UPLOAD_DIR):",
            "            logger.debug(f'Fetch uploaded file by user {request.user} => {filepath}')",
            "            file_upload = FileUpload.objects.filter(file=filepath).last()",
            "",
            "            if file_upload is not None and file_upload.has_permission(request.user):",
            "                url = file_upload.file.storage.url(file_upload.file.name, storage_url=True)",
            "        elif filepath.startswith(settings.AVATAR_PATH):",
            "            user = User.objects.filter(avatar=filepath).first()",
            "            if user is not None and request.user.active_organization.has_user(user):",
            "                url = user.avatar.storage.url(user.avatar.name, storage_url=True)",
            "",
            "        if url is None:",
            "            return Response(status=status.HTTP_403_FORBIDDEN)",
            "",
            "        protocol = urlparse(url).scheme",
            "",
            "        # Let NGINX handle it",
            "        response = HttpResponse()",
            "        # The below header tells NGINX to catch it and serve, see docker-config/nginx-app.conf",
            "        redirect = '/file_download/' + protocol + '/' + url.replace(protocol + '://', '')",
            "",
            "        response['X-Accel-Redirect'] = redirect",
            "        response['Content-Disposition'] = 'attachment; filename=\"{}\"'.format(filepath)",
            "        return response",
            "",
            "",
            "class PresignAPIMixin:",
            "    def handle_presign(self, request: HttpRequest, fileuri: str, instance: Union[Task, Project]) -> Response:",
            "        model_name = type(instance).__name__",
            "",
            "        if not instance.has_permission(request.user):",
            "            return Response(status=status.HTTP_403_FORBIDDEN)",
            "",
            "        # Attempt to base64 decode the fileuri",
            "        try:",
            "            fileuri = base64.urlsafe_b64decode(fileuri.encode()).decode()",
            "        # For backwards compatibility, try unquote if this fails",
            "        except Exception as exc:",
            "            logger.debug(",
            "                f'Failed to decode base64 {fileuri} for {model_name} {instance.id}: {exc} falling back to unquote'",
            "            )",
            "            fileuri = unquote(fileuri)",
            "",
            "        try:",
            "            resolved = instance.resolve_storage_uri(fileuri)",
            "        except Exception as exc:",
            "            logger.error(f'Failed to resolve storage uri {fileuri} for {model_name} {instance.id}: {exc}')",
            "            return Response(status=status.HTTP_404_NOT_FOUND)",
            "",
            "        if resolved is None or resolved.get('url') is None:",
            "            return Response(status=status.HTTP_404_NOT_FOUND)",
            "",
            "        url = resolved['url']",
            "        max_age = 0",
            "        if resolved.get('presign_ttl'):",
            "            max_age = resolved.get('presign_ttl') * 60",
            "",
            "        # Proxy to presigned url",
            "        response = HttpResponseRedirect(redirect_to=url, status=status.HTTP_303_SEE_OTHER)",
            "        response.headers['Cache-Control'] = f'no-store, max-age={max_age}'",
            "",
            "        return response",
            "",
            "",
            "class TaskPresignStorageData(PresignAPIMixin, APIView):",
            "    \"\"\"A file proxy to presign storage urls at the task level.\"\"\"",
            "",
            "    swagger_schema = None",
            "    http_method_names = ['get']",
            "    permission_classes = (IsAuthenticated,)",
            "",
            "    def get(self, request, *args, **kwargs):",
            "        \"\"\"Get the presigned url for a given fileuri\"\"\"",
            "        request = self.request",
            "        task_id = kwargs.get('task_id')",
            "        fileuri = request.GET.get('fileuri')",
            "",
            "        if fileuri is None or task_id is None:",
            "            return Response(status=status.HTTP_400_BAD_REQUEST)",
            "",
            "        try:",
            "            task = Task.objects.get(pk=task_id)",
            "        except Task.DoesNotExist:",
            "            return Response(status=status.HTTP_404_NOT_FOUND)",
            "",
            "        return self.handle_presign(request, fileuri, task)",
            "",
            "",
            "class ProjectPresignStorageData(PresignAPIMixin, APIView):",
            "    \"\"\"A file proxy to presign storage urls at the project level.\"\"\"",
            "",
            "    swagger_schema = None",
            "    http_method_names = ['get']",
            "    permission_classes = (IsAuthenticated,)",
            "",
            "    def get(self, request, *args, **kwargs):",
            "        \"\"\"Get the presigned url for a given fileuri\"\"\"",
            "        request = self.request",
            "        project_id = kwargs.get('project_id')",
            "        fileuri = request.GET.get('fileuri')",
            "",
            "        if fileuri is None or project_id is None:",
            "            return Response(status=status.HTTP_400_BAD_REQUEST)",
            "",
            "        try:",
            "            project = Project.objects.get(pk=project_id)",
            "        except Project.DoesNotExist:",
            "            return Response(status=status.HTTP_404_NOT_FOUND)",
            "",
            "        return self.handle_presign(request, fileuri, project)"
        ],
        "afterPatchFile": [
            "\"\"\"This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.",
            "\"\"\"",
            "import base64",
            "import json",
            "import logging",
            "import mimetypes",
            "import time",
            "from typing import Union",
            "from urllib.parse import unquote, urlparse",
            "",
            "import drf_yasg.openapi as openapi",
            "from core.decorators import override_report_only_csp",
            "from core.feature_flags import flag_set",
            "from core.permissions import ViewClassPermission, all_permissions",
            "from core.redis import start_job_async_or_sync",
            "from core.utils.common import retry_database_locked, timeit",
            "from core.utils.exceptions import LabelStudioValidationErrorSentryIgnored",
            "from core.utils.params import bool_from_request, list_of_strings_from_request",
            "from csp.decorators import csp",
            "from django.conf import settings",
            "from django.db import transaction",
            "from django.http import HttpRequest, HttpResponse, HttpResponseRedirect",
            "from django.utils.decorators import method_decorator",
            "from drf_yasg.utils import swagger_auto_schema",
            "from projects.models import Project, ProjectImport, ProjectReimport",
            "from ranged_fileresponse import RangedFileResponse",
            "from rest_framework import generics, status",
            "from rest_framework.exceptions import ValidationError",
            "from rest_framework.parsers import FormParser, JSONParser, MultiPartParser",
            "from rest_framework.permissions import IsAuthenticated",
            "from rest_framework.response import Response",
            "from rest_framework.views import APIView",
            "from tasks.models import Prediction, Task",
            "from users.models import User",
            "from webhooks.models import WebhookAction",
            "from webhooks.utils import emit_webhooks_for_instance",
            "",
            "from .functions import (",
            "    async_import_background,",
            "    async_reimport_background,",
            "    reformat_predictions,",
            "    set_import_background_failure,",
            "    set_reimport_background_failure,",
            ")",
            "from .models import FileUpload",
            "from .serializers import FileUploadSerializer, ImportApiSerializer, PredictionSerializer",
            "from .uploader import create_file_uploads, load_tasks",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "task_create_response_scheme = {",
            "    201: openapi.Response(",
            "        description='Tasks successfully imported',",
            "        schema=openapi.Schema(",
            "            title='Task creation response',",
            "            description='Task creation response',",
            "            type=openapi.TYPE_OBJECT,",
            "            properties={",
            "                'task_count': openapi.Schema(",
            "                    title='task_count', description='Number of tasks added', type=openapi.TYPE_INTEGER",
            "                ),",
            "                'annotation_count': openapi.Schema(",
            "                    title='annotation_count', description='Number of annotations added', type=openapi.TYPE_INTEGER",
            "                ),",
            "                'predictions_count': openapi.Schema(",
            "                    title='predictions_count', description='Number of predictions added', type=openapi.TYPE_INTEGER",
            "                ),",
            "                'duration': openapi.Schema(",
            "                    title='duration', description='Time in seconds to create', type=openapi.TYPE_NUMBER",
            "                ),",
            "                'file_upload_ids': openapi.Schema(",
            "                    title='file_upload_ids',",
            "                    description='Database IDs of uploaded files',",
            "                    type=openapi.TYPE_ARRAY,",
            "                    items=openapi.Schema(title='File Upload IDs', type=openapi.TYPE_INTEGER),",
            "                ),",
            "                'could_be_tasks_list': openapi.Schema(",
            "                    title='could_be_tasks_list',",
            "                    description='Whether uploaded files can contain lists of tasks, like CSV/TSV files',",
            "                    type=openapi.TYPE_BOOLEAN,",
            "                ),",
            "                'found_formats': openapi.Schema(",
            "                    title='found_formats',",
            "                    description='The list of found file formats',",
            "                    type=openapi.TYPE_ARRAY,",
            "                    items=openapi.Schema(title='File format', type=openapi.TYPE_STRING),",
            "                ),",
            "                'data_columns': openapi.Schema(",
            "                    title='data_columns',",
            "                    description='The list of found data columns',",
            "                    type=openapi.TYPE_ARRAY,",
            "                    items=openapi.Schema(title='Data column name', type=openapi.TYPE_STRING),",
            "                ),",
            "            },",
            "        ),",
            "    ),",
            "    400: openapi.Schema(",
            "        title='Incorrect task data', description='String with error description', type=openapi.TYPE_STRING",
            "    ),",
            "}",
            "",
            "",
            "@method_decorator(",
            "    name='post',",
            "    decorator=swagger_auto_schema(",
            "        tags=['Import'],",
            "        responses=task_create_response_scheme,",
            "        manual_parameters=[",
            "            openapi.Parameter(",
            "                name='id',",
            "                type=openapi.TYPE_INTEGER,",
            "                in_=openapi.IN_PATH,",
            "                description='A unique integer value identifying this project.',",
            "            ),",
            "        ],",
            "        operation_summary='Import tasks',",
            "        operation_description=\"\"\"",
            "            Import data as labeling tasks in bulk using this API endpoint. You can use this API endpoint to import multiple tasks.",
            "            One POST request is limited at 250K tasks and 200 MB.",
            "",
            "            **Note:** Imported data is verified against a project *label_config* and must",
            "            include all variables that were used in the *label_config*. For example,",
            "            if the label configuration has a *$text* variable, then each item in a data object",
            "            must include a \"text\" field.",
            "            <br>",
            "",
            "            ## POST requests",
            "            <hr style=\"opacity:0.3\">",
            "",
            "            There are three possible ways to import tasks with this endpoint:",
            "",
            "            ### 1\\. **POST with data**",
            "            Send JSON tasks as POST data. Only JSON is supported for POSTing files directly.",
            "            Update this example to specify your authorization token and Label Studio instance host, then run the following from",
            "            the command line.",
            "",
            "            ```bash",
            "            curl -H 'Content-Type: application/json' -H 'Authorization: Token abc123' \\\\",
            "            -X POST '{host}/api/projects/1/import' --data '[{{\"text\": \"Some text 1\"}}, {{\"text\": \"Some text 2\"}}]'",
            "            ```",
            "",
            "            ### 2\\. **POST with files**",
            "            Send tasks as files. You can attach multiple files with different names.",
            "",
            "            - **JSON**: text files in JavaScript object notation format",
            "            - **CSV**: text files with tables in Comma Separated Values format",
            "            - **TSV**: text files with tables in Tab Separated Value format",
            "            - **TXT**: simple text files are similar to CSV with one column and no header, supported for projects with one source only",
            "",
            "            Update this example to specify your authorization token, Label Studio instance host, and file name and path,",
            "            then run the following from the command line:",
            "",
            "            ```bash",
            "            curl -H 'Authorization: Token abc123' \\\\",
            "            -X POST '{host}/api/projects/1/import' -F \u2018file=@path/to/my_file.csv\u2019",
            "            ```",
            "",
            "            ### 3\\. **POST with URL**",
            "            You can also provide a URL to a file with labeling tasks. Supported file formats are the same as in option 2.",
            "",
            "            ```bash",
            "            curl -H 'Content-Type: application/json' -H 'Authorization: Token abc123' \\\\",
            "            -X POST '{host}/api/projects/1/import' \\\\",
            "            --data '[{{\"url\": \"http://example.com/test1.csv\"}}, {{\"url\": \"http://example.com/test2.csv\"}}]'",
            "            ```",
            "",
            "            <br>",
            "        \"\"\".format(",
            "            host=(settings.HOSTNAME or 'https://localhost:8080')",
            "        ),",
            "    ),",
            ")",
            "# Import",
            "class ImportAPI(generics.CreateAPIView):",
            "    permission_required = all_permissions.projects_change",
            "    parser_classes = (JSONParser, MultiPartParser, FormParser)",
            "    serializer_class = ImportApiSerializer",
            "    queryset = Task.objects.all()",
            "",
            "    def get_serializer_context(self):",
            "        project_id = self.kwargs.get('pk')",
            "        if project_id:",
            "            project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=project_id)",
            "        else:",
            "            project = None",
            "        return {'project': project, 'user': self.request.user}",
            "",
            "    def post(self, *args, **kwargs):",
            "        return super(ImportAPI, self).post(*args, **kwargs)",
            "",
            "    def _save(self, tasks):",
            "        serializer = self.get_serializer(data=tasks, many=True)",
            "        serializer.is_valid(raise_exception=True)",
            "        task_instances = serializer.save(project_id=self.kwargs['pk'])",
            "        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs['pk'])",
            "        emit_webhooks_for_instance(",
            "            self.request.user.active_organization, project, WebhookAction.TASKS_CREATED, task_instances",
            "        )",
            "        return task_instances, serializer",
            "",
            "    def sync_import(self, request, project, preannotated_from_fields, commit_to_project, return_task_ids):",
            "        start = time.time()",
            "        tasks = None",
            "        # upload files from request, and parse all tasks",
            "        # TODO: Stop passing request to load_tasks function, make all validation before",
            "        parsed_data, file_upload_ids, could_be_tasks_list, found_formats, data_columns = load_tasks(request, project)",
            "",
            "        if preannotated_from_fields:",
            "            # turn flat task JSONs {\"column1\": value, \"column2\": value} into {\"data\": {\"column1\"..}, \"predictions\": [{...\"column2\"}]",
            "            parsed_data = reformat_predictions(parsed_data, preannotated_from_fields)",
            "",
            "        if commit_to_project:",
            "            # Immediately create project tasks and update project states and counters",
            "            tasks, serializer = self._save(parsed_data)",
            "            task_count = len(tasks)",
            "            annotation_count = len(serializer.db_annotations)",
            "            prediction_count = len(serializer.db_predictions)",
            "",
            "            recalculate_stats_counts = {",
            "                'task_count': task_count,",
            "                'annotation_count': annotation_count,",
            "                'prediction_count': prediction_count,",
            "            }",
            "",
            "            # Update counters (like total_annotations) for new tasks and after bulk update tasks stats. It should be a",
            "            # single operation as counters affect bulk is_labeled update",
            "            project.update_tasks_counters_and_task_states(",
            "                tasks_queryset=tasks,",
            "                maximum_annotations_changed=False,",
            "                overlap_cohort_percentage_changed=False,",
            "                tasks_number_changed=True,",
            "                recalculate_stats_counts=recalculate_stats_counts,",
            "            )",
            "            logger.info('Tasks bulk_update finished (sync import)')",
            "",
            "            project.summary.update_data_columns(parsed_data)",
            "            # TODO: project.summary.update_created_annotations_and_labels",
            "        else:",
            "            # Do nothing - just output file upload ids for further use",
            "            task_count = len(parsed_data)",
            "            annotation_count = None",
            "            prediction_count = None",
            "",
            "        duration = time.time() - start",
            "",
            "        response = {",
            "            'task_count': task_count,",
            "            'annotation_count': annotation_count,",
            "            'prediction_count': prediction_count,",
            "            'duration': duration,",
            "            'file_upload_ids': file_upload_ids,",
            "            'could_be_tasks_list': could_be_tasks_list,",
            "            'found_formats': found_formats,",
            "            'data_columns': data_columns,",
            "        }",
            "        if tasks and return_task_ids:",
            "            response['task_ids'] = [task.id for task in tasks]",
            "",
            "        return Response(response, status=status.HTTP_201_CREATED)",
            "",
            "    @timeit",
            "    def async_import(self, request, project, preannotated_from_fields, commit_to_project, return_task_ids):",
            "",
            "        project_import = ProjectImport.objects.create(",
            "            project=project,",
            "            preannotated_from_fields=preannotated_from_fields,",
            "            commit_to_project=commit_to_project,",
            "            return_task_ids=return_task_ids,",
            "        )",
            "",
            "        if len(request.FILES):",
            "            logger.debug(f'Import from files: {request.FILES}')",
            "            file_upload_ids, could_be_tasks_list = create_file_uploads(request.user, project, request.FILES)",
            "            project_import.file_upload_ids = file_upload_ids",
            "            project_import.could_be_tasks_list = could_be_tasks_list",
            "            project_import.save(update_fields=['file_upload_ids', 'could_be_tasks_list'])",
            "        elif 'application/x-www-form-urlencoded' in request.content_type:",
            "            logger.debug(f'Import from url: {request.data.get(\"url\")}')",
            "            # empty url",
            "            url = request.data.get('url')",
            "            if not url:",
            "                raise ValidationError('\"url\" is not found in request data')",
            "            project_import.url = url",
            "            project_import.save(update_fields=['url'])",
            "        # take one task from request DATA",
            "        elif 'application/json' in request.content_type and isinstance(request.data, dict):",
            "            project_import.tasks = [request.data]",
            "            project_import.save(update_fields=['tasks'])",
            "",
            "        # take many tasks from request DATA",
            "        elif 'application/json' in request.content_type and isinstance(request.data, list):",
            "            project_import.tasks = request.data",
            "            project_import.save(update_fields=['tasks'])",
            "",
            "        # incorrect data source",
            "        else:",
            "            raise ValidationError('load_tasks: No data found in DATA or in FILES')",
            "",
            "        start_job_async_or_sync(",
            "            async_import_background,",
            "            project_import.id,",
            "            request.user.id,",
            "            on_failure=set_import_background_failure,",
            "            project_id=project.id,",
            "            organization_id=request.user.active_organization.id,",
            "        )",
            "",
            "        response = {'import': project_import.id}",
            "        return Response(response, status=status.HTTP_201_CREATED)",
            "",
            "    def create(self, request, *args, **kwargs):",
            "        commit_to_project = bool_from_request(request.query_params, 'commit_to_project', True)",
            "        return_task_ids = bool_from_request(request.query_params, 'return_task_ids', False)",
            "        preannotated_from_fields = list_of_strings_from_request(request.query_params, 'preannotated_from_fields', None)",
            "",
            "        # check project permissions",
            "        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs['pk'])",
            "",
            "        if (",
            "            flag_set('fflag_feat_all_lsdv_4915_async_task_import_13042023_short', request.user)",
            "            and settings.VERSION_EDITION != 'Community'",
            "        ):",
            "            return self.async_import(request, project, preannotated_from_fields, commit_to_project, return_task_ids)",
            "        else:",
            "            return self.sync_import(request, project, preannotated_from_fields, commit_to_project, return_task_ids)",
            "",
            "",
            "# Import",
            "class ImportPredictionsAPI(generics.CreateAPIView):",
            "    permission_required = all_permissions.projects_change",
            "    parser_classes = (JSONParser, MultiPartParser, FormParser)",
            "    serializer_class = PredictionSerializer",
            "    queryset = Project.objects.all()",
            "    swagger_schema = None  # TODO: create API schema",
            "",
            "    def create(self, request, *args, **kwargs):",
            "        # check project permissions",
            "        project = self.get_object()",
            "",
            "        tasks_ids = set(Task.objects.filter(project=project).values_list('id', flat=True))",
            "",
            "        logger.debug(",
            "            f'Importing {len(self.request.data)} predictions to project {project} with {len(tasks_ids)} tasks'",
            "        )",
            "        predictions = []",
            "        for item in self.request.data:",
            "            if item.get('task') not in tasks_ids:",
            "                raise LabelStudioValidationErrorSentryIgnored(",
            "                    f'{item} contains invalid \"task\" field: corresponding task ID couldn\\'t be retrieved '",
            "                    f'from project {project} tasks'",
            "                )",
            "            predictions.append(",
            "                Prediction(",
            "                    task_id=item['task'],",
            "                    project_id=project.id,",
            "                    result=Prediction.prepare_prediction_result(item.get('result'), project),",
            "                    score=item.get('score'),",
            "                    model_version=item.get('model_version', 'undefined'),",
            "                )",
            "            )",
            "        predictions_obj = Prediction.objects.bulk_create(predictions, batch_size=settings.BATCH_SIZE)",
            "        project.update_tasks_counters(Task.objects.filter(id__in=tasks_ids))",
            "        return Response({'created': len(predictions_obj)}, status=status.HTTP_201_CREATED)",
            "",
            "",
            "class TasksBulkCreateAPI(ImportAPI):",
            "    # just for compatibility - can be safely removed",
            "    swagger_schema = None",
            "",
            "",
            "class ReImportAPI(ImportAPI):",
            "    permission_required = all_permissions.projects_change",
            "",
            "    def sync_reimport(self, project, file_upload_ids, files_as_tasks_list):",
            "        start = time.time()",
            "        tasks, found_formats, data_columns = FileUpload.load_tasks_from_uploaded_files(",
            "            project, file_upload_ids, files_as_tasks_list=files_as_tasks_list",
            "        )",
            "",
            "        with transaction.atomic():",
            "            project.remove_tasks_by_file_uploads(file_upload_ids)",
            "            tasks, serializer = self._save(tasks)",
            "        duration = time.time() - start",
            "",
            "        task_count = len(tasks)",
            "        annotation_count = len(serializer.db_annotations)",
            "        prediction_count = len(serializer.db_predictions)",
            "",
            "        # Update counters (like total_annotations) for new tasks and after bulk update tasks stats. It should be a",
            "        # single operation as counters affect bulk is_labeled update",
            "        project.update_tasks_counters_and_task_states(",
            "            tasks_queryset=tasks,",
            "            maximum_annotations_changed=False,",
            "            overlap_cohort_percentage_changed=False,",
            "            tasks_number_changed=True,",
            "            recalculate_stats_counts={",
            "                'task_count': task_count,",
            "                'annotation_count': annotation_count,",
            "                'prediction_count': prediction_count,",
            "            },",
            "        )",
            "        logger.info('Tasks bulk_update finished (sync reimport)')",
            "",
            "        project.summary.update_data_columns(tasks)",
            "        # TODO: project.summary.update_created_annotations_and_labels",
            "",
            "        return Response(",
            "            {",
            "                'task_count': task_count,",
            "                'annotation_count': annotation_count,",
            "                'prediction_count': prediction_count,",
            "                'duration': duration,",
            "                'file_upload_ids': file_upload_ids,",
            "                'found_formats': found_formats,",
            "                'data_columns': data_columns,",
            "            },",
            "            status=status.HTTP_201_CREATED,",
            "        )",
            "",
            "    def async_reimport(self, project, file_upload_ids, files_as_tasks_list, organization_id):",
            "",
            "        project_reimport = ProjectReimport.objects.create(",
            "            project=project, file_upload_ids=file_upload_ids, files_as_tasks_list=files_as_tasks_list",
            "        )",
            "",
            "        start_job_async_or_sync(",
            "            async_reimport_background,",
            "            project_reimport.id,",
            "            organization_id,",
            "            self.request.user,",
            "            on_failure=set_reimport_background_failure,",
            "            project_id=project.id,",
            "        )",
            "",
            "        response = {'reimport': project_reimport.id}",
            "        return Response(response, status=status.HTTP_201_CREATED)",
            "",
            "    @retry_database_locked()",
            "    def create(self, request, *args, **kwargs):",
            "        files_as_tasks_list = bool_from_request(request.data, 'files_as_tasks_list', True)",
            "        file_upload_ids = self.request.data.get('file_upload_ids')",
            "",
            "        # check project permissions",
            "        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs['pk'])",
            "",
            "        if not file_upload_ids:",
            "            return Response(",
            "                {",
            "                    'task_count': 0,",
            "                    'annotation_count': 0,",
            "                    'prediction_count': 0,",
            "                    'duration': 0,",
            "                    'file_upload_ids': [],",
            "                    'found_formats': {},",
            "                    'data_columns': [],",
            "                },",
            "                status=status.HTTP_200_OK,",
            "            )",
            "",
            "        if (",
            "            flag_set('fflag_fix_all_lsdv_4971_async_reimport_09052023_short', request.user)",
            "            and settings.VERSION_EDITION != 'Community'",
            "        ):",
            "            return self.async_reimport(",
            "                project, file_upload_ids, files_as_tasks_list, request.user.active_organization_id",
            "            )",
            "        else:",
            "            return self.sync_reimport(project, file_upload_ids, files_as_tasks_list)",
            "",
            "    @swagger_auto_schema(",
            "        auto_schema=None,",
            "        operation_summary='Re-import tasks',",
            "        operation_description=\"\"\"",
            "        Re-import tasks using the specified file upload IDs for a specific project.",
            "        \"\"\",",
            "    )",
            "    def post(self, *args, **kwargs):",
            "        return super(ReImportAPI, self).post(*args, **kwargs)",
            "",
            "",
            "@method_decorator(",
            "    name='get',",
            "    decorator=swagger_auto_schema(",
            "        tags=['Import'],",
            "        operation_summary='Get files list',",
            "        manual_parameters=[",
            "            openapi.Parameter(",
            "                name='all',",
            "                type=openapi.TYPE_BOOLEAN,",
            "                in_=openapi.IN_QUERY,",
            "                description='Set to \"true\" if you want to retrieve all file uploads',",
            "            ),",
            "            openapi.Parameter(",
            "                name='ids',",
            "                type=openapi.TYPE_ARRAY,",
            "                in_=openapi.IN_QUERY,",
            "                items=openapi.Schema(title='File upload ID', type=openapi.TYPE_INTEGER),",
            "                description='Specify the list of file upload IDs to retrieve, e.g. ids=[1,2,3]',",
            "            ),",
            "        ],",
            "        operation_description=\"\"\"",
            "        Retrieve the list of uploaded files used to create labeling tasks for a specific project.",
            "        \"\"\",",
            "    ),",
            ")",
            "@method_decorator(",
            "    name='delete',",
            "    decorator=swagger_auto_schema(",
            "        tags=['Import'],",
            "        operation_summary='Delete files',",
            "        operation_description=\"\"\"",
            "        Delete uploaded files for a specific project.",
            "        \"\"\",",
            "    ),",
            ")",
            "class FileUploadListAPI(generics.mixins.ListModelMixin, generics.mixins.DestroyModelMixin, generics.GenericAPIView):",
            "    parser_classes = (JSONParser, MultiPartParser, FormParser)",
            "    serializer_class = FileUploadSerializer",
            "    permission_required = ViewClassPermission(",
            "        GET=all_permissions.projects_view,",
            "        DELETE=all_permissions.projects_change,",
            "    )",
            "    queryset = FileUpload.objects.all()",
            "",
            "    def get_queryset(self):",
            "        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs.get('pk', 0))",
            "        if project.is_draft or bool_from_request(self.request.query_params, 'all', False):",
            "            # If project is in draft state, we return all uploaded files, ignoring queried ids",
            "            logger.debug(f'Return all uploaded files for draft project {project}')",
            "            return FileUpload.objects.filter(project_id=project.id, user=self.request.user)",
            "",
            "        # If requested in regular import, only queried IDs are returned to avoid showing previously imported",
            "        ids = json.loads(self.request.query_params.get('ids', '[]'))",
            "        logger.debug(f'File Upload IDs found: {ids}')",
            "        return FileUpload.objects.filter(project_id=project.id, id__in=ids, user=self.request.user)",
            "",
            "    def get(self, request, *args, **kwargs):",
            "        return self.list(request, *args, **kwargs)",
            "",
            "    def delete(self, request, *args, **kwargs):",
            "        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs['pk'])",
            "        ids = self.request.data.get('file_upload_ids')",
            "        if ids is None:",
            "            deleted, _ = FileUpload.objects.filter(project=project).delete()",
            "        elif isinstance(ids, list):",
            "            deleted, _ = FileUpload.objects.filter(project=project, id__in=ids).delete()",
            "        else:",
            "            raise ValueError('\"file_upload_ids\" parameter must be a list of integers')",
            "        return Response({'deleted': deleted}, status=status.HTTP_200_OK)",
            "",
            "",
            "@method_decorator(",
            "    name='get',",
            "    decorator=swagger_auto_schema(",
            "        tags=['Import'],",
            "        operation_summary='Get file upload',",
            "        operation_description='Retrieve details about a specific uploaded file.',",
            "    ),",
            ")",
            "@method_decorator(",
            "    name='patch',",
            "    decorator=swagger_auto_schema(",
            "        tags=['Import'],",
            "        operation_summary='Update file upload',",
            "        operation_description='Update a specific uploaded file.',",
            "        request_body=FileUploadSerializer,",
            "    ),",
            ")",
            "@method_decorator(",
            "    name='delete',",
            "    decorator=swagger_auto_schema(",
            "        tags=['Import'],",
            "        operation_summary='Delete file upload',",
            "        operation_description='Delete a specific uploaded file.',",
            "    ),",
            ")",
            "class FileUploadAPI(generics.RetrieveUpdateDestroyAPIView):",
            "    parser_classes = (JSONParser, MultiPartParser, FormParser)",
            "    permission_classes = (IsAuthenticated,)",
            "    serializer_class = FileUploadSerializer",
            "    queryset = FileUpload.objects.all()",
            "",
            "    def get(self, *args, **kwargs):",
            "        return super(FileUploadAPI, self).get(*args, **kwargs)",
            "",
            "    def patch(self, *args, **kwargs):",
            "        return super(FileUploadAPI, self).patch(*args, **kwargs)",
            "",
            "    def delete(self, *args, **kwargs):",
            "        return super(FileUploadAPI, self).delete(*args, **kwargs)",
            "",
            "    @swagger_auto_schema(auto_schema=None)",
            "    def put(self, *args, **kwargs):",
            "        return super(FileUploadAPI, self).put(*args, **kwargs)",
            "",
            "",
            "class UploadedFileResponse(generics.RetrieveAPIView):",
            "    permission_classes = (IsAuthenticated,)",
            "",
            "    @override_report_only_csp",
            "    @csp(SANDBOX=[])",
            "    @swagger_auto_schema(auto_schema=None)",
            "    def get(self, *args, **kwargs):",
            "        request = self.request",
            "        filename = kwargs['filename']",
            "        # XXX needed, on windows os.path.join generates '\\' which breaks FileUpload",
            "        file = settings.UPLOAD_DIR + ('/' if not settings.UPLOAD_DIR.endswith('/') else '') + filename",
            "        logger.debug(f'Fetch uploaded file by user {request.user} => {file}')",
            "        file_upload = FileUpload.objects.filter(file=file).last()",
            "",
            "        if not file_upload.has_permission(request.user):",
            "            return Response(status=status.HTTP_403_FORBIDDEN)",
            "",
            "        file = file_upload.file",
            "        if file.storage.exists(file.name):",
            "            content_type, encoding = mimetypes.guess_type(str(file.name))",
            "            content_type = content_type or 'application/octet-stream'",
            "            return RangedFileResponse(request, file.open(mode='rb'), content_type=content_type)",
            "",
            "        return Response(status=status.HTTP_404_NOT_FOUND)",
            "",
            "",
            "class DownloadStorageData(APIView):",
            "    \"\"\"Check auth for nginx auth_request\"\"\"",
            "",
            "    swagger_schema = None",
            "    http_method_names = ['get']",
            "    permission_classes = (IsAuthenticated,)",
            "",
            "    def get(self, request, *args, **kwargs):",
            "        \"\"\"Get export files list\"\"\"",
            "        request = self.request",
            "        filepath = request.GET.get('filepath')",
            "        if filepath is None:",
            "            return Response(status=status.HTTP_404_NOT_FOUND)",
            "",
            "        filepath = unquote(request.GET['filepath'])",
            "",
            "        url = None",
            "        if filepath.startswith(settings.UPLOAD_DIR):",
            "            logger.debug(f'Fetch uploaded file by user {request.user} => {filepath}')",
            "            file_upload = FileUpload.objects.filter(file=filepath).last()",
            "",
            "            if file_upload is not None and file_upload.has_permission(request.user):",
            "                url = file_upload.file.storage.url(file_upload.file.name, storage_url=True)",
            "        elif filepath.startswith(settings.AVATAR_PATH):",
            "            user = User.objects.filter(avatar=filepath).first()",
            "            if user is not None and request.user.active_organization.has_user(user):",
            "                url = user.avatar.storage.url(user.avatar.name, storage_url=True)",
            "",
            "        if url is None:",
            "            return Response(status=status.HTTP_403_FORBIDDEN)",
            "",
            "        protocol = urlparse(url).scheme",
            "",
            "        # Let NGINX handle it",
            "        response = HttpResponse()",
            "        # The below header tells NGINX to catch it and serve, see docker-config/nginx-app.conf",
            "        redirect = '/file_download/' + protocol + '/' + url.replace(protocol + '://', '')",
            "",
            "        response['X-Accel-Redirect'] = redirect",
            "        response['Content-Disposition'] = 'attachment; filename=\"{}\"'.format(filepath)",
            "        return response",
            "",
            "",
            "class PresignAPIMixin:",
            "    def handle_presign(self, request: HttpRequest, fileuri: str, instance: Union[Task, Project]) -> Response:",
            "        model_name = type(instance).__name__",
            "",
            "        if not instance.has_permission(request.user):",
            "            return Response(status=status.HTTP_403_FORBIDDEN)",
            "",
            "        # Attempt to base64 decode the fileuri",
            "        try:",
            "            fileuri = base64.urlsafe_b64decode(fileuri.encode()).decode()",
            "        # For backwards compatibility, try unquote if this fails",
            "        except Exception as exc:",
            "            logger.debug(",
            "                f'Failed to decode base64 {fileuri} for {model_name} {instance.id}: {exc} falling back to unquote'",
            "            )",
            "            fileuri = unquote(fileuri)",
            "",
            "        try:",
            "            resolved = instance.resolve_storage_uri(fileuri)",
            "        except Exception as exc:",
            "            logger.error(f'Failed to resolve storage uri {fileuri} for {model_name} {instance.id}: {exc}')",
            "            return Response(status=status.HTTP_404_NOT_FOUND)",
            "",
            "        if resolved is None or resolved.get('url') is None:",
            "            return Response(status=status.HTTP_404_NOT_FOUND)",
            "",
            "        url = resolved['url']",
            "        max_age = 0",
            "        if resolved.get('presign_ttl'):",
            "            max_age = resolved.get('presign_ttl') * 60",
            "",
            "        # Proxy to presigned url",
            "        response = HttpResponseRedirect(redirect_to=url, status=status.HTTP_303_SEE_OTHER)",
            "        response.headers['Cache-Control'] = f'no-store, max-age={max_age}'",
            "",
            "        return response",
            "",
            "",
            "class TaskPresignStorageData(PresignAPIMixin, APIView):",
            "    \"\"\"A file proxy to presign storage urls at the task level.\"\"\"",
            "",
            "    swagger_schema = None",
            "    http_method_names = ['get']",
            "    permission_classes = (IsAuthenticated,)",
            "",
            "    def get(self, request, *args, **kwargs):",
            "        \"\"\"Get the presigned url for a given fileuri\"\"\"",
            "        request = self.request",
            "        task_id = kwargs.get('task_id')",
            "        fileuri = request.GET.get('fileuri')",
            "",
            "        if fileuri is None or task_id is None:",
            "            return Response(status=status.HTTP_400_BAD_REQUEST)",
            "",
            "        try:",
            "            task = Task.objects.get(pk=task_id)",
            "        except Task.DoesNotExist:",
            "            return Response(status=status.HTTP_404_NOT_FOUND)",
            "",
            "        return self.handle_presign(request, fileuri, task)",
            "",
            "",
            "class ProjectPresignStorageData(PresignAPIMixin, APIView):",
            "    \"\"\"A file proxy to presign storage urls at the project level.\"\"\"",
            "",
            "    swagger_schema = None",
            "    http_method_names = ['get']",
            "    permission_classes = (IsAuthenticated,)",
            "",
            "    def get(self, request, *args, **kwargs):",
            "        \"\"\"Get the presigned url for a given fileuri\"\"\"",
            "        request = self.request",
            "        project_id = kwargs.get('project_id')",
            "        fileuri = request.GET.get('fileuri')",
            "",
            "        if fileuri is None or project_id is None:",
            "            return Response(status=status.HTTP_400_BAD_REQUEST)",
            "",
            "        try:",
            "            project = Project.objects.get(pk=project_id)",
            "        except Project.DoesNotExist:",
            "            return Response(status=status.HTTP_404_NOT_FOUND)",
            "",
            "        return self.handle_presign(request, fileuri, project)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "616": [
                "UploadedFileResponse",
                "get"
            ],
            "617": [
                "UploadedFileResponse",
                "get"
            ]
        },
        "addLocation": [
            "label_studio.data_import.api.UploadedFileResponse.self"
        ]
    }
}