import csv
import json
import re
from concurrent.futures import ThreadPoolExecutor
import networkx as nx
import requests
from bs4 import BeautifulSoup
import ast
import os
from PathFind.core.tool.Jarvis import jarvis_callgraph_gen

# 爬取github 在中国必须使用代理，必然不稳定
proxes = {
    'http': 'http://127.0.0.1:10809',
    'https': 'http://127.0.0.1:10809',
}

# 爬取github 添加github cookie
headers = {
    'Cookie': '_octo=GH1.1.1605781938.1721051173; _device_id=12e0df83ac478d985b39cca659901d3a; saved_user_sessions=33680932%3A4tNro4ZAUxmuaH0TytD5ejboZ0zXgKXTQTu_o_IMKnM-vqgb; user_session=4tNro4ZAUxmuaH0TytD5ejboZ0zXgKXTQTu_o_IMKnM-vqgb; __Host-user_session_same_site=4tNro4ZAUxmuaH0TytD5ejboZ0zXgKXTQTu_o_IMKnM-vqgb; logged_in=yes; dotcom_user=xiaotea; color_mode=%7B%22color_mode%22%3A%22auto%22%2C%22light_theme%22%3A%7B%22name%22%3A%22light%22%2C%22color_mode%22%3A%22light%22%7D%2C%22dark_theme%22%3A%7B%22name%22%3A%22dark%22%2C%22color_mode%22%3A%22dark%22%7D%7D; cpu_bucket=xlg; preferred_color_mode=light; tz=Asia%2FShanghai; _gh_sess=kn17A7%2BSG6vQOhA4NqoqFUeh5hHA%2FhX%2FEJ23H8QitU3aqnWRvCD9pGJDduLc0kc8k%2F%2F%2B%2FEac4rhG1beuPH%2BtY3cPjXLbchFL9cuTih5T99F9OqHbWDNtMQ%2B%2F1LvARKbM52sWI2iS7nokjpNYc3pgBoTw4m4HEJ0E5DEV%2BgncSTdASACwAGNXuTs%2BaoNuwLim8aLi01V4y7IDo0y3ExB1p0HvcZXxpo9wBek9h6BsEyTHL5SSdTLSvhmbKO32Pn9hAMdoo7T%2FlJSEsUb8NlmxoLDZRLH4wTDdjfJlV3fw5%2BBXHV2S7laHZQbhL2mXcbWRkojwJxAngbN9rE9bZSWMviyxBkzsZmQVMrkI0A%3D%3D--MRdrpg9To4wL1qsu--Xy5QOI%2BpIknB3g5bkZuaDQ%3D%3D',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'
}

patch_path = 'D:\D\个人文件\开源软件供应链安全\python漏洞影响性分析实验\知识图谱创建\整理好的\patchDictNew'


def findAllFileName(folder_path=None,endsfile_type=None):
    # 获取当前文件夹路径
    folder_path = folder_path or patch_path
    file_list = []
    # 遍历当前文件夹并打印所有文件名
    for filename in os.listdir(folder_path):
        if os.path.isfile(os.path.join(folder_path, filename)):
            file_list.append(filename)
    if endsfile_type is not None:
        file_list = [x for x in file_list if x.endswith(endsfile_type)]
    return file_list


def find_all_files(folder_path, endsfile_type=None):
    return [
        os.path.join(root,filename) for root, _, files in os.walk(folder_path)
        for filename in files if not endsfile_type or filename.endswith(endsfile_type)
    ]


def find_class_function(code, line_number_list):
    class_function_dict = {}  # 存储每个行号对应的类和函数名字的字典
    last_class_function_list = {}  # 存储最后一次遍历到的类和函数名字的字典，用于在函数内部更新class_function_dict
    if line_number_list == []:
        return class_function_dict

    def visit(node, class_name_dict=None):
        nonlocal class_function_dict
        class_name_dict = class_name_dict or {str(line_num): [] for line_num in line_number_list}

        if isinstance(node, ast.ClassDef) or isinstance(node, ast.FunctionDef):
            # 遍历每个待识别行号
            for line_number in line_number_list:
                # 判断节点起始行号和结束行号是否包含该行号
                if node.lineno <= line_number <= node.end_lineno:
                    # 将节点的名字添加到对应行号的类或函数名字列表中
                    class_name_dict[str(line_number)].append(node.name)

        if hasattr(node, 'lineno'):
            class_function_dict = class_name_dict  # 更新class_function_dict为当前的class_name_dict

        if hasattr(node, 'body'):
            for child_node in node.body:
                visit(child_node, class_name_dict)  # 递归遍历子节点，传入class_name_dict来保持上下文信息

    tree = ast.parse(code)
    visit(tree)

    return class_function_dict


def find_line_name(code, line_number_list):
    line_name_dict = {str(line_number): [] for line_number in line_number_list}
    if line_number_list == []:
        return {}

    try:
        # 使用 ast 模块解析 Python 代码
        # 转换 contents 字符串为 AST（抽象语法树）
        tree = ast.parse(code)
        # 遍历 AST，查找是否存在 __all__ 变量定义
        for node in ast.walk(tree):
            if isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name) and hasattr(target, 'lineno'):
                        if str(target.lineno) and str(target.lineno) in line_number_list:
                            line_name_dict[str(target.lineno)] = [target.id]
    except:
        return line_name_dict

    return line_name_dict


def downLoadPatch(patchUrlStr):
    patchhtml = requests.get(patchUrlStr, proxies=proxes, headers=headers, timeout=20)
    patchhtmlstr = patchhtml.text
    return patchhtmlstr


def analyzePatchHtml(patchhtmlstr):
    # print(patchhtmlstr)

    soup = BeautifulSoup(patchhtmlstr, 'html.parser')
    beforePatchFileStr = soup.find('script',
                                   {'type': 'application/json', 'data-target': 'react-app.embeddedData'}).get_text()
    FileDict = json.loads(beforePatchFileStr)
    PatchFileStrList = FileDict['payload']['diffEntryData']

    new_sha1 = FileDict['payload']["commit"]["sha1"]

    frontPatchUrl = re.sub(r'/commit/[a-f0-9]{2,40}', f'/commit/{new_sha1}', FileDict['payload']["commit"]["url"])
    frontPatchUrl = "https://github.com" + frontPatchUrl

    patchDict = {}
    for PatchFileDict in PatchFileStrList:
        PatchLineNum = 0

        file_name = PatchFileDict['path']
        if not file_name.endswith('.py'):
            continue
        if file_name.startswith('test/') or file_name.startswith('tests/'):
            continue
        if "diffLines" not in PatchFileDict:
            continue
        patchDict[file_name] = {}
        patchDict[file_name]['Patch'] = {}

        for diffLine in PatchFileDict["diffLines"]:
            PatchLineNumstr = str(PatchLineNum)
            OldRowNumber = diffLine["left"]
            NewRowNumber = diffLine["right"]
            PatchRowcode = diffLine["text"]
            if diffLine["type"] == "HUNK":
                continue
            elif diffLine["type"] == "ADDITION":
                OldRowNumber = ""
            elif diffLine["type"] == "DELETION":
                NewRowNumber = ""

            patchDict[file_name]['Patch'][PatchLineNumstr] = {
                'beforePatchRowNumber': OldRowNumber,
                'afterPatchRowNumber': NewRowNumber,
                'PatchRowcode': PatchRowcode
            }
            PatchLineNum += 1

    return frontPatchUrl, patchDict


def getPatchFile(PatchFileName, commitUrl):
    PatchFileNameUrl = commitUrl[::-1].replace('/commit/'[::-1], '/blob/'[::-1])[::-1] + '/' + PatchFileName
    # try:
    res = requests.get(PatchFileNameUrl, proxies=proxes, headers=headers, timeout=20)
    soup = BeautifulSoup(res.text, 'html.parser')

    beforePatchFileStr = soup.find('script',
                                   {'type': 'application/json', 'data-target': 'react-app.embeddedData'}).get_text()

    FileDict = json.loads(beforePatchFileStr)
    PatchFileStrList = FileDict['payload']['blob']['rawLines']

    # except Exception as e:
    #     return []
    return PatchFileStrList


def getPatchInfo(PatchUrl):
    patchhtmlstr = downLoadPatch(PatchUrl)

    frontPatchUrl, patchDict = analyzePatchHtml(patchhtmlstr)
    afterPatchUrl = PatchUrl

    for fileName in patchDict:
        if "→" in fileName:
            continue
        patchDict[fileName]['frontPatchFile'] = getPatchFile(fileName, frontPatchUrl)
        patchDict[fileName]['afterPatchFile'] = getPatchFile(fileName, afterPatchUrl)
    return patchDict,frontPatchUrl


def convert_array(arr):
    converted_arr = arr.copy()
    n = len(arr)
    isconvertFlagset = set()
    lFlag = 0
    while lFlag < n:
        if arr[lFlag] == '0':
            lFlag = lFlag + 1
            continue
        else:
            rflag = lFlag
            while rflag < n:
                if arr[rflag] == '-1' or arr[rflag] == '1':
                    isconvertFlagset.add(arr[rflag])
                    rflag = rflag + 1
                else:
                    break
            if len(isconvertFlagset) == 2:
                for i in range(lFlag, rflag):
                    converted_arr[i] = '2'
            lFlag = rflag

    return converted_arr


def actionAnalyze(PatchinfoDict):
    patchLine = 0
    actionList = []
    # -1 新增  1 删除   2 修改
    # print(PatchinfoDict)
    while str(patchLine) in PatchinfoDict:
        if PatchinfoDict[str(patchLine)]["beforePatchRowNumber"] == '':
            actionList.append('-1')
        elif PatchinfoDict[str(patchLine)]["afterPatchRowNumber"] == '':
            actionList.append('1')
        else:
            actionList.append('0')
        patchLine = patchLine + 1

    newactionList = convert_array(actionList)
    return newactionList


def codeLocationAnalyze(patchDict, commit_id, vul_commit_id):
    vul_pyfile_dir = 'vul_file/' + vul_commit_id + '/'
    # print(json.dumps(patchDict))
    for patch_file in patchDict:
        vul_file_path = vul_pyfile_dir + patch_file
        before_patch_code = '\n'.join(patchDict[patch_file]['frontPatchFile'])
        folder_path = os.path.dirname(vul_file_path)
        os.makedirs(folder_path, exist_ok=True)
        with open(vul_file_path, 'w', encoding='utf-8') as addfile:
            addfile.write(before_patch_code)
    if patchDict:
        vul_pyfile_list = find_all_files(vul_pyfile_dir, endsfile_type=".py")
        print(vul_pyfile_list)
        vul_call_graph = call_graph_generator(vul_pyfile_list, input_pkg=vul_pyfile_dir)

    for patch_file in patchDict:
        #  遍历每个待修改的文件
        linelength = len(patchDict[patch_file]['action'])
        add_list, revise_list, dele_list = [], [], []
        for line_numnber in range(linelength):
            if patchDict[patch_file]['action'][line_numnber] == '-1':
                add_list.append(patchDict[patch_file]['Patch'][str(line_numnber)]['afterPatchRowNumber'])
            if patchDict[patch_file]['action'][line_numnber] == '1':
                dele_list.append(patchDict[patch_file]['Patch'][str(line_numnber)]['beforePatchRowNumber'])
            if patchDict[patch_file]['action'][line_numnber] == '2':
                if patchDict[patch_file]['Patch'][str(line_numnber)]['beforePatchRowNumber'] != '':
                    revise_list.append(patchDict[patch_file]['Patch'][str(line_numnber)]['beforePatchRowNumber'])

        # 修改前后代码
        before_patch_code = '\n'.join(patchDict[patch_file]['frontPatchFile'])
        after_patch_code = '\n'.join(patchDict[patch_file]['afterPatchFile'])

        # 整理为int类型，排序去重
        before_line_list = sorted(list(set([int(line_numstr) for line_numstr in dele_list + revise_list])))
        after_line_list = sorted(list(set([int(line_numstr) for line_numstr in add_list])))

        before_line_list = find_class_function(before_patch_code, before_line_list)
        after_line_list = find_class_function(after_patch_code, after_line_list)

        # 若定位不到类或者函数，则定位变量值
        before_var_name_dict = find_line_name(before_patch_code, [line_str for line_str in before_line_list if
                                                                  before_line_list[line_str] == []])
        after_var_name_dict = find_line_name(after_patch_code, [line_str for line_str in after_line_list if
                                                                after_line_list[line_str] == []])

        # 函数、类定位和变量值合并
        for line_str in before_line_list:
            if before_line_list[line_str] == []:
                before_line_list[line_str] = before_var_name_dict[line_str]
        for line_str in after_line_list:
            if after_line_list[line_str] == []:
                after_line_list[line_str] = after_var_name_dict[line_str]

        # all_fun_dict = find_class_function(before_patch_code, [line for line in range(1, len(
        #     patchDict[patch_file]['frontPatchFile']) + 1)])
        # all_fun_set = set()
        # for line in all_fun_dict:
        #     if all_fun_dict[line] == []:
        #         continue
        #     all_fun_set.add('.'.join(all_fun_dict[line]))

        # 处理添加函数定位
        add_class_function_list = []
        file_name = patch_file
        if '/' in file_name:
            file_name = file_name.split('/')[-1]

        vul_file_path = 'vul_file/' + commit_id + '/' + patch_file
        folder_path = os.path.dirname(vul_file_path)
        os.makedirs(folder_path, exist_ok=True)
        with open(vul_file_path, 'w', encoding='utf-8') as addfile:
            addfile.write(after_patch_code)

        for add_class_function_line in after_line_list:
            if after_line_list[add_class_function_line] == []:
                continue
            add_class_function_list.append('.'.join(after_line_list[add_class_function_line]))
        add_class_function_list = list(set(add_class_function_list))
        add_class_function_list = [os.path.splitext(file_name)[0] + '.' + add_class_function for add_class_function in
                                   add_class_function_list]

        add_fun_used_fun_list = analyze_used_fun(vul_file_path, add_class_function_list,
                                                 input_pkg='vul_file/' + commit_id)
        # all_fun_set = {os.path.splitext(file_name)[0] + '.' + item for item in all_fun_set}
        # print(vul_call_graph)
        # print(commit_id, "使用的", add_fun_used_fun_list)
        add_fun_used_fun_before = set(vul_call_graph.keys()) & set(add_fun_used_fun_list)
        # print(commit_id, "漏洞前的", add_fun_used_fun_before)
        # add_fun_used_fun_before = [item[len(os.path.splitext(file_name)[0]) + 1:] for item in add_fun_used_fun_before]

        patchDict[patch_file]['dele_reviseLocation'] = before_line_list
        patchDict[patch_file]['addLocation'] = list(add_fun_used_fun_before)
        # print(commit_id, add_fun_used_fun_before)
    return patchDict


def call_graph_generator(py_file_list, input_pkg=None):
    cg = jarvis_callgraph_gen(py_file_list, package=input_pkg)
    return cg


def analyze_used_fun(analyze_file, add_funList, input_pkg=None):
    if add_funList == []:
        return []
    entry_point = [analyze_file]

    call_graph = call_graph_generator(entry_point, input_pkg=input_pkg)

    add_fun_in_callgraph = []
    for key in call_graph:
        for add_fun in add_funList:
            _add_fun = add_fun
            if _add_fun.startswith('__init__.'):
                _add_fun = _add_fun.replace('__init__.', '', 1)
            if key.endswith(_add_fun):
                add_fun_in_callgraph.append(key)

    # print(call_graph)
    # 创建有向图并添加节点、边缘
    graph = nx.DiGraph()
    for node in call_graph:
        graph.add_node(node)
    for node, calls in call_graph.items():
        for call in calls:
            graph.add_edge(node, call)
    # 分析调用路径
    result = []
    for fun in add_fun_in_callgraph:
        if fun in graph.nodes:
            callers = []
            visited = set()  # 记录已访问的节点，避免重复访问
            dfs(fun, graph, visited, callers)
            result = result + callers

    return list(set(result))


def dfs(curr_node, graph, visited, callers):
    visited.add(curr_node)
    # 遍历当前节点的所有直接调用者
    for caller in graph.predecessors(curr_node):
        # 如果调用者不是当前节点本身，并且没有被访问过，则将其添加到结果列表中，并继续深度优先搜索
        if caller != curr_node and caller not in visited:
            callers.append(caller)
            # dfs(caller, graph, visited, callers)


def main(commit_url):
    # try:
    if '/pull/' in commit_url:
        commit_url = re.sub(r'/pull/\d+/', r'/', commit_url)
        commit_url = re.sub(r'/commits/', r'/commit/', commit_url)
    commit_id = re.search(r"commit/(.+)", commit_url).group(1)

    if commit_id in file_list:
        return

    print(commit_url)

    # if not commit_id.startswith('2e40b'):
    #     continue

    patchDict,frontPatchUrl = getPatchInfo(commit_url)
    # print(patchDict)

    dealpatchDict = patchDict.copy()
    for file in patchDict:
        newactionList = actionAnalyze(patchDict[file]['Patch'])
        dealpatchDict[file]['action'] = newactionList

    dealpatchDict = codeLocationAnalyze(dealpatchDict, commit_id,frontPatchUrl.split('/')[-1])

    with open(os.path.join(patch_path, commit_id), 'w', encoding='utf-8') as file:
        file.write(json.dumps(dealpatchDict, indent=4))

    # except Exception as e:
    #     print(e)


if __name__ == "__main__":
    commit_dict = {}
    file_list = findAllFileName()
    with open(r'F:\python_project\all_vul\vul_patch_人工二次寻找.csv', 'r',
              encoding='latin-1') as file:
        reader = csv.reader(file)
        for row in reader:
            commit = row[1].split('\n')
            commit = [x for x in commit if 'github' in x]
            commit_dict[row[0]] = commit
            # print(commit)
    with ThreadPoolExecutor(max_workers=5) as executor:
        for cvename in commit_dict:
            for commit_url in commit_dict[cvename]:
                # if 'a411c944af78c36f2fdb87d305ba452dc52d7ed3' not in commit_url:
                #     continue
                # main(commit_url)
                executor.submit(main, commit_url)
